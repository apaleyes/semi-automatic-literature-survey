doi,title,publisher,content_type,abstract,html_url,publication_title,publication_date,database,query_name,query_value
10.1109/ICRA.2019.8793690,A Fog Robotics Approach to Deep Robot Learning: Application to Object Recognition and Grasp Planning in Surface Decluttering,IEEE,Conferences,"The growing demand of industrial, automotive and service robots presents a challenge to the centralized Cloud Robotics model in terms of privacy, security, latency, bandwidth, and reliability. In this paper, we present a `Fog Robotics' approach to deep robot learning that distributes compute, storage and networking resources between the Cloud and the Edge in a federated manner. Deep models are trained on non-private (public) synthetic images in the Cloud; the models are adapted to the private real images of the environment at the Edge within a trusted network and subsequently, deployed as a service for low-latency and secure inference/prediction for other robots in the network. We apply this approach to surface decluttering, where a mobile robot picks and sorts objects from a cluttered floor by learning a deep object recognition and a grasp planning model. Experiments suggest that Fog Robotics can improve performance by sim-to-real domain adaptation in comparison to exclusively using Cloud or Edge resources, while reducing the inference cycle time by 4× to successfully declutter 86% of objects over 213 attempts.",https://ieeexplore.ieee.org/document/8793690/,2019 International Conference on Robotics and Automation (ICRA),20-24 May 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRAE53653.2021.9657761,A ROS Based Open Source Simulation Environment for Robotics Beginners,IEEE,Conferences,"This paper presents an open source robot simulation environment based on the robot operating system (ROS). To help novice to learn robotics, we have designed several important experiments that most robotics beginners will need to practice. Our simulation environment provides an efficient, safe, and transferable testing and operating environment for the rapid verification of robot algorithms. Through practicing the experiments in this paper, robotics beginners can learn robotics knowledge faster, better and at a lower cost, which should great facilitate them to operate robots in real-world applications. Source code, simulation video and detailed wiki for the simulation environment are available online at https://github.com/Suyixiu/robot_sim.",https://ieeexplore.ieee.org/document/9657761/,2021 6th International Conference on Robotics and Automation Engineering (ICRAE),19-22 Nov. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICACTE.2008.125,A Simple and Robust Persian Speech Recognition System and Its Application to Robotics,IEEE,Conferences,"In this paper, a Persian speech recognition system is proposed to recognize Persian isolated spoken words. The main contribution of this work in comparison with the previous ones is simplicity and generality. The proposed system can be widely used in various real world applications when the designer does not need so much expertise in pattern and speech recognition. Due to generality, robustness, computational simplicity and reachability, general frequency domain feature as well as multi-layer percepron neural network as classifier are considered. To demonstrate the efficiency of the proposed speech recognition system, a wheeled mobile robot is navigated in a real domestic environment via Persian spoken commands. The results indicate the high potential of the proposed system to deal with real world applications.",https://ieeexplore.ieee.org/document/4736958/,2008 International Conference on Advanced Computer Theory and Engineering,20-22 Dec. 2008,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ROBIO.2009.4913063,A bio-inspired haptic interface for tele-robotics applications,IEEE,Conferences,"This paper presents the design concept for a bio-inspired exoskeleton intended for applications in tele-robotics and virtual reality. We based the development on an attentive analysis of the human arm anatomy with the intent to synthesize a system that will be able to interface with the human limb in a natural way. Our main goal is to develop a multi contact-point haptic interface that does not restrict the arm mobility and therefore increases the operational workspace. We propose a simplified kinematic model of the human arm using a notation coming from the robotics field. To figure out the best kinematic architecture we employed real movement data, measured from a human subject, and integrated them with the kinematic model of the exoskeleton. This allows us to test the system before its construction and to formalize specific requirements. We also implemented and tested a first passive version of the shoulder joint.",https://ieeexplore.ieee.org/document/4913063/,2008 IEEE International Conference on Robotics and Biomimetics,22-25 Feb. 2009,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ROBIO.2014.7090308,A chaotic neural network as motor path generator for mobile robotics,IEEE,Conferences,This work aims at developing a motor path generator for applications in mobile robotics based on a chaotic neural network. The computational paradigm inspired by the neural structure of microcircuits located in the human prefrontal cortex is adapted to work in real-time and used to generate the joints trajectories of a lightweight quadruped robot. The recurrent neural network was implemented in Matlab and a software framework was developed to test the performances of the system with the robot dynamic model. Preliminary results demonstrate the capability of the neural controller to learn period signals in a short period of time allowing adaptation during the robot operation.,https://ieeexplore.ieee.org/document/7090308/,2014 IEEE International Conference on Robotics and Biomimetics (ROBIO 2014),5-10 Dec. 2014,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ETFA.1999.815411,Advanced control techniques based in artificial intelligence for robotics manipulators,IEEE,Conferences,"The performance quality in nonlinear model based control of mechanical manipulators is conditioned to the reliability of the mathematical model and precision in the knowledge of all the involved parameters. Control methods based on artificial intelligence techniques (learning algorithms, system identification and neural networks) can be applied to improve its performance. A neural control scheme is proposed, consisting basically of a neural network for learning the robot inverse dynamics and online generating the control signal. Also an online supervision based on optimisation techniques is designed and implemented for such neural control. Simulation results are provided to evaluate the alternative variations to the proposed central scheme.",https://ieeexplore.ieee.org/document/815411/,1999 7th IEEE International Conference on Emerging Technologies and Factory Automation. Proceedings ETFA '99 (Cat. No.99TH8467),18-21 Oct. 1999,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICEKIM52309.2021.00040,Application of Teaching Innovation Based on robotics engineering,IEEE,Conferences,"As the core major of “Internet + Industrial Intelligence”, robotics engineering is an upgrade and reconstruction of traditional engineering major. The industrial robot course is the professional core course of the Robotics Engineering. It is also a comprehensive course of multi-discipline integration, which involved mechanical engineering, automatic control, computer, sensor, electronic technology, artificial intelligence and other multi-disciplinary content. Robotics Engineering is characterized by broad foundation, great difficulty, emphasis on practice, rapid development and application of new knowledge. In the process of implementation of the teaching innovation, the new concept of engineering education was applied to propose a new form of curriculum system. Taking the projects of engineering as the study objects, disassemble the knowledge points involved in industrial robots, break the course boundaries, reshape the knowledge system, draw knowledge maps and then design teaching activities. In teaching innovation, teachers extend classroom through formation of subject competition teams, promote teaching and promote learning by competition, realize the integration of “teaching, class and competition”, build a bridge between theory and practice, then complete the transformation from knowledge learning to ability training. Besides, they also keep contact with intelligent manufacturing enterprises in Zhuhai and the Bay Area to obtain real-time new developments in enterprises. Thus, the latest information was introduced into classroom. Therefore, the meaning of “production, teaching, research and application” has been deepened. According to the characteristics of the knowledge points of the course, experts were invited to make special lectures for students which can bring them with international perspective and frontier knowledge.",https://ieeexplore.ieee.org/document/9479656/,"2021 2nd International Conference on Education, Knowledge and Information Management (ICEKIM)",29-31 Jan. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ROBOT.2000.844768,Application of automatic action planning for several work cells to the German ETS-VII space robotics experiments,IEEE,Conferences,"Experiences in space robotics show, that the user normally has to cope with a huge amount of data. So, only robot and mission specialists are able to control the robot arm directly in teleoperation mode. By means of an intelligent robot control in cooperation with virtual reality methods, it is possible for non-robot specialists to generate tasks for a robot or an automation component intuitively. Furthermore, the intelligent robot control improves the safety of the entire system. The on-ground robot control and command station for the robot arm ERA onboard the satellite ETS-VII builds on a new resource-based action planning approach to manage robot manipulators and other automation components. In the case of ERA, the action planning system also takes care of the ""real"" robot onboard the satellite and the ""virtual"" robot in the simulation system. By means of the simulation system, the user can plan tasks ahead as well as analyze and visualize different strategies. The paper describes the mechanism of resource-based action planning, its application to different work cells, the practical experiences gained from the implementation for the on-ground robot control and command station for the robot arm ERA developed in the GETEX project as well as the services it provides to support VR-based man machine interfaces.",https://ieeexplore.ieee.org/document/844768/,Proceedings 2000 ICRA. Millennium Conference. IEEE International Conference on Robotics and Automation. Symposia Proceedings (Cat. No.00CH37065),24-28 April 2000,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICACITE51222.2021.9404749,Artificial Intelligence and Robotics: Impact & Open issues of automation in Workplace,IEEE,Conferences,"In engineering province robotics is one of the cognitive perspective to human communication or it concern with synod of perception of action. In Today's Tech World Artificial Intelligence is an essential tool which provides effective analytical business solutions & plays significant role in the domain of robotics and have several similarities like human behavior which may drive the real world. This paper shows the significant blend of Artificial Intelligence and robotics which transform entire industries, technological improvement of robotics application & utilization. It also focuses on different aspects of targets like marketing, home appliances, medical science, Smart agriculture and many more which includes open issues and technological challenges arises by this combination and conclude that robotics with AI can work in real world with real objects. Further AI based robotics are very important area in economics and organizational consequence, implementation of automation in any organizational design give impact on overall economy and infrastructure provide a wider direction for further research on Robotics and IoT are two terms each covering a myriad of technologies and concepts.",https://ieeexplore.ieee.org/document/9404749/,2021 International Conference on Advance Computing and Innovative Technologies in Engineering (ICACITE),4-5 March 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ECMR.2019.8870908,Autonomous Robots as Actors in Robotics Theatre - Tribute to the Centenary of R.U.R.,IEEE,Conferences,"In the eyes of the roboticists, the play R.U.R. (Rossum's Universal Robots) of Czech writer Karel Čapek is seen as the messenger of the new robot age. R.U.R. is renown for the first mentioning of the word robot for a humanoid machine that looks, moves, feels, thinks and works like a human. Inspired by the 100th anniversary of R.U.R. in 2020, we have decided to make a performance with Pepper and NAO humanoid robots acting together with human actors. Performing in a theatrical performance is very demanding even for human actors, so we see the implementation of R.U.R. with robotic co-actors as a real challenge. For this purpose, we have analyzed human-robot and robot-robot interaction in the R.U.R. script to evaluate whether NAO and Pepper robots that we have are apt to act autonomously. Due to specific robot deficiencies that we found, we have made the robot casting first and then adapted the R.U.R. script to enable Pepper and NAO robots to perform their roles.",https://ieeexplore.ieee.org/document/8870908/,2019 European Conference on Mobile Robots (ECMR),4-6 Sept. 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICDL-EpiRob48136.2020.9278071,Bayesian Optimization for Developmental Robotics with Meta-Learning by Parameters Bounds Reduction,IEEE,Conferences,"In robotics, methods and softwares usually require optimizations of hyperparameters in order to be efficient for specific tasks, for instance industrial bin-picking from homogeneous heaps of different objects. We present a developmental framework based on long-term memory and reasoning modules (Bayesian Optimisation, visual similarity and parameters bounds reduction) allowing a robot to use meta-learning mechanism increasing the efficiency of such continuous and constrained parameters optimizations. The new optimization, viewed as a learning for the robot, can take advantage of past experiences (stored in the episodic and procedural memories) to shrink the search space by using reduced parameters bounds computed from the best optimizations realized by the robot with similar tasks of the new one (e.g. bin-picking from an homogenous heap of a similar object, based on visual similarity of objects stored in the semantic memory). As example, we have confronted the system to the constrained optimizations of 9 continuous hyperparameters for a professional software (Kamido) in industrial robotic arm bin-picking tasks, a step that is needed each time to handle correctly new object. We used a simulator to create bin-picking tasks for 8 different objects (7 in simulation and one with real setup, without and with meta-learning with experiences coming from other similar objects) achieving goods results despite a very small optimization budget, with a better performance reached when meta-learning is used (84.3 % vs 78.9 % of success overall, with a small budget of 30 iterations for each optimization) for every object tested (p-value=0.036).",https://ieeexplore.ieee.org/document/9278071/,2020 Joint IEEE 10th International Conference on Development and Learning and Epigenetic Robotics (ICDL-EpiRob),26-30 Oct. 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CSCS52396.2021.00073,Bluetooth Communications in Educational Robotics,IEEE,Conferences,"In a world in a continuous and rapid change, it is absolutely necessary for our students to keep up with the rapid progress of new technologies: Internet of Things (IoT), Robotics, Artificial Intelligence (AI), Virtual Reality (VR), Augmented Reality (AR) etc. The rapid evolution and diversification of these emerging technologies has recently led to their introduction into the educational offer of the school curriculum for the gymnasium. The discipline of Information and Communication Technology (ICT) has already been implemented, a discipline that involves both the formation of skills to use new technologies and the formation of computational thinking necessary for the efficient and intelligent use of these technologies. In order to teach and learn Physics from a STEM (Science, Technology, Engineering and Mathematics) educational perspective, we initiated optional school courses of IoT, Robotics and AI (approached through Machine Learning). These courses stimulate, at the level of students, computational thinking, creativity and innovation and lead, from an interdisciplinary perspective, to the development of emerging specializations such as Mathematics-Physics-Automation, Mathematics-Physics-Electronics, Mathematics-Physics-Informatics-Robotics etc. In this paper we presented a method of approaching, in the school educational space, the study of wireless communication technologies between smart devices, through an Educational Robotics project. The project consisted of creating a wireless controlled mobile robotic platform (robot car) via a Bluetooth module connected to an Arduino Uno board.",https://ieeexplore.ieee.org/document/9481012/,2021 23rd International Conference on Control Systems and Computer Science (CSCS),26-28 May 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA.2019.8793510,Bonnet: An Open-Source Training and Deployment Framework for Semantic Segmentation in Robotics using CNNs,IEEE,Conferences,"The ability to interpret a scene is an important capability for a robot that is supposed to interact with its environment. The knowledge of what is in front of the robot is, for example, relevant for navigation, manipulation, or planning. Semantic segmentation labels each pixel of an image with a class label and thus provides a detailed semantic annotation of the surroundings to the robot. Convolutional neural networks (CNNs) are popular methods for addressing this type of problem. The available software for training and the integration of CNNs for real robots, however, is quite fragmented and often difficult to use for non-experts, despite the availability of several high-quality open-source frameworks for neural network implementation and training. In this paper, we propose a tool called Bonnet, which addresses this fragmentation problem by building a higher abstraction that is specific for the semantic segmentation task. It provides a modular approach to simplify the training of a semantic segmentation CNN independently of the used dataset and the intended task. Furthermore, we also address the deployment on a real robotic platform. Thus, we do not propose a new CNN approach in this paper. Instead, we provide a stable and easy-to-use tool to make this technology more approachable in the context of autonomous systems. In this sense, we aim at closing a gap between computer vision research and its use in robotics research. We provide an open-source codebase for training and deployment. The training interface is implemented in Python using TensorFlow and the deployment interface provides C++ library that can be easily integrated in an existing robotics codebase, a ROS node, and two standalone applications for label prediction in images and videos.",https://ieeexplore.ieee.org/document/8793510/,2019 International Conference on Robotics and Automation (ICRA),20-24 May 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CASE49439.2021.9551562,Building Skill Learning Systems for Robotics,IEEE,Conferences,"Skill-generating policies have enabled robots to perform a wide range of applications as for example assembly tasks. However, the manual engineering effort for such policies is fairly high and the environment is frequently required to be rather deterministic. For expanding robot deployment to low-volume manufacturing two challenges need to be addressed. First, the robot should acquire the skill-generating policy not from a robot programmer but rather from an expert on the task and second, the robot needs to be able to operate in unstructured environments. In this paper we present a learning approach that combines imitation learning and reinforcement learning to provide a tool for intuitive task teaching followed by self-optimization of the system. The presented approach is applied to a dual-arm assembly task using a real robot and appropriate simulation models. Whereas pure imitation learning does not result in an acceptable success rate for the considered example, after 400 episodes of reinforcement learning the robot can successfully solve the assembly task.",https://ieeexplore.ieee.org/document/9551562/,2021 IEEE 17th International Conference on Automation Science and Engineering (CASE),23-27 Aug. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ROBOT.1993.292250,Cellular robotics: simulation and HW implementation,IEEE,Conferences,"Aspects of self-organization are presented in this paper. Computer simulations as well as a real prototypical implementation are used to illustrate the proposed approach. Results of simulations are presented to compare different strategies of self-organization enabling a system of autonomous robots to form a chain between two landmarks in a completely unknown environment. This chain implicitly represents a path between any two points of the environment without an explicit representation of free space (no single robot has a global map of the environment). The experimental part, even if restricted to a few robots, demonstrates that the set of stimuli-action processes used in the simulations are indeed feasible on real systems.<>",https://ieeexplore.ieee.org/document/292250/,[1993] Proceedings IEEE International Conference on Robotics and Automation,2-6 May 1993,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS.2001.977213,Computation principles for the development of visual skills in robotics,IEEE,Conferences,"Different working principles are often considered when different visual behaviors are implemented in an agent. This occurs basically because the physical interaction between the behavior and the environment is not studied in depth. The paper shows how apparently different visual behaviors share common theoretical principles for their working mechanism. In particular properties related to the navigation vector field they compute in the environment, provide a base to explain visual learning, guidance, topological navigation, sub goal placement, obstacle avoidance and navigation enhancement. To handle the mathematics of a vector field robust tools are needed. Techniques borrowed from computer vision literature provide the necessary mathematical tools. All behaviors described have been tested in real robots. On going research is still in progress for topological navigation and subgoal placement.",https://ieeexplore.ieee.org/document/977213/,Proceedings 2001 IEEE/RSJ International Conference on Intelligent Robots and Systems. Expanding the Societal Role of Robotics in the the Next Millennium (Cat. No.01CH37180),29 Oct.-3 Nov. 2001,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ROBIO49542.2019.8961433,CyberEarth: a Virtual Simulation Platform for Robotics and Cyber-Physical Systems,IEEE,Conferences,"The increasing sophisticated robot and intelligent system applications require universal visualization platforms which can guarantee the security and efficiency of task process execution in the situation of user-programming and using different kinds of automated equipment. In this paper, we present a universal visualization framework to build up program-driven simulation software of complex robots and intelligent systems by integrating several open-source technical modules, including Ubuntu Linux operation-system, QT Creator IDE environment, ROS robot operation system, OSG(OpenSceneGraph) 3D scene, osgEarth GIS(Geographic Information System)-based 3D scene, and also Python based user-programing robotic script language. Many complex visualization simulation systems of complex tasks in wide area and dynamic scenarios are realized by using this framework. Based on this framework, we built a virtual simulation platform CyberEarth for robotics and Cyber-Physical systems. The typical robotic simulation task, which is a visual coverage task for Multi-Agent/UAV, is also introduced to demonstration the universality of this platform.",https://ieeexplore.ieee.org/document/8961433/,2019 IEEE International Conference on Robotics and Biomimetics (ROBIO),6-8 Dec. 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/LARS/SBR/WRE.2018.00066,Deep Reinforcement Learning in Robotics Logistic Task Coordination,IEEE,Conferences,"Reinforcement Learning is a conventional ap-proach in the robotics field for solving control problems asan alternative to classic control algorithms. By learning tosolve logistics tasks through simulated experience (explorationand exploitation paradigm) and making use of artificial neu-ral networks, Deep RL algorithms showed the capability ofgeneralizing solutions even for situations in which it hadnever been trained. In this paper, we present our resultsof adopting DRL as a high-level control algorithm in twodifferent logistic tasks proposed by the Logistics League at 2017Latin America Robotics Competition. We also show the roboticand computational architectures built over Festo's RobotinoPlatform to accommodate our algorithm and the implicationsof training and fine-tuning it in a computational environmentbefore deploying it to the real robot.",https://ieeexplore.ieee.org/document/8588572/,"2018 Latin American Robotic Symposium, 2018 Brazilian Symposium on Robotics (SBR) and 2018 Workshop on Robotics in Education (WRE)",6-10 Nov. 2018,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/VRW55335.2022.00036,Developing a VR Simulator for Robotics Navigation and Human Robot Interactions employing Digital Twins,IEEE,Conferences,"Providing care to seniors and adults with Developmental Disabilities (DD) has seen increased use and development of assistive technologies including service robots. Such robots ease the challenges associated with care, companionship, medication intake, and fall prevention, among others. Research and development in this field rely on in-person data collection to ensure proper robot navigation, interactions, and service. However, the current COVID-19 pandemic has caused the implementation of physical distancing and access restrictions to long-term care facilities, thus making data collection very difficult. This traditional method poses numerous challenges as videos may not be representative of the population in terms of how people move, interact with the environment, or fall. In this paper, we present the development of a VR simulator for robotics navigation and fall detection with digital twins as a solution to test the virtual robot without having access to the real physical location, or real people. The development process required the development of virtual sensors that are able to create LIDAR data for the virtual robot to navigate and detect obstacles. Preliminary testing has allowed us to obtain promising results for the virtual simulator to train a service robot to navigate and detect falls. Our results include virtual maps, robot navigation, and fall detection.",https://ieeexplore.ieee.org/document/9757529/,2022 IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops (VRW),12-16 March 2022,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/MWSCAS.2018.8624056,EMG-based hand gesture control system for robotics,IEEE,Conferences,"In this paper, a Electromyogram (EMG) based hand gesture control system is developed. A wearable human machine interface (HMI) device is designed for an in-home assistance service robot. An EMG-based control system utilizes MyoWave muscle sensor to acquire and amplify EMG signal. A microcontroller system is used to an artificial neural network (ANN) to classify the EMG signal. Based on different hand movements, commands are sent through WiFi to control the motor in a service robot. The on-board Camera system mounted the robot can capture video real-time. In addition, a web server is implemented to provide live video feedback for robot navigation and user instructions.",https://ieeexplore.ieee.org/document/8624056/,2018 IEEE 61st International Midwest Symposium on Circuits and Systems (MWSCAS),5-8 Aug. 2018,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
,Engineering Safety in Swarm Robotics,IEEE,Conferences,"Robotics, artificial intelligence, and the Internet-of-Things are driving current research and development for the technology sector. Robotic and multi-robot systems are becoming pervasive and more and more lives rely on their proper functioning in transportation, medical systems, personal robotics, and manufacturing. Assuring the security and safety of these systems is of primary importance to guarantee the real-world applicability of current research, and we argue that it should be an integral part of system design. Current software standards for safety and security for critical systems (e.g. industrial and aerospace) are not directly applicable to the large distributed systems that are envisioned for the near future. In this paper, we propose to address safety and security of swarm robotics systems at the programming language level. We propose to extend the Buzz multi-robot scripting language with constructs and code analysis that allow the verification of safety and security during development. We believe that detecting and correcting issues with what are inherently emergent systems, i.e. where collective behavior might not be immediately apparent from a single robot's code, during development would allow for a more effective advancement of swarm robotics.",https://ieeexplore.ieee.org/document/8445818/,2018 IEEE/ACM 1st International Workshop on Robotics Software Engineering (RoSE),27 May-3 June 2018,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SECON.2010.5453897,Enhancing student learning in artificial intelligence using robotics,IEEE,Conferences,"Artificial intelligence (AI) techniques may be applied to a variety of real-world problems. At Embry-Riddle Aeronautical University, CS 455: Artificial Intelligence was offered during the Spring 2008 semester in which students from all disciplines were invited to attend. Robot kits are incorporated into the course as a pedagogical tool to motivate and encourage learning by applying theoretically abstract algorithms to concrete real-world problems. This paper discusses the approach to incorporating robotics in the AI classroom. A set of commercial off-the-shelf robot kits are discussed and analyzed with respect to the students' work during the semester. Finally, recommendations for improvements on teaching AI to a multi-disciplinary audience with the help of robot kits will be discussed.",https://ieeexplore.ieee.org/document/5453897/,Proceedings of the IEEE SoutheastCon 2010 (SoutheastCon),18-21 March 2010,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/EMBC.2012.6346821,Error amplification to promote motor learning and motivation in therapy robotics,IEEE,Conferences,"To study the effects of different feedback error amplification methods on a subject's upper-limb motor learning and affect during a point-to-point reaching exercise, we developed a real-time controller for a robotic manipulandum. The reaching environment was visually distorted by implementing a thirty degrees rotation between the coordinate systems of the robot's end-effector and the visual display. Feedback error amplification was provided to subjects as they trained to learn reaching within the visually rotated environment. Error amplification was provided either visually or through both haptic and visual means, each method with two different amplification gains. Subjects' performance (i.e., trajectory error) and self-reports to a questionnaire were used to study the speed and amount of adaptation promoted by each error amplification method and subjects' emotional changes. We found that providing haptic and visual feedback promotes faster adaptation to the distortion and increases subjects' satisfaction with the task, leading to a higher level of attentiveness during the exercise. This finding can be used to design a novel exercise regimen, where alternating between error amplification methods is used to both increase a subject's motor learning and maintain a minimum level of motivational engagement in the exercise. In future experiments, we will test whether such exercise methods will lead to a faster learning time and greater motivation to pursue a therapy exercise regimen.",https://ieeexplore.ieee.org/document/6346821/,2012 Annual International Conference of the IEEE Engineering in Medicine and Biology Society,28 Aug.-1 Sept. 2012,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IJCNN.2019.8852425,Exploring Deep Models for Comprehension of Deictic Gesture-Word Combinations in Cognitive Robotics,IEEE,Conferences,"In the early stages of infant development, gestures and speech are integrated during language acquisition. Such a natural combination is therefore a desirable, yet challenging, goal for fluid human-robot interaction. To achieve this, we propose a multimodal deep learning architecture, for comprehension of complementary gesture-word combinations, implemented on an iCub humanoid robot. This enables human-assisted language learning, with interactions like pointing at a cup and labelling it with a vocal utterance. We evaluate various depths of the Mask Regional Convolutional Neural Network (for object and wrist detection) and the Residual Network (for gesture classification). Validation is carried out with two deictic gestures across ten real-world objects on frames recorded directly from the iCub's cameras. Results further strengthen the potential of gesture-word combinations for robot language acquisition.",https://ieeexplore.ieee.org/document/8852425/,2019 International Joint Conference on Neural Networks (IJCNN),14-19 July 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA.2019.8793593,"Fast Instance and Semantic Segmentation Exploiting Local Connectivity, Metric Learning, and One-Shot Detection for Robotics",IEEE,Conferences,"Semantic scene understanding is important for autonomous robots that aim to navigate dynamic environments, manipulate objects, or interact with humans in a natural way. In this paper, we address the problem of jointly performing semantic segmentation as well as instance segmentation in an online fashion, so that autonomous robots can use this information on-the-go and without sacrificing accuracy. We achieve this by exploiting a local connectivity prior of objects in the real world and a multi-task convolutional neural network architecture. The network identifies the individual object instances and their classes without region proposals or pre-segmentation of the images into individual classes. We implemented and thoroughly evaluated our approach, and our experiments suggest that our method can be used to accurately segment instance masks of objects and identify their class in an online fashion.",https://ieeexplore.ieee.org/document/8793593/,2019 International Conference on Robotics and Automation (ICRA),20-24 May 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SPCA.2006.297452,From Robotics to Pervasive Computing Environments,IEEE,Conferences,"This talk proposes that the digital home is virtually identical to the software and hardware architecture used to construct mobile robots leading to the proposition that ""pervasive computing environments can be regarded as robots that we live inside"". The author argue that it is possible and rational to apply robotic techniques to pervasive computing problems in the digital home",https://ieeexplore.ieee.org/document/4079023/,2006 First International Symposium on Pervasive Computing and Applications,3-5 Aug. 2006,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ROBIO.2011.6181717,Human-like gradual multi-agent Q-learning using the concept of behavior-based robotics for autonomous exploration,IEEE,Conferences,"In the last few years, the field of mobile robotics has made lots of advancements. These advancements are due to the extensive application of mobile robots for autonomous exploration. Mobile robots are being popularly used for applications in space, underwater explorations, underground coal mines monitoring, inspection in chemical/toxic/ nuclear factories etc. But if these environments are unknown/unpredictable, conventional/ classical robotics may not serve the purpose. In such cases robot learning is the best option. Learning from the past experiences, is one such way for real time application of robots for completely unknown environments. Reinforcement learning is one of the best learning methods for robots using a constant system-environment interaction. Both single and multi-agent concepts are available for implementation of learning. The current research work describes a multi-agent based reinforcement learning using the concept of behaviour-based robotics for autonomous exploration of mobile robots. The concept has also been tested both in indoor and outdoor environments using real-time robots.",https://ieeexplore.ieee.org/document/6181717/,2011 IEEE International Conference on Robotics and Biomimetics,7-11 Dec. 2011,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/FIE.2006.322407,Incorporating an Affective Model to an Intelligent Tutor for Mobile Robotics,IEEE,Conferences,"Emotions have been identified as important players in motivation, and motivation is very important for learning. When a tutor recognizes the affective state of the student and responds accordingly, the tutor may be able to motivate students and improve the learning process. We propose a general affective behavior model which integrates information from the student's pedagogical state, affective state, and the tutorial situation, to decide the best tutorial action, considering the tutor preferences from a pedagogical and affective point of view. Our proposal is based on emotions models, personality theories and teachers' expertise. The affective model is implemented as a dynamic decision network, with utility measures on both learning and motivation, and is being incorporated to an intelligent tutor within a virtual laboratory for learning mobile robotics. This paper presents preliminary results in the construction of the affective behavior model",https://ieeexplore.ieee.org/document/4116913/,Proceedings. Frontiers in Education. 36th Annual Conference,27-31 Oct. 2006,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICTAI.2011.60,"Interactive Surface for Bio-inspired Robotics, Re-examining Foraging Models",IEEE,Conferences,"In this paper we propose a new experimental device for defining and studying self-organized systems, especially those including physical or chemical interactions such as those encountered in collective natural phenomena. We want to be able to reproduce with real robots several paradigms such as stigmergy. In these phenomena the environment stores, diffuses, evaporates chemical substances (pheromones) that drive the behavior of each entity. The proposed device is a smart surface which relies on a graphical environment on top of which robots can move but also read/write information thanks colorimetric sensors and infrared emitters. The surface itself is able to perform some computation, implementing e.g. diffusion/evaporation mechanisms. More generally, the proposed robotic system allows to re-examine theoretical/simulated models in the perspective of defining self-organized robots. We consider in this paper the foraging problem as a case study. In particular we re-examine the expression of the model proposed by Drogoul&Ferber to implement pheromone-based exploration and transport with robots. We then analyze self-organized behaviors, as emergence of chains of robots, and their robustness.",https://ieeexplore.ieee.org/document/6103350/,2011 IEEE 23rd International Conference on Tools with Artificial Intelligence,7-9 Nov. 2011,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISIE.2007.4374932,Learning Wall Following Behaviour in Robotics through Reinforcement and Image-based States,IEEE,Conferences,"In this work, a visual and reactive wall following behaviour is learned by reinforcement. With artificial vision the environment is perceived in 3D, and it is possible to avoid obstacles that are invisible to other sensors that are more common in mobile robotics. Reinforcement learning reduces the need for intervention in behaviour design, and simplifies its adjustment to the environment, the robot and the task. In order to facilitate its generalization to other behaviours and to reduce the role of the designer, we propose a regular image-based codification of states. Even though this is much more difficult, our implementation converges and is robust. Results are presented with a Pioneer 2 AT. Learning phase has been realized on the Gazebo 3D simulator and the test phase has been proved in simulated and real environments to demonstrate the correct design and robustness of our algorithms.",https://ieeexplore.ieee.org/document/4374932/,2007 IEEE International Symposium on Industrial Electronics,4-7 June 2007,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISCAS.2019.8702186,"Live Demonstration: Neuromorphic Robotics, from Audio to Locomotion Through Spiking CPG on SpiNNaker",IEEE,Conferences,"This live demonstration presents an audio-guided neuromorphic robot: from a Neuromorphic Auditory Sensor (NAS) to locomotion using Spiking Central Pattern Generators (sCPGs). Several gaits are generated by sCPGs implemented on a SpiNNaker board. The output of these sCPGs is sent in a real-time manner to an Field Programmable Gate Array (FPGA) board using an AER-to-SpiNN interface. The control of the hexapod robot joints is performed by the FPGA board. The robot behavior can be changed in real-time by means of the NAS. The audio information is sent to the SpiNNaker board which classifies it using a Spiking Neural Network (SNN). Thus, the input sound will activate a specific gait pattern which will eventually modify the behavior of the robot.",https://ieeexplore.ieee.org/document/8702186/,2019 IEEE International Symposium on Circuits and Systems (ISCAS),26-29 May 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AIMS.2014.35,Mobile Robot Performance in Robotics Challenges: Analyzing a Simulated Indoor Scenario and Its Translation to Real-World,IEEE,Conferences,"This paper discusses the pros and cons of using 3D simulators for testing the autonomous behavior of mobile robots in indoor environments. Major contribution of the paper is the discussion about which problems that can be faced using the simulator and those that cannot. We present the integration and calibration of a real non-commercial robot in a simulator, the characterization of the errors in sensing, navigation, and manipulation, and how these errors would impact in the real performance of the robot. The experimental support of the claims made in the paper has been developed using the gazebo simulator. RoCKIn competition rulebook defined the indoor restrictions.",https://ieeexplore.ieee.org/document/7102451/,"2014 2nd International Conference on Artificial Intelligence, Modelling and Simulation",18-20 Nov. 2014,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SECON.2014.6950737,Modified reinforcement learning for sequential action behaviors and its application to robotics,IEEE,Conferences,"When developing a robot or other automaton, the efficacy of the agent is highly dependent on the performance of the behaviors which underpin the control system. Especially in the case of agents which must act in real world or disorganized environments, the design of robust behaviors can be both difficult and time consuming, and often requires the use of sensitive tuning. In response to this need, we present a behavioral, goal-oriented, reinforcement-based machine learning strategy which is flexible, simple to implement, and designed for application in real-world environments, but with the capability of software-based training. In this paper, we will explain our design paradigms, the formal implementation thereof, and the algorithm proper. We will show that the algorithm is able to emulate standard reinforcement learning within comparable training time, and to extend the capabilities thereof as well. We also demonstrate extension of learning beyond the scope of training examples, and present an example of a physical robot which learns a sequential action behavior by experimentation.",https://ieeexplore.ieee.org/document/6950737/,IEEE SOUTHEASTCON 2014,13-16 March 2014,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA.2018.8460709,Near-optimal Irrevocable Sample Selection for Periodic Data Streams with Applications to Marine Robotics,IEEE,Conferences,"We consider the task of monitoring spatiotemporal phenomena in real-time by deploying limited sampling resources at locations of interest irrevocably and without knowledge of future observations. This task can be modeled as an instance of the classical secretary problem. Although this problem has been studied extensively in theoretical domains, existing algorithms require that data arrive in random order to provide performance guarantees. These algorithms will perform arbitrarily poorly on data streams such as those encountered in robotics and environmental monitoring domains, which tend to have spatiotemporal structure. We focus on the problem of selecting representative samples from phenomena with periodic structure and introduce a novel sample selection algorithm that recovers a near-optimal sample set according to any monotone submodular utility function. We evaluate our algorithm on a seven-year environmental dataset collected at the Martha's Vineyard Coastal Observatory and show that it selects phytoplankton sample locations that are nearly optimal in an information-theoretic sense for predicting phytoplankton concentrations in locations that were not directly sampled. The proposed periodic secretary algorithm can be used with theoretical performance guarantees in many real-time sensing and robotics applications for streaming, irrevocable sample selection from periodic data streams.",https://ieeexplore.ieee.org/document/8460709/,2018 IEEE International Conference on Robotics and Automation (ICRA),21-25 May 2018,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ITNG.2011.116,Nerve: A Lightweight Middleware for Quality-of-service Networked Robotics,IEEE,Conferences,"Social robots must adapt to dynamic environments, human interaction partners and challenging new stringent tasks. Their inner software should be designed and deployed carefully because slight changes in the robot's requirements can have an important impact in the existing code. This paper focus on the design and implementation of a lightweight middleware for networked robotics called \textit{Nerve}, which guarantees the scalability and quality-of-service requirements for this kind of real-time software. Its benefits have been proved through its use in a Robot Learning by Imitation control architecture, but its design guidelines are general enough to be also applied with common distributed and real-time embedded applications.",https://ieeexplore.ieee.org/document/5945314/,2011 Eighth International Conference on Information Technology: New Generations,11-13 April 2011,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CIRA.2005.1554245,Plenary talk June 29; The 3rdGeneration of Robotics: Ubiquitous Robot,IEEE,Conferences,"This talk shows its possibility of implementation in real life through demonstrations using a Sobot, Rity: i) continuous interface between physical and virtual worlds ii) seamless transmission of Sobot between a PC and a Mobot, and iii) omnipresence of Sobot. Rity, developed at the Robot Intelligence Technology (RIT) Laboratory, KAIST, is a Sobot implemented as a 12 DOF artificial creature in the virtual 3D world created in a PC. It has virtual sensors to survive in the virtual world and physical sensors attached to the PC to interact with the real world. Based on sensor information it can express its emotion, and interact with human beings through a web camera in the real world. It can generate behaviors autonomously and has its own IP. This means that it can be accessed through a network at anywhere and anytime using any device. With this technique omnipresence of Sobot can be realized in a ubiquitous space. The eventual goal of this research is to integrate Sobot, Embot, and Mobot to build up a Ubibot so that ubiquitous services through it can be available in a ubiquitous era",https://ieeexplore.ieee.org/document/1554245/,2005 International Symposium on Computational Intelligence in Robotics and Automation,27-30 June 2005,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AIRC56195.2022.9836454,Real-Time Implementation of an Enhanced PID CONTROLLER based on Marine Predator Algorithm (MPA) for Micro-robotics System,IEEE,Conferences,"This paper presents a new approach to control the position of the micro-robotics system with a proportional-integral-derivative (PID) controller. By using Marine Predator algorithm (MPA), the optimal PID controller indicators were obtained by applying a new objective function namely, integral square time multiplied square error (ISTES). The efficiency of the proposed MPA-based controller was verified by comparisons made with Grey Wolf Optimization (GWO) algorithm-based controllers in terms of time. Each control technique will be applied to the identified model using MATLAB Simulink and the experimental test facility was conducted using LABVIEW software. The simulation and experimental results show that the performance of MPA-PID controller based on ISTES cost function achieves the best performance among various techniques. Moreover, the MPA technique had the highest performance compared to GWO technique based on rising time, settling time and settling error. Thus, it is recommended to apply MPA for tuning the parameters of PID as it can enhance its performance in micro-robotic systems. It was found that the amount of error is reduced by 33.75&#x0025; using MPA than other former experiments",https://ieeexplore.ieee.org/document/9836454/,"2022 3rd International Conference on Artificial Intelligence, Robotics and Control (AIRC)",10-12 May 2022,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ACIIW.2019.8925192,Real-time pain detection in facial expressions for health robotics,IEEE,Conferences,"Automatic pain detection is an important challenge in health computing. In this paper we report on our efforts to develop a real-time, real-world pain detection system from human facial expressions. Although many studies addressed this challenge, most of them use the same dataset for training and testing. There is no cross-check with other datasets or implementation in real-time to check performance on new data. This is problematic, as evidenced in this paper, because the classifiers overtrain on dataset-specific features. This limits realtime, real-world usage. In this paper, we investigate different methods of real-time pain detection. The training data uses a combination of pain and emotion datasets, unlike other papers. The best model shows an accuracy of 88.4% on a dataset including pain and 7 non-pain emotional expressions. Results suggest that convolutional neural networks (CNN) are not the best methods in some cases as they easily overtrain if the dataset is biased. Finally we implemented our pain detection method on a humanoid robot for physiotherapy. Our work highlights the importance of cross-corpus evaluation & real-time testing, as well as the need for a well balanced and ecologically valid pain dataset.",https://ieeexplore.ieee.org/document/8925192/,2019 8th International Conference on Affective Computing and Intelligent Interaction Workshops and Demos (ACIIW),3-6 Sept. 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CEC.1999.781948,Realization of robust controllers in evolutionary robotics: a dynamically-rearranging neural network approach,IEEE,Conferences,"The evolutionary robotics approach has been attracting a lot of attention in the field of robotics and artificial life. In this approach, neural networks are widely used to construct controllers for autonomous mobile agents, since they intrinsically have generalization, noise-tolerant abilities and so on. However, there are still open questions: (1) the gap between simulated and real environments, (2) the evolutionary and learning phase are completely separated, and (3) the conflict between stability and evolvability/adaptability. In this paper, we try to overcome these problems by incorporating the concept of dynamic rearrangement function of biological neural networks with the use of neuromodulators. Simulation results show that the proposed approach is highly promising.",https://ieeexplore.ieee.org/document/781948/,Proceedings of the 1999 Congress on Evolutionary Computation-CEC99 (Cat. No. 99TH8406),6-9 July 1999,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CIG.2009.5286456,Realtime execution of automated plans using evolutionary robotics,IEEE,Conferences,"Applying neural networks to generate robust agent controllers is now a seasoned practice, with time needed only to isolate particulars of domain and execution. However we are often constrained to local problems due to an agents inability to reason in an abstract manner. While there are suitable approaches for abstract reasoning and search, there is often the issues that arise in using offline processes in real-time situations. In this paper we explore the feasibility of creating a decentralised architecture that combines these approaches. The approach in this paper explores utilising a classical automated planner that interfaces with a library of neural network actuators through the use of a Prolog rule base. We explore the validity of solving a variety of goals with and without additional hostile entities as well as added uncertainty in the the world. The end results providing a goal driven agent that adapts to situations and reacts accordingly.",https://ieeexplore.ieee.org/document/5286456/,2009 IEEE Symposium on Computational Intelligence and Games,7-10 Sept. 2009,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICIS.2017.7959982,Robotics data real-time management based on NoSQL solution,IEEE,Conferences,"In nowadays, robotics database management systems are increasing. These systems ensure good storage of data and with big data analytic, a new approach demands new structures and methods for collecting, recording, and analyzing enterprise data. This paper work deals with the NoSQL databases which are the secret of the continual progression data that new data management solutions have been emerged. They crossed several areas as personalization, profile management, big data in real-time, content management, catalogue, view of customers, mobile applications, internet of things, digital communication and fraud detection. Machine learning, for example, thrives on more data, so smart machines can learn more and faster, the Robotics are our use of case to focus on our Test. The implementation of NoSQL for Robotics wrestle all the data they acquire into usable form because with the ordinary type of Robotics we are facing very big limits to manage and find the exact information in real-time. Our original proposed approach was demonstrated by experimental studies and running example used as a use case.",https://ieeexplore.ieee.org/document/7959982/,2017 IEEE/ACIS 16th International Conference on Computer and Information Science (ICIS),24-26 May 2017,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/DSN.2019.00027,SOTER: A Runtime Assurance Framework for Programming Safe Robotics Systems,IEEE,Conferences,"The recent drive towards achieving greater autonomy and intelligence in robotics has led to high levels of complexity. Autonomous robots increasingly depend on third-party off-the-shelf components and complex machine-learning techniques. This trend makes it challenging to provide strong design-time certification of correct operation. To address these challenges, we present SOTER, a robotics programming framework with two key components: (1) a programming language for implementing and testing high-level reactive robotics software, and (2) an integrated runtime assurance (RTA) system that helps enable the use of uncertified components, while still providing safety guarantees. SOTER provides language primitives to declaratively construct a RTA module consisting of an advanced, high-performance controller (uncertified), a safe, lower-performance controller (certified), and the desired safety specification. The framework provides a formal guarantee that a well-formed RTA module always satisfies the safety specification, without completely sacrificing performance by using higher performance uncertified components whenever safe. SOTER allows the complex robotics software stack to be constructed as a composition of RTA modules, where each uncertified component is protected using a RTA module. To demonstrate the efficacy of our framework, we consider a real-world case-study of building a safe drone surveillance system. Our experiments both in simulation and on actual drones show that the SOTER-enabled RTA ensures the safety of the system, including when untrusted third-party components have bugs or deviate from the desired behavior.",https://ieeexplore.ieee.org/document/8809550/,2019 49th Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN),24-27 June 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/URAI.2016.7734049,Secure robotics,IEEE,Conferences,"Security is an under-studied problem within robotics and Internet of Things. Part of the reason for this is that currently most robots and IoT devices remain in the lab at all times. Recent trends show more robots and IoT devices moving “out into the wild” with no humans to protect them. This creates vulnerabilities beyond the well known and well studied network/internet based threat. These threats include external network, local network, software, physical access, tricking the artificial intelligence, and intellectual property theft. This document discribes the above and shows our current work towards detection and mitigation.",https://ieeexplore.ieee.org/document/7734049/,2016 13th International Conference on Ubiquitous Robots and Ambient Intelligence (URAI),19-22 Aug. 2016,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCVW.2017.84,SkiMap++: Real-Time Mapping and Object Recognition for Robotics,IEEE,Conferences,"We introduce SkiMap++, an extension to the recently proposed SkiMap mapping framework for robot navigation [1]. The extension deals with enriching the map with semantic information concerning the presence in the environment of certain objects that may be usefully recognized by the robot, e.g. for the sake of grasping them. More precisely, the map can accommodate information about the spatial locations of certain 3D object features, as determined by matching the visual features extracted from the incoming frames through a random forest learned off-line from a set of object models. Thereby, evidence about the presence of object features is gathered from multiple vantage points alongside with the standard geometric mapping task, so to enable recognizing the objects and estimating their 6 DOF poses. As a result, SkiMap++ can reconstruct the geometry of large scale environments as well as localize some relevant objects therein (Fig.1) in real-time on CPU. As an additional contribution, we present an RGB-D dataset featuring ground-truth camera and object poses, which may be deployed by researchers interested in pursuing SLAM alongside with object recognition, a topic often referred to as Semantic SLAM1.",https://ieeexplore.ieee.org/document/8265293/,2017 IEEE International Conference on Computer Vision Workshops (ICCVW),22-29 Oct. 2017,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISRIMT53730.2021.9596792,Smart Seeing Eye Dog Wheeled Assistive Robotics,IEEE,Conferences,"To date, approximately 12 million people over the age of 40 in the United States are visually impaired, 3 million of whom are blind. Meanwhile, many people are blinded by accidents. Guide dogs can assist blind people in their daily life. However, the cost to breed, raise, train, and match a Guiding Eyes dog with a person with vision loss is estimated to be approximately $50,000. Another weakness is that the life expectancy of a human is 73.4 years around the world and the average working life of a guide dog is around eight years. As a result, people with visual impairments need more guide dogs throughout their lives. If our design is put into use, we can reduce the cost significantly. In this paper, we are going to build a smart seeing eye dog, which is a kind of wheeled Assistive Robotics that could guide the blind people walk on the road like but perform better than a real seeing eye dog. We started this paper from four aspects. They are the structure of the mechanical dog, TensorFlow-based image recognition, ROS-based motion simulation and human-computer interaction. Because of the cost of training an assistive dog is extremely high and the dogs are all red-green color-blind, which means that they cannot recognize the traffic lights and road sign. In our paper, these problems can be solved very well. Soon, between this mechanical dog paper may help blind people to better integrate into society.",https://ieeexplore.ieee.org/document/9596792/,2021 3rd International Symposium on Robotics & Intelligent Manufacturing Technology (ISRIMT),24-26 Sept. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/EMSOFT.2018.8537236,Special Session: Embedded Software for Robotics: Challenges and Future Directions,IEEE,Conferences,"This paper surveys recent challenges and solutions in the design, implementation, and verification of embedded software for robotics. Emphasis is placed on mobile robots, like self-driving cars. In design, it addresses programming support for robotic systems, secure state estimation, and ROS-based monitor generation. In the implementation phase, it describes the synthesis of control software using finite precision arithmetic, real-time platforms and architectures for safety-critical robotics, efficient implementation of neural network based-controllers, and standards for computer vision applications. The issues in verification include verification of neural network-based robotic controllers, and falsification of closed-loop control systems. The paper also describes notable open-source robotic platforms. Along the way, we highlight important research problems for developing the next generation of high-performance, low-resource-usage, correct embedded software.",https://ieeexplore.ieee.org/document/8537236/,2018 International Conference on Embedded Software (EMSOFT),30 Sept.-5 Oct. 2018,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ROBOT.2004.1308800,The artificial ecosystem: a distributed approach to service robotics,IEEE,Conferences,"We propose a multiagent, distributed approach to autonomous mobile robotics which is an alternative to most existing systems in literature: robots are thought of as mobile units within an intelligent environment where they coexist and co-operate with fixed, intelligent devices that are assigned different roles: helping the robot to localize itself, controlling automated doors and elevators, detecting emergency situations, etc. To achieve this, intelligent sensors and actuators (i.e. physical agents) are distributed both onboard the robot and throughout the environment, and they are handled by Real-Time software agents which exchange information on a distributed message board. The paper outlines the benefits of the approach in terms of efficiency and Real-Time responsiveness.",https://ieeexplore.ieee.org/document/1308800/,"IEEE International Conference on Robotics and Automation, 2004. Proceedings. ICRA '04. 2004",26 April-1 May 2004,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CSCI46756.2018.00293,"Timing and its Implementation in a Language, Communication, and Systems Integration SDK and Platform for Intelligent Entities and Robotics",IEEE,Conferences,"A framework to integrate different artificial intelligence and machine learning algorithms is combined with an execution framework to create a powerful cloud computing system development platform. By providing an execution framework and control software that is native to cloud architectures and supports interactivity and time synchronization, the true utility of cloud computing and Big Data systems can be increased. Many Big Data software systems are not interactive, automated, or able to run in real-time. An integration example is provided.",https://ieeexplore.ieee.org/document/8947742/,2018 International Conference on Computational Science and Computational Intelligence (CSCI),12-14 Dec. 2018,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SSRR53300.2021.9597686,Towards Ethics Training in Disaster Robotics: Design and Usability Testing of a Text-Based Simulation,IEEE,Conferences,"Rescue robots are expected to soon become commonplace at disaster sites, where they are increasingly being deployed to provide rescuers with improved access and intervention capabilities while mitigating risks. The presence of robots in operation areas, however, is likely to carry a layer of additional ethical complexity to situations that are already ethically challenging. In addition, limited guidance is available for ethically informed, practical decision-making in real-life disaster settings, and specific ethics training programs are lacking. The contribution of this paper is thus to propose a tool aimed at supporting ethics training for rescuers operating with rescue robots. To this end, we have designed an interactive text-based simulation. The simulation was developed in Python, using Tkinter, Python's de-facto standard GUI. It is designed in accordance with the Case-Based Learning approach, a widely used instructional method that has been found to work well for ethics training. The simulation revolves around a case grounded in ethical themes we identified in previous work on ethical issues in rescue robotics: fairness and discrimination, false or excessive expectations, labor replacement, safety, and trust. Here we present the design of the simulation and the results of usability testing.",https://ieeexplore.ieee.org/document/9597686/,"2021 IEEE International Symposium on Safety, Security, and Rescue Robotics (SSRR)",25-27 Oct. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ROMAN.2017.8172430,"Towards the use of consumer-grade electromyographic armbands for interactive, artistic robotics performances",IEEE,Conferences,"In recent years, gesture-based interfaces have been explored in order to control robots in non-traditional ways. These require the use of systems that are able to track human body movements in 3D space. Deploying Mo-cap or camera systems to perform this tracking tend to be costly, intrusive, or require a clear line of sight, making them ill-adapted for artistic performances. In this paper, we explore the use of consumer-grade armbands (Myo armband) which capture orientation information (via an inertial measurement unit) and muscle activity (via electromyography) to ultimately guide a robotic device during live performances. To compensate for the drop in information quality, our approach rely heavily on machine learning and leverage the multimodality of the sensors. In order to speed-up classification, dimensionality reduction was performed automatically via a method based on Random Forests (RF). Online classification results achieved 88% accuracy over nine movements created by a dancer during a live performance, demonstrating the viability of our approach. The nine movements are then grouped into three semantically-meaningful moods by the dancer for the purpose of an artistic performance achieving 94% accuracy in real-time. We believe that our technique opens the door to aesthetically-pleasing sequences of body motions as gestural interface, instead of traditional static arm poses.",https://ieeexplore.ieee.org/document/8172430/,2017 26th IEEE International Symposium on Robot and Human Interactive Communication (RO-MAN),28 Aug.-1 Sept. 2017,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS.2018.8593799,Utility Model Re-description within a Motivational System for Cognitive Robotics,IEEE,Conferences,"This paper describes a re-descriptive approach to the efficient acquisition of ever higher level and more precise utility models within the motivational system (MotivEn) of a cognitive architecture. The approach is based on a two-step process whereby, as a first step, simple imprecise sensor correlation related utility models are obtained from the interaction traces of the robot. These utility models allow the robot to increase the frequency of achieving goals, and thus, provide lots of traces that can be used to try to train precise value functions implemented as artificial neural networks. The approach is tested experimentally on a real robotic setup that involves the coordination of two robots.",https://ieeexplore.ieee.org/document/8593799/,2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),1-5 Oct. 2018,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ACCESS.2020.3007064,Drive Through Robotics: Robotic Automation for Last Mile Distribution of Food and Essentials During Pandemics,IEEE,Journals,"The COVID-19 pandemic unraveled the weak points in the global supply chain for goods. Specifically, people all over the world, including those in the most advanced nations have had to go without medical supplies and personal protective equipment. Scarcity of essentials increases anxiety and uncertainty exacerbating unproductive behaviors like hoarding and price gouging. Left to market forces, such unfair practices are likely to aggravate hardships and increase the loss of lives. Thus, there is a critical need to ensure safe distribution of food and essential supplies to all citizens to sustain them through challenging times. To this end, we propose a simple, affordable and contact-less robotic system for preparing and dispensing food and survival-kits at community scale. The system has provisions to prevent hoarding and price gouging. Design, simulation, and, validation of the system has been completed to ensure readiness for real world implementation. This project is part of an open-source program and detailed designs are available upon request to entities interested in using it to serve their communities.",https://ieeexplore.ieee.org/document/9133423/,IEEE Access,2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ACCESS.2019.2938366,Grasping Objects From the Floor in Assistive Robotics: Real World Implications and Lessons Learned,IEEE,Journals,"This paper presents a system enabling a mobile robot to autonomously pick-up objects a human is pointing at from the floor. The system does not require object models and is designed to grasp unknown objects. The robot decides by itself if an object is suitable for grasping by considering measures of size, position and the environment suitability. The implementation is built on the second prototype of the home care robot Hobbit, thereby verifying that complex robotic manipulation tasks can be performed with economical hardware. The presented system was already tested in real apartments with elderly people. We highlight this by discussing the additional complexity for complete autonomous behavior in apartments compared with tests in labs.",https://ieeexplore.ieee.org/document/8819885/,IEEE Access,2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ACCESS.2018.2873597,Hierarchical Semantic Mapping Using Convolutional Neural Networks for Intelligent Service Robotics,IEEE,Journals,"The introduction of service robots in the public domain has introduced a paradigm shift in how robots are interacting with people, where robots must learn to autonomously interact with the untrained public instead of being directed by trained personnel. As an example, a hospital service robot is told to deliver medicine to Patient Two in Ward Three. Without awareness of what “Patient Two” or “Ward Three” is, a service robot must systematically explore the environment to perform this task, which requires a long time. The implementation of a Semantic Map allows for robots to perceive the environment similar to people by associating semantic information with spatial information found in geometric maps. Currently, many semantic mapping works provide insufficient or incorrect semantic-metric information to allow a service robot to function dynamically in human-centric environments. This paper proposes a semantic map with a hierarchical semantic organization structure based on a hybrid metric-topological map leveraging convolutional neural networks and spatial room segmentation methods. Our results are validated using multiple simulated and real environments on our lab's custom developed mobile service robot and demonstrate an application of semantic maps by providing only vocal commands. We show that this proposed method provides better capabilities in terms of semantic map labeling and retain multiple levels of semantic information.",https://ieeexplore.ieee.org/document/8490234/,IEEE Access,2018,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/TLT.2018.2833111,Inquiry-Based Learning With RoboGen: An Open-Source Software and Hardware Platform for Robotics and Artificial Intelligence,IEEE,Journals,"It has often been found that students appreciate hands-on work, and find that they learn more with courses that include a project than those relying solely on conventional lectures and tests. This type of project driven learning is a key component of “Inquiry-based learning” (IBL), which aims at teaching methodology as well as content by incorporating the student as an actor rather than a spectator. Robotics applications are especially well-suited for IBL due to the value of trial and error experience, the multiple possibilities for students to implement their own ideas, and the importance of programming, problem-solving, and electro-mechanical skills in real world engineering and science jobs. Furthermore, robotics platforms can be useful teaching media and learning tools for a variety of topics. Here, we present RoboGen: an open-source, web-based, software, and hardware platform for Robotics and Artificial Intelligence with a particular focus on Evolutionary Robotics. We describe the platform in detail, compare it to existing alternatives, and present results of its use as a platform for Inquiry-based learning within a master's level course at the Ecole Polytechnique Fédérale de Lausanne.",https://ieeexplore.ieee.org/document/8354804/,IEEE Transactions on Learning Technologies,1 July-Sept. 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/TIE.2016.2597119,Multidimensional Modeling of Physiological Tremor for Active Compensation in Handheld Surgical Robotics,IEEE,Journals,"Precision, robustness, dexterity, and intelligence are the design indices for current generation surgical robotics. To augment the required precision and dexterity into normal microsurgical work-flow, handheld robotic instruments are developed to compensate physiological tremor in real time. The hardware (sensors and actuators) and software (causal linear filters) employed for tremor identification and filtering introduces time-varying unknown phase delay that adversely affects the device performance. The current techniques that focus on three-dimensions (3-D) tip position control involves modeling and canceling the tremor in three axes (x-, y-, and z -axes) separately. Our analysis with the tremor recorded from surgeons and novice subjects shows that there exists significant correlation in tremor across the dimensions. Based on this, a new multidimensional modeling approach based on extreme learning machines is proposed in this paper to correct the phase delay and to accurately model 3-D tremor simultaneously. Proposed method is evaluated through both simulations and experiments. Comparison with the state-of-the art techniques highlight the suitability and better performance of the proposed approach for tremor compensation in handheld surgical robotics.",https://ieeexplore.ieee.org/document/7529172/,IEEE Transactions on Industrial Electronics,Feb. 2017,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/TSMCB.2008.920227,Multihierarchical Interactive Task Planning: Application to Mobile Robotics,IEEE,Journals,"To date, no solution has been proposed to human-machine interactive task planning that deals simultaneously with two important issues: 1) the capability of processing large amounts of information in planning (as it is needed in any real application) and 2) being efficient in human-machine communication (a proper set of symbols for human-machine interaction may not be suitable for efficient automatic planning and vice versa). In this paper, we formalize a symbolic model of the environment to solve these issues in a natural form through a human-inspired mechanism that structures knowledge in multiple hierarchies. Planning with a hierarchical model may be efficient even in cases where the lack of hierarchical information would make it intractable. However, in addition, our multihierarchical model is able to use the symbols that are most familiar to each human user for interaction, thus achieving efficiency in human-machine communication without compromising the task-planning performance. We formalize here a general interactive task-planning process which is then particularized to be applied to a mobile robotic application. The suitability of our approach has been demonstrated with examples and experiments.",https://ieeexplore.ieee.org/document/4505426/,"IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics)",June 2008,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/JPROC.2018.2856739,Navigating the Landscape for Real-Time Localization and Mapping for Robotics and Virtual and Augmented Reality,IEEE,Journals,"Visual understanding of 3-D environments in real time, at low power, is a huge computational challenge. Often referred to as simultaneous localization and mapping (SLAM), it is central to applications spanning domestic and industrial robotics, autonomous vehicles, and virtual and augmented reality. This paper describes the results of a major research effort to assemble the algorithms, architectures, tools, and systems software needed to enable delivery of SLAM, by supporting applications specialists in selecting and configuring the appropriate algorithm and the appropriate hardware, and compilation pathway, to meet their performance, accuracy, and energy consumption goals. The major contributions we present are: 1) tools and methodology for systematic quantitative evaluation of SLAM algorithms; 2) automated, machine-learning-guided exploration of the algorithmic and implementation design space with respect to multiple objectives; 3) end-to-end simulation tools to enable optimization of heterogeneous, accelerated architectures for the specific algorithmic requirements of the various SLAM algorithmic approaches; and 4) tools for delivering, where appropriate, accelerated, adaptive SLAM solutions in a managed, JIT-compiled, adaptive runtime context.",https://ieeexplore.ieee.org/document/8436423/,Proceedings of the IEEE,Nov. 2018,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/JSSC.2020.3028298,NeuroSLAM: A 65-nm 7.25-to-8.79-TOPS/W Mixed-Signal Oscillator-Based SLAM Accelerator for Edge Robotics,IEEE,Journals,"Simultaneous localization and mapping (SLAM) is a quintessential problem in autonomous navigation, augmented reality, and virtual reality. In particular, low-power SLAM has gained increasing importance for its applications in power-limited edge devices such as unmanned aerial vehicles (UAVs) and small-sized cars that constitute devices with edge intelligence. This article presents a 7.25-to-8.79-TOPS/W mixed-signal oscillator-based SLAM accelerator for applications in edge robotics. This study proposes a neuromorphic SLAM IC, called NeuroSLAM, employing oscillator-based pose-cells and a digital head direction cell to mimic place cells and head direction cells that have been discovered in a rodent brain. The oscillatory network emulates a spiking neural network and its continuous attractor property achieves spatial cognition with a sparse energy distribution, similar to the brains of rodents. Furthermore, a lightweight vision system with a max-pooling is implemented to support low-power visual odometry and re-localization. The test chip fabricated in a 65-nm CMOS exhibits a peak energy efficiency of 8.79 TOPS/W with a power consumption of 23.82 mW.",https://ieeexplore.ieee.org/document/9222208/,IEEE Journal of Solid-State Circuits,Jan. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/13.485240,Robotics laboratory exercises,IEEE,Journals,"The authors report new laboratory exercises in robotic manipulation, computer vision, artificial intelligence, and mechatronics, four areas that are central to any robotics curriculum. The laboratory exercises supply the student with hands-on experience that complements classroom lectures and software development. Through this experience, the student confronts the hard realities of robot systems and learns to deal with them. Such hands-on experience is essential for a sound robotics education, because many critical lessons about the real world can only be learned through personal experience.",https://ieeexplore.ieee.org/document/485240/,IEEE Transactions on Education,Feb. 1996,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/TETC.2017.2769705,Robust Robot Tracking for Next-Generation Collaborative Robotics-Based Gaming Environments,IEEE,Journals,"The collaboration between humans and robots is one of the most disruptive and challenging research areas. Even considering advances in design and artificial intelligence, humans and robots could soon ally to perform together a number of different tasks. Robots could also became new playmates. In fact, an emerging trend is associated with the so-called phygital gaming, which builds upon the idea of merging the physical world with a virtual one in order to let physical and virtual entities, such as players, robots, animated characters and other game objects interact seamlessly as if they were all part of the same reality. This paper specifically focuses on mixed reality gaming environments that can be created by using floor projection, and tackles the issue of enabling accurate and robust tracking of off-the-shelf robots endowed with limited sensing capabilities. The proposed solution is implemented by fusing visual tracking data gathered via a fixed camera in a smart environment with odometry data obtained from robot's on-board sensors. The solution has been tested within a phygital gaming platform in a real usage scenario, by experimenting with a robotic game that exhibits many challenging situations which would be hard to manage using conventional tracking techniques.",https://ieeexplore.ieee.org/document/8094867/,IEEE Transactions on Emerging Topics in Computing,1 July-Sept. 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/TASE.2019.2938316,Semiautomatic Labeling for Deep Learning in Robotics,IEEE,Journals,"In this article, we propose an augmented reality semiautomatic labeling (ARS), a semiautomatic method which leverages on moving a 2-D camera by means of a robot, proving precise camera tracking, and an augmented reality pen (ARP) to define initial object bounding box, to create large labeled data sets with minimal human intervention. By removing the burden of generating annotated data from humans, we make the deep learning technique applied to computer vision, which typically requires very large data sets, truly automated and reliable. With the ARS pipeline, we created two novel data sets effortlessly, one on electromechanical components (industrial scenario) and other on fruits (daily-living scenario) and trained two state-of-the-art object detectors robustly, based on convolutional neural networks, such as you only look once (YOLO) and single shot detector (SSD). With respect to conventional manual annotation of 1000 frames that takes us slightly more than 10 h, the proposed approach based on ARS allows to annotate 9 sequences of about 35 000 frames in less than 1 h, with a gain factor of about 450. Moreover, both the precision and recall of object detection is increased by about 15% with respect to manual labeling. All our software is available as a robot operating system (ROS) package in a public repository alongside with the novel annotated data sets. Note to Practitioners-This article was motivated by the lack of a simple and effective solution for the generation of data sets usable to train a data-driven model, such as a modern deep neural network, so as to make them accessible in an industrial environment. Specifically, a deep learning robot guidance vision system would require such a large amount of manually labeled images that it would be too expensive and impractical for a real use case, where system reconfigurability is a fundamental requirement. With our system, on the other hand, especially in the field of industrial robotics, the cost of image labeling can be reduced, for the first time, to nearly zero, thus paving the way for self-reconfiguring systems with very high performance (as demonstrated by our experimental results). One of the limitations of this approach is the need to use a manual method for the detection of objects of interest in the preliminary stages of the pipeline (ARP or graphical interface). A feasible extension, related to the field of collaborative robotics, could be used to exploit the robot itself, manually moved by the user, even for this preliminary stage, so as to eliminate any source of inaccuracy.",https://ieeexplore.ieee.org/document/8844069/,IEEE Transactions on Automation Science and Engineering,April 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/TE.2012.2224867,SyRoTek—Distance Teaching of Mobile Robotics,IEEE,Journals,"E-learning is a modern and effective approach for training in various areas and at different levels of education. This paper gives an overview of SyRoTek, an e-learning platform for mobile robotics, artificial intelligence, control engineering, and related domains. SyRoTek provides remote access to a set of fully autonomous mobile robots placed in a restricted area with dynamically reconfigurable obstacles, which enables solving a huge variety of problems. A user is able to control the robots in real time by their own developed algorithms as well as being able to analyze gathered data and observe activity of the robots by provided interfaces. The system is currently used for education at the Czech Technical University in Prague, Prague, Czech Republic, and at the University of Buenos Aires, Buenos, Aires, Argentina, and it is freely accessible to other institutions. In addition to the system overview, this paper presents the experience gained from the actual deployment of the system in teaching activities.",https://ieeexplore.ieee.org/document/6341862/,IEEE Transactions on Education,Feb. 2013,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS.1992.601935,"""Arnie P."" - A Robot Golfing System Using Binocular And A Heuristic Feedback Mechanism",IEEE,Conferences,"This paper describes a robot vision golfing system. The Automated Robotic Navigational unit with Intelligent Eye and Putter (ARNIE P)τproject was initiated to investigate the problems and develop software solutions for robotic tasks that require good hand-eye coordination and an intelligent sensor feedback mechanism. This system has only one frame buffer and no specialized hardware, so quasi-real time 3D tracking is accomplished in software using the Unix Spline facility. The single frame buffer and digitizer, stores and retains the location of the ball from two separate cameras during the time interval between the golf ball initially crossing a trigger scan line and the ball coming to a complete stop. The most novel aspect of this study is that by attempting to build or model a difficult perceptory task such as golf, which requires integrating many complicated computational pieces (binocular stereo vision, robot arm motion, heuristic feedback, learning), it appears to be a good plarform to experiment with artificial intelligence techniques and robotics.",https://ieeexplore.ieee.org/document/601935/,Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems,7-10 July 1992,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCCYB.2004.1437628,2004 International Conference on Computational Cybernetics Proceedings,IEEE,Conferences,The following topics are dealt with: cybernetics; control; hardware architectures; vision systems; genetic algorithms; evolutionary computing; sensors; real-time systems; multiagents systems; robotics; software engineering; intelligent systems; learning; fractional-order systems; and engineering systems,https://ieeexplore.ieee.org/document/1437628/,"Second IEEE International Conference on Computational Cybernetics, 2004. ICCC 2004.",30 Aug.-1 Sept. 2004,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AIMS52415.2021.9466061,3D Control System of Arm Robot Prototype for Skin Cancer Detection,IEEE,Conferences,"Arm robot has a lack of control systems that depend on desired control for assistive medical. Our laboratory robotics & artificial intelligent at Padjadjaran University created skin cancer detection of arm robot with dark flow framework to identify skin cancer in real-time. The implementation of the arm robot was for increasing the accuracy, precision, and stability. The main purpose of this paper was to control an arm robot for skin cancer detection that is capable to scan the whole body skin to localize the skin cancers by driving the manipulator in circular or elliptical skimming. To initiate the communication with the arm robot which used Dynamixel as the actuators, we applied USB2Dynamixel as the communicator. SMPS2Dynamixel was used to supply the power into servo motors. 3D Control system software has designed, and it had some features such as; forward kinematic movement, inverse kinematic movement, and 3D simulation to help user visualize the position of the arm robot. Control software was built in MATLAB GUI environment and 3D simulation adapted Peter Corke Robotics Toolbox.",https://ieeexplore.ieee.org/document/9466061/,2021 International Conference on Artificial Intelligence and Mechatronics Systems (AIMS),28-30 April 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICIT.2006.372290,3D Modelling from Multi-view Registered Range Images Using K-means Clustering,IEEE,Conferences,"3D modelling from range images captured using laser scanning systems finds a wide range of applications in computer vision and industrial robotics. However due to the presence of scanning noise, accumulative registration errors, and improper data fusion, the reconstructed surfaces from multiple registered range images captured from different viewpoints are often distorted with thick patches, false connections and blurred features. Moreover, the existing integration methods are often expensive in the sense of computational time and data storage. These shortcomings will hinder the wide applications of 3D modelling using the latest laser scanning systems. In this paper, the k-means clustering approach from the pattern recognition and machine learning literatures is employed to optimally fuse the overlapping areas between two range images captured from two neighbouring viewpoints and to iteratively minimize the integration error. The final fused point set is then triangulated using an improved Delaunay method, guaranteeing a watertight surface. The new method is theoretically guaranteed to converge. A comparative study based on real images shows that the proposed algorithm is computationally efficient and significantly reduces the integration error, while desirably retaining geometric details of object surface.",https://ieeexplore.ieee.org/document/4237612/,2006 IEEE International Conference on Industrial Technology,15-17 Dec. 2006,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/3DV50981.2020.00038,3D-Aware Ellipse Prediction for Object-Based Camera Pose Estimation,IEEE,Conferences,"In this paper, we propose a method for coarse camera pose computation which is robust to viewing conditions and does not require a detailed model of the scene. This method meets the growing need of easy deployment of robotics or augmented reality applications in any environments, especially those for which no accurate 3D model nor huge amount of ground truth data are available. It exploits the ability of deep learning techniques to reliably detect objects regardless of viewing conditions. Previous works have also shown that abstracting the geometry of a scene of objects by an ellipsoid cloud allows to compute the camera pose accurately enough for various application needs. Though promising, these approaches use the ellipses fitted to the detection bounding boxes as an approximation of the imaged objects. In this paper, we go one step further and propose a learning-based method which detects improved elliptic approximations of objects which are coherent with the 3D ellipsoid in terms of perspective projection. Experiments prove that the accuracy of the computed pose significantly increases thanks to our method and is more robust to the variability of the boundaries of the detection boxes. This is achieved with very little effort in terms of training data acquisition - a few hundred calibrated images of which only three need manual object annotation.",https://ieeexplore.ieee.org/document/9320405/,2020 International Conference on 3D Vision (3DV),25-28 Nov. 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ECTI-CON54298.2022.9795616,6D Valves Pose Estimation based on YOLACT and DenseFusion for the Offshore Robot Application,IEEE,Conferences,"Offshore oil and gas operations are most concerned with safety, it often requires workers to be stationed on the platform to control ongoing process. The cost of transportation can be expensive, and the working environment is dangerous, so it is natural for certain operations to be automated away by autonomous robots. One such operation is maneuvering valves, which is a common and important task most suitable for robotics. To achieve this goal, the robot itself need the ability to perceive the location and orientation of the target valve. In this paper, we proposed a computer vision methods based on the RGB-D image inputs for guiding the real-world spatial information of valves to the robot. In detail, we first exploited the YOLACT as an instance segmentation method to precisely extract the valves from the background then pass the segmented valve pixel to the 6D pose estimation algorithm. The 2D pixels and 3D points generated are then utilized by the DenseFusion algorithm to predict the valve&#x2019;s 6D pose composed of position and orientation based on fusion of RGB and depth features. In order to evaluate the validity of the proposed method, these algorithms were then implemented in ROS2 and tested on edge device which embedded in the real offshore robot system. The results show that our proposed method is promising and could be effectively utilized by the offshore autonomous robot to operate the valves.",https://ieeexplore.ieee.org/document/9795616/,"2022 19th International Conference on Electrical Engineering/Electronics, Computer, Telecommunications and Information Technology (ECTI-CON)",24-27 May 2022,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/A-SSCC53895.2021.9634782,A 40nm Embedded SG-MONOS Flash Macro for High-end MCU Achieving 200MHz Random Read Operation and 7.91Mb/mm2 Density with Charge Assisted Offset Cancellation Sense Amplifier,IEEE,Conferences,"The fusion of AI and IoT technologies in conjunction with 5G mobile communication is strongly driving the realization of smarter societies and factories. In particular, endpoint devices for home automation, machine vision, robotics, etc. need higher performance and higher intelligence. That is why some of these devices are moving to so-called “crossover area”, which is located at the boundary between high-end MCU area and low-end MPU area. In this area, it is essential to achieve both high performance and low cost as shown in Fig. 1. Users familiar with conventional MCUs prefer to continue the approach with MCUs from the viewpoints of reusability of existing software assets, real-time operations and BOM costs (no need of external Flash memories). To meet their expectations for performance and cost, embedded Flash memory (eFlash), one of the key components in an MCU, should achieve high-speed random read operation corresponding to CPU operations at higher frequency (600MHz-1GHz) while reducing its macro size which occupies a significant part of MCU die size.",https://ieeexplore.ieee.org/document/9634782/,2021 IEEE Asian Solid-State Circuits Conference (A-SSCC),7-10 Nov. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AIVR.2018.00028,A Benchmark of Four Methods for Generating 360° Saliency Maps from Eye Tracking Data,IEEE,Conferences,"Modeling and visualization of user attention in Virtual Reality is important for many applications, such as gaze prediction, robotics, retargeting, video compression, and rendering. Several methods have been proposed to model eye tracking data as saliency maps. We benchmark the performance of four such methods for 360° images. We provide a comprehensive analysis and implementations of these methods to assist researchers and practitioners. Finally, we make recommendations based on our benchmark analyses and the ease of implementation.",https://ieeexplore.ieee.org/document/8613647/,2018 IEEE International Conference on Artificial Intelligence and Virtual Reality (AIVR),10-12 Dec. 2018,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ACCC51160.2020.9347897,A Comparative Analysis of Kinematics of Industrial Robot KUKA KR 60–3 Using Scientific Computing Languages,IEEE,Conferences,"In the field of robotics, there are kinematic analysis methods that are responsible for describing the positions and orientations of the end effectors, as well as the angles, velocities and trajectories of industrial robots; such techniques are: forward kinematics, inverse kinematics and velocity kinematics. For the solutions of these complex mathematical calculations, the use of scientific computing languages or programs is required; which more and more algorithms, libraries and complements are implemented, that achieve a reduction in programming hours and result in the creation of better solutions in areas of all kinds. For this reason, the kinematics of the Industrial Robot KUKA KR 60-3 was programmed in the languages and programs most used in scientific computing, with the aim of comparing the performance (real time) when carrying out symbolic and numerical analysis in said studies.",https://ieeexplore.ieee.org/document/9347897/,2020 Asia Conference on Computers and Communications (ACCC),18-20 Sept. 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICOEI53556.2022.9777153,A Comparative Study of Machine Learning Based Image Captioning Models,IEEE,Conferences,"Automated image captioning is a crucial concept for numerous real-world applications as it is useful in robotics, image indexing, self-driving vehicles and greatly helpful for impaired eyesight people. An image provided in real-time can be converted into text using image captioning models developed by machine learning algorithms. Understanding an image mostly depends on the features of the image. Machine learning techniques are widely used for image captioning tasks. This research study has performed a comparative analysis on three Machine Learning (ML) algorithms, i.e. k-Nearest Neighbor (KNN), Convolution Neural Network (CNN) with Long Short Term Memory (LSTM) and Attention Based LSTM. In addition, an improved KNN algorithm with reduced time complexity and an improved CNN with LSTM and Attention Based LSTM model with an added beam search method is proposed to improve the underlying approaches further. The performance of the three selected models are empirically evaluated using BLEU, ROUGE and METEOR scores on the widely used flickr8k dataset, and the experimental results demonstrate the supremacy of the Attention Based LSTM over the other two approaches. Finally, the current study&#x0027;s findings help guide the researchers and practitioners in selecting the appropriate approach for Image Captioning with empirical evidence in terms of standard evaluation metrics.",https://ieeexplore.ieee.org/document/9777153/,2022 6th International Conference on Trends in Electronics and Informatics (ICOEI),28-30 April 2022,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/KSE.2018.8573394,A Comparative Study on Detection and Estimation of a 3-D Object Model in a Complex Scene,IEEE,Conferences,"In this paper, we tackle the approaches to detect and estimate 3-D object model in a complex scene. Although it is fundamental research in the computer vision and robotics community, this task still has many challenges especially when the scene is complex with contaminated or occluded data. To do this, we compare three common approaches including two conventional ways (e.g., geometrical and appearance-based techniques) and the proposed scheme. While geometrical approaches tend to directly detect and estimate objects without any learning procedure, the appearance-based required a training process to model the interested object. We show that a combination of recent advantages of deep learning (e.g., RCNN, Yolo) could resolve the detection task, while the geometrical based approaches estimate full 3-D model. The evaluation utilizes two different dataset. One from a public available, second one is our self-prepared dataset. Difference scenarios are considered in the evaluation. The results confirmed that the proposed technique achieves the best performances. As a consequence, it suggests to deploy real application supporting visually impaired people in detecting and grasping common objects in their activities of daily living.",https://ieeexplore.ieee.org/document/8573394/,2018 10th International Conference on Knowledge and Systems Engineering (KSE),1-3 Nov. 2018,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CODESISSS51650.2020.9244038,A Fast Design Space Exploration Framework for the Deep Learning Accelerators: Work-in-Progress,IEEE,Conferences,"The Capsule Networks (CapsNets) is an advanced form of Convolutional Neural Network (CNN), capable of learning spatial relations and being invariant to transformations. CapsNets requires complex matrix operations which current accelerators are not optimized for, concerning both training and inference passes. Current state-of-the-art simulators and design space exploration (DSE) tools for DNN hardware neglect the modeling of training operations, while requiring long exploration times that slow down the complete design flow. These impediments restrict the real-world applications of CapsNets (e.g., autonomous driving and robotics) as well as the further development of DNNs in life-long learning scenarios that require training on low-power embedded devices. Towards this, we present XploreDL, a novel framework to perform fast yet high-fidelity DSE for both inference and training accelerators, supporting both CNNs and CapsNets operations. XploreDL enables a resource-efficient DSE for accelerators, focusing on power, area, and latency, highlighting Pareto-optimal solutions which can be a green-lit to expedite the design flow. XploreDL can reach the same fidelity as ARM's SCALE-sim, while providing 600x speedup and having a 50x lower memory-footprint. Preliminary results with a deep CapsNet model on MNIST for training accelerators show promising Pareto-optimal architectures with up to 0.4 TOPS/squared-mm and 800 fJ/op efficiency. With inference accelerators for AlexNet the Pareto-optimal solutions reach up to 1.8 TOPS/squared-mm and 200 fJ/op efficiency.",https://ieeexplore.ieee.org/document/9244038/,2020 International Conference on Hardware/Software Codesign and System Synthesis (CODES+ISSS),20-25 Sept. 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA.2019.8793600,A Fog Robotic System for Dynamic Visual Servoing,IEEE,Conferences,"Cloud Robotics is a paradigm where multiple robots are connected to cloud services via Internet to access “unlimited” computation power, at the cost of network communication. However, due to limitations such as network latency and variability, it is difficult to control dynamic, human compliant service robots directly from the cloud. In this work, we combine cloud robotics with an agile edge device to build a Fog Robotic system by leveraging an asynchronous protocol with a “heartbeat” signal. We use the system to enable robust teleoperation of a dynamic self-balancing robot from the cloud. We use the system to pick up boxes from static locations, a task commonly performed in warehouse logistics. To make cloud teleoperation more intuitive and efficient, we program a cloud-based image based visual servoing (IBVS) module to automatically assist the cloud teleoperator during the object pickups. Visual feedbacks, including apriltag recognition and tracking, are performed in the cloud to emulate a Fog Robotic object recognition system for IBVS. We demonstrate the feasibility of a dynamic real-time automation system using this cloud-edge hybrid design, which opens up possibilities of deploying dynamic robotic control with deep-learning recognition systems in Fog Robotics. Finally, we show that Fog Robotics enables the self-balancing service robot to pick up a box automatically from a person under unstructured environments.",https://ieeexplore.ieee.org/document/8793600/,2019 International Conference on Robotics and Automation (ICRA),20-24 May 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCAD51958.2021.9643557,A General Hardware and Software Co-Design Framework for Energy-Efficient Edge AI,IEEE,Conferences,"A huge number of edge applications including self-driving cars, mobile health, robotics, and augmented reality / virtual reality are enabled by deep neural networks (DNNs). Currently, much of this computation for these applications happens in the cloud, but there are several good reasons to perform the processing on local edge platforms such as smartphones: improved accessibility to different parts of the world, low latency, and data privacy. In this paper, we present a general hardware and software co-design framework for energy-efficient edge AI for both simple classification and structured output prediction tasks (e.g., 3D shapes from images). This framework relies on two key ideas. First, we design a space of DNNs of increasing complexity (coarse to fine) and perform input-specific adaptive inference by selecting a DNN of appropriate complexity depending on the hardness of input examples. Second, we execute the selected DNN on the target edge platform using a resource management policy to save energy. We also provide instantiations of our co-design framework for three qualitatively different problem settings: convolutional neural networks for image classification, graph convolutional networks for predicting 3D shapes from images, and generative adversarial networks on photo-realistic unconditional image generation. Our experiments on real-world benchmarks and mobile platforms show the effectiveness of our co-design framework in achieving significant gain in energy with little to no loss in accuracy of predictions.",https://ieeexplore.ieee.org/document/9643557/,2021 IEEE/ACM International Conference On Computer Aided Design (ICCAD),1-4 Nov. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IJCNN52387.2021.9534180,A Lightweight sequence-based Unsupervised Loop Closure Detection,IEEE,Conferences,"Stable, effective and lightweight loop closure detection is an always pursued goal in real-time SLAM systems, that can be ported on embedded processors and deployed on autonomous robotics. Deep learning methods have extended the expressive ability and adaptability of the descriptor, and sequence-based methods can greatly improve the matching accuracy. However, the increased computation complexity and storage bandwidth requirements of matching calculations for high-dimensional descriptor make it infeasible for real-time deployment, especially for robots that navigate in relatively big maps. To address this challenge, we propose a lightweight sequence-based unsupervised loop closure detection scheme. To be specific, Principal Component Analysis (PCA) is applied to squeeze the descriptor dimensions while maintaining sufficient expressive ability. Additionally, with the consideration of the image sequence and combining linear query with fast approximate nearest neighbor search to further reduce the execution time and improve the efficiency of sequence matching. We implement our method on CALC, a state-of-the-art unsupervised solution, and conduct experiments on NVIDIA TX2, results demonstrate that the accuracy has been improved by 5%, while the execution speed is 2× faster. Source code is available at https://github.com/Mingrui-Yu/Seq-CALC.",https://ieeexplore.ieee.org/document/9534180/,2021 International Joint Conference on Neural Networks (IJCNN),18-22 July 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/UEMCON47517.2019.8993080,A Low-Cost Arm Robotic Platform based on Myoelectric Control for Rehabilitation Engineering,IEEE,Conferences,"Rehabilitation robotics is a recent kind of service robot that include devices such as robotic prosthesis and exoskeletons. These devices could help motor disabled people to rehabilitate their motor functions, and could provide functional compensation to accomplish motor activities. In order to control robotic prosthesis and exoskeletons it is required to identify human movement intention, to be converted into commands for the device. Motor impaired people may use surface electromyography (sEMG) signals to control these devices, taking into account that sEMG signals directly reflects the human motion intention. Myoelectric control is an advanced technique related with the detection, processing, classification, and application of sEMG signals to control human-assisting robots or rehabilitation devices. Despite recent advances with myoelectric control algorithms, currently there is still an important need to develop suitable methods involving usability, for controlling prosthesis and exoskeletons in a natural way. Traditionally, acquiring EMG signals and developing myoelectric control algorithms require expensive hardware. With the advent of low-cost technologies (i.e. sensors, actuators, controllers) and hardware support of simulation software packages as Matlab, affordable research tools could be used to develop novel myoelectric control algorithms. This work describes the implementation and validation of a Matlab-based robotic arm using low-cost technologies such as Arduino commanded using myoelectric control. The platform permits implementation of a variety of EMG-based algorithms. It was carried out a set of experiments aimed to evaluate the platform, through an application of pattern recognition based myoelectric control to identify and execute seven movements of the robotic upper limb: 1-forearm pronation; 2- forearm supination; 3-wrist flexion; 4-wrist extension; 5- elbow flexion; 6- elbow extension; 7-resting. The algorithm use a feature extraction stage based on a combination of time and frequency domain features (mean absolute value, waveform length, root mean square) and a widely used k-NN classifier. Obtained mean classification errors were 5.9%. As future work, additional features in the myoelectric control algorithm will be evaluated, for real-time applications.",https://ieeexplore.ieee.org/document/8993080/,"2019 IEEE 10th Annual Ubiquitous Computing, Electronics & Mobile Communication Conference (UEMCON)",10-12 Oct. 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SMC.2019.8914519,A Multimodal Perception System for Detection of Human Operators in Robotic Work Cells,IEEE,Conferences,"Workspace monitoring is a critical hw/sw component of modern industrial work cells or in service robotics scenarios, where human operators share their workspace with robots. Reliability of human detection is a major requirement not only for safety purposes but also to avoid unnecessary robot stops or slowdowns in case of false positives. The present paper introduces a novel multimodal perception system for human tracking in shared workspaces based on the fusion of depth and thermal images. A machine learning approach is pursued to achieve reliable detection performance in multi-robot collaborative systems. Robust experimental results are finally demonstrated on a real robotic work cell.",https://ieeexplore.ieee.org/document/8914519/,"2019 IEEE International Conference on Systems, Man and Cybernetics (SMC)",6-9 Oct. 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/RTSS52674.2021.00016,A ROS 2 Response-Time Analysis Exploiting Starvation Freedom and Execution-Time Variance,IEEE,Conferences,"Robots are commonly subject to real-time constraints. To ensure that such constraints are met, recent work has analyzed the response times of processing chains under ROS 2, a popular robotics framework. However, prior work supports only scalar worst-case execution time bounds and does not exploit that the ROS 2 scheduling mechanism is starvation-free. This paper proposes a novel response-time analysis for ROS 2 processing chains that accounts for both the high execution-time variance typically encountered in robotics workloads and the starvation freedom of the default ROS 2 callback scheduler. Experimental results from both synthetic callback graphs and a real ROS 2 workload empirically show the proposed analysis to be much more accurate (often by a factor of 2x or more).",https://ieeexplore.ieee.org/document/9622336/,2021 IEEE Real-Time Systems Symposium (RTSS),7-10 Dec 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS51168.2021.9635878,A Registration-aided Domain Adaptation Network for 3D Point Cloud Based Place Recognition,IEEE,Conferences,"In the field of large-scale SLAM for autonomous driving and mobile robotics, 3D point cloud based place recognition has aroused significant research interest due to its robustness to changing environments with drastic daytime and weather variance. However, it is time-consuming and effort-costly to obtain high-quality point cloud data for place recognition model training and ground truth for registration in the real world. To this end, a novel registration-aided 3D domain adaptation network for point cloud based place recognition is proposed. A structure-aware registration network is introduced to help to learn features with geometric information and a 6-DoFs pose between two point clouds with partial overlap can be estimated. The model is trained through a synthetic virtual LiDAR dataset through GTA-V with diverse weather and daytime conditions and domain adaptation is implemented to the real-world domain by aligning the global features. Our results outperform state-of-the-art 3D place recognition baselines or achieve comparable on the real-world Oxford RobotCar dataset with the visualization of registration on the virtual dataset.",https://ieeexplore.ieee.org/document/9635878/,2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),27 Sept.-1 Oct. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICARM52023.2021.9536056,A Review of Bilateral Teleoperation Control Strategies with Soft Environment,IEEE,Conferences,"In the past two decades, bilateral teleoperation with haptic feedback has attracted great research and application interests in both robotics and other areas. Initially triggered by the need to handle dangerous and remote distance tasks such as nuclear materials manipulation and space exploration, bilateral teleoperation has found its way into other applications as a result of development of control theory, robotic technology (both hardware and software) and latest breakthrough in artificial intelligence and machine learning. Consequently, bilateral teleoperation is found facing new challenges brought by these new applications. One major and obvious change is the working environment for the slave manipulator: different from rigid or solid contact environments which are reasonably assumed in early applications in industrial, nuclear and aerospace applications, the slave environment is now more complex and often the objects in contact are much softer in term of stiffness and can not be described by simple elastic model if good teleoperation performance (accurate and transparent) is expected. In this paper, the research of bilateral teleoperation system considering soft environment in recent 20 years has been surveyed for the first time in literature, to the knowledge of the authors. Following the difference in real applications, in this review the definition of soft environment covers linear elastic environment with much lower stiffness than conventional industrial environment and nonlinear complex soft environment with/out time-varying characteristics. Accordingly, the surveyed control strategies and structures in recent literature to improve the stability and accuracy of bilateral teleoperation with soft environment are classified and explained. Finally, the main applications, current challenges and future perspectives of bilateral teleoperation with soft environment are discussed.",https://ieeexplore.ieee.org/document/9536056/,2021 6th IEEE International Conference on Advanced Robotics and Mechatronics (ICARM),3-5 July 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICAITPR51569.2022.9844197,A Review on Self Stabilizing Platform in Scope of Merchant Navy Applications,IEEE,Conferences,"The advancement in robotics in the resent years has led to the integration of multiple disciplines of sciences to research and develop effective and reliable solutions for industries. This model is mainly focused on the Merchant navy industry where we have proposed the implementation if Self Stabilizing platform for various navy applications such as GLB/GLBE cranes, Helipads, etc. This model will be a 6 DoF/6 Axis model which is suitable for the movements for the scope of the applications proposed above. We have tried to fuse two different control methods for this model which are PID controller method and MEMS sensors methods which is a MUP6050 sensor. This model is better than the old mechanical link self-stabilizing platforms which are non-feedback control models. As the ship experiences storms the tilt in the bottom surface is detected and the upper platform tiles accordingly to maintain level and give stability to the GLB crane or the Helipad. This is a dynamic model which has real time active feedback.",https://ieeexplore.ieee.org/document/9844197/,2022 First International Conference on Artificial Intelligence Trends and Pattern Recognition (ICAITPR),10-12 March 2022,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA48506.2021.9561941,A Robot Walks into a Bar: Automatic Robot Joke Success Assessment,IEEE,Conferences,"Effective social robots should leverage humor’s unique ability to improve relationship connections and dispel stress, but current robots possess limited (if any) humorous abilities. In this paper, we aim to supplement one aspect of autonomous robots by giving robotic systems the ability to ""read the room"" to assess how their humorous statements are received by nearby people in real time. Using a dataset of the audio of crowd responses to a robotic comedian over multiple performances (first presented in past work), we establish human-labeled joke success ground truths and compare individual human rater accuracy against the outputs of lightweight Machine Learning (ML) approaches that are easy to deploy in real-time joke assessment. Our results indicate that all three ML approaches (naïve Bayes, support vector machines, and single-hidden-layer feedforward neural networks) performed significantly better than the baseline approach used in our past work. In particular, support vector machines and neural network approaches are comparable to a human rater in the task of assessing if a joke failed or not in certain cases. The products of this work will inform self-assessment techniques for robots and help social robotics researchers test their own assessment methods on realistic data from human crowds.",https://ieeexplore.ieee.org/document/9561941/,2021 IEEE International Conference on Robotics and Automation (ICRA),30 May-5 June 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA48506.2021.9561722,A Scavenger Hunt for Service Robots,IEEE,Conferences,"Creating robots that can perform general-purpose service tasks in a human-populated environment has been a longstanding grand challenge for AI and Robotics research. One particularly valuable skill that is relevant to a wide variety of tasks is the ability to locate and retrieve objects upon request. This paper models this skill as a Scavenger Hunt (SH) game, which we formulate as a variation of the NP-hard stochastic traveling purchaser problem. In this problem, the goal is to find a set of objects as quickly as possible, given probability distributions of where they may be found. We investigate the performance of several solution algorithms for the SH problem, both in simulation and on a real mobile robot. We use Reinforcement Learning (RL) to train an agent to plan a minimal cost path, and show that the RL agent can outperform a range of heuristic algorithms, achieving near optimal performance. In order to stimulate research on this problem, we introduce a publicly available software stack and associated website that enable users to upload scavenger hunts which robots can download, perform, and learn from to continually improve their performance on future hunts.",https://ieeexplore.ieee.org/document/9561722/,2021 IEEE International Conference on Robotics and Automation (ICRA),30 May-5 June 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS.2018.8593714,A Software Framework for Planning Under Partial Observability,IEEE,Conferences,"Planning under partial observability is both challenging and critical for reliable robot operation. The past decade has seen substantial advances in this domain: The mathematically principled approach for addressing such problems, namely the Partially Observable Markov Decision Process (POMDP), has started to become practical for various robotics tasks. Good approximate solutions for problems framed as POMDPs can now be computed on-line, with a few classes of problems being solved in near real-time. However, applications of these more recent advances are often hindered by the lack of easy-to-use software tools. Implementation of state of the art algorithms exist, but most (if not all)require the POMDP model to be hard-coded inside the program, increasing the difficulty of applying them. To alleviate this problem, we propose a software toolkit, called On-line POMDP Planning Toolkit (OPPT)(downloadable from http://robotics.itee.uq.edu.au/~oppt). By providing a well-defined and general abstract solver API, OPPT enables the user to quickly implement new POMDP solvers. Furthermore, OPPT provides an easy-to-use plug-in architecture with interfaces to the high-fidelity simulator Gazebo that, in conjunction with user-friendly configuration files, allows users to specify POMDP models of a standard class of robot motion planning under partial observability problems with no additional coding effort.",https://ieeexplore.ieee.org/document/8593714/,2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),1-5 Oct. 2018,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS.1998.727477,A constraint-based controller for soccer-playing robots,IEEE,Conferences,"Soccer meets the requirements of the situated agent approach and as a task domain is sufficiently rich to support research integrating many branches of robotics and AI. A robot is an integrated system, with a controller embedded in its plant. A robotic system is the coupling of a robot to its environment. Robotic systems are, in general, hybrid dynamic systems, consisting of continuous, discrete and event-driven components. Constraint nets provide a semantic model for modeling hybrid dynamic systems. Controllers are embedded constraint solvers that solve constraints in real-time. A controller for our new softbot soccer team, UBC Dynamo98, has been modeled in constraint nets, and implemented in Java, using the Java Beans architecture. The paper demonstrates that the formal constraint net approach is a practical tool for designing and implementing controllers for robots in multi-agent real-time environments.",https://ieeexplore.ieee.org/document/727477/,"Proceedings. 1998 IEEE/RSJ International Conference on Intelligent Robots and Systems. Innovations in Theory, Practice and Applications (Cat. No.98CH36190)",17-17 Oct. 1998,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS.2016.7759384,A convolutional neural network for robotic arm guidance using sEMG based frequency-features,IEEE,Conferences,"Recently, robotics has been seen as a key solution to improve the quality of life of amputees. In order to create smarter robotic prosthetic devices to be used in an everyday context, one must be able to interface them seamlessly with the end-user in an inexpensive, yet reliable way. In this paper, we are looking at guiding a robotic device by detecting gestures through measurement of the electrical activity of muscles captured by surface electromyography (sEMG). Reliable sEMG-based gesture classifiers for end-users are challenging to design, as they must be extremely robust to signal drift, muscle fatigue and small electrode displacement without the need for constant recalibration. In spite of extensive research, sophisticated sEMG classifiers for prostheses guidance are not yet widely used, as systems often fail to solve these issues simultaneously. We propose to address these problems by employing Convolutional Neural Networks. Specifically as a first step, we demonstrate their viability to the problem of gesture recognition for a low-cost, low-sampling rate (200Hz) consumer-grade, 8-channel, dry electrodes sEMG device called Myo armband (Thalmic Labs) on able-bodied subjects. To this effect, we assessed the robustness of this machine learning oriented approach by classifying a combination of 7 hand/wrist gestures with an accuracy of ∼97.9% in real-time, over a period of 6 consecutive days with no recalibration. In addition, we used the classifier (in conjunction with orientation data) to guide a 6DoF robotic arm, using the armband with the same speed and precision as with a joystick. We also show that the classifier is able to generalize to different users by testing it on 18 participants.",https://ieeexplore.ieee.org/document/7759384/,2016 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),9-14 Oct. 2016,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/MWSCAS.2016.7870095,A framework for teaching robotic control using a novel visual programming language,IEEE,Conferences,"This paper proposes an education robotic platform that aims to improve teaching methods of programming and robotics skills, for both beginner and advanced users. We propose an innovative platform that consists of a versatile set of sensors and actuators, controlled by utilizing a user-friendly visual programming language through a mobile phone interface, or by utilizing a representational state transfer application programming interface for more advanced users. Suggested methods form the foundation of problem-based learning, by emphasizing hands-on experimental assignments and activities, and collaborative learning. We present the software and hardware architecture of the system, and case studies for utilizing different control modes. Consequently, students can improve their understanding of basic robotic concepts by observing real-time response and feedback of the actuator and sensor modules integrated in the robotic platform.",https://ieeexplore.ieee.org/document/7870095/,2016 IEEE 59th International Midwest Symposium on Circuits and Systems (MWSCAS),16-19 Oct. 2016,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICARSC.2018.8374189,A generic visual perception domain randomisation framework for Gazebo,IEEE,Conferences,"The impressive results of applying deep neural networks in tasks such as object recognition, language translation, and solving digital games are largely attributed to the availability of massive amounts of high quality labelled data. However, despite numerous promising steps in incorporating these techniques in robotic tasks, the cost of gathering data with a real robot has halted the proliferation of deep learning in robotics. In this work, a plugin for the Gazebo simulator is presented, which allows rapid generation of synthetic data. By introducing variations in simulator-specific or irrelevant aspects of a task, one can train a model which exhibits some degree of robustness against those aspects, and ultimately narrow the reality gap between simulated and real-world data. To show a use-case of the developed software, we build a new dataset for detection and localisation of three object classes: box, cylinder and sphere. Our results in the object detection and localisation task demonstrate that with small datasets generated only in simulation, one can achieve comparable performance to that achieved when training on real-world images.",https://ieeexplore.ieee.org/document/8374189/,2018 IEEE International Conference on Autonomous Robot Systems and Competitions (ICARSC),25-27 April 2018,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS.2014.6942859,A machine learning approach for real-time reachability analysis,IEEE,Conferences,"Assessing reachability for a dynamical system, that is deciding whether a certain state is reachable from a given initial state within a given cost threshold, is a central concept in controls, robotics, and optimization. Direct approaches to assess reachability involve the solution to a two-point boundary value problem (2PBVP) between a pair of states. Alternative, indirect approaches involve the characterization of reachable sets as level sets of the value function of an appropriate optimal control problem. Both methods solve the problem accurately, but are computationally intensive and do no appear amenable to real-time implementation for all but the simplest cases. In this work, we leverage machine learning techniques to devise query-based algorithms for the approximate, yet real-time solution of the reachability problem. Specifically, we show that with a training set of pre-solved 2PBVP problems, one can accurately classify the cost-reachable sets of a differentially-constrained system using either (1) locally-weighted linear regression or (2) support vector machines. This novel, query-based approach is demonstrated on two systems: the Dubins car and a deep-space spacecraft. Classification errors on the order of 10% (and often significantly less) are achieved with average execution times on the order of milliseconds, representing 4 orders-of-magnitude improvement over exact methods. The proposed algorithms could find application in a variety of time-critical robotic applications, where the driving factor is computation time rather than optimality.",https://ieeexplore.ieee.org/document/6942859/,2014 IEEE/RSJ International Conference on Intelligent Robots and Systems,14-18 Sept. 2014,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CISTI.2015.7170600,A mixed reality game using 3Pi robots — “PiTanks”,IEEE,Conferences,"In the growing field of Robotics, one of the many possible paths to explore is the social aspect that it can influence upon the present society. The combination of the goal-oriented development of robots with the interactivity used in games while employing mixed reality is a promising route to take in regard to designing user-friendly robots and improving problem solving featured in artificial intelligence software. In this paper, we present a competitive team-based game using Pololu's 3Pi robots moving in a projected map, capable of human interaction via game controllers. The game engine was developed utilizing the framework Qt Creator with C++ and OpenCV for the image processing tasks. The technical framework uses the ROS framework for communications that may be, in the future, used to connect different modules. Various parameters of the implementation are tested, such as position tracking errors.",https://ieeexplore.ieee.org/document/7170600/,2015 10th Iberian Conference on Information Systems and Technologies (CISTI),17-20 June 2015,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SACI.2013.6608963,A new NIR camera for gesture control of electronic devices,IEEE,Conferences,"Since the introduction Gesture Control technology in the electronic gaming technology a series of attempts have been made to deploy it also on other domains such as robotics, teaching, medical, automotive and many others. Human gesture used for Man-Machine Interaction became attractive as it offers a simpler way of controlling sophisticated devices, in a sci-fi-like scenario, in return of an increasingly computational power required by the artificial intelligence algorithms needed to detect, track and recognize them. There have been attempts to bring a solution to it by using 2D or 3D based image processing methods. There is a clear balance incline towards 3D methods in the consumer product as besides the almost insurmountable difficulties for producing robust and stable results, the price constraint added supplementary hurdles. As perfect illumination conditions are core factors in obtaining the above results, the infrared light was unanimously adopted by the domain technologies. In this paper, a novel real-time depth-mapping principle and a corresponding hardware solution for an IR depth-mapping camera is introduced. The new IR camera architecture comprises an illuminator module which is pulsed and modulated via a monotonic function using a phase-locked loop control for the laser intensity, while the reflected infrared light is captured during the increasing and decreasing monotonic function. A reconfigurable hardware architecture (RHA) unit calculates the depth and controls the IR waves in synchronism with the infrared sensor. The resolution of the depth map is variable depending on the resolution and gating possibilities of the image sensor. A sensor of 1 megapixel is used, providing a resolution of 1024×1024. Images of real objects are reconstructed in 3D based on the data obtained by the laser controlled by the RHA. A corresponding image processing algorithm builds the 3D map of the object in real-time. In this paper the camera is used to control consumer electronic products such as TV sets, laptops and others.",https://ieeexplore.ieee.org/document/6608963/,2013 IEEE 8th International Symposium on Applied Computational Intelligence and Informatics (SACI),23-25 May 2013,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISCAS.2000.856157,A new board for CNN stereo vision algorithm,IEEE,Conferences,Artificial vision for environment recognition is a very useful tool in autonomous robotics. Specifically the use of stereo vision algorithms implemented via a hardware neural architecture allows real time scene reconstruction. In this paper the follow-on of previous work on an analogue hardware Cellular Neural Network implementation of the algorithm is presented. In this paper a new CNN based PCI electronic board is presented.,https://ieeexplore.ieee.org/document/856157/,2000 IEEE International Symposium on Circuits and Systems (ISCAS),28-31 May 2000,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/FTC.2016.7821768,A non-biological AI approach towards natural language understanding,IEEE,Conferences,"The problem being addressed in this paper is that using brute force in Natural Language Processing and Machine Learning combined with advanced statistics will only approximate meaning and thus will not deliver in terms of real text understanding. Counting words and tracking word order or parsing by syntax will also result in probability and guesswork at best. Their vendors struggle in delivering accurate quality and this results in ill-functioning applications. The newer generation methodologies like Deep Learning and Cognitive Computing are breaking barriers in the (Big Data) fields of Internet of Things, Robotics and Image/Video Recognition but cannot be successfully deployed for text without huge amounts of training and sample data. In the short term, we believe non-biological Artificial Intelligence will produce the best results for text understanding. Miia applied advanced Linguistic and Semantic Technologies combined with ConceptNet modeling and Machine Learning to successfully cater deep intelligent and cross-language quality to several industries.",https://ieeexplore.ieee.org/document/7821768/,2016 Future Technologies Conference (FTC),6-7 Dec. 2016,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SICE.2002.1195611,A reinforcement learning using adaptive state space construction strategy for real autonomous mobile robots,IEEE,Conferences,"In the recent robotics, much attention has been focused on utilizing reinforcement learning for designing robot controllers. However, there still exists difficulties, one of them is well known as state space explosion problem. As the state space for a learning system becomes continuous and high dimensional, its combinational state space exponentially explodes and the learning process is time consuming. In this paper, we propose an adaptive state space recruitment strategy for reinforcement learning, which enables the system to divide state space gradually according to task complexity and progress of learning. Some simulation results and real robot implementation show the validity of the method.",https://ieeexplore.ieee.org/document/1195611/,Proceedings of the 41st SICE Annual Conference. SICE 2002.,5-7 Aug. 2002,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IRDS.2002.1041504,A reinforcement learning with adaptive state space recruitment strategy for real autonomous mobile robots,IEEE,Conferences,"In the recent robotics, much attention has been focused on utilizing reinforcement learning for designing robot controllers. However, there still exists difficulties, one of them is well known as state space explosion problem. As the state space for learning system becomes continuous and high dimensional, the learning process results in time-consuming since its combinational states explodes exponentially. In order to adopt reinforcement learning for such complicated systems, it should be taken not only ""adaptability"" but ""computational efficiencies"" into account. In the paper, we propose an adaptive state space recruitment strategy for reinforcement learning, which enables the system to divide state space gradually according to task complexity and progress of learning. Some simulation results and real robot implementation show the validity of the method.",https://ieeexplore.ieee.org/document/1041504/,IEEE/RSJ International Conference on Intelligent Robots and Systems,30 Sept.-4 Oct. 2002,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICARCV.2012.6485305,A robust real-time tracking system based on an adaptive selection mechanism for mobile robots,IEEE,Conferences,"Extensive research has been conducted in the domain of object tracking. Among the existing tracking methods, most of them mainly focus on using various cues such as color, texture, contour, features, motion as well as depth information to achieve a robust tracking performance. The tracking methods themselves are highly emphasized while properties of the objects to be tracked are usually not exploited enough. In this paper, we first propose a novel adaptive tracking selection mechanism dependent on the properties of the objects. The system will automatically choose the optimal tracking algorithm after examining the textureness of the object. In addition, we propose a robust tracking algorithm for uniform objects based on color information which can cope with real world constraints. In the mean time, we deployed a textured object tracking algorithm which combines the Lucas-Kanade tracker and a model based tracker using the Random Forests classifier. The whole system was tested and the experimental results on a variety of objects show the effectiveness of the adaptive tracking selection mechanism. Moreover, the promising tracking performance shows the robustness of the proposed tracking algorithm. The computation cost of the algorithm is very low, which proves that it can be further used in various real-time robotics applications.",https://ieeexplore.ieee.org/document/6485305/,2012 12th International Conference on Control Automation Robotics & Vision (ICARCV),5-7 Dec. 2012,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IJCNN.2017.7965912,A self-driving robot using deep convolutional neural networks on neuromorphic hardware,IEEE,Conferences,"Neuromorphic computing is a promising solution for reducing the size, weight and power of mobile embedded systems. In this paper, we introduce a realization of such a system by creating the first closed-loop battery-powered communication system between an IBM Neurosynaptic System (IBM TrueNorth chip) and an autonomous Android-Based Robotics platform. Using this system, we constructed a dataset of path following behavior by manually driving the Android-Based robot along steep mountain trails and recording video frames from the camera mounted on the robot along with the corresponding motor commands. We used this dataset to train a deep convolutional neural network implemented on the IBM NS1e board containing a TrueNorth chip of 4096 cores. The NS1e, which was mounted on the robot and powered by the robot's battery, resulted in a self-driving robot that could successfully traverse a steep mountain path in real time. To our knowledge, this represents the first time the IBM TrueNorth has been embedded on a mobile platform under closed-loop control.",https://ieeexplore.ieee.org/document/7965912/,2017 International Joint Conference on Neural Networks (IJCNN),14-19 May 2017,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ROBOT.2010.5509238,A voice-commandable robotic forklift working alongside humans in minimally-prepared outdoor environments,IEEE,Conferences,"One long-standing challenge in robotics is the realization of mobile autonomous robots able to operate safely in existing human workplaces in a way that their presence is accepted by the human occupants. We describe the development of a multi-ton robotic forklift intended to operate alongside human personnel, handling palletized materials within existing, busy, semi-structured outdoor storage facilities. The system has three principal novel characteristics. The first is a multimodal tablet that enables human supervisors to use speech and pen-based gestures to assign tasks to the forklift, including manipulation, transport, and placement of palletized cargo. Second, the robot operates in minimally-prepared, semi-structured environments, in which the forklift handles variable palletized cargo using only local sensing (and no reliance on GPS), and transports it while interacting with other moving vehicles. Third, the robot operates in close proximity to people, including its human supervisor, other pedestrians who may cross or block its path, and forklift operators who may climb inside the robot and operate it manually. This is made possible by novel interaction mechanisms that facilitate safe, effective operation around people. We describe the architecture and implementation of the system, indicating how real-world operational requirements motivated the development of the key subsystems, and provide qualitative and quantitative descriptions of the robot operating in real settings.",https://ieeexplore.ieee.org/document/5509238/,2010 IEEE International Conference on Robotics and Automation,3-7 May 2010,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICIRCA48905.2020.9182995,An Approach for Digital Farming using Mobile Robot,IEEE,Conferences,"Farming is the backbone of the Indian economy and it has been unchartered territory for a technological solution. As of late developments in Artificial Intelligence technology combined with Robotics has paved the way for an option of digital farming. As a matter of fact, Indian farming has been facing various challenges that include abrupt change in climatic conditions, spoiling of yields, soil nutrient requirement, pests/weed control and so forth. Robotics and Artificial Intelligence (AI) along with the integration of various sensors ensures the possibility of better outcome. In this work the simulation of Mobile robot for the purpose of seed sowing along with its movement has been presented. The implementation comprises of the Motor schema for the navigation of robot and Gale Shapley (GS) algorithm for stable match of seed and yield combination. Such a robotic system combined with AI in real time will form excellent means of farming in terms of yield.",https://ieeexplore.ieee.org/document/9182995/,2020 Second International Conference on Inventive Research in Computing Applications (ICIRCA),15-17 July 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISEC52395.2021.9763986,An Autonomous Driving Simulation Platform as a Virtual HSAVC Competition Environment,IEEE,Conferences,"At ISEC 2018, Professor Marc E. Herniter presented the High School Autonomous Vehicle Competition (HSAVC), which introduces autonomous driving to high school students. The competition promotes STEM education by challenging participants to use MATLAB to create a vision-based track detection algorithm and Simulink to build a motor controller model. Following the COVID-19 global pandemic, many in-person STEM competitions were canceled, including HSAVC. The goal of the Autonomous Driving Simulation Platform is to replicate the physical conditions of HSAVC using simulation to allow students to continue the activity virtually. Using MATLAB, the Simulation Platform creates a real-time virtual environment for students to test their HSAVC track detection algorithms and motor controller models. The Simulation Platform consists of two MATLAB apps: a Track Generator and a Driving Simulator. The Track Generator application can create fixed tracks based on user inputs or randomized tracks based on user-defined lengths. The Track Generator utilizes a growth and mutation algorithm to create a track with three distinct track sections: straight, left curve, and right curve. The Track Generator&#x2019;s randomized track replicates the HSAVC&#x2019;s physical track, and the Driving Simulator replicates the HSAVC&#x2019;s 1:18 scale autonomous vehicle equipped with a linescan camera, two drive motors, and a servo motor with a vehicle model and a camera model. The Track Generator and Driving Simulator have been successfully designed and implemented with MATLAB App Designer. Users can create a track and test their algorithms and models through an intuitive interface, making it an effective tool for STEM education in any classroom. The Autonomous Driving Simulation Platform holds potential as a solution to continue the HSAVC during the pandemic and can increase student engagement in the HSAVC from high schools around the world like Amazon Web Services DeepRacer. Another benefit of the Simulation Platform is convenient and controlled virtual algorithm testing, which allows for repetitive experimentation to be simulated without risk of damaged materials. The simulation platform has broad potential as an educational tool, such as complementing high school robotics curriculums to teach motor control algorithms and training reinforced learning racing models. The successful virtual adaptation of HSAVC demonstrates how simulation can provide many educational benefits when borrowing the framework of STEM competition.",https://ieeexplore.ieee.org/document/9763986/,2021 IEEE Integrated STEM Education Conference (ISEC),13-13 March 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ROBOT.2010.5509310,An Inertia-Based Surface Identification System,IEEE,Conferences,"In many robotics applications, knowing the material properties around a robot is often critical for the robot's successful performance. For example, in mobility, knowledge about the ground surface may determine the success of a robot's gait. In manipulation, the physical properties of an object may dictate the results of a grasping strategy. Thus, a reliable surface identification system would be invaluable for these applications. This paper presents an Inertia-Based Surface Identification System (ISIS) based on accelerometer sensor data. Using this system, a robot actively “knocks” on a surface with an accelerometer-equipped device (e.g., hand or leg), collects the accelerometer data in real-time, and then analyzes and extracts three critical physical properties, the hardness, the elasticity, and the stiffness, of the surface. A lookup table and k-nearest neighbors techniques are used to classify the surface material based on a database of previously known materials. This technique is low-cost and efficient in computation. It has been implemented on the modular and self-reconfigurable SuperBot and has achieved high accuracy (95% and 85%) in several identification experiments with real-world material.",https://ieeexplore.ieee.org/document/5509310/,2010 IEEE International Conference on Robotics and Automation,3-7 May 2010,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISSE46696.2019.8984462,An IoT Reconfigurable SoC Platform for Computer Vision Applications,IEEE,Conferences,"The field of Internet of Things (IoT) and smart sensors has expanded rapidly in various fields of research and industrial applications. The area of IoT robotics has become a critical component in the evolution of Industry 4.0 standard. In this paper, we developed an IoT based reconfigurable System on Chip (SoC) robot that is fast and efficient for computer vision applications. It can be deployed in other IoT robotics applications and achieve its intended function. A Terasic Hexapod Spider Robot (TSR) was used with its DE0-Nano SoC board to implement our IoT robotics system. The TSR was designed to provide a competent computer vision application to recognize different shapes using a machine learning classifier. The data processing for image detection was divided into two parts, the first part involves hardware implementation on the SoC board and to provide real-time interaction of the robot with the surrounding environment. The second part of implementation is based on the cloud processing technique, where further data analysis was performed. The image detection algorithm for the computer vision component was tested and successfully implemented to recognize shapes. The TSR moves or reacts based on the detected image. The Field Programmable Gate Array (FPGA) part is programmed to handle the movement of the robot and the Hard Processor System (HPS) handles the shape recognition, Wi-Fi connectivity, and Bluetooth communication. This design is implemented, tested and can be used in real-time applications in harsh environments where movements of other robots are restricted.",https://ieeexplore.ieee.org/document/8984462/,2019 International Symposium on Systems Engineering (ISSE),1-3 Oct. 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/DCOSS.2019.00111,An Open Source and Open Hardware Deep Learning-Powered Visual Navigation Engine for Autonomous Nano-UAVs,IEEE,Conferences,"Nano-size unmanned aerial vehicles (UAVs), with few centimeters of diameter and sub-10 Watts of total power budget, have so far been considered incapable of running sophisticated visual-based autonomous navigation software without external aid from base-stations, ad-hoc local positioning infrastructure, and powerful external computation servers. In this work, we present what is, to the best of our knowledge, the first 27g nano-UAV system able to run aboard an end-to-end, closed-loop visual pipeline for autonomous navigation based on a state-of-the-art deep-learning algorithm, built upon the open-source CrazyFlie 2.0 nano-quadrotor. Our visual navigation engine is enabled by the combination of an ultra-low power computing device (the GAP8 system-on-chip) with a novel methodology for the deployment of deep convolutional neural networks (CNNs). We enable onboard real-time execution of a state-of-the-art deep CNN at up to 18Hz. Field experiments demonstrate that the system's high responsiveness prevents collisions with unexpected dynamic obstacles up to a flight speed of 1.5m/s. In addition, we also demonstrate the capability of our visual navigation engine of fully autonomous indoor navigation on a 113m previously unseen path. To share our key findings with the embedded and robotics communities and foster further developments in autonomous nano-UAVs, we publicly release all our code, datasets, and trained networks.",https://ieeexplore.ieee.org/document/8804776/,2019 15th International Conference on Distributed Computing in Sensor Systems (DCOSS),29-31 May 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/WCICA.2000.859945,An agent team for RoboCup simulator league,IEEE,Conferences,"RoboCup is an attempt to promote AI and robotics research by providing a common task, soccer playing, for evaluation of various theories, algorithms and agent architectures. RoboCup consists of both the real robot league and the simulator league, where the soccer server is a standard software platform. A wide range of key issues on AI research emerges when designing simulator teams, such as the agent architecture, multi-agent teamwork, machine learning, etc. These issues are what we concerned most when developing our simulator team. Our team participated the first RoboCup tournament in China, and won the second place in the competition.",https://ieeexplore.ieee.org/document/859945/,Proceedings of the 3rd World Congress on Intelligent Control and Automation (Cat. No.00EX393),26 June-2 July 2000,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICARCV.2002.1235010,An autonomous mobile robot with fuzzy obstacle avoidance behaviors and a visual landmark recognition system,IEEE,Conferences,"Multi-sensor fusion has been a hot topic in the field of robotics. Inspired by the modern philosophy's spirit, the behavior-based systems interact with the real world directly. In this study, a fully autonomous mobile robot is developed that extracts all its knowledge from physical sensors and expresses all its goals and desires as physical action to affect its environment. The control software implements behavior-based artificial intelligence, where the coordination between various sensors are realized by layers of several simple and primitive behaviors similar to those observed in animals. In the developed mobile robot, each module itself generates behaviors. Behaviors corresponding to different sensors have different priorities, where the vision system has the lowest priority, and the ultrasonic sensors and bumper sensors have higher priority. The effectiveness of the developed system is demonstrated by experimental studies.",https://ieeexplore.ieee.org/document/1235010/,"7th International Conference on Control, Automation, Robotics and Vision, 2002. ICARCV 2002.",2-5 Dec. 2002,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ROBOT.2010.5509935,An insect-based method for learning landmark reliability using expectation reinforcement in dynamic environments,IEEE,Conferences,"Navigation in unknown dynamic environments still remains a major challenge in robotics. Whereas insects like the desert ant with very limited computing and memory capacities solve this task with great efficiency. Thus, the understanding of the underlying neural mechanisms of insect navigation can inform us on how to build simpler yet robust autonomous robots. Based on recent developments in insect neuroethology and cognitive psychology, we propose a method for landmark navigation in dynamic environments. Our method enables the navigator to learn the reliability of landmarks using an expectation reinforcement method. For that end, we implemented a real-time neuronal model based on the Distributed Adaptive Control framework. The results demonstrate that our model is capable of learning the stability of landmarks by reinforcing its expectations. Also, the proposed mechanism allows the navigator to optimally restore its confidence when its expectations are violated. We also perform navigational experiments with real ants to compare with the results of our model. The behavior of the proposed autonomous navigator closely resembles real ant navigational behavior. Moreover, our model explains navigation in dynamic environments as a memory consolidation process, harnessing expectations and their violations.",https://ieeexplore.ieee.org/document/5509935/,2010 IEEE International Conference on Robotics and Automation,3-7 May 2010,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICPR.1992.201634,An intelligent mobile robot golfing system using binocular stereo vision,IEEE,Conferences,"This paper describes a robot vision golfing system. The ARNIE P/sup tau / (Automated Robotic Navigational unit with Intelligent Eye and Putter) project was initiated to investigate the problems and develop software solutions for robotic tasks that require good hand-eye coordination and an intelligent feedback mechanism. This system has only one frame buffer and no specialized hardware, so quasi-real-time 3D tracking is accomplished in software using the unix spline facility. Golf is a difficult perceptory task which requires the integration of many complicated computational tasks. It is therefore a good platform to experiment with artificial intelligence techniques and robotics.<>",https://ieeexplore.ieee.org/document/201634/,[1992] Proceedings. 11th IAPR International Conference on Pattern Recognition,30 Aug.-3 Sept. 1992,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SYSCON.2018.8369547,An interactive architecture for industrial scale prediction: Industry 4.0 adaptation of machine learning,IEEE,Conferences,"According to wiki definition, there are four design principles in Industry 4.0. These principles support companies in identifying and implementing Industry 4.0 scenarios, namely, Interoperability, Information transparency, Technical assistance, Decentralized decisions. In this paper we have discussed our work on an implementation of a machine learning based interactive architecture for industrial scale prediction for dynamic distribution of water resources across the continent, keeping the four corners of Industry 4.0 in place. We report the possibility of producing most probable high resolution estimation regarding the water balance in any region within Australia by implementation of an intelligent system that can integrate spatial-temporal data from various independent sensors and models, with the ground truth data produced by 250 practitioners from the irrigation industry across Australia. This architectural implementation on a cloud computing platform linked with a freely distributed mobile application, allowing interactive ground truthing of a machine learning model on a continental scale, shows accuracy of 90% with 85% sensitivity of correct surface soil moisture estimation with end users at its complete control. Along with high level of information transparency and interoperability, providing on-demand technical supports and motivating users by allowing them to customize and control their own local predictive models, show the successfulness of principles in Industry 4.0 in real environmental issues in the future adaptation in various industries starting from resource management to modern generation soft robotics.",https://ieeexplore.ieee.org/document/8369547/,2018 Annual IEEE International Systems Conference (SysCon),23-26 April 2018,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICSMC.2004.1398386,Ant colony optimization based swarms: implementation for the mine detection application,IEEE,Conferences,"Mine detection is a sensitive task confronting the battlefield strategists. There is an ever-increasing demand for proper and sophisticated resources for many issues involved in the task. Traditional practices still involve human force directly in executing the tasks in spite of the advances in technology for tools and implements for the operation [GAO, 2001]. The problem includes various facets inherently: two of the prominent issues are location of mines over a minefield and secondly removal of the mines once located [GAO, 2001]. These two issues are not totally independent as technology used for one can directly or indirectly affect the other. Developments in artificial intelligence, natural heuristics, computational optimization and robotics have endowed us with the ability to realize unmanned robots (or robot like vehicles) that work intelligently on a real time basis in attempting at the problem of mine detection. In this paper we focus on the algorithms developed using ant colony optimization based approaches to the mine detection application and its implementation on a real-time basis. We focus on certain optimization techniques that could be used for effective realization of the algorithm. Generic groundscout robots had been already built at the MABL, RIT [Sahin F. et al., 2003]. These robots have been used to demonstrate the implementation",https://ieeexplore.ieee.org/document/1398386/,"2004 IEEE International Conference on Systems, Man and Cybernetics (IEEE Cat. No.04CH37583)",10-13 Oct. 2004,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/DASA54658.2022.9765294,"Application of AI, IOT and ML for Business Transformation of The Automotive Sector",IEEE,Conferences,"Automotive industry is essential in human lives. It is not possible to imagine a day without driving or some public transport. Today, digital technologies are making motor vehicles and the industry more intelligent. The entire value chain of automotive business is transforming. A better connect with customers is needed. All this is possible through advanced digital technologies. Automotive companies are overhauling business processes and relationships. Legacy IT systems for manufacturing, engineering, supply chain etc. are being reinvented. This transformation encompasses software, robotics, connected devices, and artificial intelligence. Artificial intelligence (AI) made the dream of self-driving cars possible. AI will soon transform every device. Tesla, Google Waymo, and Nvidia are examples of machine learning algorithms used to detect how far different objects are, from the car. Augmented reality (AR) and virtual reality (VR) analysis enables users to watch blind spots. AI enhances security by simultaneous coordination with many sensors. With AR, VR and mixed reality (MR), automotive companies have a personalized retail platform and a competitive edge. This paper studies AI applications in the automotive sector. It studies the recent developments, and applications of AI. It discusses how companies use AI for cost reduction, market strategies, sales promotion, and even funding.",https://ieeexplore.ieee.org/document/9765294/,2022 International Conference on Decision Aid Sciences and Applications (DASA),23-25 March 2022,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICNN.1996.549172,Applying self-organizing networks to recognizing rooms with behavior sequences of a mobile robot,IEEE,Conferences,"We describe the application of a self-organizing network to the robot which learns to recognize rooms (enclosures) using behavior sequences. In robotics research, most studies on recognizing environments have tried to build the precise geometric map with highly sensitive sensors. However many natural agents like animals recognize the environments with low sensitivity sensors, and a geometric map may not be necessary. Thus we attempt to build a mobile robot using a self-organizing network to recognize the enclosures, in which it acts, with low sensitivity and local sensors. The mobile robot is behavior-based and does wall-following in an enclosure. Then the sequences of behaviors executed in each enclosure are obtained. The sequences are transformed into real-value vectors, and inputted to the Kohonen self-organizing network. Unsupervised learning is done and a mobile robot becomes able to distinguish and identify enclosures. We fully implemented the system using a real mobile robot and made experiments for evaluating the ability. Consequently we found out the recognition of enclosures was done well and our method was robust against small obstacles in an enclosure.",https://ieeexplore.ieee.org/document/549172/,Proceedings of International Conference on Neural Networks (ICNN'96),3-6 June 1996,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IVS.2018.8500457,Artificial Intelligence Course Design: iSTREAM-based Visual Cognitive Smart Vehicles,IEEE,Conferences,"New intelligent era calls for new learners and thus urgently needs a series of artificial intelligence. As a good educational platform for teaching artificial intelligence, smart cars have aroused concern and practices of all parties. However, at present, most courses and training pay more attention to basic knowledge and technology of smart cars, seldom to training based on artificial intelligence curriculum system and comprehensive competency integrating science, technology, art and management. Therefore, based on concept of iSTREAM (intelligence for Science, Technology, Robotics, Engineering, Art, and Management) and Raspberry intelligent vehicle teaching platform, this paper introduced a smart car-themed artificial intelligence courses including basic courses, specialized courses, specialized technical courses and elective courses. This course can guide learners to develop smart cars based on visual cognition, in-depth learning, VR and 3D printing integrated artistic creativity. It combines disciplines such as science, technology, art, games and management to upgrade a single knowledge and technology course into a comprehensive competency course that integrates knowledge, skills, emotion and management. Practice in Beijing NO.13 and NO.101 High School shows that this course allows students to experience scientific research process, learn artificial intelligence related knowledge and skills, understand scientific way of thinking and scientific research methods, stimulate learners' responsibility and scientific passion, and cultivate leadership skills through self-learning and partly project management.",https://ieeexplore.ieee.org/document/8500457/,2018 IEEE Intelligent Vehicles Symposium (IV),26-30 June 2018,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SIBCON50419.2021.9438884,Assessment of Map Construction in vSLAM,IEEE,Conferences,"Vision-based Simultaneous Localization and Mapping (vSLAM) is a challenging task in modern computer vision. vSLAM is particularly important as mobile robotics application. It allows to localize the robot and build the map of unknown environment in 3D in real-time. During research and development of new methods, it needs extensive evaluation on trajectory and map quality compared to known methods. In this work we focus on map quality estimation. We develop the simulated ground-truth data in photo-realistic environment and introduce new metrics in order to estimate map quality. We evaluate neural network based vSLAM methods with our framework in order to show that it fits map quality estimation more than standard approaches. Open-source implementation of our map metrics is available at https://github.com/CnnDepth/slam_comparison.",https://ieeexplore.ieee.org/document/9438884/,2021 International Siberian Conference on Control and Communications (SIBCON),13-15 May 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ic-ETITE47903.2020.203,Automated Detection Of Driving Pathway Using Image Processing,IEEE,Conferences,"Image data is one of the most popular real world input data that can be used for variety of applications ranging from robotics and computer vision to security systems. In combination with other methods such as neural network, Artificial neural network and image processing techniques, manipulation of image data can lead to applications such as detection of objects, tracking, identification and vision based robotics and so on. Advanced Driver Assistance System (ADAS) also use image for camera based driver assistance systems.The report covers a hardware model system that tests the software work of detection of traffic signs and path for it own ADAS systems. Different problems were tackled, including the choice of OS, and additional hardware components needed to tackle. The choice of programming languages, equipment, OS and methods were based on simplicity and practicality. Artificial neural network in combination with Open CV libraries were used for stop sign, traffic light and path road detection. The hardware model consisted of RC Car attached to raspberry pi board with a mounted pi camera for video streaming and an arduino controller attached to a radio transmitter for controlling through Open CV running in windows PC.",https://ieeexplore.ieee.org/document/9077722/,2020 International Conference on Emerging Trends in Information Technology and Engineering (ic-ETITE),24-25 Feb. 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SMC.2018.00655,Automated Training Plan Generation for Athletes,IEEE,Conferences,"In sports, athletes need detailed and individualised training plans for maintaining and improving their skills in order to achieve their best performance in competitions. This presents a considerable workload for coaches, who besides setting objectives have to formulate extremely detailed training plans. Automated Planning, which has already been successfully deployed in many real-world applications such as space exploration, robotics, and manufacturing processes, embodies a useful mechanism that can be exploited for generating training plans for athletes. In this paper, we propose the use of Automated Planning techniques for generating individual training plans, which consist of exercises the athlete has to perform during training, given the athlete's current performance, period of time, and target performance that should be achieved. Our experimental analysis, which considers general training of kickboxers, shows that apart of considerable less planning time, training plans automatically generated by the proposed approach are more detailed and individualised than plans prepared manually by an expert coach.",https://ieeexplore.ieee.org/document/8616652/,"2018 IEEE International Conference on Systems, Man, and Cybernetics (SMC)",7-10 Oct. 2018,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICACTA54488.2022.9753620,Automatic Traffic Sign Detection System With Voice Assistant,IEEE,Conferences,"In the arena of artificial intelligence, the world is revolutionizing with many technological applications being incorporated with Artificial Intelligence due to improved efficiency and performance. AI has penetrated drastically, delving deep into locker room decisions in many fields like agriculture, healthcare, military, manufacturing, robotics, transportation and so on. AI does a lot more than improving our lives, in most cases, it saves our lives too. Autonomous vehicles, the so-called self-driving cars, are one of the greatest applications of AI and are very instrumental in making the machine work autonomously by observing and interpreting the real-life scenario of the environment. This paper deals with the deployment of an Automatic Traffic sign detection System with voice assistant, which is one of the applications of autonomous vehicles, which can tone down the driver from puzzling traffic conditions significantly increasing driving safety and comfort. This will require an appropriate database and algorithm for improved accuracy in performance. This paper, therefore, compares the features, accuracy, and efficiency of various deep learning algorithms and comes up with a varied model thus saving computational resources.",https://ieeexplore.ieee.org/document/9753620/,2022 International Conference on Advanced Computing Technologies and Applications (ICACTA),4-5 March 2022,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICAC.2004.1301379,Autonomic systems for mobile robots,IEEE,Conferences,"Mobile robots are an excellent testbed for autonomic computing research. The ultimate goal of robotics research is to develop a platform that can function autonomously in the face of hardware and software failures. This goal is becoming more important as robots are increasingly being deployed outside of controlled environments. In this paper, we discuss our work toward implementing an autonomic system for a mobile robot. This work is motivated by our experiences with existing mobile robot control software during real-world deployments.",https://ieeexplore.ieee.org/document/1301379/,"International Conference on Autonomic Computing, 2004. Proceedings.",17-18 May 2004,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICAMIMIA47173.2019.9223365,Autonomous Car Simulation Using Evolutionary Neural Network Algorithm,IEEE,Conferences,"Automation with artificial intelligence (AI) has widely implemented in robotics, transportation and manufacture. AI has become a powerful technology that change human life and help human more flexible doing something. In this paper, it will show a result of simulation from an autonomous car using the evolutionary neural network algorithm which combines genetic algorithm and neural network. The purpose of the simulation is to test the model that we develop to know the right direction based on the track, so the evolutionary neural network that implemented to the autonomous car be able to deliver the best solution before it implements in the real machine or car technology. Genetic algorithm combines with a neural network to reach an evolution condition. The evolution process is achieved through crossover, mutation and selection process, so the algorithm will give the best result from the iteration of the experiment. The result of our experiment shows that evolutionary neural network algorithm give the best result within 3 layer architecture, with iteration average is 14.5 reach finish point (check point) 3 in the track simulation. Based on the simulation, our car model can find out the right direction.",https://ieeexplore.ieee.org/document/9223365/,"2019 International Conference on Advanced Mechatronics, Intelligent Manufacture and Industrial Automation (ICAMIMIA)",9-10 Oct. 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS.2017.8202143,Autonomous skill-centric testing using deep learning,IEEE,Conferences,Software testing is an important tool to ensure software quality. This is a hard task in robotics due to dynamic environments and the expensive development and time-consuming execution of test cases. Most testing approaches use model-based and/or simulation-based testing to overcome these problems. We propose model-free skill-centric testing in which a robot autonomously executes skills in the real world and compares it to previous experiences. The skills are selected by maximising the expected information gain on the distribution of erroneous software functions. We use deep learning to model the sensor data observed during previous successful skill executions and to detect irregularities. Sensor data is connected to function call profiles such that certain misbehaviour can be related to specific functions. We evaluate our approach in simulation and in experiments with a KUKA LWR 4+ robot by purposefully introducing bugs to the software. We demonstrate that these bugs can be detected with high accuracy and without the need for the implementation of specific tests or task-specific models.,https://ieeexplore.ieee.org/document/8202143/,2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),24-28 Sept. 2017,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICAR53236.2021.9659396,Autonomy in robotic prostate biopsy through AI-assisted fusion,IEEE,Conferences,"Artificial intelligence (AI) and especially deep learning (DL) are applied in many fields and are the leading technologies towards robot autonomy. Proper DL algorithms and sufficient amount of medical data may be applied in medical robotics to increase autonomy, safety, accuracy and precision of a medical procedure. We investigate in this paper the implementation and the integration of DL techniques in a new robotic system for prostate biopsy-PROST. The DL algorithm, named PROST-Net, is a convolutional neural network (CNN) employed for the segmentation of the prostate in different types of medical images: pre-operative magnetic resonance (MRI) and intra-operative ultrasound (US). The US images come from different acquisition planes (axial and sagittal) with different alignments of the sensors (convex and linear) making the design of the CNN challenging. Tests on patient data produced an accuracy of 86% in US images and 77% in MRI and were estimated by using Dice similarity Coefficient (DC). The biopsy robot will use the output of PROST-Net for the initial fusion of pre-operative and intra-operative images to define the biopsy targets and the planning of the procedure. Real-time processing of the data with PROST-Net will empower dynamic update of the initial fusion by following the current position of the prostate with respect to the robotic reference frame.",https://ieeexplore.ieee.org/document/9659396/,2021 20th International Conference on Advanced Robotics (ICAR),6-10 Dec. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/BigComp48618.2020.00-21,Benchmarking Jetson Platform for 3D Point-Cloud and Hyper-Spectral Image Classification,IEEE,Conferences,"Modern innovations of embedded system platforms (hardware accelerations) play a vital role in revolutionizing deep learning into practical scenarios, transforming human efforts into an automated intelligent system such as autonomous driving, robotics, IoT (Internet-of-Things) and many other useful applications. NVIDIA Jetson platform provides promising performance in terms of energy efficiency, favorable accuracy, and throughput for running deep learning algorithms. In this paper, we present benchmarking of Jetson platforms (Nano, TX1, and Xavier) by evaluating its performance based on computationally expensive deep learning algorithms. Previously, most of the benchmark results were based on 2-D images with conventional deep learning models for image processing. However, the implementation of many other complex data types at Jetson platform has remained a challenge. We also showed the practical impact of optimizing the algorithm vs improving the hardware accelerations by deploying a diverse range of dense and intensive deep learning architectures at all three aforementioned Jetson platforms, to make a better comparison of performance. In this regard, we have used two entirely different data-types, namely (i) ModelNet-40(Princeton-3D point-cloud) data-set along with PointNet deep learning architecture for classification of 3D point-cloud, and (ii) hyperspectral images (HSI) datasets (KSC and Pavia) alongside stacked autoencoders(SAE) to classify HSI correspondingly. This will broaden the scope of edge-devices to handle 3-D and HSI data whilst real-time classification will be processed at edge-server under the umbrella of edge-computing. The selection of (i) was made to exploit GPU heavily as the code uses TensorFlowgpu whereas (ii) was chosen to challenge the CPU cores of each platform as the code is based on Theano and may suffer from under-utilizing the GPU cores. We have presented the detailed evaluation exclusively in term of performance indices as inference time, the maximum number of concurrent processes, resource utilization per process and efficiency",https://ieeexplore.ieee.org/document/9070378/,2020 IEEE International Conference on Big Data and Smart Computing (BigComp),19-22 Feb. 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS40897.2019.8967694,Benchmarking and Workload Analysis of Robot Dynamics Algorithms,IEEE,Conferences,"Rigid body dynamics calculations are needed for many tasks in robotics, including online control. While there currently exist several competing software implementations that are sufficient for use in traditional control approaches, emerging sophisticated motion control techniques such as nonlinear model predictive control demand orders of magnitude more frequent dynamics calculations. Current software solutions are not fast enough to meet that demand for complex robots. The goal of this work is to examine the performance of current dynamics software libraries in detail. In this paper, we (i) survey current state-of-the-art software implementations of the key rigid body dynamics algorithms (RBDL, Pinocchio, Rigid-BodyDynamics.jl, and RobCoGen), (ii) establish a methodology for benchmarking these algorithms, and (iii) characterize their performance through real measurements taken on a modern hardware platform. With this analysis, we aim to provide direction for future improvements that will need to be made to enable emerging techniques for real-time robot motion control. To this end, we are also releasing our suite of benchmarks to enable others to help contribute to this important task.",https://ieeexplore.ieee.org/document/8967694/,2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),3-8 Nov. 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS.2005.1545040,Broker: an interprocess communication solution for multi-robot systems,IEEE,Conferences,"We describe in this paper a novel implementation of the interprocess communication (IPC) technology, called Broker, in support of the development and the operation of a complex robot system. We view each robot system as a collection of processes that need to exchange information, e.g. motion commands and sensory data, in a flexible and convenient fashion, without affecting each other's operations in case of a process's scheduled termination or unexpected failure. We argue that the IPC technology provides an ideal framework for this purpose, and we carefully make our design decisions about its implementation based on the needs of robotics applications. Broker is programming language, operating system, and hardware platform independent and has served us well in a RoboCup project and collective robotics experiments, in both simulation and real-world environments.",https://ieeexplore.ieee.org/document/1545040/,2005 IEEE/RSJ International Conference on Intelligent Robots and Systems,2-6 Aug. 2005,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISDA.2010.5687045,Bézier curve based dynamic obstacle avoidance and trajectory learning for autonomous mobile robots,IEEE,Conferences,"This paper addresses the problem of avoiding dynamic obstacles while following the learned trajectory through non-point based maps directly through laser data. The geometric representation of free configuration area changes while a moving obstacle enters into the safety region of autonomous mobile robot. We have applied the Bézier curve properties to the free configuration eigenspaces to satisfy the dynamic obstacle avoidance path constraints. The algorithm is designed to accurately represent the mobile robot's characteristics while avoiding obstacle such as minimum turning radius. Moreover, we also discuss the obstacle avoided path feasibility as a vectorial combination of free configuration eigen-vectors at discrete time scan-frames to manifest a trajectory, which once followed and mapped onto the two control signals of mobile robot will enable it to build an efficient and accurate online environment map. Preliminary results in Matlab have been shown to validate the idea, while the same has been implemented in Player/stage (robotics real-time software) to analyze the performance of the proposed system.",https://ieeexplore.ieee.org/document/5687045/,2010 10th International Conference on Intelligent Systems Design and Applications,29 Nov.-1 Dec. 2010,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCE-Berlin.2018.8576251,CNN Inference: Dynamic and Predictive Quantization,IEEE,Conferences,"Deep Learning techniques like Convolutional Neural Networks (CNN) are the de-facto method for image classification with broad usage spanning across automotive, industrial, medicine, robotics etc. Efficient implementation of CNN inference on embedded device requires a quantization method, which minimizes the accuracy loss, ability to generalize across deployment scenarios as well as real-time processing. Existing literature doesn't address all these three requirements simultaneously. In this paper, we propose a novel quantization algorithm to overcome above mentioned challenges. The proposed solution dynamically selects the scale for quantizing activations and uses Kalman filter to predict quantization scale to reduce accuracy loss. The proposed solution exploits the range statistics from previous inference processes to estimate quantization scale, enabling real-time solution. The proposed solution is implemented on TI's TDA family of embedded automotive processors. The proposed solution is running real time semantic segmentation on TDA2x processor within 0.1% accuracy loss compared floating point algorithm. The solution performs well across multiple deployment scenarios (e.g. rain, snow, night etc) demonstrating generalization capability of the solution.",https://ieeexplore.ieee.org/document/8576251/,2018 IEEE 8th International Conference on Consumer Electronics - Berlin (ICCE-Berlin),2-5 Sept. 2018,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.23919/ACC.1989.4790240,CONDOR: A Coarse-grained Parallel Architecture for Robot Control,IEEE,Conferences,"This paper presents an overview of the CONDOR, a real-time development environment designed for robotics research applications. The architecture is based on standard hardware components consisting of upto eight microprocessors interconnected through a shared memory bus, and is coupled with a powerful software environment based on message passing that enables the development of control programs for complicated robots. The hardware is extremely easy to set up since it uses standard components. Besides program libraries tuned for real-time control, the software utilities include a multi-processor pseudo-terminal emulator, a file-server and a flexible symbolic debugger that greatly enhance programmer productivity.",https://ieeexplore.ieee.org/document/4790240/,1989 American Control Conference,21-23 June 1989,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICECCME52200.2021.9591113,Cobots for FinTech,IEEE,Conferences,"Embedded devices enabling payments transaction processing in Financial Services industry cannot have any margin for error. These devices need to be tested & validated by replicating production like environment to the extent possible. This means literally handling payments related events like swiping a credit card, tapping a mobile phone or pressing buttons amongst many other things like in real world. Embedded Software development is time consuming as it involves multiple man-machine interactions and dependencies such as managing and handling embedded devices, operating devices (Push buttons, interpret display panels, read receipt printouts etc.) and sharing devices for collaboration within team. During the current pandemic, it was impossible for software teams to travel to office, share devices or even procure necessary devices on time for project related tasks. This caused delay to project delivery and increased Time to market. The paper describes how the team used Capgemini's flexible Robotics as a Service (RaaS) platform that helped during pandemic to automate feasible man-machine interactions using Robotic arms. The paper provides details of the work done by the team that involves internet of things (IoT), Artificial Intelligence (AI) to remotely handle and operate hardware and devices thereby completing embedded software development life cycles faster and well within budget while ensuring superior product quality and importantly ensuring team's health and safety. This is novel in Financial Services space.",https://ieeexplore.ieee.org/document/9591113/,"2021 International Conference on Electrical, Computer, Communications and Mechatronics Engineering (ICECCME)",7-8 Oct. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/MSM49833.2020.9202398,Collaborative Robot System for Playing Chess,IEEE,Conferences,"In recent years, number of collaborative robots industrial applications has made a significant increasment. Implementation of collaborative robots is a safe and effective way for designing robot-human cooperation systems. Combined with constantly developing artificial intelligence, collaborative systems are actually able to solve complex problems that require some sort of intelligence. For humans, board games are a good example of the visualization of robot intelligence. Such systems require estimation and detection of board and pieces in manipulator workspace, some kind of decision-making algorithms and robot control system to move pieces. The flagship of such systems are chess playing robots. The chess game has a defined and easy to understand set of rules which makes it interesting example of intelligent robotics systems application. In this paper, we present an implementation of collaborative robots for chess playing system which was designed to play against human or another robot. The system is able to track state of the game via camera, calculate the optimal move using implemented decision-making algorithm, detect illegal moves and execute pick-and-place task to physically move pieces. We test the developed system in a real-world setup and provide experimental results documenting the performance of proposed approach.",https://ieeexplore.ieee.org/document/9202398/,2020 International Conference Mechatronic Systems and Materials (MSM),1-3 July 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISM52913.2021.00039,Combining Linked Open Data and Multimedia Knowledge Base for Digital Cultural Heritage Robotic Applications,IEEE,Conferences,"The current trends in society evolution are showing rapid changes in our habitual environments and consequently affecting human interactions with them. Among the key factors of this process we can certainly cite the growth of BigData favored by the knowledge digitalization, the dissemination of sensors in environments and the advancements of connectivity capabilities. At the same time, the progress in artificial intelligence and cognitive robotics has lead to the production of sophisticated humanoid robots, which are progressively spreading to a wide public. In this way, research efforts are needed for a proper knowledge management and knowledge acquisition by machines, in order to have more natural and friendly human-robot interactions in daily tasks, usually performed by human beings. In this paper we show an approach related to a human-robot interaction in cultural heritage context, simulating a digital ecosystem where a robot plays the role of a guide for tourists and it is able to proactively interact with its interlocutors by combining both semantic and visual information. The proposed approach, its implementation and experimental results on a real robotic platform are shown and discussed.",https://ieeexplore.ieee.org/document/9666077/,2021 IEEE International Symposium on Multimedia (ISM),29 Nov.-1 Dec. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IVCNZ51579.2020.9290542,Comparison of Face Detection Algorithms on Mobile Devices,IEEE,Conferences,"Face detection is a fundamental task for many computer vision applications such as access control, security, advertisement, automatic payment, and healthcare. Due to technological advances mobile robots are becoming increasingly common in such applications (e.g. healthcare and security robots) and consequently there is a need for efficient and effective face detection methods on such platforms. Mobile robots have different hardware configurations and operating conditions from desktop applications, e.g. unreliable network connections and the need for lower power consumption. Hence results for face detection methods on desktop platforms cannot be directly translated to mobile platforms.We compare four common face detection algorithms, Viola-Jones, HOG, MTCNN and MobileNet-SSD, for use in mobile robotics using different face data bases. Our results show that for a typical mobile configuration (Nvidia Jetson TX2) Mobile-NetSSD performed best with 90% detection accuracy for the AFW data set and a frame rate of almost 10 fps with GPU acceleration. MTCNN had the highest precision and was superior for more difficult face data sets, but did not achieve real-time performance with the given implementation and hardware configuration.",https://ieeexplore.ieee.org/document/9290542/,2020 35th International Conference on Image and Vision Computing New Zealand (IVCNZ),25-27 Nov. 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/MI-STA52233.2021.9464484,Comparison of PID and Artificial Neural Network Controller in on line of Real Time Industrial Temperature Process Control System,IEEE,Conferences,"Due to its simple structure and robustness, the traditional proportional-integral-derivative (PID) controller is commonly used in the field of industrial automation and process control, but it does not function well with nonlinear systems, time-delayed linear systems and time-varying systems. A new type of PID controller based on artificial neural networks and evolutionary algorithms is presented in this paper. An powerful instrument for a highly nonlinear system is the Artificial Neural Network. The interest in the study of the nonlinear system has increased through the implementation of a high-speed computer system,. In complex systems such as robotics and process control systems, the Neuro Control Algorithm is often applied. Systems of process management is also nonlinear and hard to control consistently.. This paper presents a comprehensive analysis in Which is offline trained by a multilayered feed forward back propagation neural network to act as a process control system controller, That is to say, a temperature control device without prior knowledge of its dynamics. Via the implementation of a range of input vectors to the neural network, the inverse dynamics model is developed. Based on these input vectors, the output of the neural network It is being studied by explicitly configuring it to monitor the operation. In this paper, based on set-point adjustment, impact of disturbances in load and variable dead time, compassion between the PID controller and ANN is conducted. The outcome shows that ANN outperforms the controller of the PID.",https://ieeexplore.ieee.org/document/9464484/,2021 IEEE 1st International Maghreb Meeting of the Conference on Sciences and Techniques of Automatic Control and Computer Engineering MI-STA,25-27 May 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICIT.2002.1189341,Computer based robot training in a virtual environment,IEEE,Conferences,"As more market segments are welcoming automation, the robotic field continues to expand. With the accepted breadth of viable industrial robotic applications increasing, the need for flexible robotic training also grows. In the area of simulation and offline programming there have been innovative developments to Computer Aided Robotics (CAR) Systems. New and notable releases have been introduced to the public, especially among the small, affordable, and easy to use systems. These CAR-Systems are mainly aimed at system integrators in general industry business fields to whom the complex, powerful software tools used by the automotive industry (and its suppliers) are oversized. In general, CAR-Systems are used to design robot cells and to create the offline programs necessary to reduce start-up time and to achieve a considerable degree of planning reliability. Another potential yet to be fully considered, is the use of such CAR-Systems as an inexpensive and user-friendly tool for robotics training. This paper will show the educational potential and possibility inherent in simulation and introduce a successful example of this new method of training. Finally, this presentation should be seen as an attempt to outline novel methods for future education in an industrial environment characterized by the increased occurrence and implementation of the virtual factory.",https://ieeexplore.ieee.org/document/1189341/,"2002 IEEE International Conference on Industrial Technology, 2002. IEEE ICIT '02.",11-14 Dec. 2002,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CEC.2000.870793,Cultural algorithms: concepts and experiments,IEEE,Conferences,"Evolutionary computation is a generic name given to the resolution of computational problems that are planned and implemented based on models of the evolutionary process. Most of the evolutionary algorithms that have been proposed follow biological paradigms and the concepts of natural selection, mutation and reproduction. There are, however, other paradigms which may be adopted in the creation of evolutionary algorithms. Several problems involving unstructured environments may be addressed from the point of view of cultural paradigms, which offer plenty of categories of models where one does not know all possible solutions to a problem - a very common situation in real life. This work applies the computational properties of cultural technology to the solution of a specific problem, adapted from the robotics literature. A test environment denoted the ""Cultural Algorithms Simulator"" was developed to allow anyone to learn more about the rather unconventional characteristics of a cultural technology.",https://ieeexplore.ieee.org/document/870793/,Proceedings of the 2000 Congress on Evolutionary Computation. CEC00 (Cat. No.00TH8512),16-19 July 2000,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICECS46596.2019.8964645,Data-Driven Video Grasping Classification for Low-Power Embedded System,IEEE,Conferences,"Video-based hand grasp analysis can support both robotics and prosthetics. Indeed, computational aspects represent a major issue, as hand grasp analysis is expected to support grasping systems that are hosted on low-power embedded systems. This paper proposes a framework for video-based grasping classification that is designed for implementation on resource-constrained devices. The framework adopts a fully data-driven strategy and relies on deep learning to deal with advanced analysis of video signals. Nonetheless, the overall design takes advantage of CNN architectures that can cope with the constraints imposed by embedded systems. The experimental session involved a real-world dataset containing daily life activities collected using egocentric perspective. In addition, the complete inference system is implemented on a NVIDIA Jetson-TX2 obtaining real time performances. The results confirm that the proposed system can suitably balance the trade off between accuracy and computational costs.",https://ieeexplore.ieee.org/document/8964645/,"2019 26th IEEE International Conference on Electronics, Circuits and Systems (ICECS)",27-29 Nov. 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AERO.2018.8396547,Data-driven quality prognostics for automated riveting processes,IEEE,Conferences,"Technologies based in robotics and automatics are reshaping the aerospace industry. Aircraft manufacturers and top-tier suppliers now rely on robotics to perform most of its operational tasks. Over the years, a succession of implemented mobile robots has been developed with the mission of automating important industrial processes such as welding, material handling or assembly procedures. However, despite the progress achieved, a major limitation is that the process still requires human supervision and an extensive quality control process. An approach to address this limitation is to integrate machine learning methods within the quality control process. The idea is to develop algorithms that can direct manufacturing experts towards critical areas requiring human supervision and quality control. In this paper we present an application of machine learning to a concrete industrial problem involving the quality control of a riveting machine. The proposal consists of an intelligent predictive model that can be integrated within the existing real time sensing and pre-processing sub-systems at the equipment level. The framework makes use of several data-driven techniques for pre-processing and feature engineering, combined with the most accurate algorithms, validated through k-folds cross validation technique which also estimates prediction errors. The model is able to classify the manufacturing process of the machine as nominal or anomalous according to a real-world data set of design requirements and operational data. Several machine learning algorithms are compared such as linear regression, nearest neighbor, support vector machines, decision trees, random forests and extreme gradient boost. Results obtained from the case study suggest that the proposed model produces accurate predictions which meet industrial standards.",https://ieeexplore.ieee.org/document/8396547/,2018 IEEE Aerospace Conference,3-10 March 2018,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/MSM49833.2020.9201666,Deep Learning-based Algorithm for Mobile Robot Control in Textureless Environment,IEEE,Conferences,"For the implementation of stereo image-based visual servoing algorithm in the eye-in-hand robotics applications, one of the main concerns is the accurate point feature detection and matching algorithm. Since the visual servoing is carried out in the textureless environment, the feature detection process is even more challenging. To fulfill the requirement of a robust and reliable point feature detection process, in this paper we present the novel deep learning-based algorithm. The approach based on convolutional neural networks and algorithm for detection of manufacturing entities is proposed and detected regions of interest are utilized for the improvement of the point feature detection algorithm. The proposed algorithm is experimentally evaluated in real-world settings by using wheeled nonholonomic mobile robot RAICO equipped with stereo vision system. The experimental results show the improvement of 58% in the accuracy of matched point features in the images obtained during the visual servoing process. Moreover, with the implementation of the proposed deep learning-based approach, the number of successful experimental runs has increased by 80%.",https://ieeexplore.ieee.org/document/9201666/,2020 International Conference Mechatronic Systems and Materials (MSM),1-3 July 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA48506.2021.9561145,Deep Reinforcement Learning Framework for Underwater Locomotion of Soft Robot,IEEE,Conferences,"Soft robotics is an emerging technology with excellent application prospects. However, due to the inherent compliance of the materials used to build soft robots, it is extremely complicated to control soft robots accurately. In this paper, we introduce a data-based control framework for solving the soft robot underwater locomotion problem using deep reinforcement learning (DRL). We first built a soft robot that can swim based on the dielectric elastomer actuator (DEA). We then modeled it in a simulation for the purpose of training the neural network and tested the performance of the control framework through real experiments on the robot. The framework includes the following: a simulation method for the soft robot that can be used to collect data for training the neural network, the neural network controller of the swimming robot trained in the simulation environment, and the computer vision method to collect the observation space from the real robot using a camera. We confirmed the effectiveness of the learning method for the soft swimming robot in the simulation environment by allowing the robot to learn how to move from a random initial state to a specific direction. After obtaining the trained neural network through the simulation, we deployed it on the real robot and tested the performance of the control framework. The soft robot successfully achieved the goal of moving in a straight line in disturbed water. The experimental results suggest the potential of using deep reinforcement learning to improve the locomotion ability of mobile soft robots.",https://ieeexplore.ieee.org/document/9561145/,2021 IEEE International Conference on Robotics and Automation (ICRA),30 May-5 June 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IV48863.2021.9575616,Deep Reinforcement Learning based control algorithms: Training and validation using the ROS Framework in CARLA Simulator for Self-Driving applications,IEEE,Conferences,"This paper presents a Deep Reinforcement Learning (DRL) framework adapted and trained for Autonomous Vehicles (AVs) purposes. To do that, we propose a novel software architecture for training and validating DRL based control algorithms that exploits the concepts of standard communication in robotics using the Robot Operating System (ROS), the Docker approach to provide the system with portability, isolation and flexibility, and CARLA (CAR Learning to Act) as our hyper-realistic open-source simulation platform. First, the algorithm is introduced in the context of Self-Driving and DRL tasks. Second, we highlight the steps to merge the proposed algorithm with ROS, Docker and the CARLA simulator, as well as how the training stage is carried out to generate our own model, specifically designed for the AV paradigm. Finally, regarding our proposed validation architecture, the paper compares the trained model with other state-of-the-art traditional control approaches, demonstrating the full strength of our DL based control algorithm, as a preliminary stage before implementing it in our real-world autonomous electric car.",https://ieeexplore.ieee.org/document/9575616/,2021 IEEE Intelligent Vehicles Symposium (IV),11-17 July 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.23919/SoftCOM50211.2020.9238313,Deep Semantic Image Segmentation for UAV-UGV Cooperative Path Planning: A Car Park Use Case,IEEE,Conferences,"Navigation of Unmanned Ground Vehicles (UGV) in unknown environments is an active area of research for mobile robotics. A main hindering factor for UGV navigation is the limited range of the on-board sensors that process only restricted areas of the environment at a time. In addition, most existing approaches process sensor information under the assumption of a static environment. This restrains the exploration capability of the UGV especially in time-critical applications such as search and rescue. The cooperation with an Unmanned Aerial Vehicle (UAV) can provide the UGV with an extended perspective of the environment which enables a better-suited path planning solution that can be adjusted on demand. In this work, we propose a UAV-UGV cooperative path planning approach for dynamic environments by performing semantic segmentation on images acquired from the UAV’s view via a deep neural network. The approach is evaluated in a car park scenario, with the goal of providing a path plan to an empty parking space for a ground-based vehicle. The experiments were performed on a created dataset of real-world car park images located in Croatia and Germany, in addition to images from a simulated environment. The segmentation results demonstrate the viability of the proposed approach in producing maps of the dynamic environment on demand and accordingly generating path plans for ground-based vehicles.",https://ieeexplore.ieee.org/document/9238313/,"2020 International Conference on Software, Telecommunications and Computer Networks (SoftCOM)",17-19 Sept. 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCSP.2017.8286790,Design framework for general purpose object recognition on a robotic platform,IEEE,Conferences,"The advancement in the broader field of Computer Vision is consequential, through past few decades. Therefore, a considerable improvement in object detection and tagging using convolutional neural networks has given way to accurate yet complex methods, which can identify objects in real-time. However, the growth in the area of implementing the algorithms on low powered portable devices has been relatively slow. This paper aims to converge the fields of computer vision and robotics, focusing on implementation of image description applications on an embedded system platform. We aim to integrate Neural Network powered object recognition system ‘YOLO v2’ with a robotic platform to explore the potential applications in the advancing domain of service and personal robotics.",https://ieeexplore.ieee.org/document/8286790/,2017 International Conference on Communication and Signal Processing (ICCSP),6-8 April 2017,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CNNA.2000.876849,Design of a dedicated CNN chip for autonomous robot navigation,IEEE,Conferences,"Obstacle avoidance is the main issue in autonomous robotics. It requires a three-dimensional effective environment sensing in real time. Among the others, the stereo vision approach to environmental information extraction seems to be very appealing, even if it leads an extremely high computational cost. However, a high performance implementation of this algorithm on a cellular neural network is able to overcome these difficulties. In the paper, the design of a CNN chip well suited for this algorithm is presented. This chip, performing a real time processing of the stereo vision data, will improve the cruising speed of a robotic platform.",https://ieeexplore.ieee.org/document/876849/,Proceedings of the 2000 6th IEEE International Workshop on Cellular Neural Networks and their Applications (CNNA 2000) (Cat. No.00TH8509),25-25 May 2000,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SECON.2008.4494306,Design of an integrated environment for operation and control of robotic arms (non-reviewed),IEEE,Conferences,"As more advanced control algorithms are becoming available for the control of robotic arms, traditional fixed controller boards and associated code generators are becoming less convenient way to test such control algorithms in real-time. The process of using such boards is complex, time consuming, and inflexible. In this work, an integrated hardware-software environment was developed and presented where researchers can simply use any Matlab/Simulink basic function block and/or toolbox, such as fuzzy logic or neural network, to design, implement, and test different controller algorithms in real-time for robotic arm operations. The hardware includes a computer, the dSPACE-ds1103 digital processing board, an amplifier board, and the Zebra-ZERO robotics arm as a test-bed. Also, Matlab GUI, m-file, Matlab/Simulink blocks, and dSPACE interface functions are combined together to form the software environment. Control algorithms can be designed in the Matlab/Simulink then converted to c-code and download to the dSPACE processing board. The Matlab m-file are used to code the arm inverse kinematics model and the path planning to calculate the joint angles then send them to the dSPACE processing board using the dSPACE interface functions. Finally, the dSPACE processing board generates physical signal to control the robot arm in real-time. The proposed hardware-software components are developed and integrated together, and several control algorithms can be tested on it. The development steps and some of the real-time testing results conducted on the hardware are explained next in this extended abstract.",https://ieeexplore.ieee.org/document/4494306/,IEEE SoutheastCon 2008,3-6 April 2008,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CONIELECOMP.2017.7891823,Detecting falling people by autonomous service robots: A ROS module integration approach,IEEE,Conferences,"In this paper is presented the integration of diverse modules for people fallen detection by a mobile service robot. This integration has been achieved in the middleware ROS (Robotics Operation System). The proposed implementation are arranged over an modular architecture of three layers: Hardware, Processing and Decision. The modules implemented are on the processing layer. The first module uses an RGB-D camera to detect and track a person in the environment. This module calculate features to detect the fallen pose. In the second module, a PID controller in a pan/tilt unit is used, in order to track the person with a minimum error and soft movement. For this purpose the centroid of the person is located at the center of the plane image. The main characteristics in our architecture are: 1) Segmentation in depth is used, because 3D information is required for detecting the fallen pose; 2) The parameters of PID control are tuned using a manual method and a genetic algorithm, to compare and improve the performance of the tracking person module. Once the PID controller was optimized, the architecture to follow the person and detect the fallen pose, is probed in real time.",https://ieeexplore.ieee.org/document/7891823/,"2017 International Conference on Electronics, Communications and Computers (CONIELECOMP)",22-24 Feb. 2017,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCSPN46366.2019.9150190,Developing A Framework for A Tactile Internet Enabled Robot Assisted Real-Time Interactive Medical System,IEEE,Conferences,"In this paper we outline a high-level framework and architecture for a robotic assisted real time interactive medical system for use in developing countries to help cure the acute shortage of qualified skill medical personnel in the health sector in of an internet of skills domain. We explore the application of new and innovative advancements in technological areas such as AI, 5G mobile networks, the tactile internet networks, robotics and haptic technology to aid in the digital transfer of medical expertise over a wide geographical area. We describe and propose technical specifications of such systems and review existing literature and current technologies in these areas. We interrogate the potential benefits and challenges facing the deployment of these technologies.",https://ieeexplore.ieee.org/document/9150190/,"2019 International Conference on Communications, Signal Processing and Networks (ICCSPN)",29-31 May 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/MHS.2000.903293,Developing Khepera robot applications in a Webots environment,IEEE,Conferences,"Khepera is a high performance mini-robot. Its compact power allows an efficient experimentation using a real robot and applying the basic simulation tools. Webots is a high quality Khepera simulator used in the fields of autonomous systems, intelligent robotics, evolutionary robotics, machine learning, computer vision, and artificial intelligence. The simulation program can be transferred to the real robots easily. The aim of this article is to support the development of Khepera applications in the Webots environment. Starting from the introduction of Khepera robot and its development methodologies, the paper presents and analyses an application example of Khepera robot in the Webots environment. Finally, current applications and future research directions are presented.",https://ieeexplore.ieee.org/document/903293/,MHS2000. Proceedings of 2000 International Symposium on Micromechatronics and Human Science (Cat. No.00TH8530),22-25 Oct. 2000,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/INTERCON.2019.8853573,Development of a hand pose recognition system on an embedded computer using Artificial Intelligence,IEEE,Conferences,"The recognition of hand gestures is a very interesting research topic due to the growing demand in recent years in robotics, virtual reality, autonomous driving systems, human-machine interfaces and in other new technologies. Despite several approaches for a robust recognition system, gesture recognition based on visual perception has many advantages over devices such as sensors, or electronic gloves. This paper describes the implementation of a visual-based recognition system on a embedded computer for 10 hand poses recognition. Hand detection is achieved using a tracking algorithm and classification by a light convolutional neural network. Results show an accuracy of 94.50%, a low power consumption and a near real-time response. Thereby, the proposed system could be applied in a large range of applications, from robotics to entertainment.",https://ieeexplore.ieee.org/document/8853573/,"2019 IEEE XXVI International Conference on Electronics, Electrical Engineering and Computing (INTERCON)",12-14 Aug. 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/MetroAgriFor52389.2021.9628544,Digital Technologies and Automation in Livestock Production Systems: a Digital Footprint from Multisource Data,IEEE,Conferences,"In the last years, dairy production is changing fast towards smart livestock systems, driven by the rapid pace of technological advancements as Internet of Things, big data, machine learning, augmented reality and robotics. These technologies are pushing widespread collection, implementation, transmission and use of digitized information in livestock farms. Currently, this an uncontrolled process that poses questions on sustainability of the virtual environment where such processes take place. The aim of the present paper is to introduce a preliminary digitization footprint approach, which parameterizes the amount of digital information so as to quantify these data in terms of volumes invested for data storage, processing or transfer.",https://ieeexplore.ieee.org/document/9628544/,2021 IEEE International Workshop on Metrology for Agriculture and Forestry (MetroAgriFor),3-5 Nov. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICVES.2009.5400189,Digital implementation of fuzzy logic controller for wide range speed control of brushless DC motor,IEEE,Conferences,"The brushless DC motors find wide applications such as in battery operated vehicles, wheel chairs, automotive fuel pumps, robotics, machine tools, aerospace and in many industrial applications due to their superior electrical and mechanical characteristics and its capability to operate in hazardous environment. Conventional controllers fail to yield desired performance in BLDC motor control systems due to the non-linearity arising out of variation in the system parameters and change in load. The main focus is now on the application of artificial intelligent techniques such as fuzzy logic to solve this problem. Another great challenge is to reduce the size and cost of the drive system without compromising the performance. In this paper, the design and digital implementation of fuzzy logic controller using a versatile ADUC812 microcontroller, and low-cost, compact, superior performance components are used in order to reduce the cost and size of the drive system. The experimental results are presented to prove the flexibility of the control scheme in real time.",https://ieeexplore.ieee.org/document/5400189/,2009 IEEE International Conference on Vehicular Electronics and Safety (ICVES),11-12 Nov. 2009,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CRV50864.2020.00024,Domain Generalization via Optical Flow: Training a CNN in a Low-Quality Simulation to Detect Obstacles in the Real World,IEEE,Conferences,"Many applications in robotics and autonomous systems benefit from machine learning applied to computer vision, but often the acquisition and preparation of data for training is complex and time-consuming. Simulation can significantly reduce the effort and potential risk of data collection, thereby allowing faster prototyping. However, the ability of a data-driven system to generalize from simulated data to the real world is far from obvious and often leading to inconsistent real-world results. This paper demonstrates that some properties of optical flow can be exploited to address this generalization problem. In this work, we train a neural network to detect collisions with simulated optical flow data. Our network, FlowDroNet, is able to correctly predict up to 89 percent of the collisions of a realworld dataset and easily achieves a higher detection accuracy when compared to a network trained on a similar dataset of realworld collisions. We release our code, models and a real-world dataset for collision avoidance as open-source. We also explore the relationship between the complexity of the input information and the ability to generalize to unseen environments, and show that in some situations, optical flow is an interesting tool to bridge the reality gap.",https://ieeexplore.ieee.org/document/9108671/,2020 17th Conference on Computer and Robot Vision (CRV),13-15 May 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SCCC.2001.972633,Domain-dependent option policies in autonomous robot learning,IEEE,Conferences,"In control-related applications such as robotics, determination of optimal solutions is made very difficult for many reasons. Among these stands the difficulty in finding out an appropriate model of the domain, as defined by the control agent (robot), environment where it acts and their interaction. Reinforcement learning is a theory which defines a collection of algorithms for determination of control actions under model-free assumptions, which allows control agents to learn optimal actions in an autonomous way. In reinforcement learning, a cost functional to be optimised is determined in advance. The agent then learns how to perform this optimisation via trial and error on its environment. A trial corresponds to execution of actions chosen by the agent, and the error is the immediate result (a real-valued reinforcement) of this action. In the work reported, we consider trials by a learning robotic agent which are not based on low level actions, but instead on sequences of actions (options or macro-operators). We analysed the performance both in terms of learning speed and quality of learned control-for options that correspond to mappings from states to action policies (O/sub /spl Pi// options). Experimental results show that careful (domain-dependent) selection of options (via methods such as discretised potential fields) produce much faster learning for option-based robots when compared to their action-based counterparts. Of critical importance, however, is the option mapping in regions of the state space where the options are not assumed to be necessary: as performance of reinforcement learning algorithms is strongly dependent on sufficient exploration of the state space, even in such regions a careful, ad-hoc selection of actions is of foremost importance.",https://ieeexplore.ieee.org/document/972633/,SCCC 2001. 21st International Conference of the Chilean Computer Science Society,9-9 Nov. 2001,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/FCCM51124.2021.00046,Edge Accelerator for Lifelong Deep Learning using Streaming Linear Discriminant Analysis,IEEE,Conferences,"Lifelong deep learning models are expected to continuously adapt and acquire new knowledge in dynamic environments. This capability is essential for numerous vision tasks in robotics and drones, and the models must be deployed on the edge to achieve real-time performance. We propose a FPGA accelerator of a streaming classifier for lifelong deep learning, which is based on streaming linear discriminant analysis (SLDA). When combined with a frozen Convolutional Neural Network (CNN) model, the proposed system is capable of class incremental lifelong learning for object classification.",https://ieeexplore.ieee.org/document/9444094/,2021 IEEE 29th Annual International Symposium on Field-Programmable Custom Computing Machines (FCCM),9-12 May 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/WiSPNET51692.2021.9419475,Emotion based Media Playback System using PPG Signal,IEEE,Conferences,"The study involved identifying human emotions and integrates the identified emotion with the music system. The idea is to develop a complete product to utilize the detected emotion in a real-time application and also to achieve more accuracy and less memory. Human emotions are identified using physiological signals such as electrocardiography, electromyography, photoplethysmography, respiration, skin temperature, etc. Obtaining photoplethysmography (PPG) from the sensor is a simple, cost-effective, and non-invasive method. PPG sensors are capable of providing accurate heart-rate (HR) by detecting the variations in the blood flow. Signals are acquired using wearable technology from a personal whereas not compromising comfort and privacy. The attributes of Heart Rate Variability are analyzed to describe emotions, namely happy, calm, unhappy (sad), and fear using Machine learning technology. We deployed this recognized emotion to automate the music system associated with its emotion. To bring this, we built an Android app to communicate with the smart wearable utilized. Totally 150 members from both genders have participated. The accuracy of 91.81% is achieved. This emotion recognition system can be used in various fields like robotics, medicine, virtual reality, and gaming, advertising, education, automotive working conditions and safety, home appliances.",https://ieeexplore.ieee.org/document/9419475/,"2021 Sixth International Conference on Wireless Communications, Signal Processing and Networking (WiSPNET)",25-27 March 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ROMAN.2013.6628436,Emotional evaluation of bandit problems,IEEE,Conferences,"In this paper, we discuss an approach to evaluate decisions made during a multi-armed bandit learning experiment. Usually, the results of machine learning algorithms applied on multi-armed bandit scenarios are rated in terms of earned reward and optimal decisions taken. These criteria are valuable for objective comparison in finite experiments. But learning algorithms used in real scenarios, for example in robotics, need to have instantaneous criteria to evaluate their actual decisions taken. To overcome this problem, in our approach each decision updates the Zürich model which emulates the human sense of feeling secure and aroused. Combining these two feelings results in an emotional evaluation of decision policies and could be used to model the emotional state of an intelligent agent.",https://ieeexplore.ieee.org/document/6628436/,2013 IEEE RO-MAN,26-29 Aug. 2013,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA48506.2021.9561889,End-to-End Semi-supervised Learning for Differentiable Particle Filters,IEEE,Conferences,"Recent advances in incorporating neural networks into particle filters provide the desired flexibility to apply particle filters in large-scale real-world applications. The dynamic and measurement models in this framework are learnable through the differentiable implementation of particle filters. Past efforts in optimising such models often require the knowledge of true states which can be expensive to obtain or even unavailable in practice. In this paper, in order to reduce the demand for annotated data, we present an end-to-end learning objective based upon the maximisation of a pseudo-likelihood function which can improve the estimation of states when large portion of true states are unknown. We assess performance of the proposed method in state estimation tasks in robotics with simulated and real-world datasets.",https://ieeexplore.ieee.org/document/9561889/,2021 IEEE International Conference on Robotics and Automation (ICRA),30 May-5 June 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/EAEEIE54893.2022.9820462,Ethics in Engineering Education,IEEE,Conferences,"Although ethics was introduced into the engineering curricula in the 1970s for the first time, it is not yet a standard and integral part of engineering education. As a research discipline, engineering ethics education appeared in the late 1990s and early 2000s.Ethics was introduced to biomedical engineering curricula at the Czech Technical University in Prague already in the 1990s and since then it became an integral part of biomedical engineering study programs. However, until now we do not find ethics, for example, in electrical engineering or information technology study programs. Currently, ethics is intensively discussed in relation to artificial intelligence, robotics, and human-machine interaction.Artificial intelligence and robotics are not purely theoretical research areas anymore. They already entered our daily life. The development of autonomous vehicles opened many questions that were previously touched only on a high theoretical level. Nowadays, artificial intelligence and robotics constitute frequently independent study programs and it is highly desirable to introduce the ethical and social issues directly into the curricula. We are well aware of the fact that the curricula have a limited number of courses, should be studied in a limited time and it is impossible to think that students become experts in all studied fields. The conclusion we agreed on is that ethics should become integrated into engineering courses that illustrate the necessity of ethical aspects in real-life applications and examples of practical solutions.In the paper, we present a brief description of our approach to the integration of ethical questions in specific courses in biomedical engineering and biomedical informatics study programs.",https://ieeexplore.ieee.org/document/9820462/,2022 31st Annual Conference of the European Association for Education in Electrical and Information Engineering (EAEEIE),29 June-1 July 2022,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/RVSP.2013.73,Evolution of Neural Controllers for Simulated and Real Quadruped Robots,IEEE,Conferences,"Evolutionary robotics is an approach that employs evolutionary computation to develop a controller for an autonomous robotic system. Evolutionary computing usually operates depending on a population of candidate controllers, initially selected from a random distribution. The population is iteratively modified according to the fitness function. In this paper, an automatic control system is designed for quadruped robots using an Evolutionary Neural Network (ENN) and the performance is measured in terms of the distance travelled by the robot from its origin. The evolved neural controllers are analyzed in the simulation environment and the results are implemented in a real quadruped robot. The comparison between the simulated and real robot shows the performance of the quadruped robot in terms of number of iterations over the distance covered in the desired direction. The developed ENN helps the robot to choose the best possible solution to achieve the maximum distance.",https://ieeexplore.ieee.org/document/6830033/,"2013 Second International Conference on Robot, Vision and Signal Processing",10-12 Dec. 2013,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ROMAN.2003.1251857,Exploiting value statistics for similar continuing tasks,IEEE,Conferences,"In this paper, we try to consider interaction design for adaptation from the viewpoint of transfer of knowledge. Advancements in robotics are amazing, and their interaction processes with outside world (including human) are getting to be longer in time scale. We investigate these matters in an abstract agent that faces multiple learning tasks within its lifetime, transferring past learning experiences to improve its performance. We formulize the multitask reinforcement learning problem at first, and then we present two ways of incorporating past learning experiences into the agent's learning algorithm.",https://ieeexplore.ieee.org/document/1251857/,"The 12th IEEE International Workshop on Robot and Human Interactive Communication, 2003. Proceedings. ROMAN 2003.",2-2 Nov. 2003,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SIEDS49339.2020.9106581,"Explorer51 – Indoor Mapping, Discovery, and Navigation for an Autonomous Mobile Robot",IEEE,Conferences,"The nexus of robotics, autonomous systems, and artificial intelligence (AI) has the potential to change the nature of human guided exploration of indoor and outdoor spaces. Such autonomous mobile robots can be incorporated into a variety of applications, ranging from logistics and maintenance, to intelligence gathering, surveillance, and reconnaissance (ISR). One such example is that of a tele-operator using the robot to generate a map of the inside of a building while discovering and tagging the objects of interest. During this process, the tele-operator can also assign an area for the robot to navigate autonomously or return to a previously marked area/object of interest. Search and rescue and ISR abilities could be immensely improved with such capabilities. The goal of this research is to prototype and demonstrate the above autonomous capabilities in a mobile ground robot called Explorer51. Objectives include: (i) enabling an operator to drive the robot non-line of sight to explore a space by incorporating a first-person view (FPV) system to stream data from the robot to the base station; (ii) implementing automatic collision avoidance to prevent the operator from running the robot into obstacles; (iii) creating and saving 2D and 3D maps of the space in real time by using a 2D laser scanner, tracking, and depth/RGB cameras; (iv) locating and tagging objects of interest as waypoints within the map; (v) autonomously navigate within the map to reach a chosen waypoint.To accomplish these goals, we are using the AION Robotics R1 Unmanned Ground Vehicle (UGV) rover as the platform for Explorer51 to demonstrate the autonomous features. The rover runs the Robot Operating System (ROS) onboard an NVIDIA Jetson TX2 board, connected to a Pixhawk controller. Sensors include a 2D scanning LiDAR, depth camera, tracking camera, and an IMU. Using existing ROS packages such as Cartographer and TEB planner, we plan to implement ROS nodes for accomplishing these tasks. We plan to extend the mapping ability of the rover using Visual Inertial Odometry (VIO) using the cameras. In addition, we will explore the implementation of additional features such as autonomous target identification, waypoint marking, collision avoidance, and iterative trajectory optimization. The project will culminate in a series of demonstrations to showcase the autonomous navigation, and tele-operation abilities of the robot. Success will be evaluated based on ease of use by the tele-operator, collision avoidance ability, autonomous waypoint navigation accuracy, and robust map creation at high driving speeds.",https://ieeexplore.ieee.org/document/9106581/,2020 Systems and Information Engineering Design Symposium (SIEDS),24-24 April 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICUAS48674.2020.9214045,Extensions of the open-source framework Aerostack 3.0 for the development of more interactive flights between UAVs,IEEE,Conferences,"The basis for properly verified R&D works is to provide reliable prototyping tools at three most important stages: computer simulation, laboratory tests and real-world experiments. In the laboratory-limited conditions, particular importance is attributed to the first two stages, especially in the context of the safety development of autonomous flights of unmanned aerial vehicle (UAV) groups in various missions. The open-source framework Aerostack support those needs and its effectiveness has been proven in the International Micro Air Vehicle Indoor Competitions (IMAV 2013, 2016, 2017) and Mohammed Bin Zayed International Robotics Challenge (MBZIRC 2020). In the paper, the exemplary functionalities for the new version of Aerostack Version 3.0 Distribution Sirocco (Aerial robotics framework for the industry), extended additionally with a library of new behaviors, are presented. The mission of UAVs can be developed fast and effectively in order to conduct test flights with real drones in lab, before one will decide to fly autonomously outdoor. The representative results obtained for low-cost AR.Drone 2.0 UAV models in two missions, are presented. The first mission is autonomous patrolling the area by a pair of UAVs, the second - intercepting the intruder in guarded area by the guard UAV.",https://ieeexplore.ieee.org/document/9214045/,2020 International Conference on Unmanned Aircraft Systems (ICUAS),1-4 Sept. 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICECS49266.2020.9294790,FPGA Implementation of Simplified Spiking Neural Network,IEEE,Conferences,"Spiking Neural Networks (SNN) are third generation Artificial Neural Networks (ANN), which are close to the biological neural system. In recent years SNN has become popular in the area of robotics and embedded applications, therefore, it has become imperative to explore its real-time and energy-efficient implementations. SNNs are more powerful than their predecessors because of their ability to encode temporal information and to use biologically plausible plasticity rules. In this paper, a simpler and computationally efficient SNN model is described. The proposed model is implemented and validated utilizing a Xilinx Virtex 6 FPGA. It is demonstrated that the proposed model analyzes a fully connected network consisting of 800 neurons and 12,544 synapses in real-time.",https://ieeexplore.ieee.org/document/9294790/,"2020 27th IEEE International Conference on Electronics, Circuits and Systems (ICECS)",23-25 Nov. 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SACI.2007.375494,FPGA Parallel Implementation of CMAC Type Neural Network with on Chip Learning,IEEE,Conferences,"The hardware implementation of neural networks is a new step in the evolution and use of neural networks in practical applications. The CMAC cerebellar model articulation controller is intended especially for hardware implementation, and this type of network is used successfully in the areas of robotics and control, where the real time capabilities of the network are of particular importance. The implementation of neural networks on FPGA's has several benefits, with emphasis on parallelism and the real time capabilities. This paper discusses the hardware implementation of the CMAC type neural network, the architecture and parameters and the functional modules of the hardware implemented neuro-processor.",https://ieeexplore.ieee.org/document/4262496/,2007 4th International Symposium on Applied Computational Intelligence and Informatics,17 Yearly-18 May 2007,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/WiSPNET51692.2021.9419438,Faster Training of Edge-attention Aided 6D Pose Estimation Model using Transfer Learning and Small Customized Dataset,IEEE,Conferences,"Computer Vision is the field of machine learning that deals with computers gaining knowledge from digital images/videos and performing tasks that human vision is capable of doing. It is widely used in the field of robotics for designing guidance systems where objects in the robot's field of view are identified and located. This research work is an application-specific project enabling a half-humanoid to find the 6D pose and bounding boxes of its hand and other objects within its field of view. We add an edge prediction head to the NOCS (Normalised Object Coordinate Space) model, which predicts the edges of each object from the predicted instance maps. An additional edge-agreement-loss found from the predicted edges is added to the total loss. This increases the attention to the edges and improves the accuracy of prediction of the instance masks. This edge-attention aided model is initialized with pre-trained weights of CAMERA and REAL dataset using transfer learning. The backbone layers of the model are frozen and the head layers alone are trained using a synthetic dataset (HAND dataset) we created using a software called blender. The model gives promising results when tested with objects kept in varying lighting conditions and at different distances from the camera. The use of transfer learning in models as large as the NOCS model allows us to train the model for a new class by only training the top few layers with a significantly small dataset.",https://ieeexplore.ieee.org/document/9419438/,"2021 Sixth International Conference on Wireless Communications, Signal Processing and Networking (WiSPNET)",25-27 March 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SSCI.2017.8280891,Fault diagnosis in robot swarms: An adaptive online behaviour characterisation approach,IEEE,Conferences,"The need for an active approach to fault tolerance in swarm robotics systems is well established. This will necessarily include an approach to fault diagnosis if robot swarms are to retain long-term autonomy. This paper proposes a novel method for fault diagnosis, based around behavioural feature vectors, that incorporates real-time learning and memory. Initial results are encouraging, and show that an unsupervised learning approach is able to diagnose common electro-mechanical fault types, and arrive at an appropriate recovery option in the majority of the cases tested.",https://ieeexplore.ieee.org/document/8280891/,2017 IEEE Symposium Series on Computational Intelligence (SSCI),27 Nov.-1 Dec. 2017,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SBR-LARS.2012.57,Fixed-Point Neural Network Ensembles for Visual Navigation,IEEE,Conferences,"Visual navigation is an important research field in robotics because of the low cost and the high performance that is usually achieved by visual navigation systems. Pixel classification as a road pixel or a non-road pixel is a task that can be well performed by Artificial Neural Networks. In the case of real-time instances of the image classification problem, as when applied to autonomous vehicles navigation, it is interesting to achieve the best possible execution time. Hardware implementations of these systems can achieve fast execution times but the floating-point implementation of Neural Networks are commonly complex and resource intensive. This work presents the implementation and analysis of a fixed-point Neural Network Ensemble for image classification. The system is composed by six fixed-point Neural Networks verified with cross-validation technique, using some proposed voting schemes and analyzed considering the execution time, precision, memory consumption and accuracy for hardware implementation. The results show that the fixed-point implementation is faster, consumes less memory and has an acceptable precision compared to the floating-point implementation. This fact suggests that the fixed point implementation should be used in systems that need a fast execution time. Some questions about ensembles and voting have to be reviewed for fixed-point Neural Network Ensembles.",https://ieeexplore.ieee.org/document/6363361/,2012 Brazilian Robotics Symposium and Latin American Robotics Symposium,16-19 Oct. 2012,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.23919/OCEANS44145.2021.9705773,Fusion-UWnet: Multi-channel Fusion-based Deep CNN for Underwater Image Enhancement,IEEE,Conferences,"Underwater image enhancement has been considered as one of the prime research areas due to its massive significance in underwater surveillance and the development of underwater autonomous robotics. Deep learning methods have been used for image processing, where heavy models like GANs and very deep CNNs are being deployed for the task. Due to the bulky nature of the models, they consume significant memory and are numerically expensive in computational tasks, making them inefficient to some degree in underwater exploration tasks. These models are primarily trained over synthetically generated data which makes them less correlative for real-world tasks. This paper proposes a deep network architecture that uses a series of convolutional blocks to fuse significant complementary features of two separate enhanced versions of the input image along with the input one. Further, a combination of perceptual and structural similarity losses is used to find out the error. We have also benchmarked our model on three underwater datasets, highlighting the generalizing capabilities over a mix of real-world and synthetic data.",https://ieeexplore.ieee.org/document/9705773/,OCEANS 2021: San Diego – Porto,20-23 Sept. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/INDIN.2009.5195905,GPS and sonar based area mapping and navigation by mobile robots,IEEE,Conferences,"In this paper, we have presented a GPS and sonar based area mapping and navigation scheme for a mobile robot. A mapping is achieved between the GPS space and the world coordinates of the mobile robot which enables us to generate direct motion commands for it. This mapping enables the robot to navigate among different GPS locations within the mapped area. The GPS data is extracted online to get the latitude and longitude information of a particular location. In the training phase, a 2-D axis transformation is used to relate local robot frame with the robot world coordinates and then the actual world coordinates are mapped from the GPS data using a RBFN (radial basis function network) based Neural Network. In the second phase, direct GPS data is used to get the mapping into the world coordinates of mobile robot using the trained network and the motion commands are generated accordingly. The physical placement of sonar devices, their ranging limits and beam opening angles are considered during navigation for possible collision detection and obstacle avoidance. This scheme is successfully implemented in real time with Pioneer mobile robot from ActivMedia Robotics and GPS receiver. The scheme is also tested in the simulation to justify its application in the real world.",https://ieeexplore.ieee.org/document/5195905/,2009 7th IEEE International Conference on Industrial Informatics,23-26 June 2009,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/3ICT.2019.8910276,Ground Operations Management using a Data Governance Dashboard,IEEE,Conferences,"An incident involving the use of chemical, biological, radiological, and nuclear (CBRN) materials might represent a significant challenge for crime scene investigators. The paper presents a full system architecture to assess the hazardous situations resulting from CBRN materials. This issue is crucial since there were a number of incidents that occurred in which forensics people could not reach the location either due to being unreachable or due to harmful emissions. The proposed solution integrates various inputs including data from sensors, video streaming, geo-data along with using Artificial Intelligence (AI) for good decision-making and data analysis. A geo-dashboard was also designed to demonstrate, in real-time, the collected data from several angles and according to various queries. It also monitors the performance in real-time. The topic is not new however the novelty of the proposed solution is the integration of multiple sources of data, applying deep neural nets and projecting the data and data analytics in real-time on a dashboard that displays the analysis and data from different perspectives considering the viewpoint of the individuals who will use that system. The paper also presents how the ROCSAFE multidisciplinary research project addresses the identified scenario. The project combines topics from robotics, sensor technology, analytical and situation awareness software, transforming data into knowledgeable insights to support the decision-making process.",https://ieeexplore.ieee.org/document/8910276/,"2019 International Conference on Innovation and Intelligence for Informatics, Computing, and Technologies (3ICT)",22-23 Sept. 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SII46433.2020.9025951,Gym-Ignition: Reproducible Robotic Simulations for Reinforcement Learning,IEEE,Conferences,"This paper presents Gym-Ignition, a new framework to create reproducible robotic environments for reinforcement learning research. It interfaces with the new generation of Gazebo, part of the Ignition Robotics suite, which provides three main improvements for reinforcement learning applications compared to the alternatives: 1) the modular architecture enables using the simulator as a C++ library, simplifying the interconnection with external software; 2) multiple physics and rendering engines are supported as plugins, simplifying their selection during the execution; 3) the new distributed simulation capability allows simulating complex scenarios while sharing the load on multiple workers and machines. The core of Gym-Ignition is a component that contains the Ignition Gazebo simulator and exposes a simple interface for its configuration and execution. We provide a Python package that allows developers to create robotic environments simulated in Ignition Gazebo. Environments expose the common OpenAI Gym interface, making them compatible out-of-the-box with third-party frameworks containing reinforcement learning algorithms. Simulations can be executed in both headless and GUI mode, the physics engine can run in accelerated mode, and instances can be parallelized. Furthermore, the Gym-Ignition software architecture provides abstraction of the Robot and the Task, making environments agnostic on the specific runtime. This abstraction allows their execution also in a real-time setting on actual robotic platforms, even if driven by different middlewares.",https://ieeexplore.ieee.org/document/9025951/,2020 IEEE/SICE International Symposium on System Integration (SII),12-15 Jan. 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS51168.2021.9636167,HARL-A: Hardware Agnostic Reinforcement Learning Through Adversarial Selection,IEEE,Conferences,"The use of reinforcement learning (RL) has led to huge advancements in the field of robotics. However data scarcity, brittle convergence and the gap between simulation & real world environments, mean that most common RL approaches are subject to over fitting and fail to generalise to unseen environments. Hardware agnostic policies would mitigate this by allowing a single network to operate in a variety of test domains, where dynamics vary due to changes in robotic morphologies or internal parameters. We utilise the idea that learning to adapt a known and successful control policy is easier and more flexible than jointly learning numerous control policies for different morphologies.This paper presents the idea of Hardware Agnostic Reinforcement Learning using Adversarial selection (HARL-A). In this approach training examples are sampled using a novel adversarial loss function. This is designed to self regulate morphologies based on their learning potential. Simply applying our learning potential based loss function to current state-of-the-art already provides ~ 30% improvement in performance. Meanwhile experiments using the full implementation of HARL-A report an average increase of 70% to a standard RL baseline and 55% compared with current state-of-the-art.",https://ieeexplore.ieee.org/document/9636167/,2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),27 Sept.-1 Oct. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ECMR50962.2021.9568815,HATSDF SLAM &#x2013; Hardware-accelerated TSDF SLAM for Reconfigurable SoCs,IEEE,Conferences,"Simultaneous Localization and Mapping (SLAM) is one of the fundamental problems in autonomous robotics. Over the years, many approaches to solve this problem for 6D poses and 3D maps based on LiDAR sensors or depth cameras have been proposed. One of the main drawbacks of the solutions found in the literature is the required computational power and corresponding energy consumption. In this paper, we present an approach for LiDAR-based SLAM that maintains a global truncated signed distance function (TSDF) to represent the map. It is implemented on a System On Chip (SoC) with an integrated FPGA accelerator. The proposed system is able to track the position of a Velodyne VLP-16 LiDAR in real time, while maintaining a global TSDF map that can be used to create a polygonal map of the environment. We show that our implementation delivers competitive results compared to state-of-the-art algorithms while drastically reducing the power consumption compared to classical CPU or GPU-based methods.",https://ieeexplore.ieee.org/document/9568815/,2021 European Conference on Mobile Robots (ECMR),31 Aug.-3 Sept. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISIC.1992.225127,Hierarchical architecture for multi-sensor robot cell operation,IEEE,Conferences,"The authors describe a hierarchical architecture designed to carry out experiments in multisensor integration and sensor-based control in robotics. The hierarchical model is composed of three major levels: a high-level information processing and planning structure at the top, a logic-branching control structure at the intermediate level, and a real-time continuous sensory feedback loop at the bottom level. The two lower control structures are addressed. The principal submodules of the intermediate structure are described, with particular emphasis on communication issues and on the available software mechanisms for configuration and online maintenance of the robot cell. The architecture of the real-time continuous control structure that composes the bottom level is also described. The application of the adaptive self-tuning scheme in controlling position and force, specified in task-space coordinates, is discussed. Practical issues and experimental results are summarized.<>",https://ieeexplore.ieee.org/document/225127/,Proceedings of the 1992 IEEE International Symposium on Intelligent Control,11-13 Aug. 1992,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ROBOT.1999.770002,High-speed navigation using the global dynamic window approach,IEEE,Conferences,"Many applications in mobile robotics require the safe execution of a collision-free motion to a goal position. Planning approaches are well suited for achieving a goal position in known static environments, while real-time obstacle avoidance methods allow reactive motion behavior in dynamic and unknown environments. This paper proposes the global dynamic window approach as a generalization of the dynamic window approach. It combines methods from motion planning and real-time obstacle avoidance to result in a framework that allows robust execution of high-velocity, goal-directed reactive motion for a mobile robot in unknown and dynamic environments. The global dynamic window approach is applicable to nonholonomic and holonomic mobile robots.",https://ieeexplore.ieee.org/document/770002/,Proceedings 1999 IEEE International Conference on Robotics and Automation (Cat. No.99CH36288C),10-15 May 1999,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCICC50026.2020.9450269,Human Capability Augmentation through Cognitive and Autonomous Systems,IEEE,Conferences,"The Covid-19 pandemic reminds us again about our limited knowledge and understanding in the nature including both micro and macro worlds. We have been developing a variety of tools such as automation, robotics, internet, and artificial intelligence (AI), etc. to augment human capability for improved safety, quality, and productivity in work and life, but human lives are still vulnerable over 100 years since the last Spanish Flu in 1918. We are even more vulnerable when the tools we developed (e.g., automation and AI) do not understand human intent or follow human instructions. Recent accidents to the Boeing 737 Max passengers ring the alarm again about the imperative needs of appropriate design concepts and scientific methodologies for developing safety critical cognitive and/or autonomous systems or AI functions and collaborative partnership of human and intelligent systems. With AI and its related technologies reach their bottleneck, it is even more vital to follow scientific and systematic methodology to understand well about capacity and limitation of both human intelligence and machine intelligence so that their strengths can be optimized for a collaborative partnership when dealing with safety critical situations. This talk discusses about the needs for the researchers, designers, developers, and all practitioners who are interested in building and using 21st century human-autonomy symbiosis technologies (Why). It touches the topics of proper analytical methodologies for functional requirements of the intelligent systems, design methodologies, implementation strategies, evaluation approaches, and trusted relationships (How). These aspects will be explained with real-world examples when considering contextual constraints of technology, human capability and limitations, and functionalities that AI and autonomous systems should achieve (When). Audience will gain insights of context-based and interaction-centered design approach for developing a safe, trusted, and collaborative partnership between human and technology by optimizing the interaction between human intelligence and AI. The challenges and potential issues will also be discussed for guiding future research and development activities when augmenting human capabilities with AI, and cognitive and/or autonomous systems.",https://ieeexplore.ieee.org/document/9450269/,2020 IEEE 19th International Conference on Cognitive Informatics & Cognitive Computing (ICCI*CC),26-28 Sept. 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ROBIO.2012.6491182,Human recognition with a hardware-accelerated multi-prototype learning and classification system,IEEE,Conferences,"This paper reports a hardware-accelerated multi-prototype learning and classification system which is suitable for real-time recognition systems. The real-world applicability of robotics or surveillance systems is dependent upon their real-time performance. Hardware based solutions can meet the needs for real-time limited problems; however, hardware-friendly solutions have lacked the flexibility to handle a large range of complex tasks. Software based solutions have been used to tackle complex tasks and allow for greater flexibility but lack the speeds which hardware systems can provide. The developed multi-prototype learning and classification system surmounts these limitations and is applied to the problem of human recognition for demonstrating its capabilities. A fully digital Euclidian distance searching circuit is developed in order to reduce the computational cost within the learning and classification process. The system outperforms other implementations by significantly reducing training times and attains a per sample recognition speed of 1.03 μs.",https://ieeexplore.ieee.org/document/6491182/,2012 IEEE International Conference on Robotics and Biomimetics (ROBIO),11-14 Dec. 2012,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ROMAN.1992.253866,Hybrid architectures for intelligent robotic systems,IEEE,Conferences,"Hybrid architectures, based on combinations of analogic, symbolic, and neural methods, are well suited for real-time applications in advanced robotics. Real-time industrial applications are mainly based on the correction of preplanned programs. So far, the planning and control modules of these kind of applications are often unable to react and/or classify un-expected events. The approach described attempts to integrate the sensor-based analogic method and the neural method into a multiple-level architecture that operates on an analogic world model, so that the action planning can be performed in a smart, reactive way. Given the task, the system builds the world model of the scenario. The reasoning and planning modules act both at the strategic as well as reactive levels, and the activated sensor-based motor strategies handle the sensorial data inputs and drive the robot controller module in the execution of the stream of motor commands. The interaction between the different levels is mainly based on the idea of maintaining and updating in real-time the world model, so that each module can locally operate on specific parts of the whole world model.<>",https://ieeexplore.ieee.org/document/253866/,[1992] Proceedings IEEE International Workshop on Robot and Human Communication, 1992,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICM52667.2021.9664950,Image Inpainting and Classification Agent Training Based on Reinforcement Learning and Generative Models with Attention Mechanism,IEEE,Conferences,"What distinguishes the field of artificial intelligence (AI) from others is to develop fully independent agents that learn optimal behavior, change, and evolve solely through the communication of trial and error with the surrounding environment. Reinforcement learning (RL) can be seen in multiple aspects of Machine Learning (ML), provided the environment, reward, actions, the state will be defined. Agent training in previous years is seen to only relate to robotics, games, and self-driving cars. While trying to divert the focus of researchers from the view of self-driving cars, games, robots, etc. Here, we investigated using reinforcement learning in the aspect of task completion. We deployed our architecture in an inpainting task where the agent generates the distorted or missing image content into an eminent fidelity completed the image by using reinforcement learning to influence the generative model utilized. The Generative Adversary Network (GAN) problem of not being steady and challenging to train was overwhelmed by utilizing latent space representation. The dimension is reduced compared to the distorted or corrupted image in training the GAN. Then reinforcement learning was deployed to pick the correct GAN input to get the image’s latent space representation that is most suitable for the current input of the missing or distorted image region. In this paper, we also learned that the trained agent enhances the accuracy in a classification task of images with missing data. We successfully examined the classification enhancement on images missing 30%, 50%, and 70%.",https://ieeexplore.ieee.org/document/9664950/,2021 International Conference on Microelectronics (ICM),19-22 Dec. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICECCME52200.2021.9590955,Impact of Real-World Market Conditions on Returns of Deep Learning based Trading Strategies,IEEE,Conferences,"Based on recent advancements in natural language processing, computer vision and robotics, a growing number of researchers and traders attempt to predict future asset prices using deep learning techniques. Typically, the goal is to find a profitable and at the same time low-risk trading strategy. However, it is not straightforward to evaluate a found trading strategy. Evaluating solely on historic price data neglects important factors arising in real markets. In this paper, we analyze the impact of real-world market conditions in terms of trading fees, borrow interests, slippage and spreads on trading returns. For that, we propose a deep learning trading bot based on Temporal Convolutional Networks, which is deployed to a real cryptocurrency exchange. We compare the results obtained in the real market with simulated returns and investigate the impact of the different real-world market conditions. Our results show that besides trading fees (which have the biggest impact on returns), factors like slippage and spread also affect the returns of the trading strategy.",https://ieeexplore.ieee.org/document/9590955/,"2021 International Conference on Electrical, Computer, Communications and Mechatronics Engineering (ICECCME)",7-8 Oct. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/DEVLRN.2014.6983001,Incremental training of Restricted Boltzmann Machines using information driven saccades,IEEE,Conferences,"In the context of developmental robotics, a robot has to cope with complex sensorimotor spaces by reducing their dimensionality. In the case of sensor space reduction, classical approaches for pattern recognition use either hardcoded feature detection or supervised learning. We believe supervised learning and hard-coded feature extraction must be extended with unsupervised learning of feature representations. In this paper, we present an approach to learn representations using space-variant images and saccades. The saccades are driven by a measure of quantity of information in the visual scene, emerging from the activations of Restricted Boltzmann Machines (RBMs). The RBM, a generative model, is trained incrementally on locations where the system saccades. Our approach is implemented using real data captured by a NAO robot in indoor conditions.",https://ieeexplore.ieee.org/document/6983001/,4th International Conference on Development and Learning and on Epigenetic Robotics,13-16 Oct. 2014,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CASE48305.2020.9216902,Industrial Robot Grasping with Deep Learning using a Programmable Logic Controller (PLC),IEEE,Conferences,"Universal grasping of a diverse range of previously unseen objects from heaps is a grand challenge in e-commerce order fulfillment, manufacturing, and home service robotics. Recently, deep learning based grasping approaches have demonstrated results that make them increasingly interesting for industrial deployments. This paper explores the problem from an automation systems point-of-view. We develop a robotics grasping system using Dex-Net, which is fully integrated at the controller level. Two neural networks are deployed on a novel industrial AI hardware acceleration module close to a PLC with a power footprint of less than 10 W for the overall system. The software is tightly integrated with the hardware allowing for fast and efficient data processing and real-time communication. The success rate of grasping an object form a bin is up to 95% with more than 350 picks per hour, if object and receptive bins are in close proximity. The system was presented at the Hannover Fair 2019 (world's largest industrial trade fair) and other events, where it performed over 5,000 grasps per event.",https://ieeexplore.ieee.org/document/9216902/,2020 IEEE 16th International Conference on Automation Science and Engineering (CASE),20-21 Aug. 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CADCG.2007.4407908,Intelligent Robotic Peg-in-Hole Insertion Learning Based on Haptic Virtual Environment,IEEE,Conferences,A new approach is explored to transfer human manipulation skills to a robotics system. A skill acquisition algorithm utilizes the position and contact force/torque data generated in the virtual environment combined with a priori knowledge about the task to generate the skills required to perform such a task. Such skills are translated into actual robotic trajectories for implementation in real time. The peg-in-hole insertion problem is used as a case study. The results are reported.,https://ieeexplore.ieee.org/document/4407908/,2007 10th IEEE International Conference on Computer-Aided Design and Computer Graphics,15-18 Oct. 2007,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISDA.2010.5687225,Intelligent online case-based planning agent model for real-time strategy games,IEEE,Conferences,"Research in learning and planning in real-time strategy (RTS) games is very interesting in several industries such as military industry, robotics, and most importantly game industry. A recent published work on online case-based planning in RTS Games does not include the capability of online learning from experience, so the knowledge certainty remains constant, which leads to inefficient decisions. In this paper, an intelligent agent model based on both online case-based planning (OLCBP) and reinforcement learning (RL) techniques is proposed. In addition, the proposed model has been evaluated using empirical simulation on Wargus (an open-source clone of the well known RTS game Warcraft 2). This evaluation shows that the proposed model increases the certainty of the case base by learning from experience, and hence the process of decision making for selecting more efficient, effective and successful plans.",https://ieeexplore.ieee.org/document/5687225/,2010 10th International Conference on Intelligent Systems Design and Applications,29 Nov.-1 Dec. 2010,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CIMCA.2006.133,International Conference on Computational Inteligence for Modelling Control and Automation and International Conference on Intelligent Agents Web Technologies and International Commerce - Title,IEEE,Conferences,"The following topics are dealt with: intelligent agents and ontologies; data mining, knowledge discovery and decision making; intelligent systems; Web technologies and Web services; virtual reality and games; image processing and image understanding techniques; adaptive control and automation; modelling, prediction and control; multi-agent systems and computational intelligence; agent systems, personal assistant agents and profiling; fuzzy systems for industrial automation; control strategies; neural network applications; clustering, classification, data mining and risk analysis; dynamics systems; innovative control systems, hardware design and implementation; robotics and automation; e-business, e-commerce, innovative Web applications; Web databases; diagnosis and medical applications; learning systems; optimization, hybrid systems, genetic algorithms and evolutionary computation control applications; online learning and ERP; knowledge acquisition and classification; nanomechatronics; simulation and control; mobile network applications; information retrieval; Bayesian networks; human computer interaction; cognitive science; mobile agents; knowledge management; intelligent control; e-search and navigation; security.",https://ieeexplore.ieee.org/document/4052645/,2006 International Conference on Computational Inteligence for Modelling Control and Automation and International Conference on Intelligent Agents Web Technologies and International Commerce (CIMCA'06),28 Nov.-1 Dec. 2006,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CogInfoCom47531.2019.9089994,Investigating the Social Robots’ Role in Improving Children Attitudes toward Recycling. The case of PeppeRecycle,IEEE,Conferences,"In this paper we investigate the impact of a social robot in the context of serious games in which the robot plays the role of a game opponent by challenging and, at the same time, teaching the child to correctly recycle waste materials. To this aim we performed a study in which we investigated the dimensions that are used to evaluate serious games integrated with those that are typical of the interaction with a social robot. To endow the robot with the capability to play as a game opponent in a real-world context, we implemented an image recognition module based on a Convolutional Neural Network so that the robot could detect and classify the waste material as a child would do, by seeing it. After a preliminary evaluation of the approach, we started a formal experiment in which we measured the effectiveness of game design, the robot evaluation and the evaluation of cognitive and affective elements that can form the pro-environmental attitude and then the tendency to recycling. A primary school classroom was involved in the study and, results obtained so far, are encouraging and drew promising possibilities for robotics education in changing recycling attitude for children since Pepper is positively evaluated as trustful and believable and this allowed to be concentrated on the `memorization' task during the game.",https://ieeexplore.ieee.org/document/9089994/,2019 10th IEEE International Conference on Cognitive Infocommunications (CogInfoCom),23-25 Oct. 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CCWC51732.2021.9376024,Joint Activity Localization and Recognition with Ultra Wideband based on Machine Learning and Compressed Sensing,IEEE,Conferences,"Joint human activity localization and recognition has broad application prospects in human-computer interaction, virtual reality, smart healthcare system, security monitoring and robotics. Ultra-wideband (UWB) is an emerging technology adopted in real-time location system (RTLS) and has shown satisfactory performance in the task of human activity localization. However, few studies have been carried out to simultaneously recognize human activities based on UWB RTLS, which limits the use of UWB RTLS in many applications. In this study, we develop a RTLS based on UWB for the joint task of activity localization and recognition. A compressed sensing-based activity recognition approach is proposed for the task of activity recognition and several machine learning methods are designed to further improve the activity localization accuracy for the task of activity localization. The experimental results show that our UWB RTLS achieves good performance in this joint task.",https://ieeexplore.ieee.org/document/9376024/,2021 IEEE 11th Annual Computing and Communication Workshop and Conference (CCWC),27-30 Jan. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCTA48790.2019.9478839,Keynote Speech II: Readiness for the Impact of Emerging Technologies,IEEE,Conferences,"Summary form only given. The complete presentation was not made available for publication as part of the conference proceedings. The digital world is becoming increasingly intertwined with the physical world of machines, to which it is bringing ubiquitous intelligence and a perpetual flow of information. These trends are driving us towards a very different future. That future has already started. A new wave of social, economic, and psychological changes is expected to abruptly affect almost everything we do. With change, many opportunities come along. Those who anticipate the course of the future, and prepare for it, will be ready to seize these opportunities and will come out winners. Those who chose to ignore the signs of change, will risk losing their livelihood and eventually hurting their families, businesses, and societies. Those who see the storm coming but react by standing still in panic, disgruntlement, and lamentation will be defenseless when the inevitable waves hit their shores. This presentation overviews the trends in technology and applications, including Artificial Intelligence, Big Data Analytics, Robotics, Internet of Things, Industry 4.0, etc. The impact that such advances are likely to have on the high-tech as well as the low-tech job markets is outlined. Some actions and initiatives are proposed and discussed, with the purpose of triggering a larger debate on how individuals, businesses, academic institutions, and governments should prepare for the anticipated massive changes that are already beginning to affect our world.",https://ieeexplore.ieee.org/document/9478839/,2019 29th International Conference on Computer Theory and Applications (ICCTA),29-31 Oct. 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISORCW.2012.36,Knowledge Representation for Cognitive Robotic Systems,IEEE,Conferences,"Cognitive robotics are autonomous systems capable of artificial reasoning. Such systems can be achieved with a logical approach, but still AI struggles to connect the abstract logic with real-world meanings. Knowledge representation and reasoning help to resolve this problem and to establish the vital connection between knowledge, perception, and action of a robot. Cognitive robots must use their knowledge against the perception of their world and generate appropriate actions in that world in compliance with some goals and beliefs. This paper presents an approach to multi-tier knowledge representation for cognitive robots, where ontologies are integrated with rules and Bayesian networks. The approach allows for efficient and comprehensive knowledge structuring and awareness based on logical and statistical reasoning.",https://ieeexplore.ieee.org/document/6196117/,2012 IEEE 15th International Symposium on Object/Component/Service-Oriented Real-Time Distributed Computing Workshops,11-11 April 2012,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CIS.2013.6782155,Knowledge representation with KnowLang the marXbot case study,IEEE,Conferences,"Intelligent systems are capable of AI exhibited via knowledge representation and reasoning, which helps to connect abstract knowledge symbols to real-world meanings. This paper presents a formal language for knowledge representation called KnowLang. The language implies a multi-tier specification model emphasizing knowledge corpuses, knowledge base operators and inference primitives. The approach allows for efficient and comprehensive knowledge structuring where ontologies are integrated with rules and Bayesian networks. The paper presents the KnowLang specification constructs formally along with a case study based on a mobile robotics platform.",https://ieeexplore.ieee.org/document/6782155/,2012 IEEE 11th International Conference on Cybernetic Intelligent Systems (CIS),23-24 Aug. 2012,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS.2010.5649358,LCM: Lightweight Communications and Marshalling,IEEE,Conferences,"We describe the Lightweight Communications and Marshalling (LCM) library for message passing and data marshalling. The primary goal of LCM is to simplify the development of low-latency message passing systems, especially for real-time robotics research applications. Messages can be transmitted between different processes using LCM's publish/subscribe message-passing system. A platformand language-independent type specification language separates message description from implementation. Message specifications are automatically compiled into language-specific bindings, eliminating the need for users to implement marshalling code while guaranteeing run-time type safety. LCM is notable in providing a real-time deep traffic inspection tool that can decode and display message traffic with minimal user effort and no impact on overall system performance. This and other features emphasize LCM's focus on simplifying both the development and debugging of message passing systems. In this paper, we explain the design of LCM, evaluate its performance, and describe its application to a number of autonomous land, underwater, and aerial robots.",https://ieeexplore.ieee.org/document/5649358/,2010 IEEE/RSJ International Conference on Intelligent Robots and Systems,18-22 Oct. 2010,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA.2018.8460502,Learning Motion Predictors for Smart Wheelchair Using Autoregressive Sparse Gaussian Process,IEEE,Conferences,"Constructing a smart wheelchair on a commercially available powered wheelchair (PWC) platform avoids a host of seating, mechanical design and reliability issues but requires methods of predicting and controlling the motion of a device never intended for robotics. Analog joystick inputs are subject to black-box transformations which may produce intuitive and adaptable motion control for human operators, but complicate robotic control approaches; furthermore, installation of standard axle mounted odometers on a commercial PWC is difficult. In this work, we present an integrated hardware and software system for predicting the motion of a commercial PWC platform that does not require any physical or electronic modification of the chair beyond plugging into an industry standard auxiliary input port. This system uses an RGB-D camera and an Arduino interface board to capture motion data, including visual odometry and joystick signals, via ROS communication. Future motion is predicted using an autoregressive sparse Gaussian process model. We evaluate the proposed system on real-world short-term path prediction experiments. Experimental results demonstrate the system's efficacy when compared to a baseline neural network model.",https://ieeexplore.ieee.org/document/8460502/,2018 IEEE International Conference on Robotics and Automation (ICRA),21-25 May 2018,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICSMC.1997.633274,Learning and reasoning method using fuzzy coloured Petri nets under uncertainty,IEEE,Conferences,"Petri nets have been widely used to model computer systems. Manufacturing systems, robotics systems, knowledge-based systems, and other kinds of engineering applications. Further, to present complex real-world knowledge fuzzy Petri net models have been proposed to perform fuzzy reasoning automatically. However, in Petri nets one has to represent all kinds of processes by separate subnets even though the process has the same behavior as another. Real-world knowledge often contains many parts which are similar, but not identical. This means that the total number of Petri nets becomes very large. Therefore, it becomes difficult to see the similarities and the differences among the individual subnets representing similar parts. The problems may be annoying for a small system, and catastrophic for the description of a large-scale system. To avoid this kind of problem the authors propose a learning and reasoning method using fuzzy coloured Petri nets (FCPN) under uncertainty. For the correction of rules of the knowledge-based system a hand-built classifier and empirical learning method based on domain theory have been proposed as machine learning methods, where there is a significant gap between the knowledge-intensive approach in the former and the virtually knowledge-free approach in the letter. To resolve such problems simultaneously they propose a hybrid learning method which is built on top of the knowledge-based fuzzy coloured Petri net and genetic algorithms.",https://ieeexplore.ieee.org/document/633274/,"1997 IEEE International Conference on Systems, Man, and Cybernetics. Computational Cybernetics and Simulation",12-15 Oct. 1997,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.23919/ACC.1992.4792313,Learning for Skill Acquisition and Refinement: Toward Exploring Everyday Physics,IEEE,Conferences,"The present talk claims that ""robotics"" is not a test bed for AI but should involve a research frontier, which attempts to account for intelligibility of everyday physics underlying human activities such as perception, remembrance, planning, practices, and skill. In addition to traditional AI and neuro-network approaches, more of new domains that can account for any aspect of human intellectual behaviors must be exploited, and also more of new tools that actualize real implementation of intelligence in machines need to be devised. To aim at going on an expedition in this direction, this talk introduces one new domain and another new tool. The former is practice-based learning for skill refinement and the latter is a design tool of signal-based structured information base for skill acquirement.",https://ieeexplore.ieee.org/document/4792313/,1992 American Control Conference,24-26 June 1992,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS.1991.174419,Learning for skill refinement,IEEE,Conferences,"It is claimed that 'robotics' is not a test bed for AI but should involve a research frontier relating to the physics underlying human activities such as perception, remembering, planning, practice, and skill. In addition to traditional AI and neural network approaches, other domains that can account for any aspect of human intellectual behavior must be exploited, and tools that actualize real implementation of intelligence in machines need to be devised. A practice-based learning domain for skill refinement and a design tool for a signal-based structured information base for skill acquisition are presented.<>",https://ieeexplore.ieee.org/document/174419/,Proceedings IROS '91:IEEE/RSJ International Workshop on Intelligent Robots and Systems '91,3-5 Nov. 1991,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCIC.2015.7435795,Learning mechanism for RT task scheduling,IEEE,Conferences,"The fascinations of Internet of Things (IoT) necessitate a large number of devices are to be integrated with the existing IoT. These devices are very difficult to manage in a large distributed environment without a careful management design. These location based devices generate data at fixed intervals of time and need configure these devices to software platform to analyze data and understand environment in better way. So, learning capability should incorporate within the system as the environment of system changes dynamically. As the Internet of Things continues to develop, further potential is estimated by a combination with related technology approaches and concepts such as Cloud Computing, Future Internet, Big Data, Robotics and Semantic Technologies. The idea is becomes now evident as those related concepts have started to reveal synergies by combining them.",https://ieeexplore.ieee.org/document/7435795/,2015 IEEE International Conference on Computational Intelligence and Computing Research (ICCIC),10-12 Dec. 2015,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA46639.2022.9811592,Learning to Detect Slip with Barometric Tactile Sensors and a Temporal Convolutional Neural Network,IEEE,Conferences,"The ability to perceive object slip via tactile feedback enables humans to accomplish complex manipulation tasks including maintaining a stable grasp. Despite the utility of tactile information for many applications, tactile sensors have yet to be widely deployed in industrial robotics settings; part of the challenge lies in identifying slip and other events from the tactile data stream. In this paper, we present a learning-based method to detect slip using barometric tactile sensors. These sensors have many desirable properties including high durability and reliability, and are built from inexpensive, off-the-shelf components. We train a temporal convolution neural network to detect slip, achieving high detection accuracies while displaying robustness to the speed and direction of the slip motion. Further, we test our detector on two manipulation tasks involving a variety of common objects and demonstrate successful generalization to real-world scenarios not seen during training. We argue that barometric tactile sensing technology, combined with data-driven learning, is suitable for many manipulation tasks such as slip compensation.",https://ieeexplore.ieee.org/document/9811592/,2022 International Conference on Robotics and Automation (ICRA),23-27 May 2022,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA46639.2022.9812166,Legged Robots that Keep on Learning: Fine-Tuning Locomotion Policies in the Real World,IEEE,Conferences,"Legged robots are physically capable of traversing a wide range of challenging environments, but designing controllers that are sufficiently robust to handle this diversity has been a long-standing challenge in robotics. Reinforcement learning presents an appealing approach for automating the controller design process and has been able to produce remarkably robust controllers when trained in a suitable range of environments. However, it is difficult to predict all likely conditions the robot will encounter during deployment and enumerate them at training-time. What if instead of training controllers that are robust enough to handle any eventuality, we enable the robot to continually learn in any setting it finds itself in? This kind of real-world reinforcement learning poses a number of challenges, including efficiency, safety, and autonomy. To address these challenges, we propose a practical robot reinforcement learning system for fine-tuning locomotion policies in the real world. We demonstrate that a modest amount of real-world training can substantially improve performance during deployment, and this enables a real A1 quadrupedal robot to autonomously fine-tune multiple locomotion skills in a range of environments, including an outdoor lawn and a variety of indoor terrains. (Videos and code<sup>1</sup><sup>1</sup>https://sites.google.com/berkele.edu/fine-tuning-locomotion)",https://ieeexplore.ieee.org/document/9812166/,2022 International Conference on Robotics and Automation (ICRA),23-27 May 2022,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICIP.2019.8803544,Lightweight Monocular Depth Estimation Model by Joint End-to-End Filter Pruning,IEEE,Conferences,"Convolutional neural networks (CNNs) have emerged as the state-of-the-art in multiple vision tasks including depth estimation. However, memory and computing power requirements remain as challenges to be tackled in these models. Monocular depth estimation has significant use in robotics and virtual reality that requires deployment on low-end devices. Training a small model from scratch results in a significant drop in accuracy and it does not benefit from pre-trained large models. Motivated by the literature of model pruning, we propose a lightweight monocular depth model obtained from a large trained model. This is achieved by removing the least important features with a novel joint end-to-end filter pruning. We propose to learn a binary mask for each filter to decide whether to drop the filter or not. These masks are trained jointly to exploit relations between filters at different layers as well as redundancy within the same layer. We show that we can achieve around 5x compression rate with small drop in accuracy on the KITTI driving dataset. We also show that masking can improve accuracy over the baseline with fewer parameters, even without enforcing compression loss.",https://ieeexplore.ieee.org/document/8803544/,2019 IEEE International Conference on Image Processing (ICIP),22-25 Sept. 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.23919/SPA.2019.8936736,Machine Learning for Embodied Agents: From Signals to Symbols and Actions,IEEE,Conferences,"The aim of this tutorial lecture is to show the role of machine learning and some other AI-related techniques in embodied autonomous agents, and autonomous robots in particular. In this tutorial we bring to the forefront the aspects of robotics that are closely related to computer science. We believe that the progress in algorithms and data processing methods together with the rapid increase in the available computing power were the driving forces behind the successes of modern robotics in the last decade. During this period robots of various classes migrated from university laboratories to commercial companies and then to our everyday life, as now everybody can buy an autonomous vacuum cleaner or lawnmower, while self-driving cars and drones for goods delivery are waiting for proper legal regulations to enter the market. Robotics and Artificial Intelligence already went a long path of mutual inspiration and common development, starting from the symbolic AI (aka Good Old-Fashioned Artificial Intelligence) and its extensive use in early autonomous robots, such as Shakey the robot, created in SRI International by Nils Nilsson, considered one of the ""fathers"" of modern AI. We briefly characterize the range of the most important applications of typical AI methods in modern robotics, including motion planning algorithms [2,3], interpretation of sensory data leading to creation of a world model [4 ,5], and classical learning methods, such as reinforcement learning [6]. However, what made robotics a part of the new wave of AI applications was the recent ""revolution"" of machine learning, mostly grounded in the enormous success of the deep learning paradigm and its many variants that proved to outclass classic methods in a broad range of problems related to the processing of images and other types of signals. The quick adoption of the recent advances in Machine Learning (ML) in robotics seems to be motivated by the fact that ML gives the possibility to infer solutions from data, as opposed to the classic model-based paradigm that was for decades used in robotics. Whereas the modelbased solutions are mathematically elegant and theoretically provable (with respect to stability, convergence, etc.) they often fail once confronted with real-world problems and real sensory data, as their underlying mathematical models are only a very rough approximation of the real world. Therefore, a wider adoption of ML in robotics gives a chance to make robots more robust and adaptive. On the other hand, we should try to use the new techniques without discarding the knowledge and expertise we already have - machine learning methods can benefit a lot from the prior knowledge and the known structure of the problem that has to be solved by learning. This knowledge and structure can be adopted from the model-based methods that a re already well-established in robotics. In the lecture robots are understood in a broad sense, as all embodied agents that have means to physically interact with the environment. They can be either manipulators, mobile robots, aerial vehicles, self-driving cars, and various ""smart"" devices and sensors. In the second part of the lecture attention is paid to specific problems that appear in application of machine learning to embodied agents, such as the need to search a for solution in huge, multi-dimensional spaces (""curse of dimensionality""), and the ever-present problem of representation and incorporation of uncertainty in the processing of real-world data. Some examples of applications of autonomous robots are given, which were successful due to the use of AI - in particular the probabilistic representation of knowledge and machine learning. The most prominent examples are the DARPA competitions: ""Grand Challenge"", ""Urban Challenge"" and ""Robotics Challenge"" (DRC), and the ""Amazon Picking Challenge"", which proves the interest of large corporations in the development of AI-based robotics [7]. In the third part of the lecture new research directions offered by machine learning and the increased availability of training data are discussed. An overview of the most popular application areas of ML in robotics and other autonomous systems is presented along with the typical machine learning paradigms applied in these areas. The focus is on deep learning, mostly using convolutional neural networks to process various sensory data. We discuss three aspects of embodied agents that make machine learning in robotics quite specific with respect to other application areas, such as medical images or natural language processing. The first aspect is dealing with the ""open world"", in which autonomous robots usually operate. This situation breaks the assumptions underlying some popular ML methods, and creates the need to face the problem of unknown classes identification [8] incremental learning [9], and the uncertainty of sensory data [10]. We also stress out that an embodied agent has the ability to actively acquire information [11]. The second aspect is the inference about the scene seen by the agent, where in the case of robotics, semantics and geometry intermingle [12], because the robot has to work in a three-dimensional world, although it often perceives it through twodimensional images [13,14]. The third aspect of our analysis is related to the most important feature of robots that distinguishes them from all other learning agents (software-based). Robots are embodied agents, that is they have a physical ""body"", and are subject to physical constraints, such as the maximum speed of motion or maximum range of perception. Therefore, in ML for robots analysis of the spatio-temporal dependencies in data is very important [15]. Robots support advanced learning methods thanks to the possibility of interaction with the environment - a simple example is active vision with moving camera, a much more complex one is manipulation with active testing of the behavior of objects (repositioning, pushing) [16]. At the end of the lecture, in the context of specific needs and limitations characteristic to the applications of ML in robotics, new concepts of machine learning (e.g. deep reinforcement learning [17], interactive perception [18]) are presented. The lecture is summarized with a brief discussion of the most important challenges and open problems of ML applied to embodied agents.",https://ieeexplore.ieee.org/document/8936736/,"2019 Signal Processing: Algorithms, Architectures, Arrangements, and Applications (SPA)",18-20 Sept. 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CVPRW53098.2021.00141,MarkerPose: Robust Real-time Planar Target Tracking for Accurate Stereo Pose Estimation,IEEE,Conferences,"Despite the attention marker-less pose estimation has attracted in recent years, marker-based approaches still provide unbeatable accuracy under controlled environmental conditions. Thus, they are used in many fields such as robotics or biomedical applications but are primarily implemented through classical approaches, which require lots of heuristics and parameter tuning for reliable performance under different environments. In this work, we propose MarkerPose, a robust, real-time pose estimation system based on a planar target of three circles and a stereo vision system. MarkerPose is meant for high-accuracy pose estimation applications. Our method consists of two deep neural networks for marker point detection. A SuperPoint-like network for pixel-level accuracy keypoint localization and classification, and we introduce EllipSegNet, a lightweight ellipse segmentation network for sub-pixel-level accuracy keypoint detection. The marker’s pose is estimated through stereo triangulation. The target point detection is robust to low lighting and motion blur conditions. We compared MarkerPose with a detection method based on classical computer vision techniques using a robotic arm for validation. The results show our method provides better accuracy than the classical technique. Finally, we demonstrate the suitability of MarkerPose in a 3D freehand ultrasound system, which is an application where highly accurate pose estimation is required. Code is available in Python and C++ at https://github.com/jhacsonmeza/MarkerPose.",https://ieeexplore.ieee.org/document/9523117/,2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW),19-25 June 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCCI50826.2021.9402304,Maze Solving with humanoid robot NAO using Real-Time object detection,IEEE,Conferences,"In a future that is not too distant from today, humanoids are surely going to be an integral part of both our professional and private lives, assisting us with various tasks. Unlike normal robots that we may encounter in our everyday lives, humanoids are designed in specific manners to give them more human-like capabilities that enable them to perform complex tasks such as climbing a flight of stairs. In this paper, we present a Maze-Solving Algorithm which is a software developed specifically for the humanoid robot, NAO, and gives it the capability to enter and exit a maze autonomously. NAO is a next-gen humanoid bot developed by SoftBank Robotics using the power of AI. The bot is equipped with numerous sensors and cameras. Though various quantitative approaches were considered and experimented with, we stuck onto the one which had the least average time complexity of all after a thorough comparative study. We suggest an approach where the humanoid can detect and localize objects from a distance and take programmable decisions based on them. AI constantly tries to give robots human-thinking capabilities to make their decision-making skills similar to those of humans, if not better than them. This algorithm was developed taking into consideration how a human intellect would react rationally if he is stuck in a maze. The methodology used revolves primarily around the combined use of SONAR(Sound navigation ranging) and tactical sensors, and cameras equipped within the bot. The output values from this hardware were then evaluated to judge the distance from a wall and the reactions from the bot were calculated by the suggested algorithm accordingly.",https://ieeexplore.ieee.org/document/9402304/,2021 International Conference on Computer Communication and Informatics (ICCCI),27-29 Jan. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/OCEANS.1998.724376,Model development of an underwater manipulator for coordinated arm-vehicle control,IEEE,Conferences,"This paper presents research on the hydrodynamic modeling of a manipulator for an autonomous underwater scientific vehicle. The focus is on improving the modeling accuracy of the in-line hydrodynamic coupling between a two-link manipulator and a small, free-floating vehicle in order to achieve better control for coordinated motion of the combined system. Loads predicted using existing models for underwater arms were determined to be off by as much as 25% when applied to a real, two-link arm in a test tank. In this new approach, an experimentally-determined model has been developed that takes into account the 3D flow effects that have previously not been included. The end result is a model that provides accurate predictions for the joint torques of a two-link arm in a form simple enough to be implemented in algorithms for precision planning and control. This project is part of a joint program between the Aerospace Robotics Laboratory at Stanford University and the Monterey Bay Aquarium Research Institute.",https://ieeexplore.ieee.org/document/724376/,IEEE Oceanic Engineering Society. OCEANS'98. Conference Proceedings (Cat. No.98CH36259),28 Sept.-1 Oct. 1998,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ROBOT.1996.503841,Modeling hybrid systems as the limit of discrete computational processes,IEEE,Conferences,This paper outlines a new formalism for hybrid systems in which continuous dynamics are represented as the limit of a discrete computational process. Hybrid systems are systems which are composed of continuous and discrete components. Such systems often arise in robotics where operations in the continuous real world are controlled by a discrete software system. Most existing hybrid formalisms treat their continuous and discrete components as distinct types of process. This new approach brings the two together and provides a platform for the investigation of the transition between continuous change and discrete events.,https://ieeexplore.ieee.org/document/503841/,Proceedings of IEEE International Conference on Robotics and Automation,22-28 April 1996,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SMCIA.2003.1231343,Modeling sensory motor control,IEEE,Conferences,"Daniel Gardner in The Neurobiology of Neural Networks (1993) calls for a more neuromorphic generation of artificial neural networks. He states in his preface: ""For neural network modelers, especially those without a background in the life sciences, the book critically examines aspects of network models that are likely to be both computationally significant and comparable to our current view of real neurons and synapses and the networks they form."" Succinctly stated in its preface is Gardner's thesis: ""It is a series of variations on a single theme: that both artificial neural networks and neurobiology will benefit by a partial synthesis of their disparate natures"". The October/November 1998 issue of Neural Networks, entitled ""Neural Control and Robotics: Biology and Technology"", reflected Gardner's view. Further, Randall Beer's (1990) book contains a description of spiking neurons, designed for the purpose of control rather than of memory. Artificial neural circuits, created to act as controllers rather than as memories, are the theme of all of the above mentioned works. It is with the above in mind that the author is implementing two models which will offer the chance of studying neural control mechanisms.",https://ieeexplore.ieee.org/document/1231343/,"Proceedings of the 2003 IEEE International Workshop on Soft Computing in Industrial Applications, 2003. SMCia/03.",25-25 June 2003,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/EHB52898.2021.9657633,Modular Telepresence Robot for Distance Medical Education,IEEE,Conferences,"In special conditions such as pandemics, wars or any other problem can lead to the physical suspension of the education system including courses in medical schools. Telepresence robotics is a solution through which courses and laboratories can be held remotely where medical students can interact physically, through robots, with teaching or laboratory equipment, thus actively participating in the educational process. Although there are various variants of telepresence robots, this paper presents the study of the possibility to design and make a telepresence robot kit, low cost, containing modules easy to assemble and adapted for various teaching situations, respectively for the development of remote laboratory experiments. The proposed kit has mechanical components that can be printed on a 3D printer. The electronic components are compatible with the Arduino environment and include three modules: sensory, navigation and a communications and artificial intelligence module. The software elements are also modularly designed and allow adaptation to various study and laboratory situations. Robotic elements compatible with the Arduino environment such as arms or lifting or throwing systems can be added to the robot. The results of the experimental measurements showed that the robot has acceptable travel errors for unpretentious applications, but to increase its accuracy in laboratory conditions, the software was programmed so that, with the help of artificial intelligence, the robot can perform real-time measurements and correct automatically. This mechanism is useful in medical applications such as assisting and guiding blind patients.",https://ieeexplore.ieee.org/document/9657633/,2021 International Conference on e-Health and Bioengineering (EHB),18-19 Nov. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS51168.2021.9636224,Moving Forward in Formation: A Decentralized Hierarchical Learning Approach to Multi-Agent Moving Together,IEEE,Conferences,"Multi-agent path finding in formation has many potential real-world applications like mobile warehouse robotics. However, previous multi-agent path finding (MAPF) methods hardly take formation into consideration. Further-more, they are usually centralized planners and require the whole state of the environment. Other decentralized partially observable approaches to MAPF are reinforcement learning (RL) methods. However, these RL methods encounter difficulties when learning path finding and formation problems at the same time. In this paper, we propose a novel decentralized partially observable RL algorithm that uses a hierarchical structure to decompose the multi-objective task into unrelated ones. It also calculates a theoretical weight that makes each tasks reward has equal influence on the final RL value function. Additionally, we introduce a communication method that helps agents cooperate with each other. Experiments in simulation show that our method outperforms other end-to-end RL methods and our method can naturally scale to large world sizes where centralized planner struggles. We also deploy and validate our method in a real-world scenario.",https://ieeexplore.ieee.org/document/9636224/,2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),27 Sept.-1 Oct. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SSCI.2016.7850240,Multi-Channel Bayesian ART for robot fusion perception,IEEE,Conferences,"Multiple sensor data fusion is the technique of associate information from a number of different sensors to produce a robust and comprehensive description. Data fusion pose is using in various robotics application such as environment mapping, object recognition and robot localization. Their relation is generally hard coded and difficult to learn incrementally if new objects or events arise. In this paper, we propose a new learning architecture termed as Multi-Channel Bayesian ART which is very flexible can be adapted to new domains or different sensor configurations easily. The other advantages of the proposed method are: (1) it is capable of incremental on-line learning without forgetting previously-learned knowledge (2) It can process data real time and does not require any prior training to make it work in natural environment. The effectiveness of our proposed method is validated by real experimental results implemented on robot.",https://ieeexplore.ieee.org/document/7850240/,2016 IEEE Symposium Series on Computational Intelligence (SSCI),6-9 Dec. 2016,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICTAI50040.2020.00088,Multi-Robot Collision Avoidance with Map-based Deep Reinforcement Learning,IEEE,Conferences,"Multi-robot collision avoidance in a communication-free environment is one of the key issues for mobile robotics and autonomous driving. In this paper, we propose a map-based deep reinforcement learning (DRL) approach for collision avoidance of multiple robots, where robots do not communicate with each other and only sense other robots' positions and the obstacles around them. We use the egocentric grid map of a robot to represent the environmental information around it, which can be easily generated by using multiple sensors or sensor fusion. The learned policy generated from the DRL model directly maps 3 frames of egocentric grid maps and the robot's relative local goal positions into low-level robot control commands. We first train a convolutional neural network for the navigation policy in a simulator of multiple mobile robots using proximal policy optimization (PPO). Then we deploy the trained model to real robots to perform collision avoidance in their navigation. We evaluate the approach with various scenarios both in the simulator and on three differential-drive mobile robots in the real world. Both qualitative and quantitative experiments show that our approach is efficient with a high success rate. The demonstration video can be found at https://youtu.be/jcLKlEXuFuk.",https://ieeexplore.ieee.org/document/9288300/,2020 IEEE 32nd International Conference on Tools with Artificial Intelligence (ICTAI),9-11 Nov. 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.23919/MIPRO52101.2021.9596665,Multi-Stakeholder Engagement in Agile Service Platform Co-Creation,IEEE,Conferences,"In the recent decade, the concept of Smart Tourism Destination (STD) has emerged to represent both a strategic aim to develop sustainable competitive advantages for tourism destinations or wider regions, and a managerial approach and dimensions to enhance data-driven development to measure and apply smart technologies for competitive and inclusive change in a tourism ecosystem. In tourism destination development globally, the aim for sustainable digital transformation in low-productivity business sectors is one of the key drivers for smart tourism and smart destination development. More recently, the global Covid-19 pandemic has given a boost to touchless, digital and green tourism development initiatives in the EU and worldwide. To advance sustainable digital transformation with agile methods in STD development, a systemic service platform approach and agile prototyping with smart emerging technologies, (eg. with AI, IoT, Augmented & Virtual Reality, 5G and Robotics), and multi-stakeholder co-creation is needed to engage key tourism industry ecosystem representatives. This conceptual paper advocates that open-source solutions combined with comprehensive multi-stakeholder co-creation aid in prototyping a systemic digital platform solution for smart tourism destinations. The paper concludes with an illustration of a conceptual model of service platform development for smart destinations, utilizing network co-creation with quadruple-helix stakeholders for sustainable regional impacts.",https://ieeexplore.ieee.org/document/9596665/,"2021 44th International Convention on Information, Communication and Electronic Technology (MIPRO)",27 Sept.-1 Oct. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS45743.2020.9341372,Multiplicative Controller Fusion: Leveraging Algorithmic Priors for Sample-efficient Reinforcement Learning and Safe Sim-To-Real Transfer,IEEE,Conferences,"Learning-based approaches often outperform hand-coded algorithmic solutions for many problems in robotics. However, learning long-horizon tasks on real robot hardware can be intractable, and transferring a learned policy from simulation to reality is still extremely challenging. We present a novel approach to model-free reinforcement learning that can leverage existing sub-optimal solutions as an algorithmic prior during training and deployment. During training, our gated fusion approach enables the prior to guide the initial stages of exploration, increasing sample-efficiency and enabling learning from sparse long-horizon reward signals. Importantly, the policy can learn to improve beyond the performance of the sub-optimal prior since the prior's influence is annealed gradually. During deployment, the policy's uncertainty provides a reliable strategy for transferring a simulation-trained policy to the real world by falling back to the prior controller in uncertain states. We show the efficacy of our Multiplicative Controller Fusion approach on the task of robot navigation and demonstrate safe transfer from simulation to the real world without any fine-tuning. The code for this project is made publicly available at https://sites.google.com/view/mcf-nav/home.",https://ieeexplore.ieee.org/document/9341372/,2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),24 Oct.-24 Jan. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICUAS.2016.7502665,Natural user interfaces for human-drone multi-modal interaction,IEEE,Conferences,"Personal drones are becoming part of every day life. To fully integrate them into society, it is crucial to design safe and intuitive ways to interact with these aerial systems. The recent advances on User-Centered Design (UCD) applied to Natural User Interfaces (NUIs) intend to make use of human innate features, such as speech, gestures and vision to interact with technology in the way humans would with one another. In this paper, a Graphical User Interface (GUI) and several NUI methods are studied and implemented, along with computer vision techniques, in a single software framework for aerial robotics called Aerostack which allows for intuitive and natural human-quadrotor interaction in indoor GPS-denied environments. These strategies include speech, body position, hand gesture and visual marker interactions used to directly command tasks to the drone. The NUIs presented are based on devices like the Leap Motion Controller, microphones and small size monocular on-board cameras which are unnoticeable to the user. Thanks to this UCD perspective, the users can choose the most intuitive and effective type of interaction for their application. Additionally, the strategies proposed allow for multi-modal interaction between multiple users and the drone by being able to integrate several of these interfaces in one single application as is shown in various real flight experiments performed with non-expert users.",https://ieeexplore.ieee.org/document/7502665/,2016 International Conference on Unmanned Aircraft Systems (ICUAS),7-10 June 2016,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICEE.2018.8472657,Neural Control of Mobile Robot Motion Based on Feedback Error Learning and Mimetic Structure,IEEE,Conferences,"Mobile robots motion control is a basic problem in robotics and there are still some control difficulties such as uncertainty in a real implementation which should be considered. This paper is concerned with the neural control of wheeled mobile robots trajectory tracking and posture stabilization. In the trajectory-tracking problem, the Feedback Error Learning (FEL) structure is used and for the posture stabilization problem, the Mimetic structure is employed. These neural based structures use a classic controller, Dynamic Feedback Linearization (DFL), and help to improve the adaptiveness of it. The effectiveness of the proposed controllers is verified by simulation in Webots robotic simulator and on the e-puck which is a differential wheeled mobile robot. The simulation results verify the ability of the proposed methods for controlling the robot and handling uncertainties.",https://ieeexplore.ieee.org/document/8472657/,"Electrical Engineering (ICEE), Iranian Conference on",8-10 May 2018,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AIM.2009.5229901,Neural Q-Learning controller for mobile robot,IEEE,Conferences,"In recent years, increasing trend in application of autonomous mobile robot worldwide has highlighted the importance of path planning controller in robotics-related fields, especially where dynamic and unknown environment is involved. Writing a good robot controller program can be a very time consuming process. It is inevitably wasting of resources and efforts if we have to rewrite the controller over and over again whenever there is emergence of changes in the environment. Reinforcement Learning (RL) algorithms and Artificial Neural Network (ANN) are used to assist autonomous mobile robot to learn in an unrecognized environment. This research study is focused on exploring integration of multi-layer neural network and Q-Learning as an online learning controller. Learning process is divided into two stages. In the initial stage the agent will map the environment through collecting state-action information according to the Q-Learning procedure. Second training process involves neural network training which will utilize the state-action information gathered in earlier phase as training samples. During final application of the controller, Q-Learning would be used as the primary navigating tool whereas the trained neural network will be employed when approximation is needed. MATLAB simulation was developed to verify the validity of the algorithm before it is real-time implemented on the real world using Team AmigoBot™ robot. The results obtained from both simulation and actual application confirmed on-spot learning ability of the controller accompanied with certain degree of flexibility and robustness.",https://ieeexplore.ieee.org/document/5229901/,2009 IEEE/ASME International Conference on Advanced Intelligent Mechatronics,14-17 July 2009,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/OPTIM.2008.4602496,Neural control based on RBF network implemented on FPGA,IEEE,Conferences,"The RBF radial basis function network is intended especially for hardware implementation and this type of network is used successfully in the areas of robotics and control, where the real time capabilities of the network are of particular importance. The implementation of neural networks on FPGA has several benefits, with emphasis on parallelism and the real time capabilities. This paper discusses the hardware implementation of the RBF type neural network, the architecture and parameters and the functional modules of the hardware implemented neuro-processor.",https://ieeexplore.ieee.org/document/4602496/,2008 11th International Conference on Optimization of Electrical and Electronic Equipment,22-24 May 2008,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISESD.2016.7886710,Neural network implementation for invers kinematic model of arm drawing robot,IEEE,Conferences,"Nowadays, the research in robotics field is growing. One of the studies in robotics is the control method of the robotic arm movement. In this research, a 3 DOF arm drawing robot was built. An inverse kinematic models of the robot arm is made using artificial neural network method. Artificial neural network model was implemented in a GUI application. The ANN model can work in real-time to control arm robot movement to reach certain coordinates. Based on test results, the inverse kinematic models of the arm drawing robot had an error rate under 2%. It is of 0.16% for X coordinate and 0.46% for Y coordinate.",https://ieeexplore.ieee.org/document/7886710/,2016 International Symposium on Electronics and Smart Devices (ISESD),29-30 Nov. 2016,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SOFA.2009.5254883,Neurodynamic optimization with its application for model predictive control,IEEE,Conferences,"Summary form only given. Optimization problems arise in a wide variety of scientific and engineering applications. It is computationally challenging when optimization procedures have to be performed in real time to optimize the performance of dynamical systems. For such applications, classical optimization techniques may not be competent due to the problem dimensionality and stringent requirement on computational time. One very promising approach to dynamic optimization is to apply artificial neural networks. Because of the inherent nature of parallel and distributed information processing in neural networks, the convergence rate of the solution process is not decreasing as the size of the problem increases. Neural networks can be implemented physically in designated hardware such as ASICs where optimization is carried out in a truly parallel and distributed manner. This feature is particularly desirable for dynamic optimization in decentralized decision-making situations arising frequently in control and robotics. In this talk, the author presents the historic review and the state of the art of neurodynamic optimization models and selected applications in robotics and control. Specifically, starting from the motivation of neurodynamic optimization, we will review various recurrent neural network models for optimization. Theoretical results about the stability and optimality of the neurodynamic optimization models will be given along with illustrative examples and simulation results. It will be shown that many problems in control systems, such model predictive control, can be readily solved by using the neurodynamic optimization models. Specifically, linear and nonlinear model predictive control based on neurodynamic optimization will be delineated.",https://ieeexplore.ieee.org/document/5254883/,2009 3rd International Workshop on Soft Computing Applications,29 July-1 Aug. 2009,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/NGCT.2015.7375178,Neuronal Logic gates realization using Vedic mathematics,IEEE,Conferences,"Gates are the fundamental building block of all logic circuits. Artificial neural networks (ANN) have processing capabilities in a parallel architecture, and due to this they are useful in applications like pattern recognition, system identification, prediction problems, robotics, and control problems. Boolean logic realization using artificial neural network is known as Neuronal Logic. Simple and low precision computations are the basic requirements of ANN which can be performed faster. This can be implemented on cheap and low precision hardware. Neural network involves enormous number of multiplication and addition calculations. It has been already proved that multipliers based on Vedic mathematics are faster in speed than the standard multipliers. In this paper, the possibility of hardware realization of neuronal logic gates using Vedic multipliers herein referred to as Vedic neuron has been explored. This is achieved by performing the neural network computations using Vedic mathematics rather than the conventional multiplication process. Basic logic gates like AND, OR and AND-NOT have been studied and its hardware implementation using neural network has been simulated using VHDL. A comparative study was carried out on the computation speed of neuronal logic gates implemented using conventional multipliers as well as neuronal logic gates implemented using Vedic multipliers. The increase in processing speed with Vedic neuron implementation has been observed which can be of use in several real time operations where speed is critical.",https://ieeexplore.ieee.org/document/7375178/,2015 1st International Conference on Next Generation Computing Technologies (NGCT),4-5 Sept. 2015,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CNN53494.2021.9580216,Neuropunk revolution and its implementation via real-time neurosimulations and their integrations,IEEE,Conferences,"In this paper I present the perspectives of the &#x201C;neuropunk revolution&#x201D; technologies. One could understand the &#x201C;neuropunk revolution&#x201D; as the integration of real-time neurosimulations into biological nervous/motor system via neurostimulation or artificial robotic systems via integration with actuators. I see the added value of the real-time neurosimulations as bridge technology for the set of developed technologies: BCI, neuroprosthetics, AI, robotics to provide bio-compatible integration into biological or artificial limbs. Here I present the three types of integration of the &#x201C;neuropunk revolution&#x201D; technologies as inbound, outbound and closed-loop in-outbound systems. I see the shift of the perspective how we see now the set of technologies including AI, BCI, neuroprosthetics and robotics due to the proposed concept for example the integration of external to a body simulated part of nervous system back into the biological nervous system or muscles.",https://ieeexplore.ieee.org/document/9580216/,2021 Third International Conference Neurotechnologies and Neurointerfaces (CNN),13-15 Sept. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/HUMANOIDS.2018.8625038,NimbRo-OP2X: Adult-Sized Open-Source 3D Printed Humanoid Robot,IEEE,Conferences,"Humanoid robotics research depends on capable robot platforms, but recently developed advanced platforms are often not available to other research groups, expensive, dangerous to operate, or closed-source. The lack of available platforms forces researchers to work with smaller robots, which have less strict dynamic constraints or with simulations, which lack many real-world effects. We developed NimbRo-OP2X to address this need. At a height of 135 cm our robot is large enough to interact in a human environment. Its low weight of only 19kg makes the operation of the robot safe and easy, as no special operational equipment is necessary. Our robot is equipped with a fast onboard computer and a GPU to accelerate parallel computations. We extend our already open-source software by a deep-learning based vision system and gait parameter optimisation. The NimbRo-OP2X was evaluated during RoboCup 2018 in Montreál, Canada, where it won all possible awards in the Humanoid AdultSize class.",https://ieeexplore.ieee.org/document/8625038/,2018 IEEE-RAS 18th International Conference on Humanoid Robots (Humanoids),6-9 Nov. 2018,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CLEI53233.2021.9639989,ODROM: Object Detection and Recognition supported by Ontologies and applied to Museums,IEEE,Conferences,"In robotics, object detection in images or videos, obtained in real-time from sensors of robots can be used to support the implementation of service robot tasks (e.g., navigation, model its social behavior, recognize objects in a specific domain), usually accomplished in indoor environments. However, traditional deep learning based object detection techniques present limitations in such indoor environments, specifically related to the detection of small objects and the management of high density of multiple objects. Coupled with these limitations, for specific domains (e.g., hospitals, museums), it is important that the robot, apart from detecting objects, extracts and knows information of the targeted objects. Ontologies, as a part of the Semantic Web, are presented as a feasible option to formally represent the information related to the objects of a particular domain. In this context, this work proposes an object detection and recognition process based on a Deep Learning algorithm, object descriptors, and an ontology. ODROM, an Object Detection and Recognition algorithm supported by Ontologies and applied to Museums, is an implementation to validate the proposal. Experiments show that the usage of ontologies is a good way of desambiguating the detection, obtained with a and <tex>$\mathbf{mAP}{@}0.5=0.88$</tex> and a <tex>$\mathbf{mAP}{@}[0.5:0.95]=61\%$</tex>.",https://ieeexplore.ieee.org/document/9639989/,2021 XLVII Latin American Computing Conference (CLEI),25-29 Oct. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCV48922.2021.01064,ORBIT: A Real-World Few-Shot Dataset for Teachable Object Recognition,IEEE,Conferences,"Object recognition has made great advances in the last decade, but predominately still relies on many high-quality training examples per object category. In contrast, learning new objects from only a few examples could enable many impactful applications from robotics to user personalization. Most few-shot learning research, however, has been driven by benchmark datasets that lack the high variation that these applications will face when deployed in the real-world. To close this gap, we present the ORBIT dataset and benchmark, grounded in the real-world application of teachable object recognizers for people who are blind/low-vision. The dataset contains 3,822 videos of 486 objects recorded by people who are blind/low-vision on their mobile phones. The benchmark reflects a realistic, highly challenging recognition problem, providing a rich playground to drive research in robustness to few-shot, high-variation conditions. We set the benchmark&#x2019;s first state-of-the-art and show there is massive scope for further innovation, holding the potential to impact a broad range of real-world vision applications including tools for the blind/low-vision community. We release the dataset at https://doi.org/10.25383/city.14294597 and benchmark code at https://github.com/microsoft/ORBIT-Dataset.",https://ieeexplore.ieee.org/document/9711109/,2021 IEEE/CVF International Conference on Computer Vision (ICCV),10-17 Oct. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IRC.2019.00061,"ORC—A Lightweight, Lightning-Fast Middleware",IEEE,Conferences,"Robotic tasks are commonly solved by integrating numerous different software and hardware modules into one working application. The necessary integration work typically contributes a considerable share of the total work required for a project, which is why past research on robotics computing has pushed towards generating higher-level abstraction layers, like middlewares. However, the current state-of-the-art cannot provide reliable, low-latency communication performance as we will show in the experimental evaluation. In this paper we propose the Open Robot Communication framework (ORC). Compared to previous middlewares, ORC is lightweight and geared towards applications with high-performance requirements. We consider ORC especially useful for applications with Human Robot Interaction or collaborative tasks involving multiple robots. In the paper, we compare the runtime performance of ORC to the robot operating system (ROS). We can show that ORC enables message transfer with delays far below one millisecond and we demonstrate the real-time capabilities of ORC in a force-control task implemented in Python.",https://ieeexplore.ieee.org/document/8675625/,2019 Third IEEE International Conference on Robotic Computing (IRC),25-27 Feb. 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA.2016.7487351,Object discovery and grasp detection with a shared convolutional neural network,IEEE,Conferences,"Grasp an object from a stack of objects in real-time is still a challenge in robotics. This requires the robot to have the ability of both fast object discovery and grasp detection: a target object should be picked out from the stack first and then a proper grasp configuration is applied to grasp the object. In this paper, we propose a shared convolutional neural network (CNN) which can simultaneously implement these two tasks in real-time. The processing speed of the model is about 100 frames per second on a GPU which largely satisfies the requirement. Meanwhile, we also establish a labeled RGBD dataset which contains scenes of stacked objects for robotic grasping. At last, we demonstrate the implementation of our shared CNN model on a real robotic platform and show that the robot can accurately discover a target object from the stack and successfully grasp it.",https://ieeexplore.ieee.org/document/7487351/,2016 IEEE International Conference on Robotics and Automation (ICRA),16-21 May 2016,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS.2016.7759720,Object identification from few examples by improving the invariance of a Deep Convolutional Neural Network,IEEE,Conferences,"The development of reliable and robust visual recognition systems is a main challenge towards the deployment of autonomous robotic agents in unconstrained environments. Learning to recognize objects requires image representations that are discriminative to relevant information while being invariant to nuisances, such as scaling, rotations, light and background changes, and so forth. Deep Convolutional Neural Networks can learn such representations from large web-collected image datasets and a natural question is how these systems can be best adapted to the robotics context where little supervision is often available. In this work, we investigate different training strategies for deep architectures on a new dataset collected in a real-world robotic setting. In particular we show how deep networks can be tuned to improve invariance and discriminability properties and perform object identification tasks with minimal supervision.",https://ieeexplore.ieee.org/document/7759720/,2016 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),9-14 Oct. 2016,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ROBIO.2009.4913200,Object orientation recognition based on SIFT and SVM by using stereo camera,IEEE,Conferences,"The goal of this research is to recognize an object and its orientation in space by using stereo camera. The principle of object orientation recognition in this paper was based on the scale invariant feature transform (SIFT) and support vector machine (SVM). SIFT has been successfully implemented on object recognition but it had a problem recognizing the object orientation. For many autonomous robotics applications, such as using a vision-guided industrial robot to grab a product, not only correct object recognition will be needed in this process but also object orientation recognition is required. In this paper we used SVM to recognize object orientation. SVM has been known as a promising method for classification accuracy and its generalization ability. The stereo camera system adopted in this research provided more useful information compared to single camera one. The object orientation recognition technique was implemented on an industrial robot in a real application. The proposed camera system and recognition algorithms were used to recognize a specific object and its orientation and then guide the industrial robot to perform some alignment operations on the object.",https://ieeexplore.ieee.org/document/4913200/,2008 IEEE International Conference on Robotics and Biomimetics,22-25 Feb. 2009,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS51168.2021.9635977,On Assessing the Usefulness of Proxy Domains for Developing and Evaluating Embodied Agents,IEEE,Conferences,"In many situations it is either impossible or impractical to develop and evaluate agents entirely on the target domain on which they will be deployed. This is particularly true in robotics, where doing experiments on hardware is much more arduous than in simulation. This has become arguably more so in the case of learning-based agents. To this end, considerable recent effort has been devoted to developing increasingly realistic and higher fidelity simulators. However, we lack any principled way to evaluate how good a ""proxy domain"" is, specifically in terms of how useful it is in helping us achieve our end objective of building an agent that performs well in the target domain. In this work, we investigate methods to address this need. We begin by clearly separating two uses of proxy domains that are often conflated: 1) their ability to be a faithful predictor of agent performance and 2) their ability to be a useful tool for learning. In this paper, we attempt to clarify the role of proxy domains and establish new proxy usefulness (PU) metrics to compare the usefulness of different proxy domains. We propose the relative predictive PU to assess the predictive ability of a proxy domain and the learning PU to quantify the usefulness of a proxy as a tool to generate learning data. Furthermore, we argue that the value of a proxy is conditioned on the task that it is being used to help solve. We demonstrate how these new metrics can be used to optimize parameters of the proxy domain for which obtaining ground truth via system identification is not trivial.",https://ieeexplore.ieee.org/document/9635977/,2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),27 Sept.-1 Oct. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICTAACS48474.2019.8988124,Online Adversarial Planning in μRTS : A Survey,IEEE,Conferences,"Online planning is an important research area focusing on the problem of real-time decision making, using information extracted from the environment. The aim is to compute, at each decision point, the best decision possible that contributes to the realization of a fixed objective. Relevant application domains include robotics, control engineering and computer games. Real-time strategy (RTS) games pose considerable challenges to artificial intelligence techniques, due to their dynamic, complex and adversarial aspects, where online planning plays a prominent role. They also constitute an ideal research platform and test-bed for online planning. μRTS is an open-source AI research platform that features a minimalistic, yet complete RTS implementation, used by AI researchers for developing and testing intelligent RTS game-playing agents. The unique characteristics of μRTS helped for the emergence of interesting online adversarial planning techniques, dealing with multiple levels of abstraction. This paper presents the major μRTS online planning approaches to date, categorized by the degree of abstraction, in fully and partially observable environments.",https://ieeexplore.ieee.org/document/8988124/,2019 International Conference on Theoretical and Applicative Aspects of Computer Science (ICTAACS),15-16 Dec. 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS.2017.8202247,Online learning for human classification in 3D LiDAR-based tracking,IEEE,Conferences,"Human detection and tracking are essential aspects to be considered in service robotics, as the robot often shares its workspace and interacts closely with humans. This paper presents an online learning framework for human classification in 3D LiDAR scans, taking advantage of robust multi-target tracking to avoid the need for data annotation by a human expert. The system learns iteratively by retraining a classifier online with the samples collected by the robot over time. A novel aspect of our approach is that errors in training data can be corrected using the information provided by the 3D LiDAR-based tracking. In order to do this, an efficient 3D cluster detector of potential human targets has been implemented. We evaluate the framework using a new 3D LiDAR dataset of people moving in a large indoor public space, which is made available to the research community. The experiments analyse the real-time performance of the cluster detector and show that our online learned human classifier matches and in some cases outperforms its offline version.",https://ieeexplore.ieee.org/document/8202247/,2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),24-28 Sept. 2017,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/INFOTEH48170.2020.9066274,Ontology-Driven Generation of Interactive 3D Worlds,IEEE,Conferences,"In this paper, the ontology-driven approach to automated generation of interactive 3D applications is presented. The ontologies describing both static and dynamic application properties combined with domain-specific knowledge are leveraged. The approach is evaluated in two case studies: smart city simulator and virtual robotics testbed. The generated output is JavaScript code using Three.js library. According to the achieved results, the adoption of ontologies dramatically speeds up the development time compared to manual process.",https://ieeexplore.ieee.org/document/9066274/,2020 19th International Symposium INFOTEH-JAHORINA (INFOTEH),18-20 March 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IJCNN.2014.6889837,Optimising the overall power usage on the SpiNNaker neuromimetic platform,IEEE,Conferences,"Simulations of biological tissue have been extensively used to replicate phenomena observed by in-vivo and in-vitro experiments as an alternative methodology for explaining how computations could take place in a brain region. Additional benefits of simulated neural networks over in-vivo experiments include greater observability, experimental control and reproducibility. General-purpose supercomputers provide the computational power and parallelism required to implement highly complex neural models, but this comes at the expense of high power requirements and communication overheads. Moreover, there are certain cases where real-time simulation performance is a desirable feature, for example in the field of cognitive robotics where embodied agents need to interact with their environment through biologically inspired asynchronous sensors. The SpiNNaker neuromimetic platform is a scalable architecture that has been designed to enable energy-efficient, large-scale simulations of spiking neurons in biological realtime. This work is based on a recent study which revealed that while they are generally energy efficient, SpiNNaker chips dissipate significant amount of power whilst in the idle state. In this paper we perform a systematic investigation into the overall energy consumption of a SpiNNaker system and propose a number of optimised suspend modes in order to reduce this. The proposed implementation is 60% more energy efficient in the idle state, 50% in the uploading and 52% in the downloading phases, while the power dissipation of the whole simulation is reduced by 52%. For demonstration purposes, we run a neural network simulation comprising thousands of neurons and millions of complex synapses on a 48-chip SpiNNaker board, generating millions of synaptic events per second.",https://ieeexplore.ieee.org/document/6889837/,2014 International Joint Conference on Neural Networks (IJCNN),6-11 July 2014,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/FormaliSE.2019.00012,Parallelizable Reachability Analysis Algorithms for Feed-Forward Neural Networks,IEEE,Conferences,"Artificial neural networks (ANN) have displayed considerable utility in a wide range of applications such as image processing, character and pattern recognition, self-driving cars, evolutionary robotics, and non-linear system identification and control. While ANNs are able to carry out complicated tasks efficiently, they are susceptible to unpredictable and errant behavior due to irregularities that emanate from their complex non-linear structure. As a result, there have been reservations about incorporating them into safety-critical systems. In this paper, we present a reachability analysis method for feed-forward neural networks (FNN) that employ rectified linear units (ReLUs) as activation functions. The crux of our approach relies on three reachable-set computation algorithms, namely exact schemes, lazy-approximate schemes, and mixing schemes. The exact scheme computes an exact reachable set for FNN, while the lazy-approximate and mixing schemes generate an over-approximation of the exact reachable set. All schemes are designed efficiently to run on parallel platforms to reduce the computation time and enhance the scalability. Our methods are implemented in a toolbox called, NNV, and is evaluated using a set of benchmarks that consist of realistic neural networks with sizes that range from tens to a thousand neurons. Notably, NNV successfully computes and visualizes the exact reachable sets of the real world ACAS Xu deep neural networks (DNNs), which are a variant of a family of novel airborne collision detection systems known as the ACAS System X, using a representation of tens to hundreds of polyhedra.",https://ieeexplore.ieee.org/document/8807491/,2019 IEEE/ACM 7th International Conference on Formal Methods in Software Engineering (FormaliSE),27-27 May 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS51168.2021.9635967,Particle MPC for Uncertain and Learning-Based Control,IEEE,Conferences,"As robotic systems move from highly structured environments to open worlds, incorporating uncertainty from dynamics learning or state estimation into the control pipeline is essential for robust performance. In this paper we present a nonlinear particle model predictive control (PMPC) approach to control under uncertainty, which directly incorporates any particle-based uncertainty representation, such as those common in robotics. Our approach builds on scenario methods for MPC, but in contrast to existing approaches, which either constrain all or only the first timestep to share actions across scenarios, we investigate the impact of a partial consensus horizon. Implementing this optimization for nonlinear dynamics by leveraging sequential convex optimization, our approach yields an efficient framework that can be tuned to the particular information gain dynamics of a system to mitigate both over-conservatism and over-optimism. We investigate our approach for two robotic systems across three problem settings: time-varying, partially observed dynamics; sensing uncertainty; and model-based reinforcement learning, and show that our approach improves performance over baselines in all settings.",https://ieeexplore.ieee.org/document/9635967/,2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),27 Sept.-1 Oct. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISCID.2016.1055,Path Planning for UUV in Dynamic Environment,IEEE,Conferences,"From naval operations to ocean science missions, the importance of autonomous vehicles is increasing with the advances in underwater robotics technology. Due to the dynamic and intermittent underwater environment and physical limitations of underwater unmanned vehicle (UUV), feasible and optimal path planning is crucial for autonomous underwater operations. According to different mission, the path planning method of UUV is divided into two categories: the point to point path planning and the complete coverage path planning. The objective of this thesis is to develop and demonstrate an efficient underwater path planning method that is adapted to complicated ocean environment. In this thesis, existing path planning method for the fields of ocean science and robotics are first reviewed, and then local dynamic obstacle avoidance method is proposed to avoid dynamic obstacles. Based on this again, the path planning of UUV in local dynamic environment can be efficiently implemented by adopting rolling window path planning method and local dynamic obstacle avoidance method. This method with the guide point strategy combines global path planning with local dynamic path planning, so that not only the requirements of real-time on-line path planning for UUV are met, the global optimality is also considered. A navigation route for UUV is planned in advance by using priori environmental information based on ant colony algorithm, so it provides the reference information for the selection of guide point. In order to solved the problem of area coverage search, a complete coverage path planning method is proposed by combining ant colony algorithm with biologically inspires neural network. In order to demonstrate underwater path planning method, all of the above ideas and methods developed were tested in simulation experiments.",https://ieeexplore.ieee.org/document/7830329/,2016 9th International Symposium on Computational Intelligence and Design (ISCID),10-11 Dec. 2016,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CRV52889.2021.00019,PathBench: A Benchmarking Platform for Classical and Learned Path Planning Algorithms,IEEE,Conferences,"Path planning is a key component in mobile robotics. A wide range of path planning algorithms exist, but few attempts have been made to benchmark the algorithms holistically or unify their interface. Moreover, with the recent advances in deep neural networks, there is an urgent need to facilitate the development and benchmarking of such learning-based planning algorithms. This paper presents PathBench, a platform for developing, visualizing, training, testing, and benchmarking of existing and future, classical and learned 2D and 3D path planning algorithms, while offering support for Robot Operating System (ROS). Many existing path planning algorithms are supported; e.g. A*, wavefront, rapidly-exploring random tree, value iteration networks, gated path planning networks; and integrating new algorithms is easy and clearly specified. We demonstrate the benchmarking capability of PathBench by comparing implemented classical and learned algorithms for metrics, such as path length, success rate, computational time and path deviation. These evaluations are done on built-in PathBench maps and external path planning environments from video games and real world databases. PathBench is open source <sup>1</sup>.",https://ieeexplore.ieee.org/document/9469507/,2021 18th Conference on Robots and Vision (CRV),26-28 May 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/FPA.1994.636094,Perception systems implemented in analog VLSI for real-time applications,IEEE,Conferences,"We point out that analog VLSI can now be considered as the ideal medium to implement computational systems intended to carry out real time perceptive or even cognitive tasks that are not well handled by traditional computers. By exploiting the analog features of the transistors, only a few devices are needed to realise most of the elementary functions required to implement perceptive systems, resulting in very dense, sophisticated circuits and low power consumption. Elementary artificial retinas in silicon based on their biological counterparts have already been successfully used in industrial applications. Artificial cochleas and noses are also under development. This new enabling technology is of great interest over a wide range of industrial sectors, including robotics, automotive, surveillance and food industry.",https://ieeexplore.ieee.org/document/636094/,Proceedings of PerAc '94. From Perception to Action,7-9 Sept. 1994,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA46639.2022.9812287,Prediction of Metacarpophalangeal Joint Angles and Classification of Hand Configurations Based on Ultrasound Imaging of the Forearm,IEEE,Conferences,"With the advancement in computing and robotics, it is necessary to develop fluent and intuitive methods for inter-acting with digital systems, augmented/virtual reality (AR/VR) interfaces, and physical robotic systems. Hand movement recognition is widely used to enable such interaction. Hand configuration classification and metacarpophalangeal (MCP) joint angle detection are important for a comprehensive reconstruction of hand motion. Surface electromyography (sEMG) and other technologies have been used for the detection of hand motions. Ultrasound images of the forearm offer a way to visualize the internal physiology of the hand from a musculoskeletal perspective. Recent works have shown that these images can be classified using machine learning to predict various hand configurations. In this paper, we propose a Convolutional Neu-ral Network (CNN) based deep learning pipeline for predicting the MCP joint angles. We supplement our results by using a Support Vector Classifier (SVC) to classify the ultrasound information into several predefined hand configurations based on activities of daily living (ADL). Ultrasound data from the forearm were obtained from six subjects who were instructed to move their hands according to predefined hand configurations relevant to ADLs. Motion capture data was acquired as the ground truth for hand movements at three speeds (0.5 Hz, 1 Hz, and 2 Hz) for the index, middle, ring, and pinky fingers. We demonstrated the perfect prediction of hand configurations through SVC classification and a correspondence between the predicted MCP joint angles and the actual MCP joint angles for the fingers, with an average root mean square error of 7.35 degrees. A low latency (6.25 &#x2013; 9.10 Hz) pipeline was implemented for the prediction of both MCP joint angles and hand configuration estimation aimed for real-time implementation.",https://ieeexplore.ieee.org/document/9812287/,2022 International Conference on Robotics and Automation (ICRA),23-27 May 2022,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/RO-MAN46459.2019.8956461,Privacy First: Designing Responsible and Inclusive Social Robot Applications for in the Wild Studies,IEEE,Conferences,"Deploying social robots applications in public spaces for conducting in the wild studies is a significant challenge but critical to the advancement of social robotics. Real world environments are complex, dynamic, and uncertain. Human-Robot interactions can be unstructured and unanticipated. In addition, when the robot is intended to be a shared public resource, management issues such as user access and user privacy arise, leading to design choices that can impact on users' trust and the adoption of the designed system. In this paper we propose a user registration and login system for a social robot and report on people's preferences when registering their personal details with the robot to access services. This study is the first iteration of a larger body of work investigating potential use cases for the Pepper social robot at a government managed centre for startups and innovation. We prototyped and deployed a system for user registration with the robot, which gives users control over registering and accessing services with either face recognition technology or a QR code. The QR code played a critical role in increasing the number of users adopting the technology. We discuss the need to develop social robot applications that responsibly adhere to privacy principles, are inclusive, and cater for a broad spectrum of people.",https://ieeexplore.ieee.org/document/8956461/,2019 28th IEEE International Conference on Robot and Human Interactive Communication (RO-MAN),14-18 Oct. 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/GloSIC.2018.8570124,Probabilistic Estimations of Increasing Expected Reliability and Safety for Intelligent Manufacturing,IEEE,Conferences,"In the near future the possibilities of the modern probabilistic models, artificial intelligence and machine learning methods can provide an intelligent support of making decisions by an operator in real time. An agile recovery of intelligent manufacturing integrity can be implemented owing to the development of industrial robotics. For intelligent manufacturing it means the expected reliability and safety may be in the near future at the expense of intelligent support of decision making and the agile recovery of integrity. To answer the question “How much essential may be this increasing?” here are proposed: general analytical approaches for a probabilistic estimation of the expected reliability and safety for every monitored element or the system of intelligent manufacturing on a level of probability distribution functions (PDF) of the time between the losses of system integrity; estimations of increasing the expected reliability and safety for intelligent manufacturing at the expense of the intelligent support of decision making and agile recovery of integrity; the comparisons of the estimations on a prognostic period up to 10 years using the identical model in applications to expected reliability and safety. The applications of the proposed approaches allow the customers, designers, developers, users and experts of Industry 4.0 intelligent manufacturing to be guided by the proposed probabilistic estimations for solving problems of reliability and safety in the system life cycle. The results are demonstrated by examples.",https://ieeexplore.ieee.org/document/8570124/,2018 Global Smart Industry Conference (GloSIC),13-15 Nov. 2018,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/UPCON52273.2021.9667555,Probabilistic Modeling of Human Locomotion for Biped Robot Trajectory Generation,IEEE,Conferences,"The wheel-type robot has found numerous applications in hospitals, restaurants, entertainment, the automation industry, etc., and shows its applicability in solving the tasks efficiently. However, it failed to achieve the same efficiency in an unstructured environment that is mostly found in the real world. Thus, a biped robot can replace the wheel-type robot for better performance. The biped robot has many joints which make it a complex higher degree of freedom system. Hence, the designing of the controller, reference trajectory generation, state estimation and, filter design for feedback signal is a very cumbersome task. This paper focuses on the generation of the reference trajectories. Since human locomotion is optimal naturally, therefore, the human data is used for this study, which is collected at Robotics and Machine ANalytics (RAMAN) Lab, MNIT, Jaipur, India. In the literature, various authors have implemented model-based learning methods to develop a model based on data. However, these models suffer from model bias i.e., it is assumed that learned model accurately define the real system. Therefore, in this paper, the authors have proposed probabilistic models to model the human locomotion data. The reference trajectory is generated using the Bayesian ridge regression, Automatic relevance determination regression, and Gaussian process regression. The performance evaluation of developed models are based on average error, maximum error, root mean square error, and percentage normalized root mean square error.",https://ieeexplore.ieee.org/document/9667555/,"2021 IEEE 8th Uttar Pradesh Section International Conference on Electrical, Electronics and Computer Engineering (UPCON)",11-13 Nov. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRC.2016.7738697,Processor-in-memory support for artificial neural networks,IEEE,Conferences,"Hardware acceleration of artificial neural network (ANN) processing has potential for supporting applications benefiting from real time and low power operation, such as autonomous vehicles, robotics, recognition and data mining. Most interest in ANNs targets acceleration of deep multi-layered ANNs that can require days of offline training to converge on a desired network behavior. Interest has grown in ANNs capable of supporting unsupervised training, where networks can learn new information from unlabeled data dynamically without the need for offline training. These ANNs require large memories with bandwidths much higher than supported in modern GPGPUs. Custom hardware acceleration and memory co-design holds the potential to provide real-time performance in cases where the performance requirements cannot be met by modern GPGPUs. This work presents a custom processor solution to accelerate two hetero-associative memories (Sparsey and HTM) capable of unsupervised and one-hot learning. This custom processor is implemented as an expandable ASIP built upon a configurable SIMD engine for exploiting parallelism. Functional specialization is implemented utilizing processor-in-memory techniques, which results in up to a 20× speedup and a 2000× reduction in energy per frame compared to a software implementation operating on a dataset for recognition of human actions.",https://ieeexplore.ieee.org/document/7738697/,2016 IEEE International Conference on Rebooting Computing (ICRC),17-19 Oct. 2016,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRoM.2015.7367861,ReMoRo; A mobile robot platform based on distributed I/O modules for research and education,IEEE,Conferences,"We present our recent work on the electrical and hardware design of the mobile robot platform ReMoRo that is based on distributed input/output modules. We have designed three generation of this platform with different specifications, which it help us to design more compatible and applied mobile robot. Nevertheless, the goal of this project was to develop a low-cost and robust but extensible modular robot platform for research and educational purposes. In this paper we describe a new affordable robot structure that enables large-scale innovative, new curriculum, multi robot research and multi-robotics outreach to computer and artificial intelligent students. We introduce the ReMoRo platform, which offers a balance between capabilities, accessibility, cost and an opendesign. All of electrical devices like sensors module, motor drivers and device communication manager are designed based on ARM Cortex M3 microcontrollers that runs under Real-Time Operating System (freeRTOS) for manages each modules internal scheduling and activation control in communication bus. With a range of different sensors, cylindrical manipulator and omnidirectional locomotion system, RoMeRo can interact with environment in multiple ways, handle common objects and therefore be used in various service robot scenarios like warehouse robots or multi agent mobile robots. We demonstrate the usability of our concept by quantifying the object-handling task and also briefly describe the software design based on ROS framework for educational usage.",https://ieeexplore.ieee.org/document/7367861/,2015 3rd RSI International Conference on Robotics and Mechatronics (ICROM),7-9 Oct. 2015,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA48506.2021.9562075,Reaching Pruning Locations in a Vine Using a Deep Reinforcement Learning Policy,IEEE,Conferences,"We outline a neural network-based pipeline for perception, control and planning of a 7 DoF robot for tasks that involve reaching into a dormant grapevine canopy. The proposed system consists of a 6 DoF industrial robot arm and a linear slider that can actuate on an entire grape vine. Our approach uses Convolutional Neural Networks to detect buds in dormant grape vines and a Reinforcement Learning based control strategy to reach desired cut-point locations for pruning tasks. Within this framework, three methodologies are developed and compared to reach the desired locations: the learned policy-based approach (RL), a hybrid method that uses the learned policy and an inverse kinematics solver (RL+IK), and lastly a classical approach commonly used in robotics. We first tested and validated the suitability of the proposed learning methodology in a simulated environment that resembled laboratory conditions. A reaching accuracy of up to 61.90% and 85.71% for the RL and RL+IK approaches respectively was obtained for a vine that the agent observed while learning. When testing in a new vine, the accuracy was up to 66.66% and 76.19% for RL and RL+IK, respectively. The same methods were then deployed on a real system in an end to end procedure: autonomously scan the vine using a vision system, create its model and finally use the learned policy to reach cutting points. The reaching accuracy obtained in these tests was 73.08%.",https://ieeexplore.ieee.org/document/9562075/,2021 IEEE International Conference on Robotics and Automation (ICRA),30 May-5 June 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/I2CT54291.2022.9825287,Real Time Protein Crystal Monitoring System,IEEE,Conferences,"High-resolution crystal structures of the biological macromolecules (protein, DNA, RNA or their complexes) are essential for understanding their biological functions and causes of diseases at a molecular level. X-ray crystallographic technique gives the atomic positions of these biological macromolecules and decodes the intermolecular interactions between protein-drug, protein-protein, protein-nucleic acids which are crucial for rational drug design. To aid the macromolecular crystallization experiments, we have indigenously developed automated system, called Real Time Protein Crystal Monitoring System (RT-PCMS) for crystal imaging and to monitor the progress of protein crystal growth during crystallization. RT-PCMS images crystallization drops at frequent time intervals in 24- or 96-well crystallization plate formats. The system consists of precise robotics motion and a custom designed motorized microscopic imaging system, capturing multi-focus composite images of protein crystals in droplets in multiple wells. In a typical successful trial, the crystallization drop comprises protein crystals of size 20 to 300 microns located at the different regions of an elliptical droplet. We have implemented powerful image processing and deep learning algorithms in this system, fusing multi-depth crystal images within a crystallization drop and classification into different crystallization stages. The system is controlled through a personal computer that provides a custom developed graphical user interface software for visualization of all captured images at different crystallization stages and record the results of crystal formation.",https://ieeexplore.ieee.org/document/9825287/,2022 IEEE 7th International conference for Convergence in Technology (I2CT),7-9 April 2022,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/TENCON.1991.753906,Real Time Video Frame Grabber,IEEE,Conferences,Computer vision forms one of the most fascinating and rapidly developing fields of Artificial Intelligence with major applications in areas like Robotics and Image Processing.,https://ieeexplore.ieee.org/document/753906/,"TENCON '91. Region 10 International Conference on EC3-Energy, Computer, Communication and Control Systems",28-30 Aug. 1991,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCCE.2008.4580693,Real time implementation of NARMA L2 feedback linearization and smoothed NARMA L2 controls of a single link manipulator,IEEE,Conferences,"Robotics is a field of modern technology which requires knowledge in vast areas such as electrical engineering, mechanical engineering, computer science as well as finance. Nonlinearities and parametric uncertainties are unavoidable problems faced in controlling robots in industrial plants. Tracking control of a single link manipulator driven by a permanent magnet brushed DC motor is a nonlinear dynamics due to effects of gravitational force, mass of the payload, posture of the manipulator and viscous friction coefficient. Furthermore uncertainties arise because of changes of the rotor resistance with temperature and random variations of friction while operating. Due to this fact classical PID controller can not be used effectively since it is developed based on linear system theory. Neural network control schemes for manipulator control problem have been proposed by researchers; in which their competency is validated through simulation studies. On the other hand, actual real time applications are rarely established. Instead of simulation studies, this paper is aimed to implement neural network controller in real time for controlling a DC motor driven single link manipulator. The work presented in this paper is concentrating on neural NARMA L2 control and its improvement called to as Smoothed NARMA L2 control. As proposed by K. S Narendra and Mukhopadhyay, Narma L2 control is one of the popular neural network architectures for prediction and control. The real time experimentation showed that the Smoothed NARMA L2 is effective for controlling the single link manipulator for both point-to-point and continuous path motion control.",https://ieeexplore.ieee.org/document/4580693/,2008 International Conference on Computer and Communication Engineering,13-15 May 2008,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRAI.2012.6413407,Real time localization of mobile robotic platform via fusion of Inertial and Visual Navigation System,IEEE,Conferences,"Inertial Navigation System (INS) is one of the most important component of a mobile robotic platform, be it ground or air based. It is used to localize the mobile robotic platform in the real world and identify its location in terms of latitudes and longitudes or other related coordinate systems. Highly accurate and precise INS is quite expensive and is therefore not suitable for more general purpose applications. It is, therefore, a standard approach in mobile robotics to use a low grade commercial INS coupled with another navigation device to provide a more accurate triangulation. Generally, INS and Global Positioning System (GPS) are integrated using Kalman Filters to provide accurate localization information about the mobile robots. Although, in certain scenarios, the mobile robot is not able to acquire a GPS fix for long durations of time especially when navigating in indoor environments or in areas with inadequate GPS satellite coverage. In such cases, an additional source of location fix is required. This paper describes an accurate and stable data fusion filter which integrates the position of a mobile robot from a Visual Navigation System (VNS) with the position from an INS to accurately localize the robot in absence of GPS data. This research proposes a seven error states model and uses it in Kalman Filter for data fusion. The filter is tuned and tested using dynamic and static data from INS and VNS. Simulation and experimentation results show that the seven error states model based Kalman Filter provides a good balance between accuracy, robustness and processing efficiency for a real time implementation. Experiments also show that in absence of GPS data only a couple of fixes from the VNS are sufficient to quickly correct the position of the mobile robotic platform and three fixes at different times are sufficient for velocity correction of INS.",https://ieeexplore.ieee.org/document/6413407/,2012 International Conference of Robotics and Artificial Intelligence,22-23 Oct. 2012,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICTAI.2008.143,Real-Time Classification of Streaming Sensor Data,IEEE,Conferences,"The last decade has seen a huge interest in classification of time series. Most of this work assumes that the data resides in main memory and is processed offline. However, recent advances in sensor technologies require resource-efficient algorithms that can be implemented directly on the sensors as real-time algorithms. We show how a recently introduced framework for time series classification, time series bitmaps, can be implemented as efficient classifiers which can be updated in constant time and space in the face of very high data arrival rates. We describe results from a case study of an important entomological problem, and further demonstrate the generality of our ideas with an example from robotics.",https://ieeexplore.ieee.org/document/4669683/,2008 20th IEEE International Conference on Tools with Artificial Intelligence,3-5 Nov. 2008,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA.2019.8794220,Real-Time Joint Semantic Segmentation and Depth Estimation Using Asymmetric Annotations,IEEE,Conferences,"Deployment of deep learning models in robotics as sensory information extractors can be a daunting task to handle, even using generic GPU cards. Here, we address three of its most prominent hurdles, namely, i) the adaptation of a single model to perform multiple tasks at once (in this work, we consider depth estimation and semantic segmentation crucial for acquiring geometric and semantic understanding of the scene), while ii) doing it in real-time, and iii) using asymmetric datasets with uneven numbers of annotations per each modality. To overcome the first two issues, we adapt a recently proposed real-time semantic segmentation network, making changes to further reduce the number of floating point operations. To approach the third issue, we embrace a simple solution based on hard knowledge distillation under the assumption of having access to a powerful `teacher' network. We showcase how our system can be easily extended to handle more tasks, and more datasets, all at once, performing depth estimation and segmentation both indoors and outdoors with a single model. Quantitatively, we achieve results equivalent to (or better than) current state-of-the-art approaches with one forward pass costing just 13ms and 6.5 GFLOPs on 640×480 inputs. This efficiency allows us to directly incorporate the raw predictions of our network into the SemanticFusion framework [1] for dense 3D semantic reconstruction of the scene.",https://ieeexplore.ieee.org/document/8794220/,2019 International Conference on Robotics and Automation (ICRA),20-24 May 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA48506.2021.9560749,Real-Time Mesh Extraction from Implicit Functions via Direct Reconstruction of Decision Boundary,IEEE,Conferences,"The ability to estimate 3D object shape from a single image is vital to robotics and manufacturing. For instance, it enables iterative trial-and-error in simulated environments. In single-view reconstruction, implicit functions have demonstrated superior results over traditional methods. However, implicit functions suffer from the heavy computation of mesh extraction. This is due to the indirect mesh extraction, where the number of evaluation points grows cubically with resolution. On the other hand, reducing the resolution results in the discretization error of marching cubes (MC). In this work, we aim to perform efficient and accurate mesh extraction from implicit functions. The idea is to directly reconstruct the decision boundary of implicit functions as a mesh by reverse tracing from the output. It eliminates the need for evaluating massive points and error-prone MC. Consequently, we propose implementing an implicit function via a composite function of a flow and Binary-coded Input Neural Network (BCINN). The boundary of BCINN is easily identifiable, and the flow is invertible. Owing to these properties, the decision boundary of the composite function can be directly and efficiently reconstructed. In our experiments, we demonstrate that the proposed method significantly improves runtime/memory efficiency, with results comparable to those of existing methods. Specifically, our method enables real-time high-quality mesh inference from a single image.",https://ieeexplore.ieee.org/document/9560749/,2021 IEEE International Conference on Robotics and Automation (ICRA),30 May-5 June 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS45743.2020.9341473,Real-World Human-Robot Collaborative Reinforcement Learning,IEEE,Conferences,"The intuitive collaboration of humans and intelligent robots (embodied AI) in the real-world is an essential objective for many desirable applications of robotics. Whilst there is much research regarding explicit communication, we focus on how humans and robots interact implicitly, on motor adaptation level. We present a real-world setup of a human-robot collaborative maze game, designed to be non-trivial and only solvable through collaboration, by limiting the actions to rotations of two orthogonal axes, and assigning each axes to one player. This results in neither the human nor the agent being able to solve the game on their own. We use deep reinforcement learning for the control of the robotic agent, and achieve results within 30 minutes of real-world play, without any type of pre-training. We then use this setup to perform systematic experiments on human/agent behaviour and adaptation when co-learning a policy for the collaborative game. We present results on how co-policy learning occurs over time between the human and the robotic agent resulting in each participant's agent serving as a representation of how they would play the game. This allows us to relate a person's success when playing with different agents than their own, by comparing the policy of the agent with that of their own agent.",https://ieeexplore.ieee.org/document/9341473/,2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),24 Oct.-24 Jan. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/EFTA.2007.4416888,Real-time architecture for mobile assistant robots,IEEE,Conferences,"Mobile robotics is a challenging research area, with produced results that were unthinkable several years ago. There exist algorithms and methods capable of performing difficult tasks such as detect/classify objects, skill learning and SLAM. From the initial design steps, the real-time software architecture of a robotic platform requires great attention. The problem is difficult, because various components, such as sensing, perception, localization, and motor control, are required to operate and interact in real-time. This makes the system a very complex one. This paper presents a real-time control architecture designed for mobile robots and intelligent vehicles. Moreover, an example of application of the control structure consisting on a system for learning to classify places, using laser range data, is reported.",https://ieeexplore.ieee.org/document/4416888/,2007 IEEE Conference on Emerging Technologies and Factory Automation (EFTA 2007),25-28 Sept. 2007,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/RTOSS.1994.292553,Real-time platforms and environments for time constrained flexible manufacturing,IEEE,Conferences,"The Spring Kernel and associated algorithms, languages, and tools provide system support for static or dynamic real-time applications that require predictable operation. Spring currently consists of two major parts: (1) the development environment, where application and target systems are described, preprocessed and downloaded, and (2) the run-time environment, where the operating system, the Spring Kernel, creates and ensures predictable executions of application tasks. We have integrated our real-time systems technology with component technologies from robotics, computer vision, and real-time artificial intelligence, to develop a test platform for flexible manufacturing. The results being produced are generic so that they should be in many other real-time applications such as air traffic control and chemical plants. We describe this platform, identify new features developed, and comment on some lessons learned to date from this experiment.<>",https://ieeexplore.ieee.org/document/292553/,Proceedings of 11th IEEE Workshop on Real-Time Operating Systems and Software,18-19 May 1994,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISSCC42614.2022.9731734,ReckOn: A 28nm Sub-mm2 Task-Agnostic Spiking Recurrent Neural Network Processor Enabling On-Chip Learning over Second-Long Timescales,IEEE,Conferences,"The robustness of autonomous inference-only devices deployed in the real world is limited by data distribution changes induced by different users, environments, and task requirements. This challenge calls for the development of edge devices with an always-on adaptation to their target ecosystems. However, the memory requirements of conventional neural-network training algorithms scale with the temporal depth of the data being processed, which is not compatible with the constrained power and area budgets at the edge. For this reason, previous works demonstrating end-to-end on-chip learning without external memory were restricted to the processing of static data such as images [1]&#x2013;[4], or to instantaneous decisions involving no memory of the past, e.g. obstacle avoidance in mobile robots [5]. The ability to learn short-to-long-term temporal dependencies on-chip is a missing enabler for robust autonomous edge devices in applications such as gesture recognition, speech processing, and cognitive robotics.",https://ieeexplore.ieee.org/document/9731734/,2022 IEEE International Solid- State Circuits Conference (ISSCC),20-26 Feb. 2022,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCCT.2010.5640434,Recognizing & interpreting Indian Sign Language gesture for Human Robot Interaction,IEEE,Conferences,"This paper describes a novel approach towards recognizing of Indian Sign Language (ISL) gestures for Humanoid Robot Interaction (HRI). An extensive approach is being introduced for classification of ISL gesture which imparts an elegant way of interaction between humanoid robot HOAP-2 and human being. ISL gestures are being considered as a communicating agent for humanoid robot which is being used in this context explicitly. It involves different image processing techniques followed by a generic algorithm for feature extraction process. The classification technique deals with the Euclidean distance metric. The concrete HRI system has been established for initiation based learning mechanism. The Real time robotics simulation software, WEBOTS has been adopted to simulate the classified ISL gestures on HOAP-2 robot. The JAVA based software has been developed to deal with the entire HRI process.",https://ieeexplore.ieee.org/document/5640434/,2010 International Conference on Computer and Communication Technology (ICCCT),17-19 Sept. 2010,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SNPD.2017.8022704,Recovering camera motion from points and lines in stereo images: A recursive model-less approach using trifocal tensors,IEEE,Conferences,"Estimating the 3-D motion of a moving camera from images is a common task in robotics and augmented reality. Most existing marker-less approaches make use of either points or lines. Taking the advantages of both kinds of features in an unknown environment is more attractive due to their availability and differences in characteristics. A novel model-less method is presented in this paper to tackle the 3-D motion tracking problem. Two Bayesian filters, one for point measurements while another for line measurements, are embedded in the Interacting Probabilistic Switching (IPS) framework. They compensate for the weaknesses in one another by utilizing both kinds of features in the stereo images. The proposed method is able to obtain the 3D motion given as little as two line or two point correspondences in consecutive images with the use of multiple trifocal tensors. Our method outperformed two recent methods in terms of accuracy and the problem of drifting was very little in real scenarios.",https://ieeexplore.ieee.org/document/8022704/,"2017 18th IEEE/ACIS International Conference on Software Engineering, Artificial Intelligence, Networking and Parallel/Distributed Computing (SNPD)",26-28 June 2017,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICARM52023.2021.9536145,Reducing the Dimension of the Configuration Space with Self Organizing Neural Networks,IEEE,Conferences,"For robotics, especially industrial applications, it is crucial to reactively plan safe motions through efficient algorithms. Planning is more powerful in the configuration space than the task space. However, for robots with many degrees of freedom, this is challenging and computationally expensive. Sophisticated techniques for motion planning such as the Wavefront algorithm are limited by the high dimensionality of the configuration space, especially for robots with many degrees of freedom. For a neural implementation of the Wavefront algorithm in the configuration space, neurons represent discrete configurations and synapses are used for path planning. In order to decrease the complexity, we reduce the search space by pruning superfluous neurons and synapses. We present different models of self-organizing neural networks for this reduction. The approach takes real-life human motion data as input and creates a representation with reduced dimension. We compare six different neural network models and adapt the Wavefront algorithm to the different structures of the reduced output spaces. The method is backed up by an extensive evaluation of the reduced spaces, including their suitability for path planning by the Wavefront algorithm.",https://ieeexplore.ieee.org/document/9536145/,2021 6th IEEE International Conference on Advanced Robotics and Mechatronics (ICARM),3-5 July 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/DIS.2006.63,Remote Programming of Multirobot Systems within the UPC-UJI Telelaboratories: System Architecture and Agent-Based Multirobot Control,IEEE,Conferences,"One of the areas that needs more improvement within the E-Learning environments via Internet (in fact they suppose a very big effort to be accomplished) is allowing students to access and practice real experiments is a real laboratory, instead of using simulations [1]. Real laboratories allow students to acquire methods, skills and experience related to real equipment, in a manner that is very close to the way they are being used in industry. The purpose of the project is the study, development and implementation of an E-Learning environment to allow undergraduate students to practice subjects related to Robotics and Artificial Intelligence. The system, which is now at a preliminary stage, will allow the remote experimentation with real robotic devices (i.e. robots, cameras, etc.). It will enable the student to learn in a collaborative manner (remote participation with other students) where it will be possible to combine the onsite activities (performed ""in-situ"" within the real lab during the normal practical sessions), with the ""online"" one (performed remotely from home via the Internet). Moreover, the remote experiments within the E-Laboratory to control the real robots can be performed by both, students and even scientist. This project is under development and it is carried out jointly by two Universities (UPC and UJI). In this article we present the system architecture and the way students and researchers have been able to perform a Remote Programming of Multirobot Systems via web.",https://ieeexplore.ieee.org/document/1633445/,IEEE Workshop on Distributed Intelligent Systems: Collective Intelligence and Its Applications (DIS'06),15-16 June 2006,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA46639.2022.9812394,RepAr-Net: Re-Parameterized Encoders and Attentive Feature Arsenals for Fast Video Denoising,IEEE,Conferences,"Real-time video denoising finds applications in several fields like mobile robotics, satellite television, and surveillance systems. Traditional denoising approaches are more common in such systems than their deep learning-based counterparts despite their inferior performance. The large size and heavy computational requirements of neural network-based denoising models pose a serious impediment to their deployment in real-time applications. In this paper, we propose RepAr-Net, a simple yet efficient architecture for fast video de noising. We propose to use temporally separable encoders to generate feature maps called arsenals that can be cached for reuse. We also incorporate re-parameterizable blocks that improve the representative power of the network without affecting the run-time. We benchmark our model on the Set-8 and 2017 DAVIS-Test datasets. Our model achieves state-of-the-art results with up to 29.62&#x0025; improvement in PSNR and a 50&#x0025; decrease in run times over existing methods. Our codes are open-sourced at: github.com/spider-tronix/RepAr-Net.",https://ieeexplore.ieee.org/document/9812394/,2022 International Conference on Robotics and Automation (ICRA),23-27 May 2022,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS.1997.656813,RoboCup as a research program,IEEE,Conferences,"An overview of RoboCup (The World Cup Robot Soccer), which offers opportunities for AI and robotics research by providing an attractive but formidable challenge. It also provides a range of challenge programs which is designed to evaluate specific technical issues. The challenge program will be up-dated and new challenges will be offered as technology progresses. Along with other programs such as an education program, RoboCup offers a comprehensive research program which promotes AI and robotics.",https://ieeexplore.ieee.org/document/656813/,Proceedings of the 1997 IEEE/RSJ International Conference on Intelligent Robot and Systems. Innovative Robotics for Real-World Applications. IROS '97,11-11 Sept. 1997,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS40897.2019.8968306,Robot Learning via Human Adversarial Games,IEEE,Conferences,"Much work in robotics has focused on “humanin-the-loop” learning techniques that improve the efficiency of the learning process. However, these algorithms have made the strong assumption of a cooperating human supervisor that assists the robot. In reality, human observers tend to also act in an adversarial manner towards deployed robotic systems. We show that this can in fact improve the robustness of the learned models by proposing a physical framework that leverages perturbations applied by a human adversary, guiding the robot towards more robust models. In a manipulation task, we show that grasping success improves significantly when the robot trains with a human adversary as compared to training in a self-supervised manner.",https://ieeexplore.ieee.org/document/8968306/,2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),3-8 Nov. 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ROBOT.2001.933270,Robotic Antarctic meteorite search: outcomes,IEEE,Conferences,"Automation of the search for and classification of Antarctic meteorites offers a unique case for early demonstration of robotics in a scenario analogous to geological exploratory missions to other planets and to the Earth's extremes. Moreover, the discovery of new meteorite samples is of great value because meteorites are the only significant source of extraterrestrial material available to scientists. In this paper we focus on the primary outcomes and technical lessons learned from the first field demonstration of autonomous search and in situ classification of Antarctic meteorites by a robot. Using a novel autonomous control architecture, specialized science sensing, combined manipulation and visual servoing, and Bayesian classification, the Nomad robot classified five indigenous meteorites during an expedition to the remote site of Elephant Moraine in January 2000. Nomad's expedition proved the rudiments of science autonomy and exemplified the merits of machine learning techniques for autonomous geological classification in real-world settings. On the other hand, the expedition showcased the difficulty in executing reliable robotic deployment of science sensors and a limited performance in the speed and coverage of autonomous search.",https://ieeexplore.ieee.org/document/933270/,Proceedings 2001 ICRA. IEEE International Conference on Robotics and Automation (Cat. No.01CH37164),21-26 May 2001,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS.2015.7354310,Robotic agents capable of natural and safe physical interaction with human co-workers,IEEE,Conferences,"Many future application scenarios of robotics envision robotic agents to be in close physical interaction with humans: On the factory floor, robotic agents shall support their human co-workers with the dull and health threatening parts of their jobs. In their homes, robotic agents shall enable people to stay independent, even if they have disabilities that require physical help in their daily life - a pressing need for our aging societies. A key requirement for such robotic agents is that they are safety-aware, that is, that they know when actions may hurt or threaten humans and actively refrain from performing them. Safe robot control systems are a current research focus in control theory. The control system designs, however, are a bit paranoid: programmers build “software fences” around people, effectively preventing physical interactions. To physically interact in a competent manner robotic agents have to reason about the task context, the human, and her intentions. In this paper, we propose to extend cognition-enabled robot control by introducing humans, physical interaction events, and safe movements as first class objects into the plan language. We show the power of the safety-aware control approach in a real-world scenario with a leading-edge autonomous manipulation platform. Finally, we share our experimental recordings through an online knowledge processing system, and invite the reader to explore the data with queries based on the concepts discussed in this paper.",https://ieeexplore.ieee.org/document/7354310/,2015 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),28 Sept.-2 Oct. 2015,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS51168.2021.9636467,Robust Feedback Motion Policy Design Using Reinforcement Learning on a 3D Digit Bipedal Robot,IEEE,Conferences,"In this paper, a hierarchical and robust framework for learning bipedal locomotion is presented and successfully implemented on the 3D biped robot Digit built by Agility Robotics. We propose a cascade-structure controller that combines the learning process with intuitive feedback regulations. This design allows the framework to realize robust and stable walking with a reduced-dimensional state and action spaces of the policy, significantly simplifying the design and increasing the sampling efficiency of the learning method. The inclusion of feedback regulation into the framework improves the robustness of the learned walking gait and ensures the success of the sim-to-real transfer of the proposed controller with minimal tuning. We specifically present a learning pipeline that considers hardware-feasible initial poses of the robot within the learning process to ensure the initial state of the learning is replicated as close as possible to the initial state of the robot in hardware experiments. Finally, we demonstrate the feasibility of our method by successfully transferring the learned policy in simulation to the Digit robot hardware, realizing sustained walking gaits under external force disturbances and challenging terrains not incurred during the training process. To the best of our knowledge, this is the first time a learning-based policy is transferred successfully to the Digit robot in hardware experiments.",https://ieeexplore.ieee.org/document/9636467/,2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),27 Sept.-1 Oct. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA.2013.6631400,Robust real-time visual odometry for dense RGB-D mapping,IEEE,Conferences,"This paper describes extensions to the Kintinuous [1] algorithm for spatially extended KinectFusion, incorporating the following additions: (i) the integration of multiple 6DOF camera odometry estimation methods for robust tracking; (ii) a novel GPU-based implementation of an existing dense RGB-D visual odometry algorithm; (iii) advanced fused realtime surface coloring. These extensions are validated with extensive experimental results, both quantitative and qualitative, demonstrating the ability to build dense fully colored models of spatially extended environments for robotics and virtual reality applications while remaining robust against scenes with challenging sets of geometric and visual features.",https://ieeexplore.ieee.org/document/6631400/,2013 IEEE International Conference on Robotics and Automation,6-10 May 2013,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ROBOT.2009.5152197,Robust servo-control for underwater robots using banks of visual filters,IEEE,Conferences,"We present an application of machine learning to the semi-automatic synthesis of robust servo-trackers for underwater robotics. In particular, we investigate an approach based on the use of Boosting for robust visual tracking of color objects in an underwater environment. To this end, we use AdaBoost, the most common variant of the Boosting algorithm, to select a number of low-complexity but moderately accurate color feature trackers and we combine their outputs. The novelty of our approach lies in the design of this family of weak trackers, which enhances a straightforward color segmentation tracker in multiple ways. From a large and diverse family of possible filters, we select a small subset that optimizes the performance of our trackers. The tracking process applies these trackers on the input video frames, and the final tracker output is chosen based on the weights of the final array of trackers. By using computationally inexpensive, but somewhat accurate trackers as members of the ensemble, the system is able to run at quasi real-time, and thus, is deployable on-board our underwater robot. We present quantitative cross-validation results of our spatio-chromatic visual tracker, and conclude by pointing out some difficulties faced and subsequent shortcomings in the experiments we performed, along with directions of future research in the area of ensemble tracking in real-time.",https://ieeexplore.ieee.org/document/5152197/,2009 IEEE International Conference on Robotics and Automation,12-17 May 2009,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/FCCM48280.2020.00067,Scalable Full Hardware Logic Architecture for Gradient Boosted Tree Training,IEEE,Conferences,"Gradient Boosted Tree is most effective and standard machine learning algorithm in many fields especially with various type of tabular dataset. Besides, recent industry field and robotics field require high-speed, power efficient and real-time training with enormous data. FPGA is effective device which enable custom domain specific approach to give acceleration as well as power efficiency. We introduce a scalable full hardware implementation of Gradient Boosted Tree training with high performance and flexibility of hyper parameterization. Experimental work shows that our hardware implementation achieved 11–33 times faster than state-of-art GPU acceleration even with small gates and low power FPGA device.",https://ieeexplore.ieee.org/document/9114741/,2020 IEEE 28th Annual International Symposium on Field-Programmable Custom Computing Machines (FCCM),3-6 May 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/EMWRTS.1995.514298,Scheduling algorithms for improving the response in intelligent real-time environments,IEEE,Conferences,"Over the last few years, the usefulness and maturity of artificial intelligence technologies, and in particular of knowledge-based systems, have been demonstrated by the ever growing number of commercial applications using them. In the domain of real-time control systems (process control, avionics, robotics, ...), these techniques have been considered with a renewed interest, as they appear as a promising approach to cope with the increasing complexity of the systems to be controlled. The integration of nonpredictable methods in real time systems is one of the crucial points. A task model allowing the representation of activities with optional parts and several scheduling algorithms to incorporate them into real time systems is described.",https://ieeexplore.ieee.org/document/514298/,Proceedings Seventh Euromicro Workshop on Real-Time Systems,14-16 June 1995,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICNNB.2005.1614810,Security Assurance Using Face Recognition & Detection System Based On Neural Networks,IEEE,Conferences,"In this paper, we have proposed a new method of implementing an assurance system using the facial information of the people, this is a different approach to the conventional security system which uses biometric information or cryptography for assurance, here we use an efficient self-scaling face recognition system supported with a face detection system, the system is capable enough to extract the human faces from a real time video and to recognize the people using a face recognition system, we are designing the framework for face recognition system with a hybrid RBF neural network, the real advantage of the system lies in its capability to inculcate some basic features of the self organizing map (SOM) so that the system can scale on its own and it doesn't get outdated with time, for the face detection system we use a content based face detection algorithm, the facial feature so detected is inputted into the face recognition system, if the person's information is already present in the system, authentication can be accomplished, this system can be used in public places like airports and supermarkets, the information of criminals can be stored in the system and in case any of the criminals are detected by the system, the security personnel can be signaled, this system can also be implemented in robotics, the system can help the computer to identify individual users distinctly, our facial recognition system has been tested and found to be persistent in recognizing the individual even if the input facial image is of different gesture or holds some extra lineament like beard, moustache or spectacles so we can definitely state that the system is reliable and efficient",https://ieeexplore.ieee.org/document/1614810/,2005 International Conference on Neural Networks and Brain,13-15 Oct. 2005,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA.2019.8793744,Segmenting Unknown 3D Objects from Real Depth Images using Mask R-CNN Trained on Synthetic Data,IEEE,Conferences,"The ability to segment unknown objects in depth images has potential to enhance robot skills in grasping and object tracking. Recent computer vision research has demonstrated that Mask R-CNN can be trained to segment specific categories of objects in RGB images when massive hand-labeled datasets are available. As generating these datasets is time-consuming, we instead train with synthetic depth images. Many robots now use depth sensors, and recent results suggest training on synthetic depth data can transfer successfully to the real world. We present a method for automated dataset generation and rapidly generate a synthetic training dataset of 50,000 depth images and 320,000 object masks using simulated heaps of 3D CAD models. We train a variant of Mask R-CNN with domain randomization on the generated dataset to perform category-agnostic instance segmentation without any hand-labeled data and we evaluate the trained network, which we refer to as Synthetic Depth (SD) Mask R-CNN, on a set of real, high-resolution depth images of challenging, densely-cluttered bins containing objects with highly-varied geometry. SD Mask R-CNN outperforms point cloud clustering baselines by an absolute 15% in Average Precision and 20% in Average Recall on COCO benchmarks, and achieves performance levels similar to a Mask R-CNN trained on a massive, hand-labeled RGB dataset and fine-tuned on real images from the experimental setup. We deploy the model in an instance-specific grasping pipeline to demonstrate its usefulness in a robotics application. Code, the synthetic training dataset, and supplementary material are available at https://bit.ly/2letCuE.",https://ieeexplore.ieee.org/document/8793744/,2019 International Conference on Robotics and Automation (ICRA),20-24 May 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA.2019.8793595,Semi Supervised Deep Quick Instance Detection and Segmentation,IEEE,Conferences,"In this paper, we present a semi supervised deep quick learning framework for instance detection and pixelwise semantic segmentation of images in a dense clutter of items. The framework can quickly and incrementally learn novel items in an online manner by real-time data acquisition and generating corresponding ground truths on its own. To learn various combinations of items, it can synthesize cluttered scenes, in real time. The overall approach is based on the tutor-child analogy in which a deep network (tutor) is pretrained for class-agnostic object detection which generates labeled data for another deep network (child). The child utilizes a customized convolutional neural network head for the purpose of quick learning. There are broadly four key components of the proposed framework: semi supervised labeling, occlusion aware clutter synthesis, a customized convolutional neural network head, and instance detection. The initial version of this framework was implemented during our participation in Amazon Robotics Challenge (ARC), 2017. Our system was ranked 3rd rd, 4th and 5 th worldwide in pick, stow-pick and stow task respectively. The proposed framework is an improved version over ARC'17 where novel features such as instance detection and online learning has been added.",https://ieeexplore.ieee.org/document/8793595/,2019 International Conference on Robotics and Automation (ICRA),20-24 May 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/RO-MAN50785.2021.9515358,Sequential Prediction with Logic Constraints for Surgical Robotic Activity Recognition,IEEE,Conferences,"Many real-world time-sensitive and high-stake applications (e.g., surgical, rescue, and recovery robotics) exhibit sequential nature; thus, applying Recurrent Neural Network (RNN)-based sequential models is an attractive approach to detect robotic activity. One limitation of such approaches is data scarcity. As a result, limited training samples may lead to over-fitting, producing incorrect predictions during deployment. Nevertheless, abundant domain knowledge may still be available, which may help formulate logic constraints. In this paper, we propose a novel way to integrate domain knowledge into RNN-based sequential prediction. We build a Markov Logic Network (MLN)-based classifier that automatically learns constraint weights from data. We propose two methods to incorporate this MLN-based prediction: (i) PriorLayer, in which the values of the hidden layer of the RNN are combined with weights learned from logic constraints in an additional neural network layer, and (ii) Conflation, in which class probabilities from RNN predictions and constraint weights are combined based on the conflation of class probabilities. We evaluate robotic activity classification methods on a simulated OpenAI Gym environment and a real-world DESK dataset for surgical robotics. We observe that our proposed MLN-based approaches boost the performance of LSTM-based networks. In particular, MLN boosts the accuracy of LSTM from 71% to 84% on the Gym dataset and from 68% to 72% on the Taurus robot dataset. Furthermore, MLN (i.e., PriorLayer) shows regularization capability where it improves accuracy in initial LSTM training while avoiding over-fitting early, thus improves the final classification accuracy on unseen data. The code is available at https://github.com/masud99r/prediction-with-logic-constraints.",https://ieeexplore.ieee.org/document/9515358/,2021 30th IEEE International Conference on Robot & Human Interactive Communication (RO-MAN),8-12 Aug. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ITSC.2018.8569569,ShadowCam: Real-Time Detection of Moving Obstacles Behind A Corner For Autonomous Vehicles,IEEE,Conferences,"Moving obstacles occluded by corners are a potential source for collisions in mobile robotics applications such as autonomous vehicles. In this paper, we address the problem of anticipating such collisions by proposing a vision-based detection algorithm for obstacles which are outside of a vehicle's direct line of sight. Our method detects shadows of obstacles hidden around corners and automatically classifies these unseen obstacles as “dynamic” or “static”. We evaluate our proposed detection algorithm on real-world corners and a large variety of simulated environments to assess generalizability in different challenging surface and lighting conditions. The mean classification accuracy on simulated data is around 80% and on real-world corners approximately 70%. Additionally, we integrate our detection system on a full-scale autonomous wheelchair and demonstrate its feasibility as an additional safety mechanism through real-world experiments. We release our real-time-capable implementation of the proposed ShadowCam algorithm and the dataset containing simulated and real-world data under an open-source license.",https://ieeexplore.ieee.org/document/8569569/,2018 21st International Conference on Intelligent Transportation Systems (ITSC),4-7 Nov. 2018,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/DICTA.2018.8615804,Similar Gesture Recognition using Hierarchical Classification Approach in RGB Videos,IEEE,Conferences,"Recognizing human actions from the video streams has become one of the very popular research areas in computer vision and deep learning in the recent years. Action recognition is wildly used in different scenarios in real life, such as surveillance, robotics, healthcare, video indexing and human-computer interaction. The challenges and complexity involved in developing a video-based human action recognition system are manifold. In particular, recognizing actions with similar gestures and describing complex actions is a very challenging problem. To address these issues, we study the problem of classifying human actions using Convolutional Neural Networks (CNN) and develop a hierarchical 3DCNN architecture for similar gesture recognition. The proposed model firstly combines similar gesture pairs into one class, and classify them along with all other class, as a stage-1 classification. In stage-2, similar gesture pairs are classified individually, which reduces the problem to binary classification. We apply and evaluate the developed models to recognize the similar human actions on the HMDB51 dataset. The result shows that the proposed model can achieve high performance in comparison to the state-of-the-art methods.",https://ieeexplore.ieee.org/document/8615804/,2018 Digital Image Computing: Techniques and Applications (DICTA),10-13 Dec. 2018,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/RO-MAN50785.2021.9515431,Simplifying the A.I. Planning modeling for Human-Robot Collaboration,IEEE,Conferences,"For an effective deployment in manufacturing, Collaborative Robots should be capable of adapting their behavior to the state of the environment and to keep the user safe and engaged during the interaction. Artificial Intelligence (AI) enables robots to autonomously operate understanding the environment, planning their tasks and acting to achieve some given goals. However, the effective deployment of AI technologies in real industrial environments is not straightforward. There is a need for engineering tools facilitating communication and interaction between AI engineers and Domain experts. This paper proposes a novel software tool, called TENANT (Tool fostEriNg Ai plaNning in roboTics) whose aim is to facilitate the use of AI planning technologies by providing domain experts like e.g., production engineers, with a graphical software framework to synthesize AI planning models abstracting from syntactic features of the underlying planning formalism.",https://ieeexplore.ieee.org/document/9515431/,2021 30th IEEE International Conference on Robot & Human Interactive Communication (RO-MAN),8-12 Aug. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICIEM51511.2021.9445285,Simulation based vehicle movement tracking using Kalman Filter algorithm for Autonomous vehicles,IEEE,Conferences,"In the domain of Software automotive industry, one of the most widely used algorithms for performing analysis of driving operations is the Kalman filter algorithm. In today's world of advanced machine learning, the Kalman filter remains an important tool to fuse measurements from several sensors to estimate the real time state of robotics systems such as a self-driving vehicle. Kalman filter is able to update an estimate of evolving nature of continuously changing states of the common filters to take a probabilistic estimate. The driving scenario results are updated in real time using 2-steps update and correction method. In this paper, we have described the process of Kalman filter and its variant to estimate about the detection of moving object in a given traffic scenario using advance toolboxes of MATLAB. Results have been shown for multiple changing parameters.",https://ieeexplore.ieee.org/document/9445285/,2021 2nd International Conference on Intelligent Engineering and Management (ICIEM),28-30 April 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS.2018.8593856,Skill-Oriented Designer of Conceptual Robotic Structures,IEEE,Conferences,"This communication presents an application for the use of ontologies in the generation of robot structures. The ontology developed for this app relies on the IEEE Standard Ontologies for Robotics and Automation (ORA) and it incorporates a set of concepts, relations and axioms that link robotic skills with the structural parts needed for their realization. The user can select a base configuration and/or a set of desired skills that the robot should be able to perform. Then, the application evaluates the axioms and returns an abstract structure that can carry out the requested skills. The final implementation of the structure can be achieved with any modular robotic platform that could identify each structural part with a physical device.",https://ieeexplore.ieee.org/document/8593856/,2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),1-5 Oct. 2018,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/UBMK.2019.8907108,Smart Class Applications for Education,IEEE,Conferences,"We use technological developments in education and adapt technology to education. Educational computers, tablets, smart phones and other technological devices that integrate with these devices aim to increase the quality of education within the scope of computer-aided training. At the same time, with the development of the Internet, it has become much easier to access resources. The environments where the hardware and software technologies used in education are used together can be considered as intelligent classes. Video conferencing and live broadcasts used in the first smart classrooms have now turned into different applications. Intelligent classes that support learning with robotics, mobile learning, virtual reality and augmented reality applications, and different learning environments and materials are used today to improve the quality of education. The Individualized Smart Class (BAS) application proposed in this study is designed in two steps, hardware and software. It is envisaged to use laptop computers, microphones, headphones, tablets and virtual reality glasses in the classroom as hardware. At the same time the broadband of the classes will be ensured to have the internet. Even in this class, wireless network systems such as infrared, Bluetooth, Wi-Fi can be used. As a software, a class that can be included in the virtual classes with cloud architecture and can use the increased reality applications is considered. In addition, mobile applications that provide virtual reality will enrich the course materials. With this application, equality of opportunity will be provided for disadvantaged students in education and training, quality of education will be improved and it will be beneficial for the development of our country in this context.",https://ieeexplore.ieee.org/document/8907108/,2019 4th International Conference on Computer Science and Engineering (UBMK),11-15 Sept. 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISC2.2016.7580798,SmartSEAL: A ROS based home automation framework for heterogeneous devices interconnection in smart buildings,IEEE,Conferences,"With this paper we present the SmartSEAL inter-connection system developed for the nationally founded SEAL project. SEAL is a research project aimed at developing Home Automation (HA) solutions for building energy management, user customization and improved safety of its inhabitants. One of the main problems of HA systems is the wide range of communication standards that commercial devices use. Usually this forces the designer to choose devices from a few brands, limiting the scope of the system and its capabilities. In this context, SmartSEAL is a framework that aims to integrate heterogeneous devices, such as sensors and actuators from different vendors, providing networking features, protocols and interfaces that are easy to implement and dynamically configurable. The core of our system is a Robotics middleware called Robot Operating System (ROS). We adapted the ROS features to the HA problem, designing the network and protocol architectures for this particular needs. These software infrastructure allows for complex HA functions that could be realized only levering the services provided by different devices. The system has been tested in our laboratory and installed in two real environments, Palazzo Fogazzaro in Schio and “Le Case” childhood school in Malo. Since one of the aim of the SEAL project is the personalization of the building environment according to the user needs, and the learning of their patterns of behaviour, in the final part of this work we also describe the ongoing design and experiments to provide a Machine Learning based re-identification module implemented with Convolutional Neural Networks (CNNs). The description of the adaptation module complements the description of the SmartSEAL system and helps in understanding how to develop complex HA services through it.",https://ieeexplore.ieee.org/document/7580798/,2016 IEEE International Smart Cities Conference (ISC2),12-15 Sept. 2016,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA40945.2020.9197523,SnapNav: Learning Mapless Visual Navigation with Sparse Directional Guidance and Visual Reference,IEEE,Conferences,"Learning-based visual navigation still remains a challenging problem in robotics, with two overarching issues: how to transfer the learnt policy to unseen scenarios, and how to deploy the system on real robots. In this paper, we propose a deep neural network based visual navigation system, SnapNav. Unlike map-based navigation or Visual-Teach-and-Repeat (VT&R), SnapNav only receives a few snapshots of the environment combined with directional guidance to allow it to execute the navigation task. Additionally, SnapNav can be easily deployed on real robots due to a two-level hierarchy: a high level commander that provides directional commands and a low level controller that provides real-time control and obstacle avoidance. This also allows us to effectively use simulated and real data to train the different layers of the hierarchy, facilitating robust control. Extensive experimental results show that SnapNav achieves a highly autonomous navigation ability compared to baseline models, enabling sparse, map-less navigation in previously unseen environments.",https://ieeexplore.ieee.org/document/9197523/,2020 IEEE International Conference on Robotics and Automation (ICRA),31 May-31 Aug. 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ROMAN.2002.1045681,Socially interactive robots. Why our current beliefs about them still work,IEEE,Conferences,"Discussion about the application of scientific knowledge in robotics in order to build people helpers is widespread. The issue herein addressed is philosophically poignant, that of robots that are 'people'. It is currently popular to speak about robots and the image of Man. Behind this lurks the dialogical mind and the questions on its artificial existence. Without intending to defend or refute the discourse in favour of 'recreating' Man, a lesser familiar question is brought forth: 'Given that we are capable of creating a man (constructing a robot-person), what would the consequences of this be and would we be satisfied with such technology?' Thorny topic; it questions the entire knowledge foundation upon which strong AI/Robotics is positioned. The author argues for improved monitoring of technological progress and thus favours 'soft' (weak) implementation techniques.",https://ieeexplore.ieee.org/document/1045681/,Proceedings. 11th IEEE International Workshop on Robot and Human Interactive Communication,27-27 Sept. 2002,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICSTW.2019.00028,Software Testing: According to Plan!,IEEE,Conferences,"Automated planning and scheduling represents a branch of classical artificial intelligence (AI) research. Although initially used in robotics and intelligent agents, the use of planning for testing purposes has increased over the years. There sequences of actions representing interactions with the system under test guide the test execution towards reaching a test purpose. A planning problem is formally defined as a model that resembles the interaction with a real system under test (SUT). The obtained solutions are generated, i.e., the plans, directly correspond to test cases. The planning model offers the possibility to generate test cases with a great variety of interactions without the need for an extensive model definition. Until now, planning has proven to be efficient in detecting both functional and non-functional issues. The second play a major role in uncovering vulnerabilities in software. In fact, testing of any domain can be specified as a planning problem. The purpose of this paper is to summarize previous research in the domain of planning for testing including discussing examples from multiple domains.",https://ieeexplore.ieee.org/document/8728938/,"2019 IEEE International Conference on Software Testing, Verification and Validation Workshops (ICSTW)",22-23 April 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ITMS52826.2021.9615342,Speaker Identification using Triplet Loss Function Combined with Clustering Techniques,IEEE,Conferences,"Speaker identification plays a critical role in many applications like robotics specially the applications that focus on humanoid robotics. The speaker identification includes comparing unknown utterances against pre-stored utterances of speakers. In general, the encoded features are stored from the pre-known speakers database and 1:N comparisons between the extracted encoded features of the unknown utterances and the pre-stored N known speakers are implemented. Different techniques can be used for these types of comparisons of which cosine similarity is the most used one. However, the more the number of the pre-stored known speakers, the longer the execution time the model will need to finish these comparisons, and hence it may not be suitable for real-time applications. In this paper, we combined previously published Triple Neural Network for speaker identification with clustering techniques on the speakers dataset. We employed different clustering techniques and presented two different methods for comparing unknown utterances against pre-stored utterances. The obtained results showed a significant enhancement in the comparisons time with a few reductions in the obtained accuracy. The proposed approach provided a framework that can represent a trade-off between execution time and obtained accuracy.",https://ieeexplore.ieee.org/document/9615342/,2021 62nd International Scientific Conference on Information Technology and Management Science of Riga Technical University (ITMS),14-15 Oct. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CCWC.2019.8666589,Status of Smart Manufacturing in the United States,IEEE,Conferences,"Smart Manufacturing (SM) has been widely recognized as a groundbreaking advanced manufacturing trend that will revolutionize the manufacturing industries and profoundly influence human society. Although the Industry 4.0-based Smart Manufacturing has been a conversation topic for many manufacturing media, strategists, and leaders, many in the field are uncertain what SM entails, its importance, or how it is even relevant to their organizations. Programs in deploying SM technologies have been under development in Germany, European Union, and Korea since 2011. This paper investigates the current status of Smart Manufacturing in the United States, and the trends in its technologies such as Industrial Internet of Things and artificial intelligence in standardized industrial robotics.",https://ieeexplore.ieee.org/document/8666589/,2019 IEEE 9th Annual Computing and Communication Workshop and Conference (CCWC),7-9 Jan. 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.23919/EECSI53397.2021.9624311,Strawberry Fruit Quality Assessment for Harvesting Robot using SSD Convolutional Neural Network,IEEE,Conferences,"Strawberry has a tremendous economic value as well as being visually appealing. Therefore, strawberry farmers need to ensure that they only harvest good quality strawberries. However, assessing the quality of strawberries is not an easy problem, especially for local plantations which do not have enough human resources. As robotics becomes accessible and widely used for agriculture work such as harvesting fruit, the real-time embedded system computation power becomes much more powerful nowadays. This paper discusses the harvesting robot&#x0027;s ability to distinguish the quality of strawberries in realtime detection using computer vision technology in the form of object detection by utilizing a deep neural network in a single board computer (SBC). The robot software is built on Robot Operating System (ROS) framework. The proposed method is tested on a robot equipped with a monocular camera. The learning process shows that the robot can detect and differentiate between good and bad quality strawberries with 90&#x0025; accuracy and maintain a high frame rate.",https://ieeexplore.ieee.org/document/9624311/,"2021 8th International Conference on Electrical Engineering, Computer Science and Informatics (EECSI)",20-21 Oct. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS51168.2021.9636743,Success Weighted by Completion Time: A Dynamics-Aware Evaluation Criteria for Embodied Navigation,IEEE,Conferences,"We present Success weighted by Completion Time (SCT), a new metric for evaluating navigation performance for mobile robots. Several related works on navigation have used Success weighted by Path Length (SPL) as the primary method of evaluating the path an agent makes to a goal location, but SPL is limited in its ability to properly evaluate agents with complex dynamics. In contrast, SCT explicitly takes the agent’s dynamics model into consideration, and aims to accurately capture how well the agent has approximated the fastest navigation behavior afforded by its dynamics. While several embodied navigation works use point-turn dynamics, we focus on unicycle-cart dynamics for our agent, which better exempli-fies the dynamics model of popular mobile robotics platforms (e.g., LoCoBot, TurtleBot, Fetch, etc.). We also present RRT*-Unicycle, an algorithm for unicycle dynamics that estimates the fastest collision-free path and completion time from a starting pose to a goal location in an environment containing obstacles. We experiment with deep reinforcement learning and reward shaping to train and compare the navigation performance of agents with different dynamics models. In evaluating these agents, we show that in contrast to SPL, SCT is able to capture the advantages in navigation speed a unicycle model has over a simpler point-turn model of dynamics. Lastly, we show that we can successfully deploy our trained models and algorithms outside of simulation in the real world. We embody our agents in a real robot to navigate an apartment, and show that they can generalize in a zero-shot manner. A video summary is available here: https://youtu.be/QOQ56XVIYVE",https://ieeexplore.ieee.org/document/9636743/,2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),27 Sept.-1 Oct. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AIMS.2013.4,Table of contents,IEEE,Conferences,"The following topics are dealt with: artificial intelligence; neural networks and fuzzy systems; evolutionary computation; bioinformatics and bioengineering; data and semantic mining; games, VR and visualization; intelligent systems and applications; systems intelligence; control intelligence; e-science and e-systems; robotics, cybernetics, engineering, and manufacturing; operations research; discrete-event and real-time systems; image, speech and signal processing; industry, business, management, human factors and social issues; energy, power, transport, logistics, harbour, shipping and marine simulation; parallel, distributed, and software architectures and systems; mobile-ad hoc wireless networks, Mobicast, sensor placement, and target tracking; performance engineering of computer and communications systems; and circuits and devices.",https://ieeexplore.ieee.org/document/6959881/,"2013 1st International Conference on Artificial Intelligence, Modelling and Simulation",3-5 Dec. 2013,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCEA.2010.4,Table of contents - Volume 1,IEEE,Conferences,The following deals with the following topics: algorithms; artificial intelligence; software engineering; bioinformatics; computer graphics; computer architecture; information systems; computer aided instruction; computer games; virtual reality; data security; digital simulation; computer aided design; ethical aspects; database systems; digital libraries; signal processing; image processing; logic design; e-commerce; human computer interaction; embedded systems; Internet; mobile computing; multimedia systems; natural language processing; neural networks; programming languages; robotics; control systems; theoretical computer science; and wireless sensor networks.,https://ieeexplore.ieee.org/document/5445635/,2010 Second International Conference on Computer Engineering and Applications,19-21 March 2010,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCRD.2011.5764067,Table of contents vol. 01,IEEE,Conferences,The following topics are dealt with: computer research and development; event driven programming; artificial intelligence; expert systems; algorithm analysis; high performance computing; automated software engineering; human computer interaction; bioinformatics; scientific computing; image processing; information retrieval; compilers; interpreters; computational intelligence; computer architecture; embedded systems; computer animation; Internet; Web applications; communication/networking; knowledge data engineering; computer system implementation; logics; VLSI; mathematical software; information systems; computer based education; mathematical logic; mobile computing; computer games; multimedia applications; computer graphics; virtual reality; natural language processing; neural networks; computer modeling; parallel computing; distributed computing; computer networks; pattern recognition; computer security; computer simulation; computer vision; probability; statistics; performance evaluation; computer aided design/manufacturing; computing ethics; programming languages; problem complexity; control systems; physical sciences; engineering; discrete mathematics; reconfigurable computing systems; data communications; robotics; automation; system security; cryptography; data compression; data encryption; data mining; database systems; document processing; text processing; educational technology; digital library; technology management; digital signal processing; theoretical computer science; digital systems; logic design; ubiquitous computing; and visualizations.,https://ieeexplore.ieee.org/document/5764067/,2011 3rd International Conference on Computer Research and Development,11-13 March 2011,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/FIE.2008.4720346,"Teaching concepts in fuzzy logic using low cost robots, PDAs, and custom software",IEEE,Conferences,"Fuzzy logic is a topic traditionally taught in artificial intelligence, machine learning, and robotics courses. Students receive the necessary mathematical and theoretical foundation in lecture format. The final learning experience may require that students create and code their own fuzzy logic application that solves a real world problem. This can be an issue when the target is a bioengineering course that introduces classical control theory, fuzzy logic, neural networks, genetic algorithms and genetic programming through the use of a low cost robot, personal digital assistant (PDA) handheld computer, and custom PDA software. In this course, the concepts and theories discussed in lecture are reinforced and extended in a corresponding laboratory through the use of wireless robots and PDAs. Fuzzy logic libraries and software modules for laptops and desktop computers are readily available, however, when it comes to handheld computers no such libraries exist. Students are able to spend more time experimenting with different fuzzy logic controllers when a custom fuzzy logic library and PDA graphical user interface are utilized. In this paper we introduce and discuss a unique low cost wireless robot, a custom fuzzy logic library, a custom fuzzy logic GUI for the PDA, and the implementation results for the fuzzy logic section in a newly created bioengineering course. Diagnostic and summative assessment in the form of a pre-test and post-test was administered for each section of the course, however, only the results for the fuzzy logic section will be provided.",https://ieeexplore.ieee.org/document/4720346/,2008 38th Annual Frontiers in Education Conference,22-25 Oct. 2008,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICARSC.2015.19,Testing a Fully Autonomous Robotic Salesman in Real Scenarios,IEEE,Conferences,"Over the past decades, the number of robots deployed in museums, trade shows and exhibitions have grown steadily. This new application domain has become a key research topic in the robotics community. Therefore, new robots are designed to interact with people in these domains, using natural and intuitive channels. Visual perception and speech processing have to be considered for these robots, as they should be able to detect people in their environment, recognize their degree of accessibility and engage them in social conversations. They also need to safely navigate around dynamic, uncontrolled environments. They must be equipped with planning and learning components, that allow them to adapt to different scenarios. Finally, they must attract the attention of the people, be kind and safe to interact with. In this paper, we describe our experience with Gualzru, a salesman robot endowed with the cognitive architecture RoboCog. This architecture synchronizes all previous processes in a social robot, using a common inner representation as the core of the system. The robot has been tested in crowded, public daily life environments, where it interacted with people that had never seen it before nor had a clue about its functionality. Experimental results presented in this paper demonstrate the capabilities of the robot and its limitations in these real scenarios, and define future improvement actions.",https://ieeexplore.ieee.org/document/7101621/,2015 IEEE International Conference on Autonomous Robot Systems and Competitions,8-10 April 2015,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICIA.2006.305788,The Design and Implementation of OpenGL-based Comprehensive Educational Robot System,IEEE,Conferences,"In this paper, the authors present the design and implementation of MountTai, a cost effective OpenGL based comprehensive educational robot system for China's primary and high school education. Firstly the system's goal and framework is introduced, then it is described the MountTai robot's functions and construction in hardware. The paper expatiates at length how VR technology is used to implement the system software as well as how the software's functions are designed to illustrate robotics in different perspectives relating to mechanics, electronics, communication, artificial intelligence, language programming. The Web-based teaching course dedicated to robot-DIY tutorials is also shown. Finally, concluding remarks for future works are given.",https://ieeexplore.ieee.org/document/4097992/,2006 IEEE International Conference on Information Acquisition,20-23 Aug. 2006,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/GHTC.2018.8601597,The EDNA Public Safety Drone: Bullet-Stopping Lifesaving,IEEE,Conferences,"Urban gun violence in cities across the world is a serious issue for public safety agencies and disaster management organizations. This led us to the development of the EDNA drone, an aerial robotics solution designed to equip first responders in high-risk settings with lifesaving-edge tools for situational awareness and non-lethal conflict resolution. The EDNA is an unmanned aerial vehicle (UAV) that delivers the patent-pending “Predictive Probable Cause” technology. The EDNA drone is designed to provide automated real-time analysis to assist teams entering high-risk situations where gun violence may occur. By leveraging machine learning, biometric sensors, and advanced materials in the field and routing feedback to an intuitive augmented-reality interface, the EDNA will provide autonomous threat detection and bullet-stopping capabilities wherever those features are needed--to groups such as Police and Sheriff's Departments, Fire Departments, and EMT and emergency rescue teams. Data from the EDNA drone's sensors is fed to machine learning algorithms running on the drone in real-time. Through a neural network trained on past data, the EDNA is able to detect the presence and location of firearms and explosives, even through walls or other obstacles. Through the use of advanced metal foams and composite materials, the armored drone can even stop bullets-functionality which has obvious benefits for humanitarian deployment.",https://ieeexplore.ieee.org/document/8601597/,2018 IEEE Global Humanitarian Technology Conference (GHTC),18-21 Oct. 2018,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CT.1997.617707,The Intelligent Room project,IEEE,Conferences,"At the MIT Artificial Intelligence Laboratory, we have been working on technologies for an Intelligent Room. Rather than pull people into the virtual world of the computer, we are trying to pull the computer out into the real world of people. To do this, we are combining robotics and vision technology with speech understanding systems and agent-based architectures to provide ready-at-hand computation and information services for people engaged in day-to-day activities, both on their own and in conjunction with others. We have built a layered architecture where, at the bottom level, vision systems track people and identify their activities and gestures, and, through word spotting, decide whether people in the room are talking to each other or to the room itself. At the next level, an agent architecture provides a uniform interface to such specially-built systems, and to other off-the-shelf software, such as Web browsers, etc. At the highest level, we are able to build application systems that provide occupants of the room with specialized services; examples we have built include systems for command-and-control situations rooms and as a room for giving presentations.",https://ieeexplore.ieee.org/document/617707/,Proceedings Second International Conference on Cognitive Technology Humanizing the Information Age,25-28 Aug. 1997,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IEMC.1998.727776,The importance of artificial intelligence-expert systems in computer integrated manufacturing,IEEE,Conferences,"In order to maintain their competitiveness, companies feel compelled to adopt productivity increasing measures. Yet, they cannot relinquish the flexibility their production cycles need in order to improve their response, and thus, their positioning in the market. To achieve this, companies must combine these two seemingly opposed principles. Thanks to new technological advances, this combination is already a working reality in some companies. It is made possible today by the implementation of computer integrated manufacturing (CIM) and artificial intelligence (AI) techniques, fundamentally by means of expert systems (ES) and robotics. Depending on how these (AI/CIM) techniques contribute to automation, their immediate effects are an increase in productivity and cost reductions. Yet also, the system's flexibility allows for easier adaptation and, as a result, an increased ability to generate value, in other words, competitiveness is improved. The authors have analyzed three studies to identify the possible benefits or advantages, as well as the inconveniences, that this type of technique may bring to companies, specifically in the production field. Although the scope of the studies and their approach differ from one to the other, their joint contribution can be of unquestionable value in order to understand a little better the importance of ES within the production system.",https://ieeexplore.ieee.org/document/727776/,IEMC '98 Proceedings. International Conference on Engineering and Technology Management. Pioneering New Technologies: Management Issues and Challenges in the Third Millennium (Cat. No.98CH36266),11-13 Oct. 1998,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ROBOT.2007.364220,Towards Mapping of Cities,IEEE,Conferences,"Map learning is a fundamental task in mobile robotics because maps are required for a series of high level applications. In this paper, we address the problem of building maps of large-scale areas like villages or small cities. We present our modified car-like robot which we use to acquire the data about the environment. We introduce our localization system which is based on an information filter and is able to merge the information obtained by different sensors. We furthermore describe out mapping technique that is able to compactly model three-dimensional scenes and allows us efficient and accurate incremental map learning. We additionally apply a global optimization techniques in order to accurately close loops in the environment. Our approach has been implemented and deeply tested on a real car equipped with a series of sensors. Experiments described in this paper illustrate the accuracy and efficiency of the presented techniques.",https://ieeexplore.ieee.org/document/4209838/,Proceedings 2007 IEEE International Conference on Robotics and Automation,10-14 April 2007,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/DEVLRN.2005.1490968,Towards Robot Soccer Team Behaviours Through Approximate Simulation,IEEE,Conferences,"Robot soccer is now recognized as one of the most popular and efficient testbeds for intelligent robotics. It involves many challenges for computation, mechanics, control, software engineering, machine learning, and other fields. The international RoboCup initiative supports research into robot soccer and provides an excellent environment to investigate machine learning for robotics in simulation and the real world",https://ieeexplore.ieee.org/document/1490968/,"Proceedings. The 4th International Conference on Development and Learning, 2005",19-21 July 2005,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA46639.2022.9811766,"Towards Safe, Realistic Testbed for Robotic Systems with Human Interaction",IEEE,Conferences,"Simulation has been a necessary, safe testbed for robotics systems (RS). However, testing in simulation alone is not enough for robotic systems operating in close proximity, or interacting directly with, humans, because simulated humans are very limited. Furthermore, testing with real humans can be unsafe and costly. As recent advances in machine learning are being brought to physical robotic systems, how to collect data as well as evaluate them with human interactions safely yet realistically is a critical question. This paper presents a Mixed-Reality (MR) system toward human-centered development of robotic systems emphasizing benefits as a data collection and testbed tool. MR testbeds allow humans to interact with various levels of virtuality to maintain both realism and safety. We detail the advantages and limitations of these different levels of realism or virtualization, and report our MR-based RS testbed implemented using off-the-shelf MR devices with the Unity game engine and ROS. We demonstrate our testbed in a multi-robot, multi-person tracking and monitoring application. We share our vision and insights earned during the development and data collection.",https://ieeexplore.ieee.org/document/9811766/,2022 International Conference on Robotics and Automation (ICRA),23-27 May 2022,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS.2015.7354134,Towards bridging the reality gap between tensegrity simulation and robotic hardware,IEEE,Conferences,"Using a new hardware implementation of our designs for tunably compliant spine-like tensegrity robots, we show that the NASA Tensegrity Robotics Toolkit can effectively generate and predict desirable locomotion strategies for these many degree of freedom systems. Tensegrity, which provides structural integrity through a tension network, shows promise as a design strategy for more compliant robots capable of interaction with rugged environments, such as a tensegrity interplanetary probe prototype surviving multi-story drops. Due to the complexity of tensegrity structures, modeling through physics simulation and machine learning improves our ability to design and evaluate new structures and their controllers in a dynamic environment. The kinematics of our simulator, the open source NASA Tensegrity Robotics Toolkit, have been previously validated within 1.3% error on position through motion capture of the six strut robot ReCTeR. This paper provides additional validation of the dynamics through the direct comparison of the simulator to forces experienced by the latest version of the Tetraspine robot. These results give us confidence in our strategy of using tensegrity to impart future robotic systems with properties similar to biological systems such as increased flexibility, power, and mobility in extreme terrains.",https://ieeexplore.ieee.org/document/7354134/,2015 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),28 Sept.-2 Oct. 2015,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IJCNN.2017.7966054,Towards real-time robot simulation on uneven terrain using neural networks,IEEE,Conferences,"Simulation is a valuable tool for robotics research and development, and various simulation packages have been proposed. However, we are aware of no freely-available packages which implement the required fidelity to accurately model earth-moving robots that manipulate the terrain itself. The software which does exist for this is difficult if not impossible to run in real-time while achieving the desired accuracy. This paper proposes a simulation system in which a neural network is trained using data generated in a 3D high-fidelity, non-real-time simulator. The resulting neural network is used to accurately predict the motion of a robot in a 2D simulator, while also taking into consideration a height-field representing a 3D terrain. Using a trained neural network to drive the new simulation provides considerable speedup over the high-fidelity 3D simulation, allowing behaviour to be simulated in real-time while still capturing the physics of the agents and the environment.",https://ieeexplore.ieee.org/document/7966054/,2017 International Joint Conference on Neural Networks (IJCNN),14-19 May 2017,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SECON.2017.7925321,Towards real-time segmentation of 3D point cloud data into local planar regions,IEEE,Conferences,"This article describes an algorithm for efficient segmentation of point cloud data into local planar surface regions. This is a problem of generic interest to researchers in the computer graphics, computer vision, artificial intelligence and robotics community where it plays an important role in applications such as object recognition, mapping, navigation and conversion from point clouds representations to 3D surface models. Prior work on the subject is either computationally burdensome, precluding real time applications such as robotic navigation and mapping, prone to error for noisy measurements commonly found at long range or requires availability of coregistered color imagery. The approach we describe consists of 3 steps: (1) detect a set of candidate planar surfaces, (2) cluster the planar surfaces merging redundant plane models, and (3) segment the point clouds by imposing a Markov Random Field (MRF) on the data and planar models and computing the Maximum A-Posteriori (MAP) of the segmentation labels using Bayesian Belief Propagation (BBP). In contrast to prior work which relies on color information for geometric segmentation, our implementation performs detection, clustering and estimation using only geometric data. Novelty is found in the fast clustering technique and new MRF clique potentials that are heretofore unexplored in the literature. The clustering procedure removes redundant detections of planes in the scene prior to segmentation using BBP optimization of the MRF to improve performance. The MRF clique potentials dynamically change to encourage distinct labels across depth discontinuities. These modifications provide improved segmentations for geometry-only depth images while simultaneously controlling the computational cost. Algorithm parameters are tunable to enable researchers to strike a compromise between segmentation detail and computational performance. Experimental results apply the algorithm to depth images from the NYU depth dataset which indicate that the algorithm can accurately extract large planar surfaces from depth sensor data.",https://ieeexplore.ieee.org/document/7925321/,SoutheastCon 2017,30 March-2 April 2017,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/EDUCON52537.2022.9766804,Training of Engineers: Approaches to Customization of Educational Programs,IEEE,Conferences,"Since the field of information technology (IT) is constantly and rapidly developing, the training of engineering personnel couldn&#x2019;t be behind this global digital transformation. The new digital reality focuses on the mandatory formation of competencies which are end-to-end technology-oriented: Big Data, machine learning and artificial intelligence, augmented and virtual reality, robotics, blockchain, Internet of Things, 5G technologies, quantum technologies and others. Considering the speed of the development of these technologies, concerns have arisen about the relevance of the content of educational programs of higher education, as well as the degree of flexibility of educational trajectories of engineering graduates. To ensure the relevance and flexibility, constant revision of training programs regarding the study and development of new paradigms and solutions is required. This paper calls into question how to meet the requirements of the labor market at the time of graduation, considering that the educational programs are compiled at the time of the beginning of education and in accordance with the established federal standards of higher education. The goal is to adjust the results of the educational program or even to introduce completely new results in accordance with the changes that have occurred in the industry at the time of the implementation of the program to ensure students form the most relevant and in-demand competencies. The article describes the approaches of the Institute of Computer Technologies and Information Security (ICTIS) of the Southern Federal University to the creation of flexible, interdisciplinary bachelor&#x2019;s and master&#x2019;s degree educational programs. The content of these programs directly corresponds to the current and promising demands of the labor market. The presented approaches to the customization of educational programs meet the needs of students in personal and professional development. These approaches contribute to the development of a student-centered learning system that is specific for each level of education. Project-based learning is a key tool for the formation of relevant professional competencies within bachelor&#x2019;s degree programs. The master&#x2019;s degree focuses on the formation of unique research competencies in the context of the current agenda. For this purpose, the ICTIS has opened special research programs that involve a grant system for undergraduates. What is also important is the inclusion of undergraduates in research groups led by postdocs of the Institute. The key principle uniting these approaches into a single system is the introduction of its own educational standards in engineering areas in the ICTIS. The standards of the ICTIS regulate the possibility for students to choose variable professional competencies after mastering the basic educational component. These competencies are formed annually based on the analysis of prospective labor market demands and can be included in the program even after the start of its implementation. The analysis on the results of the ICTIS educational standards implementation has shown the effectiveness of the concept of these standards in the field of computer technology and information security. The effectiveness of this concept of the educational standard allows gradually being implemented in other fields of knowledge.",https://ieeexplore.ieee.org/document/9766804/,2022 IEEE Global Engineering Education Conference (EDUCON),28-31 March 2022,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/VLSID.2018.20,Tutorial T2A: Safe Autonomous Systems: Real-Time Error Detection and Correction in Safety-Critical Signal Processing and Control Algorithms,IEEE,Conferences,"While the last two decades have seen revolutions in computing and communications systems, the next few decades will see a revolution in the use of every-day robotics and artificial intelligence in broad societal applications. Examples of such systems include sensor networks, the smart power grid, self-driven cars and autonomous drones. Such systems are driven by signal processing, control and learning algorithms that process sensor data, actuate control functions and learn about the environment in which these systems operate. The trustworthiness and safety of such systems is of paramount importance and has significant impact on the commercial viability of the underlying technology. As a consequence, anomalies in system operation due to computation errors in on-board processors, degradation and failure of embedded sensors, actuators and electro-mechanical subsystems and unforeseen changes in their operation environment need to detected with minimum latency. Such anomalies also need to be mitigated in ways that ensure the safety of such systems under all possible failure scenarios. Many future systems will be selflearning in the field. It is necessary to ensure that such learning does not compromise the safety of all human personnel involved in the operation of such systems. To enable safe operation of such systems, the underlying hardware needs to be tuned in the field to maximize performance, reliability and error-resilience while minimizing power consumption. To enable such dynamic adaptation, device operating conditions and the onset of soft errors are sensed using post-manufacture and real-time checking mechanisms. These mechanisms rely on the use of built-in sensors and/or low-overhead function encoding techniques to detect anomalies in system functions. A key capability is that of being able to deduce multiple performance parameters of the system-under-test using compact optimized stimulus using learning algorithms. The sensors and function encodings assess the loss in performance of the relevant systems due to workload uncertainties, manufacturing process imperfections, soft errors and hardware malfunction and failures induced by electromechanical degradation. These are then mitigated through the use of algorithm-through-circuit level compensation techniques based on pre-deployment simulation and post-deployment self-learning. These techniques continuously trade off performance vs. power of the individual software and hardware modules in such a way as to deliver the end-to-end desired application level Quality of Service (QoS), while minimizing energy/power consumption and maximizing reliability and safety. Applications to signal processing, and control algorithms for example autonomous systems will be discussed.",https://ieeexplore.ieee.org/document/8326883/,2018 31st International Conference on VLSI Design and 2018 17th International Conference on Embedded Systems (VLSID),6-10 Jan. 2018,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ETFA.2016.7733537,UAV degradation identification for pilot notification using machine learning techniques,IEEE,Conferences,"Unmanned Aerial Vehicles are currently investigated as an important sub-domain of robotics, a fast growing and truly multidisciplinary research field. UAVs are increasingly deployed in real-world settings for missions in dangerous environments or in environments which are challenging to access. Combined with autonomous flying capabilities, many new possibilities, but also challenges, open up. To overcome the challenge of early identification of degradation, machine learning based on flight features is a promising direction. Existing approaches build classifiers that consider their features to be correlated. This prevents a fine-grained detection of degradation for the different hardware components. This work presents an approach where the data is considered uncorrelated and, using machine learning techniques, allows the precise identification of UAV's damages.",https://ieeexplore.ieee.org/document/7733537/,2016 IEEE 21st International Conference on Emerging Technologies and Factory Automation (ETFA),6-9 Sept. 2016,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/M2VIP.2018.8600864,Unsupervised Video Prediction Network with Spatio-temporal Deep Features,IEEE,Conferences,"Predicting the future states of things is an important performance form of intelligence and it is also of vital importance in real-time systems such as autonomous cars and robotics. This paper aims to tackle a video prediction task. Previous methods for future frame prediction are always subject to restrictions from environment, leading to poor accuracy and blurry prediction details. In this work, we present an unsupervised video prediction framework which iteratively anticipates the raw RGB pixel values in future video frames. Extensive experiments are implemented on advanced datasets — KTH and KITTI. The results demonstrate that our method achieves a good performance.",https://ieeexplore.ieee.org/document/8600864/,2018 25th International Conference on Mechatronics and Machine Vision in Practice (M2VIP),20-22 Nov. 2018,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICECS53924.2021.9665511,Using Hardware Performance Counters to support infield GPU Testing,IEEE,Conferences,"Graphics Processing Units (GPUs) have gained importance in several domains where a high computational effort is required (e.g., where Artificial Intelligence is used). At the same time, their adoption extended to domains (e.g., automotive, robotics, aerospace) where the effects of possible hardware faults can be extremely serious. Hence, it is crucial to identify methods allowing to quickly detect the occurrence of faults in a GPU while it works in the operational phase. In this paper we propose a method based on the adoption of hardware performance counters. We show that the method is able to detect permanent faults occurring in some critical modules, thus allowing to increase the overall reliability of the system. The method does not involve significant costs, since performance counters already exist in all real GPUs, e.g., to support silicon debug, and can be easily accessed in software.",https://ieeexplore.ieee.org/document/9665511/,"2021 28th IEEE International Conference on Electronics, Circuits, and Systems (ICECS)",28 Nov.-1 Dec. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IDAACS-SWS50031.2020.9297062,Using a COTS Smartphone to Control an Autonomous Self-Driving Platform,IEEE,Conferences,"Recent interest in self-driving cars has boosted related fields like autonomous systems and robotics. This paper describes a simple and inexpensive small-scale self driving platform called ASV, which is based on a lowcost microcontroller and a COTS smartphone connected via WiFi. The camera of the phone, which is fixed to the platform, acquires images which are processed in a Convolutional Neural Network (CNN) inspired by the Nvidia's PilotNet. The network is trained in end-to-end learning to produce steering command to follow highway style lanes with markers on both sides. On the microcontroller, the steering commands are used for motor actuation and control of the physical movement of the platform. This paper presents the structure and implementation of ASV and evaluates its real-time performance and latency. For typical speeds encountered in small-scale systems, the performance is found more than sufficient for lane following with the CNN, leaving plenty of room for extensions. The platform's simplicity allows it to be used in research, education, and to spark interest in self-driving systems and neural networks. It can form the basis for general robot control.",https://ieeexplore.ieee.org/document/9297062/,2020 IEEE 5th International Symposium on Smart and Wireless Systems within the Conferences on Intelligent Data Acquisition and Advanced Computing Systems (IDAACS-SWS),17-18 Sept. 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IEEECONF51394.2020.9443272,VLSI Hardware Architecture for Gaussian Process,IEEE,Conferences,"Gaussian process (GP) is a popular machine learning technique that is widely used in many application domains, especially in robotics. However, GP is very computation intensive and time consuming during the inference phase, thereby bringing severe challenges for its large-scale deployment in real-time applications. In this paper, we propose two efficient hardware architecture for GP accelerator. One architecture targets for general GP inference, and the other architecture is specifically optimized for the scenario when the data point is gradually observed. Evaluation results show that the proposed hardware accelerator provides significant hardware performance improvement than the general-purpose computing platform.",https://ieeexplore.ieee.org/document/9443272/,"2020 54th Asilomar Conference on Signals, Systems, and Computers",1-4 Nov. 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA.2019.8793556,VPE: Variational Policy Embedding for Transfer Reinforcement Learning,IEEE,Conferences,"Reinforcement Learning methods are capable of solving complex problems, but resulting policies might perform poorly in environments that are even slightly different. In robotics especially, training and deployment conditions often vary and data collection is expensive, making retraining undesirable. Simulation training allows for feasible training times, but on the other hand suffer from a reality-gap when applied in real-world settings. This raises the need of efficient adaptation of policies acting in new environments.We consider the problem of transferring knowledge within a family of similar Markov decision processes. We assume that Q-functions are generated by some low-dimensional latent variable. Given such a Q-function, we can find a master policy that can adapt given different values of this latent variable. Our method learns both the generative mapping and an approximate posterior of the latent variables, enabling identification of policies for new tasks by searching only in the latent space, rather than the space of all policies. The low-dimensional space, and master policy found by our method enables policies to quickly adapt to new environments. We demonstrate the method on both a pendulum swing-up task in simulation, and for simulation-to-real transfer on a pushing task.",https://ieeexplore.ieee.org/document/8793556/,2019 International Conference on Robotics and Automation (ICRA),20-24 May 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/EEEI.2014.7005895,Verification of safety for autonomous unmanned ground vehicles,IEEE,Conferences,"The existing tools for hardware and software reliability and safety engineering do not supply sufficient solutions regarding AI (Artificial Intelligent) adaptive and learning algorithms, which are being used in autonomous robotics and massively rely on designer experience and include methods such as Heuristic, Rules based decision, Fuzzy Logic, Neural Networks, and Genetic Algorithms, Bayes Networks, etc. Since it is obvious that only this kind of algorithms can deal with the complexity and the uncertainty of the real world environment, suitable safety validation methodology is required. In this paper we present the limitation of the existing reliability and safety engineering tools in dealing with autonomous systems and propose a novel methodology based on statistical testing in simulated environment.",https://ieeexplore.ieee.org/document/7005895/,2014 IEEE 28th Convention of Electrical & Electronics Engineers in Israel (IEEEI),3-5 Dec. 2014,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/VR.2019.8798186,Virtual Reality and Photogrammetry for Improved Reproducibility of Human-Robot Interaction Studies,IEEE,Conferences,"Collecting data in robotics, especially human-robot interactions, traditionally requires a physical robot in a prepared environment, that presents substantial scalability challenges. First, robots provide many possible points of system failure, while the availability of human participants is limited. Second, for tasks such as language learning, it is important to create environments that provide interesting' varied use cases. Traditionally, this requires prepared physical spaces for each scenario being studied. Finally, the expense associated with acquiring robots and preparing spaces places serious limitations on the reproducible quality of experiments. We therefore propose a novel mechanism for using virtual reality to simulate robotic sensor data in a series of prepared scenarios. This allows for a reproducible dataset that other labs can recreate using commodity VR hardware. We demonstrate the effectiveness of this approach with an implementation that includes a simulated physical context, a reconstruction of a human actor, and a reconstruction of a robot. This evaluation shows that even a simple “sandbox” environment allows us to simulate robot sensor data, as well as the movement (e.g., view-port) and speech of humans interacting with the robot in a prescribed scenario.",https://ieeexplore.ieee.org/document/8798186/,2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR),23-27 March 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS.1999.811679,What we learned from RoboCup-97 and RoboCup-98,IEEE,Conferences,"RoboCup is an increasingly successful attempt to promote the full integration of robotics and AI research. The most prominent feature of RoboCup is that it provides the researchers with the opportunity to demonstrate their research results as a form of competition in a dynamically changing hostile environment, defined as the international standard game definition, in which the gamut of intelligent robotics research issues are naturally involved. The article describes what we have learned from the past RoboCup activities, and overview the future perspectives of RoboCup in the next century, mainly focusing on the real robot leagues. Finally, we introduce the new leagues, one of which will have been held at RoboCup-99 in Stockholm.",https://ieeexplore.ieee.org/document/811679/,Proceedings 1999 IEEE/RSJ International Conference on Intelligent Robots and Systems. Human and Environment Friendly Robots with High Intelligence and Emotional Quotients (Cat. No.99CH36289),17-21 Oct. 1999,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICSMC.2011.6083632,[Copyright notice],IEEE,Conferences,The following topics are dealt with: brain-machine interface; machine learning technology; service systems; homeland security systems; virtual reality; agent-based modeling; human centered transportation systems; awareness science and engineering; soft computing; enterprise information systems; social signal processing; infrastructure system; manufacturing systems; pattern recognition; medical mechatronics; minimally invasive surgery; medical robotics; medical technology; intelligent power systems; discrete event systems; Petri nets; biometrics; bioinformatics; computational intelligence; supply chain management; shared control; fault diagnosis; systems engineering; Internet; support vector machines; knowledge acquisition; cloud computing; grey systems; humanoid robots; redundant manipulators; formal methods; granular computing; wireless sensor networks; nonlinear control systems; gesture-based interaction; software engineering; multi-agent systems; cognitive computing; social robotics; natural language processing; conflict resolution; intelligent transportation systems; human-robot interaction; image processing; medical informatics; decision support systems; assistive technology; human-centered design; data mining; and anti-terrorism applications.,https://ieeexplore.ieee.org/document/6083632/,"2011 IEEE International Conference on Systems, Man, and Cybernetics",9-12 Oct. 2011,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/UKSim.2012.123,[Cover art],IEEE,Conferences,The following topics are dealt with: neural networks; evolutionary computation; adaptive dynamic programming; re-enforcement learning; bio-informatics; bio-engineering; computational finance; economics; semantic mining; data mining; virtual reality; data visualization; intelligent systems; soft computing; hybrid computing; e-science; e-systems; robotics; cybernetics; manufacturing; engineering; operations research; discrete event systems; real time systems; image processing; speech processing; signal processing; industry; business; social issues; human factors; marine simulation; power systems; logistics;parallel systems; distributed systems; software architectures;Internet modelling; semantic Web; ontologies; mobile ad hoc wireless networks; Mobicast; sensor placement; target tracking; circuits; sensors and devices.,https://ieeexplore.ieee.org/document/6205540/,2012 UKSim 14th International Conference on Computer Modelling and Simulation,28-30 March 2012,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/INES.2015.7329762,[Front cover],IEEE,Conferences,The following topics are dealt with: learning; Web; urban water-supply system; IP Core; DCI approach; real-time sensor network; linked open data source; process mining; image coding; deep neural network architecture; human computer interaction; social human-robot interaction; VANET; authorized V2V communication; MIMO system; surgical robotics; ontologies; genetic algorithm; image reconstruction; mobile robot; artificial neural network; fuzzy reasoning; heat exchanger; fuzzy controller design; mobile device; human machine interface design; decision support system; data mining technique; discrete-time SISO system; augmented reality; visual analysis; content management system; Androids; nonlinear MPC; collaborative filtering; recommendation; wireless sensor networks;; humidity control; temperature control and stability.,https://ieeexplore.ieee.org/document/7329762/,2015 IEEE 19th International Conference on Intelligent Engineering Systems (INES),3-5 Sept. 2015,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AIMS.2015.1,[Title page i],IEEE,Conferences,The following topics are dealt with: artificial intelligence; neural networks; fuzzy systems; evolutionary computation; bioinformatics; bioengineering; data mining; semantic mining; games; VR; visualization; intelligent systems applications; hybrid computing; soft computing; intelligent systems control; control intelligence; e-science; e-systems; robotics; cybernetics; manufacturing system; operations research; discrete event systems; real time systems; signal processing; speech processing; image processing; natural language processing; human factors; social issues; shipping; marine simulation; transport; logistics; mobile ad hoc wireless networks; Mobicast; sensor placement; target tracking; software architectures; distributed systems; parallel systems; power simulation; performance engineering; communication systems; and circuits.,https://ieeexplore.ieee.org/document/7604531/,"2015 3rd International Conference on Artificial Intelligence, Modelling and Simulation (AIMS)",2-4 Dec. 2015,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCET.2010.5486348,[Title page],IEEE,Conferences,The following topics are dealt with: artificial intelligence; automated software engineering; bioinformatics; scientific computing; biomedical engineering; compilers; interpreters; computational intelligence; computer animation; computer architecture; VLSI; information systems; computer based education; computer games; digital systems; distributed systems; parallel processing; e-commerce; e-governance; event driven programming; expert systems; human computer interaction; image processing; information retrieval; embedded systems; Internet; Web application; knowledge data engineering; virtual reality; mobile computing; multimedia applications; computer modeling; natural language processing; computer networks; neural networks; computer security; computer simulation; pattern recognition; computer vision; performance evaluation; computer aided design; programming languages; computing ethics; robotics; control systems; cryptography; data communication; software engineering; system security; data compression; data encryption; data mining; digital library; wireless sensor networks; ubiquitous computing; technology management.,https://ieeexplore.ieee.org/document/5486348/,2010 2nd International Conference on Computer Engineering and Technology,16-18 April 2010,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA48506.2021.9561446,"droidlet: modular, heterogenous, multi-modal agents",IEEE,Conferences,"In recent years, there have been significant advances in building end-to-end Machine Learning (ML) systems that learn at scale. But most of these systems are: (a) isolated (perception, speech, or language only); (b) trained on static datasets. On the other hand, in the field of robotics, large-scale learning has always been difficult. Supervision is hard to gather and real world physical interactions are expensive.In this work we introduce and open-source droidlet, a modular, heterogeneous agent architecture and platform. It allows us to exploit both large-scale static datasets in perception and language and sophisticated heuristics often used in robotics; and provides tools for interactive annotation. Furthermore, it brings together perception, language and action onto one platform, providing a path towards agents that learn from the richness of real world interactions.",https://ieeexplore.ieee.org/document/9561446/,2021 IEEE International Conference on Robotics and Automation (ICRA),30 May-5 June 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS45743.2020.9340956,robo-gym – An Open Source Toolkit for Distributed Deep Reinforcement Learning on Real and Simulated Robots,IEEE,Conferences,"Applying Deep Reinforcement Learning (DRL) to complex tasks in the field of robotics has proven to be very successful in the recent years. However, most of the publications focus either on applying it to a task in simulation or to a task in a real world setup. Although there are great examples of combining the two worlds with the help of transfer learning, it often requires a lot of additional work and fine-tuning to make the setup work effectively. In order to increase the use of DRL with real robots and reduce the gap between simulation and real world robotics, we propose an open source toolkit: robo-gym1. We demonstrate a unified setup for simulation and real environments which enables a seamless transfer from training in simulation to application on the robot. We showcase the capabilities and the effectiveness of the framework with two real world applications featuring industrial robots: a mobile robot and a robot arm. The distributed capabilities of the framework enable several advantages like using distributed algorithms, separating the workload of simulation and training on different physical machines as well as enabling the future opportunity to train in simulation and real world at the same time. Finally, we offer an overview and comparison of robo-gym with other frequently used state-of-the-art DRL frameworks.",https://ieeexplore.ieee.org/document/9340956/,2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),24 Oct.-24 Jan. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/JIOT.2019.2917066,A 64-mW DNN-Based Visual Navigation Engine for Autonomous Nano-Drones,IEEE,Journals,"Fully miniaturized robots (e.g., drones), with artificial intelligence (AI)-based visual navigation capabilities, are extremely challenging drivers of Internet-of-Things edge intelligence capabilities. Visual navigation based on AI approaches, such as deep neural networks (DNNs) are becoming pervasive for standard-size drones, but are considered out of reach for nano-drones with a size of a few cm2. In this paper, we present the first (to the best of our knowledge) demonstration of a navigation engine for autonomous nano-drones capable of closed-loop end-to-end DNN-based visual navigation. To achieve this goal we developed a complete methodology for parallel execution of complex DNNs directly on board resource-constrained milliwatt-scale nodes. Our system is based on GAP8, a novel parallel ultralow-power computing platform, and a 27-g commercial, open-source Crazyflie 2.0 nano-quadrotor. As part of our general methodology, we discuss the software mapping techniques that enable the DroNet state-of-the-art deep convolutional neural network to be fully executed aboard within a strict 6 frame-per-second real-time constraint with no compromise in terms of flight results, while all processing is done with only 64 mW on average. Our navigation engine is flexible and can be used to span a wide performance range: at its peak performance corner, it achieves 18 frames/s while still consuming on average just 3.5% of the power envelope of the deployed nano-aircraft. To share our key findings with the embedded and robotics communities and foster further developments in autonomous nano-unmanned aerial vehicles (UAVs), we publicly release all our code, datasets, and trained networks.",https://ieeexplore.ieee.org/document/8715489/,IEEE Internet of Things Journal,Oct. 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ACCESS.2018.2851841,A Brain-Inspired Multi-Modal Perceptual System for Social Robots: An Experimental Realization,IEEE,Journals,"We propose a multi-modal perceptual system that is inspired by the inner working of the human brain; in particular, the hierarchical structure of the sensory cortex and the spatial-temporal binding criteria. The system is context independent and can be applied to many on-going problems in social robotics, including but not limited to person recognition, emotion recognition, and multi-modal robot doctor to name a few. The system encapsulates the parallel distributed processing of real-world stimuli through different sensor modalities and encoding them into features vectors which in turn are processed via a number of dedicated processing units (DPUs) through hierarchical paths. DPUs are algorithmic realizations of the cell assemblies in neuroscience. A plausible and realistic perceptual system is presented via the integration of the outputs from these units by spiking neural networks. We will also discuss other components of the system including top-down influences and the integration of information through temporal binding with fading memory and suggest two alternatives to realize these criteria. Finally, we will demonstrate the implementation of this architecture on a hardware platform as a social robot and report experimental studies on the system.",https://ieeexplore.ieee.org/document/8400512/,IEEE Access,2018,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/TCYB.2020.3002892,A Developmental Cognitive Architecture for Trust and Theory of Mind in Humanoid Robots,IEEE,Journals,"As artificial systems are starting to be widely deployed in real-world settings, it becomes critical to provide them with the ability to discriminate between different informants and to learn from reliable sources. Moreover, equipping an artificial agent to infer beliefs may improve the collaboration between humans and machines in several ways. In this article, we propose a hybrid cognitive architecture, called Thrive, with the purpose of unifying in a computational model recent discoveries regarding the underlying mechanism involved in trust. The model is based on biological observations that confirmed the role of the midbrain in trial-and-error learning, and on developmental studies that indicate how essential is a theory of mind in order to build empathetic trust. Thrive is build on top of an actor&#x2013;critic framework that is used to stabilize the weights of two self-organizing maps. A Bayesian network embeds prior knowledge into an intrinsic environment, providing a measure of cost that is used to boostrap learning without an external reward signal. Following a developmental robotics approach, we embodied the model in the iCub humanoid robot and we replicated two psychological experiments. The results are in line with real data, and shed some light on the mechanisms involved in trust-based learning in children and robots.",https://ieeexplore.ieee.org/document/9136927/,IEEE Transactions on Cybernetics,March 2022,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ACCESS.2018.2835302,A Fast and Deterministic Algorithm for Consensus Set Maximization,IEEE,Journals,"With the current booming applications of virtual reality, augmented reality, and robotics, efficiently extracting the maximum consensus set among large-scale corrupted data has become a critical challenge. However, existing methods typically focus on optimization and are rarely concerned about the running time. In this paper, we propose a new fast and deterministic algorithm to address the consensus set maximization problem. First, we propose a novel formulation that transforms the original problem into a sequence of decision problems (DPs). Second, we propose an efficient algorithm to assess the feasibility of these DPs. Comprehensive experiments on linear hyper-plane regression and non-linear homography matrix estimation show that our approach is fully deterministic and can effectively process large-scale and highly corrupted data without any special initialization. Under a pure MATLAB implementation and a laptop CPU, our method can successfully determine the maximum consensus set from 1000 input data points (with 70% of them being outliers) at 30 Hz.",https://ieeexplore.ieee.org/document/8360018/,IEEE Access,2018,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/TCDS.2020.2968056,A Framework of Hybrid Force/Motion Skills Learning for Robots,IEEE,Journals,"Human factors and human-centered design philosophy are highly desired in today's robotics applications such as human-robot interaction (HRI). Several studies showed that endowing robots of human-like interaction skills can not only make them more likeable but also improve their performance. In particular, skill transfer by imitation learning can increase the usability and acceptability of robots by users without computer programming skills. In fact, besides positional information, muscle stiffness of the human arm and contact force with the environment also play important roles in understanding and generating human-like manipulation behaviors for robots, e.g., in physical HRI and teleoperation. To this end, we present a novel robot learning framework based on dynamic movement primitives (DMPs), taking into consideration both the positional and contact force profiles for human-robot skills transferring. Distinguished from the conventional method involving only the motion information, the proposed framework combines two sets of DMPs, which are built to model the motion trajectory and the force variation of the robot manipulator, respectively. Thus, a hybrid force/motion control approach is taken to ensure the accurate tracking and reproduction of the desired positional and force motor skills. Meanwhile, in order to simplify the control system, a momentum-based force observer is applied to estimate the contact force instead of employing force sensors. To deploy the learned motion-force robot manipulation skills to a broader variety of tasks, the generalization of these DMP models in actual situations is also considered. Comparative experiments have been conducted using a Baxter robot to verify the effectiveness of the proposed learning framework on real-world scenarios like cleaning a table.",https://ieeexplore.ieee.org/document/8964480/,IEEE Transactions on Cognitive and Developmental Systems,March 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ACCESS.2020.3038605,A Gentle Introduction to Reinforcement Learning and its Application in Different Fields,IEEE,Journals,"Due to the recent progress in Deep Neural Networks, Reinforcement Learning (RL) has become one of the most important and useful technology. It is a learning method where a software agent interacts with an unknown environment, selects actions, and progressively discovers the environment dynamics. RL has been effectively applied in many important areas of real life. This article intends to provide an in-depth introduction of the Markov Decision Process, RL and its algorithms. Moreover, we present a literature review of the application of RL to a variety of fields, including robotics and autonomous control, communication and networking, natural language processing, games and self-organized system, scheduling management and configuration of resources, and computer vision.",https://ieeexplore.ieee.org/document/9261348/,IEEE Access,2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/TCSVT.2017.2726564,A Hardware Architecture for Cell-Based Feature-Extraction and Classification Using Dual-Feature Space,IEEE,Journals,"Many computer-vision and machine-learning applications in robotics, mobile, wearable devices, and automotive domains are constrained by their real-time performance requirements. This paper reports a dual-feature-based object recognition coprocessor that exploits both histogram of oriented gradient (HOG) and Haar-like descriptors with a cell-based parallel sliding-window recognition mechanism. The feature extraction circuitry for HOG and Haar-like descriptors is implemented by a pixel-based pipelined architecture, which synchronizes to the pixel frequency from the image sensor. After extracting each cell feature vector, a cell-based sliding window scheme enables parallelized recognition for all windows, which contain this cell. The nearest neighbor search classifier is, respectively, applied to the HOG and Haar-like feature space. The complementary aspects of the two feature domains enable a hardware-friendly implementation of the binary classification for pedestrian detection with improved accuracy. A proof-of-concept prototype chip fabricated in a 65-nm SOI CMOS, having thin gate oxide and buried oxide layers (SOTB CMOS), with 3.22-mm2 core area achieves an energy efficiency of 1.52 nJ/pixel and a processing speed of 30 fps for 1024 × 1616-pixel image frames at 200-MHz recognition working frequency and 1-V supply voltage. Furthermore, multiple chips can implement image scaling, since the designed chip has image-size flexibility attributable to the pixel-based architecture.",https://ieeexplore.ieee.org/document/7979565/,IEEE Transactions on Circuits and Systems for Video Technology,Oct. 2018,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/TIM.2022.3156982,A LiDAR SLAM With PCA-Based Feature Extraction and Two-Stage Matching,IEEE,Journals,"Simultaneous localization and mapping (SLAM) has been studied for decades in the field of robotics, in which light detection and ranging (LiDAR) is widely used in various application areas benefiting from its accessibility of direct, accurate, and reliable 3-D measurements. However, the performance of LiDAR SLAM may be degraded when running in degenerate scenario, which makes it still a challenging problem to realize real-time, robust, and accurate state estimation in complex environments. In this article, we propose a keyframe-based 3-D LiDAR SLAM using an accurate principal component analysis (PCA)-based feature extraction method and an efficient two-stage matching strategy, toward a more robust, accurate, and globally consistent estimation performance. The effectiveness and performance are demonstrated and evaluated by comparing our method with the state-of-the-art open-source methods, LOAM and LeGo-LOAM, on KITTI datasets and custom datasets collected by our sensor system. The experimental results show obvious improvement of odometry accuracy and mapping consistency without loss of real-time performance.",https://ieeexplore.ieee.org/document/9729241/,IEEE Transactions on Instrumentation and Measurement,2022,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/TNSRE.2020.3044113,A Novel Point-in-Polygon-Based sEMG Classifier for Hand Exoskeleton Systems,IEEE,Journals,"In the early 2000s, data from the latest World Health Organization estimates paint a picture where one-seventh of the world population needs at least one assistive device. Fortunately, these years are also characterized by a marked technological drive which takes the name of the Fourth Industrial Revolution. In this terrain, robotics is making its way through more and more aspects of everyday life, and robotics-based assistance/rehabilitation is considered one of the most encouraging applications. Providing high-intensity rehabilitation sessions or home assistance through low-cost robotic devices can be indeed an effective solution to democratize services otherwise not accessible to everyone. However, the identification of an intuitive and reliable real-time control system does arise as one of the critical issues to unravel for this technology in order to land in homes or clinics. Intention recognition techniques from surface ElectroMyoGraphic (sEMG) signals are referred to as one of the main ways-to-go in literature. Nevertheless, even if widely studied, the implementation of such procedures to real-case scenarios is still rarely addressed. In a previous work, the development and implementation of a novel sEMG-based classification strategy to control a fully-wearable Hand Exoskeleton System (HES) have been qualitatively assessed by the authors. This paper aims to furtherly demonstrate the validity of such a classification strategy by giving quantitative evidence about the favourable comparison to some of the standard machine-learning-based methods. Real-time action, computational lightness, and suitability to embedded electronics will emerge as the major characteristics of all the investigated techniques.",https://ieeexplore.ieee.org/document/9291412/,IEEE Transactions on Neural Systems and Rehabilitation Engineering,Dec. 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ACCESS.2016.2571058,A Review of Theoretical and Practical Challenges of Trusted Autonomy in Big Data,IEEE,Journals,"Despite the advances made in artificial intelligence, software agents, and robotics, there is little we see today that we can truly call a fully autonomous system. We conjecture that the main inhibitor for advancing autonomy is lack of trust. Trusted autonomy is the scientific and engineering field to establish the foundations and ground work for developing trusted autonomous systems (robotics and software agents) that can be used in our daily life, and can be integrated with humans seamlessly, naturally, and efficiently. In this paper, we review this literature to reveal opportunities for researchers and practitioners to work on topics that can create a leap forward in advancing the field of trusted autonomy. We focus this paper on the trust component as the uniting technology between humans and machines. Our inquiry into this topic revolves around three subtopics: (1) reviewing and positioning the trust modeling literature for the purpose of trusted autonomy; (2) reviewing a critical subset of sensor technologies that allow a machine to sense human states; and (3) distilling some critical questions for advancing the field of trusted autonomy. The inquiry is augmented with conceptual models that we propose along the way by recompiling and reshaping the literature into forms that enable trusted autonomous systems to become a reality. This paper offers a vision for a Trusted Cyborg Swarm, an extension of our previous Cognitive Cyber Symbiosis concept, whereby humans and machines meld together in a harmonious, seamless, and coordinated manner.",https://ieeexplore.ieee.org/document/7480763/,IEEE Access,2016,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/TCYB.2019.2946090,A Robust Collision Perception Visual Neural Network With Specific Selectivity to Darker Objects,IEEE,Journals,"Building an efficient and reliable collision perception visual system is a challenging problem for future robots and autonomous vehicles. The biological visual neural networks, which have evolved over millions of years in nature and are working perfectly in the real world, could be ideal models for designing artificial vision systems. In the locust's visual pathways, a lobula giant movement detector (LGMD), that is, the LGMD2, has been identified as a looming perception neuron that responds most strongly to darker approaching objects relative to their backgrounds; similar situations which many ground vehicles and robots are often faced with. However, little has been done on modeling the LGMD2 and investigating its potential in robotics and vehicles. In this article, we build an LGMD2 visual neural network which possesses the similar collision selectivity of an LGMD2 neuron in locust via the modeling of biased-ON and -OFF pathways splitting visual signals into parallel ON/OFF channels. With stronger inhibition (bias) in the ON pathway, this model responds selectively to darker looming objects. The proposed model has been tested systematically with a range of stimuli including real-world scenarios. It has also been implemented in a micro-mobile robot and tested with real-time experiments. The experimental results have verified the effectiveness and robustness of the proposed model for detecting darker looming objects against various dynamic and cluttered backgrounds.",https://ieeexplore.ieee.org/document/8922628/,IEEE Transactions on Cybernetics,Dec. 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/TRO.2011.2119910,A Simple Tactile Probe for Surface Identification by Mobile Robots,IEEE,Journals,"This paper describes a tactile probe designed for surface identification in a context of all-terrain low-velocity mobile robotics. The proposed tactile probe is made of a small metallic rod with a single-axis accelerometer attached near its tip. Surface identification is based on analyzing acceleration patterns induced at the tip of this mechanically robust tactile probe, while it is passively dragged along a surface. A training dataset was collected over ten different indoor and outdoor surfaces. Classification results for an artificial neural network were positive, with an 89.9% and 94.6% success rate for 1- and 4-s time windows of data, respectively. We also demonstrated that the same tactile probe can be used for unsupervised learning of terrains. For 1-s time windows of data, the classification success rate was only reduced to 74.1%. Finally, a blind mobile robot, performing real-time classification of surfaces, demonstrated the feasibility of this tactile probe as a guidance mechanism.",https://ieeexplore.ieee.org/document/5752869/,IEEE Transactions on Robotics,June 2011,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ACCESS.2020.3001277,"A Survey of Multi-Access Edge Computing in 5G and Beyond: Fundamentals, Technology Integration, and State-of-the-Art",IEEE,Journals,"Driven by the emergence of new compute-intensive applications and the vision of the Internet of Things (IoT), it is foreseen that the emerging 5G network will face an unprecedented increase in traffic volume and computation demands. However, end users mostly have limited storage capacities and finite processing capabilities, thus how to run compute-intensive applications on resource-constrained users has recently become a natural concern. Mobile edge computing (MEC), a key technology in the emerging fifth generation (5G) network, can optimize mobile resources by hosting compute-intensive applications, process large data before sending to the cloud, provide the cloud-computing capabilities within the radio access network (RAN) in close proximity to mobile users, and offer context-aware services with the help of RAN information. Therefore, MEC enables a wide variety of applications, where the real-time response is strictly required, e.g., driverless vehicles, augmented reality, robotics, and immerse media. Indeed, the paradigm shift from 4G to 5G could become a reality with the advent of new technological concepts. The successful realization of MEC in the 5G network is still in its infancy and demands for constant efforts from both academic and industry communities. In this survey, we first provide a holistic overview of MEC technology and its potential use cases and applications. Then, we outline up-to-date researches on the integration of MEC with the new technologies that will be deployed in 5G and beyond. We also summarize testbeds and experimental evaluations, and open source activities, for edge computing. We further summarize lessons learned from state-of-the-art research works as well as discuss challenges and potential future directions for MEC research.",https://ieeexplore.ieee.org/document/9113305/,IEEE Access,2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/TRO.2004.833801,A hybrid strategy to solve the forward kinematics problem in parallel manipulators,IEEE,Journals,"A parallel manipulator is a closed kinematic structure with the necessary rigidity to provide a high payload to self-weight ratio suitable for many applications in manufacturing, flight simulation systems, and medical robotics. Because of its closed structure, the kinematic control of such a mechanism is difficult. The inverse kinematics problem for such manipulators has a mathematical solution; however, the forward kinematics problem (FKP) is mathematically intractable. This work addresses the FKP and proposes a neural-network-based hybrid strategy that solves the problem to a desired level of accuracy, and can achieve the solution in real time. Two neural-network (NN) concepts using a modified form of multilayered perceptrons with backpropagation learning were implemented. The better performing concept was then combined with a standard Newton-Raphson numerical technique to yield a hybrid solution strategy. Simulation studies were carried out on a flight simulation syystem to check the validity o the approach. Accuracy of close to 0.01 mm and 0.01/spl deg/ in the position and orientation parameters was achieved in less than two iterations and 0.02 s of execution time for the proposed strategy.",https://ieeexplore.ieee.org/document/1391011/,IEEE Transactions on Robotics,Feb. 2005,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/TITB.2004.840062,A tele-operated mobile ultrasound scanner using a light-weight robot,IEEE,Journals,"This paper presents a new tele-operated robotic chain for real-time ultrasound image acquisition and medical diagnosis. This system has been developed in the frame of the Mobile Tele-Echography Using an Ultralight Robot European Project. A light-weight six degrees-of-freedom serial robot, with a remote center of motion, has been specially designed for this application. It holds and moves a real probe on a distant patient according to the expert gesture and permits an image acquisition using a standard ultrasound device. The combination of mechanical structure choice for the robot and dedicated control law, particularly nearby the singular configuration allows a good path following and a robotized gesture accuracy. The choice of compression techniques for image transmission enables a compromise between flow and quality. These combined approaches, for robotics and image processing, enable the medical specialist to better control the remote ultrasound probe holder system and to receive stable and good quality ultrasound images to make a diagnosis via any type of communication link from terrestrial to satellite. Clinical tests have been performed since April 2003. They used both satellite or Integrated Services Digital Network lines with a theoretical bandwidth of 384 Kb/s. They showed the tele-echography system helped to identify 66% of lesions and 83% of symptomatic pathologies.",https://ieeexplore.ieee.org/document/1402447/,IEEE Transactions on Information Technology in Biomedicine,March 2005,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/TIE.2021.3090707,Active Object Detection Based on a Novel Deep Q-Learning Network and Long-Term Learning Strategy for the Service Robot,IEEE,Journals,"This article focuses on active object detection (AOD), one of the greatest challenges in the robotics field. A novel deep-Q-learning-network-based approach is proposed to utilize more useful status information for enhancing the training efficiency and testing accuracy of AOD by adding the cropped target object (TGOJ) from the current state as a new input. Different from the existing researches, a novel reward function, combing the area factor and distance factor of the bounding box, is designed to make the robot not only get closer to the TGOJ but also obtain a better observation viewpoint. Moreover, to overcome the differences between the training dataset and new environments as well as improving the adaptation of the AOD model, a reward-based long-term learning strategy including a novel training strategy is presented. The comparable experiments and the ablation study have been implemented in an AOD dataset, proving that our method owns better performance and efficiency than the comparable methods. Meanwhile, the experiments in the real-world scenario with a robot indicate the validity of the proposed method.",https://ieeexplore.ieee.org/document/9464751/,IEEE Transactions on Industrial Electronics,June 2022,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ACCESS.2020.2996576,An Embedded System for Collection and Real-Time Classification of a Tactile Dataset,IEEE,Journals,"Tactile perception of the material properties in real-time using tiny embedded systems is a challenging task and of grave importance for dexterous object manipulation such as robotics, prosthetics and augmented reality. As the psychophysical dimensions of the material properties cover a wide range of percepts, embedded tactile perception systems require efficient signal feature extraction and classification techniques to process signals collected by tactile sensors in real-time. For this purpose, we developed two embedded systems, one that served as a vibrotactile stimulator system and one that recorded and classified the vibrotactile signals collected by its sensors. The quality of the collected data was first verified offline using Fourier transform for feature extraction and then applying powerful machine learning classifiers such as support vector machines and neural networks. We implemented the proposed memory-less signal feature extraction method in order to achieve real-time processing as the data is being collected. The experimental results have shown that the proposed method significantly reduces the computational complexity of feature extraction and still has led to high classification accuracy even when fed to the less complex classifiers such as random forests that can be easily implemented on embedded systems. Finally, we have also shown that low-cost, highly accurate, and real-time tactile texture classification can be achieved using the proposed approach with an ensemble of sensors.",https://ieeexplore.ieee.org/document/9098886/,IEEE Access,2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/TCDS.2021.3049907,An Empirical Study of Active Inference on a Humanoid Robot,IEEE,Journals,"One of the biggest challenges in robotics is interacting under uncertainty. Unlike robots, humans learn, adapt, and perceive their body as a unity when interacting with the world. Here, we investigate the suitability of <i>active inference</i>, a computational model proposed for the brain and governed by the free-energy principle, for robotic body perception and action in a nonsimulated environment. We designed and deployed the algorithm on the humanoid iCub showing how our proposed model enabled the robot to have adaptive body perception and to perform robust upper body reaching and head object tracking behaviors even under high levels of sensor noise and discrepancies between the model and the real robot. Estimation and control are formalized as an inference problem where the body posterior state distribution is approximated by means of the variational free-energy bound, yielding to a minimization of the prediction error. Besides, our study forecasts reactive actions in the presence of sensorimotor conflicts, a mechanism that may be relevant in human body adaptation to uncertain situations.",https://ieeexplore.ieee.org/document/9316712/,IEEE Transactions on Cognitive and Developmental Systems,June 2022,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/3477.931510,"An approach to the design of reinforcement functions in real world, agent-based applications",IEEE,Journals,"The success of any reinforcement learning (RL) application is in large part due to the design of an appropriate reinforcement function. A methodological framework to support the design of reinforcement functions has not been defined yet, and this critical and often underestimated activity is left to the ability of the RL application designer. We propose an approach to support reinforcement function design in RL applications concerning learning behaviors for autonomous agents. We define some dimensions along which we can describe reinforcement functions; we consider the distribution of reinforcement values, their coherence and their matching with the designer's perspective. We give hints to define measures that objectively describe the reinforcement function; we discuss the trade-offs that should be considered to improve learning and we introduce the dimensions along which this improvement can be expected. The approach we are presenting is general enough to be adopted in a large number of RL projects. We show how to apply it in the design of learning classifier systems (LCS) applications. We consider a simple, but quite complete case study in evolutionary robotics, and we discuss reinforcement function design issues in this sample context.",https://ieeexplore.ieee.org/document/931510/,"IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics)",June 2001,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/LRA.2022.3190086,Arena-Bench: A Benchmarking Suite for Obstacle Avoidance Approaches in Highly Dynamic Environments,IEEE,Journals,"The ability to autonomously navigate safely, especially within dynamic environments, is paramount for mobile robotics. In recent years, DRL approaches have shown superior performance in dynamic obstacle avoidance. However, these learning-based approaches are often developed in specially designed simulation environments and are hard to test against conventional planning approaches. Furthermore, the integration and deployment of these approaches into real robotic platforms are not yet completely solved. In this letter, we present Arena-bench, a benchmark suite to train, test, and evaluate navigation planners on different robotic platforms within 3D environments. It provides tools to design and generate highly dynamic evaluation worlds, scenarios, and tasks for autonomous navigation and is fully integrated into the robot operating system. To demonstrate the functionalities of our suite, we trained a DRL agent on our platform and compared it against a variety of existing different model-based and learning-based navigation approaches on a variety of relevant metrics. Finally, we deployed the approaches towards real robots and demonstrated the reproducibility of the results. The code is publicly available at github.com/ignc-research/arena-bench",https://ieeexplore.ieee.org/document/9827557/,IEEE Robotics and Automation Letters,Oct. 2022,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ACCESS.2021.3093233,Assessment of a Robotic Assistant for Supporting Homework Activities of Children With ADHD,IEEE,Journals,"Robotics, Artificial Intelligence (AI), and the Internet of Things (IoT) support various processes in many scenarios of modern life such as e-health and psychological treatments. This article presents the design, development, implementation, and assessment of a Robotic Assistant (RA), named “Atent@”, as a support tool in the homework activities of children with Attention Deficit Hyperactivity Disorder (ADHD). Interacting with the children the RA helps them correct their bad habits and misbehavior caused by the disorder. Its features and functionalities were designed by therapists, implementing AI algorithms to process information and make decisions in real-time to help children to be focused on their homework. This RA interacts with smart objects deployed at home, which are associated with the activity under observation (desk and chair). This solution allows therapists to receive more accurate information about the homework sessions inside the home. At the same time, remote interaction with the child is made possible (through the RA) to provide new instructions and support him/her along with the sessions. This RA is a significant evolution of an earlier version. All the improvements brought to the project by the modifications in technical and qualitative features are explained. Furthermore, the experiment and its results are presented to illustrate the clinical potential. This project shows that the RA can not only make observations with a high degree of precision like an expert (teacher/therapist) but also positively influences the homework performance of children with and without ADHD.",https://ieeexplore.ieee.org/document/9466828/,IEEE Access,2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ACCESS.2019.2925087,Automatic Gauge Detection via Geometric Fitting for Safety Inspection,IEEE,Journals,"For safety considerations in electrical substations, the inspection robots are recently deployed to monitor important devices and instruments with the presence of skilled technicians in the high-voltage environments. The captured images are transmitted to a data station and are usually analyzed manually. Toward automatic analysis, a common task is to detect gauges from captured images. This paper proposes a gauge detection algorithm based on the methodology of geometric fitting. We first use the Sobel filters to extract edges which usually contain the shapes of gauges. Then, we propose to use line fitting under the framework of random sample consensus (RANSAC) to remove straight lines that do not belong to gauges. Finally, the RANSAC ellipse fitting is proposed to find most fitted ellipse from the remaining edge points. The experimental results on a real-world dataset captured by the GuoZi Robotics demonstrate that our algorithm provides more accurate gauge detection results than several existing methods.",https://ieeexplore.ieee.org/document/8746263/,IEEE Access,2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/LRA.2022.3154019,Autonomous Single-Image Drone Exploration With Deep Reinforcement Learning and Mixed Reality,IEEE,Journals,"Autonomous exploration is a longstanding goal of the robotics community. Aerial drone navigation has proven to be especially challenging. The stringent requirements on cost, weight, maneuverability, and power consumption do not allow exploration approaches to easily be employed or adapted to different types of environments. End-to-End Deep Reinforcement Learning (DRL) techniques based on Convolutional Networks approximators, which grant constant-time computation, predefined memory usage, and deliver high visual perception capabilities, represent a very promising alternative to current state of the art solutions relying on metric environment reconstruction. In this work, we address the autonomous exploration problem with aerial robots with a monocular camera based on DRL. Specifically, we propose a novel asymmetric actor-critic model for drone exploration that efficiently leverages ground truth information provided by the simulator environment to speed up learning and enhance final exploration performances. Furthermore, in order to reduce the sim-to-real gap for exploration, we present a novel mixed reality framework that allows an easier, smoother, and safer simulation to real-world transition. Both aspects allow to further exploit the great potential of simulation engines and contribute to reducing the risk associated with directly deploying algorithms on a physical platform with no intermediate step between the simulation and the real world. This is well-known to create several safety concerns and be dangerous when deploying aerial vehicles. Experimental results with a drone exploring multiple environments show the effectiveness of the proposed approach.",https://ieeexplore.ieee.org/document/9721080/,IEEE Robotics and Automation Letters,April 2022,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/LRA.2017.2737046,Baxter's Homunculus: Virtual Reality Spaces for Teleoperation in Manufacturing,IEEE,Journals,"We demonstrate a low-cost telerobotic system that leverages commercial virtual reality (VR) technology and integrates it with existing robotics control infrastructure. The system runs on a commercial gaming engine using off-the-shelf VR hardware and can be deployed on multiple network architectures. The system is based on the homunculus model of mind wherein we embed the user in a VR control room. The control room allows for multiple sensor displays, and dynamic mapping between the user and robot. This dynamic mapping allows for selective engagement between the user and the robot. We compared our system with state-of-the-art automation algorithms and standard VR-based telepresence systems by performing a user study. The study showed that new users were faster and more accurate than the automation or a direct telepresence system. We also demonstrate that our system can be used for pick and place, assembly, and manufacturing tasks.",https://ieeexplore.ieee.org/document/8003431/,IEEE Robotics and Automation Letters,Jan. 2018,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/TNN.2010.2060352,"Clifford Support Vector Machines for Classification, Regression, and Recurrence",IEEE,Journals,"This paper introduces the Clifford support vector machines (CSVM) as a generalization of the real and complex-valued support vector machines using the Clifford geometric algebra. In this framework, we handle the design of kernels involving the Clifford or geometric product. In this approach, one redefines the optimization variables as multivectors. This allows us to have a multivector as output. Therefore, we can represent multiple classes according to the dimension of the geometric algebra in which we work. We show that one can apply CSVM for classification and regression and also to build a recurrent CSVM. The CSVM is an attractive approach for the multiple input multiple output processing of high-dimensional geometric entities. We carried out comparisons between CSVM and the current approaches to solve multiclass classification and regression. We also study the performance of the recurrent CSVM with experiments involving time series. The authors believe that this paper can be of great use for researchers and practitioners interested in multiclass hypercomplex computing, particularly for applications in complex and quaternion signal and image processing, satellite control, neurocomputation, pattern recognition, computer vision, augmented virtual reality, robotics, and humanoids.",https://ieeexplore.ieee.org/document/5586658/,IEEE Transactions on Neural Networks,Nov. 2010,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ACCESS.2021.3105136,Cocktail Glass Network: Fast Depth Estimation Using Channel to Space Unrolling,IEEE,Journals,"Depth-estimation from a single input image can be used in applications such as robotics and autonomous driving. Recently, depth-estimation networks with UNet encoder/decoder structures have been widely used. In these decoders, operations are repeated to gradually increase the image resolution, while decreasing the channel size. If the upsampling operation at a high magnification can be processed at once, the amount of computation in the decoder can be dramatically reduced. To achieve this, we propose a new network structure, i.e., a cocktail glass network. In this network, convolution layers in the decoder are reduced, and a novel fast upsampling method is used that is known as channel-to-space unrolling, which converts thick channel data into high-resolution data. The proposed method can be easily implemented using simple reshaping operations; therefore, it is suitable for reducing the depth-estimation network. Considering the experimental results based on the NYU V2 and KITTI datasets, we demonstrate that the proposed method reduces the amount of computation in the decoder by half, while maintaining the same level of accuracy; it can be used in both lightweight and large-model-capacity networks.",https://ieeexplore.ieee.org/document/9514839/,IEEE Access,2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/TIE.2015.2425359,Coordination of Multiple Robotic Fish With Applications to Underwater Robot Competition,IEEE,Journals,"This paper is concerned with the coordination control of multiple biomimetic robotic fish in highly dynamic aquatic environments by building a hybrid centralized system. With the aid of the results of biorobotics and control techniques, a radio-controlled multijoint robotic fish and its locomotion control are developed. To enable a closed control loop, a visual subsystem that is responsible for tracking of multiple moving objects is constructed and implemented in real time. Furthermore, a behavior-based hierarchical architecture in conjunction with fuzzy reinforcement learning is proposed to accomplish effective coordination among multiple swimming robots. Finally, experiments on 2vs2 water polo game are carried out to verify the proposed coordination control scheme. Over the past eight years, this multirobot platform has been successfully applied to international underwater robot competitions to promote innovative research and education in underwater robotics.",https://ieeexplore.ieee.org/document/7091905/,IEEE Transactions on Industrial Electronics,Feb. 2016,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/TSMC.2020.2967936,Deep Q-Learning With Q-Matrix Transfer Learning for Novel Fire Evacuation Environment,IEEE,Journals,"Deep reinforcement learning (RL) is achieving significant success in various applications like control, robotics, games, resource management, and scheduling. However, the important problem of emergency evacuation, which clearly could benefit from RL, has been largely unaddressed. Indeed, emergency evacuation is a complex task that is difficult to solve with RL. An emergency situation is highly dynamic, with a lot of changing variables and complex constraints that make it challenging to solve. Also, there is no standard benchmark environment available that can be used to train RL agents for evacuation. A realistic environment can be complex to design. In this article, we propose the first fire evacuation environment to train RL agents for evacuation planning. The environment is modeled as a graph capturing the building structure. It consists of realistic features like fire spread, uncertainty, and bottlenecks. The implementation of our environment is in the OpenAI gym format, to facilitate future research. We also propose a new RL approach that entails pretraining the network weights of a DQN-based agent [DQN/Double-DQN (DDQN)/Dueling-DQN] to incorporate information on the shortest path to the exit. We achieved this by using tabular  $Q$ -learning to learn the shortest path on the building model’s graph. This information is transferred to the network by deliberately overfitting it on the  $Q$ -matrix. Then, the pretrained DQN model is trained on the fire evacuation environment to generate the optimal evacuation path under time varying conditions due to fire spread, bottlenecks, and uncertainty. We perform comparisons of the proposed approach with state-of-the-art RL algorithms like DQN, DDQN, Dueling-DQN, PPO, VPG, state-action-reward-state-action (SARSA), actor–critic method, and ACKTR. The results show that our method is able to outperform state-of-the-art models by a huge margin including the original DQN-based models. Finally, our model is tested on a large and complex real building consisting of 91 rooms, with the possibility to move to any other room, hence giving 8281 actions. In order to reduce the action space, we propose a strategy that involves one step simulation. That is, an action importance vector is added to the final output of the pretrained DQN and acts like an attention mechanism. Using this strategy, the action space is reduced by 90.1%. In this manner, the model is able to deal with large action spaces. Hence, our model achieves near optimal performance on the real world emergency environment.",https://ieeexplore.ieee.org/document/8989970/,"IEEE Transactions on Systems, Man, and Cybernetics: Systems",Dec. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/LRA.2020.2965415,DeepFactors: Real-Time Probabilistic Dense Monocular SLAM,IEEE,Journals,"The ability to estimate rich geometry and camera motion from monocular imagery is fundamental to future interactive robotics and augmented reality applications. Different approaches have been proposed that vary in scene geometry representation (sparse landmarks, dense maps), the consistency metric used for optimising the multi-view problem, and the use of learned priors. We present a SLAM system that unifies these methods in a probabilistic framework while still maintaining real-time performance. This is achieved through the use of a learned compact depth map representation and reformulating three different types of errors: photometric, reprojection and geometric, which we make use of within standard factor graph software. We evaluate our system on trajectory estimation and depth reconstruction on realworld sequences and present various examples of estimated dense geometry.",https://ieeexplore.ieee.org/document/8954779/,IEEE Robotics and Automation Letters,April 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/TNSRE.2013.2294685,"Demonstration of a Semi-Autonomous Hybrid Brain–Machine Interface Using Human Intracranial EEG, Eye Tracking, and Computer Vision to Control a Robotic Upper Limb Prosthetic",IEEE,Journals,"To increase the ability of brain-machine interfaces (BMIs) to control advanced prostheses such as the modular prosthetic limb (MPL), we are developing a novel system: the Hybrid Augmented Reality Multimodal Operation Neural Integration Environment (HARMONIE). This system utilizes hybrid input, supervisory control, and intelligent robotics to allow users to identify an object (via eye tracking and computer vision) and initiate (via brain-control) a semi-autonomous reach-grasp-and-drop of the object by the MPL. Sequential iterations of HARMONIE were tested in two pilot subjects implanted with electrocortico-graphic (ECoG) and depth electrodes within motor areas. The subjects performed the complex task in 71.4% (20/28) and 67.7% (21/31) of trials after minimal training. Balanced accuracy for detecting movements was 91.1% and 92.9%, significantly greater than chance accuracies (p <; 0.05). After BMI-based initiation, the MPL completed the entire task 100% (one object) and 70% (three objects) of the time. The MPL took approximately 12.2 s for task completion after system improvements implemented for the second subject. Our hybrid-BMI design prevented all but one baseline false positive from initiating the system. The novel approach demonstrated in this proof-of-principle study, using hybrid input, supervisory control, and intelligent robotics, addresses limitations of current BMIs.",https://ieeexplore.ieee.org/document/6683036/,IEEE Transactions on Neural Systems and Rehabilitation Engineering,July 2014,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/LRA.2021.3062323,Differentiable Simulation for Physical System Identification,IEEE,Journals,"Simulating frictional contacts remains a challenging research topic in robotics. Recently, differentiable physics emerged and has proven to be a key element in model-based Reinforcement Learning (RL) and optimal control fields. However, most of the current formulations deploy coarse approximations of the underlying physical principles. Indeed, the classic simulators loose precision by casting the Nonlinear Complementarity Problem (NCP) of frictional contact into a Linear Complementarity Problem (LCP) to simplify computations. Moreover, such methods deploy non-smooth operations and cannot be automatically differentiated. In this letter, we propose (i) an extension of the staggered projections algorithm for more accurate solutions of the problem of contacts with friction. Based on this formulation, we introduce (ii) a differentiable simulator and an efficient way to compute the analytical derivatives of the involved optimization problems. Finally, (iii) we validate the proposed framework with a set of experiments to present a possible application of our differentiable simulator. In particular, using our approach we demonstrate accurate estimation of friction coefficients and object masses both in synthetic and real experiments.",https://ieeexplore.ieee.org/document/9363565/,IEEE Robotics and Automation Letters,April 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/LRA.2022.3183791,Enhancing Gait Assistance Control Robustness of a Hip Exosuit by Means of Machine Learning,IEEE,Journals,"Optimally synchronising the assistance provided by wearable devices with the human voluntary motion is still an open challenge in robotics. In order to provide accurate and robust assistance, this paper presents a novel approach that combines a layered implementation of a controller for an underactuated exosuit assisting hip flexion during human locomotion: the first layer is based on Adaptive Oscillators (<i>AOs layer</i>), while the second one uses Machine Learning (<i>ML layer</i>). The latter has been introduced to enhance the robustness of the AOs-based controller in abrupt changes of the gait frequency, with the final goal to achieve higher synchronisation and symbiosis between the user and assistive devices in presence of variable and unpredictable locomotion patterns. The effectiveness of the layered controller has been tested on six healthy subjects. Preliminary results suggested that the additional <i>ML layer</i> provided improvement to the overall performances during overground walking. In addition, we found a reduction of metabolic rates when receiving assistance from the device: 7.4&#x0025; on average on treadmill evaluations and 10&#x0025; overground including the extra <i>ML</i> layer, without alteration of the physiological human motion.",https://ieeexplore.ieee.org/document/9797853/,IEEE Robotics and Automation Letters,July 2022,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/TPDS.2020.3006238,GPU-Accelerated Real-Time Stereo Estimation With Binary Neural Network,IEEE,Journals,"Depth estimation from stereo images is essential to many applications such as robotics and autonomous vehicles, most of which ask for the real-time response, high energy and storage efficiency. Recent work has shown deep neural networks (DNN) perform extremely well for stereo estimation. However, these state-of-the-art DNN based algorithms are challenging to be deployed into real-world applications due to the high computational complexities of DNNs. Most of them are too slow for real-time inference and require several seconds of GPU computation to process image frames. In this article, we address the problem of fast stereo estimation and propose an efficient and light-weighted stereo matching system, called StereoBit, to produce a disparity map in a real-time manner while achieving close to state-of-the-art accuracy. To achieve this goal, we propose a binary neural network to generate weighted Hamming distance for an efficient similarity join in stereo estimation. In addition, we propose a novel approximation approach to derive StereoBit network directly from the well-trained network with the cosine similarity. Our approximation strategies enable a significant speedup while maintaining almost the same accuracy compared to the network with the cosine similarity. Furthermore, we present an optimization framework for fully exploiting the computing power of StereoBit. The framework provides a significant speedup of stereo estimation routines, and at the same time, reduces the memory usage for storing parameters. The effectiveness of StereoBit is evaluated by comprehensive experiments. StereoBit can achieve 60 frames per second on an NVIDIA TITAN Xp GPU on KITTI 2012 benchmark while achieving 3-pixel non-occluded stereo error 3.56 percent.",https://ieeexplore.ieee.org/document/9130887/,IEEE Transactions on Parallel and Distributed Systems,1 Dec. 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/TLA.2019.9011550,Gate Detection for Micro Aerial Vehicles using a Single Shot Detector,IEEE,Journals,"Object detection has become an essential tool in aerial robotics thanks to the use of onboard cameras in drones that enables find objects using techniques of vision. However, vision algorithms may become unreliable presenting drawback by the illumination changes. Deep learning has been used to solve tasks of classification, segmentation and detection using traditional Convolutional Neural Network (CNN) like VGG16, YOLO and AlexNet. This paper presents a gates detector system in a real-time using CNN based on a Single Shot Detector Network (SSD) for drone racing circuits. For the latter, we have adopted the SSD7 architecture to modified and present an implementation with five layers, reducing the prediction time and improve detection velocity in comparison with other architectures. For evaluation purpose, we selected three environments: simulation, indoors and outdoors to compare the prediction time, average fps and the confidence obtained in the detections of the gates.",https://ieeexplore.ieee.org/document/9011550/,IEEE Latin America Transactions,December 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ACCESS.2021.3109733,Hardware-Aware Affordance Detection for Application in Portable Embedded Systems,IEEE,Journals,"Affordance detection in computer vision allows segmenting an object into parts according to functions that those parts afford. Most solutions for affordance detection are developed in robotics using deep learning architectures that require substantial computing power. Therefore, these approaches are not convenient for application in embedded systems with limited resources. For instance, computer vision is used in smart prosthetic limbs, and in this context, affordance detection could be employed to determine the graspable segments of an object, which is a critical information for selecting a grasping strategy. This work proposes an affordance detection strategy based on hardware-aware deep learning solutions. Experimental results confirmed that the proposed solution achieves comparable accuracy with respect to the state-of-the-art approaches. In addition, the model was implemented on real-time embedded devices obtaining a high FPS rate, with limited power consumption. Finally, the experimental assessment in realistic conditions demonstrated that the developed method is robust and reliable. As a major outcome, the paper proposes and characterizes the first complete embedded solution for affordance detection in embedded devices. Such a solution could be used to substantially improve computer vision based prosthesis control but it is also highly relevant for other applications (e.g., resource-constrained robotic systems).",https://ieeexplore.ieee.org/document/9527234/,IEEE Access,2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ACCESS.2019.2895653,High-Quality 3D Reconstruction With Depth Super-Resolution and Completion,IEEE,Journals,"The 3D reconstruction is an important topic in computer vision with many applications, such as robotics and augmented reality. Since the raw depth images captured by consumer RGB-D cameras are often low resolution (LR), noisy, and incomplete. How to obtain high-quality 3D models with a consumer RGB-D camera is still a challenge for the existing systems. In this paper, we propose a new depth super-resolution and completion method implemented in a deep learning framework and build a high-quality 3D reconstruction system. We first improve the resolution of LR depth image with a depth super-resolution network and remove the outliers in high-resolution (HR) depth image based on gradient saliency. To further enhance the quality of HR depth image with the guide of HR color image, we learn surface normal and occlusion boundary images from the corresponding HR color image through two deep fully convolutional networks. In particular, the blurriness of HR color image is also detected and pixel-wise quantized. Finally, we obtain a completed HR depth image by optimizing the HR depth image with the surface normal, occlusion boundary, and color image blurriness. We have carried out qualitative and quantitative evaluations with baseline methods on public datasets. The experimental results demonstrate that our method has better performance both on single depth image enhancement and 3D reconstruction.",https://ieeexplore.ieee.org/document/8628990/,IEEE Access,2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/LRA.2021.3061336,Imitation Learning of Hierarchical Driving Model: From Continuous Intention to Continuous Trajectory,IEEE,Journals,"One of the challenges to reduce the gap between the machine and the human level driving is how to endow the system with the learning capacity to deal with the coupled complexity of environments, intentions, and dynamics. In this letter, we propose a hierarchical driving model with explicit models of continuous intention and continuous dynamics, which decouples the complexity in the observation-to-action reasoning in the human driving data. Specifically, the continuous intention module takes perception to generate a potential map encoded with obstacles and intentions. Then, the potential map is regarded as a condition, together with the current dynamics, to generate a continuous trajectory as output by a continuous function approximator network, whose derivatives can be used for supervision without additional parameters. Finally, our method is validated by both datasets and stimulation, demonstrating that our method has higher prediction accuracy of displacement and velocity and generates smoother trajectories. Our method is also deployed on the real vehicle with loop latency, validating its effectiveness. To the best of our knowledge, this is the first work to produce the driving trajectory using a continuous function approximator network. Our code is available at https://github.com/ZJU-Robotics-Lab/CICT.",https://ieeexplore.ieee.org/document/9361054/,IEEE Robotics and Automation Letters,April 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/LRA.2018.2870466,Introduction to the Special Issue on AI for Long-Term Autonomy,IEEE,Journals,"The papers in this special section focus on the use of artificial intelligence (AI) for long term autonomy. Autonomous systems have a long history in the fields of AI and robotics. However, only through recent advances in technology has it been possible to create autonomous systems capable of operating in long-term, real-world scenarios. Examples include autonomous robots that operate outdoors on land, in air, water, and space; and indoors in offices, care homes, and factories. Designing, developing, and maintaining intelligent autonomous systems that operate in real-world environments over long periods of time, i.e. weeks, months, or years, poses many challenges. This special issue focuses on such challenges and on ways to overcome them using methods from AI. Long-term autonomy can be viewed as both a challenge and an opportunity. The challenge of long-term autonomy requires system designers to ensure that an autonomous system can continue operating successfully according to its real-world application demands in unstructured and semi-structured environments. This means addressing issues related to hardware and software robustness (e.g., gluing in screws and profiling for memory leaks), as well as ensuring that all modules and functions of the system can deal with the variation in the environment and tasks that is expected to occur over its operating time. ",https://ieeexplore.ieee.org/document/8478420/,IEEE Robotics and Automation Letters,Oct. 2018,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/LRA.2020.3013937,Invariant Transform Experience Replay: Data Augmentation for Deep Reinforcement Learning,IEEE,Journals,"Deep Reinforcement Learning (RL) is a promising approach for adaptive robot control, but its current application to robotics is currently hindered by high sample requirements. To alleviate this issue, we propose to exploit the symmetries present in robotic tasks. Intuitively, symmetries from observed trajectories define transformations that leave the space of feasible RL trajectories invariant and can be used to generate new feasible trajectories, which could be used for training. Based on this data augmentation idea, we formulate a general framework, called Invariant Transform Experience Replay that we present with two techniques: (i) Kaleidoscope Experience Replay exploits reflectional symmetries and (ii) Goal-augmented Experience Replay which takes advantage of lax goal definitions. In the Fetch tasks from OpenAI Gym, our experimental results show significant increases in learning rates and success rates. Particularly, we attain a 13, 3, and 5 times speedup in the pushing, sliding, and pick-and-place tasks respectively in the multi-goal setting. Performance gains are also observed in similar tasks with obstacles and we successfully deployed a trained policy on a real Baxter robot. Our work demonstrates that invariant transformations on RL trajectories are a promising methodology to speed up learning in deep RL. Code, video, and supplementary materials are available at [1].",https://ieeexplore.ieee.org/document/9158366/,IEEE Robotics and Automation Letters,Oct. 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/LRA.2022.3146515,Kineverse: A Symbolic Articulation Model Framework for Model-Agnostic Mobile Manipulation,IEEE,Journals,"Service robots in the future need to execute abstract instructions such as “fetch the milk from the fridge”. To translate such instructions into actionable plans, robots require in-depth background knowledge. With regards to interactions with doors and drawers, robots require articulation models that they can use for state estimation and motion planning. Existing frameworks model articulated connections as abstract concepts such as prismatic, or revolute, but do not provide a parameterized model of these connections for computation. In this letter, we introduce a novel framework that uses symbolic mathematical expressions to model articulated structures – robots and objects alike – in a unified and extensible manner. We provide a theoretical description of this framework, and the operations that are supported by its models, and introduce an architecture to exchange our models in robotic applications, making them as flexible as any other environmental observation. To demonstrate the utility of our approach, we employ our practical implementation Kineverse for solving common robotics tasks from state estimation and mobile manipulation, and use it further in real-world mobile robot manipulation.",https://ieeexplore.ieee.org/document/9695204/,IEEE Robotics and Automation Letters,April 2022,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/LRA.2022.3159288,Learning Robotic Manipulation of Natural Materials With Variable Properties for Construction Tasks,IEEE,Journals,"The introduction of robotics and machine learning to architectural construction is leading to more efficient construction practices. So far, robotic construction has largely been implemented on standardized materials, conducting simple, predictable, and repetitive tasks. We present a novel mobile robotic system and corresponding learning approach that takes a step towards assembly of natural materials with anisotropic mechanical properties for more sustainable architectural construction. Through experiments both in simulation and in the real world, we demonstrate a dynamically adjusted curriculum and randomization approach for the problem of learning manipulation tasks involving materials with biological variability, namely bamboo. Using our approach, robots are able to transport bamboo bundles and reach to goal-positions during the assembly of bamboo structures.",https://ieeexplore.ieee.org/document/9735376/,IEEE Robotics and Automation Letters,April 2022,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/LRA.2022.3178807,Learning on the Job: Long-Term Behavioural Adaptation in Human-Robot Interactions,IEEE,Journals,"In this work, we propose a framework for allowing autonomous robots deployed for extended periods of time in public spaces to adapt their own behaviour online from user interactions. The robot behaviour planning is embedded in a Reinforcement Learning (RL) framework, where the objective is maximising the level of overall user engagement during the interactions. We use the Upper-Confidence-Bound Value-Iteration (UCBVI) algorithm, which gives a helpful way of managing the exploration-exploitation trade-off for real-time interactions. An engagement model trained end-to-end generates the reward function in real-time during policy execution. We test this approach in a public museum in Lincoln (U.K.), where the robot is deployed as a tour guide for the visitors. Results show that after a couple of months of exploration, the robot policy learned to maintain the engagement of users for longer, with an increase of 22.8&#x0025; over the initial static policy in the number of items visited during the tour and a 30&#x0025; increase in the probability of completing the tour. This work is a promising step toward behavioural adaptation in long-term scenarios for robotics applications in social settings.",https://ieeexplore.ieee.org/document/9785458/,IEEE Robotics and Automation Letters,July 2022,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/JETCAS.2020.3033135,Learning to Walk: Bio-Mimetic Hexapod Locomotion via Reinforcement-Based Spiking Central Pattern Generation,IEEE,Journals,"Online learning for the legged robot locomotion under performance and energy constraints remains to be a challenge. Methods such as stochastic gradient, deep reinforcement learning (RL) have been explored for bipeds, quadrupeds and hexapods. These techniques are computationally intensive and thus difficult to implement on edge computing platforms. These methods are also inefficient in energy consumption and throughput because of their reliance on complex sensors and pre-processing of data. On the other hand, neuromorphic computing paradigms, such as spiking neural networks (SNN), become increasingly favorable in low power computing on edge intelligence. SNN has exhibited the capability of performing reinforcement learning mechanisms with biomimetic spike time-dependent plasticity (STDP) of synapses. However, training a legged robot to walk in the synchronized gait patterns generated by a central pattern generator (CPG) in an SNN framework has not yet been explored. Such a method can combine the efficiency of SNNs with the synchronized locomotion of CPG based systems - providing breakthrough performance improvement of end-to-end learning in mobile robotics. In this paper, we propose a reinforcement based stochastic learning technique for training a spiking CPG for a hexapod robot which learns to walk using bio-inspired tripod gait without prior knowledge. The whole system is implemented on a lightweight raspberry pi platform with integrated sensors. Our method opens new opportunities for online learning with limited edge computing resources.",https://ieeexplore.ieee.org/document/9235477/,IEEE Journal on Emerging and Selected Topics in Circuits and Systems,Dec. 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/TAMD.2015.2507439,Lifelong Augmentation of Multimodal Streaming Autobiographical Memories,IEEE,Journals,"Robot systems that interact with humans over extended periods of time will benefit from storing and recalling large amounts of accumulated sensorimotor and interaction data. We provide a principled framework for the cumulative organization of streaming autobiographical data so that data can be continuously processed and augmented as the processing and reasoning abilities of the agent develop and further interactions with humans take place. As an example, we show how a kinematic structure learning algorithm reasons a-posteriori about the skeleton of a human hand. A partner can be asked to provide feedback about the augmented memories, which can in turn be supplied to the reasoning processes in order to adapt their parameters. We employ active, multimodal remembering, so the robot as well as humans can gain insights of both the original and augmented memories. Our framework is capable of storing discrete and continuous data in real-time. The data can cover multiple modalities and several layers of abstraction (e.g., from raw sound signals over sentences to extracted meanings). We show a typical interaction with a human partner using an iCub humanoid robot. The framework is implemented in a platform-independent manner. In particular, we validate its multi platform capabilities using the iCub, Baxter and NAO robots. We also provide an interface to cloud based services, which allow automatic annotation of episodes. Our framework is geared towards the developmental robotics community, as it: 1) provides a variety of interfaces for other modules; 2) unifies previous works on autobiographical memory; and 3) is licensed as open source software.",https://ieeexplore.ieee.org/document/7350228/,IEEE Transactions on Cognitive and Developmental Systems,Sept. 2016,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/LRA.2020.2970679,Low to High Dimensional Modality Hallucination Using Aggregated Fields of View,IEEE,Journals,"Real-world robotics systems deal with data from a multitude of modalities, especially for tasks such as navigation and recognition. The performance of those systems can drastically degrade when one or more modalities become inaccessible, due to factors such as sensors' malfunctions or adverse environments. Here, we argue modality hallucination as one effective way to ensure consistent modality availability and thereby reduce unfavorable consequences. While hallucinating data from a modality with richer information, e.g., RGB to depth, has been researched extensively, we investigate the more challenging low-to-high modality hallucination with interesting use cases in robotics and autonomous systems. We present a novel hallucination architecture that aggregates information from multiple fields of view of the local neighborhood to recover the lost information from the extant modality. The process is implemented by capturing a non-linear mapping between the data modalities and the learned mapping is used to aid the extant modality to mitigate the risk posed to the system in the adverse scenarios which involve modality loss. We also conduct extensive classification and segmentation experiments on UWRGBD and NYUD datasets and demonstrate that hallucination allays the negative effects of the modality loss. Implementation and models: https://github.com/kausic94/Hallucination.",https://ieeexplore.ieee.org/document/8977350/,IEEE Robotics and Automation Letters,April 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/TIM.2018.2884450,Low-order Nonlinear Finite-Impulse Response Soft Sensors for Ionic Electroactive Actuators Based on Deep Learning,IEEE,Journals,"This paper introduces a soft sensor (SS) for the estimation of the deflection of a polymeric mechanical actuator. The actuator is based on ionic polymer-metal composites (IPMCs). Applications of IPMCs have been proposed in fields such as robotics, surgery, and aerospace, to mention the most interesting ones. In such application fields, both the complexity and the size of the actuating system are of chief importance. An SS can be, therefore, preferred to hardware measuring the actuator output, for estimating the actuator motion. Also, low-order models are of interest to limit the computational load, which can be a constraint in real-time applications. To this aim, several data-driven nonlinear finite-impulse response (NFIR) models have been investigated. Data, used for the model identification, have been acquired, in controlled environmental conditions, by using swept signals as the input to the IPMC actuator. Linear and nonlinear models, based on principal component analysis, shallow, and deep neural networks (NNs), have been investigated, for different model orders. The best results have been obtained by an SS based on a fifth-order NFIR model, implemented by a deep belief NN.",https://ieeexplore.ieee.org/document/8584087/,IEEE Transactions on Instrumentation and Measurement,May 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/TPAMI.2008.260,Monocular Pedestrian Detection: Survey and Experiments,IEEE,Journals,"Pedestrian detection is a rapidly evolving area in computer vision with key applications in intelligent vehicles, surveillance, and advanced robotics. The objective of this paper is to provide an overview of the current state of the art from both methodological and experimental perspectives. The first part of the paper consists of a survey. We cover the main components of a pedestrian detection system and the underlying models. The second (and larger) part of the paper contains a corresponding experimental study. We consider a diverse set of state-of-the-art systems: wavelet-based AdaBoost cascade, HOG/linSVM, NN/LRF, and combined shape-texture detection. Experiments are performed on an extensive data set captured onboard a vehicle driving through urban environment. The data set includes many thousands of training samples as well as a 27-minute test sequence involving more than 20,000 images with annotated pedestrian locations. We consider a generic evaluation setting and one specific to pedestrian detection onboard a vehicle. Results indicate a clear advantage of HOG/linSVM at higher image resolutions and lower processing speeds, and a superiority of the wavelet-based AdaBoost cascade approach at lower image resolutions and (near) real-time processing speeds. The data set (8.5 GB) is made public for benchmarking purposes.",https://ieeexplore.ieee.org/document/4657363/,IEEE Transactions on Pattern Analysis and Machine Intelligence,Dec. 2009,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/TASE.2020.3041427,Physiological Tremor Filtering Without Phase Distortion for Robotic Microsurgery,IEEE,Journals,"All existing physiological tremor filtering algorithms, developed for robotic microsurgery, use nonlinear phase prefilters to isolate the tremor signal. Such filters cause phase distortion to the filtered tremor signal and limit the filtering accuracy. We revisited this long-standing problem to enable filtering of the physiological tremor without any phase distortion. We developed a combined estimation–prediction paradigm that offers zero-phase type filtering. The estimation is achieved with the mathematically modified recursive singular spectrum analysis algorithm, and the prediction is delivered with the standard extreme learning machine. In addition, to limit the computational cost, we developed two moving window versions of this structure, which are appropriate for real-time implementation. The proposed paradigm preserved the natural phase of the filtered tremor. It achieved the key performance index of error limitation below  $10\mu \text{m}$ , yielding the estimation accuracy larger than 70%, at a time delay of 36 ms only. Both moving window versions of the proposed approach restricted the computational cost considerably while offering the same performance. It is the first time that the effective estimation of the physiological tremor is achieved, without any prefiltering and phase distortion. This proposed method is feasible for real-time implantation. Clinical translation of the proposed paradigm can significantly enhance the outcome in hand-held surgical robotics. Note to Practitioners—The imprecision caused by physiological hand tremor in microsurgeries has motivated researchers to innovate an efficient tremor compensating technique that can improve surgical performance. Yet, all the existing tremor filtering algorithms, implemented in hand-held surgical instruments, use nonlinear phase prefilters to separate the tremor signal. The inherent phase distortion caused by such prefilters restricts the filtering performance significantly and renders the existing methods inadequate for hand-held robotic surgery. Motivated by this, we proposed a novel estimator-predictor-based framework, by adopting the modified recursive singular spectrum analysis estimator and the extreme learning machine predictor. The proposed framework filters the tremor signal accurately, without distorting it, but at a small fixed lag. In a set of rigorous testing performed by emulating real-time processing, the proposed algorithm showed higher performance compared with the state-of-the-art algorithms. This validates not only its suitability for real-time implantation but also its potential to improve surgical performance, which has been limited by the distorted filtering. Nonetheless, we have presented a proof-of-principle framework for distortion-free filtering, but its full implementation in a real surgical instrument, such as Micron or ITrem, requires a substantial amount of experimental testing and verification. It can be also applicable in a wide range of areas, including health-care, digital manufacturing, smart automation and control, and various other robotic technologies where efficient filtering of advanced sensor data is highly desirable. In the future, we will develop the multidimensional model of the proposed framework to enable filtering of tremor in the xyz-axes simultaneously.",https://ieeexplore.ieee.org/document/9298924/,IEEE Transactions on Automation Science and Engineering,Jan. 2022,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/TIM.2021.3115564,Real-Time and Efficient 6-D Pose Estimation From a Single RGB Image,IEEE,Journals,"6-D pose estimation is an important branch in the field of vision measurement and is widely used in the fields of robotics, autonomous driving, and reality augmentation. The latest research trend in 6-D pose estimation is to train a deep neural network to directly predict the 2-D projection position of the 3-D keypoint from the image, establish the corresponding relationship, and, finally, use the perspective-n-point (PnP) algorithm to perform pose estimation. The current challenge of pose estimation is that, when objects are textureless, occluded, or scene-cluttered, the detection accuracy is reduced, and most of the existing algorithm models are large and cannot accommodate real-time requirements. In this article, we introduce a densely connected feature pyramid network (DFPN) that can efficiently integrate and utilize features. We combine the cross-stage partial network (CSPNet) with DFPN to design a new network for 6-D pose estimation, DFPN-6-D, a new approach for 6-D object pose estimation. DFPN-6-D can efficiently and accurately handle objects with textureless, occluded, and scene clutter and estimate their full 6-D poses in a single shot. Furthermore, we propose a new confidence calculation method and loss function for object pose estimation, which can fully consider spatial information. Finally, we propose a novel augmentation method for direct 6-D pose estimation approaches to improve performance and generalization ability in the case of occlusion, which is called 6-D augmentation. Our approach achieves a new state-of-the-art accuracy of 98.06 and 87.09 in terms of the ADD(-S) metric on the Linemod dataset and the Occluded-Linemod dataset, and our method also achieves the best result in terms of the different metric on the MULT-I dataset, the BIN-P dataset, and the T-LESS dataset, respectively, while still running end-to-end at over 65 frames/s. The experimental results demonstrate that our algorithm is robust to textureless materials and occlusion while running more efficiently than other methods. We also deploy our proposed method to a real robot to grasp and manipulate objects based on the estimated pose.",https://ieeexplore.ieee.org/document/9548068/,IEEE Transactions on Instrumentation and Measurement,2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/TASE.2019.2940543,Robust Visual Localization in Dynamic Environments Based on Sparse Motion Removal,IEEE,Journals,"Visual localization has been well studied in recent decades and applied in many fields as a fundamental capability in robotics. However, the success of the state of the arts usually builds on the assumption that the environment is static. In dynamic scenarios where moving objects are present, the performance of the existing visual localization systems degrades a lot due to the disturbance of the dynamic factors. To address this problem, we propose a novel sparse motion removal (SMR) model that detects the dynamic and static regions for an input frame based on a Bayesian framework. The similarity between the consecutive frames and the difference between the current frame and the reference frame are both considered to reduce the detection uncertainty. After the detection process is finished, the dynamic regions are eliminated while the static ones are fed into a feature-based visual simultaneous localization and mapping (SLAM) system for further visual localization. To verify the proposed method, both qualitative and quantitative experiments are performed and the experimental results have demonstrated that the proposed model can significantly improve the accuracy and robustness for visual localization in dynamic environments.<;/p><;p><;i>Note to Practitioners<;/i>-This article was motivated by the visual localization problem in dynamic environments. Visual localization is well applied in many robotic fields such as path planning and exploration as the basic capability for a mobile robot. In the GPS-denied environments, one robot needs to localize itself through perceiving the unknown environment based on a visual sensor. In real-world scenes, the existence of the moving objects will significantly degrade the localization accuracy, which makes the robot implementation unreliable. In this article, an SMR model is designed to handle this problem. Once receiving a frame, the proposed model divides it into dynamic and static regions through a Bayesian framework. The dynamic regions are eliminated, while the static ones are maintained and fed into a feature-based visual SLAM system for further visual localization. The proposed method greatly improves the localization accuracy in dynamic environments and guarantees the robustness for robotic implementation.",https://ieeexplore.ieee.org/document/8855084/,IEEE Transactions on Automation Science and Engineering,April 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/LRA.2017.2665694,Shakey 2016—How Much Does it Take to Redo Shakey the Robot?,IEEE,Journals,"Shakey the robot was one of the first autonomous robots that showed impressive capabilities of navigation and mobile manipulation. Since then, robotics research has made great progress, showing more and more capable robotic systems for a large variety of application domains and tasks. In this letter, we look back on decades of research by rebuilding Shakey with modern robotics technology in the open-source Shakey 2016 system. Hereby, we demonstrate the impact of research by showing that ideas from the original Shakey are still alive in state-of-the-art systems, while robotics in general has improved to deliver more robust and more capable software and hardware. Our Shakey 2016 system has been implemented on real robots and leverages mostly open-source software. We experimentally evaluate the system in real-world scenarios on a PR2 robot and a Turtlebot-based robot and particularly investigate the development effort. The experiments documented in this letter demonstrate that results from robotics research are readily available for building complex robots such as Shakey within a short amount of time and little effort.",https://ieeexplore.ieee.org/document/7847341/,IEEE Robotics and Automation Letters,April 2017,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ACCESS.2019.2934998,Survey on Collaborative Smart Drones and Internet of Things for Improving Smartness of Smart Cities,IEEE,Journals,"Smart cities contain intelligent things which can intelligently automatically and collaboratively enhance life quality, save people's lives, and act a sustainable resource ecosystem. To achieve these advanced collaborative technologies such as drones, robotics, artificial intelligence, and Internet of Things (IoT) are required to increase the smartness of smart cities by improving the connectivity, energy efficiency, and quality of services (QoS). Therefore, collaborative drones and IoT play a vital role in supporting a lot of smart-city applications such as those involved in communication, transportation, agriculture,safety and security, disaster mitigation, environmental protection, service delivery, energy saving, e-waste reduction, weather monitoring, healthcare, etc. This paper presents a survey of the potential techniques and applications of collaborative drones and IoT which have recently been proposed in order to increase the smartness of smart cities. It provides a comprehensive overview highlighting the recent and ongoing research on collaborative drone and IoT in improving the real-time application of smart cities. This survey is different from previous ones in term of breadth, scope, and focus. In particular, we focus on the new concept of collaborative drones and IoT for improving smart-city applications. This survey attempts to show how collaborative drones and IoT improve the smartness of smart cities based on data collection, privacy and security, public safety, disaster management, energy consumption and quality of life in smart cities. It mainly focuses on the measurement of the smartness of smart cities, i.e., environmental aspects, life quality, public safety, and disaster management.",https://ieeexplore.ieee.org/document/8795473/,IEEE Access,2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/LRA.2022.3146945,"TACTO: A Fast, Flexible, and Open-Source Simulator for High-Resolution Vision-Based Tactile Sensors",IEEE,Journals,"Simulators perform an important role in prototyping, debugging, and benchmarking new advances in robotics and learning for control. Although many physics engines exist, some aspects of the real world are harder than others to simulate. One of the aspects that have so far eluded accurate simulation is touch sensing. To address this gap, we present TACTO – a fast, flexible, and open-source simulator for vision-based tactile sensors. This simulator allows to render realistic high-resolution touch readings at hundreds of frames per second, and can be easily configured to simulate different vision-based tactile sensors, including DIGIT and OmniTact. In this letter, we detail the principles that drove the implementation of TACTO and how they are reflected in its architecture. We demonstrate TACTO on a perceptual task, by learning to predict grasp stability using touch from 1 million grasps, and on a marble manipulation control task. Moreover, we provide a proof-of-concept that TACTO can be successfully used for Sim2Real applications. We believe that TACTO is a step towards the widespread adoption of touch sensing in robotic applications, and to enable machine learning practitioners interested in multi-modal learning and control.",https://ieeexplore.ieee.org/document/9697425/,IEEE Robotics and Automation Letters,April 2022,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/TC.2020.3038286,Task Splitting and Load Balancing of Dynamic Real-Time Workloads for Semi-Partitioned EDF,IEEE,Journals,"Many real-time software systems, such as those commonly found in the context of multimedia, cloud computing, robotics, and real-time databases, are characterized by a dynamic workload, where applications can join and leave the system at runtime. Global schedulers can transparently support dynamic workload without requiring any off-line task-allocation phase, thus providing advantages to the system designer. Nevertheless, such schedulers exhibit poor worst-case performance when compared to semi-partitioned schedulers, which instead can achieve near-optimal schedulability performance when used in conjunction with smart task splitting and partitioning techniques, and they are also lighter in terms of run-time overhead. This article proposes an approach to efficiently schedule dynamic real-time workloads on multiprocessor systems by means of semi-partitioned scheduling. A linear-time approximation scheme for the C=D splitting algorithm under partitioned EDF scheduling is proposed. Then, a load-balancing algorithm is presented to admit new real-time workloads with a limited number of re-allocations. The article finally reports on a large-scale experimental study showing that (i) the linear-time approximation is characterized by a very limited utilization loss compared with the corresponding exact approach (that has a much higher complexity), and that (ii) the whole approach allows achieving considerable improvements with respect to global and partitioned EDF scheduling.",https://ieeexplore.ieee.org/document/9261105/,IEEE Transactions on Computers,1 Dec. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/JIOT.2021.3068736,Terra: A Smart and Sensible Digital Twin Framework for Robust Robot Deployment in Challenging Environments,IEEE,Journals,"Digital twin (DT) systems that replicate the physical world digitally are powerful tools for monitoring physical systems and evaluating algorithms, but current DT systems are commonly not applicable for robotic deployment and investigation. Meanwhile, current 3-D simulation-based robotic platforms do not model the dynamics of the physical world on-the-fly as done in DT systems, limiting their potential for the development of robotics in challenging environments. To tackle this issue, we propose the first robot-centered smart DT framework, namely, Terra, to facilitate the deployment of robots in challenging environments. The proposed Terra framework introduces a comprehensive DT representation to encode the useful real-time dynamics of both the physical world and the robot agent deployed therein. A multiview multimodality perception module is further devised for Terra to obtain high-level semantics and deliver a precise description of the current status of the environment and the robot agent. By mapping the perceived results to the virtual replica of the physical environment, Terra actively updates the action policy and sends it back to the agent, forming an integral and real-time information feedback loop. In practice, to help demonstrate the effectiveness and feasibility of the proposed framework, we deliberately set up a challenging unordered physical environment with many obstacles and a very simple robot aiming to fulfill a navigation task. Empirical results show that the proposed Terra framework successfully facilitates the robot to accomplish the task without causing hazards.",https://ieeexplore.ieee.org/document/9386242/,IEEE Internet of Things Journal,"15 Sept.15, 2021",ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/TCDS.2018.2826921,The Perception of Emotion in Artificial Agents,IEEE,Journals,"Given recent technological developments in robotics, artificial intelligence, and virtual reality, it is perhaps unsurprising that the arrival of emotionally expressive and reactive artificial agents is imminent. However, if such agents are to become integrated into our social milieu, it is imperative to establish an understanding of whether and how humans perceive emotion in artificial agents. In this review, we incorporate recent findings from social robotics, virtual reality, psychology, and neuroscience to examine how people recognize and respond to emotions displayed by artificial agents. First, we review how people perceive emotions expressed by an artificial agent, such as facial and bodily expressions. Second, we evaluate the similarities and differences in the consequences of perceived emotions in artificial compared to human agents. Besides accurately recognizing the emotional state of an artificial agent, it is critical to understand how humans respond to those emotions. Does interacting with an angry robot induce the same responses in people as interacting with an angry person? Similarly, does watching a robot rejoice when it wins a game elicit similar feelings of elation in the human observer? Here, we provide an overview of the current state of emotion expression and perception during interactions with artificial agents, as well as a clear articulation of the challenges and guiding principles to be addressed as we move ever closer to truly emotional artificial agents.",https://ieeexplore.ieee.org/document/8341761/,IEEE Transactions on Cognitive and Developmental Systems,Dec. 2018,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/TPAMI.2004.1262308,The writer independent online handwriting recognition system frog on hand and cluster generative statistical dynamic time warping,IEEE,Journals,"In this paper, we give a comprehensive description of our writer-independent online handwriting recognition system frog on hand. The focus of this work concerns the presentation of the classification/training approach, which we call cluster generative statistical dynamic time warping (CSDTW). CSDTW is a general, scalable, HMM-based method for variable-sized, sequential data that holistically combines cluster analysis and statistical sequence modeling. It can handle general classification problems that rely on this sequential type of data, e.g., speech recognition, genome processing, robotics, etc. Contrary to previous attempts, clustering and statistical sequence modeling are embedded in a single feature space and use a closely related distance measure. We show character recognition experiments of frog on hand using CSDTW on the UNIPEN online handwriting database. The recognition accuracy is significantly higher than reported results of other handwriting recognition systems. Finally, we describe the real-time implementation of frog on hand on a Linux Compaq iPAQ embedded device.",https://ieeexplore.ieee.org/document/1262308/,IEEE Transactions on Pattern Analysis and Machine Intelligence,March 2004,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ACCESS.2019.2939195,Toward a Clustering-Based Approach for Self-Adjusting Impact Factors in Robotic Control Model,IEEE,Journals,"In mobile robotic control models, control parameters are always generated by sensors' information and a set of Impact Factors (IFs, such as the P-value in the PID model). The IFs take forms of fixed coefficients in control models and need to be pre-defined at design-time. However, when operating in an open environment, IFs of the control model are expected to be adjusted automatically at run-time in order to adapt to the environment changes and improve the operation of robotics. This paper presents a clustering-based approach to continuously updating the IFs in robot control model. The proposed approach utilizes the density-based clustering method to classify environmental changes based on the effects of these changes on robots. In each cluster, the regression method is designed to learn the relationship between IFs and environment changes, and therefore generate corresponding IF adjustment model. Such approach can decrease the mutual interference of environmental changes and enhance the rationality of robotic actions. The paper presents the self-adjusting framework and designs corresponding IFs update algorithms. This paper develops robotics path-following scenario and object-following scenario in open environment and conducts experiments to evaluate the effectiveness of the proposed approach. The results show that the proposed approach has faster response to environmental changes than DQN and MPC approaches, along with a lower deviation of robot's actions.",https://ieeexplore.ieee.org/document/8822939/,IEEE Access,2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/LRA.2021.3123374,Uncertainty for Identifying Open-Set Errors in Visual Object Detection,IEEE,Journals,"Deployed into an open world, object detectors are prone to open-set errors, false positive detections of object classes not present in the training dataset.We propose GMM-Det, a real-time method for extracting epistemic uncertainty from object detectors to identify and reject open-set errors. GMM-Det trains the detector to produce a structured logit space that is modelled with class-specific Gaussian Mixture Models. At test time, open-set errors are identified by their low log-probability under all Gaussian Mixture Models. We test two common detector architectures, Faster R-CNN and RetinaNet, across three varied datasets spanning robotics and computer vision. Our results show that GMM-Det consistently outperforms existing uncertainty techniques for identifying and rejecting open-set detections, especially at the low-error-rate operating point required for safety-critical applications. GMM-Det maintains object detection performance, and introduces only minimal computational overhead. We also introduce a methodology for converting existing object detection datasets into specific open-set datasets to evaluate open-set performance in object detection.",https://ieeexplore.ieee.org/document/9591346/,IEEE Robotics and Automation Letters,Jan. 2022,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/LRA.2019.2894216,VR-Goggles for Robots: Real-to-Sim Domain Adaptation for Visual Control,IEEE,Journals,"In this letter, we deal with the reality gap from a novel perspective, targeting transferring deep reinforcement learning (DRL) policies learned in simulated environments to the real-world domain for visual control tasks. Instead of adopting the common solutions to the problem by increasing the visual fidelity of synthetic images output from simulators during the training phase, we seek to tackle the problem by translating the real-world image streams back to the synthetic domain during the deployment phase, to make the robot feel at home. We propose this as a lightweight, flexible, and efficient solution for visual control, as first, no extra transfer steps are required during the expensive training of DRL agents in simulation; second, the trained DRL agents will not be constrained to being deployable in only one specific real-world environment; and third, the policy training and the transfer operations are decoupled, and can be conducted in parallel. Besides this, we propose a simple yet effective shift loss that is agnostic to the downstream task, to constrain the consistency between subsequent frames which is important for consistent policy outputs. We validate the shift loss for artistic style transfer for videos and domain adaptation, and validate our visual control approach in indoor and outdoor robotics experiments.",https://ieeexplore.ieee.org/document/8620258/,IEEE Robotics and Automation Letters,April 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/LRA.2022.3187276,Variable Impedance Skill Learning for Contact-Rich Manipulation,IEEE,Journals,"Contact-rich manipulation tasks remain a hard problem in robotics that requires interaction with unstructured environments. Reinforcement Learning (RL) is one potential solution to such problems, as it has been successfully demonstrated on complex continuous control tasks. Nevertheless, current state-of-the-art methods require policy training in simulation to prevent undesired behavior and later domain transfer even for simple skills involving contact. In this paper, we address the problem of learning contact-rich manipulation policies by extending an existing skill-based RL framework with a variable impedance action space. Our method leverages a small set of suboptimal demonstration trajectories and learns from both position, but also crucially impedance-space information. We evaluate our method on a number of peg-in-hole task variants with a Franka Panda arm and demonstrate that learning variable impedance actions for RL in Cartesian space can be deployed directly on the real robot, without resorting to learning in simulation.",https://ieeexplore.ieee.org/document/9812508/,IEEE Robotics and Automation Letters,July 2022,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/LRA.2021.3068106,Visual Navigation in Real-World Indoor Environments Using End-to-End Deep Reinforcement Learning,IEEE,Journals,"Visual navigation is essential for many applications in robotics, from manipulation, through mobile robotics to automated driving. Deep reinforcement learning (DRL) provides an elegant map-free approach integrating image processing, localization, and planning in one module, which can be trained and therefore optimized for a given environment. However, to date, DRL-based visual navigation was validated exclusively in simulation, where the simulator provides information that is not available in the real world, e.g., the robot's position or segmentation masks. This precludes the use of the learned policy on a real robot. Therefore, we present a novel approach that enables a direct deployment of the trained policy on real robots. We have designed a new powerful simulator capable of domain randomization. To facilitate the training, we propose visual auxiliary tasks and a tailored reward scheme. The policy is fine-tuned on images collected from real-world environments. We have evaluated the method on a mobile robot in a real office environment. The training took approximately 30 hours on a single GPU. In 30 navigation experiments, the robot reached a 0.3-meter neighbourhood of the goal in more than 86.7% of cases. This result makes the proposed method directly applicable to tasks like mobile manipulation.",https://ieeexplore.ieee.org/document/9384194/,IEEE Robotics and Automation Letters,July 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ACCESS.2020.3030963,Waypoint Mobile Robot Exploration Based on Biologically Inspired Algorithms,IEEE,Journals,"This article proposes stochastic exploration algorithms for mobile robot exploration problems. Navigation with uncertain conditions in the absence of initial parameters is a situation wherein precomputation and prediction are impossible for a robot. Therefore, stochastic optimization techniques were applied to find the optimal solution for the robot exploration problem. Driving to the unknown areas, the robot updates the frontier line of sensor visibility during the exploration mission. The points of the frontier line are assumed as the swarm population with their own positions and costs, which allows the computation of the next global waypoint. The calculation of global waypoints is carried out by a nature-inspired optimization algorithm that can place a waypoint in uncertainties. This study offers to apply three metaheuristic algorithms individually, such as Whale Optimization, Grey Wolf Optimizer, and Particle Swarm Optimization algorithms, for comparison and testing their performances in the mobile robotics. At first, the simulations based on the proposed exploration algorithms were implemented and evaluated in a created environment. The results were compared in a single and average cases. Then, the real-world experiments using Grey Wolf Optimizer exploration algorithm were conducted in the different types of environments using MATLAB-ROS integration tool. These results proved the effectiveness and applicability of the bio-inspired optimization algorithm in the mobile robotics.",https://ieeexplore.ieee.org/document/9223657/,IEEE Access,2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICARSC55462.2022.9784804,3D Face Recognition using Inception Networks for Service Robots,IEEE,Conferences,"The field of face recognition has significantly advanced as deep learning methods, such as those using CNNs, continuously show improvements. However, despite face recognition&#x2019;s promising potential, there are still many concerns regarding privacy and safety. Moreover, the first 2D algorithms, besides having good performance, turned out to be influenced by several factors like the environment&#x2019;s lighting conditions, pose, and facial expression of the subjects, compromising the model&#x2019;s accuracy. This work describes the development of a computer vision system using Deep Learning methods to detect and recognise human faces in 3D in real-time. The RGB images and depth maps from several subjects were captured using an Intel RealSense D455, processed, and consequently provided into two independent CNNs, an Inception-Resnet V1 to deal with the RGB images and an Inception V3 to deal with depth maps. The final algorithm was implemented on the anthropomorphic domestic and healthcare service robot CHARMIE (Collaborative Home Assistant Robot by Minho Industrial Electronics) to perform its tasks according to the recognised user.",https://ieeexplore.ieee.org/document/9784804/,2022 IEEE International Conference on Autonomous Robot Systems and Competitions (ICARSC),29-30 April 2022,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/COASE.2018.8560553,A Cloud-Based Robust Semaphore Mirroring System for Social Robots,IEEE,Conferences,"We present a cloud-based human-robot interaction system that automatically controls a humanoid robot to mirror a human demonstrator performing flag semaphores. We use a cloud-based framework called Human Augmented Robotic Intelligence (HARI) to perform gesture recognition of the human demonstrator and gesture control of a local humanoid robot, named Pepper. To ensure that the system is real-time, we design a system to maximize cloud computation contribution to the deep-neural-network-based gesture recognition system, OpenPose, and to minimize communication costs between the cloud and the robot. A hybrid control system is used to hide latency caused by either routing or physical distances. We conducted real-time semaphore mirroring experiments in which both the robots and the demonstrator were located in Tokyo, Japan, whereas the cloud server was deployed in the United States. The total latency was 400ms for the video streaming to the cloud and 108ms for the robot commanding from the cloud. Further, we measured the reliability of our gesture-based semaphore recognition system with two human subjects, and were able to achieve 90% and 76.7% recognition accuracy, respectively, for the two subjects with open-loop when the subjects were not allowed to see the recognition results. We could achieve 100% recognition accuracy when both subjects were allowed to adapt to the recognition system under a closed-loop setting. Lastly, we showed that we can support two humanoid robots with a single server at the same time. With this real-time cloud-based HRI system, we illustrate that we can deploy gesture-based human-robot globally and at scale.",https://ieeexplore.ieee.org/document/8560553/,2018 IEEE 14th International Conference on Automation Science and Engineering (CASE),20-24 Aug. 2018,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS51168.2021.9636798,A Collaborative Visual SLAM Framework for Service Robots,IEEE,Conferences,"We present a collaborative visual simultaneous localization and mapping (SLAM) framework for service robots. With an edge server maintaining a map database and performing global optimization, each robot can register to an existing map, update the map, or build new maps, all with a unified interface and low computation and memory cost. We design an elegant communication pipeline to enable real-time information sharing between robots. With a novel landmark organization and retrieval method on the server, each robot can acquire landmarks predicted to be in its view, to augment its local map. The framework is general enough to support both RGB-D and monocular cameras, as well as robots with multiple cameras, taking the rigid constraints between cameras into consideration. The proposed framework has been fully implemented and verified with public datasets and live experiments.",https://ieeexplore.ieee.org/document/9636798/,2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),27 Sept.-1 Oct. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SNPD.2013.12,A Fast Genetic SLAM Approach for Mobile Robots,IEEE,Conferences,"This paper presents a new SLAM (simultaneous localization and mapping) method using genetic algorithm (GA) for mobile robots. A laser range finder (LRF) is installed on a mobile robot for collecting point-distance information about the surroundings. From the LRF points, several important ones are extracted for describing the main features of the surroundings. A new form of chromosomes for representing the changes of feature LRF points that are caused by the robot's movement is designed. The matching of current LRF features and the robot's possible movement is done by a fast genetic algorithm. A restart mechanism that re-initializes all chromosomes for increasing the diversity of solutions is developed and works with the matching process. Some constrains are developed for filtering out irrational chromosomes after the operation of crossover and mutation. With these mechanisms and constrains, our proposed method generates feasible solutions in several hundreds of GA iterations. Experiments are conducted on a real mobile robot. The experimental results show that our proposed method is efficient and effective for SLAM.",https://ieeexplore.ieee.org/document/6598520/,"2013 14th ACIS International Conference on Software Engineering, Artificial Intelligence, Networking and Parallel/Distributed Computing",1-3 July 2013,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/UR49135.2020.9144789,A Markerless Deep Learning-based 6 Degrees of Freedom Pose Estimation for Mobile Robots using RGB Data,IEEE,Conferences,"Augmented Reality has been subject to various integration efforts within industries due to its ability to enhance human machine interaction and understanding. Neural networks have achieved remarkable results in areas of computer vision, which bear great potential to assist and facilitate an enhanced Augmented Reality experience. However, most neural networks are computationally intensive and demand huge processing power, thus are not suitable for deployment on Augmented Reality devices. In this work, we propose a method to deploy state of the art neural networks for real time 3D object localization on augmented reality devices. As a result, we provide a more automated method of calibrating the AR devices with mobile robotic systems. To accelerate the calibration process and enhance user experience, we focus on fast 2D detection approaches which are extracting the 3D pose of the object fast and accurately by using only 2D input. The results are implemented into an Augmented Reality application for intuitive robot control and sensor data visualization. For the 6D annotation of 2D images, we developed an annotation tool, which is, to our knowledge, the first open source tool to be available. We achieve feasible results which are generally applicable to any AR device, thus making this work promising for further research in combining high demanding neural networks with Internet of Things devices.",https://ieeexplore.ieee.org/document/9144789/,2020 17th International Conference on Ubiquitous Robots (UR),22-26 June 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/LARS/SBR/WRE.2018.00082,A Visual Memory System for Humanoid Robots,IEEE,Conferences,"Competitions like RoboCup, require a robot that is capable of playing soccer competitively. Therefore the robot must have functional control, vision, localization and decision systems. This project proposes the implementation of an object tracking system, using the vision system of a robot in the domain of RoboCup KidSize Soccer. The proposed system detects multiple objects like: robots, landmarks and the ball; transmit these informations to the tracking system and keep track of their positions, even when the vision system loses the objects. It also considers the movement of the robot in order to predict the objects positions relative to the robot. Real humanoid robots were used in order to implement and test the proposal. Experiments showed that the system was able to track the position of a second moving robot in the field.",https://ieeexplore.ieee.org/document/8588588/,"2018 Latin American Robotic Symposium, 2018 Brazilian Symposium on Robotics (SBR) and 2018 Workshop on Robotics in Education (WRE)",6-10 Nov. 2018,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/iCREATE.2014.6828372,A comparison of various robotic control architectures for autonomous navigation of mobile robots,IEEE,Conferences,"For mobile robots, the most fundamental and pressing issue is that of autonomous navigation. Successful navigation of mobile robots is closely dependent on four vitals i.e. perception, localization, cognition and motion control. Implementation of each of these vital blocks requires consideration of at least one of the two well-known control architectures Deliberative Navigation Control and Reactive Navigation Control or a combination of the two, also known as a Hybrid Navigation Control. This paper compares each of these control architectures on the basis of their flexibility, ease of implementation, reactivity, robustness, efficiency and many other architecture specifications. The paper concludes with suggesting the schema that seems to be the best of each of these control schemes, on the basis of the analysis made, in order to cope with unknown and dynamic navigation problems encountered in real life scenarios.",https://ieeexplore.ieee.org/document/6828372/,2014 International Conference on Robotics and Emerging Allied Technologies in Engineering (iCREATE),22-24 April 2014,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCS45141.2019.9065549,A low power Artificial Intelligence Processor for Autonomous Mobile Robots,IEEE,Conferences,"The robot which makes use of AI as a mode of processing is getting more popular day by day, starting from the autonomous room cleaning robot to Amazon Prime Air. This autonomous robot overtakes traditional robots in following aspects such as implementing effective decision making in order to reduce the computational overhead by reducing the overall power usage of the robot. In this report, we have designed a low power [1] AIP without compensating in performance. The AIP which we have designed is a 64 processing element that uses parallel processing architecture. A map with 8 different routes is created in Xilinx where it calculates the shortest path from the source to destination using conditional operators. A* algorithm is implemented in Matlab to calculate the shortest distance and Dijkstra’s algorithm is converted to VHDL using Vivado HLS coder. A neural network is also created using Matlab to detect and avoid real time obstacle. The overall power report of the processor is implemented in Cadence.",https://ieeexplore.ieee.org/document/9065549/,2019 International Conference on Intelligent Computing and Control Systems (ICCS),15-17 May 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AIM.2001.936513,A radial basis function networks approach for the tracking problem of mobile robots,IEEE,Conferences,Proposes a radial basis function network (RBFN) approach to the solution of the tracking problem for mobile robots. RBFN-based controllers are investigated in order to introduce some degree of robustness in the control system and to avoid the main disadvantage of multilayer neural networks (MNN) to be highly nonlinear in the parameters. The training of the nets and the control performances analysis have been done in a real experimental setup. The proposed solutions are implemented on a PC-based control architecture for the real-time control of the LabMate mobile base and are compared with MNN-based control schemes. The experimental results are satisfactory in terms of tracking errors and computational efforts.,https://ieeexplore.ieee.org/document/936513/,2001 IEEE/ASME International Conference on Advanced Intelligent Mechatronics. Proceedings (Cat. No.01TH8556),8-12 July 2001,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISIE.2010.5637497,A society of agents for service robots,IEEE,Conferences,"This article presents an agent based distributed software architecture for machine and robot control. The functionality of agents of this architecture has been inspired by Marvin Minsky's definition of the term in his book “The Society of Mind” (1986) [1]. Minsky, widely considered to be one of the fathers of artificial intelligence, tried to describe from an engineering point of view, in this book, how he thought the mind works: “I'll call “Society of Mind” this scheme in which each mind is made of many smaller processes. These we'll call agents. Each mental agent by itself can only do some simple thing that needs no mind or thought at all. Yet when we join these agents in societies—in certain very special ways—this leads to true intelligence.” Societies of simple behaving agents have been implemented in Fatronik, in real robots, and have been demonstrated to be able to perform complex tasks in industrial environments. This article explains the features of such societies of agents and presents their implementation in a real robot.",https://ieeexplore.ieee.org/document/5637497/,2010 IEEE International Symposium on Industrial Electronics,4-7 July 2010,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CADCG.2009.5246869,A study on autonomous animated robots: Anibots,IEEE,Conferences,"In this paper, we demonstrate a design of autonomous virtual creatures (called animated robots: Anibots in this paper) and develop a design tool for animated robots. An animated robot can behave autonomously by using its own sensors and controllers on three-dimensional physically modeled environment. The developed tool can enable us to execute the simulation of Anibots on physical environment at any time during the modeling process. In order to simulate more realistic world, an approximate fluid environment model with low computational costs is presented. It is shown that a combinatorial use of neural network implementation for controllers and the genetic algorithm (GA) or the particle swarm optimization (PSO) is effective for emerging more realistic autonomous behaviours of animated robots.",https://ieeexplore.ieee.org/document/5246869/,2009 11th IEEE International Conference on Computer-Aided Design and Computer Graphics,19-21 Aug. 2009,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS.1994.407376,A two-phase navigation system for mobile robots in dynamic environments,IEEE,Conferences,"This paper presents an implemented navigation system for mobile robots in dynamic environments. In order to take advantage of existing knowledge of the world and to deal with unknown obstacles in real time, our system divides motion planning into global path planning and local reactive navigation. The former uses genetic algorithm methods to find a collision-free path; the latter is implemented using neural network techniques to track the path generated by the global planner while avoiding unknown obstacles on the way. As a result, the system can adapt to dynamic environmental changes. Our experiments, both in simulation and on a real robot, showed that the system can find a reasonably good free path in a fraction of the time necessary to find an optimal free path, and it can effectively achieve its goal configurations without collision.<>",https://ieeexplore.ieee.org/document/407376/,Proceedings of IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS'94),12-16 Sept. 1994,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICSMC.2004.1400779,A user-oriented framework for the design and implementation of pet robots,IEEE,Conferences,"In recent years, application of intelligent autonomous robots for home amusement has become an important research criterion, and pet robots have been designed to become the electronic toys for the next generation. To develop pet robots that can act in real time in the real world, this work adopts the behavior-based control architecture. In our control framework, an imitation-based learning system is included to build robot behaviors. Moreover an emotional model is embedded to the control architecture. By giving the pet robot an emotional model it can explicitly express its internal conditions through its various external behaviors, as the real living creature does. To evaluate the proposed framework, we have developed an interactive environment and successfully used it to design a pet robot.",https://ieeexplore.ieee.org/document/1400779/,"2004 IEEE International Conference on Systems, Man and Cybernetics (IEEE Cat. No.04CH37583)",10-13 Oct. 2004,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ROBIO.2009.5420410,AMF: A novel reactive approach for motion planning of mobile robots in unknown dynamic environments,IEEE,Conferences,"This paper presents a new approach based on Artificial Potential Fields (APF) which provides real-time and very effective methodology for practical motion planners in unknown dynamic environments. The Maxwell's equations are exploited to define Artificial Magnetoquasistatic Fields (AMF) as an extension of APF, which provides a predictive, intelligent, and natural behavior in contrast with other approaches. The essential aim of the AMF is dealing with moving obstacles, as well as static ones. The main idea is to consider an electrical current in the direction of each moving obstacle which induces magnetic field around it. These moving obstacles could be arbitrary in shape, size, and number. Neither the motion-trajectory of the moving obstacles nor the model of their motion is known. The only available information is their instantaneous velocity at each time step. In this method, the magnetoquasistatic approximation is used to obtain the electric and magnetic fields around robot. Next, using Lorentz equation, the necessary force can be calculated which should be applied to robot to avoid the collision with obstacles. A path planner based on this approach has been implemented and tested by various scenarios containing both static and moving obstacles. Simulations and experimental results illustrate the efficacy of the proposed method.",https://ieeexplore.ieee.org/document/5420410/,2009 IEEE International Conference on Robotics and Biomimetics (ROBIO),19-23 Dec. 2009,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/DevLrn.2012.6400818,ASP+POMDP: Integrating non-monotonic logic programming and probabilistic planning on robots,IEEE,Conferences,"Mobile robots equipped with multiple sensors and deployed in real-world domains frequently find it difficult to process all sensor inputs, or to operate without any human input and domain knowledge. At the same time, robots cannot be equipped with all relevant domain knowledge in advance, and humans are unlikely to have the time and expertise to provide elaborate and accurate feedback. This paper presents a novel framework that addresses these challenges by integrating high-level logical inference with low-level probabilistic sequential decision-making. Specifically, Answer Set Programming (ASP), a non-monotonic logic programming paradigm, is used to represent, reason with and revise domain knowledge obtained from sensor inputs and high-level human feedback, while hierarchical partially observable Markov decision processes (POMDPs) are used to automatically adapt visual sensing and information processing to the task at hand. Furthermore, a psychophysics-inspired strategy is used to merge the output of logical inference with probabilistic beliefs. All algorithms are evaluated in simulation and on wheeled robots localizing target objects in indoor domains.",https://ieeexplore.ieee.org/document/6400818/,2012 IEEE International Conference on Development and Learning and Epigenetic Robotics (ICDL),7-9 Nov. 2012,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ROMAN.2006.314387,Adaptive Social Skills for Robots Interacting with Virtual Characters in Real Worlds,IEEE,Conferences,"We propose the implementation of a new interaction type that allows the creation of adaptive social relationships between robots and virtual characters in a real world environment, using reinforcement learning. We present the implementation of a storytelling scenario, which results in an immersion experience for the robot. The robot is able to interact and learn dynamically from the virtual character",https://ieeexplore.ieee.org/document/4107778/,ROMAN 2006 - The 15th IEEE International Symposium on Robot and Human Interactive Communication,6-8 Sept. 2006,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISCID.2008.170,An Emotion Generation Model for Interactive Virtual Robots,IEEE,Conferences,"Making a computer generate its own emotion is an important part of the affective computing, and this would have wide applications in human-computer interaction and artificial intelligence. In this paper, we will describe an emotion generation model for a multimodal virtual human. The relationship among the emotion, mood and personality are discussed firstly, and the PAD (pleasure-arousal-dominance) emotion space is used to define the emotion and the mood. Then, we use a random graphical model to generate emotion based on the evaluation of the overall influence of mood, previous emotion and the outside stimulations. Finally, a 3D virtual human head with facial expressions is designed to show the emotion generation outputs. Experimental results demonstrate that the emotion generation model based on random graphs works effectively and meets the basic principle of human emotion generation.",https://ieeexplore.ieee.org/document/4725498/,2008 International Symposium on Computational Intelligence and Design,17-18 Oct. 2008,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/COASE.2018.8560532,An EtherCAT-Based Real-Time Control System Architecture for Humanoid Robots,IEEE,Conferences,"The design of humanoid robots naturally requires the simultaneous control of a high number of joints. Moreover, the performance of the overall robot is strongly determined by the low-level control system as all high-level software e.g. for locomotion planning and control is built on top of it. In order to achieve high update rates and high bandwidth for the joint control, an advanced real-time control system architecture is required. However, outdated communication protocols with associated limits in the achievable update rates are still used in nowadays humanoid robots. Moreover, the performance of the low-level control systems is not analyzed in detail or the systems rely on specialized hardware, which lacks reliability and persistence. We present a reliable and high-performance control system architecture for humanoid robots based on the ETHERCAT technology. To the authors' knowledge this is the only system, which operates at control rates beyond 2 khz and input/output latencies below 1 ms. Our control architecture includes a learning-based feedforward control strategy to improve joint tracking performance. The improved joint control method and the communication system are evaluated on our humanoid robot LOLA. Our software framework is available online to allow other researchers to benefit from our experiences.",https://ieeexplore.ieee.org/document/8560532/,2018 IEEE 14th International Conference on Automation Science and Engineering (CASE),20-24 Aug. 2018,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SoutheastCon42311.2019.9020532,An IoT-based Common Platform Integrating Robots and Virtual Characters for High Performance and Cybersecurity,IEEE,Conferences,"Two humanoid robots are developed. Both robots are human-like in appearance though one is more human-like than the other. A virtual human with human-like appearance is also developed. Various similar functionalities and interaction modalities for the robots and the virtual human are developed. Various technologies are incorporated with them to make them intelligent and autonomous. A common platform in the form of an internet of things (IoT) is developed that can integrate the robots and the virtual human for their real-world collaboration. Then, the collaboration between each robot and the virtual human is separately implemented via the common platform based on some control algorithms for finding a hidden object in a homely environment. The collaboration between the robot and the virtual human is evaluated. The status of cybersecurity in the IoT is briefly analyzed. The results show that the collaboration is satisfactory in various terms, which justify their social integration in the form of an IoT. Two robots with different appearance are actually used to investigate the effects of anthropomorphism on the interaction. The results can help employ artificial intelligent agents of heterogeneous realities to perform real-world tasks through their cooperation in the form of IoT that can provide high performance and cybersecurity.",https://ieeexplore.ieee.org/document/9020532/,2019 SoutheastCon,11-14 April 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CCECE.1993.332425,An adaptive control scheme for robots with unknown dynamics,IEEE,Conferences,"In this paper, a stable adaptive control scheme for robot manipulators with unknown dynamics is proposed. It consists of an off-line least-mean-square (LMS) type identifier to identify structured system dynamics and an online dynamic compensator to compensating for dynamic uncertainties. Taking advantage of the unique structure of the robot regressor dynamics, the former uses an LMS type algorithm to identify, using a set of trial data, the structured dynamic parameters of the robot while the latter uses an online stable parameter updating mechanism determined using Lyapunov theory to compensate for both unknown and uncertain dynamics. The off-line identified parameters we used as initial values for the online dynamic parameter estimation. Since both identifier and compensator are implemented using the regressor dynamics, the recursive formula, for the computation of robot regressor dynamics previously proposed can be used to achieve high computational efficiency in real-time implementations. An illustrative simulation example is included to show the proposed adaptive control algorithm.<>",https://ieeexplore.ieee.org/document/332425/,Proceedings of Canadian Conference on Electrical and Computer Engineering,14-17 Sept. 1993,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISMS.2010.32,Application of Feed-Forward Neural Network and MMI-Supervised Vector Quantizer to the Task of Content Based Audio Segmentation by Co-operative Unmanned Flying Robots,IEEE,Conferences,"This paper deals with the preliminary experiments on general audio segmentation using a MMI-supervised tree-based vector quantizer and feed-forward neural network. This method has been tested with the aim of detection of environmental sounds and speech in a sound stream. The segmentation of an audio stream is needed for successful localization of speech or environmental sounds in a stream and their possible future classification or even separation. This method has been developed as a preliminary solution of the task of real-world audio signal segmentation by a set of co-operative unmanned flying robots. Application of the proposed method has been tested in simulating software NESCUAR 1.0. (Natural Environment Simulator for Cooperative Unmanned Aerial Robots, version 1.0), a simulating software tool developed by the authors of this paper. The presented method can be also applied separately; its application is not dependent on the simulating software NESCUAR 1.0.",https://ieeexplore.ieee.org/document/5416111/,"2010 International Conference on Intelligent Systems, Modelling and Simulation",27-29 Jan. 2010,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/RTSE.1998.766515,Application of mobile autonomous robots to artificial intelligence and information systems curricula,IEEE,Conferences,"Applies pedagogical ideas of teaching curricula by using strategies of themes and breadth-first coverage, together with the technology of intelligent agents (e.g. mobile autonomous robots), to a system of courses in computer science (artificial intelligence) and information systems (systems engineering). The project brings the issues and constraints of real-time systems, especially the programming component, to students in computer science and information systems curricula. This project's background started in June 1997 and continued during the first part of the 1997-1998 academic year. The actual project work started in January 1998 and is still continuing.",https://ieeexplore.ieee.org/document/766515/,Proceedings Real-Time Systems Education III,21-21 Nov. 1998,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ROBOT.1993.291974,Application of neural network with real-time training to robust position/force control of multiple robots,IEEE,Conferences,A robust controller that compensates the uncertainties of the dynamic system of the multiple robotic system in order to obtain good tracking performance of position and force simultaneously while satisfying the constraint conditions is presented. A neural network architecture is proposed as one approach to its design and implementation. An online learning rule is provided for repeatedly assigned tasks so that the system is robust to the structured and unstructured uncertainties and the controller adjusts itself repeatedly to improve the performance progressively for each repeated task.<>,https://ieeexplore.ieee.org/document/291974/,[1993] Proceedings IEEE International Conference on Robotics and Automation,2-6 May 1993,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS45743.2020.9341340,Applying Surface Normal Information in Drivable Area and Road Anomaly Detection for Ground Mobile Robots,IEEE,Conferences,"The joint detection of drivable areas and road anomalies is a crucial task for ground mobile robots. In recent years, many impressive semantic segmentation networks, which can be used for pixel-level drivable area and road anomaly detection, have been developed. However, the detection accuracy still needs improvement. Therefore, we develop a novel module named the Normal Inference Module (NIM), which can generate surface normal information from dense depth images with high accuracy and efficiency. Our NIM can be deployed in existing convolutional neural networks (CNNs) to refine the segmentation performance. To evaluate the effectiveness and robustness of our NIM, we embed it in twelve state-of-the-art CNNs. The experimental results illustrate that our NIM can greatly improve the performance of the CNNs for drivable area and road anomaly detection. Furthermore, our proposed NIM-RTFNet ranks 8th on the KITTI road benchmark and exhibits a real-time inference speed.",https://ieeexplore.ieee.org/document/9341340/,2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),24 Oct.-24 Jan. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA46639.2022.9811771,Asynchronous Reinforcement Learning for Real-Time Control of Physical Robots,IEEE,Conferences,"An oft-ignored challenge of real-world reinforcement learning is that the real world does not pause when agents make learning updates. As standard simulated environments do not address this real-time aspect of learning, most available implementations of RL algorithms process environment interactions and learning updates sequentially. As a consequence, when such implementations are deployed in the real world, they may make decisions based on significantly delayed observations and not act responsively. Asynchronous learning has been proposed to solve this issue, but no systematic comparison between sequential and asynchronous reinforcement learning was conducted using real-world environments. In this work, we set up two vision-based tasks with a robotic arm, implement an asynchronous learning system that extends a previous architecture, and compare sequential and asynchronous reinforcement learning across different action cycle times, sensory data dimensions, and mini-batch sizes. Our experiments show that when the time cost of learning updates increases, the action cycle time in sequential implementation could grow excessively long, while the asynchronous implementation can always maintain an appropriate action cycle time. Consequently, when learning updates are expensive, the performance of sequential learning diminishes and is outperformed by asynchronous learning by a substantial margin. Our system learns in real-time to reach and track visual targets from pixels within two hours of experience and does so directly using real robots, learning completely from scratch. Our code is available at: https://github.com/YufengYuan/ur5_async_r1.",https://ieeexplore.ieee.org/document/9811771/,2022 International Conference on Robotics and Automation (ICRA),23-27 May 2022,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/FIE.2006.322654,Autonomous Robots as a Generic Teaching Tool,IEEE,Conferences,"An undergraduate bioengineering laboratory course using small autonomous robots has been developed to demonstrate control theory, learning, and behavior. The lab consists of several modules that demonstrate concepts in classical control theory, fuzzy logic, neural network control, and genetic algorithms. The autonomous agents are easy-to-build, inexpensive kit robots. Each robot functions independently in a real-world environment. Students program and retrieve data wirelessly using handheld computers. The hands-on nature of the lab modules engages students in ways that lectures, readings and software simulations cannot. By interacting with these robots, students directly experience the effects of unexpected environmental factors on designs and deviations from software simulations. The robots are easily adapted for use in many different aspects of two-year college and K-12 STEM education. Students are motivated to understand engineering, math and science principles in order to control the robots. Examples of use of the robots and modules by a local community college are presented",https://ieeexplore.ieee.org/document/4117154/,Proceedings. Frontiers in Education. 36th Annual Conference,27-31 Oct. 2006,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AERO47225.2020.9172804,Autonomous Search for Underground Mine Rescue Using Aerial Robots,IEEE,Conferences,"In this paper we present a comprehensive solution for autonomous underground mine rescue using aerial robots. In particular, a new class of Micro Aerial Vehicles are equipped with the ability to localize and map in subterranean settings, explore unknown mine environments on their own, and perform detection and localization of objects of interest for the purposes of mine rescue (i.e., “human survivors” and associated objects such as “backpacks”, “smartphones” or “tools”). For the purposes of GPS-denied localization and mapping in the visually-degraded underground environments (e.g., a smoke-filled mine during an accident) the solution relies on the fusion of LiDAR data with thermal vision frames and inertial cues. Autonomous exploration is enabled through a graph-based search algorithm and an online volumetric representation of the environment. Object search is then enabled through a deep learning-based classifier, while the associated location is queried using the online reconstructed map. The complete software framework runs onboard the aerial robots utilizing the integrated embedded processing resources. The overall system is extensively evaluated in real-life deployments in underground mines.",https://ieeexplore.ieee.org/document/9172804/,2020 IEEE Aerospace Conference,7-14 March 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA.2011.5980435,Autonomous learning of vision-based layered object models on mobile robots,IEEE,Conferences,"Although mobile robots are increasingly being used in real-world applications, the ability to robustly sense and interact with the environment is still missing. A key requirement for the widespread deployment of mobile robots is the ability to operate autonomously by learning desired environmental models and revising the learned models in response to environmental changes. This paper presents an approach that enables a mobile robot to autonomously learn layered models for environmental objects using temporal, local and global visual cues. A temporal assessment of image gradient features is used to detect candidate objects, which are then modeled using color distribution statistics and a spatial representation of gradient features. The robot incrementally revises the learned models and uses them for object recognition and tracking based on a matching scheme comprising a spatial similarity measure and second order distribution statistics. All algorithms are implemented and tested on a wheeled robot platform in dynamic indoor environments.",https://ieeexplore.ieee.org/document/5980435/,2011 IEEE International Conference on Robotics and Automation,9-13 May 2011,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SSCI.2018.8628809,Bidirectional Fuzzy Brain Emotional Learning Control for Aerial Robots,IEEE,Conferences,This paper proposes a Bidirectional Fuzzy Brain Emotional Learning (BFBEL) control system to control Aerial Robots. The proposed controller is based on the emotional and logical processing of the brain. The proposed control system merges fuzzy inference and a bidirectional brain emotional learning algorithm. The Bidirectional Fuzzy Brain Emotional Learning (BFBEL) control can learn from scratch and adapt rapidly in real-time to control the system without much prior information. The proposed controller is tested against simulations of both a 1-Degree-Of-Freedom (DOF) flapping wing and a 6DOF flapping wing model and successfully implemented on a 1DOF flapping wing experiment which showcases the learning and adaptation capability in a real-time environment.,https://ieeexplore.ieee.org/document/8628809/,2018 IEEE Symposium Series on Computational Intelligence (SSCI),18-21 Nov. 2018,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IECON.2007.4460382,Biomimetics Robots From Bio-inspiration to Implementation,IEEE,Conferences,"Biomimetics focuses on making nature as a model of inspiration that would immensely help conscious abstraction of new principles and ideas, foster innovative design collections, find out new techniques and functionalities, seek new paradigms and methods, develop new materials, and design new streams of intelligent machines, robots, systems, devices, algorithms, etc. Biomimetics incorporates materials, concepts and techniques drawn from naturally made substances, and resembles biological systems in structure, mechanism and/or function as necessary. Smart materials are the foundation supporting the development of new biomimetic based technology. Wide range of biologically inspired robots and intelligent systems has been developed. However, engineering such biomimetic intelligent creatures were hampered by physical and technological constraints, and it is still a challenge. Making robots and intelligent machines that are actuated by biologically inspired artificial muscles would create new reality with great potentials. This paper provides the concept and the importance of Biomimetic as an interdisciplinary field. In addition, the paper introduces and discusses scientific ideas and directions of research activities in the field. The paper presents key development in the field of Biomimetic robots, and finally it underlines the potential of the field and the challenges facing it.",https://ieeexplore.ieee.org/document/4460382/,IECON 2007 - 33rd Annual Conference of the IEEE Industrial Electronics Society,5-8 Nov. 2007,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/HUMANOIDS.2014.7041490,Can active impedance protect robots from landing impact?,IEEE,Conferences,"This paper studies the effect of passive and active impedance for protecting jumping robots from landing impacts. The theory of force transmissibility is used for selecting the passive impedance of the system to minimize the shock propagation. The active impedance is regulated online by a joint-level controller. On top of this controller, a reflex-based leg retraction scheme is implemented which is optimized using direct policy search reinforcement learning based on particle filtering. Experiments are conducted both in simulation and on a real-world hopping leg. We show that although the impact dynamics is fast, the addition of passive impedance provides enough time for the active impedance controller to react to the impact and protect the robot from damage.",https://ieeexplore.ieee.org/document/7041490/,2014 IEEE-RAS International Conference on Humanoid Robots,18-20 Nov. 2014,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ROMAN.2005.1513816,Cognitive robots: perceptual associative memory and learning,IEEE,Conferences,"In this position paper we attempt to derive an architecture and mechanism for perceptual associative memory and learning for software agents and cognitive robots from what is known, or believed, about the same faculties in human and other animal cognition. Based on that of the IDA model of global workspace theory, a conceptual and computational model of cognition, this architecture, together with its mechanisms, offers the real possibility of autonomous software agents and cognitive robots learning their own ontologies during a developmental period. Thus the onerous chore of designing and implementing such an ontology can be avoided.",https://ieeexplore.ieee.org/document/1513816/,"ROMAN 2005. IEEE International Workshop on Robot and Human Interactive Communication, 2005.",13-15 Aug. 2005,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA48506.2021.9560981,Continuous Shortest Path Vector Field Navigation on 3D Triangular Meshes for Mobile Robots,IEEE,Conferences,"We present a highly efficient approach to compute continuous shortest path vector fields on arbitrarily shaped 3D triangular meshes for robot navigation in complex real-world outdoor environments. The continuity of the vector field allows to query the shortest distance, direction and geodesic path to the goal at any point within the mesh triangles, resulting in accurate paths. In order to avoid impassable areas, our wavefront propagation method runs on a modular extendable multilayer map architecture taking different geometric cost layers into account. We describe the mathematical foundation of the geodesic distances and continuous vector field computation and demonstrate the performance in real-world and multilevel environments on our campus with a tunnel, ramps and stair- cases, and in a difficult, steep forest area with a stone quarry. For reproducibility, we provide a ready-to-use ROS software stack as well as Gazebo simulations.",https://ieeexplore.ieee.org/document/9560981/,2021 IEEE International Conference on Robotics and Automation (ICRA),30 May-5 June 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ROBOT.1998.677351,Cooperative behavior acquisition in multi-mobile robots environment by reinforcement learning based on state vector estimation,IEEE,Conferences,"This paper proposes a method that acquires robots' behaviors based on the estimation of the state vectors. In order to acquire the cooperative behaviors in multi-robot environments, each learning robot estimates the local predictive model between the learner and the other objects separately. Based on the local predictive models, the robots learn the desired behaviors using reinforcement learning. The proposed method is applied to a soccer playing situation, where a rolling ball and other moving robots are well modeled and the learner's behaviors are successfully acquired by the method. Computer simulations and real experiments are shown and a discussion is given.",https://ieeexplore.ieee.org/document/677351/,Proceedings. 1998 IEEE International Conference on Robotics and Automation (Cat. No.98CH36146),20-20 May 1998,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISIC.2000.882949,Cooperative learning and planning for multiple robots,IEEE,Conferences,"The paper deals with the the subject of learning and planning for real mobile robots, using Sutton's (1991) Dyna algorithm. The Dyna algorithm integrates reinforcement learning, planning and reactive execution. We present an extension of the Dyna algorithm which includes symmetric and cooperative learning with multiple robots. We applied the extended version of the algorithm to a population of two real robots. Practical problems associated with the implementation of the algorithm on a real setup are solved. Results obtained from simulations and real experiments are presented and discussed.",https://ieeexplore.ieee.org/document/882949/,Proceedings of the 2000 IEEE International Symposium on Intelligent Control. Held jointly with the 8th IEEE Mediterranean Conference on Control and Automation (Cat. No.00CH37147),19-19 July 2000,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS.2012.6385982,Cooperative sensing and recognition by a swarm of mobile robots,IEEE,Conferences,"We present an approach for distributed real-time recognition tasks using a swarm of mobile robots. We focus on the visual recognition of hand gestures, but the solutions that we provide have general applicability and address a number of challenges common to many distributed sensing and classification problems. In our approach, robots acquire and process hand images from multiple points of view, most of which do not allow for a satisfactory classification. Each robot is equipped with a statistical classifier, which is used to generate an opinion for the sensed gesture. Using a low-bandwidth wireless channel, the robots locally exchange their opinions. They also exploit mobility to adapt their positions to maximize the mutual information collectively gathered by the swarm. A distributed consensus protocol is implemented, to allow to rapidly settle on a decision once enough evidence is available. The system is implemented and demonstrated on real robots. In addition, extensive quantitative results of emulation experiments, based on a real image dataset, are reported. We consider different scenarios and study the scalability and the robustness of the swarm performance for distributed recognition.",https://ieeexplore.ieee.org/document/6385982/,2012 IEEE/RSJ International Conference on Intelligent Robots and Systems,7-12 Oct. 2012,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA48506.2021.9562019,Decentralized Circle Formation Control for Fish-like Robots in the Real-world via Reinforcement Learning,IEEE,Conferences,"In this paper, the circle formation control problem is addressed for a group of cooperative underactuated fish-like robots involving unknown nonlinear dynamics and disturbances. Based on the reinforcement learning and cognitive consistency theory, we propose a decentralized controller without the knowledge of the dynamics of the fish-like robots. The proposed controller can be transferred from simulation to reality. It is only trained in our established simulation environment, and the trained controller can be deployed to real robots without any manual tuning. Simulation results confirm that the proposed model-free robust formation control method is scalable with respect to the group size of the robots and outperforms other representative RL algorithms. Several experiments in the real world verify the effectiveness of our RL-based approach for circle formation control.",https://ieeexplore.ieee.org/document/9562019/,2021 IEEE International Conference on Robotics and Automation (ICRA),30 May-5 June 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.23919/ICCAS52745.2021.9650051,Deep Learning based-State Estimation for Holonomic Mobile Robots Using Intrinsic Sensors,IEEE,Conferences,"State estimation is a fundamental component of the navigation system of autonomous mobile robots. Generally, the robot setup is equipped with intrinsic and extrinsic sensors. The state estimators have relied almost on intrinsic sensors such as wheel encoders and inertial measurement units in textureless and structureless environments. This paper will analyze and propose the learning state estimation frameworks for the dead-reckoning of autonomous holonomic vehicles based only on intrinsic sensors. First, we review and categories the intrinsic-only estimation problem. Second, we describe the problem formulation using learning-based techniques. Next, the learning inertial-only estimation is presented with several strategies using the deep learning technique. The initial experiment results are analyzed and deployed using a holonomic mobile robot in real-world environments.",https://ieeexplore.ieee.org/document/9650051/,"2021 21st International Conference on Control, Automation and Systems (ICCAS)",12-15 Oct. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA48506.2021.9561729,Deep Neuromorphic Controller with Dynamic Topology for Aerial Robots,IEEE,Conferences,"Current aerial robots are increasingly adaptive; they can morph to enable operation in changing conditions to complete diverse missions. Each mission may require the robot to conduct a different task. A conventional learning approach can handle these variations when the system is trained for similar tasks in a representative environment. However, it may result in overfitting to the new data stream or the failure to adapt, leading to degradation or a potential crash. These problems can be mitigated with an excessive amount of data and embedded model, but the computational power and the memory of the aerial robots are limited. In order to address the variations in the model, environment as well as the tasks within onboard computation limitations, we propose a deep neuromorphic controller approach with variable topologies to handle each different condition and the data stream with a feasible computation and memory allocation. The proposed approach is based on a deep neuromorphic (multi and variable layered neural network) controller with dynamic depth and progressive layer adaptation for each new data stream. This adaptive structure is combined with a switching function to form a sliding mode controller. The network parameter update rule guarantees the stability of the closed loop system by the convergence of the error dynamics to the sliding surface. Being the first implementation on an aerial robot in this context, the results illustrate the adaptation capability, stability, computational efficiency as well as the real-time validation.",https://ieeexplore.ieee.org/document/9561729/,2021 IEEE International Conference on Robotics and Automation (ICRA),30 May-5 June 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SoutheastCon44009.2020.9249654,Deep Reinforcement Learning For Visual Navigation of Wheeled Mobile Robots,IEEE,Conferences,"A study is presented on applying deep reinforcement learning (DRL) for visual navigation of wheeled mobile robots (WMR) in dynamic and unknown environments. Two DRL algorithms, namely, value-learning deep Q-network (DQN) and policy gradient based asynchronous advantage actor critic ( A 3C), have been considered. RGB (red, green and blue) and depth images have been used as inputs in implementation of both DRL algorithms to generate control commands for autonomous navigation of WMR in simulation environments. The initial DRL networks were generated and trained progressively in OpenAI Gym Gazebo based simulation environments within robot operating system (ROS) framework for a popular target WMR, Kobuki TurtleBot2. A pre-trained deep neural network ResNet50 was used after further training with regrouped objects commonly found in laboratory setting for target-driven mapless visual navigation of Turlebot2 through DRL. The performance of A 3C with multiple computation threads (4, 6, and 8) was simulated on a desktop. The navigation performance of DQN and A 3C networks, in terms of reward statistics and completion time, was compared in three simulation environments. As expected, A 3C with multiple threads (4, 6, and 8) performed better than DQN and the performance of A 3C improved with number of threads. Details of the methodology, simulation results are presented and recommendations for future work towards real-time implementation through transfer learning of the DRL models are outlined.",https://ieeexplore.ieee.org/document/9249654/,2020 SoutheastCon,28-29 March 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/WCNC45663.2020.9120611,Deep Reinforcement Learning based Indoor Air Quality Sensing by Cooperative Mobile Robots,IEEE,Conferences,"Confronted with the severe indoor air pollution nowadays, we propose the usage of multiple robots to detect the indoor air quality (IAQ) cooperatively for fewer sensors and larger sensing area. To acquire the complete real-time IAQ distribution map, we exploit the real statistical data to construct the IAQ data model and adopt Kalman Filter to obtain the estimation of the unmeasured area. Since the movement of the robots affects the estimation accuracy, a proper movement strategy should be planned to minimize the total estimation error. To solve this optimization problem, we design a deep Q-learning approach, which provides sub-optimal movement strategies for real-time robot sensing. By simulations, we verify the adopted IAQ data model and testify the effectiveness of the proposed solution. For application considerations, we have deployed this system in Peking University since Dec. 2018 and developed a website to visualize the IAQ distribution.",https://ieeexplore.ieee.org/document/9120611/,2020 IEEE Wireless Communications and Networking Conference (WCNC),25-28 May 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICUAS.2019.8797770,Deep learning based semantic situation awareness system for multirotor aerial robots using LIDAR,IEEE,Conferences,"In this work, we present a semantic situation awareness system for multirotor aerial robots, based on 2D LIDAR measurements, targeting the understanding of the environment and assuming to have a precise robot localization as an input of our algorithm. Our proposed situation awareness system calculates a semantic map of the objects of the environment as a list of circles represented by their radius, and the position and the velocity of their center in world coordinates. Our proposed algorithm includes three main parts. First, the LIDAR measurements are preprocessed and an object segmentation clusters the candidate objects present in the environment. Secondly, a Convolutional Neural Network (CNN) that has been designed and trained using an artificially generated dataset, computes the radius and the position of the center of individual circles in sensor coordinates. Finally, an indirect-EKF provides the estimate of the semantic map in world coordinates, including the velocity of the center of the circles in world coordinates.We have quantitative and qualitative evaluated the performance of our proposed situation awareness system by means of Software-In-The-Loop simulations using VRep with one and multiple static and moving cylindrical objects in the scene, obtaining results that support our proposed algorithm. In addition, we have demonstrated that our proposed algorithm is capable of handling real environments thanks to real laboratory experiments with non-cylindrical static (i.e. a barrel) and moving (i.e. a person) objects.",https://ieeexplore.ieee.org/document/8797770/,2019 International Conference on Unmanned Aircraft Systems (ICUAS),11-14 June 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISORC.2019.00025,DeepNNCar: A Testbed for Deploying and Testing Middleware Frameworks for Autonomous Robots,IEEE,Conferences,"This demo showcases the features of an adaptive middleware framework for resource constrained autonomous robots like DeepNNCar (Figure 1). These robots use Learning Enabled Components (LECs), trained with deep learning models to perform control actions. However, these LECs do not provide any safety guarantees and testing them is challenging. To overcome these challenges, we have developed an adaptive middleware framework that (1) augments the LEC with safety controllers that can use different weighted simplex strategies to improve the systems safety guarantees, and (2) includes a resource manager to monitor the resource parameters (temperature, CPU Utilization), and offload tasks at runtime. Using DeepNNCar we will demonstrate the framework and its capability to adaptively switch between the controllers and strategies based on its safety and speed performance.",https://ieeexplore.ieee.org/document/8759365/,2019 IEEE 22nd International Symposium on Real-Time Distributed Computing (ISORC),7-9 May 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IEEECONF49454.2021.9382646,Development and Testing of Garbage Detection for Autonomous Robots in Outdoor Environments,IEEE,Conferences,"In Japan, there is a growing concern about labor shortages due to the declining birthrate and aging population, and there are high expectations for robots to help solve such social problems and create industries. However, due to the prohibition of public road tests in Japan, there are few examples of actual applications of robots. Therefore, considerations and problems in the practical application of robots are still unclear. In this paper, by focusing on the implementation of garbage collection technology, we have developed an autonomous garbage collection robot using deep learning. In addition, we have verified the usefulness of our garbage detection technology in outdoor environments by conducting actual demonstrations at HANEDA INNOVATION CITY, which is a large-scale commercial and business complex belonged private property, Utsunomiya University, and Nakanoshima Challenge 2019, which is a field of demonstration experiment in the outdoor environment. Our garbage detector was designed to detect cans, plastic bottles, and lunch boxes automatically. Through experiments on test data and outdoor experiments in the real-world, we have confirmed that our detector has a 95.6% Precision and 96.8% Recall. Conparisons to other state-of-the-art detectors are also presented.",https://ieeexplore.ieee.org/document/9382646/,2021 IEEE/SICE International Symposium on System Integration (SII),11-14 Jan. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/EMS.2017.12,Development of Components of Multi-agent CASE-System for Describing the Logic of Behavior of Mobile Robots,IEEE,Conferences,"In the article there are substantiation of architectural and technical solutions, with the basis of the universal CASE-tool for describing (""programming"") the behavior of mobile robots. The development tool intended for carrying out experiments in the field of artificial intelligence and it is based on multi-agent technology. In addition, the toolkit will be the maximum possible reuse of elements (tasks, processes, etc.). The basis for the development is the idea of combining, within the framework of one tool, both the real execution of the algorithm by the robot, and its simulation. It allows talking about testing partially implemented hardware (sensors and actuators). Development is carried out based on open source technology; all texts of programs are available at web source: https://github.com/unclesal/tenguai.",https://ieeexplore.ieee.org/document/8356782/,2017 European Modelling Symposium (EMS),20-21 Nov. 2017,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICAR53236.2021.9659389,Distributed Active Learning for Semantic Segmentation on Walking Robots,IEEE,Conferences,"Quickly adapting to new and unknown environments is a vital functionality for autonomous robots. To increase their capabilities, they need a sophisticated self- and environmental awareness. Understanding the surroundings can be aided by a pixelwise semantic segmentation of the images taken by the robot. However, to achieve viable results a large database of annotated images is needed in advance. To aid in a quick understanding of the surroundings a distributed active learning approach for semantic segmentation is presented. The robot evaluates and selects images where it thinks annotation would lead to a better overall segmentation results. Since the robot is potentially on an autonomous remote mission, the communication might be limited. Therefore, the robot selects images which are stored in a buffer before they are transmitted in batches to the base station. The annotation of the images and training of a new model for the segmentation can be performed asynchronously to the robot's mission. The model of the robot is then updated once ready. Four different stream-based query metrics are deployed and tested on a dataset and on ANYmal in a real scenario. The proposed approach allows a robot to be deployed in an unknown surrounding and through quick and continuous learning it gains an understanding of the area.",https://ieeexplore.ieee.org/document/9659389/,2021 20th International Conference on Advanced Robotics (ICAR),6-10 Dec. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/WCICA.2018.8630497,Dynamic Gesture Recognition Algorithm based on ROI and CNN for Social Robots,IEEE,Conferences,"To eliminate barriers to communication between social robots and disabled people in listening and/or speaking, this work proposes a gesture recognition method that is based on the region of interest (ROI) and a convolutional neural network (CNN) for social robots. This method can track and recognize gestures in real time in a complex background. ROI and OpenCV are first used to obtain the dynamic gesture, which is then treated as the input of a CNN model to output a gesture feature model. Furthermore, a gesture-controlled social robot is implemented by the obtained feature model. Finally, the performance of the system is verified, and experimental results show that the proposed technology can track and recognize user gestures in real time.",https://ieeexplore.ieee.org/document/8630497/,2018 13th World Congress on Intelligent Control and Automation (WCICA),4-8 July 2018,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS.2016.7759250,Efficient learning of stand-up motion for humanoid robots with bilateral symmetry,IEEE,Conferences,"Standing up after falling is an essential ability for humanoid robots in order to resume their tasks without help from humans. Although many humanoid robots, especially small-size humanoid robots, have their own stand-up motions, there has not been a generalized method to automatically learn flexible stand-up motions for humanoid robots which can be applied to various fallen positions. In this research, we propose a method for learning stand-up motions for humanoid robots using Q-learning making use of their bilateral symmetry. We implemented this method on DarwIn-OP humanoid robots and learned an optimal policy in simulation. We compared the resulting stand-up motion with manually designed stand-up motions and with stand-up motions learned without considering bilateral symmetry. Both in simulation and on the real robot, the new stand-up motion was successful in most trials while other motions took longer or were not as robust.",https://ieeexplore.ieee.org/document/7759250/,2016 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),9-14 Oct. 2016,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS.2001.976268,Embedding cooperation in robots to play soccer game,IEEE,Conferences,"Robotic soccer provides an opportunity to explore such a challenging research topic that multiple agents (physical robots or sofbots) work together in a realtime, noisy and adversarial environment to obtain specific objectives. It requires each agent can not only deal with infinite unpredictable situations, but also present cooperation with others. The previous researches about cooperation often put emphasis on task decomposition and conflict avoidance among team members. In this paper, we describe a robot architecture, which addresses ""scaling cooperation"" among robots, and meanwhile keeps each robot making decision independently. The architecture is based on ""ideal cooperation"" principle and implemented for Small Robot League in RoboCup Experimental results prove its effectiveness and reveal several primary characteristics of behaviors in robotic soccer. Finally, some important problems of future work are discussed.",https://ieeexplore.ieee.org/document/976268/,Proceedings 2001 IEEE/RSJ International Conference on Intelligent Robots and Systems. Expanding the Societal Role of Robotics in the the Next Millennium (Cat. No.01CH37180),29 Oct.-3 Nov. 2001,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CMCSN.2012.100,Experimental Study on Long-Range Navigation Behavior of Agricultural Robots,IEEE,Conferences,"In this paper, we study on the navigating bahevior of CRFNFP[1]-based agricultual robots in real scenes. We designed two sets of experiments and three navigating system with different configuration of software for comparative study. The experimental results indicate that, compared to the traditional local-map-based navigating systems, the CRFNFP-based navigating system does enhance the long-range perception for mobile robots and helps planning more efficient paths for the navigation.",https://ieeexplore.ieee.org/document/6245858/,"2012 International Conference on Computing, Measurement, Control and Sensor Network",7-9 July 2012,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AMC.2019.8371065,"Extending the life of legacy robots: MDS-Ach, a real-time, process based, networked, secure middleware based on the x-Ach methodology",IEEE,Conferences,"This work shows how to add modern tools to legacy robots while retaining the original tools and original calibration procedures/utilities through the use of a lightweight middleware connected to the communications level of the robot. MDS-Ach is a middleware made for the Xitome Mobile Dexterous Social (MDS) Robot originally released in 2008. The robot is being actively used at multiple locations including the U.S. Naval Research Laboratory's Laboratory for Autonomous Systems Research (NRL-LASR). The MDS-Ach middleware gives the MDS Robot the software capabilities of modern robot systems using the x- Ach real-time processes based architecture. It controls the MDS Robot directly over the controller area network (CAN) bus via a dedicated real-time daemon. Each process communicates with the others over a network capable shared memory. The shared memory is a ""first-in-last-out"" (i.e. reads the newest data first) non-head-of-line blocking ring buffer which ensures readability of latest data first while retaining the ability to retrieve the older data. When running over a network, UDP or TCP protocol can be utilized depending on the timing and reliability requirements. SSH tunneling is used when secure connections between networked controllers are required. The MDS-Ach middleware is designed to allow for simple and easy development with modern robotic tools while adding accessibility and usability to our non-hardware-focused partners. Real-time collision avoidance and a robust inverse kinematics solution are implemented within the MDS-Ach system. Examples of collision avoidance, inverse kinematics implementation, and the software architecture are given.",https://ieeexplore.ieee.org/document/8371065/,2018 IEEE 15th International Workshop on Advanced Motion Control (AMC),9-11 March 2018,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICSyS47076.2019.8982469,FPGA-enabled Binarized Convolutional Neural Networks toward Real-time Embedded Object Recognition System for Service Robots,IEEE,Conferences,"In this presentation, we report the results of applying a binarized Convolutional Neural Network (CNN) and a Field Programmable Gate Array (FPGA) for image-based object recognition. While the demand rises for robots with robust object recognition implemented with Neural Networks, a tradeoff between data processing rate and power consumption persists. Some applications utilise Graphics Processing Units (GPU), which results in high power consumption, thus undesirable for embedded systems, while the others communicate with cloud computers to minimise computational resources at the clients' side, i.e. robots, raising another concern that the robots are unable to perform object recognition without the servers and network connections. To overcome these difficulties, we propose an embedded object recognition system implemented with a binarized CNN and an FPGA. FPGAs consist of a matrix of reconfigurable logic gates allowing parallel computing which befit most image processing algorithms such as the CNN. We train the binarized CNN on one of our datasets that contain images of several kinds of food and beverages. The results of the experiments show that the binarized CNN with an FPGA maintains high accuracy as well as real-time computation, suggesting that the proposed system is suitable for robots to perform their tasks in a real-world environment without needing to communicate with a server.",https://ieeexplore.ieee.org/document/8982469/,2019 IEEE International Circuits and Systems Symposium (ICSyS),18-19 Sept. 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCC54389.2021.9674390,Face Recognition System with Feature Fusion for Rehabilitation Robots in Healthcare,IEEE,Conferences,"Face recognition is a research problem across multiple disciplines such as computer vision, pattern recognition, artificial intelligence, psychology, healthcare etc. At present, the rehabilitation centers in hospitals are dependent on the nursing staff with no implementation of rehabilitation automation. In this study of face detection, a facial expression recognition system was developed which can be useful for robots of rehabilitation nursing beds. This article mainly researches two aspects: face image feature extraction and classifier design. Since many low-resolution face images are often encountered in actual environments (such as surveillance), if you use traditional methods to directly extract image features, the results are often not satisfactory. For the influence of face recognition results, this paper proposes a feature fusion extraction method based on KNN with LDA(Fisherfaces) and PCA (eigenfaces). First, the original image (low resolution) is decomposed into three different resolution images, and then the three images are used as the basis for the position to extract facial features, then LDA and PCA are used to perform preliminary analysis to obtain reduced dimensional reduction features. Finally use the K-nearest neighbor classifier to perform classification prediction. Study compares KNN-LDA with KNN-PCA with different distances of KNN algorithm.",https://ieeexplore.ieee.org/document/9674390/,2021 7th International Conference on Computer and Communications (ICCC),10-13 Dec. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/COASE.2017.8256157,Full automatic path planning of cooperating robots in industrial applications,IEEE,Conferences,"Parts made of carbon fiber reinforced plastics (CFRP) for airplane components can be so huge that a single industrial robot is no longer able to handle them, and cooperating robots are required. Manual programming of cooperating robots is difficult, but with large numbers of different sized and shaped cut-pieces, it is almost impossible. This paper presents an automated production system consisting of a camera for the precise detection of the position of each cut-piece and a collision-free path planner which can dynamically react to different positions for the transfer motions. The path is planned for multiple robots adhering to motion constrains, such as the requirement that the textile cut-piece must form a catenary which can change during transport. Additionally a technique based on machine learning has been implemented which correctly resolves redundancy for a linear axis during planning. Finally, all components are tested on a real robot system in industrial scale.",https://ieeexplore.ieee.org/document/8256157/,2017 13th IEEE Conference on Automation Science and Engineering (CASE),20-23 Aug. 2017,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/TAI.2000.889888,History checking of temporal fuzzy logic formulas for monitoring behavior-based mobile robots,IEEE,Conferences,"Behavior-based robot control systems have shown remarkable success for controlling robots evolving in real world environments. However, they can fail in different manners due to their distributed control and their local decision making. In this case, monitoring can be used to detect failures and help to recover from them. In this work, we present an approach for specifying monitoring knowledge and a method for using this knowledge to detect failures. In particular we show how temporal fuzzy logic can be used to represent monitoring knowledge and then utilized to effectively detect runtime failures. New semantics are introduced to take into consideration uncertainty and noisy information. There are numbers of advantages to our approach including a declarative semantics for the monitoring knowledge and an independence of this knowledge from the implementation details of the control system. Moreover we show how our system can deal effectively with noisy information and sensor readings. Experiments with two real world robots and the simulator are used to illustrate failure examples and the benefits of failure detection and noise elimination.",https://ieeexplore.ieee.org/document/889888/,Proceedings 12th IEEE Internationals Conference on Tools with Artificial Intelligence. ICTAI 2000,15-15 Nov. 2000,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CEC.2009.4983067,HyperNEAT controlled robots learn how to drive on roads in simulated environment,IEEE,Conferences,"In this paper we describe simulation of autonomous robots controlled by recurrent neural networks, which are evolved through indirect encoding using HyperNEAT algorithm. The robots utilize 180 degree wide sensor array. Thanks to the scalability of the neural network generated by HyperNEAT, the sensor array can have various resolution. This would allow to use camera as an input for neural network controller used in real robot. The robots were simulated using software simulation environment. In the experiments the robots were trained to drive with imaximum average speed. Such fitness forces them to learn how to drive on roads and avoid collisions. Evolved neural networks show excellent scalability. Scaling of the sensory input breaks performance of the robots, which should be gained back with re-training of the robot with a different sensory input resolution.",https://ieeexplore.ieee.org/document/4983067/,2009 IEEE Congress on Evolutionary Computation,18-21 May 2009,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AIIoT52608.2021.9454183,Image Classification with Knowledge-Based Systems on the Edge for Real-Time Danger Avoidance in Robots,IEEE,Conferences,"Mobile robots are increasingly common in society and are increasingly being used for complex and high-stakes tasks such as search and rescue. The growing requirements for these robots demonstrate a need for systems which can review and react in real time to environmental hazards, which will allow robots to handle environments that are both dynamic and dangerous. We propose and test a system which allows mobile robots to reclassify environmental objects during operation in conjunction with an edge system. We train an image classification model with 99 percent accuracy and deploy it in conjunction with an edge server and JSON-based ruleset to allow robots to react to and avoid hazards.",https://ieeexplore.ieee.org/document/9454183/,2021 IEEE World AI IoT Congress (AIIoT),10-13 May 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ELECTR.1991.718282,Imaging And Controls For Mars Robots With Neural Networks,IEEE,Conferences,"Two aspects of the design of space robots is covered implemented by neural networks and by hybrid approach with artificial intelligence. One is a neurocontroller for a real-time autonomous system. An optical control system developed saves the time for the image processing that analyzes an image sensor through the environment and induces a transformation over the sensor array. A prototype of the neurocontroller is able to learn and control by itself. The second aspect deals with the design of a Servo Control System for a Robot with the capability of ""learning in Unanticipated Situations"" incorporated in the system. The robot is assumed to be employed to perform useful tasks in an alien evironment. The model developed is shown to provide the robot with the capability to recover from unanticipated situations that can lead to the disruption of its normal operation, and to learn to avoid such situations in the future. These two aspects will be integrated for a design of a very intelligent autonomous space robot.",https://ieeexplore.ieee.org/document/718282/,"Electro International, 1991",16-18 April 1991,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ROBOT.1991.131908,Instinctive behaviors and personalities in societies of cellular robots,IEEE,Conferences,"A description is presented of the social organization of societies of cellular mobile units featuring instinctive behavior. Each robotic unit has its own personality and lives independently from the others. Useful tasks are carried out through collaboration rather than by individual effort. The behavior of each unit derives from a subsumption-like control structure, which emphasizes the roles of innate personality, external stimuli, and communication. A number of different robotic personalities are described and techniques of implementing them in real robot units are outlined. The implementation of instinctive behavior is described for the case of a robotic vehicle system (ROBBIE).<>",https://ieeexplore.ieee.org/document/131908/,Proceedings. 1991 IEEE International Conference on Robotics and Automation,9-11 April 1991,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IJCNN.2014.6889647,Intelligent Facial Action and emotion recognition for humanoid robots,IEEE,Conferences,"This research focuses on the development of a realtime intelligent facial emotion recognition system for a humanoid robot. In our system, Facial Action Coding System is used to guide the automatic analysis of emotional facial behaviours. The work includes both an upper and a lower facial Action Units (AU) analyser. The upper facial analyser is able to recognise six AUs including Inner and Outer Brow Raiser, Upper Lid Raiser etc, while the lower facial analyser is able to detect eleven AUs including Upper Lip Raiser, Lip Corner Puller, Chin Raiser, etc. Both of the upper and lower analysers are implemented using feedforward Neural Networks (NN). The work also further decodes six basic emotions from the recognised AUs. Two types of facial emotion recognisers are implemented, NN-based and multi-class Support Vector Machine (SVM) based. The NN-based facial emotion recogniser with the above recognised AUs as inputs performs robustly and efficiently. The Multi-class SVM with the radial basis function kernel enables the robot to outperform the NN-based emotion recogniser in real-time posed facial emotion detection tasks for diverse testing subjects.",https://ieeexplore.ieee.org/document/6889647/,2014 International Joint Conference on Neural Networks (IJCNN),6-11 July 2014,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICETIETR.2018.8529028,IoT Enabled Robots with QR Code Based Localization,IEEE,Conferences,"Robots are sophisticated form of IoT devices as they are smart devices that scrutinize sensor data from multiple sources and observe events to decide the best procedural actions to supervise and manoeuvre objects in the physical world. In this paper, localization of the robot is addressed by QR code Detection and path optimization is accomplished by Dijkstras algorithm. The robot can navigate automatically in its environment with sensors and shortest path is computed whenever heading measurements are updated with QR code landmark recognition. The proposed approach highly reduces computational burden and deployment complexity as it reflects the use of artificial intelligence to self-correct its course when required. An Encrypted communication channel is established over wireless local area network using SSHv2 protocol to transfer or receive sensor data(or commands) making it an IoT enabled Robot.",https://ieeexplore.ieee.org/document/8529028/,2018 International Conference on Emerging Trends and Innovations In Engineering And Technological Research (ICETIETR),11-13 July 2018,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS51168.2021.9636458,Laser-Based Side-by-Side Following for Human-Following Robots,IEEE,Conferences,"A mobile robot that follows behind humans in structured environments has to face the challenge of full occlusion caused by the walls when the target person makes a turn at the corridor intersections. This may result in short-term, even a permanent loss of the target from the field of view of the Human-Following Robots (HFRs). Concerning this issue, a novel side-by-side following method for HFRs is addressed. In this paper, HFRs detect the legs of target person and different types of corridor intersections using the onboard laser scanner at first. Then, we provide a corridor detector method to cluster the geometric structure constraint between the target and corridor intersections. At last, a Side-by-side Following Leg Tracker (SFLT) is designed by integrating the laser information, in order to increase the visible time of the target person, while the target is turning at the corridor intersections. The corridor detector method and SFLT method have been simulated in MATLAB. Moreover, the approach of side-by-side following has been implemented in the Robot Operating System (ROS) of real-life robots in the corridor environment. The results from simulation and practical experiment show that, by using our method, HFRs were able to successfully follow the human92.0% while a mobile robot meeting potential occlusions at corridor intersections.",https://ieeexplore.ieee.org/document/9636458/,2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),27 Sept.-1 Oct. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/UR52253.2021.9494666,Learning Multi-modal Attentional Consensus in Action Recognition for Elderly-Care Robots,IEEE,Conferences,"This paper addresses a practical action recognition method for elderly-care robots. Multi-stream based models are one of the promising approaches for solving the complexity of real-world environments. While multi-modal action recognition have been actively studied, there is a lack of research on models that effectively combine features of different modalities. This paper proposes a new mid-level feature fusion method for two-stream based action recognition network. In multi-modal approaches, extracting complementary information between different modalities is an essential task. Our network model is designed to fuse features at an intermediate level of feature extraction, which leverages a whole feature map from each modality. Consensus feature map and consensus attention mechanism are proposed as effective ways to extract information from two different modalities: RGB data and motion features. We also introduce ETRI-Activity3D-LivingLab, a real-world RGB-D dataset for robots to recognize daily activities of the elderly. It is the first 3D action recognition dataset obtained in a variety of home environments where the elderly actually reside. We expect our new dataset to contribute to the practical study of action recognition with the previously released ETRI-Activity3D dataset. To prove the effectiveness of the method, extensive experiments are performed on NTU RGB+D, ETRI-Activity3D and, ETRI-Activity3D-LivingLab dataset. Our mid-level fusion method achieves competitive performance in various experimental settings, especially for domain-changing situations.",https://ieeexplore.ieee.org/document/9494666/,2021 18th International Conference on Ubiquitous Robots (UR),12-14 July 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS51168.2021.9635911,Learning Navigation Skills for Legged Robots with Learned Robot Embeddings,IEEE,Conferences,"Recent work has shown results on learning navigation policies for idealized cylinder agents in simulation and transferring them to real wheeled robots. Deploying such navigation policies on legged robots can be challenging due to their complex dynamics, and the large dynamical difference between cylinder agents and legged systems. In this work, we learn hierarchical navigation policies that account for the low-level dynamics of legged robots, such as maximum speed, slipping, contacts, and learn to successfully navigate cluttered indoor environments. To enable transfer of policies learned in simulation to new legged robots and hardware, we learn dynamics-aware navigation policies across multiple robots with robot-specific embeddings. The learned embedding is optimized on new robots, while the rest of the policy is kept fixed, allowing for quick adaptation. We train our policies across three legged robots in simulation - 2 quadrupeds (A1, AlienGo) and a hexapod (Daisy). At test time, we study the performance of our learned policy on two new legged robots in simulation (Laikago, 4-legged Daisy), and one real-world quadrupedal robot (A1). Our experiments show that our learned policy can sample-efficiently generalize to previously unseen robots, and enable sim-to-real transfer of navigation policies for legged robots.",https://ieeexplore.ieee.org/document/9635911/,2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),27 Sept.-1 Oct. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AIVR52153.2021.00038,Machine Learning Concepts for Dual-Arm Robots within Virtual Reality,IEEE,Conferences,"The collaboration between humans and artificial intelligence (AI) driven robots lay the foundations for new approaches in industrial production. However, intensive research is required to develop machine learning behavior that is not only able to execute shared tasks but also acts following the expectations of the human partner. Rigid setups and restrictive safety measures deny the acquisition of adequate training samples to build general-purpose machine learning solutions for evaluation within experimental studies. Based on established research that trains AI systems within simulated environments, we present a machine learning implementation that enables the training of a dual-arm robot within a virtual reality (VR) application. Building upon preceding research, an activity diagram for a shared task for the machine learning model to learn, was conceptualized. A first approach, using vector distances, led to flawed results, whereas a revised solution based on collision boxes resulted in a stable outcome. While the implementation of the machine learning model is fixed on the activity diagram of the shared task, the presented approach is expandable as a universal platform for evaluating Human-Robot Collaboration (HRC) scenarios in VR. Future iterations of this VR sandbox application can be used to explore optimal workplace arrangements and procedures with autonomous industrial robots in a wide range of possible scenarios.",https://ieeexplore.ieee.org/document/9644376/,2021 IEEE International Conference on Artificial Intelligence and Virtual Reality (AIVR),15-17 Nov. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/LARS.2008.23,MecaTeam Framework: An Infrastructure for the Development of Soccer Agents for Simulated Robots,IEEE,Conferences,"This paper presents the MecaTeam framework, a solution to reduce the effort on developing new soccer teams of robots for the 2D simulation category of the RoboCup. MecaTeam is an object-oriented framework based on features of two robot soccer teams: the MecaTeam 2006 and Uva Trilearn. The architecture of the proposed framework is presented and aspects of its use are discussed. Besides facilitating the development of new teams, the use of the MecaTeam framework may decrease the impact of changes in chunks of related code. Finally, the MecaTeam framework can be used by new researchers interested in simulated robots for soccer games.",https://ieeexplore.ieee.org/document/4812639/,2008 IEEE Latin American Robotic Symposium,29-30 Oct. 2008,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISCCSP.2008.4537417,Neuro-adaptive dynamic control for mobile robots: Experimental validation,IEEE,Conferences,"This paper reports on the design and implementation of a neuro-adaptive controlled nonholonomic mobile robot. It presents experimental results to validate the employed control scheme on a physical setup for the first time, after it was originally proposed by the same authors and tested by simulations only. The control system is composed of a trajectory tracking kinematic controller which generates the reference wheel velocities, and a cascaded dynamic controller which employs a neural network for the real-time estimation of the robot's nonlinear dynamics so as to attain precise velocity tracking, even in the presence of unknown and/or time-varying dynamics. Details about the hardware and software setup, as well as salient implementation issues are also reported in this work.",https://ieeexplore.ieee.org/document/4537417/,"2008 3rd International Symposium on Communications, Control and Signal Processing",12-14 March 2008,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IJCNN.2001.938487,Neuro-controller for high performance induction motor drives in robots,IEEE,Conferences,"Presents an approach to the speed control of an induction motor (IM) as a robust high performance drive (HPD) using an online self-tuning adapted artificial neural network (ANN). Based on motor dynamics and nonlinear unknown load characteristics such as robot systems, a neuro speed controller is developed. The proposed controller is very simple and serves as an identifier and a controller at the same time. The combination of the adaptive learning rate with the epochs used through the online training offers a unique feature of system identification and adaptive control. The performance of the controller was evaluated under various operating conditions to track different speed trajectories. The results validate the efficacy of the ANN for the precise tracking control of IM. Furthermore the use of the ANN makes the drive system robust, accurate, and insensitive to parameter variations. Also the drive system is implemented in real-time using a digital signal processor (DSP) TMS320C31.",https://ieeexplore.ieee.org/document/938487/,IJCNN'01. International Joint Conference on Neural Networks. Proceedings (Cat. No.01CH37222),15-19 July 2001,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CCMB.2014.7020689,Neuromodulation based control of autonomous robots in ROS environment,IEEE,Conferences,"The paper presents a control approach based on vertebrate neuromodulation and its implementation on autonomous robots in the open-source, open-access environment of robot operating system (ROS) within a cloud computing framework. A spiking neural network (SNN) is used to model the neuromodulatory function for generating context based behavioral responses of the robots to sensory input signals. The neural network incorporates three types of neurons- cholinergic and noradrenergic (ACh/NE) neurons for attention focusing and action selection, dopaminergic (DA) neurons for rewards- and curiosity-seeking, and serotonergic (5-HT) neurons for risk aversion behaviors. The model depicts description of neuron activity that is biologically realistic but computationally efficient to allow for large-scale simulation of thousands of neurons. The model is implemented using graphics processing units (GPUs) for parallel computing in real-time using the ROS environment. The model is implemented to study the risk-taking, risk-aversive, and distracted behaviors of the neuromodulated robots in single- and multi-robot configurations. The entire process is implemented in a distributed computing framework using ROS where the robots communicate wirelessly with the computing nodes through the on-board laptops. Results are presented for both single- and multi-robot configurations demonstrating interesting behaviors.",https://ieeexplore.ieee.org/document/7020689/,"2014 IEEE Symposium on Computational Intelligence, Cognitive Algorithms, Mind, and Brain (CCMB)",9-12 Dec. 2014,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SSCI.2017.8280907,Obstacle avoidance of hexapod robots using fuzzy Q-learning,IEEE,Conferences,"Safe and autonomous obstacle avoidance plays an important role in the navigation control of hexapod robots. In this paper, we combine the method of reinforcement learning with fuzzy control to achieve the autonomous obstacle avoidance for a hexapod robot in complex environments. A fuzzy Q-learning algorithm is first presented and an obstacle avoidance approach is proposed using the Fuzzy Q-learning algorithm regarding the specific requirements of the hexapod robot. Then, the proposed approach is implemented for a real hexapod robot system that uses ultrasonic sensors to detect the obstacles in an unknown environment and learns an optimal policy to avoid the obstacles. Several groups of experiments are carried out to verify the performance of the proposed approach.",https://ieeexplore.ieee.org/document/8280907/,2017 IEEE Symposium Series on Computational Intelligence (SSCI),27 Nov.-1 Dec. 2017,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA40945.2020.9196769,Online LiDAR-SLAM for Legged Robots with Robust Registration and Deep-Learned Loop Closure,IEEE,Conferences,"In this paper, we present a 3D factor-graph LiDAR-SLAM system which incorporates a state-of-the-art deeply learned feature-based loop closure detector to enable a legged robot to localize and map in industrial environments. Point clouds are accumulated using an inertial-kinematic state estimator before being aligned using ICP registration. To close loops we use a loop proposal mechanism which matches individual segments between clouds. We trained a descriptor offline to match these segments. The efficiency of our method comes from carefully designing the network architecture to minimize the number of parameters such that this deep learning method can be deployed in real-time using only the CPU of a legged robot, a major contribution of this work. The set of odometry and loop closure factors are updated using pose graph optimization. Finally we present an efficient risk alignment prediction method which verifies the reliability of the registrations. Experimental results at an industrial facility demonstrated the robustness and flexibility of our system, including autonomous following paths derived from the SLAM map.",https://ieeexplore.ieee.org/document/9196769/,2020 IEEE International Conference on Robotics and Automation (ICRA),31 May-31 Aug. 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.23919/ACC45564.2020.9147898,Optimal Control of Wheeled Mobile Robots: From Simulation to Real World,IEEE,Conferences,"We study the problem of taking simulations to the real world (RW) for autonomous robotic systems with dynamic uncertainties and unknown disturbances while maintaining the optimal performance and stability of the designed controller designed in simulation. In general, an optimal and robust controller that is designed through simulation often does not perform similarly when deployed in the RW. We focus on using simulations to generate an optimal control policy utilizing the Memetic algorithm (MA) iteratively. The simulation-to-RW performance and stability are realized by using an adaptive fuzzy system to learn the uncertain part of the dynamic model, disturbance and noises. We demonstrate experimentally that this method permits the development of optimal control design in simulations and integrates adaptive learning rules to enable precise and repetitive trajectory tracking for the wheeled mobile robot (WMR) with disturbances and uncertainties.",https://ieeexplore.ieee.org/document/9147898/,2020 American Control Conference (ACC),1-3 July 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA46639.2022.9812167,Parametric Path Optimization for Wheeled Robots Navigation,IEEE,Conferences,"Collision risk and smoothness are the most important factors in global path planning. Currently, planning methods that reduce global path collision risk and improve its smoothness through numerical optimization have achieved good results. However, these methods cannot always optimize the path. The reason is all points on the path are considered as decision variables, which leads to the high dimensionality of the defined optimization problem. Therefore, we propose a novel global path optimization method. The method characterizes the path as a parametric curve and then optimizes the curve&#x0027;s parameters with a defined objective function, which successfully reduces the dimension of optimization problem. The proposed method is compared with baseline and state-of-the-art methods. Experimental results show the path optimized by our method is not only optimal in collision risk, but also in efficiency and smoothness. Furthermore, the proposed method is also implemented and tested in both simulation and real robots.",https://ieeexplore.ieee.org/document/9812167/,2022 International Conference on Robotics and Automation (ICRA),23-27 May 2022,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICECCT.2015.7226205,Performance analysis of path planning techniques for autonomous mobile robots,IEEE,Conferences,"This paper presents a comparative study on path planning techniques for autonomous mobile robots in a cluttered environment. It investigates four well known path planning algorithms and compares their performance with the proposed free configuration eigen-spaces (FCE) path planning method. In total, five path planning algorithms are considered towards the solution of the path planning problem under certain working parameters. These working parameters are the computation time needed to find a solution, the distance traveled and the amount of turning by the autonomous mobile robot. A comparison of results has been analyzed. This study will enable readers to identify, which of the proposed methods is most suitable for application under the working parameters the user wants to optimize. The findings have been summarized in the conclusion section. The techniques were implemented in the real-time robotic software Player/Stage. Further analysis were done using MATLAB mathematical computation software.",https://ieeexplore.ieee.org/document/7226205/,"2015 IEEE International Conference on Electrical, Computer and Communication Technologies (ICECCT)",5-7 March 2015,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA48506.2021.9561387,Pointing at Moving Robots: Detecting Events from Wrist IMU Data,IEEE,Conferences,"We propose a practical approach for detecting the event that a human wearing an IMU-equipped bracelet points at a moving robot; the approach uses a learned classifier to verify if the robot motion (as measured by its odometry) matches the wrist motion, and does not require that the relative pose of the operator and robot is known in advance. To train the model and validate the system, we collect datasets containing hundreds of real-world pointing events. Extensive experiments quantify the performance of the classifiers and relevant metrics of the resulting detectors; the approach is implemented in a real-world demonstrator that allows users to land quadrotors by pointing at them.",https://ieeexplore.ieee.org/document/9561387/,2021 IEEE International Conference on Robotics and Automation (ICRA),30 May-5 June 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ROBIO49542.2019.8961870,Probabilistic Inferences on Quadruped Robots: An Experimental Comparison,IEEE,Conferences,"Due to the reality gap, computer software cannot fully model the physical robot in its environment, with noise, ground friction, and energy consumption. Consequently, a limited number of researchers work on applying machine learning in real-world robots. In this paper, we use two intelligent black-box optimization algorithms, Bayesian Optimization (BO) and Covariance Matrix Adaptation Evolution Strategy (CMA-ES), to solve a quadruped robot gait's parametric search problem in 10 dimensions, and compare these two methods to find which one is more suitable for legged robots' controller parameters tuning. Our results show that both methods can find an optimal solution in 130 iterations. BO converges faster than CMA-ES within its constrained range, while CMA-ES finds the optimum in the continuous space. Compared with the specific controller parameters of two methods, we also find that for quadruped robot's oscillators, the angular amplitude is the most important parameter. Thus, it is very beneficial for the quick parametric search of legged robots&#x2019; controllers and avoids time-consuming manual tuning.",https://ieeexplore.ieee.org/document/8961870/,2019 IEEE International Conference on Robotics and Biomimetics (ROBIO),6-8 Dec. 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IJCNN.1999.832705,Programming robots with associative memories,IEEE,Conferences,"Today, there are several drawbacks that impede the necessary and much needed use of robot learning techniques in real applications. First, the time needed to achieve the synthesis of any behavior is prohibitive. Second, the robot behavior during the learning phase is by definition bad, it may even be dangerous. Third, except within the lazy learning approach, a new behavior implies a new learning phase. We propose in this paper to use self-organizing maps to encode the nonexplicit model of the robot-world interaction sampled by the lazy memory, and then generate a robot behavior by means of situations to be achieved, i.e., points on the self-organizing maps. Any behavior can instantaneously be synthesized by the definition of a goal situation. Its performance will be minimal (not evidently bad) and will improve by the mere repetition of the behavior.",https://ieeexplore.ieee.org/document/832705/,IJCNN'99. International Joint Conference on Neural Networks. Proceedings (Cat. No.99CH36339),10-16 July 1999,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICARCV.2014.7064347,RSAW: A situation awareness system for autonomous robots,IEEE,Conferences,"Services and technologies are in evolution in order to develop a new generation of robotic systems that might operate in dynamic real-world environments. In this paper, we focus on the ability of robot to understand and to surpass the blocked situations autonomously without operator intervention. Such situations may occur when the robot cannot succeed the current action and cannot move to the next one. We remark that in the literature, the operator has a crucial role consisting in providing all information about the environment and in making interpretations. In this paper, we propose an RSAW (Robot Situation AWareness) system, developed in order to help a robot to surpass a blocked situation and accomplish its goal whilst minimizing the operator intervention. RSAW is a new general system aiming to increase the autonomy of the robot; It is inspired by the notion of Situation Awareness (SA). In fact, RSAW defines a knowledge representation using ontologies and a process in order to surpass a blocked situation. RSAW is designed according to the Model Driven Engineering (MDE) methodology. This choice is done to preserve the generality of our system. This paper focalizes on the process of the RSAW system and the interaction between the process and the knowledge representation. The experimentations conducted in real environment with the Smart Autonomous Majordomo (SAM) robot, have shown the robustness and the efficiency of the proposed system.",https://ieeexplore.ieee.org/document/7064347/,2014 13th International Conference on Control Automation Robotics & Vision (ICARCV),10-12 Dec. 2014,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SmartWorld.2018.00106,Real-Time Data Processing Architecture for Multi-Robots Based on Differential Federated Learning,IEEE,Conferences,"The emergency of ubiquitous intelligence in various things has become the ultimate cornerstone in building a smart interconnection of the physical world and the human world, which also caters to the idea of Internet of Things (IoT). Nowadays, robots as a new type of ubiquitous IoT devices have gained much attention. With the increasing number of distributed multi-robots, such smart environment generates unprecedented amounts of data. Robotic applications are faced with challenges of such big data: the serious real-time assurance and data privacy. Therefore, in order to obtain the big data values via knowledge sharing under the premise of ensuring the real-time data processing and data privacy, we propose a real-time data processing architecture for multi-robots based on the differential federated learning, called RT-robots architecture. A global shared model with differential privacy protection is trained on the cloud iteratively and distributed to multiple edge robots in each round, and the robotic tasks are processed locally in real time. Our implementation and experiments demonstrate that our architecture can be applied on multiple robotic recognition tasks, balance the trade-off between the performance and privacy.",https://ieeexplore.ieee.org/document/8560084/,"2018 IEEE SmartWorld, Ubiquitous Intelligence & Computing, Advanced & Trusted Computing, Scalable Computing & Communications, Cloud & Big Data Computing, Internet of People and Smart City Innovation (SmartWorld/SCALCOM/UIC/ATC/CBDCom/IOP/SCI)",8-12 Oct. 2018,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICMLC.2005.1527001,Real-Time Path Planning for Mobile Robots,IEEE,Conferences,"A new on-line real-time approach with obstacle avoidance for mobile robots moving in an uncertain environment has been proposed and implemented. With the integration of global planning and local planning, this path planning approach is based on polar coordinates in which the desirable direction angle is taken into consideration as an optimization index. Detecting unknown obstacles with local feedback information by robot’s sensor system, this approach orients the desirable direction of mobile robot so as to generate local sub-goal in every planning window. As a result, the difference between real direction angle and desirable direction angle of robot motion steers the mobile robot to detour collisions and advance toward the target without stopping to re-plan a path when new sensor data become available. This approach is not only simple and flexible, but also overcomes flaws of global planning and local planning. The effectiveness, feasibility, real-time performance, optimization capability, high precision and perfect stability are demonstrated by means of simulation examples.",https://ieeexplore.ieee.org/document/1527001/,2005 International Conference on Machine Learning and Cybernetics,18-21 Aug. 2005,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CAC53003.2021.9727507,Real-time Object Detection Algorithm For Underwater Robots,IEEE,Conferences,"The marine aquaculture fishing industry has caused many problems such as low work efficiency and hidden safety hazards due to low automation. Research on marine fishing robots that can replace humans for autonomous operations has excellent significance and prospects. Aiming at the problems of uneven lighting, poor visibility, and many magazines in the underwater environment, the underwater fishing robot can quickly and autonomously detect marine products and other targets. This paper implements an underwater image enhancement algorithm that can be deployed on a small AI system and an object detection algorithm based on YOLOv5 (You Only Look Once Version 5). The algorithm includes using the limited contrast adaptive histogram equalization image enhancement technology to solve the image quality problems of blue-green and blurry underwater images and then using the YOLOv5 object detection network to detect and locate underwater creatures. Experimental results show that the algorithm can effectively solve poor underwater image quality and unclear targets and rapidly detect seafood targets. The detection accuracy of this algorithm can reach 85%. It has been applied to the underwater fishing robot independently developed by the team and deployed on the Jetson Xavier NX small AI system. The detection accuracy can reach 80%, and the detection speed can reach 30FPS.",https://ieeexplore.ieee.org/document/9727507/,2021 China Automation Congress (CAC),22-24 Oct. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICMLA.2017.0-161,Realistic Traffic Generation for Web Robots,IEEE,Conferences,"Critical to evaluating the capacity, scalability, and availability of web systems are realistic web traffic generators. Web traffic generation is a classic research problem, no generator accounts for the characteristics of web robots or crawlers that are now the dominant source of traffic to a web server. Administrators are thus unable to test, stress, and evaluate how their systems perform in the face of ever increasing levels of web robot traffic. To resolve this problem, this paper introduces a novel approach to generate synthetic web robot traffic with high fidelity. It generates traffic that accounts for both the temporal and behavioral qualities of robot traffic by statistical and Bayesian models that are fitted to the properties of robot traffic seen in web logs from North America and Europe. We evaluate our traffic generator by comparing the characteristics of generated traffic to those of the original data. We look at session arrival rates, inter-arrival times and session lengths, comparing and contrasting them between generated and real traffic. Finally, we show that our generated traffic affects cache performance similarly to actual traffic, using the common LRU and LFU eviction policies.",https://ieeexplore.ieee.org/document/8260631/,2017 16th IEEE International Conference on Machine Learning and Applications (ICMLA),18-21 Dec. 2017,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/WFCS53837.2022.9779202,Redundancy Concepts for Real-Time Cloud- and Edge-based Control of Autonomous Mobile Robots,IEEE,Conferences,"Deploying navigation algorithms on an edge or cloud server according to the Software-as-a-Service paradigm has many advantages for autonomous mobile robots in indus-trial environments, e.g. cooperative planning and less onboard energy consumption. However, outsourcing corresponding real-time critical control functions requires a high level of reliability, which cannot be guaranteed either by modern wireless networks nor by the outsourced computing infrastructure. This work introduces redundancy concepts, which enable real-time capability within these uncertain infrastructures by providing redundant computation nodes, as well as robot-controlled switching between them. Redundancies can vary regarding their physical location, robot behavior during the switchover process and degree of activeness while quality of service concerning the primary controller is sufficient. In the case that fallback redun-dancies are not continuously active, when a disturbance occurs an initial state estimation of the robot pose has to be provided and an activation time has to be anticipated. To gain some insights on expected behavior, redundant computation nodes are deployed locally on the robot and on an outsourced computation node and consequently evaluated empirically. Quantitative and qualitative results in simulation and a real environment show that redun-dancies help to significantly improve the robot-trajectory within an unreliable network. Moreover, resource-saving redundancies, which are not continuously active, can robustly take over control by using an estimated state.",https://ieeexplore.ieee.org/document/9779202/,2022 IEEE 18th International Conference on Factory Communication Systems (WFCS),27-29 April 2022,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICAMechS49982.2020.9310129,Reinforcement Learning based Method for Autonomous Navigation of Mobile Robots in Unknown Environments,IEEE,Conferences,"The Reinforcement Learning is a subset of machine learning that deals with learning decisions from rewards given by the environment. The model classic reinforcement learning (RL) algorithms are usually applied to small sets of states and an action. However, in real applications, the state spaces are of a large scale and this will bring the problems in the generalization and the curse of dimensionality. In this research, authors integrate neural networks into reinforcement learning methods to generalize the value of all the states. The simulation results on the Gazebo software framework show the feasibility of the model proposed method algorithm. The robot can safely navigate an unprotected work environment and becomes a truly intelligent system with the ability to learn and adapt itself to the model.",https://ieeexplore.ieee.org/document/9310129/,2020 International Conference on Advanced Mechatronic Systems (ICAMechS),10-13 Dec. 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCIA54998.2022.9737166,Reinforcement Learning based Sequential Controller for Mobile Robots with Obstacle Avoidance,IEEE,Conferences,"Obstacle avoidance and path planning play substantial roles in mobile robot applications. This paper has two main parts: first, an object detection algorithm based on YOLO-v4 with custom classes and depth camera in real-time has been implemented in Robot Operating System (ROS) to resolve robot obstacle avoidance issues. Then, controlling of the robot was discussed by dividing the path into three parts with their specific PID or PD controllers. The optimum parameters of each controller are then calculated by Reinforcement Learning (RL). For ensuring the precision of the obstacle avoidance method, the accuracy of distance is evaluated by the significant result of real thorough objects. Then for evaluating the controllers, the desired-and the traveled path were compared and the positional error was calculated.",https://ieeexplore.ieee.org/document/9737166/,"2022 8th International Conference on Control, Instrumentation and Automation (ICCIA)",2-3 March 2022,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ROMAN.2006.314459,Reinforcement Learning with Human Teachers: Understanding How People Want to Teach Robots,IEEE,Conferences,"While reinforcement learning (RL) is not traditionally designed for interactive supervisory input from a human teacher, several works in both robot and software agents have adapted it for human input by letting a human trainer control the reward signal. In this work, we experimentally examine the assumption underlying these works, namely that the human-given reward is compatible with the traditional RL reward signal. We describe an experimental platform with a simulated RL robot and present an analysis of real-time human teaching behavior found in a study in which untrained subjects taught the robot to perform a new task. We report three main observations on how people administer feedback when teaching a robot a task through reinforcement learning: (a) they use the reward channel not only for feedback, but also for future-directed guidance; (b) they have a positive bias to their feedback -possibly using the signal as a motivational channel; and (c) they change their behavior as they develop a mental model of the robotic learner. In conclusion, we discuss future extensions to RL to accommodate these lessons",https://ieeexplore.ieee.org/document/4107833/,ROMAN 2006 - The 15th IEEE International Symposium on Robot and Human Interactive Communication,6-8 Sept. 2006,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/FUZZY.1998.687475,Reinforcement function design and bias for efficient learning in mobile robots,IEEE,Conferences,"The main paradigm in sub-symbolic learning robot domain is the reinforcement learning method. Various techniques have been developed to deal with the memorization/generalization problem, demonstrating the superior ability of artificial neural network implementations. In this paper, we address the issue of designing the reinforcement so as to optimize the exploration part of the learning. We also present and summarize works relative to the use of bias intended to achieve the effective synthesis of the desired behavior. Demonstrative experiments involving a self-organizing map implementation of the Q-learning and real mobile robots (Nomad 200 and Khepera) in a task of obstacle avoidance behavior synthesis are described.",https://ieeexplore.ieee.org/document/687475/,1998 IEEE International Conference on Fuzzy Systems Proceedings. IEEE World Congress on Computational Intelligence (Cat. No.98CH36228),4-9 May 1998,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICORR.2017.8009451,Representing high-dimensional data to intelligent prostheses and other wearable assistive robots: A first comparison of tile coding and selective Kanerva coding,IEEE,Conferences,"Prosthetic devices have advanced in their capabilities and in the number and type of sensors included in their design. As the space of sensorimotor data available to a conventional or machine learning prosthetic control system increases in dimensionality and complexity, it becomes increasingly important that this data be represented in a useful and computationally efficient way. Well structured sensory data allows prosthetic control systems to make informed, appropriate control decisions. In this study, we explore the impact that increased sensorimotor information has on current machine learning prosthetic control approaches. Specifically, we examine the effect that high-dimensional sensory data has on the computation time and prediction performance of a true-online temporal-difference learning prediction method as embedded within a resource-limited upper-limb prosthesis control system. We present results comparing tile coding, the dominant linear representation for real-time prosthetic machine learning, with a newly proposed modification to Kanerva coding that we call selective Kanerva coding. In addition to showing promising results for selective Kanerva coding, our results confirm potential limitations to tile coding as the number of sensory input dimensions increases. To our knowledge, this study is the first to explicitly examine representations for realtime machine learning prosthetic devices in general terms. This work therefore provides an important step towards forming an efficient prosthesis-eye view of the world, wherein prompt and accurate representations of high-dimensional data may be provided to machine learning control systems within artificial limbs and other assistive rehabilitation technologies.",https://ieeexplore.ieee.org/document/8009451/,2017 International Conference on Rehabilitation Robotics (ICORR),17-20 July 2017,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICT4DA53266.2021.9672240,Rescuing the Fresh Water Lakes of Africa through the Use of Drones and Underwater Robots,IEEE,Conferences,"In this paper, we present a conceptual system architecture for real-time monitoring, predicting and controlling of invasive water hyacinth in freshwater bodies through the use of emerging technologies. The proposed system is planned to be deployed as one of the rescue efforts to preserve the fresh water lakes of Africa. The case study and the system presented in this paper are based on the Lake Tana, situated near the city of Bahir Dar, in Ethiopia. The rescuing efforts of Lake Tana so far focused on removal of the weed by hand and using harvesting machines. With the weed invasion doubling every two weeks, the current approaches will not be able to control the rapid invasion of the weed, which is causing considerable socioeconomic losses. The proposed system architecture employs networked underwater robots, aerial drones and other environmental sensors for better mapping of the weed coverage in real-time, predicting the floating paths of the weed, and learning the favourable environmental conditions of the lake for eradicating the invasive weed. The advantages of the proposed technical intervention lie not only in accurate monitoring and fast removal of the weed, but also in facilitating data collection for better understanding of the underlying environmental and chemical conditions that facilitate the rapid infestation and growth of the invasive weed.",https://ieeexplore.ieee.org/document/9672240/,2021 International Conference on Information and Communication Technology for Development for Africa (ICT4DA),22-24 Nov. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AIM.2019.8868670,Rhino: An Open-source Embedded Motherboard Design Enabling Complex Behavior of Intelligent Robots,IEEE,Conferences,"In recent years, Robot Operating System (ROS) has become a de facto standard for many robotic systems. However, there lacks a general-purpose control hardware to perfectly support ROS applications in an embedded fashion. In this paper, we take a hardware/software co-design methodology and a loosely coupled design methodology to develop a ROSoriented motherboard dedicatedly for facilitating high-end intelligent robotic applications. First, orienting around the ROS computational graph level, the hardware/software co-design is proposed to realize a mirrored modular design paradigm. Second, an open-source ROS motherboard, namely the ""Rhino"", is accordingly designed with the highlight of accelerating the embedded neuromorphic computation. Third, real-time performance and feasibility of Rhino are validated at different scales. Experimentation shows that the open-source prototype motherboard is eligible for ROS-based robot development and outperforms the conventional IPC and tailor-made control board. ROS-oriented hardware/software codesign paradigm complements the ROS ecosystem with an open-source AI-enabled motherboard for developing intelligent robots.",https://ieeexplore.ieee.org/document/8868670/,2019 IEEE/ASME International Conference on Advanced Intelligent Mechatronics (AIM),8-12 July 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA.2019.8793720,Robots Learn Social Skills: End-to-End Learning of Co-Speech Gesture Generation for Humanoid Robots,IEEE,Conferences,"Co-speech gestures enhance interaction experiences between humans as well as between humans and robots. Most existing robots use rule-based speech-gesture association, but this requires human labor and prior knowledge of experts to be implemented. We present a learning-based co-speech gesture generation that is learned from 52 h of TED talks. The proposed end-to-end neural network model consists of an encoder for speech text understanding and a decoder to generate a sequence of gestures. The model successfully produces various gestures including iconic, metaphoric, deictic, and beat gestures. In a subjective evaluation, participants reported that the gestures were human-like and matched the speech content. We also demonstrate a co-speech gesture with a NAO robot working in real time.",https://ieeexplore.ieee.org/document/8793720/,2019 International Conference on Robotics and Automation (ICRA),20-24 May 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA.2018.8462969,Robust Human Following by Deep Bayesian Trajectory Prediction for Home Service Robots,IEEE,Conferences,"The capability of following a person is crucial in service-oriented robots for human assistance and cooperation. Though a vast variety of following systems exist, they lack robustness against dynamic changes of the environment and relocating to continue following a lost target. Here we present a robust human following system that has the extendability to commercial service robot platforms having a RGB-D camera. The proposed framework integrates deep learning methods for perception and variational Bayesian techniques for trajectory prediction. Deep learning modules enable robots to accompany a person by detecting the target, learning the target and following while avoiding collision within the dynamic home environment. The variational Bayesian techniques robustly predict the trajectory of the target by empowering the following ability of the robot when target is lost. We experimentally demonstrate the capability of the deep Bayesian trajectory prediction method on real-time usage, following abilities, collision avoidance and trajectory prediction of the system. The proposed system was deployed at the RoboCup@Home 2017 Social Standard Platform League and successfully demonstrated its robust functions and smooth person following capability resulting in winning the 1st place.",https://ieeexplore.ieee.org/document/8462969/,2018 IEEE International Conference on Robotics and Automation (ICRA),21-25 May 2018,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS.2018.8594067,Robust Object Recognition Through Symbiotic Deep Learning In Mobile Robots,IEEE,Conferences,"Despite the recent success of state-of-the-art deep learning algorithms in object recognition, when these are deployed as-is on a mobile service robot, we observed that they failed to recognize many objects in real human environments. In this paper, we introduce a learning algorithm in which robots address this flaw by asking humans for help, also known as a symbiotic autonomy approach. In particular, we bootstrap YOLOv2, a state-of-the-art deep neural network and train a new neural network, that we call HHELP, using only data collected from human help. Using an RGB camera and an onboard tablet, the robot proactively seeks human input to assist it in labeling surrounding objects. Pepper, located at CMU, and Monarch Mbot, located at ISR-Lisbon, were the service robots that we used to validate the proposed approach. We conducted a study in a realistic domestic environment over the course of 20 days with 6 research participants. To improve object detection, we used the two neural networks, YOLOv2 + HHELP, in parallel. Following this methodology, the robot was able to detect twice the number of objects compared to the initial YOLOv2 neural network, and achieved a higher mAP (mean Average Precision) score. Using the learning algorithm the robot also collected data about where an object was located and to whom it belonged to by asking humans. This enabled us to explore a future use case where robots can search for a specific person's object. We view the contribution of this work to be relevant for service robots in general, in addition to Pepper, and Mbot.",https://ieeexplore.ieee.org/document/8594067/,2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),1-5 Oct. 2018,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ROBOT.2000.844830,Self-learning vision-guided robots for searching and grasping objects,IEEE,Conferences,"An approach to control vision-guided robots is introduced. It allows searching and grasping differently shaped objects that may be located anywhere in the robot's work space, even not visible in the initial fields of view of cameras. It eliminates the need for a calibration of the robot and of the vision system, it uses no world coordinates and no inverse perspective or kinematic transformations, and it comprises an automatic adaptation to changing parameters. The approach has been implemented on a calibration-free vision-guided manipulator with five degrees of freedom (DOF) and was evaluated in real-word experiments.",https://ieeexplore.ieee.org/document/844830/,Proceedings 2000 ICRA. Millennium Conference. IEEE International Conference on Robotics and Automation. Symposia Proceedings (Cat. No.00CH37065),24-28 April 2000,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ROBOT.2002.1014331,Self-organized flocking with agent failure: Off-line optimization and demonstration with real robots,IEEE,Conferences,"This paper presents an investigation of flocking by teams of autonomous mobile robots using principles of Swarm Intelligence. First, we present a simple flocking task, and we describe a leaderless distributed flocking algorithm (LD) that is more conducive to implementation on embodied agents than the established algorithms used in computer animation. Next, we use an embodied simulator and reinforcement learning techniques to optimize LD performance under different conditions, showing that this method can be used not only to improve performance but also to gain insight into which algorithm components contribute most to system behavior. Finally, we demonstrate that a group of real robots executing LD with emulated sensors can successfully flock (even in the presence of individual agent failure) and that systematic characterization (and therefore optimization) of real robot flocking performance is achievable.",https://ieeexplore.ieee.org/document/1014331/,Proceedings 2002 IEEE International Conference on Robotics and Automation (Cat. No.02CH37292),11-15 May 2002,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICUAS48674.2020.9214063,Semantic situation awareness of ellipse shapes via deep learning for multirotor aerial robots with a 2D LIDAR,IEEE,Conferences,"In this work, we present a semantic situation awareness system for multirotor aerial robots equipped with a 2D LIDAR sensor, focusing on the understanding of the environment, provided to have a drift-free precise localization of the robot (e.g. given by GNSS/INS or motion capture system). Our algorithm generates in real-time a semantic map of the objects of the environment as a list of ellipses represented by their radii, and their pose and velocity, both in world coordinates. Two different Convolutional Neural Network (CNN) architectures are proposed and trained using an artificially generated dataset and a custom loss function, to detect ellipses in a segmented (i.e. with one single object) LIDAR measurement. In cascade, a specifically designed indirect-EKF estimates the ellipses based semantic map in world coordinates, as well as their velocity. We have quantitative and qualitatively evaluated the performance of our proposed situation awareness system. Two sets of Software-In-The-Loop simulations using CoppeliaSim with one and multiple static and moving cylindrical objects are used to evaluate the accuracy and performance of our algorithm. In addition, we have demonstrated the robustness of our proposed algorithm when handling real environments thanks to real laboratory experiments with non-cylindrical static (i.e. a barrel) objects and moving persons.",https://ieeexplore.ieee.org/document/9214063/,2020 International Conference on Unmanned Aircraft Systems (ICUAS),1-4 Sept. 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/KIMAS.2003.1245110,Sharing learning policies between multiple mobile robots,IEEE,Conferences,"Learning of a complex task usually requires a long learning period. In order to reduce the time of learning, the task is divided into several subtasks. Multiple agents can be used to serve a complex task by learning these subtasks concurrently. With a good knowledge sharing mechanism, the learning policy can be shared or exchanged among these agents and can enhance their learning efficiency. The learning policy is a mapping from system states to actions. The mechanism of sharing or exchanging learning knowledge among multiagent system is proposed. An index of expertise, which indicates the skill level of each learning agent, is presented. This index is used to select the best preferable advice among multiple advices, which can increase the probability of finding solution in the search space. The experiment in which the learning knowledge is exchanged between a mobile robot and a computer simulated agent is implemented in order to verify the validity of the proposed algorithm. The experimental results show that the learning efficiency of the advisor agent is increased and the advisee robot can use the given advice for avoiding collision with obstacle successfully in the real world implementation.",https://ieeexplore.ieee.org/document/1245110/,IEMC '03 Proceedings. Managing Technologically Driven Organizations: The Human Side of Innovation and Change (IEEE Cat. No.03CH37502),30 Sept.-4 Oct. 2003,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS45743.2020.9341328,Software Development Framework for Cooperating Robots with High-level Mission Specification,IEEE,Conferences,"In recent years, there has been a growing interest in multiple robots performing a single task through different types of collaboration. There are two software challenges when deploying collaborative robots: how to specify a cooperative mission and how to program each robot to accomplish its mission. In this paper, we propose a novel software development framework to support distributed robot systems, swarm robots, and their hybrid. We extend the service-oriented and model-based (SeMo) framework [1] to improve the robustness, scalability, and flexibility of robot collaboration. To enable a casual user to specify various types of cooperative missions easily, the high-level mission scripting language is extended with new features such as team hierarchy, group service, one-to-many communication. The script program is refined to the robot codes through two intermediate steps, strategy description and task graph generation, in the proposed framework. The viability of the proposed framework is evidenced by two preliminary experiments using real robots and a robot simulator.",https://ieeexplore.ieee.org/document/9341328/,2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),24 Oct.-24 Jan. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS.1996.570655,Specification and validation of a control architecture for autonomous mobile robots,IEEE,Conferences,"We describe the specification of a software control architecture for autonomous mobile robots. The architecture, designed to provide the robot (in a task-dependent context) with the capacity to react to events but also to intelligently anticipate the future and plan its actions, is based on the decomposition of the robot system into a functional and a decisional level. The article is mainly focused on some aspects of the organisation and of the operation of the system such as execution control, inter-levels communication, reactivity. An important aspect that is developed is the possibility to prove some temporal and logical properties of parts of the system.",https://ieeexplore.ieee.org/document/570655/,Proceedings of IEEE/RSJ International Conference on Intelligent Robots and Systems. IROS '96,8-8 Nov. 1996,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CAC48633.2019.8996630,Target Recognition and 3D Pose Estimation Based on Prior Knowledge and Convolutional Neural Network for Robots,IEEE,Conferences,"In the competition of RoboMaster, the robot needs to trigger the target, the “Energy Mechanism” which consists of nine different dynamic flame numbers, in a nine square area by shooting projectiles. Therefore, 3D target detection should be implemented including target recognition and 3D pose estimation in real-time. As the targets are dynamic flame numbers and quite small in the whole image, it increases the difficulty to detect. The robot should achieve to shoot the target in multi-angle and multi-scale to adjust the competition. To address these issues, we propose a fast and accurate method to detect all nine numbers and estimate each 3D pose based on prior knowledge and convolutional neural network only by a monocular camera. The geometric constraints around the target are employed as prior knowledge when estimating the target pose. Then, we utilize the relative position information to detect the region of each dynamic number in the image, which is recognized by a convolutional neural network trained by flame numbers. Experiments in the actual environment show that our method can achieve the detection of each dynamic number in real-time and high accuracy. The runtime is 29ms on average (about 11ms in detection and 18ms in recognition) and the recognition accuracy is about 94.69%. And our method wins the first place in the technical challenge of 2018 RoboMaster competition.",https://ieeexplore.ieee.org/document/8996630/,2019 Chinese Automation Congress (CAC),22-24 Nov. 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS.2013.6696802,Teaching mobile robots to cooperatively navigate in populated environments,IEEE,Conferences,"Mobile service robots are envisioned to operate in environments that are populated by humans and therefore ought to navigate in a socially compliant way. Since the desired behavior of the robots highly depends on the application, we need flexible means for teaching a robot a certain navigation policy. We present an approach that allows a mobile robot to learn how to navigate in the presence of humans while it is being teleoperated in its designated environment. Our method applies feature-based maximum entropy learning to derive a navigation policy from the interactions with the humans. The resulting policy maintains a probability distribution over the trajectories of all the agents that allows the robot to cooperatively avoid collisions with humans. In particular, our method reasons about multiple homotopy classes of the agents' trajectories, i. e., on which sides the agents pass each other. We implemented our approach on a real mobile robot and demonstrate that it is able to successfully navigate in an office environment in the presence of humans relying only on on-board sensors.",https://ieeexplore.ieee.org/document/6696802/,2013 IEEE/RSJ International Conference on Intelligent Robots and Systems,3-7 Nov. 2013,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/PADSW.2000.884672,Teleoperation system for real world robots-adaptive robot navigation based on sensor fusion,IEEE,Conferences,"The authors propose a teleoperation system with an autonomous robot which is able to solve tasks even without a large load for the operator and the system. Most teleoperation systems require skilled operators and expensive interfaces to solve tasks because they assume that the operator controls a robot completely. For these problems, we propose a teleoperation system which consists of an operation system and an autonomous robot. The operation system has a man-machine interface and allows a user to specify the working space and the tasks to be done. The autonomous robot follows the instruction from the operation system to solve the specific tasks. The paper focuses on navigation problems of the autonomous robot as an essential part of the proposed system. Namely, the autonomous robot should keep on the instructed paths in the real world to achieve a goal of the tasks. Our approach is based on a sensor fusion method based on two learning schemes: self-organizing map (SOM) and reinforcement learning. These learning schemes allow the system to be able to solve the tasks in an unreliable environment such as outdoors. Computational simulations reveal the effectiveness and robustness of the proposed method in the navigation problem.",https://ieeexplore.ieee.org/document/884672/,Proceedings Seventh International Conference on Parallel and Distributed Systems: Workshops,4-7 July 2000,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IRI-05.2005.1506505,The behavior evolving model and application of virtual robots,IEEE,Conferences,"We suggest a model that evolves the behavioral knowledge of a virtual robot. The knowledge is represented in classification rules and a neural network, and is learned by a genetic algorithm. The model consists of a virtual robot with behavior knowledge, an environment that it moves in, and an evolution performer that includes a genetic algorithm. We have also applied our model to an environment where the robots gather food into a nest. When comparing our model with the conventional method on various test cases, our model showed superior overall learning.",https://ieeexplore.ieee.org/document/1506505/,"IRI -2005 IEEE International Conference on Information Reuse and Integration, Conf, 2005.",15-17 Aug. 2005,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISIE.1998.711559,The sensor-control Jacobian as a basis for controlling calibration-free robots,IEEE,Conferences,"A method for controlling the motions of robots is presented. It is based on the newly introduced sensor-control Jacobian matrix and avoids all quantitative modeling of the robot and the sensor system. The sensor-control Jacobian contains the coefficients that relate those changes in sensor data which are caused by a motion of the robot to the robot control words that caused the robot to move and, thus, the sensor data to change. A wide variety of tasks of robots can be reduced to minimizing the differences between actual sensor data and a set of hypothetical sensor data corresponding to some desired state. All these tasks can be solved by this method. The method is especially useful for calibration-free robots, since neither quantitative models of the mechanical, kinematic and control characteristics of the robot, nor knowledge of the sensor characteristics are required. The sensor-control Jacobian may be determined automatically in real time while the robot is operating. This yields a high degree of adaptability and flexibility against unforeseen changes in the robot's parameters. Because the concept has an open structure it allows further extensions and improvements, e.g., in terms of the utilization of sensor data redundancy and machine learning. For the purpose of evaluation, the concept has been implemented on a calibration-free camera-manipulator system. Real-world grasping experiments have demonstrated the effectiveness of the method.",https://ieeexplore.ieee.org/document/711559/,IEEE International Symposium on Industrial Electronics. Proceedings. ISIE'98 (Cat. No.98TH8357),7-10 July 1998,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA.2011.5980333,To look or not to look: A hierarchical representation for visual planning on mobile robots,IEEE,Conferences,"Mobile robots are increasingly being used in real-world applications due to the ready availability of high-fidelity sensors and the development of sophisticated information processing algorithms. However, one key challenge to the widespread deployment of mobile robots equipped with multiple sensors and processing algorithms is the ability to autonomously tailor sensing and information processing to the task at hand. This paper poses this challenge as the task of planning under uncertainty, and more specifically as an instance of probabilistic sequential decision-making. A novel hierarchy of partially observable Markov decision processes (POMDPs) is incorporated, which uses constrained-convolutional policies and automatic belief propagation to achieve efficient and reliable operation on mobile robots. All algorithms are implemented and evaluated on simulated and physical robot platforms for the task of searching for target objects in dynamic indoor environments.",https://ieeexplore.ieee.org/document/5980333/,2011 IEEE International Conference on Robotics and Automation,9-13 May 2011,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IECON.2016.7793038,Tool compensation in walk-through programming for admittance-controlled robots,IEEE,Conferences,"This paper describes a walk-through programming technique, based on admittance control and tool dynamics compensation, to ease and simplify the process of trajectory learning in common industrial setups. In the walk-through programming, the human operator grabs the tool attached at the robot end-effector and “walks” the robot through the desired positions. During the teaching phase, the robot records the positions and then it will be able to interpolate them to reproduce the trajectory back. In the proposed control architecture, the admittance control allows to provide a compliant behavior during the interaction between the human operator and the robot end-effector, while the algorithm of compensation of the tool dynamics allows to directly use the real tool in the teaching phase. In this way, the setup used for the teaching can directly be the one used for performing the reproduction task. Experiments have been performed to validate the proposed control architecture and a pick and place example has been implemented to show a possible application in the industrial field.",https://ieeexplore.ieee.org/document/7793038/,IECON 2016 - 42nd Annual Conference of the IEEE Industrial Electronics Society,23-26 Oct. 2016,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CoASE.2014.6899348,Toward safe close-proximity human-robot interaction with standard industrial robots,IEEE,Conferences,"Allowing humans and robots to interact in close proximity to each other has great potential for increasing the effectiveness of human-robot teams across a large variety of domains. However, as we move toward enabling humans and robots to interact at ever-decreasing distances of separation, effective safety technologies must also be developed. While new, inherently human-safe robot designs have been established, millions of industrial robots are already deployed worldwide, which makes it attractive to develop technologies that can turn these standard industrial robots into human-safe platforms. In this work, we present a real-time safety system capable of allowing safe human-robot interaction at very low distances of separation, without the need for robot hardware modification or replacement. By leveraging known robot joint angle values and accurate measurements of human positioning in the workspace, we can achieve precise robot speed adjustment by utilizing real-time measurements of separation distance. This, in turn, allows for collision prevention in a manner comfortable for the human user.We demonstrate our system achieves latencies below 9.64 ms with 95% probability, 11.10 ms with 99% probability, and 14.08 ms with 99.99% probability, resulting in robust real-time performance.",https://ieeexplore.ieee.org/document/6899348/,2014 IEEE International Conference on Automation Science and Engineering (CASE),18-22 Aug. 2014,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ROBOT.1990.126044,Towards a real-time architecture for obstacle avoidance and path planning in mobile robots,IEEE,Conferences,"The design and partial implementation of a real-time architecture for a mobile robot, aimed particularly towards a vehicle developed for factory automation, is described. The authors develop a layered design to equip the robot with a number of behavioral competences. They examine sensing and a potential field algorithm especially to achieve modification of behavior at a speed close to the robot's operational speed. It is shown how the layered architecture interfaces to the original onboard architecture, which provided sophisticated localization but no ability to deal with environmental exceptions.<>",https://ieeexplore.ieee.org/document/126044/,"Proceedings., IEEE International Conference on Robotics and Automation",13-18 May 1990,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CIRA.1999.810023,Towards focused plan monitoring: a technique and an application to mobile robots,IEEE,Conferences,"Until recently, techniques for AI plan generation relied on highly restrictive assumptions that were almost always violated in real-world environments; consequently, robot designers adopted reactive architectures and avoided AI planning techniques. Some recent research efforts have focused on obviating such assumptions by developing techniques that enable the generation and execution of plans in dynamic, uncertain environments. In this paper, we discuss one such technique, rationale-based monitoring, originally introduced by Veloso, Pollack, and Cox (1998), and describe our use of it in a simple mobile robot environment. We review the original approach, describe how it can be adapted for a causal-link planner, and provide experimental results demonstrating that it can lead to improved plans without consuming excessive overhead. We also describe our use of rationale-based monitoring in a mobile robot office-assistant project currently in progress.",https://ieeexplore.ieee.org/document/810023/,Proceedings 1999 IEEE International Symposium on Computational Intelligence in Robotics and Automation. CIRA'99 (Cat. No.99EX375),8-9 Nov. 1999,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/INDIN.2012.6301137,Towards hierarchical self-optimization in autonomous groups of mobile robots,IEEE,Conferences,"We present a real-world scenario for investigating and demonstrating hierarchical self-optimization in autonomous groups of mobile robots. The scenario is highly dynamic and easily expandable. It offers adequate starting points for the integration of hierarchical self-optimization. Reinforcement learning, e. g., can be introduced in order to improve the individual behavior of a single robot. Also swarm intelligence algorithms can improve the overall team behavior with respect to common goals. A reference behavior system incorporating a dynamic role assignment and hierarchical state machines was implemented and has been applied to the miniature robot BeBot. The system was evaluated by conducting several tests.",https://ieeexplore.ieee.org/document/6301137/,IEEE 10th International Conference on Industrial Informatics,25-27 July 2012,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IJCNN.2011.6033258,Towards the grounding of abstract words: A Neural Network model for cognitive robots,IEEE,Conferences,"In this paper, a model based on Artificial Neural Networks (ANNs) extends the symbol grounding mechanism to abstract words for cognitive robots. The aim of this work is to obtain a semantic representation of abstract concepts through the grounding in sensorimotor experiences for a humanoid robotic platform. Simulation experiments have been developed on a software environment for the iCub robot. Words that express general actions with a sensorimotor component are first taught to the simulated robot. During the training stage the robot first learns to perform a set of basic action primitives through the mechanism of direct grounding. Subsequently, the grounding of action primitives, acquired via direct sensorimotor experience, is transferred to higher-order words via linguistic descriptions. The idea is that by combining words grounded in sensorimotor experience the simulated robot can acquire more abstract concepts. The experiments aim to teach the robot the meaning of abstract words by making it experience sensorimotor actions. The iCub humanoid robot will be used for testing experiments on a real robotic architecture.",https://ieeexplore.ieee.org/document/6033258/,The 2011 International Joint Conference on Neural Networks,31 July-5 Aug. 2011,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/iFUZZY50310.2020.9297367,Using Interval Type-2 Recurrent Fuzzy Cerebellar Model Articulation Controller Based on Improved Differential Evolution for Cooperative Carrying Controller of Mobile Robots,IEEE,Conferences,"Mobile robot is widely utilized in various fields such as navigation control, obstacle avoidance and object carrying. For keeping away from obstacles to avoid collision and preventing object carrying from dropping down, we propose a state manager (SM) designed to assist the mobile robots so that they can switch operation between wall-following carrying (WFC) and toward goal carrying (TGC) by different external condition. In this controlling model, interval type-2 recurrent fuzzy cerebellar model articulation controller (IT2RFCMAC), embedded with a modified evolutionary optimization and dynamic grouping differential evolution (DGDE), is implemented for WFC and TGC. By adopting reinforcement learning strategy, mobile robots equip with adaptively wall-following control to make cooperative carrying control in real.",https://ieeexplore.ieee.org/document/9297367/,2020 International Conference on Fuzzy Theory and Its Applications (iFUZZY),4-7 Nov. 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS45743.2020.9341569,Velocity Regulation of 3D Bipedal Walking Robots with Uncertain Dynamics Through Adaptive Neural Network Controller,IEEE,Conferences,"This paper presents a neural-network based adaptive feedback control structure to regulate the velocity of 3D bipedal robots under dynamics uncertainties. Existing Hybrid Zero Dynamics (HZD)-based controllers regulate velocity through the implementation of heuristic regulators that do not consider model and environmental uncertainties, which may significantly affect the tracking performance of the controllers. In this paper, we address the uncertainties in the robot dynamics from the perspective of the reduced dimensional representation of virtual constraints and propose the integration of an adaptive neural network-based controller to regulate the robot velocity in the presence of model parameter uncertainties. The proposed approach yields improved tracking performance under dynamics uncertainties. The shallow adaptive neural network used in this paper does not require training a priori and has the potential to be implemented on the real-time robotic controller. A comparative simulation study of a 3D Cassie robot is presented to illustrate the performance of the proposed approach under various scenarios.",https://ieeexplore.ieee.org/document/9341569/,2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),24 Oct.-24 Jan. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SMICND.2005.1558827,Virtual environment for robots interfaces design and testing,IEEE,Conferences,"This paper refers to the implementation of a virtual environment for the robot interfaces testing. This software environment is very useful because, comparing to the experiments with real robots, it allow the testing and evaluation of different types of interfaces and different working environments with diverse configurations. A very important facility of this interactive software environment is the fact that the designers of the robots sensors and interfaces are able to work in parallel to design test, optimize and realize different control devices for the robot.",https://ieeexplore.ieee.org/document/1558827/,"CAS 2005 Proceedings. 2005 International Semiconductor Conference, 2005.",3-5 Oct. 2005,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICGEC.2012.151,Vision-Based Coordinate Transformation with Back Propagation Neural Networks on Mobile Robots,IEEE,Conferences,"Target tracking is important for vision-based robots to implement tasks of grasping, assembling and avoiding obstacles. the purpose of a target tracking system is to identify a target and then to estimate the position of the target. the targets' positions are usually described by various coordinate systems for different purposes. This study focuses on the problem of coordinate transformation on mobile robots and employs the techniques of Back-Propagation Neural Networks to discover the prediction models. with such prediction models, coordinate transformation can be done with less processing time. the techniques have been implemented and integrated with a four-wheeled vision-based security robot and has been verified in real environments. the experimental results show that the proposed method is able to produce simple and precise transformation models and improves the robot's performances.",https://ieeexplore.ieee.org/document/6456866/,2012 Sixth International Conference on Genetic and Evolutionary Computing,25-28 Aug. 2012,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CDS49703.2020.00012,Welding Seam Recognition Robots Based on Edge Computing,IEEE,Conferences,"In order to meet the requirements of the accuracy and real-time performance during the working process of underwater welding robots, a scheme of welding seam recognition robots system based on the edge computing is proposed in this paper. A number of pre-processing methods for capturing welding seam image were designed, including Thresholding, Filtering and Edge Detect. A Convolutional Neural Network(CNN) model for welding seam recognition was also created. In the experiments, the image pre-processing and CNN algorithms were integrated in and deployed to the robots, and the learning and training algorithms of the CNN were deployed to the cloud servers. The image pre-processing methods filtered the interference in underwater operations and achieved the image compression and feature extraction. The cloud servers fulfilled the training and parameter optimization of the CNN, which improved the accuracy of welding seam image recognition.",https://ieeexplore.ieee.org/document/9275963/,2020 International Conference on Computing and Data Science (CDS),1-2 Aug. 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS51168.2021.9636460,micROS.BT: An Event-Driven Behavior Tree Framework for Swarm Robots,IEEE,Conferences,"In this paper, we propose micROS.BT, an event-driven behavior tree (BT) framework aiming at supporting swarm-robot coordination. Compared with other BT frame-works, micROS.BT implements the event-driven way under the multi-thread mode, which can effectively save computing resources. Moreover, in order to ensure swarm-robot coordination, we optimize the implementation of the traditional blackboard and propose the multi-mode blackboard, which supports inner-tree, inter-tree, and inter-robot data sharing. Furthermore, considering the limited modularity of a single tree, micROS.BT realizes a mechanism called hierarchical tree management which involves inter-tree notifying and waiting functionalities, while ensuring that each tree is independent and self-scheduled. The effectiveness of micROS.BT is verified by simulation and real-robot experiments for different system settings, showing that a substantial improvement is achieved in comparison with the traditional BT implementations.",https://ieeexplore.ieee.org/document/9636460/,2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),27 Sept.-1 Oct. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ACCESS.2021.3061477,A Cloud-Based Platform for Big Data-Driven CPS Modeling of Robots,IEEE,Journals,"This paper proposes an improved cyber-physical systems (CPS) architecture for a smart robotic factory based on an industrial cloud platform driven by big data based on the traditional CPS architecture. This paper uses the architecture analysis and design language to model and design a total of three scales for the underlying cell-level robot, the system-level robot shop, and the overall robotic smart factory CPS, respectively, to complete the conceptual scheme for building a robotic smart factory from a local to an overall CPS system. Using the advantages of cloud computing and combining robotic CPS with cloud computing, an architecture for an industrial management system for CPS cloud computing is proposed. Base based distributed storage architecture with Storm based distributed real-time processing architecture. In terms of modeling, the advantages and disadvantages of using AADL, structural analysis, and design language, and modelers, a physical device modeling language, are combined to analyze the advantages and disadvantages of architecture analysis & design language (AADL) for modeling CPS and propose a CPS analysis and design based on AADL and applicable to it. The paper also investigates the use of LeNet models for state identification in the HSV color space. The algorithm was verified on a self-built power equipment indicator dataset with a 100% detection rate and 99.8% state recognition accuracy after four consecutive frames of fusion detection. Simulink simulation of the trolley was carried out in terms of a cell-level robotic trolley CPS system to demonstrate the effectiveness of the design of a robotic CPS system driven by soaring data based on the industrial cloud platform proposed in this paper.",https://ieeexplore.ieee.org/document/9360827/,IEEE Access,2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ACCESS.2020.3003991,A Software Architecture for Service Robots Manipulating Objects in Human Environments,IEEE,Journals,"This paper presents a software architecture for robots providing manipulation services autonomously in human environments. In an unstructured human environment, a service robot often needs to perform tasks even without human intervention and prior knowledge about tasks and environments. For autonomous execution of tasks, varied processes are necessary such as perceiving environments, representing knowledge, reasoning with the knowledge, and planning for task and motion. While developing each of the processes is important, integrating them into a working system for deployment is also important as a robotic system can bring tangible outcomes when it works in real world. However, such an architecture has been rarely realized in the literature owing to the difficulties of a full integration, deployment, understanding high-level goals without human interventions. In this work, we suggest a software architecture that integrates the components necessary to perform tasks by a real robot without human intervention. We show our architecture composed of deep learning based perception, symbolic reasoning, AI task planning, and geometric motion planning. We implement a deep neural network that produces information about the environment, which are then stored in a knowledge base. We implement a reasoner that processes the knowledge to use the result for task planning. We show our implementation of the symbolic task planner that generates a sequence of motion predicates. We implement an interface that computes geometric information necessary for motion planning to execute the symbolic task plans. We describe the deployment of the architecture through the result of lab tests and a public demonstration. The architecture is developed based on Robot Operating System (ROS) so compatible with any robot that is capable of object manipulation and mobile navigation running in ROS. We deploy the architecture to two different robot platforms to show the compatibility.",https://ieeexplore.ieee.org/document/9122008/,IEEE Access,2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/JIOT.2020.3004339,AirScope: Mobile Robots-Assisted Cooperative Indoor Air Quality Sensing by Distributed Deep Reinforcement Learning,IEEE,Journals,"Indoor air pollution has become a growing health risk, but it is challenging to provide low-cost air quality monitoring for the indoor environment. In this article, we present “AirScope,” a mobile sensing system that employs cooperative robots to monitor the indoor air quality. Since the wireless coverage can be incomplete in some indoor areas, AirScope allows the robots to defer uploading the data to the central server by utilizing their own data buffers. In order to guarantee the timeliness of the data in the server, AirScope aims to minimize the average data latency by properly planning the routes of the robots. Such a route planning strategy has to be implemented in a distributed way since the robots that are out of wireless coverage can only make plans on their own. In addition, the cooperation of the robots is also necessary because the aggregation of the robots in a small area increases the average data latency of the other unattended areas. To solve this distributed and cooperative routing planning problem, we propose a solution based on distributed deep Q-learning (DDQL). We evaluate the system performance by simulations and real-world experiments. The results show that AirScope is effective to reduce data latency, where the proposed DDQL is 8% better than the greedy algorithm and 24% better than the random strategy.",https://ieeexplore.ieee.org/document/9123492/,IEEE Internet of Things Journal,Sept. 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/LRA.2022.3185387,Auto-Tuning of Controller and Online Trajectory Planner for Legged Robots,IEEE,Journals,"This letter presents an approach for auto-tuning feedback controllers and online trajectory planners to achieve robust locomotion of a legged robot. The auto-tuning approach uses an Unscented Kalman Filter (UKF) formulation, which adapts/calibrates control parameters online using a recursive implementation. In particular, this letter shows how to use the auto-tuning approach to calibrate cost function weights of a Model Predictive Control (MPC) stance controller and feedback gains of a swing controller for a quadruped robot. Furthermore, this letter extends the auto-tuning approach to calibrating parameters of an online trajectory planner, where the height of a swing leg and the robot&#x2019;s walking speed are optimized, while minimizing its energy consumption and foot slippage. This allows us to generate stable reference trajectories online and in real time. Results using a high-fidelity Unitree A1 robot simulator in Gazebo provided by the robot manufacturer show the advantages of using auto-tuning for calibrating feedback controllers and for computing reference trajectories online for reduced development time and improved tracking performance.",https://ieeexplore.ieee.org/document/9804740/,IEEE Robotics and Automation Letters,July 2022,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/TFUZZ.2004.832532,Automatic design of fuzzy controllers for car-like autonomous robots,IEEE,Journals,"This paper describes the design and implementation of a fuzzy control system for a car-like autonomous vehicle. The problem addressed is the diagonal parking in a constrained space, a typical problem in motion control of nonholonomic robots. The architecture proposed for the fuzzy controller is a hierarchical scheme which combines seven modules working in series and in parallel. The rules of each module employ the adequate fuzzy operators for its task (making a decision or generating a smoothly varying control output), and they have been obtained from heuristic knowledge and numerical data (with geometric information) depending on the module requirements (some of them are constrained to provide paths of near-minimal lengths). The computer-aided design tools of the environment Xfuzzy 3.0 (developed by some of the authors) have been employed to automate the different design stages: 1) translation of heuristic knowledge into fuzzy rules; 2) extraction of fuzzy rules from numerical data and their tuning to give paths of near-minimal lengths; 3) offline verification of the control system behavior; and 4) its synthesis to be implemented in a true robot and be verified on line. Real experiments with the autonomous vehicle ROMEO 4R (designed and built at the Escuela Superior de Ingenieros, University of Seville, Seville, Spain) demonstrate the efficiency of the described controller and of the methodology followed in its design.",https://ieeexplore.ieee.org/document/1321074/,IEEE Transactions on Fuzzy Systems,Aug. 2004,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/LRA.2022.3185762,Autonomous State-Based Flipper Control for Articulated Tracked Robots in Urban Environments,IEEE,Journals,"We demonstrate a hybrid approach to autonomous flipper control, focusing on a fusion of hard-coded and learned knowledge. The result is a sample-efficient and modifiable control structure that can be used in conjunction with a mapping/navigation stack. The backbone of the control policy is formulated as a state machine whose states define various flipper action templates and local control behaviors. It is also used as an interface that facilitates the gathering of demonstrations to train the transitions of the state machine. We propose a soft-differentiable state machine neural network that mitigates the shortcomings of its naively implemented counterpart and improves over a multi-layer perceptron baseline in the task of state-transition classification. We show that by training on several minutes of user-gathered demonstrations in simulation, our approach is capable of a zero-shot domain transfer to a wide range of obstacles on a similar real robotic platform. Our results show a considerable increase in performance over a previous competing approach in several essential criteria. A subset of this work was successfully used in the Defense Advanced Research Projects Agency (DARPA) Subterranean Challenge to alleviate the operator of manual flipper control. We autonomously traversed stairs and other obstacles, improving map coverage.",https://ieeexplore.ieee.org/document/9804853/,IEEE Robotics and Automation Letters,July 2022,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/TSMCB.2004.843270,Autonomous stair-climbing with miniature jumping robots,IEEE,Journals,"The problem of vision-guided control of miniature mobile robots is investigated. Untethered mobile robots with small physical dimensions of around 10 cm or less do not permit powerful onboard computers because of size and power constraints. These challenges have, in the past, reduced the functionality of such devices to that of a complex remote control vehicle with fancy sensors. With the help of a computationally more powerful entity such as a larger companion robot, the control loop can be closed. Using the miniature robot's video transmission or that of an observer to localize it in the world, control commands can be computed and relayed to the inept robot. The result is a system that exhibits autonomous capabilities. The framework presented here solves the problem of climbing stairs with the miniature Scout robot. The robot's unique locomotion mode, the jump, is employed to hop one step at a time. Methods for externally tracking the Scout are developed. A large number of real-world experiments are conducted and the results discussed.",https://ieeexplore.ieee.org/document/1408060/,"IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics)",April 2005,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/LRA.2022.3177289,Bio-Inspired Rhythmic Locomotion for Quadruped Robots,IEEE,Journals,"The mechanisms of locomotion in mammals have been extensively studied and inspire the related researches on designing the control architectures for the legged robots. Reinforcement learning (RL) is a promising approach allowing robots to automatically learn locomotion policies. However, careful reward-function adjustments are often required via trial-and-error until achieving a desired behavior, as RL policy behaviors are sensitive to the rewards. In this paper, we draw inspiration from the rhythmic locomotion behaviors of animals and propose a new control architecture by incorporating a rhythm generator to naturally stimulate periodic motor patterns, which actively participates in the timing of phase transitions in the robot step cycle. To speed up training, we use the joint position increments rather than the conventional joint positions as the outputs of the RL policy. During deployment, the rhythm generator can be reused for the state estimation of quadruped robots. We validate our method by realizing the full spectrum of quadruped locomotion in both simulated and real-world scenarios.",https://ieeexplore.ieee.org/document/9780216/,IEEE Robotics and Automation Letters,July 2022,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/TRO.2021.3084374,Cat-Like Jumping and Landing of Legged Robots in Low Gravity Using Deep Reinforcement Learning,IEEE,Journals,"In this article, we show that learned policies can be applied to solve legged locomotion control tasks with extensive flight phases, such as those encountered in space exploration. Using an off-the-shelf deep reinforcement learning algorithm, we train a neural network to control a jumping quadruped robot while solely using its limbs for attitude control. We present tasks of increasing complexity leading to a combination of 3-D (re)orientation and landing locomotion behaviors of a quadruped robot traversing simulated low-gravity celestial bodies. We show that our approach easily generalizes across these tasks and successfully trains policies for each case. Using sim-to-real transfer, we deploy trained policies in the real world on the SpaceBok robot placed on an experimental testbed designed for 2-D microgravity experiments. The experimental results demonstrate that repetitive controlled jumping and landing with natural agility is possible.",https://ieeexplore.ieee.org/document/9453856/,IEEE Transactions on Robotics,Feb. 2022,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/LRA.2022.3193239,Contrastive 3D Shape Completion and Reconstruction for Agricultural Robots Using RGB-D Frames,IEEE,Journals,"Monitoring plants and fruits is important in modern agriculture, with applications ranging from high-throughput phenotyping to autonomous harvesting. Obtaining highly accurate 3D measurements under real agricultural conditions is a challenging task. In this letter, we address the problem of estimating the 3D shape of fruits when only a partial view is available. We propose a pipeline that exploits high-resolution 3D data in the learning phase but only requires a single RGB-D frame to predict the 3D shape of a <i>complete</i> fruit during operation. To achieve this, we first learn a latent space of potential fruit appearances that we can decode into an SDF volume. With the pretrained, frozen decoder, we subsequently learn an encoder that can produce meaningful latent vectors from a single RGB-D frame. The experiments presented in this letter suggest that our approach can predict the 3D shape of whole fruits online, needing only 4 ms for inference. We evaluate our approach in controlled environments and illustrate its deployment in greenhouses without modifications.",https://ieeexplore.ieee.org/document/9837391/,IEEE Robotics and Automation Letters,Oct. 2022,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ACCESS.2020.3016893,Coping With Multiple Visual Motion Cues Under Extremely Constrained Computation Power of Micro Autonomous Robots,IEEE,Journals,"The perception of different visual motion cues is crucial for autonomous mobile robots to react to or interact with the dynamic visual world. It is still a great challenge for a micro mobile robot to cope with dynamic environments due to the restricted computational resources and the limited functionalities of its visual systems. In this study, we propose a compound visual neural system to automatically extract and fuse different visual motion cues in real-time using the extremely constrained computation power of micro mobile robots. The proposed visual system contains multiple bio-inspired visual motion perceptive neurons each with a unique role, for example to extract collision visual cues, darker collision cue and directional motion cues. In the embedded system, these multiple visual neurons share a similar presynaptic network to minimise the consumption of computation resources. In the postsynaptic part of the system, visual cues pass results to corresponding action neurons using lateral inhibition mechanism. The translational motion cues, which are identified by comparing pairs of directional cues, are given the highest priority, followed by the darker colliding cues and approaching cues. Systematic experiments with both virtual visual stimuli and real-world scenarios have been carried out to validate the system's functionality and reliability. The proposed methods have demonstrated that (1) with extremely limited computation power, it is still possible for a micro mobile robot to extract multiple visual motion cues robustly in a complex dynamic environment; (2) the cues extracted can be fused with a lateral inhibited postsynaptic network, thus enabling the micro robots to respond effectively with different actions, accordingly to different states, in real-time. The proposed embedded visual system has been modularised and can be easily implemented in other autonomous mobile platforms for real-time applications. The system could also be used by neurophysiologists to test new hypotheses pertaining to biological visual neural systems.",https://ieeexplore.ieee.org/document/9167216/,IEEE Access,2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/56.802,Dynamic multi-sensor data fusion system for intelligent robots,IEEE,Journals,"The objective of the authors is to develop an intelligent robot workstation capable of integrating data from multiple sensors. The investigation is based on a Unimation PUMA 560 robot and various external sensors. These include overhead vision, eye-in-hand vision, proximity, tactile array, position, force/torque, cross-fire, overload, and slip-sensing devices. The efficient fusion of data from different sources will enable the machine to respond promptly in dealing with the 'real world'. Towards this goal, the general paradigm of a sensor data fusion system has been developed, and some simulation results, as well as results from the actual implementation of certain concepts of sensor data fusion, have been demonstrated.<>",https://ieeexplore.ieee.org/document/802/,IEEE Journal on Robotics and Automation,Aug. 1988,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/TNSRE.2014.2302212,Embedded Human Control of Robots Using Myoelectric Interfaces,IEEE,Journals,"Myoelectric controlled interfaces have become a research interest for use in advanced prostheses, exoskeletons, and robot teleoperation. Current research focuses on improving a user's initial performance, either by training a decoding function for a specific user or implementing “intuitive” mapping functions as decoders. However, both approaches are limiting, with the former being subject specific, and the latter task specific. This paper proposes a paradigm shift on myoelectric interfaces by embedding the human as controller of the system to be operated. Using abstract mapping functions between myoelectric activity and control actions for a task, this study shows that human subjects are able to control an artificial system with increasing efficiency by just learning how to control it. The method efficacy is tested by using two different control tasks and four different abstract mappings relating upper limb muscle activity to control actions for those tasks. The results show that all subjects were able to learn the mappings and improve their performance over time. More interestingly, a chronological evaluation across trials reveals that the learning curves transfer across subsequent trials having the same mapping, independent of the tasks to be executed. This implies that new muscle synergies are developed and refined relative to the mapping used by the control task, suggesting that maximal performance may be achieved by learning a constant, arbitrary mapping function rather than dynamic subject- or task-specific functions. Moreover, the results indicate that the method may extend to the neural control of any device or robot, without limitations for anthropomorphism or human-related counterparts.",https://ieeexplore.ieee.org/document/6720133/,IEEE Transactions on Neural Systems and Rehabilitation Engineering,July 2014,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/TII.2019.2936167,End-to-End Navigation Strategy With Deep Reinforcement Learning for Mobile Robots,IEEE,Journals,"In this article, we develop a navigation strategy based on deep reinforcement learning (DRL) for mobile robots. Because of the large difference between simulation and reality, most of the trained DRL models cannot be directly migrated into real robots. Moreover, how to explore in a sparsely rewarded environment is also a long-standing problem of DRL. This article proposes an end-to-end navigation planner that translates sparse laser ranging results into movement actions. Using this highly abstract data as input, agents trained by simulation can be extended to the real scene for practical application. For map-less navigation across obstacles and traps, it is difficult to reach the target via random exploration. Curiosity is used to encourage agents to explore the state of an environment that has not been visited and as an additional reward for exploring behavior. The agent relies on the self-supervised model to predict the next state, based on the current state and the executed action. The prediction error is used as a measure of curiosity. The experimental results demonstrate that without any manual design features and previous demonstrations, the proposed method accomplishes map-less navigation in complex environments. Through a reward signal that is enhanced by intrinsic motivation, the agent explores more efficiently, and the learned strategy is more reliable.",https://ieeexplore.ieee.org/document/8807287/,IEEE Transactions on Industrial Informatics,April 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/TFUZZ.2020.3033141,Fuzzy Double Deep Q-Network-Based Gait Pattern Controller for Humanoid Robots,IEEE,Journals,"In this article, the adaptive-network-based fuzzy inference system (ANFIS) is combined with the double deep <i>Q</i>-network (DDQN) to realize a fuzzy DDQN (FDDQN) such that a humanoid robot can generate a linear inverted pendulum model-based gait pattern in real time. The FDDQN not only allows the humanoid robot to correct the gait pattern instantly but also improves its stability. The proposed scheme is designed and implemented in a toddler-sized humanoid robot called Louis. First, four pressure sensors are installed on the bottom of the sole and one inertial measurement unit is set up on the trunk of the robot. A wireless communication chip is employed to transfer the data to a computer to determine the required parameters for the robot. Next, a control system based on the Linux operating system is developed. The values of the center of pressure and acceleration obtained with the ANFIS are adopted to train the DDQN. The proposed neural network comprises four layers, and the model is cautiously selected to avoid overfitting. The proposed scheme is verified using a robot simulator and then real-time-tested on Louis. The experimental results indicate that the FDDQN can provide the robot timely feedback during walking as well as helps it in adjusting the gait pattern independently. The balancing of the robot through effective dynamic feedback is similar to the balancing ability of an infant learning to walk.",https://ieeexplore.ieee.org/document/9237162/,IEEE Transactions on Fuzzy Systems,Jan. 2022,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/TNNLS.2020.3016523,Generic Neural Locomotion Control Framework for Legged Robots,IEEE,Journals,"In this article, we present a generic locomotion control framework for legged robots and a strategy for control policy optimization. The framework is based on neural control and black-box optimization. The neural control combines a central pattern generator (CPG) and a radial basis function (RBF) network to create a CPG-RBF network. The control network acts as a neural basis to produce arbitrary rhythmic trajectories for the joints of robots. The main features of the CPG-RBF network are: 1) it is generic since it can be applied to legged robots with different morphologies; 2) it has few control parameters, resulting in fast learning; 3) it is scalable, both in terms of policy/trajectory complexity and the number of legs that can be controlled using similar trajectories; 4) it does not rely heavily on sensory feedback to generate locomotion and is thus less prone to sensory faults; and 5) once trained, it is simple, minimal, and intuitive to use and analyze. These features will lead to an easy-to-use framework with fast convergence and the ability to encode complex locomotion control policies. In this work, we show that the framework can successfully be applied to three different simulated legged robots with varying morphologies and, even broken joints, to learn locomotion control policies. We also show that after learning, the control policies can also be successfully transferred to a real-world robot without any modifications. We, furthermore, show the scalability of the framework by implementing it as a central controller for all legs of a robot and as a decentralized controller for individual legs and leg pairs. By investigating the correlation between robot morphology and encoding type, we are able to present a strategy for control policy optimization. Finally, we show how sensory feedback can be integrated into the CPG-RBF network to enable online adaptation.",https://ieeexplore.ieee.org/document/9174772/,IEEE Transactions on Neural Networks and Learning Systems,Sept. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/TCDS.2019.2954289,Human-in-the-Loop Control Strategy of Unilateral Exoskeleton Robots for Gait Rehabilitation,IEEE,Journals,"In this article, a human-in-the-loop control methodology is proposed for the gait rehabilitation of patients with hemiplegia. It utilizes a unilateral exoskeleton system consisting of a unilateral lower limb exoskeleton and a real-time robot follower, such that the affected legs can be coordinated with the healthy legs with the assistance of the exoskeleton robot. In order to achieve immersive training during the physical therapy, the human-in-the-loop controller is developed. Furthermore, a region-based barrier Lyapunov function (BLF) is designed to separate the task workspace of the exoskeleton into a human region and a robot region, enabling the human leg to follow the desired motion trajectory in a compliant region, and the motion control of the exoskeleton is determined by humans; while in the robot region, the exoskeleton dominates the movement of human subjects. In order to make the motion control transit smoothly between the robot region and the human region, an adaptive controller is exploited to counteract the system’s nonlinear uncertainties. Both the theoretical analysis and experimental results support the effectiveness and practicability on hemiplegic patients of our control strategy.",https://ieeexplore.ieee.org/document/8906035/,IEEE Transactions on Cognitive and Developmental Systems,March 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ACCESS.2019.2949835,Hybrid Path Planning Algorithm Based on Membrane Pseudo-Bacterial Potential Field for Autonomous Mobile Robots,IEEE,Journals,"A hybrid path planning algorithm based on membrane pseudo-bacterial potential field (MemPBPF) is proposed. Membrane-inspired algorithms can reach an evolutionary behavior based on biochemical processes to find the best parameters for generating a feasible and safe path. The proposed MemPBPF algorithm uses a combination of the structure and rules of membrane computing. In that sense, the proposed MemPBPF algorithm contains dynamic membranes that include a pseudo-bacterial genetic algorithm for evolving the required parameters in the artificial potential field method. This hybridization between membrane computing, the pseudo-bacterial genetic algorithm, and the artificial potential field method provides an outperforming path planning algorithm for autonomous mobile robots. Computer simulation results demonstrate the effectiveness of the proposed MemPBPF algorithm in terms of path length considering collision avoidance and smoothness. Comparisons with two different versions employing a different number of elementary membranes and with other artificial potential field based algorithms are presented. The proposed MemPBPF algorithm yields improved performance in terms of time execution by using a parallel implementation on a multi-core computer. Therefore, the MemPBPF algorithm achieves high performance yielding competitive results for autonomous mobile robot navigation in complex and real scenarios.",https://ieeexplore.ieee.org/document/8884165/,IEEE Access,2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/LRA.2020.3007455,Image Transformation and CNNs: A Strategy for Encoding Human Locomotor Intent for Autonomous Wearable Robots,IEEE,Journals,"Wearable robots have the potential to improve the lives of countless individuals; however, challenges associated with controlling these systems must be addressed before they can reach their full potential. Modern control strategies for wearable robots are predicated on activity-specific implementations, and testing is usually limited to a single, fixed activity within the laboratory (e.g., level ground walking). To accommodate various activities in real-world scenarios, control strategies must include the ability to safely and seamlessly transition between activity-specific controllers. One potential solution to this challenge is to the infer wearer's intent using pattern recognition of locomotion sensor data. To this end, we developed an intent recognition framework implementing convolutional neural networks with image encoding (i.e. spectrogram) that enables prediction of the upcoming locomotor activity of the wearer's next step. In this letter, we describe our intent recognition system, comprised of a mel-spectrogram and subsequent neural network architecture. In addition, we analyzed the effect of sensor locations and modalities on the recognition system, and compared our proposed system to state-of-the-art locomotor intent recognition strategies. We were able to attain high classification performance (error rate: 1.1%), which was comparable or better than previous systems.",https://ieeexplore.ieee.org/document/9134897/,IEEE Robotics and Automation Letters,Oct. 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/LRA.2020.3010739,Learning Force Control for Contact-Rich Manipulation Tasks With Rigid Position-Controlled Robots,IEEE,Journals,"Reinforcement Learning (RL) methods have been proven successful in solving manipulation tasks autonomously. However, RL is still not widely adopted on real robotic systems because working with real hardware entails additional challenges, especially when using rigid position-controlled manipulators. These challenges include the need for a robust controller to avoid undesired behavior, that risk damaging the robot and its environment, and constant supervision from a human operator. The main contributions of this work are, first, we proposed a learning-based force control framework combining RL techniques with traditional force control. Within said control scheme, we implemented two different conventional approaches to achieve force control with position-controlled robots; one is a modified parallel position/force control, and the other is an admittance control. Secondly, we empirically study both control schemes when used as the action space of the RL agent. Thirdly, we developed a fail-safe mechanism for safely training an RL agent on manipulation tasks using a real rigid robot manipulator. The proposed methods are validated both on simulation and a real robot with an UR3 e-series robotic arm.",https://ieeexplore.ieee.org/document/9145608/,IEEE Robotics and Automation Letters,Oct. 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/41.704895,Modeling of ultrasonic range sensors for localization of autonomous mobile robots,IEEE,Journals,"This paper presents a probabilistic model of ultrasonic range sensors using backpropagation neural networks trained on experimental data. The sensor model provides the probability of detecting mapped obstacles in the environment, given their position and orientation relative to the transducer. The detection probability can be used to compute the location of an autonomous vehicle from those obstacles that are more likely to be detected. The neural network model is more accurate than other existing approaches, since it captures the typical multilobal detection pattern of ultrasonic transducers. Since the network size is kept small, implementation of the model on a mobile robot can be efficient for real-time navigation. An example that demonstrates how the credence could be incorporated into the extended Kalman filter (EKF) and the numerical values of the final neural network weights are provided in the appendices.",https://ieeexplore.ieee.org/document/704895/,IEEE Transactions on Industrial Electronics,Aug. 1998,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/TAMD.2010.2086453,Multilevel Darwinist Brain (MDB): Artificial Evolution in a Cognitive Architecture for Real Robots,IEEE,Journals,"The multilevel Darwinist brain (MDB) is a cognitive architecture that follows an evolutionary approach to provide autonomous robots with lifelong adaptation. It has been tested in real robot on-line learning scenarios obtaining successful results that reinforce the evolutionary principles that constitute the main original contribution of the MDB. This preliminary work has lead to a series of improvements in the computational implementation of the architecture so as to achieve realistic operation in real time, which was the biggest problem of the approach due to the high computational cost induced by the evolutionary algorithms that make up the MDB core. The current implementation of the architecture is able to provide an autonomous robot with real time learning capabilities and the capability for continuously adapting to changing circumstances in its world, both internal and external, with minimal intervention of the designer. This paper aims at providing an overview or the architecture and its operation and defining what is required in the path towards a real cognitive robot following a developmental strategy. The design, implementation and basic operation of the MDB cognitive architecture are presented through some successful real robot learning examples to illustrate the validity of this evolutionary approach.",https://ieeexplore.ieee.org/document/5599851/,IEEE Transactions on Autonomous Mental Development,Dec. 2010,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/JPROC.2019.2898267,"On Proactive, Transparent, and Verifiable Ethical Reasoning for Robots",IEEE,Journals,"Previous work on ethical machine reasoning has largely been theoretical, and where such systems have been implemented, it has, in general, been only initial proofs of principle. Here, we address the question of desirable attributes for such systems to improve their real world utility, and how controllers with these attributes might be implemented. We propose that ethically critical machine reasoning should be proactive, transparent, and verifiable. We describe an architecture where the ethical reasoning is handled by a separate layer, augmenting a typical layered control architecture, ethically moderating the robot actions. It makes use of a simulation-based internal model and supports proactive, transparent, and verifiable ethical reasoning. To do so, the reasoning component of the ethical layer uses our Python-based belief-desire-intention (BDI) implementation. The declarative logic structure of BDI facilitates both transparency, through logging of the reasoning cycle, and formal verification methods. To prove the principles of our approach, we use a case study implementation to experimentally demonstrate its operation. Importantly, it is the first such robot controller where the ethical machine reasoning has been formally verified.",https://ieeexplore.ieee.org/document/8648363/,Proceedings of the IEEE,March 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ACCESS.2018.2882875,RL and ANN Based Modular Path Planning Controller for Resource-Constrained Robots in the Indoor Complex Dynamic Environment,IEEE,Journals,"Traditional Reinforcement Learning (RL) approaches are designed to work well in static environments. In many real-world scenarios, the environments are complex and dynamic, in which the performance of traditional RL approaches may drastically degrade. One of the factors which results in the dynamicity and complexity of the environment is a change in the position and number of obstacles. This paper presents a path planning approach for autonomous mobile robots in a complex dynamic indoor environment, where the dynamic pattern of obstacles will not drastically affect the performance of RL models. Two independent modules, collision avoidance without considering the goal position and goal-seeking without considering obstacles avoidance, are trained independently using artificial neural networks and RL to obtain their best control policies. Then, a switching function is used to combine the two trained modules for realizing the obstacle avoidance and global path planning in a complex dynamic indoor environment. Furthermore, this control system is designed with a special focus on the computational and memory requirements of resource-constrained robots. The design was tested in a real-world environment on a mini-robot with constrained resources. Along with the static and dynamic obstacles' avoidance, this system has the ability to achieve both static and dynamic targets. This control system can also be used to train a robot in the real world using RL when the robot cannot afford to collide. Robot behavior in the real ground shows a very strong correlation with the simulation results.",https://ieeexplore.ieee.org/document/8543176/,IEEE Access,2018,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/TSMC.2020.3032437,RoboSeg: Real-Time Semantic Segmentation on Computationally Constrained Robots,IEEE,Journals,"Real-time and high-performance segmentation is a crucial but challenging perception task for computationally constrained robots, such as the humanoid NAO robot used in the RoboCup Soccer Standard Platform League. However, most existing convolutional neural network (CNN)-based models for semantic segmentation suffer from massive computational costs, which prevents them from being applied to performing real-time inference with a NAO. In this article, we first publish meticulously annotated datasets for training and evaluating semantic segmentation models. Then, we propose a fast downsampling module that downsamples the image while maintaining the spatial information and a novel dense learning module that learns high-level semantic information while recovering the spatial details. Based on these operations, by using a multiscale fusion method to recover the resolution, we propose a more efficient and real-time segmentation model called RoboSeg primarily aimed at offering better speed and accuracy tradeoffs. Finally, to accommodate practical engineering applications, we offer a promising deployment guideline for the CNN model describing how to deploy it on computational resource-limited robots and achieve real-time performance. The experimental results show that the RoboSeg exceeds the state-of-the-art networks in RoboCup scene segmentation: we attain a mean IoU of 87.35% and a pixel accuracy of 96.88% on our dataset using a model that contains only 0.29M parameters and performs just 0.73 GFLOPs. Under the proposed deployment strategies, the network can run at above 30 FPS on NAO robots with downsampled frames.",https://ieeexplore.ieee.org/document/9259011/,"IEEE Transactions on Systems, Man, and Cybernetics: Systems",March 2022,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/TSMCB.2012.2192107,Robust Multiperson Detection and Tracking for Mobile Service and Social Robots,IEEE,Journals,"This paper proposes an efficient system which integrates multiple vision models for robust multiperson detection and tracking for mobile service and social robots in public environments. The core technique is a novel maximum likelihood (ML)-based algorithm which combines the multimodel detections in mean-shift tracking. First, a likelihood probability which integrates detections and similarity to local appearance is defined. Then, an expectation-maximization (EM)-like mean-shift algorithm is derived under the ML framework. In each iteration, the E-step estimates the associations to the detections, and the M-step locates the new position according to the ML criterion. To be robust to the complex crowded scenarios for multiperson tracking, an improved sequential strategy to perform the mean-shift tracking is proposed. Under this strategy, human objects are tracked sequentially according to their priority order. To balance the efficiency and robustness for real-time performance, at each stage, the first two objects from the list of the priority order are tested, and the one with the higher score is selected. The proposed method has been successfully implemented on real-world service and social robots. The vision system integrates stereo-based and histograms-of-oriented-gradients-based human detections, occlusion reasoning, and sequential mean-shift tracking. Various examples to show the advantages and robustness of the proposed system for multiperson tracking from mobile robots are presented. Quantitative evaluations on the performance of multiperson tracking are also performed. Experimental results indicate that significant improvements have been achieved by using the proposed method.",https://ieeexplore.ieee.org/document/6187748/,"IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics)",Oct. 2012,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/TCST.2019.2914634,Robust Regressor-Free Control of Rigid Robots Using Function Approximations,IEEE,Journals,"This paper develops a novel regressor-free robust controller for rigid robots whose dynamics can be described using the Euler-Lagrange equations of motion. The function approximation technique (FAT) is used to represent the robot's inertia matrix, the Coriolis matrix, and the gravity vector as finite linear combinations of orthonormal basis functions. The proposed controller establishes a robust FAT control framework that uses a fixed control structure. The control objectives are to track reference trajectories in worst case scenarios where the robot dynamics are too costly to develop or otherwise unavailable. Detailed stability analysis via Lyapunov functions, the passivity property, and continuous switching laws shows uniform ultimate boundedness of the closed-loop dynamics. The simulation results of a three-degree-of-freedom (DOF) robot when the robot parameters are perturbed from their nominal values show good robustness of the proposed controller when compared with some well-established control methods. We also demonstrate success in the real-time experimental implementation of the proposed controller, which validates practicality for real-world robotic applications.",https://ieeexplore.ieee.org/document/8718993/,IEEE Transactions on Control Systems Technology,July 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/21.61208,Satisficing feedback strategies for local navigation of autonomous mobile robots,IEEE,Journals,"A general approach to the local navigation problem for autonomous mobile robots (AMRs) is presented and its application to omnidirectional and conventionally steered wheelbases is described. The problem of driving an AMR to a goal in an unknown environment is formulated as a dynamic feedback control problem in which local feedback information is used to make steering decisions while the AMR is moving. To obtain a computationally tractable algorithm, a class of satisficing feedback strategies that generate reasonable, collision-free trajectories to the goal using simplified representations of the AMR dynamics and constrains is proposed. Realizations of the feedback strategy are presented and illustrated by simulation under the assumptions of perfect feedback information and zero servo error. Straightforward extensions of the approach to handle uncertainties in real systems are briefly described.<>",https://ieeexplore.ieee.org/document/61208/,"IEEE Transactions on Systems, Man, and Cybernetics",Nov.-Dec. 1990,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/70.88137,The vector field histogram-fast obstacle avoidance for mobile robots,IEEE,Journals,"A real-time obstacle avoidance method for mobile robots which has been developed and implemented is described. This method, named the vector field histogram (VFH), permits the detection of unknown obstacles and avoids collisions while simultaneously steering the mobile robot toward the target. The VFH method uses a two-dimensional Cartesian histogram grid as a world model. This world model is updated continuously with range data sampled by onboard range sensors. The VFH method subsequently uses a two-stage data-reduction process to compute the desired control commands for the vehicle. Experimental results from a mobile robot traversing densely cluttered obstacle courses in smooth and continuous motion and at an average speed of 0.6-0.7 m/s are shown. A comparison of the VFN method to earlier methods is given.<>",https://ieeexplore.ieee.org/document/88137/,IEEE Transactions on Robotics and Automation,June 1991,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/LRA.2018.2851148,Visual Navigation for Biped Humanoid Robots Using Deep Reinforcement Learning,IEEE,Journals,"In this letter, we propose a map-less visual navigation system for biped humanoid robots, which extracts information from color images to derive motion commands using deep reinforcement learning (DRL). The map-less visual navigation policy is trained using the Deep Deterministic Policy Gradients (DDPG) algorithm, which corresponds to an actor-critic DRL algorithm. The algorithm is implemented using two separate networks, one for the actor and one for the critic, but with similar structures. In addition to convolutional and fully connected layers, Long Short-Term Memory (LSTM) layers are included to address the limited observability present in the problem. As a proof of concept, we consider the case of robotic soccer using humanoid NAO V5 robots, which have reduced computational capabilities, and low-cost Red - Green - Blue (RGB) cameras as main sensors. The use of DRL allowed to obtain a complex and high performant policy from scratch, without any prior knowledge of the domain, or the dynamics involved. The visual navigation policy is trained in a robotic simulator and then successfully transferred to a physical robot, where it is able to run in 20 ms, allowing its use in real-time applications.",https://ieeexplore.ieee.org/document/8398461/,IEEE Robotics and Automation Letters,Oct. 2018,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IJCNN48605.2020.9207496,"""I’m Sorry Dave, I’m Afraid I Can’t Do That"" Deep Q-Learning from Forbidden Actions",IEEE,Conferences,"The use of Reinforcement Learning (RL) is still restricted to simulation or to enhance human-operated systems through recommendations. Real-world environments (e.g. industrial robots or power grids) are generally designed with safety constraints in mind implemented in the shape of valid actions masks or contingency controllers. For example, the range of motion and the angles of the motors of a robot can be limited to physical boundaries. Violating constraints thus results in rejected actions or entering in a safe mode driven by an external controller, making RL agents incapable of learning from their mistakes. In this paper, we propose a simple modification of a state-of-the-art deep RL algorithm (DQN), enabling learning from forbidden actions. To do so, the standard Q-learning update is enhanced with an extra safety loss inspired by structured classification. We empirically show that it reduces the number of hit constraints during the learning phase and accelerates convergence to near-optimal policies compared to using standard DQN. Experiments are done on a Visual Grid World Environment and the TextWorld domain.",https://ieeexplore.ieee.org/document/9207496/,2020 International Joint Conference on Neural Networks (IJCNN),19-24 July 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/WCICA.2016.7578819,3D vision based fast badminton localization with prediction and error elimination for badminton robot,IEEE,Conferences,"In this paper, the problem of fast badminton localization problem is investigated for a class of badminton robots. More precisely, a manifold-learning based localization method is implemented for the improvement of hitting accuracy and effectiveness. Based on the localization results, a novel badminton trajectory prediction algorithm is designed based on 3D Vision in the real world. Furthermore, clock-synchronization combined with motion compensation methods are also proposed to better localization error elimination. In the end, the validity and usefulness of our proposed algorithm is demonstrated by numerical experiments.",https://ieeexplore.ieee.org/document/7578819/,2016 12th World Congress on Intelligent Control and Automation (WCICA),12-15 June 2016,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA.2018.8461228,3DOF Pedestrian Trajectory Prediction Learned from Long-Term Autonomous Mobile Robot Deployment Data,IEEE,Conferences,"This paper presents a novel 3DOF pedestrian trajectory prediction approach for autonomous mobile service robots. While most previously reported methods are based on learning of 2D positions in monocular camera images, our approach uses range-finder sensors to learn and predict 3DOF pose trajectories (i.e. 2D position plus 1D rotation within the world coordinate system). Our approach, T-Pose-LSTM (Temporal 3DOF-Pose Long-Short-Term Memory), is trained using long-term data from real-world robot deployments and aims to learn context-dependent (environment- and time-specific) human activities. Our approach incorporates long-term temporal information (i.e. date and time) with short-term pose observations as input. A sequence-to-sequence LSTM encoder-decoder is trained, which encodes observations into LSTM and then decodes the resulting predictions. On deployment, the approach can perform on-the-fly prediction in real-time. Instead of using manually annotated data, we rely on a robust human detection, tracking and SLAM system, providing us with examples in a global coordinate system. We validate the approach using more than 15 km of pedestrian trajectories recorded in a care home environment over a period of three months. The experiments show that the proposed T-Pose-LSTM model outperforms the state-of-the-art 2D-based method for human trajectory prediction in long-term mobile robot deployments.",https://ieeexplore.ieee.org/document/8461228/,2018 IEEE International Conference on Robotics and Automation (ICRA),21-25 May 2018,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA40945.2020.9197155,A 3D-Deep-Learning-based Augmented Reality Calibration Method for Robotic Environments using Depth Sensor Data,IEEE,Conferences,"Augmented Reality and mobile robots are gaining increased attention within industries due to the high potential to make processes cost and time efficient. To facilitate augmented reality, a calibration between the Augmented Reality device and the environment is necessary. This is a challenge when dealing with mobile robots due to the mobility of all entities making the environment dynamic. On this account, we propose a novel approach to calibrate Augmented Reality devices using 3D depth sensor data. We use the depth camera of a Head Mounted Augmented Reality Device, the Microsoft Hololens, for deep learning-based calibration. Therefore, we modified a neural network based on the recently published VoteNet architecture which works directly on raw point cloud input observed by the Hololens. We achieve satisfying results and eliminate external tools like markers, thus enabling a more intuitive and flexible work flow for Augmented Reality integration. The results are adaptable to work with all depth cameras and are promising for further research. Furthermore, we introduce an open source 3D point cloud labeling tool, which is to our knowledge the first open source tool for labeling raw point cloud data.",https://ieeexplore.ieee.org/document/9197155/,2020 IEEE International Conference on Robotics and Automation (ICRA),31 May-31 Aug. 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/WI-IAT.2010.210,A Biologically-Inspired Cognitive Agent Model Integrating Declarative Knowledge and Reinforcement Learning,IEEE,Conferences,"The paper proposes a biologically-inspired cognitive agent model, known as FALCON-X, based on an integration of the Adaptive Control of Thought (ACT-R) architecture and a class of self-organizing neural networks called fusion Adaptive Resonance Theory (fusion ART). By replacing the production system of ACT-R by a fusion ART model, FALCON-X integrates high-level deliberative cognitive behaviors and real-time learning abilities, based on biologically plausible neural pathways. We illustrate how FALCON-X, consisting of a core inference area interacting with the associated intentional, declarative, perceptual, motor and critic memory modules, can be used to build virtual robots for battles in a simulated RoboCode domain. The performance of FALCON-X demonstrates the efficacy of the hybrid approach.",https://ieeexplore.ieee.org/document/5616152/,2010 IEEE/WIC/ACM International Conference on Web Intelligence and Intelligent Agent Technology,31 Aug.-3 Sept. 2010,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISCAS.2003.1205068,A CNN-based chip for robot locomotion control,IEEE,Conferences,"In this paper a VLSI chip for real-time locomotion control in legged robots is introduced. The control is based on the biological paradigm of Central Pattern Generator (CPG) and is implemented by a Cellular Neural Network (CNN). The gait generation is accomplished by the CNN and is fully analog, while a digital controller modulates the behavior of the CNN-based CPG to allow the locomotion system to adapt to sensory feedback. The chip is designed with a switched-capacitor technique, fundamental to address the speed control issue. Experimental results on the first prototype are illustrated. These results confirm the suitability of the approach and open the way to the design of a fully autonomous bio-inspired micro-robot.",https://ieeexplore.ieee.org/document/1205068/,2003 IEEE International Symposium on Circuits and Systems (ISCAS),25-28 May 2003,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IMTC.1998.679788,A CNN-based passive optical range finder for real time robotic applications,IEEE,Conferences,"The paper presents a new CNN for real-time stereo vision, useful as a passive optical range finder for autonomous robots and vehicles. The stereo matching as energy minimization is discussed and former neural approaches to the problem are analyzed. Experimental results with the new CNN both with synthetic and real images are reported, demonstrating the performance of the system.",https://ieeexplore.ieee.org/document/679788/,IMTC/98 Conference Proceedings. IEEE Instrumentation and Measurement Technology Conference. Where Instrumentation is Going (Cat. No.98CH36222),18-21 May 1998,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/RTCSA.2018.00012,A Case Study of Cyber-Physical System Design: Autonomous Pick-and-Place Robot,IEEE,Conferences,"Although modern robots in warehousing systems can perform adequately in a goods-to-person model using hand-designed algorithms that are specialized to a particular environment, developing a robotic system that is capable of handling new products at an inexpensive cost remains a challenge. A conspicuous example of this challenge is seen in Amazon's use of autonomous robots to fetch customers' orders in their massive warehouses. To encourage advance in this technology, Amazon organized the competition, Amazon Picking Challenge that asked participants to develop their own hardware and software for the general task of picking a designated set of products from inventory shelves and then placing them at a target location (called a pick-and-place task). Current technology for pick-and-place tasks is still insufficient to meet the demand for low-cost automation. Handling awkward or oddly shaped object must still depend on hand-programming or specialized robotic systems, making manufacturing automation less flexible and expensive. In this paper, we shall present the design and implementation of a software system that is a step in advancing the technology toward full automation at reasonable costs. Our system integrates a set of state-of-the-art techniques in computer vision, deep-learning, trajectory optimization, visual servoing to create a library of skills that can be composed to perform a variety of robotic tasks. We demonstrate the capability of our system for performing autonomous pick-and-place tasks with an implementation using Hoppy, an industrial robotic arm in an environment similar to the Amazon Picking Challenge.",https://ieeexplore.ieee.org/document/8607230/,2018 IEEE 24th International Conference on Embedded and Real-Time Computing Systems and Applications (RTCSA),28-31 Aug. 2018,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRITO51393.2021.9596550,A Comparative Study on Shortest Path Visualization using Artificial Intelligence,IEEE,Conferences,"In the modern computation system, that rely on various aspects to obtain the optimal results in easy manner appears more deterministic. There are several algorithms available that can distinguish a probable shortest path between two points, which helps students actively study algorithms with visualization. Therefore, in this study, we developed the GUI based shortest path finding tool consists of Dijkstra and A* algorithm. Further, in this study, the obtained shortest path results were graphically visualised and tabular output are stored in the database. The implementation of the algorithm and visualization was developed using Java AWT API and SWING package of Java. The Dijkstra and A* algorithm comparative analysis showed that the checks value and path length in terms of the A* algorithm is comparatively less than Dijkstra. Thus, it is affirmative that A * approach produces faster results and appeared as more efficient in terms of destination path finding than Dijkstra algorithm. Path finding is a fundamental feature of many significant applications and can be applied in static, interactive, and real-time situations. Such information shed light on the efficient application for computer gaming, robots, logistics, and crowd simulation.",https://ieeexplore.ieee.org/document/9596550/,"2021 9th International Conference on Reliability, Infocom Technologies and Optimization (Trends and Future Directions) (ICRITO)",3-4 Sept. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/FDL53530.2021.9568376,A Container-based Design Methodology for Robotic Applications on Kubernetes Edge-Cloud architectures,IEEE,Conferences,"Programming modern Robots&#x0027; missions and behavior has become a very challenging task. The always increasing level of autonomy of such platforms requires the integration of multi-domain software applications to implement artificial intelligence, cognition, and human-robot/robot-robot interaction applications. In addition, to satisfy both functional and nonfunctional requirements such as reliability and energy efficiency, robotic SW applications have to be properly developed to take advantage of heterogeneous (Edge-Fog-Cloud) architectures. In this context, containerization and orchestration are becoming a standard practice as they allow for better information flow among different network levels as well as increased modularity in the use of software components. Nevertheless, the adoption of such a practice along the design flow, from simulation to the deployment of complex robotic applications by addressing the de-facto development standards (i.e., robotic operating system - ROS - compliancy for robotic applications) is still an open problem. We present a design methodology based on Docker and Kubernetes that enables containerization and orchestration of ROS-based robotic SW applications for heterogeneous and hierarchical HW architectures. The design methodology allows for (i) integration and verification of multi-domain components since early in the design flow, (ii) task-to-container mapping techniques to guarantee minimum overhead in terms of performance and memory footprint, and (iii) multi-domain verification of functional and non-functional constraints before deployment. We present the results obtained in a real case of study, in which the design methodology has been applied to program the mission of a Robotnik RB-Kairos mobile robot in an industrial agile production chain. The source code of the mobile robot is publicly available on GitHub.",https://ieeexplore.ieee.org/document/9568376/,2021 Forum on specification & Design Languages (FDL),8-10 Sept. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ACSOS-C51401.2020.00067,A Deep Domain-Specific Model Framework for Self-Reproducing Robotic Control Systems,IEEE,Conferences,"As robots play more critical roles in diverse and complex scenarios in the real world, monomorphic robots are limited to repeating and rather simple tasks. How to achieve a robust, flexible, and scalable multi-robot system becomes essential research. Model-driven software development (MDSD) provides a sturdy methodology for robotic programming using multilevel domain-specific languages (DSLs). These DSLs lay a solid foundation for the design, integration, and extensibility of robotic applications. In this paper, we propose a deep domain-specific model framework for the self-reproducing robotic control system to escort reliable, versatile tasks of heterogeneous robots.",https://ieeexplore.ieee.org/document/9196470/,2020 IEEE International Conference on Autonomic Computing and Self-Organizing Systems Companion (ACSOS-C),17-21 Aug. 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISKE54062.2021.9755365,A Feasible System of Automatic Flame Detection and Tracking for Fire-fighting Robot,IEEE,Conferences,"Traditional fire-fighting robots are limited by fire detection and location technology. The detection and location accuracy are greatly affected by the environment, resulting in poor performance, complex deployment and low intellectualization. In this work, a set of automatic fire location and tracking system is designed and implemented based on deep learning, which is also integrated with video image processing technology and open source computer vision library (OpenCV). The system uses a high-real-time deep learning model for fire detection, and it eliminates false alarms by calculating and comparing the structural similarity ratio of the images, as well as combined with the dynamic characteristics of the flame, thereby further improving the detection accuracy. Additionally, our work facilitates the deployment by taking advantage of the monocular camera to locate and track the fire source. Experimental results have demonstrated that the system delivers advantages such as high detection accuracy, good real-time performance, long monitoring distance, and fast response speed. These results also allow the proposed system to be a prime candidate for fire-fighting robots in various complex environments.",https://ieeexplore.ieee.org/document/9755365/,2021 16th International Conference on Intelligent Systems and Knowledge Engineering (ISKE),26-28 Nov. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IJCNN48605.2020.9207308,A Few-shot Dynamic Obstacle Avoidance Strategy in Unknown Environments,IEEE,Conferences,"Obstacle avoidance is one of the basic capabilities of intelligent mobile robots. With the diversification of the application environment, mobile robots are required to avoid obstacles with higher generality. Benefit from the development of mobile platform and deep learning algorithm in recent years, we conceive a few-shot dynamic obstacle avoidance strategy to meet this higher generality demand. Under this metric-based metalearning method, mobile robots can quickly adapt to unknown environments by learning from several samples. In order to verify its effectiveness, we use this strategy to train a model and deploy it to the mobile robot and run multiple obstacle avoidance recognition tests in the real-world environment. The results of experiments performed on the mobile robot platform illustrates a good performance and verifies our proposed strategy. In addition to analyzing the experimental results, the advantages, disadvantages as well as application potential of the proposed strategy as a decision aid are also discussed.",https://ieeexplore.ieee.org/document/9207308/,2020 International Joint Conference on Neural Networks (IJCNN),19-24 July 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS45743.2020.9341606,A Framework for Online Updates to Safe Sets for Uncertain Dynamics,IEEE,Conferences,"Safety is crucial for deploying robots in the real world. One way of reasoning about safety of robots is by building safe sets through Hamilton-Jacobi (HJ) reachability. However, safe sets are often computed offline, assuming perfect knowledge of the dynamics, due to high compute time. In the presence of uncertainty, the safe set computed offline becomes inaccurate online, potentially leading to dangerous situations on the robot. We propose a novel framework to learn a safe control policy in simulation, and use it to generate online safe sets under uncertain dynamics. We start with a conservative safe set and update it online as we gather more information about the robot dynamics. We also show an application of our framework to a model-based reinforcement learning problem, proposing a safe model-based RL setup. Our framework enables robots to simultaneously learn about their dynamics, accomplish tasks, and update their safe sets. It also generalizes to complex high-dimensional dynamical systems, like 3-link manipulators and quadrotors, and reliably avoids obstacles, while achieving a task, even in the presence of unmodeled noise.",https://ieeexplore.ieee.org/document/9341606/,2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),24 Oct.-24 Jan. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA46639.2022.9811744,A Framework for Real-World Multi-Robot Systems Running Decentralized GNN-Based Policies,IEEE,Conferences,"Graph Neural Networks (GNNs) are a paradigm-shifting neural architecture to facilitate the learning of complex multi-agent behaviors. Recent work has demonstrated remarkable performance in tasks such as flocking, multi-agent path planning and cooperative coverage. However, the policies derived through GNN-based learning schemes have not yet been deployed to the real-world on physical multi-robot systems. In this work, we present the design of a system that allows for fully decentralized execution of GNN-based policies. We create a framework based on ROS2 and elaborate its details in this paper. We demonstrate our framework on a case-study that requires tight coordination between robots, and present first-of-a-kind results that show successful real-world deployment of GNN-based policies on a decentralized multi-robot system relying on Adhoc communication. A video demonstration of this case-study can be found online<sup>1</sup><sup>1</sup>youtube.com/watch?v&#x003D;COh-WLn4i04.",https://ieeexplore.ieee.org/document/9811744/,2022 International Conference on Robotics and Automation (ICRA),23-27 May 2022,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IJCNN48605.2020.9206637,A Lightweight Neural-Net with Assistive Mobile Robot for Human Fall Detection System,IEEE,Conferences,"Falls are a major health issue, particularly among the elderly. Increasing fall events require high service quality and dedicated medical treatment which is an economic burden. In the lack of appropriate care and support, serious injuries caused by fall will cost lives. Therefore, tracking systems with fall detection capabilities are required. Static-view sensors with machine learning techniques for human fall detection have been widely studied and achieved significant results. However, these systems unable to monitor a person if he or she is out of viewing angle which greatly impedes its performance. Mobile robots are an alternative for keeping the person in sight. However, existing mobile robots are unable to operate for a long time due to battery issues and movement constraints in complex environments. In this paper, we proposed a lightweight deep learning vision-based model for human fall detection with an assistive robot to provide assistance when a fall happens. The proposed detection system requires less computational power which can be implemented in a low-cost 2D camera and GPU board for real-time monitoring. The assistive robot equipped with various sensors that can perform SLAM, obstacle avoidance and navigation autonomously. Our proposed system integrates these two sub-systems to compensate for the weakness of each other to constitute a system that robust, adaptable, and high performance. The proposed method has been validated through a series of experiments.",https://ieeexplore.ieee.org/document/9206637/,2020 International Joint Conference on Neural Networks (IJCNN),19-24 July 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/RoSE52553.2021.00011,A Modeling Tool for Reconfigurable Skills in ROS,IEEE,Conferences,"Known attempts to build autonomous robots rely on complex control architectures, often implemented with the Robot Operating System platform (ROS). The implementation of adaptable architectures is very often ad hoc, quickly gets cumbersome and expensive. Reusable solutions that support complex, runtime reasoning for robot adaptation have been seen in the adoption of ontologies. While the usage of ontologies significantly increases system reuse and maintainability, it requires additional effort from the application developers to translate requirements into formal rules that can be used by an ontological reasoner. In this paper, we present a design tool that facilitates the specification of reconfigurable robot skills. Based on the specified skills, we generate corresponding runtime models for self-adaptation that can be directly deployed to a running robot that uses a reasoning approach based on ontologies. We demonstrate the applicability of the tool in a real robot performing a patrolling mission at a university campus.",https://ieeexplore.ieee.org/document/9474550/,2021 IEEE/ACM 3rd International Workshop on Robotics Software Engineering (RoSE),2-2 June 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISMR48331.2020.9312950,A Multi-Modal Learning System for On-Line Surgical Action Segmentation,IEEE,Conferences,"Surgical action recognition and temporal segmentation is a building block needed to provide some degrees of autonomy to surgical robots. In this paper, we present a deep learning model that relies on videos and kinematic data to output in real-time the current action in a surgical procedure. The proposed neural network architecture is composed of two sub-networks: a Spatial-Kinematic Network, which produces high-level features by processing images and kinematic data, and a Temporal Convolutional Network, which filters such features temporally over a sliding window to stabilize their changes over time. Since we are interested in applications to real-time supervisory control of robots, we focus on an efficient and causal implementation, i.e. the prediction at sample k only depends on previous observations. We tested our causal architecture on the publicly available JIGSAWS dataset, outperforming comparable state-of-the-art non-causal algorithms up to 8.6% in the edit score.",https://ieeexplore.ieee.org/document/9312950/,2020 International Symposium on Medical Robotics (ISMR),18-20 Nov. 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICICSP54369.2021.9611881,A Multi-agent Reinforcement Learning Routing Protocol in Mobile Robot Network,IEEE,Conferences,"Robots are now essential in unreachable, repeated, and dangerous real-world applications where they take place of human beings. One of the important capabilities of a multi-robot system is that it should be able to form an autonomous robot network to transmit information. However, due to the limited communication capability of a single robot, the highly variable environment where robots work, and the mobility of the robots, it is difficult for them to exchange information with each other in need. In this article, we propose a novel robot network routing protocol based on multi-agent reinforcement learning called MAQR. The robot nodes can deliver packets cooperatively. The mobility factor, buffer status, and packet delay of neighbor nodes are taken into consideration. We design a reliability model for a robot agent to make reliable routing decisions. We also design an adaptive exploration-exploitation method to balance the convergence speed, solution space as well as network fluctuation. The routing algorithm has been implemented and evaluated in simulation. Results show that MAQR can provide less packet delay, less average queue length and higher delivery ratio than other Q-learning based routing protocol.",https://ieeexplore.ieee.org/document/9611881/,2021 4th International Conference on Information Communication and Signal Processing (ICICSP),24-26 Sept. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/FUZZ48607.2020.9177557,A Novel Self-Organizing PID Approach for Controlling Mobile Robot Locomotion,IEEE,Conferences,"A novel self-organizing fuzzy proportional-integral-derivative (SOF-PID) control system is proposed in this paper. The proposed system consists of a pair of control and reference models, both of which are implemented by a first-order autonomous learning multiple model (ALMMo) neuro-fuzzy system. The SOF-PID controller self-organizes and self-updates the structures and meta-parameters of both the control and reference models during the control process ""on the fly"". This gives the SOF-PID control system the capability of quickly adapting to entirely new operating environments without a full re-training. Moreover, the SOF-PID control system is free from user- and problem-specific parameters and is entirely data-driven. Simulations and real-world experiments with mobile robots demonstrate the effectiveness and validity of the proposed SOF-PID control system.",https://ieeexplore.ieee.org/document/9177557/,2020 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE),19-24 July 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/RO-MAN46459.2019.8956259,A Reinforcement-Learning Approach for Adaptive and Comfortable Assistive Robot Monitoring Behavior,IEEE,Conferences,"Companion robots used in the field of elderly assistive care can be of great value in monitoring their everyday activities and well-being. However, in order to be accepted by the user, their behavior, while monitoring them, should not provide discomfort: robots must take into account the activity the user is performing and not be a distraction for them. In this paper, we propose a Reinforcement Learning approach to adaptively decide a monitoring distance and an approaching direction starting from an estimation of the current activity obtained by the use of a wearable device. Our goal is to improve user activity recognition performance without making the robot's presence uncomfortable for the monitored person. Results show that the proposed approach is promising for real scenario deployment, succeeding in accomplishing the task in more than 80%of episodes run.",https://ieeexplore.ieee.org/document/8956259/,2019 28th IEEE International Conference on Robot and Human Interactive Communication (RO-MAN),14-18 Oct. 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CISDA.2007.368137,A Review of Intelligent Systems Software for Autonomous Vehicles,IEEE,Conferences,"The need for intelligent unmanned vehicles has been steadily increasing. These vehicles could be air-, ground-, space-, or sea-based. This paper will review some of the most common software systems and methods that could be used for controlling such vehicles. Early attempts at mobile robots were confined to simple laboratory environments. For vehicles to operate in real-world noisy and uncertain environments, they need to include numerous sensors and they need to include both reactive and deliberative features. The most effective software systems have been hierarchical or multi-layered. Many of these systems mimic biological systems. This paper reviews several software approaches for autonomous vehicles. While there are similarities, there are differences as well. Most of these software systems are very difficult to use, and few of them have the ability to learn. Autonomous vehicles promise remarkable capabilities for both civilian and military applications, but much work remains to develop intelligent systems software which can be used for a wide range of applications. In particular there is a need for reliable open-source software that can be used on inexpensive autonomous vehicles",https://ieeexplore.ieee.org/document/4219084/,2007 IEEE Symposium on Computational Intelligence in Security and Defense Applications,1-5 April 2007,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.23919/ChiCC.2019.8865929,A Speech Interaction System Based on Cloud Service under ROS,IEEE,Conferences,"With the recent development of technologies such as artificial intelligence and cloud computing, intelligent service robots are more and more widely used, and current users are paying more attention to the speech interaction function of robots. In this paper, we design a cloud-based intelligent speech interaction system under ROS to enhance the human-robot interaction experience, and carry out related tests on the laptop. The system we designed runs stably and smoothly, and has high real-time performance, which can satisfy user's requirements.",https://ieeexplore.ieee.org/document/8865929/,2019 Chinese Control Conference (CCC),27-30 July 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ROBOT.1998.676374,A control architecture to achieve manipulation task goals for a humanoid robot,IEEE,Conferences,"Focusing on the manipulation tasks to be executed by humanoid robots, principal requirements which are to be satisfied by hardware/software of the control system are considered. In order to meet the requirements, a novel type of hardware structure and software architecture is proposed. Since the target humanoid robot consists of multiple subsystems such as a central controller for brain, a vision controller for eye, and five motion sub-controllers for two arms, two hands, one spine, the on-board hardware control system is designed to have a distributed control structure connected by pseudo real-time Ethernet interfaces. A goal-achieving software architecture is also proposed which meets the requirements of semi-autonomy, reactivity, expandability, and object-orientedness. Specifically, in order to achieve reactivity, a coordination method is proposed to configure three kinds of executive modules, primitive module, flow-control module, and goal module, which have multiple exit states. The control architecture proposed has been implemented for performing toy-block assembly tasks on a humanoid robot as well as on the graphic simulator.",https://ieeexplore.ieee.org/document/676374/,Proceedings. 1998 IEEE International Conference on Robotics and Automation (Cat. No.98CH36146),20-20 May 1998,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ROBIO.2007.4522431,A fast robot feedback decision algorithm,IEEE,Conferences,"Intelligence decision is an important subject for robotic intelligence system. This paper presents an experimental research work about robotic intelligence decision, where a fast information reduction with feedback features is proposed to solve robot intelligent decision and judgement issue. We have investigated the properties satisfied by the robotic information processing and decision process: if an attribute is the only discerning of multiple information objects, this attribute can be taken as the first core attribute, and can be reused in an iterative information reduction process to get another core attribute; furthermore, a feedback method can be used to enhance this iterative processing capability in real-time robots cooperative systems. Out feedback control method is somewhat dynamical and iterative process, and the investigated properties are utilized to design the feedback control laws. We have set up the relative experimental platform, and the experimental results show that our fast robot feedback decision algorithm is efficient and effective, which can be used in multiple robots collaborative missions.",https://ieeexplore.ieee.org/document/4522431/,2007 IEEE International Conference on Robotics and Biomimetics (ROBIO),15-18 Dec. 2007,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS.2003.1248811,A hardwired polymorphic neural network for a CPU-less autonomous mobile robot,IEEE,Conferences,"In order to construct truly autonomous mobile robots, the concept of implementability is highly indispensable: all parts such as controllers, power systems, batteries should be embedded inside a body. Therefore, the implementing controller on hardware is one of the most promising ways, since this contributes to low power consumption and miniaturization and so on. Another crucial requirement in the field of autonomous mobile robots is robustness. That is, autonomous mobile robots have to cope with their unpredictably changing environment in real time. In this study, to meet these requirements the concept of dynamically rearrangeable electrical circuit (DREC) is proposed and we implement this onto FPGAs as physical electronic circuits by borrowing the idea from neuromodulation widely observed in biological nervous systems by the diffusion-reaction mechanism of neuromodulators. We developed the DREC for the peg-pushing task as a practical example. We confirmed that the physical DREC can successfully regulate the behavior according to the situation by changing its properties in real time.",https://ieeexplore.ieee.org/document/1248811/,Proceedings 2003 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2003) (Cat. No.03CH37453),27-31 Oct. 2003,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS.2013.6696771,A learning-based approach to robust binaural sound localization,IEEE,Conferences,"Sound source localization is an important feature designed and implemented on robots and intelligent systems. Like other artificial audition tasks, it is constrained to multiple problems, notably sound reflections and noises. This paper presents a sound source azimuth estimation approach in reverberant environments. It exploits binaural signals in a humanoid robotic context. Interaural Time and Level Differences (ITD and ILD) are extracted on multiple frequency bands and combined with a neural network-based learning scheme. A cue filtering process is used to reduce the reverberations effects. The system has been evaluated with simulation and real data, in multiple aspects covering realistic robot operating conditions, and was proven satisfying and effective as will be shown and discussed in the paper.",https://ieeexplore.ieee.org/document/6696771/,2013 IEEE/RSJ International Conference on Intelligent Robots and Systems,3-7 Nov. 2013,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS.2017.8206437,A multimodal execution monitor with anomaly classification for robot-assisted feeding,IEEE,Conferences,"Activities of daily living (ADLs) are important for quality of life. Robotic assistance offers the opportunity for people with disabilities to perform ADLs on their own. However, when a complex semi-autonomous system provides real-world assistance, occasional anomalies are likely to occur. Robots that can detect, classify and respond appropriately to common anomalies have the potential to provide more effective and safer assistance. We introduce a multimodal execution monitor to detect and classify anomalous executions when robots operate near humans. Our system builds on our past work on multimodal anomaly detection. Our new monitor classifies the type and cause of common anomalies using an artificial neural network. We implemented and evaluated our execution monitor in the context of robot-assisted feeding with a general-purpose mobile manipulator. In our evaluations, our monitor outperformed baseline methods from the literature. It succeeded in detecting 12 common anomalies from 8 able-bodied participants with 83% accuracy and classifying the types and causes of the detected anomalies with 90% and 81% accuracies, respectively. We then performed an in-home evaluation with Henry Evans, a person with severe quadriplegia. With our system, Henry successfully fed himself while the monitor detected, classified the types, and classified the causes of anomalies with 86%, 90%, and 54% accuracy, respectively.",https://ieeexplore.ieee.org/document/8206437/,2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),24-28 Sept. 2017,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SBRN.1998.730998,A neural-network based approach for recognition of pose and motion gestures on a mobile robot,IEEE,Conferences,"Since a variety of changes in both robotic hardware and software suggests that service robots will soon become possible, to find ""natural"" ways of communication between human and robots is of fundamental importance for the robotic field. The paper describes a gesture-based interface for human-robot interaction, which enables people to instruct robots through easy-to-perform arm gestures. Such gestures might be static pose gestures, which involve only a specific configuration of the person's arm, or they might be dynamic motion gestures, that is, they involve motion (such as waving). Gestures are recognized in real-time at approximate frame rate, using neural networks. A fast, color-based tracking algorithm enables the robot to track and follow a person reliably through office environments with drastically changing lighting conditions. Results are reported in the context of an interactive clean-up task, where a person guides the robot to specific locations that need to be cleaned, and the robot picks up trash which it then delivers to the nearest trash-bin.",https://ieeexplore.ieee.org/document/730998/,Proceedings 5th Brazilian Symposium on Neural Networks (Cat. No.98EX209),9-11 Dec. 1998,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CIG.2011.6032027,A neuronal global workspace for human-like control of a computer game character,IEEE,Conferences,"This paper describes a system that uses a global workspace architecture implemented in spiking neurons to control an avatar within the Unreal Tournament 2004 (UT2004) computer game. This system is designed to display human-like behaviour within UT2004, which provides a good environment for comparing human and embodied AI behaviour without the cost and difficulty of full humanoid robots. Using a biologically-inspired approach, the architecture is loosely based on theories about the high level control circuits in the brain, and it is the first neural implementation of a global workspace that is embodied in a dynamic real time environment. At its current stage of development the system can navigate through UT2004 and shoot opponents. We are currently completing the implementation and testing in preparation for the human-like bot competition at CIG 2011 in September.",https://ieeexplore.ieee.org/document/6032027/,2011 IEEE Conference on Computational Intelligence and Games (CIG'11),31 Aug.-3 Sept. 2011,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICVES.2016.7548165,A new hopfield-type neural network approach to multi-goal vehicle navigation in unknown environments,IEEE,Conferences,"A Hopfield-type neural networks (HNN) algorithm associated with histogram navigation method is proposed in this paper for real-time map building and path planning for multiple goals applications. In real world applications such as rescue robots, service robots, mining mobile robots, and mine searching robots, etc., an autonomous vehicle needs to reach multiple goals with a shortest path that, in this paper, is capable of being implemented by a HNN method with minimized overall distance. Once a global trajectory is planned, a foraging-enabled trail is created to guide the vehicle to the multiple goals. A histogram-based local navigation algorithm is employed to plan a collision-free path along the trail planned by the global path planner. A re-planning-based algorithm aims to generate trajectory while an autonomous vehicle explores through a terrain with map building in unknown environments. In this paper, simulation and experimental results demonstrate that the real-time concurrent mapping and multi-goal navigation of an autonomous vehicle is successfully performed under unknown environments.",https://ieeexplore.ieee.org/document/7548165/,2016 IEEE International Conference on Vehicular Electronics and Safety (ICVES),10-12 July 2016,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/WINCOM50532.2020.9272477,A new middleware for managing heterogeneous robot in ubiquitous environments,IEEE,Conferences,"Heterogeneity is one of the main issues for the deployment of the Industry 4.0. This is due to the diversity in the available robots and the IIoT devices. These equipments use different programming languages and communication protocols. To make the integration of such equipments easy, we propose TalkRoBots, a middleware that allows heterogeneous robots and IIoT devices to communicate together and exchange data in a transparent way. The middleware was experimented in a real scenario with different robots that demonstrate its efficiency.",https://ieeexplore.ieee.org/document/9272477/,2020 8th International Conference on Wireless Networks and Mobile Communications (WINCOM),27-29 Oct. 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SNPD.2016.7515880,A novel fuzzy omni-directional gait planning algorithm for biped robot,IEEE,Conferences,"Aiming at the problems in gait planning of the biped robots, including the complex model, low stability, etc., a novel fuzzy omni-directional gait planning algorithm (FOGPA) is proposed. At first, this method puts forward a new separated omni-directional gait planning model, which combines the straight walking planning algorithm based on the improved Hermite interpolation and the rotation motion together. And then, a fuzzy gait parameter adjustment algorithm is put forward to control the gait parameters including the step size and rotation speed dynamically. At last, the fuzzy control results are used to get the gait data of robot real-timely. The experiment results show that the FOGPA improves the stability and robustness of gait in a certain degree and also improves the adaptability to the complex environment of the robot.",https://ieeexplore.ieee.org/document/7515880/,"2016 17th IEEE/ACIS International Conference on Software Engineering, Artificial Intelligence, Networking and Parallel/Distributed Computing (SNPD)",30 May-1 June 2016,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/HAPTICS.2014.6775492,A novel haptic interface and control algorithm for robotic rehabilitation of stoke patients,IEEE,Conferences,"Rehabilitation robots are gradually becoming popular for stroke rehabilitation to improve motor recovery. By using a robot, the patient may perform the training more frequently on their own, but they must be motivated to do so. Therefore, this project develops a set of rehabilitation training programs with different haptic modalities on Compact Rehabilitation Robot (CR2) - a robot used to train upper and lower limbs reaching movement. The paper present the developed haptic interface, Haptic Sense with five configurable haptic modalities that include sensations of weight, wall, spring, sponge and visual amplification. A combination of several haptic modalities was implemented into virtual reality games, Water Drop - a progressive training game with up to nine levels of difficulties that requires user to move the cup to collect the water drops.",https://ieeexplore.ieee.org/document/6775492/,2014 IEEE Haptics Symposium (HAPTICS),23-26 Feb. 2014,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IECON.1993.339087,A planning architecture for intelligent robot: fuzzy memory-based reasoning for real-time planning/control,IEEE,Conferences,"Our research's main objective is to design an architecture prototype to govern an intelligent robot which can work quickly and efficiently in a vague dynamical environment, typically where various robots and human cooperate each other to accomplish a common global goal. To realize such kind of system, a new planning and control architecture with abilities of real-time control and easy implementation of control knowledge is required. The architecture proposed here is based on the idea of memory-based reasoning systems and behavior-based control systems. Then, to confirm its performance, a simple simulation example of two mobile robots that cooperate to capture a target is showed.<>",https://ieeexplore.ieee.org/document/339087/,Proceedings of IECON '93 - 19th Annual Conference of IEEE Industrial Electronics,15-19 Nov. 1993,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICAMechS.2013.6681701,A real-time walking robot control system based on Linux RTAI,IEEE,Conferences,"This study developed a real-time control system for a walking robot. The control system for the walking robot should have a real-time operating system, a small size, and extendable IO cards. The PC104 computer is selected due to its small size, reliability and availability of many IO cards. The Linux RTAI is selected because it is an open-source, efficient and hard real-time operating system. The Turbo PMAC PC104 card is used to control motor drivers because its small size, multi-axis synchronization and powerful control functions. Compared with CAN or EtherCAT bus control scheme, this system can easily support motor drivers from different companies. The software is reusable to different robots due to its independence on communication bus protocols and motor drivers. The motor control experiments are provided to show the satisfactory real-time control performance.",https://ieeexplore.ieee.org/document/6681701/,Proceedings of the 2013 International Conference on Advanced Mechatronic Systems,25-27 Sept. 2013,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS.2003.1250667,A robot that reinforcement-learns to identify and memorize important previous observations,IEEE,Conferences,"It is difficult to apply traditional reinforcement learning algorithms to robots, due to problems with large and continuous domains, partial observability, and limited numbers of learning experiences. This paper deals with these problems by combining: (1) reinforcement learning with memory, implemented using an LSTM recurrent neural network whose inputs are discrete events extracted from raw inputs; (2) online exploration and offline policy learning. An experiment with a real robot demonstrates the methodology's feasibility.",https://ieeexplore.ieee.org/document/1250667/,Proceedings 2003 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2003) (Cat. No.03CH37453),27-31 Oct. 2003,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCITECHN.2016.7860248,A support vector machine approach for real time vision based human robot interaction,IEEE,Conferences,"Today humanoid robots are being exhibited to redact various task as a personal assistant of a human. To be an assistant, a robot needs to interact with human as a human. For this reason robot needs to understand the human gender, facial expression, facial gesture in real time. Ribo &#x2014; A humanoid robot build in RoboSUST lab which has the ability to communicate in Bangla with the people speaking in Bengali. In this article the authors show the implementation of theoretical knowledge of the recognition of real time facial expression, detection of human gender and yes / no from facial gesture in Ribo. Real time facial expression and gender detection can be performed using Support Vector Machine (SVM). A prepared dataset containing the facial landmarks leveled as five different expression: sad, angry, smile, surprise and normal, is given to SVM to construct a classifier. For the prediction of any expression, facial images are taken in real time and provided the facial landmarks data to SVM. Local Binary Pattern(LBP) algorithm is used for extracting features from face images. These features leveled as male and female are responsible to build the classifier. The face gesture for detecting &#x2018;yes/no&#x2019; is performed by tracking the movement of face in a certain time. After those implementations the principal results will make a framework that will be used in Ribo to recognize human facial expression, facial gesture movement and detect human gender.",https://ieeexplore.ieee.org/document/7860248/,2016 19th International Conference on Computer and Information Technology (ICCIT),18-20 Dec. 2016,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ITCA49981.2019.00057,A two-level stacking model for detecting abnormal users in Wechat activities,IEEE,Conferences,"Machine learning algorithms are widely employed in plenty of classification or regression problems. While in real business world, it is confronted with huge and disorder data pattern. To recognize different kinds of users on the internet accurately and fast becomes a challenge. In a Wechat online bargain activity, the staff found that some strange users are highly like robots or malicious users. Thus we tried a two-level stacking model to detect them. This design got a good result of 0.98 accuracy after the training phase and an accuracy of 0.90 in a new term of the testing set. Moreover, this model is adaptable to linear and nonlinear datasets because of its diverse stacking of first-level classifiers. Therefore, this paper indicates a potential of the stacking classification model in big data times.",https://ieeexplore.ieee.org/document/9092496/,2019 International Conference on Information Technology and Computer Application (ITCA),20-22 Dec. 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/FUZZ48607.2020.9177654,AI-FML Agent for Robotic Game of Go and AIoT Real-World Co-Learning Applications,IEEE,Conferences,"In this paper, we propose an AI-FML agent for robotic game of Go and AIoT real-world co-learning applications. The fuzzy machine learning mechanisms are adopted in the proposed model, including fuzzy markup language (FML)-based genetic learning (GFML), eXtreme Gradient Boost (XGBoost), and a seven-layered deep fuzzy neural network (DFNN) with backpropagation learning, to predict the win rate of the game of Go as Black or White. This paper uses Google AlphaGo Master sixty games as the dataset to evaluate the performance of the fuzzy machine learning, and the desired output dataset were predicted by Facebook AI Research (FAIR) ELF Open Go AI bot. In addition, we use IEEE 1855 standard for FML to describe the knowledge base and rule base of the Open Go Darkforest (OGD) prediction platform in order to infer the win rate of the game. Next, the proposed AI-FML agent publishes the inferred result to communicate with the robot Kebbi Air based on MQTT protocol to achieve the goal of human and smart machine co-learning. From Sept. 2019 to Jan. 2020, we introduced the AI-FML agent into the teaching and learning fields in Taiwan. The experimental results show the robots and students can co-learn AI tools and FML applications effectively. In addition, XGBoost outperforms the other machine learning methods but DFNN has the most obvious progress after learning. In the future, we hope to deploy the AI-FML agent to more available robot and human co-learning platforms through the established AI-FML International Academy in the world.",https://ieeexplore.ieee.org/document/9177654/,2020 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE),19-24 July 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SIBGRAPI.2001.963057,ARENA and WOXBOT: first steps towards virtual world simulations,IEEE,Conferences,"This paper reports new results of a project to build virtual worlds aimed at the graphic simulation of an arena where small mobile robots can perform requested tasks while behaving according to their own motivation and reasoning. Each robot is an intelligent agent that perceives the virtual environment via a simulated vision system and reacts to translating or rotating its body by driving its own wheels. The conception and specification of the robots and the environment are developed to create an open distributed object architecture that could serve as a testbed freely available and ready to use for testing theories in some computational areas such as evolutionary computation, artificial life, pattern recognition, artificial intelligence, cognitive neurosciences and distributed object architectures.",https://ieeexplore.ieee.org/document/963057/,Proceedings XIV Brazilian Symposium on Computer Graphics and Image Processing,15-18 Oct. 2001,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CAC53003.2021.9727698,AVR-AKG: using virtual reality for domain knowledge generation from first-person demonstration,IEEE,Conferences,"Manipulation tasks in specific environment, such as truss assembly in the space and ""Hape"" building blocks assembly in the toy room, unfold intentionally under corresponding domains, which contains a lot of domain knowledge .Being able to process contextual knowledge in these activities under corresponding domains over time can help us understand manipulation intentions .However, most researchers use methods of machine learning to make robots understand manipulation scenarios , which is a black model consuming a lot of computing resources. Moreover, the demonstration of assembly tasks in the real world is time-consuming and labor-intensive, and it is not suitable to initialize the assembly scene. To overcome these limitations, we introduced AVR-AKG: an implementing framework for domain knowledge generation that can generate dynamic knowledge graph in real time from assembly demonstrations. A combination of an Assembly Virtual Reality subsystem and an Assembly Knowledge Generation subsystem, in correspondence with assembly tasks and knowledge generation, is used to represent robot manipulation knowledge with Entity-Relation- Entity (E-R-E) and Entity-Attribute-Value (E-A-V) tuples. Using this framework, we propose a case study in which the demonstrator completes an assembly task using ""Hape"" building blocks, generating knowledge related to the operational context with knowledge graphs during first-person demonstration.",https://ieeexplore.ieee.org/document/9727698/,2021 China Automation Congress (CAC),22-24 Oct. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IEEECONF49454.2021.9382693,Accelerated Sim-to-Real Deep Reinforcement Learning: Learning Collision Avoidance from Human Player,IEEE,Conferences,"This paper presents a sensor-level mapless collision avoidance algorithm for use in mobile robots that map raw sensor data to linear and angular velocities and navigate in an unknown environment without a map. An efficient training strategy is proposed to allow a robot to learn from both human experience data and self-exploratory data. A game format simulation framework is designed to allow the human player to tele-operate the mobile robot to a goal and human action is also scored using the reward function. Both human player data and self-playing data are sampled using prioritized experience replay algorithm. The proposed algorithm and training strategy have been evaluated in two different experimental configurations: Environment 1, a simulated cluttered environment, and Environment 2, a simulated corridor environment, to investigate the performance. It was demonstrated that the proposed method achieved the same level of reward using only 16&#x0025; of the training steps required by the standard Deep Deterministic Policy Gradient (DDPG) method in Environment 1 and 20&#x0025; of that in Environment 2. In the evaluation of 20 random missions, the proposed method achieved no collision in less than 2 h and 2.5 h of training time in the two Gazebo environments respectively. The method also generated smoother trajectories than DDPG. The proposed method has also been implemented on a real robot in the real-world environment for performance evaluation. We can confirm that the trained model with the simulation software can be directly applied into the real-world scenario without further fine-tuning, further demonstrating its higher robustness than DDPG. The video and code are available: https://youtu.be/BmwxevgsdGc https://github.com/hanlinniu/turtlebot3_ddpg_collision_avoidance",https://ieeexplore.ieee.org/document/9382693/,2021 IEEE/SICE International Symposium on System Integration (SII),11-14 Jan. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/FPA.1994.636089,"Active perception, navigation, homing, and grasping: an autonomous perspective",IEEE,Conferences,"Perception is needed for action, not for the pure sake of the construction of abstract representations, although it does not exclude the role of internal representations for mediating complex behaviours. We think that, for the purpose of building autonomous robots, active perception requires specific recipes for three related aspects: the design of the physical sensory system, the modality and type of information extracted, and the structure and functioning of the control system. We outline a set of solutions for these three aspects and describe their implementation on a real mobile robot through a set of three different experiments using a combination of neural networks and genetic algorithms. The results show that active perception is a useful feature that is exploited by autonomous agents. The experiments shout that the combination of genetic algorithms and neural networks is a feasible and fruitful technique for the development of active perception in autonomous agents.",https://ieeexplore.ieee.org/document/636089/,Proceedings of PerAc '94. From Perception to Action,7-9 Sept. 1994,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA46639.2022.9812025,Adaptive Informative Path Planning Using Deep Reinforcement Learning for UAV-based Active Sensing,IEEE,Conferences,"Aerial robots are increasingly being utilized for environmental monitoring and exploration. However, a key challenge is efficiently planning paths to maximize the information value of acquired data as an initially unknown environment is explored. To address this, we propose a new approach for informative path planning based on deep reinforcement learning (RL). Combining recent advances in RL and robotic applications, our method combines tree search with an offline-learned neural network predicting informative sensing actions. We introduce several components making our approach applicable for robotic tasks with high-dimensional state and large action spaces. By deploying the trained network during a mission, our method enables sample-efficient online replanning on platforms with limited computational resources. Simulations show that our approach performs on par with existing methods while reducing runtime by 8-10&#x00D7;. We validate its performance using real-world surface temperature data.",https://ieeexplore.ieee.org/document/9812025/,2022 International Conference on Robotics and Automation (ICRA),23-27 May 2022,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICDE53745.2022.00314,Adaptive Task Planning for Large-Scale Robotized Warehouses,IEEE,Conferences,"Robotized warehouses are deployed to automatically distribute millions of items brought by the massive logistic orders from e-commerce. A key to automated item distribution is to plan paths for robots, also known as task planning, where each task is to deliver racks with items to pickers for processing and then return the rack back. Prior solutions are unfit for large-scale robotized warehouses due to the inflexibility to time-varying item arrivals and the low efficiency for high throughput. In this paper, we propose a new task planning problem called TPRW, which aims to minimize the end-to-end makespan that incorporates the entire item distribution pipeline, known as a fulfilment cycle. Direct extensions from state-of-the-art path finding methods are ineffective to solve the TPRW problem because they fail to adapt to the bottleneck variations of fulfillment cycles. In response, we propose Efficient Adaptive Task Planning, a framework for large-scale robotized warehouses with time-varying item arrivals. It adaptively selects racks to fulfill at each timestamp via rein-forcement learning, accounting for the time-varying bottleneck of the fulfillment cycles. Then it finds paths for robots to transport the selected racks. The framework adopts a series of efficient optimizations on both time and memory to handle large-scale item throughput. Evaluations on both synthesized and real data show an improvement of 37.1&#x0025; in effectiveness and 75.5&#x0025; in efficiency over the state-of-the-arts.",https://ieeexplore.ieee.org/document/9835632/,2022 IEEE 38th International Conference on Data Engineering (ICDE),9-12 May 2022,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS.2013.6697133,Adaptive collision checking for continuous robot motions within motion constraints,IEEE,Conferences,"This paper presents an adaptive algorithm for checking collisions over any continuous robot motion set when tasks or constraints are given. As robots have begun to operate in human environments, which are unstructured and dynamically changing, the need for on-line robot planning and control strategies has increased. In implementing an on-line system, a fast and reliable collision checking method for continuous paths is a critical element. However, since external objects move unexpectedly, collision checking along the continuous path of a robot's motion suffers from increased uncertainty. Furthermore, computing the desired motion path or trajectory of a complex robotic task is very complex and slow. Therefore, we have developed a new collision checking strategy that can be applied to many types of motions that satisfy many given constraints. Our algorithm defines the applicable robot motions in a constraint-based manner, which is suitable for the multiple-task motion of a complex robot. This method can check the collision for the entire motion by finding the worst case with a small amount of computation, so that we can use the method for on-line applications. Moreover, our algorithm has a feature of adaptive resolution, which provides advantages in dynamically changing environments. The proposed method has been tested on high d.o.f. robots and the experimental results show that the method is suitable for on-line applications of multiple-tasks.",https://ieeexplore.ieee.org/document/6697133/,2013 IEEE/RSJ International Conference on Intelligent Robots and Systems,3-7 Nov. 2013,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS.2010.5650226,Adaptive motion control with visual feedback for a humanoid robot,IEEE,Conferences,"The performance of a soccer robot is highly dependent on its motion ability. The kicking motion is one of the most important motions in a soccer game. However, automatic, full body motion generation for humanoid robots presents a formidable computational challenge. At the current state the most common approaches of implementing this motion are based on key frame technique. Such solutions are inflexible, i.e., in order to adjust the aimed direction of the kick the robot has to walk around the ball. The adjustment costs a lot of time especially if some precise adjustments have to be done, e.g., for a penalty kick. In this paper we present an approach for adaptive control of the motions. We implemented our approach in order to solve the task of kicking the ball on a humanoid robot Nao. The approach was tested both in simulation and on a real robot.",https://ieeexplore.ieee.org/document/5650226/,2010 IEEE/RSJ International Conference on Intelligent Robots and Systems,18-22 Oct. 2010,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/RAAD.2010.5524575,Adaptive sliding mode controller design for mobile robot fault tolerant control. introducing ARTEMIC.,IEEE,Conferences,"Current real-time applications should timely deliver synchronized data-sets, minimize latency in their response and meet their performance specifications in the presence of disturbances and faults. The adaptive features of the designed controller are present at the lower control level using specific artificial intelligence techniques. Fuzzy inference system design is the fundamental element to generate an adaptive nonlinear controller for the robot operation in the presence of disturbances and modeling inaccuracies. This paper introduces an adaptive real-time distributed control application with fault tolerance capabilities for differential wheeled mobile robots, named ARTEMIC. Specific design, development and implementation details will be provided in this paper.",https://ieeexplore.ieee.org/document/5524575/,19th International Workshop on Robotics in Alpe-Adria-Danube Region (RAAD 2010),24-26 June 2010,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS.2017.8206245,Adversarially Robust Policy Learning: Active construction of physically-plausible perturbations,IEEE,Conferences,"Policy search methods in reinforcement learning have demonstrated success in scaling up to larger problems beyond toy examples. However, deploying these methods on real robots remains challenging due to the large sample complexity required during learning and their vulnerability to malicious intervention. We introduce Adversarially Robust Policy Learning (ARPL), an algorithm that leverages active computation of physically-plausible adversarial examples during training to enable robust policy learning in the source domain and robust performance under both random and adversarial input perturbations. We evaluate ARPL on four continuous control tasks and show superior resilience to changes in physical environment dynamics parameters and environment state as compared to state-of-the-art robust policy learning methods. Code, data, and additional experimental results are available at: stanfordvl.github.io/ARPL.",https://ieeexplore.ieee.org/document/8206245/,2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),24-28 Sept. 2017,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AIM43001.2020.9159003,Amphibious Robot’s Trajectory Tracking with DNN-Based Nonlinear Model Predictive Control,IEEE,Conferences,"Amphibious robots are being deployed in field environments where they are required to handle environmental disturbances and systemic uncertainties. Efficient and accurate control strategies can guarantee high performance of the robot's trajectory tracking tasks. In this paper, we first contribute a well design deep neural network (DNN) as a precise black-box kinematic model of the amphibious robot. Then, we design a DNN based nonlinear model predictive controller (DNN-NMPC) which obtains the robot's real-time moving command by iterative optimization. To verify the proposed method's performance in amphibious robots' trajectory tracking tasks, a Gazebo based simulation platform has been built and several comparative simulations have been carried out. The simulation results indicate the proposed controller is superior to the basic controller in the robot's tracking efficiency and accuracy.",https://ieeexplore.ieee.org/document/9159003/,2020 IEEE/ASME International Conference on Advanced Intelligent Mechatronics (AIM),6-9 July 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IPEC49694.2020.9115180,An Autonomous Service Mobile Robot for Indoor Environments,IEEE,Conferences,"This paper proposes an autonomous indoor service robot system framework integrated with multi-sensors to implement the service robotic functions. Both the voice recognition and real-time object detection capabilities are incorporated in this system. Firstly, a novel mapping strategy is proposed to build a hybrid grid-topological-semantic map which is used for voice-control-based navigation for robots. Secondly, in order to reduce the power consumption of mobile robot object detection, a low-power parallel acceleration method using neural network accelerator sticks is proposed. And the power consumption is reduced by 15 times with approximately the same frame rate compared with laptop GPU GTX 960M. Moreover, the YOLO model used in this system is about 10 times faster than two-stage methods. In addition, the experiment result demonstrates that the proposed mapping strategy can be well combined with the voice recognition system to achieve voice-controlled navigation. The structure of the robot system we designed in this paper can be widely applied to indoor service robots.",https://ieeexplore.ieee.org/document/9115180/,"2020 Asia-Pacific Conference on Image Processing, Electronics and Computers (IPEC)",14-16 April 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/COINS51742.2021.9524186,An Edge AI based Robot System for Search and Rescue Applications,IEEE,Conferences,"In this work, we propose an edge AI based robot system that contains drones and multi-legged robots for search and rescue applications. To accurately search for survivors in real-time, we integrate Tiny-YOLO into the drone design. Instead of adopting a microprocessor usually used in a robot, the FPGA device is adopted as the main hardware computing architecture of the multi-legged robot. A resource-efficient quantized neural network is implemented as a hardware module and integrated into the multi-legged robot for real-time detection. When a survivor is detected from robots, the corresponding information about GPS and the triangulation localization is thus delivered to the edge server. Then, rescuers can receive the notification message from the edge server by using their mobile devices. For survivor detection, experiments show the drone and the multi-legged robot can achieve 2.164 fps and 2.404 fps, respectively.",https://ieeexplore.ieee.org/document/9524186/,2021 IEEE International Conference on Omni-Layer Intelligent Systems (COINS),23-25 Aug. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/WCICA.2006.1713761,An Embedded Platform for Intelligent Mobile Robot,IEEE,Conferences,"To overcome the limitations of the special architectures adopted by traditional industrial robots, an embedded intelligent robot platform based on Windows CE.NET is established by customizing the operating system. On this intelligent robot platform, all major necessary sensors are included, and abundant control interfaces and driver modules are available, such as movement control interface, USB camera driver, laser driver, etc. Besides, various testing software and application modules for intelligent robot are designed, such as multi-sensor data fusion, path planning, speech recognition, wireless network communication and graphic human-robot interface. The platform is modularized, extensible, transplantable and customizable. Compared to the previous robot platform, it also has many advantages such as more compact hardware, lower-power consumption, better real-time performance and higher reliability",https://ieeexplore.ieee.org/document/1713761/,2006 6th World Congress on Intelligent Control and Automation,21-23 June 2006,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IJCNN.2018.8489157,An Embedded Tracking System with Neural Network Accelerator,IEEE,Conferences,"With robots and unmanned aerial vehicles (UAVs) being more and more employed in real-life scenarios for monitoring and surveillance, there is a increasing demand for deploying various video processing applications in mobile systems. However, with limited on-board computational resources and power consumption, the application in this domain requires that the tracking platforms equipped should have outstanding computing power to handle the tasks in real-time with high-accuracy, while at the same time, fit the highly constrained environment of small size, light weight, and low power consumption (SWaP) for the purpose of long-term surveillance. In this paper, we proposed a new autonomous object tracking system based on an embedded platform, leveraging the emerging neural network hardware which is capable of massive parallel pattern recognition processing and demands only a low level power consumption. Further, a prototype of the tracking system that combines a low-power neural network chip, CogniMem, and an embedded development board, BeagleBone, is developed. Our experimental results show that the power consumption for the entire system is only about 2. 25W, which signifies a promising future of applying ultra-low-power neuromorphic hardware as a accelerator in recognition tasks.",https://ieeexplore.ieee.org/document/8489157/,2018 International Joint Conference on Neural Networks (IJCNN),8-13 July 2018,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/RCAR.2018.8621725,An Image Recognition Approach for Coal and Gangue Used in Pick-Up Robot,IEEE,Conferences,"Picking gangue from raw coal is a crucial step of coal production. Due to the potential for replacing manual workers, the study of pick-up robot is attracting much interest. Pick-up robots usually work in fixed working areas where the types of coals and gangues are unitary. Based on this fact, this paper proposes a simple, fast, and easily implemented approach for coal and gangue classification which is LS-SVM (Least Square Support Vector Machine) based using gray scale and texture as features. We firstly sampled the image dataset from Han City, Shaanxi province and Jizhong, Hebei province which are two main mining areas in China. The data of Han City consists of the images of lean coal and shale, and the data of Jizhong is coking coal and sandstone. By analyzing the gray scale and the texture of the sampled data, we discover that coal and gangue vary in the parameters including the mean and peak of gray scale, contrast ratio, and entropy. Therefore, these four parameters are chosen as features. We utilize LS-SVM as the machine learning model, and the model is trained with three groups of parameters separately. The first are the mean and peak of gray scale, the second are the contrast ratio and entropy which represents texture features, and the third are the peak of gray scale and the contrast ratio which integrates gray scale and texture features. After evaluation by using our sampled dataset, the model trained by the third group outperforms others. The classification results were 98.7% correct of coal and 96.6% correct of gangue for the data of Han city, and 98.6% correct of coal and 96.6% correct of gangue for the data of Jizhong.",https://ieeexplore.ieee.org/document/8621725/,2018 IEEE International Conference on Real-time Computing and Robotics (RCAR),1-5 Aug. 2018,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/JCSSE.2018.8457382,An Intelligent Locomotion Control Architecture for Hexapod Robot,IEEE,Conferences,"This paper presents an intelligent locomotion control architecture for a hexapod robot. The proposed architecture provides a hexapod robot abilities to perceive different terrains and adjust gait accordingly by utilizing CPG-network, torque sensing, and radial basis function neural network. The architecture is suitable for implementing in an embedded system for on-board small hexapod robots. The effectiveness of the proposed intelligent control architecture is demonstrated through real robot experiments.",https://ieeexplore.ieee.org/document/8457382/,2018 15th International Joint Conference on Computer Science and Software Engineering (JCSSE),11-13 July 2018,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.23919/OCEANS44145.2021.9705666,An Underwater Simulation Server Oriented to Cooperative Robotic Interventions: The Educational Approach,IEEE,Conferences,"Experiments that require the use of Supervised Autonomous Underwater Vehicles for Intervention (I-AUV) are not easy to be performed, specially when deployed in the sea or in scenarios where the robot might face lack of space and communication (e.g. interior of pipes). Also, there are some applications where the robots need to cooperate in a closed manner, for example when transporting and assembling big pipes. In fact, these two scenarios are being studied in the context of the H2020-ElPeacetolero and TWINBOT (TWIN roBOTs for cooperative underwater intervention mission) [1] projects, being necessary to have a simulation tool that offer more realistic rendering and being compatible with the real robot Application Programming Interface.This paper presents a new underwater simulation server, implemented using video game and robotic techniques, which operates by enabling the researchers control the robots in the scene in a simple and efficient manner, while using HTTP commands that have demonstrated a huge facility in the project integration process. Moreover, this simplicity has allowed the application of the simulation server in the educational context. The use of this tool has resulted to be very adequate for the students, who have used it to learn computer science and artificial intelligence algorithms to solve problems like a cooperative transportation robotic task. As case study, four educational experiments are presented, performed by master’s degree students, focusing on user interfaces, image compression for underwater channels, autonomous cooperative grasping and robot arm movement in a AUV.",https://ieeexplore.ieee.org/document/9705666/,OCEANS 2021: San Diego – Porto,20-23 Sept. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICIEV.2012.6317522,An adaptive Neuro-Fuzzy control approach for motion control of a robot arm,IEEE,Conferences,"This paper proposes an adaptive Neuro-Fuzzy control approach for controlling the link variables of a 4 degree-of-freedom Selective Compliant Assembly Robot Arm (SCARA) type robot arm / manipulator. In the real world environment, the mathematical models of many robots are often not accurate, due to the presence of continuous disturbances that effect their dynamic equations, in addition to errors in parameter knowledge. Consequently, method that rely less on precise mathematical models are often preferred. One such Adaptive Machine Learning Technique is proposed to be applied here, for motion control of the robot arm. The controller uses an inverse learning Adaptive Neuro-Fuzzy Inference System (ANFIS) model only to train itself from certain given robot trajectories. Ideally, these trajectories should be obtained by directly measuring the robot arm responses for given inputs to capture the actual dynamics in the presence of all uncertainties. However, for algorithm validation, trajectories generated through simulations based on mathematical models assumed to be reasonably accurate, can also be used for the training purpose. This approach is used for design and implementation of an ANFIS controller which is shown to act work satisfactorily. Further possible developments of this method are also outlined.",https://ieeexplore.ieee.org/document/6317522/,"2012 International Conference on Informatics, Electronics & Vision (ICIEV)",18-19 May 2012,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS.1991.174539,An approach to on-line obstacle avoidance for robot arms,IEEE,Conferences,"Presents an approach to on-line obstacle avoidance for fixed-base robot manipulators. It guarantees a collision-free path for the robot during real-time operations. This approach is based on analytic geometry and is suitable for continuous path control. Considering the potential collision with obstacles, the next trajectory point to move to is corrected. This strategy is direct formulated in the operational space in which the tasks are described and applicable for two-dimensional as well as for three-dimensional space. Because this algorithm requires no access to joint control, it can be also used for commercial robots given the desired path. One can assign it for various robots, here the implementation for the PUMA 560 is presented as an example.<>",https://ieeexplore.ieee.org/document/174539/,Proceedings IROS '91:IEEE/RSJ International Workshop on Intelligent Robots and Systems '91,3-5 Nov. 1991,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/FWC.2017.8368522,An auction based smart service robot implemented on a Fog Computing node,IEEE,Conferences,"Adopting AR/VR technology on smart retail services is gaining more momentum with the progress in indoor map scanning technology and the research on AI deep learning algorithms. In this paper we propose the use of a Fog computing node to generate an AR/VR view of the real store on a web page. The customers can then use the service robot to view the merchandise in the real store via the web and make purchases. Since the service robot is a precious resource on the AR/VR business model, we develop an auction method to optimize the customer satisfaction and the owner satisfaction in terms of customer waiting time and the average number of transactions that are assisted by the service robot respectively. We demonstrate that the auction method is a critical part in the AR/VR smart business services when the number of service robots is much less than the number of active customers from the web and that it performs better than the standard preemptive method.",https://ieeexplore.ieee.org/document/8368522/,2017 IEEE Fog World Congress (FWC),30 Oct.-1 Nov. 2017,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS.2007.4399219,An extended policy gradient algorithm for robot task learning,IEEE,Conferences,"In real-world robotic applications, many factors, both at low-level (e.g., vision and motion control parameters) and at high-level (e.g., the behaviors) determine the quality of the robot performance. Thus, for many tasks, robots require fine tuning of the parameters, in the implementation of behaviors and basic control actions, as well as in strategic decisional processes. In recent years, machine learning techniques have been used to find optimal parameter sets for different behaviors. However, a drawback of learning techniques is time consumption: in practical applications, methods designed for physical robots must be effective with small amounts of data. In this paper, we present a method for concurrent learning of best strategy and optimal parameters, by extending the policy gradient reinforcement learning algorithm. The results of our experimental work in a simulated environment and on a real robot show a very high convergence rate.",https://ieeexplore.ieee.org/document/4399219/,2007 IEEE/RSJ International Conference on Intelligent Robots and Systems,29 Oct.-2 Nov. 2007,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ITRE.2004.1393663,An integrated programming environment for undergraduate computing courses,IEEE,Conferences,"The Computing Division of the Business School at University College Worcester provides computing and information technology education to a range of undergraduate students. Topics include various approaches to programming, artificial intelligence, operating systems and digital technologies. Each of these has its own potentially conflicting requirements for a pedagogically sound programming environment. This paper describes an endeavor to develop a common programming paradigm across all topics. This involves the combined use of autonomous robots and Java simulations.",https://ieeexplore.ieee.org/document/1393663/,ITRE 2004. 2nd International Conference Information Technology: Research and Education,28 June-1 July 2004,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISESD.2017.8253306,Analysis of artificial intelligence application using back propagation neural network and fuzzy logic controller on wall-following autonomous mobile robot,IEEE,Conferences,"This paper presents a comparison of two methods of artificial intelligence which applied in Wallfollowing Autonomous Mobile Robot; both of them are Neural Network Backpropagationand Fuzzy Logic. The robot has three input variables and two output variables. The inputs are distance between the robot and the wall which is sensed by HC-SR04 ultrasonic sensors. The output variables are the speed of the two wheels which is driving by 12 Volt DC motor. In this case mobile robot is designed to avoid the collision with any obstacles like wall or other mobile robots. In this implementation mobile robot is designed with a numbers of ultrasonic sensors and placed on certain position like center front, left front and left back. The sensor will send the data in real time. After being processed, the input produces output in form of speed value governing motor rotation mounted on both wheels of the robot to find the optimum point. In this comparison, both methods Backpropagation Neural Network and Fuzzy Logic are treated the same. Wallfollowing Autonomous Mobile Robot is using Atmega2560 microcontroller. The logic is uploaded to the microcontroller. The result of the comparison of these two methods when applied in Wall-following Autonomous Mobile Robot is the movement of the robot using Neural NetworkBackpropagation is faster than using Fuzzy Logic Controller.",https://ieeexplore.ieee.org/document/8253306/,2017 International Symposium on Electronics and Smart Devices (ISESD),17-19 Oct. 2017,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICC.2018.8422231,Anticipatory Mobility Management by Big Data Analytics for Ultra-Low Latency Mobile Networking,IEEE,Conferences,"Massive deployment of autonomous vehicles, un- manned aerial vehicles, and robots, brings in a new technology challenge to establish ultra-low end-to-end latency mobile networking to enable holistic computing mechanisms. With the aid of open-loop wireless communication and proactive network association in vehicle-centric heterogeneous network architecture, anticipatory mobility management relying on inference and learning from big vehicular data plays a key role to facilitate such a new technological paradigm. Anticipatory mobility management aims to predict APs to be connected in the next time instant and in a real-time manner, such that ultra-low latency downlink open-loop communication can be realized with proactive network association. In this paper, we successfully respond this technology challenge using big data analytics with location-based learning and inference tech- niques, to achieve satisfactory performance of predicting APs. Real vehicular movement data have been used to verify that the proposed prediction methods are effective for the purpose of anticipatory mobility management and thus ultra-low latency mobile networking.",https://ieeexplore.ieee.org/document/8422231/,2018 IEEE International Conference on Communications (ICC),20-24 May 2018,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CYBER53097.2021.9588269,Application of YOLO Object Detection Network In Weld Surface Defect Detection,IEEE,Conferences,"As industrial production becomes more modern and intelligent today, the inspection of product quality of the workshop is becoming more and more accustomed to replacing the old manual visual inspection methods with automated inspection systems. In the welding field, automated welding robots are not only used in traditional large-scale automobile assembly lines. In more general welding work, welding robots also plays an important role. The inspection of the welding quality of the welding robot is mainly to detect the four main types of weld defects. Compared to traditional defect classification based on support vector machines and defect detection based on template matching, this paper uses a welding surface defect detection system designed based on deep learning methods. By working with workshop welding experts, a large-scale image of nearly 5000 pictures is built. Large-scale weld defect datasets, while using the real-time and accuracy of the YOLO series of deep learning object detection frameworks, the weld defects detection model reaches 75.5% mean average precision(mAP) in constructed weld defect data set. In addition, the construction cost of the detection model and the deployment time of the detection system are greatly reduced. During the field test of the system in the workshop, among a batch of welding workpieces provided by the factory, the detection accuracy of weld defects reached 71%, which initially met the requirements of the workshop for an automated defect detection system.",https://ieeexplore.ieee.org/document/9588269/,"2021 IEEE 11th Annual International Conference on CYBER Technology in Automation, Control, and Intelligent Systems (CYBER)",27-31 July 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICAICTA.2016.7803114,Applications of artificial intelligence control for Parallel Discrete-Manipulators,IEEE,Conferences,"Parallel Discrete-Manipulators are a special kind of force regulated manipulators which can undergo continuous motions despite being commanded through a large but finite number of states only. Real-time control of such systems requires very fast and efficient methods for solving their inverse static analysis. In this paper, artificial intelligence techniques (AI) are investigated for addressing the inverse static analysis of a planar parallel array featuring ten three-state force actuators and two applications using 3D Massively Parallel Robots (MPRs) with one and two layers. In particular, the research method used simulation software and hardware testing with the case of parallel manipulator with two level discrete pneumatic actuators. Simulations with typical desired displacement inputs are presented and a good performance of the results compared to AI is obtained. The comparison showed that the parallel manipulator has the Root Mean Squared Error (RMSE) has less than 10% and can be used for controlling the ternary states of discrete manipulators via AI.",https://ieeexplore.ieee.org/document/7803114/,"2016 International Conference On Advanced Informatics: Concepts, Theory And Application (ICAICTA)",16-19 Aug. 2016,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IEMTRONICS55184.2022.9795727,Artificial Intelligence Applied in Human Medicine with the Implementation of Prostheses,IEEE,Conferences,"The use of artificial intelligence (AI) in medicine is already a reality. Everywhere there is talk of the advantages that AI can mean for the future in our daily lives, as well as its possible applications. The future of ""standard"" medical practice could appear here ahead of schedule, where a patient could go to a computer before seeing a doctor. Through advances in AI, it becomes more possible for the days of misdiagnosis and treatment of the symptoms of the disease, rather than its root cause, to be left behind. Think about how many years of blood pressure measurements you have or how much storage you would need to remove so that you can fit a complete 3D image of an organ on your laptop. The idea of artificial intelligence in medicine may make you think of robots roaming the halls of a hospital in the distant future, but AI is already here.",https://ieeexplore.ieee.org/document/9795727/,"2022 IEEE International IOT, Electronics and Mechatronics Conference (IEMTRONICS)",1-4 June 2022,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA.2018.8462967,Auctioning over Probabilistic Options for Temporal Logic-Based Multi-Robot Cooperation Under Uncertainty,IEEE,Conferences,"Coordinating a team of robots to fulfill a common task is still a demanding problem. This is even more the case when considering uncertainty in the environment, as well as temporal dependencies within the task specification. A multi-robot cooperation from a single goal specification requires mechanisms for decomposing the goal as well as an efficient planning for the team. However, planning action sequences offline is insufficient in real world applications. Rather, due to uncertainties, the robots also need to closely coordinate during execution and adjust their policies when additional observations are made. The framework presented in this paper enables the robot team to cooperatively fulfill tasks given as temporal logic specifications while explicitly considering uncertainty and incorporating observations during execution. We present the effectiveness of our ROS implementation of this approach in a case study scenario.",https://ieeexplore.ieee.org/document/8462967/,2018 IEEE International Conference on Robotics and Automation (ICRA),21-25 May 2018,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AIVR46125.2019.00061,Augmented Reality for Human-Robot Cooperation in Aircraft Assembly,IEEE,Conferences,"Augmented Reality (AR) is often discussed as one of the enabling technologies in Industrie 4.0. In this paper, we describe a practical application, where Augmented Reality glasses are used not only for assembly assistance, but also as a means of communication to enable the orchestration of a hybrid team consisting of a human worker and two mobile robotic systems. The task of the hybrid team is to rivet so-called stringers onto an aircraft hull. While the two robots do the physically demanding, unergonomic and possibly hazardous tasks (squeezing and sealing rivets), the human takes over those responsibilities that need experience, multi-sensory sensitiveness and specialist knowledge. We describe the working scenario, the overall architecture and give design and implementation details on the AR application.",https://ieeexplore.ieee.org/document/8942239/,2019 IEEE International Conference on Artificial Intelligence and Virtual Reality (AIVR),9-11 Dec. 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.23919/MIPRO52101.2021.9597142,Automated Robot Control for a Game of Chess in Unity Game Engine through Artificial Intelligence,IEEE,Conferences,"The topic of this paper is to study the possibility of using Unity game development engine for robot control. The aim of the work is to create a virtual environment in which the game of chess is simulated, through a duel of two robots controlled by artificial intelligence. As part of the work, real robot models were implemented in the Unity game engine. The simulated robots were ABB's IRB-120 arms with two joints. The movement of the robot is fully simulated within the physics simulation in the Unity system. The Forward and Backward Reaching Inverse Kinematics (FABRIK) algorithm was used for the inverse kinematics algorithm. For calculating the next move, external artificial intelligence library Stockfish was used and integrated with the Unity game engine. The final application has automated moves between the robots, has the option of a simple change of the viewpoint through camera movement, and is intended to be used in future work for the control of a real robot.",https://ieeexplore.ieee.org/document/9597142/,"2021 44th International Convention on Information, Communication and Electronic Technology (MIPRO)",27 Sept.-1 Oct. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/HUMANOIDS.2018.8624922,Autonomous Dual-Arm Manipulation of Familiar Objects,IEEE,Conferences,"Autonomous dual-arm manipulation is an essential skill to deploy robots in unstructured scenarios. However, this is a challenging undertaking, particularly in terms of perception and planning. Unstructured scenarios are full of objects with different shapes and appearances that have to be grasped in a very specific manner so they can be functionally used. In this paper we present an integrated approach to perform dual-arm pick tasks autonomously. Our method consists of semantic segmentation, object pose estimation, deformable model registration, grasp planning and arm trajectory optimization. The entire pipeline can be executed onboard and is suitable for on-line grasping scenarios. For this, our approach makes use of accumulated knowledge expressed as convolutional neural network models and low-dimensional latent shape spaces. For manipulating objects, we propose a stochastic trajectory optimization that includes a kinematic chain closure constraint. Evaluation in simulation and on the real robot corroborates the feasibility and applicability of the proposed methods on a task of picking up unknown watering cans and drills using both arms.",https://ieeexplore.ieee.org/document/8624922/,2018 IEEE-RAS 18th International Conference on Humanoid Robots (Humanoids),6-9 Nov. 2018,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AMS.2017.22,Autonomous Rover Navigation Using GPS Based Path Planning,IEEE,Conferences,"Nowadays, with the constant evolution of Artificial Intelligence and Machine Learning, robots are getting more perceptive than ever. For this quality they are being used in varying circumstances which humans cannot control. Rovers are special robots, capable of traversing through areas that are too difficult for humans. Even though it is a robust bot, lack of proper intelligence and automation are its basic shortcomings. As the main purpose of a rover is to traverse through areas of extreme difficulties, therefore an intelligent path generation and following system is highly required. Our research work aimed at developing an algorithm for autonomous path generation using GPS (Global Positioning System) based coordinate system and implementation of this algorithm in real life terrain, which in our case is MDRS, Utah, USA. Our prime focus was the development of a robust but easy to implement system. After developing such system, we have been able to successfully traverse our rover through that difficult terrain. It uses GPS coordinates of target points that will be fed into the rover from a control station. The rover capturing its own GPS signal generates a path between the current location and the destination location on its own. It then finds the deviation in its current course of direction and position. And eventually it uses Proportional Integral Derivative control loop feedback mechanism (PID control algorithm) for compensating the error or deviation and thus following that path and reach destination. A low cost on board computer (Raspberry Pi in our case) handles all the calculations during the process and drives the rover fulfilling its task using an microcontroller (Arduino).",https://ieeexplore.ieee.org/document/8424312/,2017 Asia Modelling Symposium (AMS),4-6 Dec. 2017,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IES53407.2021.9594013,Ball Position Transformation with Artificial Intelligence Based on Tensorflow Libraries,IEEE,Conferences,Research on wheeled soccer robots has been carried out by several researchers. This is due to the existence of national and international competitions. Previous research was to create a ball position transformation system with a modified method of neural network architecture. This research was developed by building an intelligent transformation system with the Tensorflow library. This transformation system aims to be able to directly measure the distance of objects in real terms without first changing the environmental image from an omni field to a flat plane with conventional camera calibration techniques. This process can replace manual calibration with a variety of field size changes The system can transform with mean error 0.0000026 on epoch 10000 using “conda-tensorflowneural network” libraries. It can transform the position of the ball from the omni space to the cartesian space. This system was implemented on wheeled soccer robot as keeper.,https://ieeexplore.ieee.org/document/9594013/,2021 International Electronics Symposium (IES),29-30 Sept. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISOEN.2017.7968884,Bayesian gas source localization and exploration with a multi-robot system using partial differential equation based modeling,IEEE,Conferences,"Model based approaches, such as those that use partial differential equations (PDE), lend themselves to gas distribution mapping and gas source localization. Moreover, they also permit constructing intelligent sampling strategies. However, a realistic mathematical model of gas dispersion is complex and computationally expensive to solve. This is especially the case for inverse problems, where sources are estimated based on concentration measurements. In this paper, we propose a probabilistic model based on a diffusion PDE to approximate the complex behavior of gas dispersion. This model is used (i) to identify the sources, using ideas from Sparse Bayesian Learning, and (ii) to guide a multi-agent robotic system to measurement locations, which assists the source localization. The potential of the approach is shown in experiments, where laminar gas plumes are simulated using an open-source CFD-based filament gas dispersion simulator. The exploration is carried out using multiple real robots implementing the proposed algorithm.",https://ieeexplore.ieee.org/document/7968884/,2017 ISOCS/IEEE International Symposium on Olfaction and Electronic Nose (ISOEN),28-31 May 2017,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ROMOCO.2001.973435,"Behavior learning to predict using neural networks (NN): Ttowards a fast, cooperative and adversarial robot team (RoboCup)",IEEE,Conferences,"To build a fast, cooperative and adversarial robot team (RoboCup), prediction behaviors became necessary. In the paper, a behavior learning method using neural networks (NN) is developed to enhance the behavior of GMD mobile robots. In fact, the suggested NN called NN-Prediction learns to predict successfulness of the elementary behavior ""Kick"" the ball towards the goal in order to act as consequence. The training is carried out by the supervised gradient back-propagation learning paradigm. This NN-Prediction has been specified on the Dual Dynamics Designer, to be thereafter implemented and tested on both the Dual Dynamics Simulator and GMD mobile robots, and analyzed on the Real-Time Trace Tool. NN-prediction demonstrated, during the 4/sup th/ World Championships RoboCup 2000, cooperative and adversarial behaviors especially face to situations where the successfulness of ""Kick"" is not guaranteed. Then, a discussion is given dealing with the suggested prediction behavior and how it relates to some other works.",https://ieeexplore.ieee.org/document/973435/,Proceedings of the Second International Workshop on Robot Motion and Control. RoMoCo'01 (IEEE Cat. No.01EX535),20-20 Oct. 2001,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IJCNN.2008.4633875,Bio-inspired stochastic chance-constrained multi-robot task allocation using WSN,IEEE,Conferences,"The multi-robot task allocation (MRTA) especially in unknown complex environment is one of the fundamental problems, a mostly important object in research of multi-robot. The MRTA problem is initially formulated as a chance-constrained optimization problem. Monte Carlo simulation is used to verify the accuracy of the solution provided by the algorithm. Ant colony optimization (ACO) algorithm based on bionic swarm intelligence was used. A hybrid intelligent algorithm combined Monte Carlo simulation and neural network is used for solving stochastic chance constrained models of MRTA. A practical implementation with real WSN and real mobile robots were carried out. In environment the successful implementation of tasks without collision validates the efficiency, stability and accuracy of the proposed algorithm. The convergence curve shows that as iterative generation grows, the utility increases and finally reaches a stable and optimal value. Results show that using sensor information fusion can greatly improve the efficiency. The algorithm is proved better than tradition algorithms without WSN for MRTA in real time.",https://ieeexplore.ieee.org/document/4633875/,2008 IEEE International Joint Conference on Neural Networks (IEEE World Congress on Computational Intelligence),1-8 June 2008,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/INES.2018.8523877,Body State Recognition for a Quadruped Mobile Robot,IEEE,Conferences,"The body states must be tracked by the onboard software on the robot to make good decisions. A human can pick up this machine or if the robot encounters anomalies (e.g. fall over) during locomotion, the state changes must be identified to execute the necessary responses. The authors of this paper developed a machine learning model which can recognize four states (normal, pick-up, fall over, poked) of a Sony AIBO robot. A deep neural network classifier with these predictors achieved 98% accuracy on unseen data and actual test runs on the robot proved the practical use with real-time execution speed. These properties made the proposed method a good candidate for adaption to other legged robots.",https://ieeexplore.ieee.org/document/8523877/,2018 IEEE 22nd International Conference on Intelligent Engineering Systems (INES),21-23 June 2018,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISVLSI49217.2020.00-12,CPSoSaware: Cross-Layer Cognitive Optimization Tools & Methods for the Lifecycle Support of Dependable CPSoS,IEEE,Conferences,"Cyber-physical Systems of Systems (CPSoS) are large complex systems where physical elements interact with and are controlled by a large number of distributed and networked computing elements as well as human users. Their increasingly stringent demands on efficient use of resources, high service and product quality levels and, of course low cost and competitiveness on the world market introduce big challenges related to the design operation continuum of dependable connected CPSs. The CPSoSaware project aims at developing the models and software tools to allocate computational power/resources to the CPS end devices and autonomously determining what cyber-physical processes will be handled by the devices' heterogeneous components (CPUs, GPUs, FPGA fabric, software stacks). The project relies on Artificial Intelligence (AI) support to strengthen reliability, fault tolerance and security at system level and also to lead to CPS designs that work in a decentralized way, collaboratively, in an equilibrium, by sharing tasks and data with minimal central intervention. The CPSoSaware system will interact with the human users/operators through extended reality visual and touchable interfaces increasing situational awareness. The CPSoSaware system will be evaluated: i) in the automotive sector, in mixed traffic environments with semi-autonomous connected vehicles and ii) in the manufacturing industry where inspection and repair scenarios are employed using collaborative robots.",https://ieeexplore.ieee.org/document/9155036/,2020 IEEE Computer Society Annual Symposium on VLSI (ISVLSI),6-8 July 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IJCNN52387.2021.9533738,CarSNN: An Efficient Spiking Neural Network for Event-Based Autonomous Cars on the Loihi Neuromorphic Research Processor,IEEE,Conferences,"Autonomous Driving (AD) related features provide new forms of mobility that are also beneficial for other kind of intelligent and autonomous systems like robots, smart transportation, and smart industries. For these applications, the decisions need to be made fast and in real-time. Moreover, in the quest for electric mobility, this task must follow low power policy, without affecting much the autonomy of the mean of transport or the robot. These two challenges can be tackled using the emerging Spiking Neural Networks (SNNs). When deployed on a specialized neuromorphic hardware, SNNs can achieve high performance with low latency and low power consumption. In this paper, we use an SNN connected to an event-based camera for facing one of the key problems for AD, i.e., the classification between cars and other objects. To consume less power than traditional frame-based cameras, we use a Dynamic Vision Sensor (DVS) [1]. The experiments are made following an offline supervised learning rule, followed by mapping the learnt SNN model on the Intel Loihi Neuromorphic Research Chip [2]. Our best experiment achieves an accuracy on offline implementation of 86%, that drops to 83% when it is ported onto the Loihi Chip. The Neuromorphic Hardware implementation has maximum 0.72 ms of latency for every sample, and consumes only 310 mW. To the best of our knowledge, this work is the first implementation of an event-based car classifier on a Neuromorphic Chip.",https://ieeexplore.ieee.org/document/9533738/,2021 International Joint Conference on Neural Networks (IJCNN),18-22 July 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA48506.2021.9561926,Circus ANYmal: A Quadruped Learning Dexterous Manipulation with Its Limbs,IEEE,Conferences,"Quadrupedal robots are skillful at locomotion tasks while lacking manipulation skills, not to mention dexterous manipulation abilities. Inspired by the animal behavior and the duality between multi-legged locomotion and multi-fingered manipulation, we showcase a circus ball challenge on a quadrupedal robot, ANYmal. We employ a model-free reinforcement learning approach to train a deep policy that enables the robot to balance and manipulate a light-weight ball robustly using its limbs without any contact measurement sensor. The policy is trained in the simulation, in which we randomize many physical properties with additive noise and inject random disturbance force during manipulation, and achieves zero-shot deployment on the real robot without any adjustment. In the hardware experiments, dynamic performance is achieved with a maximum rotation speed of 15 °/s, and robust recovery is showcased under external poking. To our best knowledge, it is the first work that demonstrates the dexterous dynamic manipulation on a real quadrupedal robot.",https://ieeexplore.ieee.org/document/9561926/,2021 IEEE International Conference on Robotics and Automation (ICRA),30 May-5 June 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA40945.2020.9197336,CityLearn: Diverse Real-World Environments for Sample-Efficient Navigation Policy Learning,IEEE,Conferences,"Visual navigation tasks in real-world environments often require both self-motion and place recognition feedback. While deep reinforcement learning has shown success in solving these perception and decision-making problems in an end-to-end manner, these algorithms require large amounts of experience to learn navigation policies from high-dimensional data, which is generally impractical for real robots due to sample complexity. In this paper, we address these problems with two main contributions. We first leverage place recognition and deep learning techniques combined with goal destination feedback to generate compact, bimodal image representations that can then be used to effectively learn control policies from a small amount of experience. Second, we present an interactive framework, CityLearn, that enables for the first time training and deployment of navigation algorithms across city-sized, realistic environments with extreme visual appearance changes. CityLearn features more than 10 benchmark datasets, often used in visual place recognition and autonomous driving research, including over 100 recorded traversals across 60 cities around the world. We evaluate our approach on two CityLearn environments, training our navigation policy on a single traversal per dataset. Results show our method can be over 2 orders of magnitude faster than when using raw images, and can also generalize across extreme visual changes including day to night and summer to winter transitions.",https://ieeexplore.ieee.org/document/9197336/,2020 IEEE International Conference on Robotics and Automation (ICRA),31 May-31 Aug. 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICIP42928.2021.9506200,Classification of RIGID and Non-Rigid Transformations with Autoencoder Representations,IEEE,Conferences,"Feature matching in transformed images is critical to many fields of computer science, from autonomous robots to video analysis. However, most widely used feature matching algorithms vary in their ability to track features depending on whether rigid or non-rigid image transformations occur. This makes it critical, especially in real-time calculations, to be able to identify what kind of transformation is taking place quickly in order to deploy the best feature matching algorithm for that type of transformation. The proposed research uses a combined autoencoder and neural network classification model to classify rigid or non-rigid transformations in order to improve feature matching on the image pairs. This system is the first to perform this kind of analysis with representation learning and opens new ways to improving feature matching performance. We show that using this method improves the amount of feature matches found between correctly identified image pairs.",https://ieeexplore.ieee.org/document/9506200/,2021 IEEE International Conference on Image Processing (ICIP),19-22 Sept. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS.2018.8594311,Cognition-enabled Framework for Mixed Human-Robot Rescue Teams,IEEE,Conferences,"With the advancements in robotic technology and the progress in human-robot interaction research, the interest in deploying mixed human-robot teams in rescue missions is increasing. Due to their complementary capabilities in terms of locomotion, visibility and reachability of areas, human-robot teams are considerably deployed in real-world settings, albeit the robotic agents in such scenarios are normally fully teleoperated. A major barrier to successful and efficient mission execution in those teams is the lack of cognitive skills in robotic systems. In this paper, we present a cognition-enabled framework and an implemented system where robotic agents are equipped with cognitive capabilities to naturally communicate with humans and autonomously perform tasks. The framework allows for natural tasking of robots, reasoning about robot behavior, capabilities and actions, and a common belief state representation for shared mission awareness of robots and human operators.",https://ieeexplore.ieee.org/document/8594311/,2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),1-5 Oct. 2018,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IST50367.2021.9651352,Comparison of One-Stage Object Detection Models for Weed Detection in Mulched Onions,IEEE,Conferences,"Deep learning-based computer vision enabled farming robots to detect and control weeds in the field accurately. This study compared the performance of Scaled-YOLOv4-CSP, YOLOv5s, and SSD Mobilenet V2 for image-based weed detection in mulched onions. The study showed that YOLOv5s is more suitable for the purpose. At 0.915 $\mathrm{m}\mathrm{A}\mathrm{P}^{0.5}$, it ties with Scaled-YOLOv4-CSP at first place on weed detection performance. Meanwhile, YOLOv5s consumed significantly fewer resources during training and implementation, giving it an advantage in real-time weed detection. Its mean inference time of 7.72 milliseconds is also less than half of the other two models. Lastly, the study demonstrated that increasing the number of samples with a more balanced class distribution by upsizing the dataset through data augmentation would improve the overall performance of the object detection model.",https://ieeexplore.ieee.org/document/9651352/,2021 IEEE International Conference on Imaging Systems and Techniques (IST),24-26 Aug. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/NER.2013.6696078,Conditioned behavior in a robot controlled by a spiking neural network,IEEE,Conferences,"Insects show a rich repertoire of goal-directed and adaptive behaviors that are still beyond the capabilities of today's artificial systems. Fast progress in our comprehension of the underlying neural computations make the insect a favorable model system for neurally inspired computing paradigms in autonomous robots. Here, we present a robotic platform designed for implementing and testing spiking neural network control architectures. We demonstrate a neuromorphic realtime approach to sensory processing, reward-based associative plasticity and behavioral control. This is inspired by the biological mechanisms underlying rapid associative learning and the formation of distributed memories in the insect.",https://ieeexplore.ieee.org/document/6696078/,2013 6th International IEEE/EMBS Conference on Neural Engineering (NER),6-8 Nov. 2013,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ROBOT.1988.12090,Condor: a revised architecture for controlling the Utah-MIT hand,IEEE,Conferences,"A fast architecture for controlling the Utah-MIT hand and other such complex robots is reviewed. These robots are characterized by a large number of joints and consequently demand powerful computer architectures to be controlled and utilized effectively. The version of the architecture derives its power from a novel bus-to-bus adaptor to couple a development host running a time-sharing operating system with a multimicroprocessor system devoted to real-time computations. The software is characterized by a few simple design concepts but provides the facilities out of which powerful utilities like a multiprocessor pseudoterminal emulator, a transparent fast file server, and a truly flexible powerful symbolic debugger could be constructed.<>",https://ieeexplore.ieee.org/document/12090/,Proceedings. 1988 IEEE International Conference on Robotics and Automation,24-29 April 1988,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AITest.2019.00015,Constraint-Based Testing of An Industrial Multi-Robot Navigation System,IEEE,Conferences,"Intelligent multi-robot systems get more and more deployed in industrial settings to solve complex and repetitive tasks. Due to safety and economic reasons they need to operate dependably. To ensure a high degree of dependability, testing the deployed system has to be done in a rigorous way. Advanced multi-robot systems show a rich set of complex behaviors. Thus, these systems are difficult to test manually. Moreover, the space of potential environments and tasks for such systems is enormous. Therefore, methods that are able to explore this space in a structured way are needed. One way to address these issues is through model-based testing. In this paper we present an approach for testing the navigation system of a fleet of industrial transport robots. We show how all potential environments and navigation behaviors as well as requirements and restrictions can be represented in a formal constraint-based model. Moreover, we present the concept of coverage criteria in order to handle the potentially infinite space of test cases. Finally, we show how test cases can be derived from this model in an efficient way. In order to show the feasibility of the proposed approach we present an empirical evaluation of a prototype implementation using a real industrial use case.",https://ieeexplore.ieee.org/document/8718216/,2019 IEEE International Conference On Artificial Intelligence Testing (AITest),4-9 April 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/RO-MAN47096.2020.9223341,Context Dependent Trajectory Generation using Sequence-to-Sequence Models for Robotic Toilet Cleaning,IEEE,Conferences,"A robust, easy-to-deploy robot for service tasks in a real environment is difficult to construct. Record-and-playback (R&P) is a method used to teach motor-skills to robots for performing service tasks. However, R&P methods do not scale to challenging tasks where even slight changes in the environment, such as localization errors, would either require trajectory modification or a new demonstration. In this paper, we propose a Sequence-to-Sequence (Seq2Seq) based neural network model to generate robot trajectories in configuration space given a context variable based on real-world measurements in Cartesian space. We use the offset between a target pose and the actual pose after localization as the context variable. The model is trained using a few expert demonstrations collected using teleoperation. We apply our proposed method to the task of toilet cleaning where the robot has to clean the surface of a toilet bowl using a compliant end-effector in a constrained toilet setting. In the experiments, the model is given a novel offset context and it generates a modified robot trajectory for that context. We demonstrate that our proposed model is able to generate trajectories for unseen setups and the executed trajectory results in cleaning of the toilet bowl.",https://ieeexplore.ieee.org/document/9223341/,2020 29th IEEE International Conference on Robot and Human Interactive Communication (RO-MAN),31 Aug.-4 Sept. 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/WHC.2011.5945522,Control of a desktop mobile haptic interface,IEEE,Conferences,"Most haptic devices share two main limits: they are grounded and they have limited workspace. A possible solution is to create haptic interfaces by combining mobile robots and standard grounded force-feedback devices, the so called Mobile Haptic Interfaces (MHIs). However, MHIs are characterized by dynamical limitations due to performance of the employed devices. This paper focuses on basic design issues and presents a novel (prototype) Mobile Haptics Platform that employs the coordination of numerically controlled wheel torques to render forces to a user handle placed on the top of the device. The interface, consisting in a small omni-directional robot, is link-less, fully portable and it has been designed to support home-rehabilitation exercises. In the present paper we shall review relevant choices concerning the functional aspects and the control design. In particular a specific embedded sensor fusion was implemented to allow the device to move on a desk without drifting. The sensor fusion algorithm has been optimized to provide users with a quality force feedback while ensuring accurate position tracking. The two requirements are in contrast each other and a specific variant of the Extended Kalman Filter (EKF) was required to allow the device working.",https://ieeexplore.ieee.org/document/5945522/,2011 IEEE World Haptics Conference,21-24 June 2011,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA40945.2020.9197209,Cooperative Multi-Robot Navigation in Dynamic Environment with Deep Reinforcement Learning,IEEE,Conferences,"The challenges of multi-robot navigation in dynamic environments lie in uncertainties in obstacle complexities, partially observation of robots, and policy implementation from simulations to the real world. This paper presents a cooperative approach to address the multi-robot navigation problem (MRNP) under dynamic environments using a deep reinforcement learning (DRL) framework, which can help multiple robots jointly achieve optimal paths despite a certain degree of obstacle complexities. The novelty of this work includes threefold: (1) developing a cooperative architecture that robots can exchange information with each other to select the optimal target locations; (2) developing a DRL based framework which can learn a navigation policy to generate the optimal paths for multiple robots; (3) developing a training mechanism based on dynamics randomization which can make the policy generalized and achieve the maximum performance in the real world. The method is tested with Gazebo simulations and 4 differential drive robots. Both simulation and experiment results validate the superior performance of the proposed method in terms of success rate and travel time when compared with the other state-of-art technologies.",https://ieeexplore.ieee.org/document/9197209/,2020 IEEE International Conference on Robotics and Automation (ICRA),31 May-31 Aug. 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS.2014.6942970,Coordination in human-robot teams using mental modeling and plan recognition,IEEE,Conferences,"Beliefs play an important role in human-robot teaming scenarios, where the robots must reason about other agents' intentions and beliefs in order to inform their own plan generation process, and to successfully coordinate plans with the other agents. In this paper, we cast the evolving and complex structure of beliefs, and inference over them, as a planning and plan recognition problem. We use agent beliefs and intentions modeled in terms of predicates in order to create an automated planning problem instance, which is then used along with a known and complete domain model in order to predict the plan of the agent whose beliefs are being modeled. Information extracted from this predicted plan is used to inform the planning process of the modeling agent, to enable coordination. We also look at an extension of this problem to a plan recognition problem. We conclude by presenting an evaluation of our technique through a case study implemented on a real robot.",https://ieeexplore.ieee.org/document/6942970/,2014 IEEE/RSJ International Conference on Intelligent Robots and Systems,14-18 Sept. 2014,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/WSE.2011.6081830,Cracking the Smart ClickBot,IEEE,Conferences,"Nowadays, almost every task involving Web traversing and information retrieval recurs to Web robots. Web robots are software programs that automatically traverse the Web's hypertext structure. They proliferate rapidly aside with the growth of the Web and are extremely valuable and important means not only for the large search engines, but also for many specialized services such as investment portals, competitive intelligence tools, etc. While many web robots serve useful purposes, recently, there have been cases linked to fraudulent activities committed by these Web robots. Click fraud, which is the act of generating illegitimate clicks, is one of them. This paper details the architecture and functionality of the Smart ClickBot, a sophisticated software bot that is designed to commit click fraud. It was first detected and reported by NetMosaics Inc. in March, 2010, a real time click fraud detection and prevention solution provider. We discuss the machine learning algorithms used, to identify all clicks exhibiting Smart ClickBot like patterns. We constructed a Bayesian classifier that automatically classifies server log data as being Smart ClickBot or not. We also introduce a Benchmark data set for Smart ClickBot. We disclose the results of our investigation of this bot to educate the security research community and provide information regarding the novelties of the attack.",https://ieeexplore.ieee.org/document/6081830/,2011 13th IEEE International Symposium on Web Systems Evolution (WSE),30-30 Sept. 2011,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS.2003.1248914,Creation and analysis of a scenario based universal sensory driver layer with real-time fault tolerant properties,IEEE,Conferences,"Sensor fusion and sensor integration is becoming an increasingly popular approach in dealing with complex sensor systems in autonomous mobile robots (AMR). However, the procedure for the sensor integration and sensor fusion is a non-trivial process. This paper presents a scenario based approach to sensor fusion based on the autonomous evolution of sensory and actuator driver layers through environmental constraints (AEDEC) [T.A Choi, 2002]. Using the scenario based approach, the programmer's work of creating a sensory driver will be eliminated by having the AMR learn the driver on its own. In the process of creating each scenario, sensor fusion is automatically implemented. If sensors change or even if the sensor configuration changes, the driver can be updated by having the AMR relearn the driver over again. Due to the tabular structure of the scenario based sensory drivers, malfunctioning sensors can not only be detected, but the driver can automatically adapt to the malfunctioning sensor in real-time. Furthermore, different AMR's trained using AEDEC architecture will have similar interpretations of its environment. This is guaranteed by having the AMR learn the driver in the same highly structured training environment. The behavioral coding is simplified by eliminating any reference to hardware dependent parameters. Finally, the level of abstraction and the consistency of the highly structured environment allows for coding portability.",https://ieeexplore.ieee.org/document/1248914/,Proceedings 2003 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2003) (Cat. No.03CH37453),27-31 Oct. 2003,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS51168.2021.9635949,DRQN-based 3D Obstacle Avoidance with a Limited Field of View,IEEE,Conferences,"In this paper, we propose a map-based end-to-end DRL approach for three-dimensional (3D) obstacle avoidance in a partially observed environment, which is applied to achieve autonomous navigation for an indoor mobile robot using a depth camera with a narrow field of view. We first train a neural network with LSTM units in a 3D simulator of mobile robots to approximate the Q-value function in double DRQN. We also use a curriculum learning strategy to accelerate and stabilize the training process. Then we deploy the trained model to a real robot to perform 3D obstacle avoidance in its navigation. We evaluate the proposed approach both in the simulated environment and on a robot in the real world. The experimental results show that the approach is efficient and easy to be deployed, and it performs well for 3D obstacle avoidance with a narrow observation angle, which outperforms other existing DRL-based models by 15.5% on success rate.",https://ieeexplore.ieee.org/document/9635949/,2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),27 Sept.-1 Oct. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISSCC42614.2022.9731699,DSPU: A 281.6mW Real-Time Depth Signal Processing Unit for Deep Learning-Based Dense RGB-D Data Acquisition with Depth Fusion and 3D Bounding Box Extraction in Mobile Platforms,IEEE,Conferences,"Emerging mobile platforms, such as autonomous robots and AR devices, require RGBD data and 3D bounding-box (BB) information for accurate navigation and seamless interaction with the surrounding environment. Specifically, the extraction of RGB-D data and 3D BB needs to be done in real-time (&#x003E; 30fps) while consuming low power (&#x003C; 1W) due to limited battery capacity. In addition, a conventional depth processing system consumes high power due to a high performance (HP) time-of-flight (ToF) sensor with an illuminator (&#x003E; 3W) [1]. However, even the HP ToF fails to extract depth in areas of extreme reflectance, leading to failure in navigation or AR interaction. In addition, software implementation on an application processor suffers from high latency (&#x007E; 0.1 s) to preprocess the depth data and process the 3D point cloud-based neural network (PNN) [2]. Therefore, this paper proposes an SoC for low-power and low-latency depth estimation and 3D object detection with high accuracy, as shown in Fig. 33.4.1. The system implements depth fusion [3], [4] to allow accurate RGB-D extraction without hollows, while using a low-power (LP) ToF sensor (&#x003C;0.4W). The SoC can fully accelerate the depth-processing pipeline, achieving a maximum of 45.6fps.",https://ieeexplore.ieee.org/document/9731699/,2022 IEEE International Solid- State Circuits Conference (ISSCC),20-26 Feb. 2022,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA48506.2021.9561066,Decentralized Connectivity Maintenance with Time Delays using Control Barrier Functions,IEEE,Conferences,"Connectivity maintenance is crucial for the real world deployment of multi-robot systems, as it ultimately allows the robots to communicate, coordinate and perform tasks in a collaborative way. A connectivity maintenance controller must keep the multi-robot system connected independently from the system&#x2019;s mission and in the presence of undesired real world effects such as communication delays, model errors, and computational time delays, among others. In this paper we present the implementation, on a real robotic setup, of a connectivity maintenance control strategy based on Control Barrier Functions. During experimentation, we found that the presence of communication delays has a significant impact on the performance of the controlled system, with respect to the ideal case. We propose a heuristic to counteract the effects of communication delays, and we verify its efficacy both in simulation and with physical robot experiments.",https://ieeexplore.ieee.org/document/9561066/,2021 IEEE International Conference on Robotics and Automation (ICRA),30 May-5 June 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA.2016.7487617,Decentralized multi-agent exploration with online-learning of Gaussian processes,IEEE,Conferences,"Exploration is a crucial problem in safety of life applications, such as search and rescue missions. Gaussian processes constitute an interesting underlying data model that leverages the spatial correlations of the process to be explored to reduce the required sampling of data. Furthermore, multi-agent approaches offer well known advantages for exploration. Previous decentralized multi-agent exploration algorithms that use Gaussian processes as underlying data model, have only been validated through simulations. However, the implementation of an exploration algorithm brings difficulties that were not tackle yet. In this work, we propose an exploration algorithm that deals with the following challenges: (i) which information to transmit to achieve multi-agent coordination; (ii) how to implement a light-weight collision avoidance; (iii) how to learn the data's model without prior information. We validate our algorithm with two experiments employing real robots. First, we explore the magnetic field intensity with a ground-based robot. Second, two quadcopters equipped with an ultrasound sensor explore a terrain profile. We show that our algorithm outperforms a meander and a random trajectory, as well as we are able to learn the data's model online while exploring.",https://ieeexplore.ieee.org/document/7487617/,2016 IEEE International Conference on Robotics and Automation (ICRA),16-21 May 2016,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICBBE.2008.696,Decoding Hand Kinematics and Neural States Using Gaussian Process Model,IEEE,Conferences,"Probabilistic modeling of correlated neural population firing activity is central to understanding the neural code and building practical decoding algorithms, the accurate reconstruction of a continuous motion signal is necessary for the control of devices such as computer cursors, robots, or a patient's own paralyzed limbs. For such applications we developed a realtime system that uses Gaussian process techniques to estimate hand motion from the firing rates of multiple neurons. Gaussian Processes for Machine Learning presents one of the most important Bayesian machine learning approaches based on a particularly effective method for placing a prior distribution over the space of functions. Decoding was performed using Gaussian processes model which gives an efficient method for Bayesian inference when the firing rates and hand kinematics are nonlinear. Gaussian processes provide a principled, practical, probabilistic approach to learning in noisy measurements. In off-line experiments, the Gaussian processes model reconstructions of hand trajectory were more accurate than previously reported results. The resulting decoding algorithm provides a principled probabilistic model of motor-cortical coding, decodes hand motion in real time, provides an estimate of uncertainty, and is straight to implement. Additionally the formulation unifies and extends previous models of neural coding while providing insights into the motor-cortical code.",https://ieeexplore.ieee.org/document/4535576/,2008 2nd International Conference on Bioinformatics and Biomedical Engineering,16-18 May 2008,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS40897.2019.8967874,Deep Dive into Faces: Pose & Illumination Invariant Multi-Face Emotion Recognition System,IEEE,Conferences,"One of the advancements in humanization of robots is its ability to recognize human emotions. Facial expression plays a key role in identifying human emotions relative to other cues. In this research, an intelligent network capable of real-time emotion recognition from multiple faces using deep learning technique is presented. The proposed network is based on Convolution Neural Network (CNN) in which three blocks of Convolution layers for feature extraction and two blocks of Dense layers for classification are used. The novelty of this method lies in recognizing emotions from multiple faces simultaneously in real time and its invariance to head pose, illumination and age factor. Most of reported work in literature for multiple faces is for frontal face without illumination variation. The proposed emotion recognition system is deployed on Raspberry Pi3 B+ for human robot interaction applications and achieved an average accuracy of 95.8% in real time.",https://ieeexplore.ieee.org/document/8967874/,2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),3-8 Nov. 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IJCNN.2018.8489239,Deep Learning-based Cooperative Trail Following for Multi-Robot System,IEEE,Conferences,"Following trails in the wild is an essential capability of out-door autonomous mobile robots. Recently, deep learningbased approaches have made great advancements in this field. However, the existing research only focuses on the trail following with a single robot. In contrast, many robotic tasks in the reality, such as search and patrolling, are conducted by a group of robots. While these robots are grouped to move in the wild, they can cooperate to significantly promote the trail following accuracy, for example, by sharing images of different view angles or real-time decision fusion. This paper proposes such an approach named DL-Cooper that enables multi-robot visionbased trail following based on deep learning algorithms. It allows each robot to make a decision respectively with deep neural network and then fusion the decisions on the collective level with the support of back-end cloud computing infrastructure. It also takes Quality of Service (QoS) assurance, a very essential property of robotic software, into consideration. By limiting the condition to fusion decisions, the time latency can be minimally sacrificed. Experiments on the real-world dataset show that our approach has significantly improved the accuracy of the singlerobot system.",https://ieeexplore.ieee.org/document/8489239/,2018 International Joint Conference on Neural Networks (IJCNN),8-13 July 2018,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS45743.2020.9340882,Deep Mixture Density Network for Probabilistic Object Detection,IEEE,Conferences,"Mistakes/uncertainties in object detection could lead to catastrophes when deploying robots in the real world. In this paper, we measure the uncertainties of object localization to minimize this kind of risk. Uncertainties emerge upon challenging cases like occlusion. The bounding box borders of an occluded object can have multiple plausible configurations. We propose a deep multivariate mixture of Gaussians model for probabilistic object detection. The covariances help to learn the relationship between the borders, and the mixture components potentially learn different configurations of an occluded part. Quantitatively, our model improves the AP of the baselines by 3.9% and 1.4% on CrowdHuman and MS-COCO respectively with almost no computational or memory overhead. Qualitatively, our model enjoys explainability since the resulting covariance matrices and the mixture components help measure uncertainties.",https://ieeexplore.ieee.org/document/9340882/,2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),24 Oct.-24 Jan. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ROBIO.2018.8665274,Deep Reinforcement Learning Based Brachiation Control for Two-Link Bio-Primate Robot,IEEE,Conferences,"Manually designing an effective and efficient controller for complex mechanics, such as bio-inspired robots or underactuated mechanical system, typically are very difficult. It requires precise motion planning and dynamic control. Reinforcement learning or genetic algorithm based learning methods suffers from representing the high dimensional models. The combination of deep learning and reinforcement learning provide a feasible way to handle such difficulties. However, priori-less searching sometimes tends to be low efficient and usually finds the “mechanic” solution instead of the “natural” one. In this paper, the traditional nonlinear control concept is integrated into the deep reinforcement learning (DRL) framework. The whole process is implemented on the brachiation control problem of a two link bio-primate robot. Deep Deterministic Policy Gradient (DDPG) is used to search for the optimal control policy. The searching process is realized by interacting with the dynamic model instead of real robot. The energy based planning and control concept is adopted, which utilize the fact that when the shoulder joint angle is fixed, energy of the whole system keeps constant. By regulating the angle and energy, the robot can be restricted on a particular trajectory. The energy concept is encoded within the reward function and trained in the Gym environment. For varying targets point-to-point control, the network structure is also modified to accept the target coordinates. Effectiveness of the proposed methods are verified by simulation and experimental results.",https://ieeexplore.ieee.org/document/8665274/,2018 IEEE International Conference on Robotics and Biomimetics (ROBIO),12-15 Dec. 2018,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICARSC52212.2021.9429811,Deep Reinforcement Multi-Directional Kick-Learning of a Simulated Robot with Toes,IEEE,Conferences,"This paper describes a thorough analysis of using PPO to learn kick behaviors with simulated NAO robots in the simspark environment. The analysis includes an investigation of the influence of PPO hyperparameters, network size, training setups and performance in real games. We believe to improve the state of the art mainly in four points: first, the kicks are learned with a toed version of the NAO robot, second, we improve the reliability with respect to kickable area and avoidance of falls, third, the kick can be parameterized with desired distance and direction as input to the deep network and fourth, the approach allows to integrate the learned behavior seamlessly into soccer games. The result is a significant improvement of the general level of play.",https://ieeexplore.ieee.org/document/9429811/,2021 IEEE International Conference on Autonomous Robot Systems and Competitions (ICARSC),28-29 April 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ROMAN.2017.8172429,Deep recurrent Q-learning of behavioral intervention delivery by a robot from demonstration data,IEEE,Conferences,"We present a learning from demonstration (LfD) framework that uses a deep recurrent Q-network (DRQN) to learn how to deliver a behavioral intervention (BI) from demonstrations performed by a human. The trained DRQN enables a robot to deliver a similar BI in an autonomous manner. BIs are highly structured procedures wherein children with developmental delays/disorders (e.g. autism, ADHD, etc.) are trained to perform new behaviors and life-skills. Mounting anecdotal evidence from human-robot interaction (HRI) research has shown that BI benefits from the use of robots as a delivery tool. Most of the HRI research on robot-based intervention relies on tele-operated robots. However, the need for autonomy has become increasingly evident, especially when it comes to the real-world deployment of these systems. The few studies that have used autonomy in robot-based BI relied on hand-picked features of the environment in order to trigger correct robot actions. Additionally, none of these automated architectures attempted to learn the BI from human demonstrations, though this appears to be the most natural way of learning. This paper represents the first attempt to design a robot that uses LfD to learn BI. We generate a model then correctly predict appropriate actions with greater than 80% accuracy. To the best of our knowledge, this is the first attempt to employ DRQN within an LfD framework to learn high level reasoning embedded in human actions and behaviors simply from observations.",https://ieeexplore.ieee.org/document/8172429/,2017 26th IEEE International Symposium on Robot and Human Interactive Communication (RO-MAN),28 Aug.-1 Sept. 2017,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICMLA51294.2020.00201,Defending Against Localized Adversarial Attacks on Edge-Deployed Monocular Depth Estimators,IEEE,Conferences,"Estimation of depth from a single image is an important scene understanding task in computer vision. With the advent of Deep Learning and Convolutional Neural Networks, staggeringly high accuracies have been achieved in this task. With advancements in model optimization, it has been possible to deploy these models on edge devices, allowing for efficient depth estimation in safety-critical applications in robots, rovers, drones and even self-driving vehicles. However, these models are susceptible to attacks from malicious adversaries, which aim to distort the output of the model for a seemingly clean image by adding minute perturbations. In the real-world scenario, the most plausible attack is the adversarial patch, which can be printed and used as a physical adversarial attack against Deep Learning models. In the case of Monocular Depth Estimation, we show that small adversarial patches, which range from 0.7% to 5% of the image size, greatly worsen model performance. It is thus essential that these models are made robust using defense mechanisms, to defend against malicious inputs while also not reducing performance on clean images. Moreover, it is essential that the defense mechanism be computationally efficient, for real-time inference on edge devices. In this work, we propose the first defense mechanism against adversarial patches for a regression network, in the context of Monocular Depth Estimation on an edge device. The defense mechanism adds very little overhead time of 38 milliseconds on a Raspberry Pi 3 Model B, maintaining performance on clean images while also achieving near clean image levels of performance on adversarial inputs.",https://ieeexplore.ieee.org/document/9356303/,2020 19th IEEE International Conference on Machine Learning and Applications (ICMLA),14-17 Dec. 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS51168.2021.9635917,Delay Aware Universal Notice Network: Real world multi-robot transfer learning,IEEE,Conferences,"General purpose simulators provide cheap training data to learn complex robotic skills. However, the transition from simulation to reality is often very challenging for the agent. One major issue is the delay on the physical robot that may deteriorate the performance of the deployed agent. Furthermore, once a successfully trained learning-based control policy is available, re-purposing the knowledge acquired by the agent to enable a structurally distinct agent to perform the same task is hazardous if done naively. In this work, we address the above issues with a single method, the DA-UNN (Delay Aware Universal Notice Network), which decomposes the knowledge into robot-specific and task-specific modules for fast transfer. Our framework deals with delays immanent to physical systems in order to improve sim2real transfer. We evaluate the efficiency of our approach using simulated and actual robots on a dynamic manipulation task where delay management is crucial.",https://ieeexplore.ieee.org/document/9635917/,2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),27 Sept.-1 Oct. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.23919/ICACT53585.2022.9728813,Design and Implementation of a Real-time Target Detection and Tracking System,IEEE,Conferences,"Target detection is to quickly and accurately locate the object to be measured in the video image and correctly classify it. Target tracking is to obtain the motion parameters of the object in the image sequence, and to detect, extract, identify and track the specific object. Combining target detection and tracking to follow specific objects is one of the research hotspots in the realization of intelligent embedded terminals. This paper designs and implements a system that combines target detection and tracking, which can realize real-time tracking of specific objects. Finally, the experiments prove the reliability of the proposed system, which can be used in intelligent systems such as robots.",https://ieeexplore.ieee.org/document/9728813/,2022 24th International Conference on Advanced Communication Technology (ICACT),13-16 Feb. 2022,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/BDAI52447.2021.9515251,Design and Simulation of Tracked Mobile Robot Path Planning,IEEE,Conferences,"Aiming at the problem of autonomous navigation of robots in unknown environments, the robot operating system ROS is used as a development platform, and an autonomous navigation system is designed based on open source function packages such as Gmapping and Navigation, so that robots equipped with this system can learn map information in unknown environments. And based on the map to achieve positioning, path planning, obstacle avoidance and other functions. The 3D physical simulation software Gazebo simulates the environment and loads the designed URDF robot model to simulate and verify the autonomous navigation system. The results show that the autonomous navigation system can enable the robot to achieve accurate mapping, positioning, real-time obstacle avoidance and path planning in an unknown environment.",https://ieeexplore.ieee.org/document/9515251/,2021 IEEE 4th International Conference on Big Data and Artificial Intelligence (BDAI),2-4 July 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ETFA.2015.7301549,Design and implementation for multiple-robot deployment in intelligent space,IEEE,Conferences,"This paper presents the problem of robot deployment for a number of scattered tasks. We aim to minimize the duration it takes for all robots to reach their assigned task locations. In previous work, we have proposed a team composed of one carrier robot (CR) and several servant robots to accomplish the mission. Then we have suggested an algorithm that determines a path of the CR for an efficient deployment under a few constraints, which is verified by simulations. Assuming that the servant robots are unmanned aerial vehicles (UAVs), the present paper extends the discussion to a real robot experiment. We design and implement a deployment system in intelligent space. The feasibility of the study is demonstrated through an experiment.",https://ieeexplore.ieee.org/document/7301549/,2015 IEEE 20th Conference on Emerging Technologies & Factory Automation (ETFA),8-11 Sept. 2015,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AEMCSE51986.2021.00262,Design of Motion Control System of Handling Robot,IEEE,Conferences,"Handling robots are very important for improving productivity, reducing work intensity, improving the surrounding environment, and ensuring work safety. Based on the kinematics analysis of a two-degree-of-freedom manipulator based on a slider mechanism, this paper uses an open motion controller as a development platform to carry out hardware design and software development to realize the flexible control of the handling robot, with simple structure and real-time control, open structure, standard programming and rich man-machine interface, etc. which can be widely used in material destacking and palletizing handling.",https://ieeexplore.ieee.org/document/9513197/,"2021 4th International Conference on Advanced Electronic Materials, Computers and Software Engineering (AEMCSE)",26-28 March 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICDSCA53499.2021.9650125,Design of an Indoor Surveying and Mapping Robot Based on SLAM Technology,IEEE,Conferences,"Simultaneous localization and mapping (SLAM) are a key technology for indoor mapping and navigation of mobile robots. As traditional surveying and mapping work depends on the general simple tools or Artificial completed exist inaccuracies, insecurity and high repeatability errors. To solve the problems mentioned above, an experimental system of mobile robot mapping based on SLAM technology is designed and implemented. The mobile robot is equipped with lidar and odometer sensor. Through the cartographer algorithm[1], lidar data and odometer data are integrated to realize indoor map construction and navigation. The open-source robot operating system (ROS) based on Linux was used for realizing information communication, data processing, real-time display between the upper machine and lower machine. The system simulation and real scene test are carried out. The mobile robot can build reliable and efficient map. Compared with the traditional technology of surveying and mapping, simulation and real experiment results show that the system has high practicability and reliability.",https://ieeexplore.ieee.org/document/9650125/,2021 IEEE International Conference on Data Science and Computer Application (ICDSCA),29-31 Oct. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS.2018.8594335,Detection- Tracking for Efficient Person Analysis: The DetTA Pipeline,IEEE,Conferences,"In the past decade many robots were deployed in the wild, and people detection and tracking is an important component of such deployments. On top of that, one often needs to run modules which analyze persons and extract higher level attributes such as age and gender, or dynamic information like gaze and pose. The latter ones are especially necessary for building a reactive, social robot-person interaction. In this paper, we combine those components in a fully modular detection-tracking-analysis pipeline, called DetTA. We investigate the benefits of such an integration on the example of head and skeleton pose, by using the consistent track ID for a temporal filtering of the analysis modules' observations, showing a slight improvement in a challenging real-world scenario. We also study the potential of a so-called “free-flight” mode, where the analysis of a person attribute only relies on the filter's predictions for certain frames. Here, our study shows that this boosts the runtime dramatically, while the prediction quality remains stable. This insight is especially important for reducing power consumption and sharing precious (GPU-)memory when running many analysis components on a mobile platform, especially so in the era of expensive deep learning methods.",https://ieeexplore.ieee.org/document/8594335/,2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),1-5 Oct. 2018,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.23919/ICAC50006.2021.9594118,Deterministic Planning for Flexible Intralogistics,IEEE,Conferences,"An automated planning unit that enables the user to deterministically schedule transportation tasks for intralogistics use cases is proposed. The developed solution aims at inducing a high degree of determinism into transportation task planning in manufacturing industries while at the same time providing the user with the opportunity to flexibly react to rapidly changing constraints, such as updated order situations. The main objective of the software tool is to facilitate the order management process and ensure conflict-free path planning and following of a centrally guided fleet of mobile robots serving transportation tasks. Furthermore, in order to meet customer demands in terms of responsiveness to altered circumstances, the system is able to re-allocate already planned transportation tasks in favor of more urgent ones that may come in without further notice. This is achieved by adopting concepts commonly used in real-time operating systems to the complex problem of intralogistics task scheduling. Sporadically incoming transportation tasks are scheduled dynamically with regard to deadlines, priority levels, available resources as well as estimated execution effort. Flexibility and system responsiveness are increased noticeable by applying on-line task migration mechanisms. The eligibility of the adapted concepts is demonstrated by deploying the proposed solution to test cases within a simulation environment. For this purpose a scalable Key Performance Indicator (KPI) function is proposed as well.",https://ieeexplore.ieee.org/document/9594118/,2021 26th International Conference on Automation and Computing (ICAC),2-4 Sept. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.23919/OCEANS44145.2021.9705662,Developing a Robotic Hybrid Network for Coastal Surveillance: the INFORE Experience,IEEE,Conferences,"INFORE EU H2020 project has the objective of developing a real-time, interactive extreme-scale analytics and forecasting system capable of handling and analysing massive data streams. The system is validated in three different real-world use cases, In this paper we describe one of them, presenting the development of the INFORE system for a coastal surveillance Maritime Situational Awareness (MSA) application. The MSA INFORE system exploits the synergy between global view (AIS data, ESA Sentinel satellite data), providing contextual information over a wider area, and local view produced by a sensorised hybrid robotic network. The network is composed of an RGB/thermal camera onshore and of two Wave Gliders robots equipped with passive sonars. We focus on the development of this network, describing the cooperative autonomy framework to control the robots. The framework enables the robots to make decisions on their navigation for improving vessel detection and tracking. The INFORE MSA network demonstrates the benefits that the synergy between machine learning, AI and autonomous robots can bring in this kind of monitoring systems. The developed autonomy strategies increase the quality of the information produced by robots. These high-quality real-time observations are fused with the global view in the INFORE MSA and decision-support platform. Data fusion from multimodal data sources results crucial for the detection of complex events (e.g. illegal fishing), which can then be communicated to decision-makers. Different modules of the systems are ready and under testing. The whole system will be validated in a trial in 2022.",https://ieeexplore.ieee.org/document/9705662/,OCEANS 2021: San Diego – Porto,20-23 Sept. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/RO-MAN50785.2021.9515353,Developing an Engagement-Aware System for the Detection of Unfocused Interaction,IEEE,Conferences,"We introduce a perception system for social robots that is able to detect a person’s engagement in an interaction from nonverbal cues independently of principal user activity. This was achieved by the introduction of a set of proxemics, body posture and attention features relevant for human-human interaction. The features were extracted from RGB-D image data of a single Kinect and utilized to train two separate machine learning models. Multiple system configurations and feature combinations were tested, and their impact on the detection of user engagement evaluated. Combining all features, our perception system reaches an F1-score of 81% when estimating an observed person’s interaction intent through binary classification. Regression of a user’s level of availability deviates from the given ground truth values by 13.27% on average. Finally, a prototype was implemented which is able to simultaneously run both previous estimates in real-time using a shared feature vector. In the following, the proposed system shall be used to design robots whose behavior shows their awareness of user engagement.",https://ieeexplore.ieee.org/document/9515353/,2021 30th IEEE International Conference on Robot & Human Interactive Communication (RO-MAN),8-12 Aug. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IntelliSys.2017.8324230,Developing video games with elementary adaptive artificial intelligence in unity: An intelligent systems approach,IEEE,Conferences,"Video games have increasingly demonstrated a great deal of audiovisual realism, in par with the massive performance improvement of computer systems. At the same time, their Artificial Intelligence (AI) component falls short in terms of realism because it is usually based on non-adaptive methods. Adaptive AI mechanisms can help increase video game realism allowing the game to adapt in real-time to the game progress and the user behavior. Following a short overview of the progress of AI in video games in the past years, this paper highlights the creation of modern video games with basic and elementary adaptive game AI using the Unity game development framework. Particular emphasis is on the details of the AI component. First, a shooter game with basic AI is created. Finally, an action-adventure video game is created featuring elementary case-based adaptive AI. The objective in this game is to create enemies which are able to perceive changes in the environment and adapt their strategies accordingly. Proposed AI practices can migrate into relevant real world applications, such as video surveillance and intrusion detection systems, mission critical autonomous networked patrolling and/or save and rescue robots, vision and hearing assistive applications, intelligent video and behavioral analytics to detect and predict threats etc.",https://ieeexplore.ieee.org/document/8324230/,2017 Intelligent Systems Conference (IntelliSys),7-8 Sept. 2017,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ROBOT.2004.1307521,Development and deployment of a line of sight virtual sensor for heterogeneous teams,IEEE,Conferences,"For a team of cooperating robots, geometry plays a vital role in operation. Knowledge of line of sight to local obstacles and adjacent teammates is critical in both the movement and planning stages to avoid collisions, maintain formation and localize the team. However, determining if other robots are within the line of sight of one another is difficult with existing sensor platforms - especially as the scale of the robot is reduced. We describe a method of exploiting collective team information to generate a virtual sensor that provides line of sight determination, greater range and resolution and the ability to generalize local sensing. We develop this sensor and apply it to the control of a tightly coupled, resource-limited robot team called Millibots.",https://ieeexplore.ieee.org/document/1307521/,"IEEE International Conference on Robotics and Automation, 2004. Proceedings. ICRA '04. 2004",26 April-1 May 2004,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IECON48115.2021.9589075,Development of Agricultural Robot Platform with Virtual Laboratory Capabilities,IEEE,Conferences,"Agricultural robots are called to help in many tasks in emerging clean and sustainable agriculture. These complex electro-mechanical systems can actually integrate artificial intelligence (AI), the Internet of Things (IoT), sensors, actuators, and advanced control methods to accomplish functions in autonomous or in collaborative ways. Before the deployment of such techniques in the field, it is convenient to carry out laboratory validations. These last could be at the sub-system, e.g., sensors or servos operation, or the whole system level. This paper proposes the development of the hardware and software parts of a platform of agricultural robot. The proposed system, highly motivated by the restrictions imposed by COVID-19 context, enables laboratory tests virtualization while keeping real-time functionalities",https://ieeexplore.ieee.org/document/9589075/,IECON 2021 – 47th Annual Conference of the IEEE Industrial Electronics Society,13-16 Oct. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICIS.2018.8466473,Development of a GPU-Based Human Emotion Recognition Robot Eye for Service Robot by Using Convolutional Neural Network,IEEE,Conferences,"Service robots can be used widely to assist elderly and disable population due to the lack of caregivers in future. Real-time human tracking, detection, focusing and implementing various algorithms are a wide range of application in emotion recognition service robots. Therefore service robots must have a properly designed robot eye model to be human-friendly with accurate human-robot interaction. Developed robot eye can be recognized the human emotional states by using well trained deep convolutional neural networks (ConvNet). This paper describes graphics processing units (GPUs) based human emotion recognition robot eye by using ConvNet. Mainly, the robot eye performs two processes in the intelligent systems. They are the robot eye focus to the human face and head by using pre-trained haar cascade classifier and recognizes the human emotional states probability with percentages as happy, sad or relaxes by using pre-trained ConvNet. The developed robot eye was implemented and tested by using different people successfully and the results of them are presented. According to the results, the emotions are detected more than 85% of overall accuracy for each person.",https://ieeexplore.ieee.org/document/8466473/,2018 IEEE/ACIS 17th International Conference on Computer and Information Science (ICIS),6-8 June 2018,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRAIE51050.2020.9358310,Development of a Neural Network Library for Resource Constrained Speech Synthesis,IEEE,Conferences,"Machine learning frameworks, like Tensorflow and PyTorch, use GPU hardware acceleration to deliver the needed performance. Since GPUs require a lot of power (and space) to operate, typical use cases involve high-performance servers, with the final deployment available as a cloud service. To address limitations of this approach, AI Accelerators have been proposed. In this context, we have designed and implemented a library of neural network algorithms, to efficiently run on “edge devices”, with AI Accelerators. Moreover, a unified interface has been provided, to allow easy experimentation with various neural networks applied to the same dataset. Here, let us stress that we do not propose new algorithms, but port known ones to, resource restricted, edge devices. The context is provided by a speech synthesis application for edge devices that is deployed on an NVIDIA Jetson Nano. This application is to be used by social robots for real-time off-cloud text-to-speech processing.",https://ieeexplore.ieee.org/document/9358310/,2020 5th IEEE International Conference on Recent Advances and Innovations in Engineering (ICRAIE),1-3 Dec. 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/FarEastCon.2018.8602651,Development of a Transport Robot for Automated Warehouses,IEEE,Conferences,"Industrial robots and manipulators are widely used as transport-loading devices in automated production. It is possible to combine equipment into coordinated production complexes of various sizes with the help of robots and they will not be bound by rigid planning and the number of installed units. Transport robots have proven themselves as flexible automated means of realizing intra-shop and interoperation material connections. More and more companies are developing technologies for vehicles through which they can communicate with each other and use real-time data from production infrastructure facilities. Electric vehicles and unmanned vehicles have become a new technological trend. In this regard, the paper deals with a prototype of an innovative transport robot, created for automated warehouses. It is proposed to use a computer vision system with image recognition based on the embedded software for the transport robot positioning inside the production facilities. The algorithm of deep machine learning was adapted to solve this problem. Using this algorithm, the prototype tests were performed successfully.",https://ieeexplore.ieee.org/document/8602651/,2018 International Multi-Conference on Industrial Engineering and Modern Technologies (FarEastCon),3-4 Oct. 2018,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICSSE.2011.5961891,Development of simulator for kid-sized humanoid soccer in RoboCup,IEEE,Conferences,"The robot soccer game system is a challenge for real-time control, which can be moderately abstracted from the standpoint of AI (Artificial Intelligence) and multi-agent systems. This simulator is developed for RoboCup Soccer Humanoid League. A humanoid robot belongs to highly intelligent system. The intelligent technologies of the humanoid robots include mechanism design, vision system, and algorithms in software programming. Therefore, its competitions can encourage creativity and technical development. To facilitate the strategy testing of these humanoid robot soccer competitions, a strategy simulator for kid-sized humanoid soccer in RoboCup is proposed. In this simulator, strategies compiled to DLL files may be explicitly loaded at run-time.",https://ieeexplore.ieee.org/document/5961891/,Proceedings 2011 International Conference on System Science and Engineering,8-10 June 2011,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CCDC.2019.8832952,Digital Implementation of the Spiking Neural Network and Its Digit Recognition,IEEE,Conferences,"Motivated by biological principles of neural systems, spiking neural network (SNN) shows a tremendous potential in solving pattern recognition and cognitive tasks in recent years. In this study, a biologically inspired SNN composed of three layers is implemented on a reconfigurable FPGA with high computational efficiency and low hardware cost. The proposed SNN is consists of spiking neurons simulated by leaky-integrate-and-fire neuron model. In addition, spiking-time-dependent-plasticity based on event-driven is utilized to train the constructed network. The real-time hardware realization of the proposed SNN demonstrates powerful and efficient learning scheme. Results on different datasets shows that the proposed SNN implementation has the merit of capability of coping with pattern recognition tasks. Furthermore, the proposed implementation with remarkable performance could be applied and embed in bio-inspired neuromorphic platform such as robots for recognition tasks and on-line applications.",https://ieeexplore.ieee.org/document/8832952/,2019 Chinese Control And Decision Conference (CCDC),3-5 June 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/I2CT54291.2022.9825379,Digitization of Chess Board and Prediction of Next Move,IEEE,Conferences,"In recent years digital technologies and gadgets have grown so much. Along with other things, the field of Artificial Intelligence has also grown dramatically. Computer vision is one of the most compelling types of AI, which everyone must have experienced in one or the other way like in OCR, Image Recognition, object detection, google lens, etc. One might even come to the assumption that given any image or video to a computer, the computer can understand what&#x2019;s going on in it and can comment a few things on it, just like WE HUMANS. But that assumption is not yet very true. It may be possible in the future but at the present these need some work to become reality. Leveraging this same motivation, we intend to give some contribution for our dream future to become reality. Recognizing and understanding any arbitrary chess board from the image, thereby digitizing it, is the thing that we will be achieving from the proposed system. Along with all that, giving a comment on the current state of chess board, thereby predicting the next optimal move in the game is another thing which is included in proposed system. Think of a human-like robot trying to play chess. Robots must have something like this in order to play the game. In the proposed research work digitization of Chess Board is done by server application from chess board image acquired by user. Current state of the board is generated and sent back as a response to client application. With the help of GUI, the chessboard is generated from the received Forsyth-Edwards Notation (FEN) on the client side. Depending upon blacks turn or whites turn next optimal move to be played is shown to the user. With our proposed system digital instance is generated with accuracy of over 90%. All blank cells of the board are generated with almost 100% accuracy.",https://ieeexplore.ieee.org/document/9825379/,2022 IEEE 7th International conference for Convergence in Technology (I2CT),7-9 April 2022,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS.2015.7353785,Distributed Particle Swarm Optimization - particle allocation and neighborhood topologies for the learning of cooperative robotic behaviors,IEEE,Conferences,"In this article we address the automatic synthesis of controllers for the coordinated movement of multiple mobile robots, as a canonical example of cooperative robotic behavior. We use five distributed noise-resistant variations of Particle Swarm Optimization (PSO) to learn in simulation a set of 50 weights of an artificial neural network. They differ on the way the particles are allocated and evaluated on the robots, and on how the PSO neighborhood is implemented. In addition, we use a centralized approach that allows for benchmarking with the distributed versions. Regardless of the learning approach, each robot measures locally and individually the performance of the group using exclusively on-board resources. Results show that four of the distributed variations obtain similar fitnesses as the centralized version, and are always able to learn. The other distributed variation fails to properly learn on some of the runs, and results in lower fitness when it succeeds. We test systematically the controllers learned in simulation in real robot experiments.",https://ieeexplore.ieee.org/document/7353785/,2015 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),28 Sept.-2 Oct. 2015,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/VDAT50263.2020.9190415,DynRP- Non-Intrusive Profiler for Dynamic Reconfigurability,IEEE,Conferences,"Emerging technological areas such as machine learning, speech recognition, computer vision, autonomous robots, AI, bioinformatics involving big data, require implementation in complex heterogeneous accelerator platforms, to be able to handle data explosion with higher efficiency, lower power, and better performance. Dynamic reconfiguration in such platforms can help in run-time optimization to meet the design goals. The required optimal platform configuration can be achieved by a flexible design space exploration and appropriate task partitioning obtained through profiling computation and communication of processes in application code. This paper focuses on profiling, it being the key to the success of obtaining optimal platform configurations. It points to existing profiling techniques, their pros and cons vis-à-vis dynamic reconfigurable architectures, and the challenges in their design for obtaining optimal profiling performance. It further outlines desirable specifications for a profiler to allow dynamic real-time profiling for effective use of dynamic reconfiguration. DynRP, a non-intrusive hardware profiler for dynamic reconfiguration is proposed based on the desirable specifications, followed by its design and implementation details.",https://ieeexplore.ieee.org/document/9190415/,2020 24th International Symposium on VLSI Design and Test (VDAT),23-25 July 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA48506.2021.9560730,Dynamic Object Aware LiDAR SLAM based on Automatic Generation of Training Data,IEEE,Conferences,"Highly dynamic environments, with moving objects such as cars or humans, can pose a performance challenge for LiDAR SLAM systems that assume largely static scenes. To overcome this challenge and support the deployment of robots in real world scenarios, we propose a complete solution for a dynamic object aware LiDAR SLAM algorithm. This is achieved by leveraging a real-time capable neural network that can detect dynamic objects, thus allowing our system to deal with them explicitly. To efficiently generate the necessary training data which is key to our approach, we present a novel end-to-end occupancy grid based pipeline that can automatically label a wide variety of arbitrary dynamic objects. Our solution can thus generalize to different environments without the need for expensive manual labeling and at the same time avoids assumptions about the presence of a predefined set of known objects in the scene. Using this technique, we automatically label over 12000 LiDAR scans collected in an urban environment with a large amount of pedestrians and use this data to train a neural network, achieving an average segmentation IoU of 0.82. We show that explicitly dealing with dynamic objects can improve the LiDAR SLAM odometry performance by 39.6% while yielding maps which better represent the environments. A supplementary video1 as well as our test data2 are available online.",https://ieeexplore.ieee.org/document/9560730/,2021 IEEE International Conference on Robotics and Automation (ICRA),30 May-5 June 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICPR.1992.201790,Dynamic neural estimation for autonomous vehicles driving,IEEE,Conferences,"Mobile robots and vehicles may be driven by dynamical neural networks which utilize image data of real-world scenes collected through a TV camera for learning and performance. An innovative system for road direction detection is proposed which is comprised of three specialized blocks performing edge extraction, image-segments detection, and road direction estimation. The road direction estimation block is implemented as a feedback neural network.<>",https://ieeexplore.ieee.org/document/201790/,"Proceedings., 11th IAPR International Conference on Pattern Recognition. Vol.II. Conference B: Pattern Recognition Methodology and Systems",30 Aug.-3 Sept. 1992,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/EH.2001.937964,Early experiments on the CAM-Brain Machine (CBM),IEEE,Conferences,"This paper presents results of some of the first evolution experiments undertaken on an actual CAM-Brain Machine (CBM), using the hardware itself and not software simulations. A CBM is a specialised piece of programmable (evolvable) hardware that uses Xilinx XC6264 programmable FPGA chips to grow and evolve, at electronic speeds, 3D cellular automata (CA) based neural network circuit modules of some 1000 neurons each. A complete run of a genetic algorithm (e.g. with 100 generations and a population size of 100) is executed in a few seconds. 64000 of these modules can be evolved separately according to the fitness definitions of human ""EEs"" (evolutionary engineers) and downloaded one by one into a gigabyte of RAM. Human ""BAs"" (brain architects) then interconnect these modules ""by hand"" according to their artificial brain architectures. The CBM then updates the binary neural signaling of the artificial brain (with 64000 ""hand"" interconnected modules, i.e. 75 million neurons) at a rate of 130 billion CA cell updates a second, which is fast enough for real time control of robots. Before such multi-moduled artificial brains can be constructed. It is essential that the quality of the evolution (the ""evolvability"") of individual modules be adequate. This paper reports on the first evolution results obtained on CBM hardware.",https://ieeexplore.ieee.org/document/937964/,Proceedings Third NASA/DoD Workshop on Evolvable Hardware. EH-2001,12-14 July 2001,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SNPD.2008.97,Early-Life Cycle Reuse Approach for Component-Based Software of Autonomous Mobile Robot System,IEEE,Conferences,"Applying software reuse to many embedded realtime systems, such as autonomous mobile robot system poses significant challenges to industrial software processes due to the resource-constrained and realtime requirements of the systems. An approach for early life-cycle systematic reuse for component-based software engineering (ELCRA) of autonomous mobile robot software is developed. The approach allows reuse at the early stage of software development process by integrating analysis patterns, component model, and component-oriented programming framework. The results of applying the approach in developing software for real robots show that the strategies and processes proposed in the approach can fulfill requirements for self-contained, platform-independent and real-time predictable mobile robot.",https://ieeexplore.ieee.org/document/4617381/,"2008 Ninth ACIS International Conference on Software Engineering, Artificial Intelligence, Networking, and Parallel/Distributed Computing",6-8 Aug. 2008,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CSCloud-EdgeCom49738.2020.00050,Edge Computing-based 3D Pose Estimation and Calibration for Robot Arms,IEEE,Conferences,"Industrial robots are widely used in current production lines, and complex pipeline processes, especially those with different assembly requirements, are designed for intelligent manufacturing in the era of industry 4.0. During the new crown epidemic, a large number of car companies used the production line to transform production of medical materials such as masks and protective clothing, which provided a strong guarantee for fighting the epidemic. In this scenario, a pipeline is often assembled from robotic arms from multiple suppliers. The traditional methods is complex and takes a lot of time. In this paper, we propose a novel deep learning based robot arm 3D pose estimation and calibration model with simple Kinect stereo cameras which can be deployed on light-weight edge computing systems. The light-weight deep CNN model can detection 5 predefined key points based on RGB-D data. In this way, when the assembly line composed of different robot arms needs to be reassembled, our model can quickly provide the robot’s pose information without additional tuning processes. Testing in Webots with Rokae xb4 robot arm model shows that our model can quickly estimate the key point of the robot arm.",https://ieeexplore.ieee.org/document/9170983/,2020 7th IEEE International Conference on Cyber Security and Cloud Computing (CSCloud)/2020 6th IEEE International Conference on Edge Computing and Scalable Cloud (EdgeCom),1-3 Aug. 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/RCAR.2018.8621810,Efficient and Low-Cost Deep-Learning Based Gaze Estimator for Surgical Robot Control,IEEE,Conferences,"Surgical robots are playing more and more important role in modern operating room. However, operations by using surgical robot are not easy to handle by doctors. Vision based human-computer interaction (HCI) is a way to ease the difficulty to control surgical robots. While the problem of this method is that eyes tracking devices are expensive. In this paper, a low cost and robust deep-learning based on gaze estimator is proposed to control surgical robots. By this method, doctors can easily control the robot by specifying the starting point and ending point of the surgical robot using eye gazing. Surgical robots can also be controlled to move in 9 directions using controllers' eyes gazing information. A Densely Connected convolutional Neural Networks (Dense CNN) model for 9-direction/36-direction gaze estimation is built. The Dense CNN architecture has much more less trainable parameters compared to traditional CNN network architecture (AlexNet like/VGG like) which is more feasible to deploy on the Field-Programmable Gate Array (FPGA) and other hardware with limited memories.",https://ieeexplore.ieee.org/document/8621810/,2018 IEEE International Conference on Real-time Computing and Robotics (RCAR),1-5 Aug. 2018,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS.2012.6385832,Elastic strips: Implementation on a physical humanoid robot,IEEE,Conferences,"For robots to operate in human environments, they are required to react safely to unexpected changes in the work area. However, existing manipulation task planning methods take more than several seconds or minutes to update their solutions when environmental changes are recognized. Furthermore, the computation time exponentially increases in case of highly complex structures such as humanoid robots. Therefore, we propose a reactive system for high d.o.f. robots to perform interactive manipulation tasks under real-time conditions. The paper describes the implementation of the Elastic Strip Framework, a plan modification approach to update initial motion plans. To improve its real-time performance and reliability, the previous geometric approximation is replaced by an implicit method that constructs an elastic tunnel for collision checking. Additionally, in order to maintain a robust system even in exceptional situations, such as undetected obstacles, the force transformer module executes compliant motions, and the current elastic strip adapts the path tracking motion by monitoring tracking errors of the actual motion. The proposed system is applied to a Honda humanoid robot. Real-time performance is successfully demonstrated in real-world experiments.",https://ieeexplore.ieee.org/document/6385832/,2012 IEEE/RSJ International Conference on Intelligent Robots and Systems,7-12 Oct. 2012,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SII.2019.8700376,Emotion Recognition from Speech for an Interactive Robot Agent,IEEE,Conferences,"Speech is one of the fundamental approaches for human to human interaction. Given this, it should be the main approach for robot human interaction as well. Towards this, the research presented here focuses on emotion recognition from human speech to aid the interaction between humans and robots. There are various steps involved in developing emotion recognition system for an interactive robot agent. The first step is to choose a suitable dataset that is Berlin database for training and testing the models developed. The second important step is extraction and choice of suitable features related to emotions. The third step is to make an appropriate classification scheme. The performance of each classifier is analyzed and acomparison among multiple frameworks of emotion recognition is made. In response to the findings in these preliminary studies, a prototype application was developed to allow the recognition of emotions from speech in real-time for future use on an interactive robot. On a preliminary test set, the application achieved performance levels between 81% and 92%. The approach offers the integration of speech collection hardware, emotion recognition software, mobile devices and robotic systems to aid assist human-robot interaction.",https://ieeexplore.ieee.org/document/8700376/,2019 IEEE/SICE International Symposium on System Integration (SII),14-16 Jan. 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/RO-MAN46459.2019.8956327,End-User Programming of Low-and High-Level Actions for Robotic Task Planning,IEEE,Conferences,"Programming robots for general purpose applications is extremely challenging due to the great diversity of end-user tasks ranging from manufacturing environments to personal homes. Recent work has focused on enabling end-users to program robots using Programming by Demonstration. However, teaching robots new actions from scratch that can be reused for unseen tasks remains a difficult challenge and is generally left up to robotic experts. We propose iRoPro, an interactive Robot Programming framework that allows end-users to teach robots new actions from scratch and reuse them with a task planner. In this work we provide a system implementation on a two-armed Baxter robot that (i) allows simultaneous teaching of low-and high-level actions by demonstration, (ii) includes a user interface for action creation with condition inference and modification, and (iii) allows creating and solving previously unseen problems using a task planner for the robot to execute in real-time. We evaluate the generalisation power of the system on six benchmark tasks and show how taught actions can be easily reused for complex tasks. We further demonstrate its usability with a user study (N=21), where users completed eight tasks to teach the robot new actions that are reused with a task planner. The study demonstrates that users with any programming level and educational background can easily learn and use the system.",https://ieeexplore.ieee.org/document/8956327/,2019 28th IEEE International Conference on Robot and Human Interactive Communication (RO-MAN),14-18 Oct. 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCE.2018.8326229,End-to-end deep learning for autonomous navigation of mobile robot,IEEE,Conferences,"This paper proposes an end-to-end method for training convolutional neural networks for autonomous navigation of a mobile robot. Traditional approach for robot navigation consists of three steps. The first step is extracting visual features from the scene using the camera input. The second step is to figure out the current position by using a classifier on the extracted visual features. The last step is making a rule for moving the direction manually or training a model to handle the direction. In contrast to the traditional multi-step method, the proposed visuo-motor navigation system can directly output the linear and angular velocities of the robot from an input image in a single step. The trained model gives wheel velocities for navigation as outputs in real-time making it possible to be implanted on mobile robots such as robotic vacuum cleaner. The experimental results show an average linear velocity error of 2.2 cm/s and average angular velocity error of 3.03 degree/s. The robot deployed with the proposed model can navigate in a real-world environment by only using the camera without relying on any other sensors such as LiDAR, Radar, IR, GPS, IMU.",https://ieeexplore.ieee.org/document/8326229/,2018 IEEE International Conference on Consumer Electronics (ICCE),12-14 Jan. 2018,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SII52469.2022.9708826,Evaluation of Variable Impedance- and Hybrid Force/MotionControllers for Learning Force Tracking Skills,IEEE,Conferences,"For robots to perform real-world force interaction tasks with human level dexterity, it is crucial to develop adaptable and compliant force controllers. Learning techniques, especially reinforcement learning, provide a platform to develop adaptable controllers for complex robotic tasks. This paper presents an evaluation of two prominent force control methods, variable impedance control and hybrid force-motion control in a robot learning framework. The controllers are evaluated on a Franka Emika Panda robotic manipulator for a robotic interaction task demanding force and motion tracking using a model-based reinforcement learning algorithm, PILCO. Utilizing the learning framework to find the optimal controller parameters has significantly improved the performance of the controllers. The implementation of the controllers integrated with the robot learning framework is available on https://github.com/martihmy/Compliant_control.",https://ieeexplore.ieee.org/document/9708826/,2022 IEEE/SICE International Symposium on System Integration (SII),9-12 Jan. 2022,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CEC45853.2021.9504892,Evolutionary Inherited Neuromodulated Neurocontrollers with Objective Weighted Ranking,IEEE,Conferences,"In the physical world, individuals compete against others within their own population or separately evolving populations. Robotic agents will soon face this coevolutionary and adversarial reality. Many challenges not encountered in the evolution of single populations are encountered in adversarial coevolution. These challenges can render single evolutionary approaches either less effective or ineffective. Most problems have a fundamental goal, but also feature secondary desirable objectives. Here, evader agents in the pursuit-evasion game that do not elude capture are effectively useless. Secondly, when applied to evolve real robots online in a coevolutionary context, non-optimal robots can be erratic and cause damage. Objective hierarchy can be used to define the importance of each objective, and promote quicker optimization of primary objectives such as evasion. The Evolutionary Inherited Neuromodulated Neurocontroller (EINN) method incorporates objective weighted ranking (OWR), a novel objective hierarchy method that promotes optimization of the primary objective in simultaneous multi-objective optimization. EINN is compared to the previously demonstrated Lamarckian-inherited Neuromodulated MultiObjective Evolutionary Neurocontroller (LNMOEN), and shown to be effective in a single evolutionary context.",https://ieeexplore.ieee.org/document/9504892/,2021 IEEE Congress on Evolutionary Computation (CEC),28 June-1 July 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CAIA.1989.49141,Experiences with the subsumption architecture,IEEE,Conferences,"A subsumption architecture has been proposed as an effective approach for the construction of robust, real-time control systems for mobile robots. To investigate its strengths and weaknesses, a simulation of the architecture was developed called the Subsumption Architecture Tool (SAT). This simulation allows various models of system behavior to be quickly built and tested. During the building and testing of the SAT, issues related to some architectural features became evident: level of commitment of each layer; code redundancy; problem decomposition and programming style; complexity of large system; and abstract reasoning capabilities. The effects of these issues are presented with respect to the design and implementation choices of two sample layers of behavior. These layers are used to illustrate considerations that need to be taken into account when a project team is considering the use of the subsumption architecture or when a subsumption-architecture-based system is being designed and implemented.<>",https://ieeexplore.ieee.org/document/49141/,[1989] Proceedings. The Fifth Conference on Artificial Intelligence Applications,6-10 March 1989,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS51168.2021.9636594,Explaining the Decisions of Deep Policy Networks for Robotic Manipulations,IEEE,Conferences,"Deep policy networks enable robots to learn behaviors to solve various real-world complex tasks in an end-to-end fashion. However, they lack transparency to provide the reasons of actions. Thus, such a black-box model often results in low reliability and disruptive actions during the deployment of the robot in practice. To enhance its transparency, it is important to explain robot behaviors by considering the extent to which each input feature contributes to determining a given action. In this paper, we present an explicit analysis of deep policy models through input attribution methods to explain how and to what extent each input feature affects the decisions of the robot policy models. To this end, we present two methods for applying input attribution methods to robot policy networks: (1) we measure the importance factor of each joint torque to re ect the influence of the motor torque on the end-effector movement, and (2) we modify a relevance propagation method to handle negative inputs and outputs in deep policy networks properly. To the best of our knowledge, this is the first report to identify the dynamic changes of input attributions of multi-modal sensor inputs in deep policy networks online for robotic manipulation.",https://ieeexplore.ieee.org/document/9636594/,2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),27 Sept.-1 Oct. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA.2019.8793829,Exploiting Trademark Databases for Robotic Object Fetching,IEEE,Conferences,"Service robots require the ability to recognize various household objects in order to carry out certain tasks, such as fetching an object for a person. Manually collecting information on all the objects a robot may encounter in a household is tedious and time-consuming; therefore this paper proposes the use of large-scale data from existing trademark databases. These databases contain logo images and a description of the goods and services the logo was registered under. For example, Pepsi is registered under soft drinks. We extend domain randomization in order to generate synthetic data to train a convolutional neural network logo detector, which outperformed previous logo detectors trained on synthetic data. We also provide a practical implementation for object fetching on a robot, which uses a Kinect and the logo detector to identify the object the human user requested. Tests on this robot indicate promising results, despite not using any real world photos for training.",https://ieeexplore.ieee.org/document/8793829/,2019 International Conference on Robotics and Automation (ICRA),20-24 May 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/RCAR52367.2021.9517650,FT-MSTC: An Efficient Fault Tolerance Algorithm for Multi-robot Coverage Path Planning,IEEE,Conferences,"Fault tolerance is very important for multi-robot systems, especially for those operated in remote environments. The ability to tolerate failures, allows robots effectively to continue performing tasks without the need for immediate human intervention. In this paper, we present a new efficient fault tolerance algorithm for multi-robot coverage path planning (mCPP). The entire coverage path is considered as a topological task loop. The ideal mCPP problem is handled by partitioning this task loop and assign each partition to individual robot. When a faulty robot is detected, we use an optimization method to minimize the overall maximum coverage cost while considering both the tasks accomplished before robot failures and the remaining tasks. We perform various experiments for regular grid maps and real field terrains. We compare our algorithm against other coverage path planning algorithms and our algorithm outperforms existing spiral-STC-based methods in terms of the overall maximum coverage cost.",https://ieeexplore.ieee.org/document/9517650/,2021 IEEE International Conference on Real-time Computing and Robotics (RCAR),15-19 July 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICMLC.2004.1380629,Fault tolerance for communication-based multirobot formation,IEEE,Conferences,"This paper investigates the ability of fault tolerance for multirobot formation, which is important for practical formation in complex environment. Our model enables mobile robots group to continue to complete given tasks by reorganizing their formation, when some members are in failure. First, to build such model, a multi-agent architecture is presented, which is implemented through communication. Second, we introduce the hierarchy graph of multirobot formation to be the theoretical foundation of the fault tolerance system. The graph analysis is suitable for general leader-follower formation format. And then, the failure detection mechanism for formation is discussed. Finally, integrated fault tolerance algorithm is investigated, including supplement for faulty robots and formation reconfiguration. The improved agent architecture adding the fault tolerance module is also presented. The experiments on real multiple mobile robots demonstrate our design is feasible.",https://ieeexplore.ieee.org/document/1380629/,Proceedings of 2004 International Conference on Machine Learning and Cybernetics (IEEE Cat. No.04EX826),26-29 Aug. 2004,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CYBER53097.2021.9588329,Fault-Aware Robust Control via Adversarial Reinforcement Learning,IEEE,Conferences,"Robots have limited adaptation ability compared to humans and animals in the case of damage. However, robot damages are prevalent in realworld applications, especially for robots deployed in extreme environments. The fragility of robots greatly limits their widespread application. We propose an adversarial reinforcement learning framework, which significantly increases robot robustness over joint damage cases in both manipulation tasks and locomotion tasks. The agent is trained iteratively under the joint damage cases where it has poor performance. We validate our algorithm on a three-fingered robot hand and a quadruped robot. Our algorithm can be trained only in simulation and directly deployed on a real robot without any fine-tuning. It also demonstrates exceeding success rates over arbitrary joint damage cases.",https://ieeexplore.ieee.org/document/9588329/,"2021 IEEE 11th Annual International Conference on CYBER Technology in Automation, Control, and Intelligent Systems (CYBER)",27-31 July 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICARSC52212.2021.9429801,Few-Shot Visual Grounding for Natural Human-Robot Interaction,IEEE,Conferences,"Natural Human-Robot Interaction (HRI) is one of the key components for service robots to be able to work in human-centric environments. In such dynamic environments, the robot needs to understand the intention of the user to accomplish a task successfully. Towards addressing this point, we propose a software architecture that segments a target object from a crowded scene, indicated verbally by a human user. At the core of our system, we employ a multi-modal deep neural network for visual grounding. Unlike most grounding methods that tackle the challenge using pre-trained object detectors via a two-stepped process, we develop a single stage zero-shot model that is able to provide predictions in unseen data. We evaluate the performance of the proposed model on real RGB-D data collected from public scene datasets. Experimental results showed that the proposed model performs well in terms of accuracy and speed, while showcasing robustness to variation in the natural language input.",https://ieeexplore.ieee.org/document/9429801/,2021 IEEE International Conference on Autonomous Robot Systems and Competitions (ICARSC),28-29 April 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/RIOS.2013.6595317,Flexible snake robot: Design and implementation,IEEE,Conferences,"This paper presents a snake robot able to pass different and difficult paths because of special physical form and movement joints mechanism. These snake robots have no passive wheels. The robot moves by friction between the robot body and the surface on which it is. The joints have been designed and fabricated in a way that each joint has two freedom grades and it may move 228 degrees in every direction. Each joint has two DC servo motors and the power is transferred from the motors output to the joint shaft through bevel gear. The flexibility of the robot makes possible to move forward, back and laterally by imitating real snake's moves. In this paper different measures have been presented in order to design and assemble the joints, motors driver, different ways to guide the robot and its vision.",https://ieeexplore.ieee.org/document/6595317/,2013 3rd Joint Conference of AI & Robotics and 5th RoboCup Iran Open International Symposium,8-8 April 2013,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/VR.2015.7223421,Flying robot manipulation system using a virtual plane,IEEE,Conferences,"The flexible movements of flying robots make it difficult for novices to manipulate them precisely with controllers such as a joystick and a proportional radio system. Moreover, the mapping of instructions between a robot and its reactions is not necessarily intuitive for users. We propose manipulation methods for flying robots using augmented reality technologies. In the proposed system, a virtual plane is superimposed on a flying robot and users control the robot by manipulating the virtual plane and drawing a moving path on it. We present the design and implementation of our system and describe experiments conducted to evaluate our methods.",https://ieeexplore.ieee.org/document/7223421/,2015 IEEE Virtual Reality (VR),23-27 March 2015,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS.2016.7759701,From indoor GIS maps to path planning for autonomous wheelchairs,IEEE,Conferences,"This work focuses on how to compute trajectories for an autonomous wheelchair based on indoor GIS maps, in particular on IndoorGML maps, which set the standard in this context. Good wheelchair trajectories are safe and comfortable for the user and the people sharing the space with him, turn gently, are high legible, and smooth (at least G2 continuos). We derive a navigation graph from a given IndoorGML map. We define and solve an optimization problem to find the desired path: given a succession of cells to traverse, the path corresponds to the best composite Bézier trajectory for the wheelchair. We discuss a related multi-objective path planning problem. Experimental results and an implementation on real robots show the planner performance.",https://ieeexplore.ieee.org/document/7759701/,2016 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),9-14 Oct. 2016,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCI-CC.2014.6921432,From information revolution to intelligence revolution: Big data science vs. intelligence science,IEEE,Conferences,"The hierarchy of human knowledge is categorized at the levels of data, information, knowledge, and intelligence. For instance, given an AND-gate with 1,000-input pins, it may be described very much differently at various levels of perceptions in the knowledge hierarchy. At the data level on the bottom, it represents a 21,000 state space, known as `big data' in recent terms, which appears to be a big issue in engineering. However, at the information level, it just represents 1,000 bit information that is equivalent to the numbers of inputs. Further, at the knowledge level, it expresses only two rules that if all inputs are one, the output is one; and if any input is zero, the output is zero. Ultimately, at the intelligence level, it is simply an instance of the logical model of an AND-gate with arbitrary inputs. This problem reveals that human intelligence and wisdom are an extremely efficient and a fast convergent induction mechanism for knowledge and wisdom elicitation and abstraction where data are merely factual materials and arbitrary instances in the almost infinite state space of the real world. Although data and information processing have been relatively well studied, the nature, theories, and suitable mathematics underpinning knowledge and intelligence are yet to be systematically studied in cognitive informatics and cognitive computing. This will leads to a new era of human intelligence revolution following the industrial, computational, and information revolutions. This is also in accordance with the driving force of the hierarchical human needs from low-level material requirements to high-level ones such as knowledge, wisdom, and intelligence. The trend to the emerging intelligent revolution is to meet the ultimate human needs. The basic approach to intelligent revolution is to invent and embody cognitive computers, cognitive robots, and cognitive systems that extend human memory capacity, learning ability, wisdom, and creativity. Via intelligence revolution, an interconnected cognitive intelligent Internet will enable ordinary people to access highly intelligent systems created based on the latest development of human knowledge and wisdom. Highly professional systems may help people to solve typical everyday problems. Towards these objectives, the latest advances in abstract intelligence and intelligence science investigated in cognitive informatics and cognitive computing are well positioned at the center of intelligence revolution. A wide range of applications of cognitive computers have been developing in ICIC [http://www.ucalgary.ca/icic/] such as, inter alia, cognitive computers, cognitive robots, cognitive learning engines, cognitive Internet, cognitive agents, cognitive search engines, cognitive translators, cognitive control systems, cognitive communications systems, and cognitive automobiles.",https://ieeexplore.ieee.org/document/6921432/,2014 IEEE 13th International Conference on Cognitive Informatics and Cognitive Computing,18-20 Aug. 2014,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CDC40024.2019.9029916,From self-tuning regulators to reinforcement learning and back again,IEEE,Conferences,"Machine and reinforcement learning (RL) are increasingly being applied to plan and control the behavior of autonomous systems interacting with the physical world. Examples include self-driving vehicles, distributed sensor networks, and agile robots. However, when machine learning is to be applied in these new settings, the algorithms had better come with the same type of reliability, robustness, and safety bounds that are hallmarks of control theory, or failures could be catastrophic. Thus, as learning algorithms are increasingly and more aggressively deployed in safety critical settings, it is imperative that control theorists join the conversation. The goal of this tutorial paper is to provide a starting point for control theorists wishing to work on learning related problems, by covering recent advances bridging learning and control theory, and by placing these results within an appropriate historical context of system identification and adaptive control.",https://ieeexplore.ieee.org/document/9029916/,2019 IEEE 58th Conference on Decision and Control (CDC),11-13 Dec. 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICSAI.2017.8248355,Fully convolutional denoising autoencoder for 3D scene reconstruction from a single depth image,IEEE,Conferences,"In this work, we propose a 3D scene reconstruction algorithm based on a fully convolutional 3D denoising autoencoder neural network. The network is capable of reconstructing a full scene from a single depth image by creating a 3D representation of it and automatically filling holes and inserting hidden elements. We exploit the fact that our neural network is capable of generalizing object shapes by inferring similarities in geometry. Our fully convolutional architecture enables the network to be unconstrained by a fixed 3D shape, and so it is capable of successfully reconstructing arbitrary scene sizes. Our algorithm was evaluated on a real word dataset of tabletop scenes acquired using a Kinect and processed using KinectFusion software in order to obtain ground truth for network training and evaluation. Extensive measurements show that our deep neural network architecture outperforms the previous state of the art both in terms of precision and recall for the scene reconstruction task. The network has been broadly profiled in terms of memory footprint, number of floating point operations, inference time and power consumption in CPU, GPU and embedded devices. Its small memory footprint and its low computation requirements enable low power, memory constrained, real time always-on embedded applications such as autonomous vehicles, warehouse robots, interactive gaming controllers and drones.",https://ieeexplore.ieee.org/document/8248355/,2017 4th International Conference on Systems and Informatics (ICSAI),11-13 Nov. 2017,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/FUZZY.2006.1681996,Fuzzy Logic based Active Map Learning for Autonomous Robot,IEEE,Conferences,"The paper proposes a fast map learning approach for real-time map building and active exploration in unknown indoor environments. This approach includes a map model, a map update method, an exploration method, and a map postprocessing method. The map adopts a grid-based representation and uses frequency value to measure the confidence that a cell is occupied by an obstacle. The exploration method is implemented by coordinating two novel behaviors: path-exploring behavior and environment-detection behavior. Fuzzy logic is used to implement the behavior design and coordination. The fast map update and path planning (i.e. the exploration method) make our approach a candidate for real-time implementation on mobile robots. The results are demonstrated by simulated experiments based on a Pioneer robot with eight forward sonar sensors.",https://ieeexplore.ieee.org/document/1681996/,2006 IEEE International Conference on Fuzzy Systems,16-21 July 2006,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ROMAN.2004.1374845,Fuzzy reinforcement learning for an evolving virtual servant robot,IEEE,Conferences,"This work presents our research in the application of reinforcement learning algorithms for the generation of autonomous intelligent virtual robots, that can learn and enhance their task performance in assisting humans in housekeeping. For the control system architecture of the virtual agents, two algorithms, based on Watkins' Q(/spl lambda/) learning and the zeroth-level classifier system (ZLCS), are incorporated with fuzzy inference systems(FlS). Performance of these algorithms is evaluated and compared. A 3D application of a virtual robot whose task is to interact with virtual humans and offer optimal services on everyday in-house needs is designed and implemented. The learning systems are incorporated in the decision-making process of the virtual robot servant to allow itself to understand and evaluate the fuzzy value requirements and enhance its performance.",https://ieeexplore.ieee.org/document/1374845/,RO-MAN 2004. 13th IEEE International Workshop on Robot and Human Interactive Communication (IEEE Catalog No.04TH8759),22-22 Sept. 2004,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS40897.2019.8967785,GQ-STN: Optimizing One-Shot Grasp Detection based on Robustness Classifier,IEEE,Conferences,"Grasping is a fundamental robotic task needed for the deployment of household robots or furthering warehouse automation. However, few approaches are able to perform grasp detection in real time (frame rate). To this effect, we present Grasp Quality Spatial Transformer Network (GQ-STN), a one-shot grasp detection network. Being based on the Spatial Transformer Network (STN), it produces not only a grasp configuration, but also directly outputs a depth image centered at this configuration. By connecting our architecture to an externally-trained grasp robustness evaluation network, we can train efficiently to satisfy a robustness metric via the backpropagation of the gradient emanating from the evaluation network. This removes the difficulty of training detection networks on sparsely annotated databases, a common issue in grasping. We further propose to use this robustness classifier to compare approaches, being more reliable than the traditional rectangle metric. Our GQ-STN is able to detect robust grasps on the depth images of the Dex-Net 2.0 dataset with 92.4 % accuracy in a single pass of the network. We finally demonstrate in a physical benchmark that our method can propose robust grasps more often than previous sampling-based methods, while being more than 60 times faster.",https://ieeexplore.ieee.org/document/8967785/,2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),3-8 Nov. 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICMA.2007.4303585,Genetic Programming in Robot Exploration,IEEE,Conferences,"Exploration using mobile robots is an active research area. In general, an optimal robot exploration strategy is difficult to obtain. In this paper an investigation is conducted using genetic programming (GP) to solve this problem. GP is a form of artificial intelligence capable of automatically creating and developing computer programs to solve problems using the theory of evolution. However, like many other learning algorithms, GP is a computationally expensive and time-consuming process. This characteristic can impede its application where learning time is limited, such as in real-time robotic control applications. Therefore, this paper further investigates the possibility of developing a time-efficient GP algorithm to reduce evolution time. This is done by directly incorporating the amount of time evolved solutions take to form into the fitness function, in order to encourage time efficient problem solving. Experimental results have shown that while the time efficient aspect of the proposed GP algorithm is not conclusive, the robot exploration using GP produces promising outcomes.",https://ieeexplore.ieee.org/document/4303585/,2007 International Conference on Mechatronics and Automation,5-8 Aug. 2007,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CVPR.2018.00945,Gibson Env: Real-World Perception for Embodied Agents,IEEE,Conferences,"Developing visual perception models for active agents and sensorimotor control in the physical world are cumbersome as existing algorithms are too slow to efficiently learn in real-time and robots are fragile and costly. This has given rise to learning-in-simulation which consequently casts a question on whether the results transfer to real-world. In this paper, we investigate developing real-world perception for active agents, propose Gibson Environment for this purpose, and showcase a set of perceptual tasks learned therein. Gibson is based upon virtualizing real spaces, rather than artificially designed ones, and currently includes over 1400 floor spaces from 572 full buildings. The main characteristics of Gibson are: I. being from the real-world and reflecting its semantic complexity, II. having an internal synthesis mechanism ""Goggles"" enabling deploying the trained models in real-world without needing domain adaptation, III. embodiment of agents and making them subject to constraints of physics and space.",https://ieeexplore.ieee.org/document/8579043/,2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition,18-23 June 2018,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CACRE52464.2021.9501291,Give Me a Wrench!: Finding Tools for Human Partners in Human-Robot Collaborative Manufacturing Contexts,IEEE,Conferences,"Manufacturing processes can be optimized by enabling human-robot collaboration. A relevant goal in this area is to create a collaborative solution in which robots can provide assisting actions to humans, thereby, reducing menial labor as well as increasing productivity. The solution is based on implementing efficient hand-over of mechanical tools from robots to humans. Hand-over tasks are inevitable in human-robot collaborative manufacturing contexts. These tasks need three-step mechanism: object identification, object grasping, and the actual hand-over. This paper presents an approach for robots to find tools for human partners in human-robot collaboration via deep learning. This is achieved using the object detection system YOLOv3 for identification of commonly used mechanical tools. By training on a custom dataset of 800 images of mechanical tools created for the study, the tool recognition is implemented in realworld human-robot hand-over tasks. Experimental results show that the proposed approach achieves a high accuracy for identification of tools in real-world human-robot collaboration. Future work of this study is also discussed.",https://ieeexplore.ieee.org/document/9501291/,"2021 6th International Conference on Automation, Control and Robotics Engineering (CACRE)",15-17 July 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/COMPSAC54236.2022.00187,Grasp Position Estimation from Depth Image Using Stacked Hourglass Network Structure,IEEE,Conferences,"In recent years, robots have been used not only in factories. However, most robots currently used in such places can only perform the actions programmed to perform in a predefined space. For robots to become widespread in the future, not only in factories, distribution warehouses, and other places but also in homes and other environments where robots receive complex commands and their surroundings are constantly being updated, it is necessary to make robots intelligent. Therefore, this study proposed a deep learning grasp position estimation model using depth images to achieve intelligence in pick-and-place. This study used only depth images as the training data to build the deep learning model. Some previous studies have used RGB images and depth images. However, in this study, we used only depth images as training data because we expect the inference to be based on the object&#x0027;s shape, independent of the color information of the object. By performing inference based on the target object&#x0027;s shape, the deep learning model is expected to minimize the need for re-training when the target object package changes in the production line since it is not dependent on the RGB image. In this study, we propose a deep learning model that focuses on the stacked encoder-decoder structure of the Stacked Hourglass Network. We compared the proposed method with the baseline method in the same evaluation metrics and a real robot, which shows higher accuracy than other methods in previous studies.",https://ieeexplore.ieee.org/document/9842726/,"2022 IEEE 46th Annual Computers, Software, and Applications Conference (COMPSAC)",27 June-1 July 2022,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/RO-MAN47096.2020.9223558,HATSUKI : An anime character like robot figure platform with anime-style expressions and imitation learning based action generation,IEEE,Conferences,"Japanese character figurines are popular and have a pivot position in Otaku culture. Although numerous robots have been developed, few have focused on otaku-culture or on embodying anime character figurines. Therefore, we take the first steps to bridge this gap by developing Hatsuki, which is a humanoid robot platform with anime based design. Hatsuki’s novelty lies in its aesthetic design, 2D facial expressions, and anime-style behaviors that allows Hatsuki to deliver rich interaction experiences resembling anime-characters. We explain our design implementation process of Hatsuki, followed by our evaluations. In order to explore user impressions and opinions towards Hatsuki, we conducted a questionnaire in the world’s largest anime-figurine event. The results indicate that participants were generally very satisfied with Hatsuki’s design, and proposed various use case scenarios and deployment contexts for Hatsuki. The second evaluation focused on imitation learning, as such a method can provide better interaction ability in the real world and generate rich, context-adaptive behaviors in different situations. We made Hatsuki learn 11 actions, combining voice, facial expressions and motions, through the neural network based policy model with our proposed interface. Results show our approach was successfully able to generate the actions through self-organized contexts, which shows the potential for generalizing our approach in further actions under different contexts. Lastly, we present our future research direction for Hatsuki and provide our conclusion.",https://ieeexplore.ieee.org/document/9223558/,2020 29th IEEE International Conference on Robot and Human Interactive Communication (RO-MAN),31 Aug.-4 Sept. 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICIEA.2006.257252,Hand Posture Recognition in Gesture-Based Human-Robot Interaction,IEEE,Conferences,"Natural and friendly interface is critical for the development of service robots. Gesture-based interface offers a way to enable untrained users to interact with robots more easily and efficiently. In this paper, we present a posture recognition system implemented on a real humanoid service robot. The system applies the RCE neural network based color segmentation algorithm to separate hand images from complex backgrounds. The topological features of the hand are then extracted from the silhouette of the segmented hand region. Based on the analysis of these simple but distinctive features, hand postures are identified accurately. Experimental results on gesture-based robot programming demonstrated the effectiveness and robustness of the system",https://ieeexplore.ieee.org/document/4025853/,2006 1ST IEEE Conference on Industrial Electronics and Applications,24-26 May 2006,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS.2008.4651150,High-dimensional underactuated motion planning via task space control,IEEE,Conferences,"Kinodynamic planning algorithms have the potential to find feasible control trajectories which accomplish a task even in very nonlinear or constrained dynamical systems. Underactuation represents a particular form of a dynamic constraint, inherently present in many machines of interest (e.g., walking robots), and necessitates planning for long-term control solutions. A major limitation in motion planning techniques, especially for real-time implementation, is that they are only practical for relatively low degree-of-freedom problems. Here we present a model-based dimensionality reduction technique based on an extension of partial feedback linearization control into a task-space framework. This allows one to plan motions for a complex underactuated robot directly in a low-dimensional task-space, and to resolve redundancy with lower-priority tasks. We illustrate the potential of this approach with an extremely simple motion planning system which solves the swing-up problem for multi-link underactuated pendula, and discuss extensions to the control of walking.",https://ieeexplore.ieee.org/document/4651150/,2008 IEEE/RSJ International Conference on Intelligent Robots and Systems,22-26 Sept. 2008,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ACSOS49614.2020.00036,How far should I watch? Quantifying the effect of various observational capabilities on long-range situational awareness in multi-robot teams,IEEE,Conferences,"In our previous work, we showed that individual robots within a multi-robot team can gain long-distance situational awareness from passive observations of a single nearby neighbor without any explicit robot-to-robot communication. However, that prior work was developed only in simulation, and performance was not measured for real robot teams in physical space with realistic hardware limitations. Toward this end, we studied the performance of these methods in real robot scenarios with methods using more sophisticated techniques in machine learning to mitigate practical implementation problems. In this study, we further extend that work by characterizing the effects of changing history length and sensor range. Rather than finding that increasing history length and sensor range always yield better estimation performance, we find that the optimal history length and sensor range varies depending on the distance between the estimating robot and the robot being estimated. For estimation problems where the estimation target is nearby, longer histories actually degrade performance, and so sensor ranges could be increased instead. Conversely, for farther targets, history length is as valuable or more valuable than sensor range. Thus, just as optimal shutter speed varies with light availability and speed of the subject, passive situational awareness in multi-robot teams is best achieved with different strategies depending on proximity to locations of interest. All studies use the teams of Thymio II physical, two-wheeled robots in laboratory environments 1.1Data and models used are available at https://github.com/PavlicLab/ACSOS2020_ReTLo_Extension.git.",https://ieeexplore.ieee.org/document/9196255/,2020 IEEE International Conference on Autonomic Computing and Self-Organizing Systems (ACSOS),17-21 Aug. 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IWSSIP.2015.7314233,How humans can help computers to solve an artificial problem?,IEEE,Conferences,"The idea of using CAPTCHA (Completely Automated Public Turing Test to Tell Computers and Humans Apart) was to protect websites from attacks initiated by the automated computer scripts or computer robots (bots). One of the most important issues about CAPTCHAs is that the test has to be designed in a way that makes it too hard or almost impossible for the computer programs to break the test however, at the same time it should be fairly easy for human users to solve. ReCAPTCHA is known as one of the most popular CAPTCHA models which is being used by the majority of well-known websites such as Yahoo!, Google, Facebook and etc. ReCAPTCHA is being used in order to help digitizing old text books and notes. In this paper we investigate the algorithm behind reCAPTCHA more in depth and find out how basically a simple script-based computer program can get use of real human users in order to solve an artificial problem for the machines. We also review some of the most important security accepts of the reCAPTCHA model.",https://ieeexplore.ieee.org/document/7314233/,"2015 International Conference on Systems, Signals and Image Processing (IWSSIP)",10-12 Sept. 2015,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA.2013.6630610,Human-friendly robot navigation in dynamic environments,IEEE,Conferences,"The vision-based mechanisms that pedestrians in social groups use to navigate in dynamic environments, avoiding obstacles and each others, have been subject to a large amount of research in social anthropology and biological sciences. We build on recent results in these fields to develop a novel fully-distributed algorithm for robot local navigation, which implements the same heuristics for mutual avoidance adopted by humans. The resulting trajectories are human-friendly, because they can intuitively be predicted and interpreted by humans, making the algorithm suitable for the use on robots sharing navigation spaces with humans. The algorithm is computationally light and simple to implement. We study its efficiency and safety in presence of sensing uncertainty, and demonstrate its implementation on real robots. Through extensive quantitative simulations we explore various parameters of the system and demonstrate its good properties in scenarios of different complexity. When the algorithm is implemented on robot swarms, we could observe emergent collective behaviors similar to those observed in human crowds.",https://ieeexplore.ieee.org/document/6630610/,2013 IEEE International Conference on Robotics and Automation,6-10 May 2013,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA.2013.6631340,Humanoid robot posture-control learning in real-time based on human sensorimotor learning ability,IEEE,Conferences,"In this paper we propose a system capable of teaching humanoid robots new skills in real-time. The system aims to simplify the robot control and to provide a natural and intuitive interaction between the human and the robot. The key element of the system is exploitation of the human sensorimotor learning ability where a human demonstrator learns how to operate a robot in the same fashion as humans adapt to various everyday tasks. Another key aspect of the proposed system is that the robot learns the task simultaneously while the human is operating the robot. This enables the control of the robot to be gradually transferred from the human to the robot during the demonstration. The control is transferred based on the accuracy of the imitated task. We demonstrated our approach using an experiment where a human demonstrator taught a humanoid robot how to maintain the postural stability in the presence of the perturbations. To provide the appropriate feedback information of the robot's postural stability to the human sensorimotor system, we utilized a custom-built haptic interface. To absorb the demonstrated skill by the robot, we used Locally Weighted Projection Regression machine learning method. A novel approach was implemented to gradually transfer the control responsibility from the human to the incrementally built autonomous robot controller.",https://ieeexplore.ieee.org/document/6631340/,2013 IEEE International Conference on Robotics and Automation,6-10 May 2013,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCSP.2018.8524377,Implementation of Robotic Vision to Perform Threaded Assembly,IEEE,Conferences,"In manufacturing of mechanical parts and assemblies, proper thread-engagement between a bolt and a nut is vital for the performance and reliability of the product. Typically, this is a precision work, requiring repetitive manual operations. In this paper, we explain how such assembly operations can be carried out by collaborative robots (co-bots) by monitoring the position and orientation of the nut and bolt using an image-sensor (camera). The focus of our discussion is the assembly-operation of bolting of a nut by the grippers of a co-bot. Slips and misalignment, leading to wrong positioning of the nut and the bolt, are identified by capturing the images of the two components in real time using Microsoft Kinect camera-sensor. 3D Reconstruction of the image captured by the camera-sensor is carried out using the Kinect Fusion application. The reconstructed image is in the form of a polygonal mesh which is further converted to 3D Point Cloud data which is less sensitive to noise. Thereafter, the Point Cloud is segmented by dividing the entire scene into many clusters in order to distinguish the objects of the scene as grippers and nut and bolt. These clusters can be used for the training of the co-bot for the proposed operation. This method of extracting object-boundaries leading to recognition of objects is a vital operation in the field of robotic vision. We provide baseline description of various machine learning techniques that can be applied to realize proper assembly of a nut and a bolt.",https://ieeexplore.ieee.org/document/8524377/,2018 International Conference on Communication and Signal Processing (ICCSP),3-5 April 2018,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS.1991.174485,Implementation of an active optical range sensor using laser slit for in-door intelligent mobile robot,IEEE,Conferences,"The sensor with real-time environment recognition ability is one of the key technologies for autonomous robots. The authors have designed and implemented a small size optical range sensor for their experimental mobile robot. The sensor consists of a laser slit generator, a CCD image sensor and a processing unit. Using this sensor, the real-time obstacle avoiding function is realized and added to the autonomous navigation aspect of the robot.<>",https://ieeexplore.ieee.org/document/174485/,Proceedings IROS '91:IEEE/RSJ International Workshop on Intelligent Robots and Systems '91,3-5 Nov. 1991,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CONIELECOMP.2014.6808580,Implementation of an embedded system on a TS7800 board for robot control,IEEE,Conferences,"Growing Functional Modules (GFM) learning based controllers need to be experimented on real robots. In 2009, looking to develop a flexible and generic embedded interface for such robots, we decided to use a TS-7800 single board computer (SBC) with a Debian Linux operating system. Despite the many advantages of this board, implementing the embedded system has been a complex task. This paper describes the implementation of protocols through the TS-7800 different ports (RS232, TCP/IP, USB, analog and digital pins) as well as the connection of external boards (TS-ADC24, TS-DIO64, SSC-32 and LCD display). This implementation was required to connect a large range of actuators, sensors and other peripherals. Furthermore, the architecture of the embedded system is exposed in detail, including topics such as the XML configuration file that specifies the peripherals connected to the SBC, the concept of virtual sensors, the implementation of parallelism and the embedded system interface launcher. Technical aspects such as the optimization of video capture and processing are detailed because their execution required specific compilers versions, EABI emulation and extra libraries (openCV libjpg and libpngand libv4l). The final embedded system was implemented in a humanoid robot and connected to the GFM controller in charge of developing its equilibrium subsystem.",https://ieeexplore.ieee.org/document/6808580/,"2014 International Conference on Electronics, Communications and Computers (CONIELECOMP)",26-28 Feb. 2014,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SMC.2017.8122654,Implementation of human-robot VQA interaction system with dynamic memory networks,IEEE,Conferences,"One of the major functions of intelligent robots such as social or home service robots is to interact with users in natural language. Moving on from simple conversation or retrieval of data stored in computer memory, we present a new Human-Robot Interaction (HRI) system which can understand and reason over environment around the user and provide information about it in a natural language. For its intelligent interaction, we integrated Dynamic Memory Networks (DMN), a deep learning network for Visual Question Answering (VQA). For its hardware, we built a robotic head platform with a tablet PC and a 3 DOF neck. Through an experiment where the user and the robot had question answering interaction in our customized environment and in real time, the feasibility our proposed system was validated, and the effectiveness of deep learning application in real world as well as a new insight on human robot interaction was demonstrated.",https://ieeexplore.ieee.org/document/8122654/,"2017 IEEE International Conference on Systems, Man, and Cybernetics (SMC)",5-8 Oct. 2017,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS45743.2020.9341029,Improving Unimodal Object Recognition with Multimodal Contrastive Learning,IEEE,Conferences,"Robots perceive their environment using various sensor modalities, e.g., vision, depth, sound or touch. Each modality provides complementary information for perception. However, while it can be assumed that all modalities are available for training, when deploying the robot in real-world scenarios the sensor setup often varies. In order to gain flexibility with respect to the deployed sensor setup we propose a new multimodal approach within the framework of contrastive learning. In particular, we consider the case of learning from RGB-D images while testing with one modality available, i.e., exclusively RGB or depth. We leverage contrastive learning to capture high-level information between different modalities in a compact feature embedding. We extensively evaluate our multimodal contrastive learning method on the Falling Things dataset and learn representations that outperform prior methods for RGB-D object recognition on the NYU-D dataset. Our code and details on the used datasets are available at: https://github.com/meyerjo/MultiModalContrastiveLearning.",https://ieeexplore.ieee.org/document/9341029/,2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),24 Oct.-24 Jan. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICEE50131.2020.9260698,"Indoor and Outdoor Face Recognition for Social Robot, Sanbot Robot as Case Study",IEEE,Conferences,"The interaction between human and robots is of paramount importance in comforting robot and human in the context of social demand. For the purpose of human-robot interaction, the robot should have the ability to perform a variety of actions including face recognition, path planning, etc. In this paper, face recognition has been implemented on the Sanbot robot. Since the Sanbot robot is intended to work in real environment, therefore indoor and outdoor environment is taken into account in proposing the corresponding face recognition algorithm. For each case a robust pre-processing algorithm should be designed and which can circumvent a challenging problem in face recognition, namely, different lighting conditions (light intensity, angle of radiation, etc.). In case of indoor environment, faces in an captured image by the robot HD camera are found using a Haar-cascade algorithm. Afterwards, a histogram equalization is applied to face images in order to standardize them. Then commonly practiced Deep convolutional neural network structures such as Inception and ResNet are used to design a model and trained end-to-end on a customized dataset with strong augmentation. Finally, by using a voting method, proper prediction is carried out on each face. In what concerns the outdoor environment, which has more challenges, upon applying histogram Equalization on the captured image, faces are found using a MultiTask Cascaded Convolutional Neural Network. Then face images are aligned as head orientation are corrected. Finally, cropped face image is fed to Siamese Network in order to extract face features and verifying individuals. From several practical results it has been inferred that the accuracy of the indoor method is nearly 93% without voting and with voting 97%, and the outdoor method is about 95%.",https://ieeexplore.ieee.org/document/9260698/,2020 28th Iranian Conference on Electrical Engineering (ICEE),4-6 Aug. 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS.2013.6696336,Inferring categories to accelerate the learning of new classes,IEEE,Conferences,"On-the-fly learning systems are necessary for the deployment of general purpose robots. New training examples for such systems are often supplied by mentor interactions. Due to the cost of acquiring such examples, it is desirable to reduce the number of necessary interactions. Transfer learning has been shown to improve classification results for classes with small numbers of training examples by pooling knowledge from related classes. Standard practice in these works is to assume that the relationship between the transfer target and related classes is already known. In this work, we explore how previously learned categories, or related groupings of classes, can be used to transfer knowledge to novel classes without explicitly known relationships to them. We demonstrate an algorithm for determining the category membership of a novel class, focusing on the difficult case when few training examples are available. We show that classifiers trained via this method outperform classifiers optimized to learn the novel class individually when evaluated on both synthetic and real-world datasets.",https://ieeexplore.ieee.org/document/6696336/,2013 IEEE/RSJ International Conference on Intelligent Robots and Systems,3-7 Nov. 2013,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/INFOCOM41043.2020.9155528,Informative Path Planning for Mobile Sensing with Reinforcement Learning,IEEE,Conferences,"Large-scale spatial data such as air quality, thermal conditions and location signatures play a vital role in a variety of applications. Collecting such data manually can be tedious and labour intensive. With the advancement of robotic technologies, it is feasible to automate such tasks using mobile robots with sensing and navigation capabilities. However, due to limited battery lifetime and scarcity of charging stations, it is important to plan paths for the robots that maximize the utility of data collection, also known as the informative path planning (IPP) problem. In this paper, we propose a novel IPP algorithm using reinforcement learning (RL). A constrained exploration and exploitation strategy is designed to address the unique challenges of IPP, and is shown to have fast convergence and better optimality than a classical reinforcement learning approach. Extensive experiments using real-world measurement data demonstrate that the proposed algorithm outperforms state-of-the-art algorithms in most test cases. Interestingly, unlike existing solutions that have to be re-executed when any input parameter changes, our RL-based solution allows a degree of transferability across different problem instances.",https://ieeexplore.ieee.org/document/9155528/,IEEE INFOCOM 2020 - IEEE Conference on Computer Communications,6-9 July 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CEC.2001.934451,Initial evolvability experiments on the CAM-brain machines (CBMs),IEEE,Conferences,"Presents the results of some of the first evolvability experiments undertaken on CAM (content-addressable memory) brain machines (CBMs), using the hardware itself (not software simulations). A CBM is a specialised piece of programmable (evolvable) hardware that uses Xilinx XC6264 programmable FPGA chips to grow and evolve, at electronic speeds, 3D cellular automata (CA) based neural network circuit modules of some 1,000 neurons each. A complete run of a genetic algorithm (e.g. with 100 generations and a population size of 100) is executed in a few seconds. 64,000 of these modules can be evolved separately according to the fitness definitions of human evolutionary engineers and downloaded, one by one, into a gigabyte of RAM. Human brain architects then interconnect these modules ""by hand"" according to their artificial brain architectures. The CBM then updates the binary neural signaling of the artificial brain (with 64,000 ""hand"" interconnected modules, i.e. 75 million neurons) at a rate of 130 billion CA cell updates a second, which is fast enough for the real-time control of robots. Before such multi-module artificial brains can be constructed, it is essential that the quality of the evolution (the ""evolvability"") of the individual modules should be adequate. This paper reports on the initial evolvability results obtained on CBM hardware.",https://ieeexplore.ieee.org/document/934451/,Proceedings of the 2001 Congress on Evolutionary Computation (IEEE Cat. No.01TH8546),27-30 May 2001,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IPRECON52453.2021.9641000,Intelligent Fault Diagnosis Mechanism for Industrial Robot Actuators using Digital Twin Technology,IEEE,Conferences,"Intelligent fault detection is a mechanism’s competency to distinguish between healthy and faulty machine signals for smart and efficient diagnosis. The modelling and analysis of the parameters that contribute to the system’s fundamental operation form the crux of the framework. A heuristic technology to enable real-time intelligent fault detection is digital twin technology. Digital twin technology allows a tandem establishment between real-world machines and the virtual domain, allowing for the inclusion of optimization and maintenance frameworks. Sparsely represented machines in the digital twin domain are linear actuators, which form essential parts of various industrial and commercial machines. Therefore, this study has modelled a data-driven and multi-physics robotic linear actuator digital twin, and integrated it with a custom designed fault detection mechanism using Naïve Bayes classifier. This architecture can autonomously be deployed in tandem to the physical machine to alarm and diagnose electrical faults as soon as they occur in the machine. As compared with conventional diagnostics this will reduce machine down-time and expedite repairs. The resultant model built on MATLAB, Simulink gave an accuracy of 96% and required minimal processing capability to operate. Widespread commercial utilization of the proposed model can pave the path for Industry 4.0 utilization of linear actuators as well as technologies including industrial robots that utilize them.",https://ieeexplore.ieee.org/document/9641000/,2021 IEEE International Power and Renewable Energy Conference (IPRECON),24-26 Sept. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICIT.2010.5472498,Intelligent control and evolutionary strategies applied to multirobotic systems,IEEE,Conferences,"This paper describes the modeling, implementation, and evaluation of RoBombeiros multirobotic system. The robotic task in this paper is performed over a natural disaster, simulated as a forest fire. The simulator supports several features to allow realistic simulation, like irregular terrains, natural processes (e.g. fire, wind) and physical constraint in the creation and application of mobile robots. The proposed system relies on two steps: (i) group formation planning and (ii) intelligent techniques to perform robots navigation for fire fighting. For planning, we used genetic algorithms to evolve positioning strategies for firefighting robots performance. For robots operation, physically simulated fire-fighting robots were built, and the sensory information of each robot (e.g. GPS, compass, sonar) was used in the input of an artificial neural network (ANN). The ANN controls the vehicle (robot) actuators and allows navigation with obstacle avoidance. Simulation results show that the ANN satisfactorily controls the mobile robots; the genetic algorithm adequately configures the fire fighting strategy and the proposed multi-robotic system can have an essential hole in the planning and execution of fire fighting in real forests.",https://ieeexplore.ieee.org/document/5472498/,2010 IEEE International Conference on Industrial Technology,14-17 March 2010,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCCYB.2004.1437660,Intelligent control application on sample identification,IEEE,Conferences,"An intelligent control implementation is proposed for sample differentiation with Raman spectroscopy, which can be used to characterize various samples for decision-making and medical diagnosis. Raman spectra are weak signals whose features are inevitably affected by numerous noises during the calibration process. These noises must be eliminated to an acceptable level. Fuzzy control has been widely used to solve uncertainty, imprecision and vague phenomena, so fuzzy logic can be used for noise filtering. The resulting intrinsic Raman spectrum has been trained using artificial neural networks. Both unsupervised learning and supervised learning are to be conducted in this preliminary research on sample identification. For unsupervised training, principal component analysis (PCA) is exploited, which is based on Hebbian rule and single value decomposition (SVD) approach, respectively. For supervised training, radial basis function (RBF) is presented. A complete procedure for sample identification consists of Raman spectra calibration, noise filtering, unsupervised classification and supervised neural network training. A systematic intelligent control approach is formulated in consequence for sample identification. The long-term objective is to create a real-time approach for sample analysis using a Raman spectrometer directly mounted at the end-effector of the medical robots to enhance robotic remote surgery",https://ieeexplore.ieee.org/document/1437660/,"Second IEEE International Conference on Computational Cybernetics, 2004. ICCC 2004.",30 Aug.-1 Sept. 2004,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/HUMANOIDS.2012.6651596,Interactive symbol generation of task planning for daily assistive robot,IEEE,Conferences,"For the development of both hardware and software, task plannings become more and more important for robots to perform various tasks. Applying task planning to robotic system for working in real environments has difficulties. Estimating required symbols before planning is difficult because real environments are partially observable. In this paper, we proposed a method for task planning in partially observable environments with unknown objects. To construct conditional plans used in these environments, we extend the description of actions to multi-effect actions, and to deal with unknown objects, robots get new symbols generated by human interaction on demand. Additionally we show experiments of Willow Garage's PR2 executing the task in the real environment with unknown objects.",https://ieeexplore.ieee.org/document/6651596/,2012 12th IEEE-RAS International Conference on Humanoid Robots (Humanoids 2012),29 Nov.-1 Dec. 2012,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/WPNC.2016.7822857,Introducing a novel marker-based geometry model in monocular vision,IEEE,Conferences,"A spherical marker-based distance capture concept using monocular vision is presented in this paper. A novel method is explored, within the concept of a virtual sphere, which shows how to improve the reading measurements of the distance of a moving object from low resolution digital images, and from a single viewpoint. The aim here is to be able to track accurately the object at a furthest possible position. A conclusion with experimental simulations carried out using 3D modeling of markers and representing the real world showing the potency of the marker's geometry on improving the accuracy of the measurements. A potential application field of the proposed method is the implementation of tracking object in mobile robots, marker-based localization, and field of topography.",https://ieeexplore.ieee.org/document/7822857/,"2016 13th Workshop on Positioning, Navigation and Communications (WPNC)",19-20 Oct. 2016,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ROBIO.2018.8665255,Knowledge-Driven Deep Deterministic Policy Gradient for Robotic Multiple Peg-in-Hole Assembly Tasks,IEEE,Conferences,"It remains a formidable challenge for traditional control strategies to perform automatic multiple peg-in-hole assembly tasks due to the complicated and dynamic contact states. Inspired by that human could generalize the learned skills to perform the different assembly tasks well, a general learning-based algorithm based on deep deterministic policy gradient (DDPG) is proposed. To make robots learn the multiple peg-in-hole assembly skills from experience efficiently and stably, the learning process is driven by the basic knowledge like PD force control strategy. To achieve a fast learning process in the real-world assembly tasks, a hybrid exploration strategy is applied to drive a efficient exploration during policy search phase. A dual peg-in-hole assembly simulation and real-world experiments are implemented to verify the effectiveness of the proposed algorithm. The performance measured by the assembly time and the maximum contact forces demonstrates that the multiple peg-in-hole assembly skills could be improved only after 150 training episodes in dual peg-in-hole assembly task.",https://ieeexplore.ieee.org/document/8665255/,2018 IEEE International Conference on Robotics and Biomimetics (ROBIO),12-15 Dec. 2018,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/INDIN.2015.7281881,Knowledge-driven finite-state machines. Study case in monitoring industrial equipment,IEEE,Conferences,Traditionally state machines are implemented by coding the desired behavior of a given system. This work proposes the use of ontological models to describe and perform computations on state machines by using SPARQL queries. This approach represents a paradigm shift relating to the customary manner in which state machines are stored and computed. The main contribution of the work is an ontological model to represent state machines and a set of generic queries that can be used in any knowledge-driven state machine to compute valuable information. The approach was tested in a study case were the state machines of industrial robots in a manufacturing line were modeled as ontological models and used for monitoring the behavior of these devices on real time.,https://ieeexplore.ieee.org/document/7281881/,2015 IEEE 13th International Conference on Industrial Informatics (INDIN),22-24 July 2015,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/GCIS.2009.206,Layered Task Allocation in Multi-robot Systems,IEEE,Conferences,"A layered task allocation method is presented for multi-robot systems in a collaboration and adversarial, dynamic, real-time environment with unreliable communication in this paper. The process of task allocation is divided into three layers: task decomposition layer, task evaluation layer and task selection layer. In task decomposition layer, robots categorize their environments into corresponding modes, and fix subtasks in every mode as experts do, in order to reduce candidate tasks and decrease the complexity of task allocation. Q-Learning based on Adaptive Neuro Fuzzy Inference System (ANFIS) is adopted to compute utilities of candidate tasks in task evaluation layer. This can not only avoid the complicated opponent modeling but also make the learning more efficient. In task selection layer, task with the maximum utility is selected in application, but in learning, task is selected according to randomized Boltzmann exploration tactics in order to get more information for optimization. Simulation experiments implemented on simulated robotic soccer show that this approach improves performances of multi-robot systems greatly.",https://ieeexplore.ieee.org/document/5209028/,2009 WRI Global Congress on Intelligent Systems,19-21 May 2009,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICAR46387.2019.8981551,Lazy Steering RRT: An Optimal Constrained Kinodynamic Neural Network Based Planner with no In-Exploration Steering,IEEE,Conferences,"Kinodynamic-RRT* provides a sampling based asymptotically-optimal solution for motion planning of kinematically- and dynamically-constrained robots. For nonlinear systems, normally, the time- and energy-clamped steering function solutions needed within the RRT* use numerical iterative schemes such as shooting methods, which are computationally cumbersome. The number of calls to these solvers increases with the size of the tree. Hence, the time complexity of finding feasible steering functions prevents kinodynamic-RRT* for non-linear systems from being utilized in realtime or in situations where fast planning and re-planning are needed. Kinematic/dynamic constraints reduction to make the steering functions solvable in real time has been proposed in literature, however, these methods would affect the optimality of the solution. In this paper, we propose a lazy-steering kinodynmaic RRT* in which, machine learning techniques are used to (1) predict if a randomly-selected node is steerable to, and (2) if the steering is deemed feasible, what would be the estimated energy cost associated, when executing it. This provides a promising framework for implementing Kinodynamic-RRT* in which the use of numerical methods is delayed (hence the name lazy steering) until a potential collision free path has been found, and only then the numerical techniques are invoked. This results in a huge improvement in the run time with little trade off on optimality. Our proposed method was tested via simulation for motion planning of an under-actuated, non-holonomic, quadcopter with nonlinear dynamics in an environment cluttered with obstacles. The lazy-steering RRT* was faster than its counterpart (which was based on some recent works) by two orders of magnitude.",https://ieeexplore.ieee.org/document/8981551/,2019 19th International Conference on Advanced Robotics (ICAR),2-6 Dec. 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SMC.2019.8914406,Learning Locomotion Skills via Model-based Proximal Meta-Reinforcement Learning,IEEE,Conferences,"Model-based reinforcement learning methods provide a promising direction for a range of automated applications, such as autonomous vehicles and legged robots, due to their sample-efficiency. However, their asymptotic performance is usually inferior compared to the state-of-the-art model-free reinforcement learning methods in locomotion control domains. One main challenge of model-based reinforcement learning is learning a dynamics model that is accurate enough for planning. This paper mitigates this issue by meta-reinforcement learning from an ensemble of dynamics models. A policy learns from dynamics models that hold different beliefs of a real environment. This procedure improves its adaptability and inaccuracy-tolerance ability. A proximal meta-reinforcement learning algorithm is introduced to improve computational efficiency and reduces variance of higher-order gradient estimation. A heteroscedastic noise is added to the training dataset, thus leading to a robust and efficient model learning. Subsequently, proximal meta-reinforcement learning maximizes the expected returns by sampling “imaginary” trajectories from the learned dynamics, which does not require real environment data and can be deployed on many servers in parallel to speed up the whole learning process. The aim of this work is to reduce the sample-complexity and computational cost of reinforcement learning in robot locomotion tasks. Simulation experiments show that the proposed algorithm achieves an asymptotic performance compared with the state-of-the-art model-free reinforcement learning methods with significantly fewer samples, which confirm our theoretical results.",https://ieeexplore.ieee.org/document/8914406/,"2019 IEEE International Conference on Systems, Man and Cybernetics (SMC)",6-9 Oct. 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS45743.2020.9341458,Learning Motion Parameterizations of Mobile Pick and Place Actions from Observing Humans in Virtual Environments,IEEE,Conferences,"In this paper, we present an approach and an implemented pipeline for transferring data acquired from observing humans in virtual environments onto robots acting in the real world, and adapting the data accordingly to achieve successful task execution. We demonstrate our pipeline by inferring seven different symbolic and subsymbolic motion parameters of mobile pick and place actions, which allows the robot to set a simple breakfast table. We propose an approach to learn general motion parameter models and discuss, which parameters can be learned at which abstraction level.",https://ieeexplore.ieee.org/document/9341458/,2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),24 Oct.-24 Jan. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IECON.1993.339280,Learning behavioral control by reinforcement for an autonomous mobile robot,IEEE,Conferences,"We present an implementation of a reinforcement learning algorithm through the use of a special neural network topology, the AHC (adaptive heuristic critic). The AHC constitutes a fusion supervisor of primitive behaviours in order to execute more complex robot behaviours as for example go to goal. This fusion supervisor is part of an architecture for the execution of mobile robot tasks which are composed of several primitive behaviours which act in a simultaneous or concurrent fashion. The architecture allows for learning to take place at the execution level, it incorporates the experience gained in executing primitive behaviours as well as the overall task. The implementation of the autonomous learning approach has been tested within OPMOR, a simulation environment for mobile robots and with our mobile platform UPM Robuter. Both simulated and real results are presented. The performance of the AHC neural network is adequate. Portions of this work have been implemented in the EEC ESPRIT 2483 PANORAMA Project.<>",https://ieeexplore.ieee.org/document/339280/,Proceedings of IECON '93 - 19th Annual Conference of IEEE Industrial Electronics,15-19 Nov. 1993,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ROMAN.2010.5598659,Learning grasp stability based on tactile data and HMMs,IEEE,Conferences,"In this paper, the problem of learning grasp stability in robotic object grasping based on tactile measurements is studied. Although grasp stability modeling and estimation has been studied for a long time, there are few robots today able of demonstrating extensive grasping skills. The main contribution of the work presented here is an investigation of probabilistic modeling for inferring grasp stability based on learning from examples. The main objective is classification of a grasp as stable or unstable before applying further actions on it, e.g. lifting. The problem cannot be solved by visual sensing which is typically used to execute an initial robot hand positioning with respect to the object. The output of the classification system can trigger a regrasping step if an unstable grasp is identified. An off-line learning process is implemented and used for reasoning about grasp stability for a three-fingered robotic hand using Hidden Markov models. To evaluate the proposed method, experiments are performed both in simulation and on a real robot system.",https://ieeexplore.ieee.org/document/5598659/,19th International Symposium in Robot and Human Interactive Communication,13-15 Sept. 2010,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ROBOT.2001.932897,Learning momentum: integration and experimentation,IEEE,Conferences,"We further study the effects of learning momentum as defined by Arkin, Clark and Ram (1992) on robots, both simulated and real, attempting to traverse obstacle fields in order to reach a goal. Integration of these results into a large-scale software architecture, MissionLab, provides the ability to exercise these algorithms in novel ways. Insight is also sought in reference to when different learning momentum strategies should be used.",https://ieeexplore.ieee.org/document/932897/,Proceedings 2001 ICRA. IEEE International Conference on Robotics and Automation (Cat. No.01CH37164),21-26 May 2001,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS.2014.6943031,Learning robot tactile sensing for object manipulation,IEEE,Conferences,"Tactile sensing is a fundamental component of object manipulation and tool handling skills. With robots entering unstructured environments, tactile feedback also becomes an important ability for robot manipulation. In this work, we explore how a robot can learn to use tactile sensing in object manipulation tasks. We first address the problem of in-hand object localization and adapt three pose estimation algorithms from computer vision. Second, we employ dynamic motor primitives to learn robot movements from human demonstrations and record desired tactile signal trajectories. Then, we add tactile feedback to the control loop and apply relative entropy policy search to learn the parameters of the tactile coupling. Additionally, we show how the learning of tactile feedback can be performed more efficiently by reducing the dimensionality of the tactile information through spectral clustering and principal component analysis. Our approach is implemented on a real robot, which learns to perform a scraping task with a spatula in an altered environment.",https://ieeexplore.ieee.org/document/6943031/,2014 IEEE/RSJ International Conference on Intelligent Robots and Systems,14-18 Sept. 2014,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS.2013.6696816,Learning sequential tasks interactively from demonstrations and own experience,IEEE,Conferences,"Deploying robots to our day-to-day life requires them to have the ability to learn from their environment in order to acquire new task knowledge and to flexibly adapt existing skills to various situations. For typical real-world tasks, it is not sufficient to endow robots with a set of primitive actions. Rather, they need to learn how to sequence these in order to achieve a desired effect on their environment. In this paper, we propose an intuitive learning method for a robot to acquire sequences of motions by combining learning from human demonstrations and reinforcement learning. In every situation, our approach treats both ways of learning as alternative control flows to optimally exploit their strengths without inheriting their shortcomings. Using a Gaussian Process approximation of the state-action sequence value function, our approach generalizes values observed from demonstrated and autonomously generated action sequences to unknown inputs. This approximation is based on a kernel we designed to account for different representations of tasks and action sequences as well as inputs of variable length. From the expected deviation of value estimates, we devise a greedy exploration policy following a Bayesian optimization criterion that quickly converges learning to promising action sequences while protecting the robot from sequences with unpredictable outcome. We demonstrate the ability of our approach to efficiently learn appropriate action sequences in various situations on a manipulation task involving stacked boxes.",https://ieeexplore.ieee.org/document/6696816/,2013 IEEE/RSJ International Conference on Intelligent Robots and Systems,3-7 Nov. 2013,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCVW.2019.00309,Learning to Navigate Robotic Wheelchairs from Demonstration: Is Training in Simulation Viable?,IEEE,Conferences,"Learning from demonstration (LfD) enables robots to learn complex relationships between their state, perception and actions that are hard to express in an optimization framework. While people intuitively know what they would like to do in a given situation, they often have difficulty representing their decision process precisely enough to enable an implementation. Here, we are interested in robots that carry passengers, such as robotic wheelchairs, where user preferences, comfort and the feeling of safety are important for autonomous navigation. Balancing these requirements is not straightforward. While robots can be trained in an LfD framework in which users drive the robot according to their preferences, performing these demonstrations can be time-consuming, expensive, and possibly dangerous. Inspired by recent efforts for generating synthetic data for training autonomous driving systems, we investigate whether it is possible to train a robot based on simulations to reduce the time requirements, cost and potential risk. A key characteristic of our approach is that the input is not images, but the locations of people and obstacles relative to the robot. We argue that this allows us to transfer the classifier from the simulator to the physical world and to previously unseen environments that do not match the appearance of the training set. Experiments with 14 subjects providing physical and simulated demonstrations validate our claim.",https://ieeexplore.ieee.org/document/9022271/,2019 IEEE/CVF International Conference on Computer Vision Workshop (ICCVW),27-28 Oct. 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA46639.2022.9811662,Learning to Socially Navigate in Pedestrian-rich Environments with Interaction Capacity,IEEE,Conferences,"Existing navigation policies for autonomous robots tend to focus on collision avoidance while ignoring human-robot interactions in social life. For instance, robots can pass along the corridor safer and easier if pedestrians notice them. Sounds have been considered as an efficient way to attract the attention of pedestrians, which can alleviate the freezing robot problem. In this work, we present a new deep reinforcement learning (DRL) based social navigation approach for autonomous robots to move in pedestrian-rich environments with interaction capacity. Most existing DRL based methods intend to train a general policy that outputs both navigation actions, i.e., expected robot&#x0027;s linear and angular velocities, and interaction actions, i.e., the beep action, in the context of reinforcement learning. Different from these methods, we intend to train the policy via both supervised learning and reinforcement learning. In specific, we first train an interaction policy in the context of supervised learning, which provides a better understanding of the social situation, then we use this interaction policy to train the navigation policy via multiple reinforcement learning algorithms. We evaluate our approach in various simulation environments and compare it to other methods. The experimental results show that our approach outperforms others in terms of the success rate. We also deploy the trained policy on a real-world robot, which shows a nice performance in crowded environments.",https://ieeexplore.ieee.org/document/9811662/,2022 International Conference on Robotics and Automation (ICRA),23-27 May 2022,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICEC.1995.487489,Learning to achieve co-operation by temporal-spatial fitness sharing,IEEE,Conferences,"We propose a co-operative GA-based learning system that would make real-world heterogeneous agents feasible with the minimum amount of communication hardware. The problem is identical to a distributed GA implemented on processors connected by local and very slow communication lines. We have developed an extension of the fitness sharing method that incorporates sharing over temporally-spatially distributed populations. Restricting an agent's task to the inter-agent conflict avoidance, this sharing is realised by exchanging estimated fitness values over all agents. The mechanism of finding conflict avoidance actions is similar to that of a self-organisation mechanism of a Kohonen-type network. Our results from simulations of a bump-avoidance task for multiple mobile robots show that it elicits a notable performance improvement compared to normal classifier systems.",https://ieeexplore.ieee.org/document/487489/,Proceedings of 1995 IEEE International Conference on Evolutionary Computation,29 Nov.-1 Dec. 1995,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICSMC.1993.390770,Learning to coordinate behaviors for real-time path planning of autonomous systems,IEEE,Conferences,"We present a neural network (NN) system which learns the appropriate simultaneous activation of primitive behaviors in order to execute more complex robot behaviors. The NN implementation is part of an architecture for the execution of mobile robot tasks which are composed of several primitive behaviors in a simultaneous or concurrent fashion. We use a supervised learning technique with a human trainer generating appropriate training for the simultaneous activation of behavior in a simulated environment. The NN implementation has been tested within OPMOR, a simulation environment for mobile robots and several results are presented. The performance of the neural network is adequate. Portions of this work has been implemented in the EEC ESPRIT 2483 PANORAMA Project.<>",https://ieeexplore.ieee.org/document/390770/,Proceedings of IEEE Systems Man and Cybernetics Conference - SMC,17-20 Oct. 1993,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SMC.2019.8914455,Learning waste Recycling by playing with a Social Robot,IEEE,Conferences,"In this paper we investigate the use of a social robot as an interface to a serious game aiming to train kids in how to recycle materials correctly. Serious games are mostly used to induce motivations and engagement in users and support knowledge transfer during playing. They are especially effective when the goal of the game concerns behavior change. In addition, social robots have been used effectively in educational settings to engage children in the learning process. Following this trend, we designed a serious game in which the social robot Pepper plays with a child to teach him to correctly recycle the materials. To endow the robot with the capability of detecting and classifying the waste material we developed an image recognition module based on a Convolutional Neural Network. Preliminary experimental results show that the implementation of a serious game about recycling into the Pepper robot improves its social behavior. The use of real objects as waste items during the game turns out to be a successful approach not only for perceived learning effectiveness but also for engagement of the children.",https://ieeexplore.ieee.org/document/8914455/,"2019 IEEE International Conference on Systems, Man and Cybernetics (SMC)",6-9 Oct. 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ITSC45102.2020.9294425,Learning-to-Fly: Learning-based Collision Avoidance for Scalable Urban Air Mobility,IEEE,Conferences,"With increasing urban population, there is global interest in Urban Air Mobility (UAM), where hundreds of autonomous Unmanned Aircraft Systems (UAS) execute missions in the airspace above cities. Unlike traditional human-inthe-loop air traffic management, UAM requires decentralized autonomous approaches that scale for an order of magnitude higher aircraft densities and are applicable to urban settings. We present Learning-to-Fly (L2F), a decentralized on-demand airborne collision avoidance framework for multiple UAS that allows them to independently plan and safely execute missions with spatial, temporal and reactive objectives expressed using Signal Temporal Logic. We formulate the problem of predictively avoiding collisions between two UAS without violating mission objectives as a Mixed Integer Linear Program (MILP). This however is intractable to solve online. Instead, we develop L2F, a two-stage collision avoidance method that consists of: 1) a learning-based decision-making scheme and 2) a distributed, linear programming-based UAS control algorithm. Through extensive simulations, we show the real-time applicability of our method which is ≈6000× faster than the MILP approach and can resolve 100% of collisions when there is ample room to maneuver, and shows graceful degradation in performance otherwise. We also compare L2F to two other methods and demonstrate an implementation on quad-rotor robots.",https://ieeexplore.ieee.org/document/9294425/,2020 IEEE 23rd International Conference on Intelligent Transportation Systems (ITSC),20-23 Sept. 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IIAI-AAI50415.2020.00021,Library Intelligent Book Recommendation System Using Facial Expression Recognition,IEEE,Conferences,"To solve the problem of information overload, the recommendation system has developed in various fields. Faced with a variety of forms and rich masses of books, the book recommendation system based on various recommendation methods is applied in the library. In the traditional book recommendation system, there are some problems, such as single recommendation mode, lack of pertinence, too centralized recommended books and so on. In order to solve these problems, this paper proposes a personalized book recommendation system. Through user expression recognition to obtain user preferences, according to user preferences to recommend books to user. Facial expression recognition is realized by using convolution neural network model. This kind of recommendation method has real-time and authenticity. The book recommendation system based on facial expression recognition can improve users' sense of use when applied to library robots.",https://ieeexplore.ieee.org/document/9430442/,2020 9th International Congress on Advanced Applied Informatics (IIAI-AAI),1-15 Sept. 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/HRI.2019.8673212,Lifespan Design of Conversational Agent with Growth and Regression Metaphor for the Natural Supervision on Robot Intelligence,IEEE,Conferences,"Human's direct supervision on robot's erroneous behavior is crucial to enhance a robot intelligence for a `flawless' human-robot interaction. Motivating humans to engage more actively for this purpose is however difficult. To alleviate such strain, this research proposes a novel approach, a growth and regression metaphoric interaction design inspired from human's communicative, intellectual, social competence aspect of developmental stages. We implemented the interaction design principle unto a conversational agent combined with a set of synthetic sensors. Within this context, we aim to show that the agent successfully encourages the online labeling activity in response to the faulty behavior of robots as a supervision process. The field study is going to be conducted to evaluate the efficacy of our proposal by measuring the annotation performance of real-time activity events in the wild. We expect to provide a more effective and practical means to supervise robot by real-time data labeling process for long-term usage in the human-robot interaction.",https://ieeexplore.ieee.org/document/8673212/,2019 14th ACM/IEEE International Conference on Human-Robot Interaction (HRI),11-14 March 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/HSI49210.2020.9142636,Lightweight Convolutional Neural Network for Real-Time Face Detector on CPU Supporting Interaction of Service Robot,IEEE,Conferences,"Face detection plays an essential role in the success of the interaction between service robots and consumers. This method is the initial stage for face-related applications. Practical applications require face detection to work in real-time and can be implemented on low-cost devices such as CPU. Traditional methods have problems when the face is not frontal, blocked, and partially covered, but real-time speed is not an obstacle. On the other hand, deep learning has succeeded in accurately distinguishing facial features and backgrounds. Face sizes that tend to be medium and large when robot interaction with consumers so it can employ Convolutional Neural Networks (CNN) with light weights. In this paper, a real-time face detector is built that can work on the CPU. This detector will be implemented explicitly in service robots to support interactions with consumers. It can overcome the occlusion and not-frontal face. Detector architecture consists of the backbone as rapidly features extractor, transition module as a transformer of prediction map, and the dual-detection layer is head of a network prediction based on scale assignment. As a result, the detector can work at speeds of 301 frames per second on CPU without ignoring the accuracy.",https://ieeexplore.ieee.org/document/9142636/,2020 13th International Conference on Human System Interaction (HSI),6-8 June 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICECCO53203.2021.9663861,MAS agents development for mining industry,IEEE,Conferences,"The essence of multi-agent technology is a fundamentally new method of solving problems. In contrast to the classical method, when a search is carried out a well-defined (deterministic) algorithm, which allows find the best solution to the problem in multi-technology solution is obtained automatically as a result of the interaction of many self-parking enforcement targeted software modules - the so-called software agents ants. Often classical methods for solving problems are not applicable in real life. There are various fields where MAS could be implemented, for the research of this paper the mining industry where taken.This paper explains MAS, its classification, shows the possibility of use of agent modeling in real industry. The article describes the steps of the development of agents, and the testing of them on a build-up layout of quarry and on working prototypes of the dumper and excavator robots.",https://ieeexplore.ieee.org/document/9663861/,2021 16th International Conference on Electronics Computer and Computation (ICECCO),25-26 Nov. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/INFOCOM.2018.8485910,MV-Sports: A Motion and Vision Sensor Integration-Based Sports Analysis System,IEEE,Conferences,"Recently, intelligent sports analytics is becoming a hot area in both industry and academia for coaching, practicing tactic and technical analysis. With the growing trend of bringing sports analytics to live broadcasting, sports robots and common playfield, a low cost system that is easy to deploy and performs real-time and accurate sports analytics is very desirable. However, existing systems, such as Hawk-Eye, cannot satisfy these requirements due to various factors. In this paper, we present MV-Sports, a cost-effective system for real-time sports analysis based on motion and vision sensor integration. Taking tennis as a case study, we aim to recognize player shot types and measure ball states. For fine-grained player action recognition, we leverage motion signal for fast action highlighting and propose a long short term memory (LSTM)-based framework to integrate MV data for training and classification. For ball state measurement, we compute the initial ball state via motion sensing and devise an extended kalman filter (EKF)-based approach to combine ball motion physics-based tracking and vision positioning-based tracking to get more accurate ball state. We implement MV-Sports on commercial off-the-shelf (COTS) devices and conduct real-world experiments to evaluate the performance of our system. The results show our approach can achieve accurate player action recognition and ball state measurement with sub-second latency.",https://ieeexplore.ieee.org/document/8485910/,IEEE INFOCOM 2018 - IEEE Conference on Computer Communications,16-19 April 2018,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/HPCC/SmartCity/DSS.2019.00339,Machine Learning Based CloudBot Detection Using Multi-Layer Traffic Statistics,IEEE,Conferences,"With the rapid development of e-commerce services and online transactions, an increasing number of advanced web robots are utilized by speculators and hackers in underground economy to perform click fraud, register fake accounts and commit other kinds of frauds, seriously harming the profit of businesses and the fairness of online activities. There is solid evidence that the vast majority of such malicious bot traffic comes from data centers. The malicious bot deployed on the hosts of data centers is referred to as a CloudBot. How to detect and block CloudBots effectively has become an urgent problem in practice, while the research on it can be seldom seen in public. To this end, we propose a traffic-based quasi-real-time method for CloudBot detection using machine learning, which exploits a new sample partitioning approach, as well as innovative multi-layer features that reveal the essential difference between CloudBots and human traffic. Our method achieves 93.4% precision in the experiment and performs well on the real-world dataset, which proves to be effective to detect unknown CloudBots and combat the concept drift caused by varying time. Besides, the approach is also privacy-preserving without using any specific application layer information. We believe our work can benefit network economy security and fairness in practice.",https://ieeexplore.ieee.org/document/8855536/,2019 IEEE 21st International Conference on High Performance Computing and Communications; IEEE 17th International Conference on Smart City; IEEE 5th International Conference on Data Science and Systems (HPCC/SmartCity/DSS),10-12 Aug. 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/BigData52589.2021.9671792,Machine learning for robot locomotion: Grounded simulation learning and adaptive planner parameter learning,IEEE,Conferences,"Summary form only given. The complete presentation was not made available for publication as part of the conference proceedings. : Robust locomotion is one of the most fundamental requirements for autonomous mobile robots. With the widespread deployment of robots in factories, warehouses, and homes, it is tempting to think that locomotion is a solved problem. However for certain robot morphologies (e.g. humanoids) and environmental conditions (e.g. narrow passages), significant challenges remain. This talk begins by introducing Grounded Simulation Learning as a way to bridge the so-called reality gap between simulators and the real world in order to enable transfer learning from simulation to a real robot (sim-toreal). It then introduces Adaptive Planner Parameter Learning as a way of leveraging human input (learning from demonstration) towards making existing robot motion planners more robust, without losing their safety properties. Grounded Simulation Learning has led to the fastest known stable walk on a widely used humanoid robot, and Adaptive Planner Parameter Learning has led to efficient learning of robust navigation policies in highly constrained spaces.",https://ieeexplore.ieee.org/document/9671792/,2021 IEEE International Conference on Big Data (Big Data),15-18 Dec. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA.2019.8793485,Making Sense of Vision and Touch: Self-Supervised Learning of Multimodal Representations for Contact-Rich Tasks,IEEE,Conferences,"Contact-rich manipulation tasks in unstructured environments often require both haptic and visual feedback. However, it is non-trivial to manually design a robot controller that combines modalities with very different characteristics. While deep reinforcement learning has shown success in learning control policies for high-dimensional inputs, these algorithms are generally intractable to deploy on real robots due to sample complexity. We use self-supervision to learn a compact and multimodal representation of our sensory inputs, which can then be used to improve the sample efficiency of our policy learning. We evaluate our method on a peg insertion task, generalizing over different geometry, configurations, and clearances, while being robust to external perturbations. We present results in simulation and on a real robot.",https://ieeexplore.ieee.org/document/8793485/,2019 International Conference on Robotics and Automation (ICRA),20-24 May 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/DDCLS52934.2021.9455635,Mask-based Object Pose Estimation with Domain Transfer,IEEE,Conferences,"Object pose estimation is important for robots to understand and interact with the real world. This problem is challenging because the various objects, clutter and occlusions between objects in the scene. Deep learning methods show better performances than traditional problems in this problem but training a convolutional neural network needs lots of annotated data which is expensive to obtain. This paper proposes a general method by using domain transfer technology to efficiently solve object pose estimation problem. Besides, the proposed method obtains mask to achieve high quality performance by combing an instance segmentation framework, Mask R-CNN. We present the results of our experiments with the LineMOD dataset. We also deploy our method to robotic grasp object based on the estimated pose.",https://ieeexplore.ieee.org/document/9455635/,2021 IEEE 10th Data Driven Control and Learning Systems Conference (DDCLS),14-16 May 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/RO-MAN47096.2020.9223436,Migratable AI: Effect of identity and information migration on users' perception of conversational AI agents,IEEE,Conferences,"Conversational AI agents are proliferating, embodying a range of devices such as smart speakers, smart displays, robots, cars, and more. We can envision a future where a personal conversational agent could migrate across different form factors and environments to always accompany and assist its user to support a far more continuous, personalized and collaborative experience. This opens the question of what properties of a conversational AI agent migrates across forms, and how it would impact user perception. To explore this, we developed a Migratable AI system where a user's information and/or the agent's identity can be preserved as it migrates across form factors to help its user with a task. We validated the system by designing a 2x2 between-subjects study to explore the effects of information migration and identity migration on user perceptions of trust, competence, likeability and social presence. Our results suggest that identity migration had a positive effect on trust, competence and social presence, while information migration had a positive effect on trust, competence and likeability. Overall, users report highest trust, competence, likeability and social presence towards the conversational agent when both identity and information were migrated across embodiments.",https://ieeexplore.ieee.org/document/9223436/,2020 29th IEEE International Conference on Robot and Human Interactive Communication (RO-MAN),31 Aug.-4 Sept. 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ROBOT.1998.681416,Mobile robot exploration and map-building with continuous localization,IEEE,Conferences,"Our research addresses how to integrate exploration and localization for mobile robots. A robot exploring and mapping an unknown environment needs to know its own location, but it may need a map in order to determine that location. In order to solve this problem, we have developed ARIEL, a mobile robot system that combines frontier based exploration with continuous localization. ARIEL explores by navigating to frontiers, regions on the boundary between unexplored space and space that is known to be open. ARIEL finds these regions in the occupancy grid map that it builds as it explores the world. ARIEL localizes by matching its recent perceptions with the information stored in the occupancy grid. We have implemented ARIEL on a real mobile robot and tested ARIEL in a real-world office environment. We present quantitative results that demonstrate that ARIEL can localize accurately while exploring, and thereby build accurate maps of its environment.",https://ieeexplore.ieee.org/document/681416/,Proceedings. 1998 IEEE International Conference on Robotics and Automation (Cat. No.98CH36146),20-20 May 1998,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/VLSI-TSA.2018.8403807,Mobile/embedded DNN and AI SoCs,IEEE,Conferences,"Summary form only. Recently, Deep Neural Networks are changing not only the technology paradigm in electronics but also the society itself with Artificial Intelligence technologies. In this presentation, firstly, the status of AI and DNN SoCs will be reviewed from two perspectives; the data-center oriented and the mobile and embedded AIs. This dichotomy shows clearly the possible application areas for the emerging future AIs. Especially, mobile and embedded deep learning hardware will be introduced together with CNN (Convolutional Neural Network) and RNN (Recurrent Neural Network). In addition, real CMOS chip implementation results of mobile/embedded DNNs will be explained with measurement results. Secondly, KAIST's approach integrating both sides of brain, right-brain for ""approximation and adaptation hardware"" and left-brain for""precise and programmable Von Neumann architecture"", will be explained with novel design methodology. The deep neural networks and the specialized intelligent hardware (mimicking right brain) capable of statistical processing or learning and the multi-core processors (mimicking left brain) performing the precise computations including software AI are integrated on the same SoC. Based on this brain-mimicking SoCs, the object recognition and the augmented reality applications are implemented with low-power and high-performance for wearable devices such as smart glasses, autonomous vehicles, and intelligent robots.",https://ieeexplore.ieee.org/document/8403807/,"2018 International Symposium on VLSI Technology, Systems and Application (VLSI-TSA)",16-19 April 2018,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICARM49381.2020.9195341,Model-Based Reinforcement Learning For Robot Control,IEEE,Conferences,"Model-free deep reinforcement learning (MFRL) algorithms have achieved many impressive results. But they are generally stricken with high sample complexity, which puts forward a critical challenge for their application to real-world robots. Dynamic models are essential for robot control laws, but it is often hard to obtain accurate analytical dynamic models. Therefore a data-driven approach to learning models becomes significant for reinforcement learning to increase data efficiency. Model-based algorithms are effective methods to reduce sample complexity by learning the system dynamic model. However, in certain environments, it has been proven that learning an accurate system dynamic model is a formidable problem, and their asymptotic performance cannot achieve to the same level as model-free algorithms. In our work, we use an ensemble of deep neural networks to learn system dynamics and incorporate model uncertainty. Then in order to merge the high asymptotic performance of the advanced model-free methods, the deep deterministic policy gradient (DDPG) algorithm is adopted to optimize robot control policy. Furthermore, it has been implemented within ROS for controlling a Baxter robot in the simulation environment.",https://ieeexplore.ieee.org/document/9195341/,2020 5th International Conference on Advanced Robotics and Mechatronics (ICARM),18-21 Dec. 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA.2016.7487661,Model-predictive control with stochastic collision avoidance using Bayesian policy optimization,IEEE,Conferences,"Robots are increasingly expected to move out of the controlled environment of research labs and into populated streets and workplaces. Collision avoidance in such cluttered and dynamic environments is of increasing importance as robots gain more autonomy. However, efficient avoidance is fundamentally difficult since computing safe trajectories may require considering both dynamics and uncertainty. While heuristics are often used in practice, we take a holistic stochastic trajectory optimization perspective that merges both collision avoidance and control. We examine dynamic obstacles moving without prior coordination, like pedestrians or vehicles. We find that common stochastic simplifications lead to poor approximations when obstacle behavior is difficult to predict. We instead compute efficient approximations by drawing upon techniques from machine learning. We propose to combine policy search with model-predictive control. This allows us to use recent fast constrained model-predictive control solvers, while gaining the stochastic properties of policy-based methods. We exploit recent advances in Bayesian optimization to efficiently solve the resulting probabilistically-constrained policy optimization problems. Finally, we present a real-time implementation of an obstacle avoiding controller for a quadcopter. We demonstrate the results in simulation as well as with real flight experiments.",https://ieeexplore.ieee.org/document/7487661/,2016 IEEE International Conference on Robotics and Automation (ICRA),16-21 May 2016,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IJCNN.2017.7965938,Modeling direction selective visual neural network with ON and OFF pathways for extracting motion cues from cluttered background,IEEE,Conferences,"The nature endows animals robust vision systems for extracting and recognizing different motion cues, detecting predators, chasing preys/mates in dynamic and cluttered environments. Direction selective neurons (DSNs), with preference to certain orientation visual stimulus, have been found in both vertebrates and invertebrates for decades. In this paper, with respect to recent biological research progress in motion-detecting circuitry, we propose a novel way to model DSNs for recognizing movements on four cardinal directions. It is based on an architecture of ON and OFF visual pathways underlies a theory of splitting motion signals into parallel channels, encoding brightness increments and decrements separately. To enhance the edge selectivity and speed response to moving objects, we put forth a bio-plausible spatial-temporal network structure with multiple connections of same polarity ON/OFF cells. Each pair-wised combination is filtered with dynamic delay depending on sampling distance. The proposed vision system was challenged against image streams from both synthetic and cluttered real physical scenarios. The results demonstrated three major contributions: first, the neural network fulfilled the characteristics of a postulated physiological map of conveying visual information through different neuropile layers; second, the DSNs model can extract useful directional motion cues from cluttered background robustly and timely, which hits at potential of quick implementation in vision-based micro mobile robots; moreover, it also represents better speed response compared to a state-of-the-art elementary motion detector.",https://ieeexplore.ieee.org/document/7965938/,2017 International Joint Conference on Neural Networks (IJCNN),14-19 May 2017,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA.2015.7139899,Modeling of movement control architectures based on motion primitives using domain-specific languages,IEEE,Conferences,"This paper introduces a model-driven approach for engineering complex movement control architectures based on motion primitives, which in recent years have been a central development towards adaptive and flexible control of complex and compliant robots. We consider rich motor skills realized through the composition of motion primitives as our domain. In this domain we analyze the control architectures of representative example systems to identify common abstractions. It turns out that the introduced notion of motion primitives implemented as dynamical systems with machine learning capabilities, provide the computational building block for a large class of such control architectures. Building on the identified concepts, we introduce domain-specific languages that allow the compact specification of movement control architectures based on motion primitives and their coordination respectively. Using a proper tool chain, we show how to employ this model-driven approach in a case study for the real world example of automatic laundry grasping with the KUKA LWR-IV, where executable source-code is automatically generated from the domain-specific language specification.",https://ieeexplore.ieee.org/document/7139899/,2015 IEEE International Conference on Robotics and Automation (ICRA),26-30 May 2015,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ROMAN.2005.1513775,Modularity and integration in the design of a socially interactive robot,IEEE,Conferences,"Designing robots that are capable of interacting with humans in real life settings is a challenging task. One key issue is the integration of multiple modalities (e.g., mobility, physical structure, navigation, vision, audition, dialogue, reasoning) into a coherent framework. Taking the AAAI mobile robot challenge (making a robot attend the national conference on artificial intelligence) as the experimental context, we are currently addressing hardware, software and computation integration issues involved in designing a robot capable of sophisticated interaction with humans. This paper reports on our design solutions and the current status of the work, along with the potential impacts this design on human-robot interaction research.",https://ieeexplore.ieee.org/document/1513775/,"ROMAN 2005. IEEE International Workshop on Robot and Human Interactive Communication, 2005.",13-15 Aug. 2005,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS.2018.8593871,"Motion Planning Among Dynamic, Decision-Making Agents with Deep Reinforcement Learning",IEEE,Conferences,"Robots that navigate among pedestrians use collision avoidance algorithms to enable safe and efficient operation. Recent works present deep reinforcement learning as a framework to model the complex interactions and cooperation. However, they are implemented using key assumptions about other agents' behavior that deviate from reality as the number of agents in the environment increases. This work extends our previous approach to develop an algorithm that learns collision avoidance among a variety of types of dynamic agents without assuming they follow any particular behavior rules. This work also introduces a strategy using LSTM that enables the algorithm to use observations of an arbitrary number of other agents, instead of previous methods that have a fixed observation size. The proposed algorithm outperforms our previous approach in simulation as the number of agents increases, and the algorithm is demonstrated on a fully autonomous robotic vehicle traveling at human walking speed.",https://ieeexplore.ieee.org/document/8593871/,2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),1-5 Oct. 2018,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICAR.2013.6766513,Move and the robot will learn: Vision-based autonomous learning of object models,IEEE,Conferences,"As robots are increasingly deployed in complex real-world domains, visual object recognition continues to be an open problem. Existing algorithms for learning and recognizing objects are predominantly computationally expensive, and require considerable training or domain knowledge. Our algorithm enables robots to use motion cues to identify and focus on a set of interesting objects, automatically extracting appearance-based and contextual cues from a small number of images to efficiently learn representative models of these objects. Learned models exploit complementary strengths of: (a) relative spatial arrangement of gradient features; (b) graph-based models of neighborhoods of gradient features; (c) parts-based models of image segments; (d) color distributions; and (e) mixture models of local context. The learned models are used in conjunction with an energy minimization algorithm and a generative model of information fusion for reliable and efficient recognition in novel scenes. The algorithm is evaluated on mobile robots in indoor and outdoor domains, and on images from benchmark datasets.",https://ieeexplore.ieee.org/document/6766513/,2013 16th International Conference on Advanced Robotics (ICAR),25-29 Nov. 2013,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CCDC49329.2020.9164429,Multi-Agent Formation Design Considering Minimum Fuel Consumption,IEEE,Conferences,"Formation control of multiple autonomous robots is an important part of autonomous robot research. And theoretical research should be based on reality. Therefore, thinking of each robot as an agent, this article considers the problems in actual situation of multi-agent formation control, namely energy consumption. The formation is accompanied by energy consumption. If fuel conservation is not considered, it will lead to three problems: 1. Environmental pollution; 2. The formation task cannot be carried out for a long time; 3. Carrying a large amount of fuel will definitely affect the flexibility of autonomous robots. Therefore, how to reduce energy consumption while implementing formation tasks is the focus of this paper. In this paper, the directed graph in graph theory is used to simulate the communication links between multi-agents for continuous systems and discrete systems in general linear systems. According to the linear quadratic method, optimal controllers are designed respectively to realize the control of target formation. At the same time, fuel consumption is reduced as much as possible to achieve the minimum performance index.",https://ieeexplore.ieee.org/document/9164429/,2020 Chinese Control And Decision Conference (CCDC),22-24 Aug. 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/INFOCOM42981.2021.9488669,Multi-Robot Path Planning for Mobile Sensing through Deep Reinforcement Learning,IEEE,Conferences,"Mobile sensing is an effective way to collect environmental data such as air quality, humidity and temperature at low costs. However, mobile robots are typically battery powered and have limited travel distances. To accelerate data collection in large geographical areas, it is beneficial to deploy multiple robots to perform tasks in parallel. In this paper, we investigate the Multi-Robot Informative Path Planning (MIPP) problem, namely, to plan the most informative paths in a target area subject to the budget constraints of multiple robots. We develop two deep reinforcement learning (RL) based cooperative strategies: independent learning through credit assignment and sequential rollout based learning for MIPP. Both strategies are highly scalable with the number of robots. Extensive experiments are conducted to evaluate the performance of the proposed and baseline approaches using real-world WiFi Received Signal Strength (RSS) data. In most cases, the RL based solutions achieve superior or similar performance as a baseline genetic algorithm (GA)-based solution but at only a fraction of running time during inference. Furthermore, when the budgets and initial positions of the robots change, the pre-trained policies can be applied directly.",https://ieeexplore.ieee.org/document/9488669/,IEEE INFOCOM 2021 - IEEE Conference on Computer Communications,10-13 May 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS.2015.7354094,Multi-robot 6D graph SLAM connecting decoupled local reference filters,IEEE,Conferences,"Teams of mobile robots can be deployed in search and rescue missions to explore previously unknown environments. Methods for joint localization and mapping constitute the basis for (semi-)autonomous cooperative action, in particular when navigating in GPS-denied areas. As communication losses may occur, a decentralized solution is required. With these challenges in mind, we designed a submap-based SLAM system that relies on inertial measurements and stereo-vision to create multi-robot dense 3D maps. For online pose and map estimation, we integrate the results of keyframe-based local reference filters through incremental graph SLAM. To the best of our knowledge, we are the first to combine these two methods to benefit from their particular advantages for 6D multi-robot localization and mapping: Local reference filters on each robot provide real-time, long-term stable state estimates that are required for stabilization, control and fast obstacle avoidance, whereas online graph optimization provides global multi-robot pose and map estimates needed for cooperative planning. We propose a novel graph topology for a decoupled integration of local filter estimates from multiple robots into a SLAM graph according to the filters' uncertainty estimates and independence assumptions and evaluated its benefits on two different robots in indoor, outdoor and mixed scenarios. Further, we performed two extended experiments in a multi-robot setup to evaluate the full SLAM system, including visual robot detections and submap matches as inter-robot loop closure constraints.",https://ieeexplore.ieee.org/document/7354094/,2015 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),28 Sept.-2 Oct. 2015,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS.2001.977153,Multi-robot path planning for dynamic environments: a case study,IEEE,Conferences,"Most multi-robot navigation planning methods make assumptions about the kind of navigation problems they are to solve and the capabilities of the robots they are to control. In this paper, we propose to select problem-adequate navigation planning methods based on empirical investigations, that is, the robots should learn by experimentation to use the best planning methods. To support this development strategy we provide software tools that enable the robots to automatically learn predictive models for the performance of different navigation planning methods in a given application domain. We show, in the context of robot soccer, that the hybrid planning method which selects planning methods based on a learned predictive model outperforms the individual planning methods. The results are validated in extensive experiments using a realistic and accurate robot simulator that has learned the dynamic model of the real robots.",https://ieeexplore.ieee.org/document/977153/,Proceedings 2001 IEEE/RSJ International Conference on Intelligent Robots and Systems. Expanding the Societal Role of Robotics in the the Next Millennium (Cat. No.01CH37180),29 Oct.-3 Nov. 2001,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS.2018.8593899,Multisensor Online Transfer Learning for 3D LiDAR-Based Human Detection with a Mobile Robot,IEEE,Conferences,"Human detection and tracking is an essential task for service robots, where the combined use of multiple sensors has potential advantages that are yet to be fully exploited. In this paper, we introduce a framework allowing a robot to learn a new 3D LiDAR-based human classifier from other sensors over time, taking advantage of a multisensor tracking system. The main innovation is the use of different detectors for existing sensors (i.e. RGB-D camera, 2D LiDAR) to train, online, a new 3D LiDAR-based human classifier based on a new “trajectory probability”. Our framework uses this probability to check whether new detection belongs to a human trajectory, estimated by different sensors and/or detectors, and to learn a human classifier in a semi-supervised fashion. The framework has been implemented and tested on a real-world dataset collected by a mobile robot. We present experiments illustrating that our system is able to effectively learn from different sensors and from the environment, and that the performance of the 3D LiDAR-based human classification improves with the number of sensors/detectors used.",https://ieeexplore.ieee.org/document/8593899/,2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),1-5 Oct. 2018,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SECON.2000.845570,Navigational task-planning of distributed cooperative robotic vehicles,IEEE,Conferences,"Real time obstacle avoidance is one of the intelligent tasks for intelligent mobile robots. Mobile robots should possess the ability to act autonomously in the presence of uncertainty and to adjust their action based on sensed information. They also should be capable of accepting high level mission oriented commands, integrate several kinds of data, including task specification, able to handle information about their own state and the state of the environment too, and be capable of reasoning under uncertainty without human intervention. The proposed technique is a behavior-based approach that blends subset navigational behaviors such as reflexive, potential field and wall following techniques. We discuss each approach separately and present a technique for adaptive navigational behavior switching that is conditional based on the availability of sensory information locally and globally. Our navigation control strategies are developed fully in the FMCell environment. We present some simulation results from FMCell software.",https://ieeexplore.ieee.org/document/845570/,Proceedings of the IEEE SoutheastCon 2000. 'Preparing for The New Millennium' (Cat. No.00CH37105),9-9 April 2000,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/eIT53891.2022.9814051,Near-Optimal Synchronization of Multiple Robot Carts Using Online Reinforcement Learning,IEEE,Conferences,"This paper studies the optimal synchronization of multiple robot carts using online reinforcement learning. The objective is to validate the use of online reinforcement learning to cooperatively control multiagent systems, which may render the potential applications in optimization and control of engineered multiple dynamical systems, such as autonomous vehicles platoon, power grids, sensor networks, et.al. To control the robots, we pro-pose an online reinforcement learning based cooperative control algorithm, which enables the individual agent to learn its own optimal control law in real time by minimizing the overall group cost function. The result of the online reinforcement learning leads to a model free near optimal cooperative control algorithm for multiagent systems. To experimentally verify the control algorithm, two robots are constructed with several integrated sensor modules. Via the XBee module, robots can communicate position and velocity information to one another and receive position and velocity information from a virtual leader. This virtual leader is following a desired trajectory, allowing us to see the control algorithm synchronize the two followers with the leader on a desired trajectory. The control algorithm is implemented using an NVIDIA Jetson Nano. The testing results have proved the effectiveness of the proposed design.",https://ieeexplore.ieee.org/document/9814051/,2022 IEEE International Conference on Electro Information Technology (eIT),19-21 May 2022,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/UR55393.2022.9826265,Neural Network Model of eFAST Target Prediction for Robotic Ultrasound Diagnostics in Austere Environments,IEEE,Conferences,"Modern robotic technology has the potential to solve complex problems in healthcare, such as providing technology to support medical care in austere environments characterized by the restricted availability of local medical professionals or with high patient-to-caregiver ratios. Medical robots can be deployed as a force multiplier in situations when skilled healthcare providers are limited and can reduce the risk of medical failures during diagnostic and intervention procedures. One example is detecting free-flowing blood in the abdomen and pneumothorax by performing the extended Focused Assessment with Sonography for Trauma (eFAST) ultrasound diagnostics examination. We developed a semi-autonomous robotic ultrasound system that intelligently perceives the patient&#x2019;s pose and determines the corresponding eFAST configuration for the robotic-assisted procedure. The dynamic pose of the patient is identified in real-time using an optimization-based algorithm, while the eFAST configurations are predicted by a neural network model. A robot manipulator holding an ultrasound device is autonomously driven to the body-normal vector of each eFAST target. This approach accomplishes robust prediction for non-linear problems, even with dynamic posture of the detected body in highly unstructured sites with variable patient shape and pose.",https://ieeexplore.ieee.org/document/9826265/,2022 19th International Conference on Ubiquitous Robots (UR),4-6 July 2022,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICIT.2004.1490796,Neural network based control of a four rotor helicopter,IEEE,Conferences,"In this paper the design and development of an intelligent controller based on neural networks for a hoverable flying robot to be capable of achieving vertical take off and landing and to be able to sustain a specified attitude is presented. The ability to be able to autonomously navigate through a predefined path was designated for a future phase. This work is different from most autonomous flying robots as it focuses on a four-propeller configuration. This is a very rare helicopter design because of its inherent instability and it is believed that an autonomous robot of this configuration has not yet been successfully developed. In addition, this project uses fixed pitch propellers instead of variable pitch rotors resulting in a greatly reduced cost and mechanical complexity. The downside is that this introduces significant additional challenges in the control. Relative stability was achieved in three axis and all the supporting modules were successfully designed and implemented. However, significant challenges were encountered including the complexities of creating a neural networks controller (NNC) to work in real-time in a slow microcontroller as well as to develop the training process.",https://ieeexplore.ieee.org/document/1490796/,"2004 IEEE International Conference on Industrial Technology, 2004. IEEE ICIT '04.",8-10 Dec. 2004,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/COASE.2008.4626446,Neural network based path planning for a multi-robot system with moving obstacles,IEEE,Conferences,"Recently, a Coordinated Hybrid Agent (CHA) framework was proposed for the control of Multi-Agent Systems (MASs). In the past few years, it has been applied to both homogeneous and heterogeneous multi-agent systems. In previous studies, the coordination among agents were implemented based on the designer’s knowledge of the system. For large complex systems, it would be desirable if we can plan the coordination among agents dynamically. It was demonstrated that an intelligent planner can be designed for the CHA framework to automatically generate desired actions for multiple robots in a multi-agent system. However, in previous studies, only static obstacles in the environment were considered. In this paper, a neural network based approach is proposed for a multi-robot system with moving obstacles. A biologically inspired neural network based intelligent planner is designed for the coordination of multi-agent systems. The dynamics of each neuron in the topologically organized neural network is characterized by a shunting neural equation. A landscape of the neural activities for all neurons of a CHA agent contains information about the agent’s local goal, and moving obstacles. The objective for building the intelligent planner is to plan actions for multiple mobile robots to coordinate with others and to achieve the global goal. The proposed approach is able to plan the paths for multiple robots while avoiding moving obstacles. The proposed approach is simulated using both Matlab and Vortex. The virtual physical world is built using Vortex to test and develop navigation strategies for robot platforms. The Vortex module executes control commands from the control system module, and provides the outputs describing the vehicle state and terrain information, which are in turn used in the control module to produce the control commands. Simulation results show that an intelligent planner can be designed for the CHA framework to control a large complex system so that coordination among agents can be achieved.",https://ieeexplore.ieee.org/document/4626446/,2008 IEEE International Conference on Automation Science and Engineering,23-26 Aug. 2008,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1145/2830772.2830789,Neuromorphic accelerators: A comparison between neuroscience and machine-learning approaches,IEEE,Conferences,"A vast array of devices, ranging from industrial robots to self-driven cars or smartphones, require increasingly sophisticated processing of real-world input data (image, voice, radio, ...). Interestingly, hardware neural network accelerators are emerging again as attractive candidate architectures for such tasks. The neural network algorithms considered come from two, largely separate, domains: machine-learning and neuroscience. These neural networks have very different characteristics, so it is unclear which approach should be favored for hardware implementation. Yet, few studies compare them from a hardware perspective. We implement both types of networks down to the layout, and we compare the relative merit of each approach in terms of energy, speed, area cost, accuracy and functionality. Within the limit of our study (current SNN and machine learning NN algorithms, current best effort at hardware implementation efforts, and workloads used in this study), our analysis helps dispel the notion that hardware neural network accelerators inspired from neuroscience, such as SNN+STDP, are currently a competitive alternative to hardware neural networks accelerators inspired from machine-learning, such as MLP+BP: not only in terms of accuracy, but also in terms of hardware cost for realistic implementations, which is less expected. However, we also outline that SNN+STDP carry potential for reduced hardware cost compared to machine-learning networks at very large scales, if accuracy issues can be controlled (or for applications where they are less important). We also identify the key sources of inaccuracy of SNN+STDP which are less related to the loss of information due to spike coding than to the nature of the STDP learning algorithm. Finally, we outline that for the category of applications which require permanent online learning and moderate accuracy, SNN+STDP hardware accelerators could be a very cost-efficient solution.",https://ieeexplore.ieee.org/document/7856622/,2015 48th Annual IEEE/ACM International Symposium on Microarchitecture (MICRO),5-9 Dec. 2015,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.23919/ASCC56756.2022.9828170,Object-Oriented Navigation with a Multi-layer Semantic Map,IEEE,Conferences,"In this paper, a novel architecture is proposed for object-oriented navigation of a mobile robot. The idea is to autonomous navigate and docking with respect to a given object in an indoor environment. For this purpose, the navigation scheme is designed based on a novel multi-layer semantic map. The map includes an occupancy layer for path planning, a localization layer for control and an object pose estimation layer for docking control. The two-dimensional (2D) occupancy map is generated from a 3D RTAB-Map method. The multi-layer semantic map integrates the outputs of a deep-learning model of real-time 3D object pose estimation for robot localization. Experimental results show that the proposed navigation system can plan a collision-free path for the mobile robot to reach and dock to the target object. The LiDAR-based navigation control runs at 15 Hz and the object-based localization is updated at 4 Hz on the Jetson TX2 embedded controller. In the current implementation, the mobile robot can dock to an object with an accuracy of 8 cm in position and 5 degrees in orientation. The navigation design has the potential to be applied for daily life tasks of mobile robots in an indoor environment.",https://ieeexplore.ieee.org/document/9828170/,2022 13th Asian Control Conference (ASCC),4-7 May 2022,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CYBER.2017.8446065,Obstacle avoidance of aerial vehicle based on monocular vision,IEEE,Conferences,"Collision-free autonomous navigation is extremely important for quadcopter and other flying robots. The implementation of autonomic moving capabilities can contribute significantly to their promotion and usage in fields such as goods delivering, aerial photos shooting, and monitoring. In order to realize the autonomous flight without crash, the obstacle avoidance problem demands a prompt solution. Also, with the concern of cost and endurance, using only single camera to perform this task would be a better choice for low-cost flying robots. Thus, this paper focuses on achieving quadcopter's collision avoidance in unknown stable (rarely changes, such as high sky or inner room) environment only by single camera. The algorithm proposed by this paper is composed of PTAM (Parallel Tracking and Mapping), DTAM (Dense Tracking and Mapping in Real-Time) algorithm and CNN (Convolutional Neural Network). PTAM is used to create the 3D map of the environment. DTAM is used to obtain the depth map of those image frames. And the CNN is used to train and get a model used for automatically avoidance. Finally, this algorithm is proved to be valid by an experiment.",https://ieeexplore.ieee.org/document/8446065/,"2017 IEEE 7th Annual International Conference on CYBER Technology in Automation, Control, and Intelligent Systems (CYBER)",31 July-4 Aug. 2017,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICDARW.2019.30062,OctShuffleMLT: A Compact Octave Based Neural Network for End-to-End Multilingual Text Detection and Recognition,IEEE,Conferences,"In recent years, scene text detection has witnessed rapid progress especially with the recent development of convolutional neural networks. However, there still exist many challenges in applying very deep networks to many real-world applications, that have hardware limitations, such as robots, and smartphones. To address these challenges, in this paper, we propose the OctShuffleMLT, an effective fully convolutional neural network, with fewer layers and parameters, which can precisely detect multilingual scene text. Our proposed model is based on the Octave Convolutions that use compact blocks, which reduces memory inference by 13.16%, FLOPS by 71.86%, and the number of parameters by 34.04% when compared to the baseline system. Extensive experiments were conducted on ICDAR 2015 and ICDAR 2017 datasets. Experimental results show that our model can produce accurate detection recognition results on both datasets. The code for the paper is made available on the GitHub repository https://github.com/victoic/OctShuffle-MLT.",https://ieeexplore.ieee.org/document/8892934/,2019 International Conference on Document Analysis and Recognition Workshops (ICDARW),22-25 Sept. 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA46639.2022.9812026,Off Environment Evaluation Using Convex Risk Minimization,IEEE,Conferences,"Applying reinforcement learning (RL) methods on robots typically involves training a policy in simulation and deploying it on a robot in the real world. Because of the model mismatch between the real world and the simulator, RL agents deployed in this manner tend to perform suboptimally. To tackle this problem, researchers have developed robust policy learning algorithms that rely on synthetic noise disturbances. However, such methods do not guarantee performance in the target environment. We propose a convex risk minimization algorithm to estimate the model mismatch between the simulator and the target domain using trajectory data from both environments. We show that this estimator can be used along with the simulator to evaluate performance of an RL agents in the target domain, effectively bridging the gap between these two environments. We also show that the convergence rate of our estimator to be of the order of <tex>$n^{-1/4}$</tex>, where <tex>$n$</tex> is the number of training samples. In simulation, we demonstrate how our method effectively approximates and evaluates performance on Gridworld, Cartpole, and Reacher environments on a range of policies. We also show that the our method is able to estimate performance of a 7 DOF robotic arm using the simulator and remotely collected data from the robot in the real world.",https://ieeexplore.ieee.org/document/9812026/,2022 International Conference on Robotics and Automation (ICRA),23-27 May 2022,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/URAI.2018.8441890,On Humanoid Co-Robot Locomotion when Mechanically Coupled to a Human Partner,IEEE,Conferences,"This work focuses on the implementation of mechanically coupled tasks between a humanoid robot and a human. The latter focus comes from the push for robots to work with humans in everyday life as an overarching goal for the field. Co-robots, or robots that work alongside humans, may be guided by the humans through physical contact, such as the human grasping the robot's hand to gently guide it along a desired path. In this work the single-handed mechanically coupled task of guiding a robot through a course is implemented with four different methods of human input. These methods include: 1) using only force-torque sensors in the wrist of the robot for the control input from the human while the arm is under high-gain position control, creating a rigid coupling between the human and the robot, 2) using the force-torque sensors in the wrist of the robot for the control input while the arm is under low-gain position control with gravity compensation, creating compliant coupling between the human and the robot, 3) using the position of the end-effector of the robot for the control input while the arm is under low-gain position control with gravity compensation, and 4) using the force-torque sensors in the wrist and the position of the end-effector of the robot for the control input while the arm is under low-gain position control with gravity compensation. Tests are performed on the real-world and simulated adult-size humanoid robot DRC-Hubo++. During these tests the human and robot are walking together “hand in hand” with the human guiding the robot in a “figure eight” path. These tests show that having a compliant arm on the robot, when the human is guiding it via moving its end-effector, is beneficial over a rigid arm.",https://ieeexplore.ieee.org/document/8441890/,2018 15th International Conference on Ubiquitous Robots (UR),26-30 June 2018,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SSRR.2016.7784329,On Using Mobile Robotic Relays for Adaptive Communication in Search and Rescue Missions,IEEE,Conferences,"This work addresses the problem of deploying and controlling mobile relays in robotic networks. We consider a general data collection scenario in which a group of robots stream data towards one or more base stations. The robots form a wireless mobile ad hoc network relaying data towards the base stations in a multi-hop fashion. Additional robots are used as mobile relays with the aim of optimizing overall network performance in terms of data throughput. To dynamically control the positions of these relays, we use the combination of a mathematical programming model that includes a number of constraints and penalties to closely model the wireless environment, and heuristics to account for variations in wireless links stability. We demonstrate that the approach is computationally affordable for reasonably sized networks. The approach is evaluated in realistic simulation scenarios, studying the impact of the model parameters on the quality of the provided solutions. An experimental validation is also carried out in a real testbed, confirming the efficacy of the proposed approach for adapting relays' positions to the changes in the robot network.",https://ieeexplore.ieee.org/document/7784329/,"2016 IEEE International Symposium on Safety, Security, and Rescue Robotics (SSRR)",23-27 Oct. 2016,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/COGINF.2008.4639192,On Visual Semantic Algebra (VSA) and the cognitive process of pattern recognition,IEEE,Conferences,"A new form of denotational mathematics known as Visual Semantic Algebra (VSA) is presented for abstract visual object and architecture manipulation. The cognitive theories for pattern recognition, such as cognitive principles of visual perception and basic mechanisms of object and pattern recognition, are explored. A number of case studies on VSA in pattern recognition are presented to demonstrate VAS’ expressive power for algebraic manipulation of visual objects. The cognitive process of pattern recognition is rigorously modeled using VSA and Real-Time Process Algebra (RTPA), which reveals the fundamental mechanisms of natural pattern recognition by the brain. The theories and case studies demonstrate that VSA can be applied not only in machine visual and spatial reasoning, but also in computational intelligence system designs as a powerful man-machine language for representing and manipulating visual geometrical systems. On the basis of VSA, computational intelligence systems such as robots and cognitive computers may process and inference visual and image object rigorously and efficiently.",https://ieeexplore.ieee.org/document/4639192/,2008 7th IEEE International Conference on Cognitive Informatics,14-16 Aug. 2008,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SSCI50451.2021.9660138,On the suitability of incremental learning for regression tasks in exoskeleton control,IEEE,Conferences,"In recent times, a new generation of modern exoskeleton robots has come into existence, that aims to utilize machine learning to learn the specific needs and preferences of its users. A simple way to facilitate a personalization of an exoskeleton to the end user is to make use of incremental algorithms that keep learning throughout their deployment. However, it is not clear, if any standard algorithms are fast enough to keep pace with sudden change points in the data stream, like for example the change in movement pattern from a normal walk to going up the stairs. In this paper, we study how well common incremental regression algorithms are suited to predict such an ongoing data stream. We use both, theoretical benchmarks and real world human movement data, to evaluate how fast an algorithm reacts to change points in the data, and how well it is able to remember reoccurring patterns. The results show that a simple KNN algorithm outperforms all other more sophisticated models.",https://ieeexplore.ieee.org/document/9660138/,2021 IEEE Symposium Series on Computational Intelligence (SSCI),5-7 Dec. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/M2VIP49856.2021.9665036,On-the-fly Learning of New Objects and Instances,IEEE,Conferences,"This paper focus on efficient learning of new objects by robots, at both instance level and class level. Despite the great achievements in deep learning, new objects are not well handled due to the big data dependency and the closed set assumption, which are not satisfied in many applications. For example, in service robots, the desired objects vary among customers and environments, and it is hard to collect big data for all objects. In addition, new objects occur from time to time that are not contained in the training data. This paper aims to learn new objects on-the-fly after deployment of the robot, without the dependency on pre-defined big data. A practical system is proposed to learn a new instance in 1.5 minutes and a new class in 15 to 25 minutes, by integrating state-of-the-art works including online learning, incremental learning, salient detection, object detection, tracking, re-identification, etc. A novel incremental object detection framework is further proposed with better performance than the state-of-the-arts. Extensive evaluations are conducted to examine the contribution of each module. The proposed system is also deployed on a real robot for end to end performance test.",https://ieeexplore.ieee.org/document/9665036/,2021 27th International Conference on Mechatronics and Machine Vision in Practice (M2VIP),26-28 Nov. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AIMS.2014.23,Online Tool for Benchmarking of Simulated Intervention Autonomous Underwater Vehicles: Evaluating Position Controllers in Changing Underwater Currents,IEEE,Conferences,"Benchmarking is nowadays an issue on robotic research platforms, due to the fact that it is not easy to reproduce previous experiments and knowing in detail in which real conditions other algorithms have been performed. Having a web-based tool to configure and execute benchmarks opens the door to new opportunities as the design of virtual tele-laboratories that permit the implementation of new algorithms using specific and detailed constraints. This is fundamental for designing benchmarks that allow the experiments to be made in a more scientific manner, taking into account that these experiments should be able to be reproduced again by other people under the same circumstances. In the context of underwater interventions with semi-autonomous robots, the situation gets even more interesting, specially those performed on real sea scenarios, which are expensive, and difficult to perform and reproduce. This paper presents the recent advances in the online configuration tool for benchmarking, a tool that is continuously being improved in our laboratory. Our last contribution focuses on evaluating position controllers for changing underwater currents and the possibility for the user to upload its own controllers to the benchmarking tool to get online performance results.",https://ieeexplore.ieee.org/document/7102468/,"2014 2nd International Conference on Artificial Intelligence, Modelling and Simulation",18-20 Nov. 2014,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS.2017.8206344,Online multi-target learning of inverse dynamics models for computed-torque control of compliant manipulators,IEEE,Conferences,"Inverse dynamics models are applied to a plethora of robot control tasks such as computed-torque control, which are essential for trajectory execution. The analytical derivation of such dynamics models for robotic manipulators can be challenging and depends on their physical characteristics. This paper proposes a machine learning approach for modeling inverse dynamics and provides information about its implementation on a physical robotic system. The proposed algorithm can perform online multi-target learning, thus allowing efficient implementations on real robots. Our approach has been tested both offline, on datasets captured from three different robotic systems and online, on a physical system. The proposed algorithm exhibits state-of-the-art performance in terms of generalization ability and convergence. Furthermore, it has been implemented within ROS for controlling a Baxter robot. Evaluation results show that its performance is comparable to the built-in inverse dynamics model of the robot.",https://ieeexplore.ieee.org/document/8206344/,2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),24-28 Sept. 2017,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCE-Berlin47944.2019.8966156,Optimizing Deep Learning Based Semantic Video Segmentation on Embedded GPUs,IEEE,Conferences,"Decision making in many industries today is being improved drastically thanks to artificial intelligence and deep learning. New algorithms address challenges such as genome mapping, medical diagnostics, self-driving cars, autonomous robots and more. Deep learning in embedded systems requires high optimization due to the high computational demand, given that power, heat dissipation, size and price constraints are numerous. In this paper we analyze several acceleration methods which include utilization of GPUs for most complex variants of deep learning, such as semantic video segmentation operating in real time. Specifically, we propose mapping of acceleration routines commonly present within deep learning SDKs to different network layers in semantic segmentation. Finally, we evaluate one implementation utilizing the enumerated techniques for semantic segmentation of front camera in autonomous driving front view.",https://ieeexplore.ieee.org/document/8966156/,2019 IEEE 9th International Conference on Consumer Electronics (ICCE-Berlin),8-11 Sept. 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CDC.2006.377499,Path Generation Using Matrix Representations of Previous Robot State Data,IEEE,Conferences,"Humans learn by repetition and using past experiences. It is possible for robots to act in a similar fashion. By representing past path traversal experiences with matrices, a new path can be generated without relying on calculations of complex dynamics or control laws. This paper presents one approach for allowing robots to use past experience to generate new paths and control actions. This approach relies on using several matrices to associate each new input value with previous robot states. An example is provided and analyzed which shows a successful simulated implementation of this approach. In addition a real world test of the approach was conducted which demonstrates that the implementation not only generates new paths, but does so fast enough to be feasible for real time systems",https://ieeexplore.ieee.org/document/4178112/,Proceedings of the 45th IEEE Conference on Decision and Control,13-15 Dec. 2006,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IWECAI50956.2020.00019,Path Planning Obstacle Avoidance Algorithm Based on Wheeled Robot,IEEE,Conferences,"There are many obstacles and movements in the indoor environment. Indoor robots need to cope with the changing environment. This paper studies the obstacle avoidance problem of wheeled robots moving in an unknown environment. Firstly, the dynamic path planning algorithm for robot autonomous obstacle avoidance is studied, and the algorithm is implemented in C# language. Then use the Unity3D game engine to simulate the algorithm. The innovations of this algorithm are as follows: 1. Vectorize the path of the robot; 2. Summarize the motion state of the obstacle and the robot into six cases. During the movement process, the obstacle movement state is continuously judged, and the speed and direction of the obstacle are analyzed. The judgment result must belong to six situations. The experiment proves that the algorithm can solve the obstacle avoidance problem when encountering obstacles of different speeds and sizes, and has stronger applicability.",https://ieeexplore.ieee.org/document/9221693/,2020 International Workshop on Electronic Communication and Artificial Intelligence (IWECAI),12-14 June 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/DISA.2018.8490605,Path Planning on Robot Based on D Lite Algorithm,IEEE,Conferences,"The increasing need of autonomous behavior of robots in fields of science and technology formed the requirement for path planning implemented by the robot without the human assistance. In this paper, D* Lite, which is a path planning graph-based algorithm, was used in order to compute the shortest path from a start to goal point in a real environment and make a Pepper robot move in a computed trajectory. The movement of robot was conducted in a static environment, with the map of the environment already known. This paper is a first step in the research focusing on a creation of a so-called intelligent workspace.",https://ieeexplore.ieee.org/document/8490605/,2018 World Symposium on Digital Intelligence for Systems and Machines (DISA),23-25 Aug. 2018,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.23919/ICINS43215.2020.9134006,Path Planning with Improved Artificial Potential Field Method Based on Decision Tree,IEEE,Conferences,"Path planning is one of the key research directions in the field of mobile robots. It ensures that moving objects can reach the target point safely and without collision in a complex obstacle environment. The path planning is to search an optimal path from the starting point to the target point for the mobile robot in an environment with obstacles, according to certain evaluation criteria (such as the time, the best path, the minimum energy consumption, etc.). The path planning based on artificial potential field method has been paid more and more attention because of its advantages such as convenient calculation, simple implementation of hardware and outstanding real-time performance. However, the artificial potential field method has some limitations, such as the local minimum, the oscillation of moving objects among obstacles and so on. To solve these problems, we can introduce the idea of decision tree into the artificial potential field method for improvement. In machine learning, decision tree is usually used for classification. It is a prediction model, which represents a mapping relationship between object attributes and object values. By utilizing the advantages of decision tree in rule expression and extraction, an improved artificial potential field path planning model based on decision tree is constructed, which can realize real-time and accurate identification of current behavior and fast decision-making of next time behavior in path planning. Aiming at the dynamic path planning problem of mobile robots in indoor complex environment, based on the traditional artificial potential field method, this paper introduces the distance term into the potential field function, and proposes an improved artificial potential field method based on the idea of decision tree, to solve the local minimum, the oscillation between obstacles and concave obstacle problems. According to repulsion coefficient, deflection angle of resultant force and velocity, a reasonable classification decision is made to meet the needs of different obstacle distribution scenarios, and the effectiveness of the proposed method is verified by simulation experiments. Simulation results show that, compared with the traditional artificial potential field method, the planning time of improved algorithm is reduced by 50%, and the smoothness of path planning by the improved algorithm is increased by 43.3%.",https://ieeexplore.ieee.org/document/9134006/,2020 27th Saint Petersburg International Conference on Integrated Navigation Systems (ICINS),25-27 May 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCEAI52939.2021.00074,Pedestrian Recognition System for Smart Security Robot using Pedestrian Re-identification Algorithm,IEEE,Conferences,"The security system is an important guarantee for the safety of citizens' lives and property. In recent years, security robots have been more and more widely used in security systems. At present, domestic security robots generally lack of pedestrian recognition ability under complex circumstances. Therefore, this paper designs and implements pedestrian recognition system for smart security robots using improved pedestrian re-identification algorithm. Experiment result shows that the system has success rate of 90 % and response speed compliance rate of 94.4% under real circumstances, which is much better than traditional system.",https://ieeexplore.ieee.org/document/9544430/,2021 International Conference on Computer Engineering and Artificial Intelligence (ICCEAI),27-29 Aug. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICSMC.1999.816641,"Perception, reasoning and learning of multiple agent systems for robot soccer",IEEE,Conferences,"Presents a supervisory control strategy for coordination of soccer playing mobile robots. Within the framework of a hierarchical control structure, three layered components of supervisor, coordinator, and executor emulate the basic three concepts of human intelligence, perception, reasoning, and learning. A small size discrete event system model is derived and implemented in the supervisor and coordinator for state-action reasoning and coordination of multiple robotic agents for a successful soccer game. Experimental results of real soccer games are given to demonstrate the feasibility and effectiveness of the developed supervisory control strategy in terms of structural simplicity and computational speed for real-time control.",https://ieeexplore.ieee.org/document/816641/,"IEEE SMC'99 Conference Proceedings. 1999 IEEE International Conference on Systems, Man, and Cybernetics (Cat. No.99CH37028)",12-15 Oct. 1999,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICBSII51839.2021.9445124,Positioning the 5-DOF Robotic Arm using Single Stage Deep CNN Model,IEEE,Conferences,"In teleoperation mechanism, the surgical robots are controlled using hand gestures from remote location. The remote location robotic arm control using hand gesture recognition is a challenging computer vision problem. The hand action recognition under complex environment (cluttered background, lighting variation, scale variation etc.) is a difficult and time consuming process. In this paper, a light weight Convolutional Neural Network (CNN) model Single Shot Detector (SSD) Lite MobileNet-V2 is proposed for real-time hand gesture recognition. SSD Lite versions tend to run hand gesture recognition applications on low-power computing devices like Raspberry Pi due to its light weight and timely recognition. The model is deployed using a Camera and two Raspberry Pi Controllers For the hand gesture recognition and data transfer to the cloud server, the Raspberry Pi controller 1 is used. The Raspberry Pi Controller 2 receives the cloud information and controls the Robotic arm operations. The performance of the proposed model is also compared with a SSD Inception-V2 model for the MITI Hand dataset-II (MITI HD-II). The average precision, average recall and F1-score for SSD Lite MobileNet-V2 and SSD Inception-V2 models are analyzed by training and testing the model with the learning rate of 0.0002 using Adam optimizer. SSD MobileNet-V2 model obtained an Average precision of 98.74% and SSD Inception-V2 model as 99.27%, The prediction time for SSD Lite MobileNet-V2 model using Raspberry Pi controller takes only 0.67s whereas, 1.2s for SSD Inception-V2 Model.",https://ieeexplore.ieee.org/document/9445124/,"2021 Seventh International conference on Bio Signals, Images, and Instrumentation (ICBSII)",25-27 March 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IEEECONF48371.2020.9078581,Possible Applications of Sixth Generation Communication Networks,IEEE,Conferences,"Our future society will be increasingly digitized and globally data-driven. Mobile communications technologies like 5Gwhile evolving make our daily lives easier and safer, as well as significantly impact on business efficiency. But we start thinking about the next steps - 6G networks. It is not yet clear what the advent of sixth-generation networks will entail. Perhaps thanks to high-resolution visualization and recognition, wearable displays, mobile robots and drones, completely new services such as telepresence and extended reality will be possible. The main issue of the studies is the transfer of data with a rate of up to 1 Tbit/s per user. This may become possible thanks to the efficient use of the spectrum in the THz domain. Another important issue will be using and implementing smart technologies, different type data sources, artificial intelligence and remote (tele-) presence that raise many technical and mathematical problems on the road toward 6G. For these reasons, 6G networks are a huge area for research. This article presents the possible services and involved technologies of the 6G network and its possible features. The following are system-wide trends that will define goals for 6G. In conclusion, a key way-points and research program are suggested.",https://ieeexplore.ieee.org/document/9078581/,2020 Systems of Signals Generating and Processing in the Field of on Board Communications,19-20 March 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/WACV45572.2020.9093599,Probabilistic Object Detection: Definition and Evaluation,IEEE,Conferences,"We introduce Probabilistic Object Detection, the task of detecting objects in images and accurately quantifying the spatial and semantic uncertainties of the detections. Given the lack of methods capable of assessing such probabilistic object detections, we present the new Probability-based Detection Quality measure (PDQ). Unlike AP-based measures, PDQ has no arbitrary thresholds and rewards spatial and label quality, and foreground/background separation quality while explicitly penalising false positive and false negative detections. We contrast PDQ with existing mAP and moLRP measures by evaluating state-of-the-art detectors and a Bayesian object detector based on Monte Carlo Dropout. Our experiments indicate that conventional object detectors tend to be spatially overconfident and thus perform poorly on the task of probabilistic object detection. Our paper aims to encourage the development of new object detection approaches that provide detections with accurately estimated spatial and label uncertainties and are of critical importance for deployment on robots and embodied AI systems in the real world.",https://ieeexplore.ieee.org/document/9093599/,2020 IEEE Winter Conference on Applications of Computer Vision (WACV),1-5 March 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IJCNN.2014.6889947,Qualitative approach for inverse kinematic modeling of a Compact Bionic Handling Assistant trunk,IEEE,Conferences,"Compact Bionic Handling Assistant (CBHA) is a continuum manipulator, with pneumatic-based actuation and compliant gripper. This bionic arm is attached to a mobile robot named Robotino. Inspired by the elephant's trunk, it can reproduce biological behaviors of trunks, tentacles, or snakes. Unlike rigid link robot manipulators, the development of high performance control algorithm of continuum robot manipulators remains a challenge, particularly due to their complex mechanical design, hyper-redundancy and presence of uncertainties. Numerous studies have been investigated for modeling of such complex systems. Such continuum robots, like the CBHA present a set of nonlinearities and uncertainties, making difficult to build an accurate analytical model, which can be used for control strategies development. Hence, learning approach becomes a suitable tool in such scenarios in order to capture un-modeled nonlinear behaviors of the continuous robots. In this paper, we present a qualitative modeling approach, based on neuronal model of the inverse kinematic of CBHA. A penalty term constraint is added to the inverse objective function into Distal Supervised Learning (DSL) scheme to select one particular inverse model from the redundancy manifold. The inverse kinematic neuronal model is validated by conducting a real-time implementation on a CBHA trunk.",https://ieeexplore.ieee.org/document/6889947/,2014 International Joint Conference on Neural Networks (IJCNN),6-11 July 2014,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS40897.2019.8968072,Quickly Inserting Pegs into Uncertain Holes using Multi-view Images and Deep Network Trained on Synthetic Data,IEEE,Conferences,"This paper explores the use of robots to autonomously assemble parts with variations in colors and textures. Specifically, we focus on peg-in-hole assembly with some initial position uncertainty and holes located on surfaces of different colors and textures. Two in-hand cameras and a force-torque sensor are used to account for the position uncertainty. A program sequence comprising learning-based visual servoing, spiral search, and impedance control is implemented to perform the peg-in-hole task with feedback from the above sensors. Contributions are mainly made in the learning-based visual servoing component of the sequence, where a deep neural network is trained with various sets of synthetic data generated using the concept of domain randomization to predict where a hole is. In the experiments and analysis section, the network is analyzed and compared, and a real-world robotic system to insert pegs to holes using the proposed method is implemented. The results show that the implemented peg-in-hole assembly system can perform successful peg-in-hole insertions on surfaces with various colors and textures. It can generally speed up the entire peg-in-hole process, especially when the initial position uncertainty is large.",https://ieeexplore.ieee.org/document/8968072/,2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),3-8 Nov. 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ROMAN.2002.1045636,RG mapping: learning compact and structured 2D line maps of indoor environments,IEEE,Conferences,"In this paper we present region and gateway (RG) mapping, a novel approach to laser-based 2D line mapping of indoor environments. RG mapping is capable of acquiring very compact, structured, and semantically annotated maps. We present and empirically analyze the method based on map acquisition experiments with autonomous mobile robots. The experiments show that RG mapping drastically compresses the data contained in line scan maps without substantial loss of accuracy.",https://ieeexplore.ieee.org/document/1045636/,Proceedings. 11th IEEE International Workshop on Robot and Human Interactive Communication,27-27 Sept. 2002,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA48506.2021.9561277,Rapid Pose Label Generation through Sparse Representation of Unknown Objects,IEEE,Conferences,"Deep Convolutional Neural Networks (CNNs) have been successfully deployed on robots for 6-DoF object pose estimation through visual perception. However, obtaining labeled data on a scale required for the supervised training of CNNs is a difficult task - exacerbated if the object is novel and a 3D model is unavailable. To this end, this work presents an approach for rapidly generating real-world, pose-annotated RGB-D data for unknown objects. Our method not only circumvents the need for a prior 3D object model (textured or otherwise) but also bypasses complicated setups of fiducial markers, turntables, and sensors. With the help of a human user, we first source minimalistic labelings of an ordered set of arbitrarily chosen keypoints over a set of RGB-D videos. Then, by solving an optimization problem, we combine these labels under a world frame to recover a sparse, keypoint-based representation of the object. The sparse representation leads to the development of a dense model and the pose labels for each image frame in the set of scenes. We show that the sparse model can also be efficiently used for scaling to a large number of new scenes. We demonstrate the practicality of the generated labeled dataset by training a CNN based 6-DoF object pose estimator.",https://ieeexplore.ieee.org/document/9561277/,2021 IEEE International Conference on Robotics and Automation (ICRA),30 May-5 June 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA.2017.7989184,Rapidly exploring learning trees,IEEE,Conferences,"Inverse Reinforcement Learning (IRL) for path planning enables robots to learn cost functions for difficult tasks from demonstration, instead of hard-coding them. However, IRL methods face practical limitations that stem from the need to repeat costly planning procedures. In this paper, we propose Rapidly Exploring Learning Trees (RLT*), which learns the cost functions of Optimal Rapidly Exploring Random Trees (RRT*) from demonstration, thereby making inverse learning methods applicable to more complex tasks. Our approach extends Maximum Margin Planning to work with RRT* cost functions. Furthermore, we propose a caching scheme that greatly reduces the computational cost of this approach. Experimental results on simulated and real-robot data from a social navigation scenario show that RLT* achieves better performance at lower computational cost than existing methods. We also successfully deploy control policies learned with RLT* on a real telepresence robot.",https://ieeexplore.ieee.org/document/7989184/,2017 IEEE International Conference on Robotics and Automation (ICRA),29 May-3 June 2017,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/EMRTS.1999.777446,Rate modulation of soft real-time tasks in autonomous robot control systems,IEEE,Conferences,"Due to the high number of sensors managed and need to perform complex reasoning activities, real-time control systems of autonomous robots exhibit a high potential for overload, i.e., real-time tasks missing their deadlines. In these systems overload should be regarded as a likely occurrence and hence managed accordingly. In this paper we illustrate a novel scheduling technique for adaptation of soft real-time load to available computational capacity in the context of autonomous robot control architectures. The technique is based on rate modulation of a set of periodic tasks in a range of admissible rates. The technique is shown to be easily computable and several variations in implementation are reviewed within the paper.",https://ieeexplore.ieee.org/document/777446/,Proceedings of 11th Euromicro Conference on Real-Time Systems. Euromicro RTS'99,9-11 June 1999,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CNSI.2011.67,Real-Time Face-Detection Engine for Robustness to Variable Illumination and Rotated Faces,IEEE,Conferences,"In this paper, we proposes a novel hardware architecture and FPGA implementation method of high performance real-time face-detection engine for robustness to variable illumination and rotation. The proposed face detection algorithm improved its performance by using MCT (Modified Census Transform), rotation transformation and AdaBoost learning algorithm. For implementation, we used a QVGA class camera, LCD display, and Virtex5 LX330 FPGA made by Xilinx Corporation. The verification results showed that it is possible to detect at least 32 faces in a wide variety of sizes at a maximum speed of 43 frames per second in real time. This finding can be applied to artificial intelligence robots for human recognition, conventional security systems for identity certification, and cutting-edge digital cameras using image processing techniques.",https://ieeexplore.ieee.org/document/5954277/,"2011 First ACIS/JNU International Conference on Computers, Networks, Systems and Industrial Engineering",23-25 May 2011,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/Cybermatics_2018.2018.00131,Real-Time Object Recognition Based on NAO Humanoid Robot,IEEE,Conferences,"This paper focuses on the real-time object recognition based indoor humanoid robots like Nao robots. Improving the perceptive ability of service robot has always been a research hotspot. The breakthrough of computer vision technology represented by object recognition provides a broader idea for this purpose. We deployed a micro-cloud layer that connects the robot with the computer vision, thereby realized the concepts of RaaS (Robot as a service). In this paper, in order to make the Nao robot to detect objects faster. We present an architecture about real-time object recognition on Nao, and offload the task of control and data collection from robot to a PC. Next, the image data is transmitted over Ethernet to the workstation, which runs multiple parallel image processing services. These services are built with the current popular deep neural network by TensorFlow and running on a GPU GTX1080 Ti. In the micro-cloud layer, we designed a universal robotic visual task queue model, and a PC registers the task queue to the LAN. There are multiple workers in the LAN, and each worker is an independent service processer. Service processer obtains the task queue from the network and processes the queue, and then the processer puts the results back to the manager. The experimental results of the Nao robot in the simulation and real word show that our model and method are effective. The robot can recognize about 90 kinds of common objects, and each frame of image processing time is about 100 milliseconds.",https://ieeexplore.ieee.org/document/8726687/,"2018 IEEE International Conference on Internet of Things (iThings) and IEEE Green Computing and Communications (GreenCom) and IEEE Cyber, Physical and Social Computing (CPSCom) and IEEE Smart Data (SmartData)",30 July-3 Aug. 2018,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/RoboSoft54090.2022.9762066,Real-Time Pressure Estimation and Localisation with Optical Tomography-inspired Soft Skin Sensors,IEEE,Conferences,"Sensing and localising pressure resulting from physical interaction between a robot and its environment is a key requirement in the deployment of soft robots in real-life scenarios. In order to adapt the robot&#x0027;s behaviour in real-time, we argue that sensors must have a high sampling rate. In this paper, we present a novel tactile sensing strategy for soft sensors, based on an imaging technique known as optical tomography. Instead of transmitting light through the soft sensor in a sequential way (as commonly done in tomography systems), we demonstrate that concurrently illuminating the sensor with multiple light sources and reading out the sensor response has several advantages. Firstly, it drastically increases the sampling rate of the sensor when compared to standard tomography approaches, making it more suitable to sense sudden and short-lived contacts. Secondly, by concurrently switching on the light sources, we increase performance in terms of pressure localisation and pressure estimation achieved through Machine Learning techniques. We carry out experiments demonstrating that our approach allows for a robust pressure estimation and contact point localisation with an accuracy up to 91.1 &#x0025; (vs 70.3&#x0025;) at a higher sampling rate.",https://ieeexplore.ieee.org/document/9762066/,2022 IEEE 5th International Conference on Soft Robotics (RoboSoft),4-8 April 2022,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICARCV.2018.8581288,Real-Time Robot Vision on Low-Performance Computing Hardware,IEEE,Conferences,"Small robots have numerous interesting applications in domains like industry, education, scientific research, and services. For most applications vision is important, however, the limitations of the computing hardware make this a challenging task. In this paper, we address the problem of real-time object recognition and propose the Fast Regions of Interest Search (FROIS) algorithm to quickly find the ROIs of the objects in small robots with low-performance hardware. Subsequently, we use two methods to analyze the ROIs. First, we develop a Convolutional Neural Network on a desktop and deploy it onto the low-performance hardware for object recognition. Second, we adopt the Histogram of Oriented Gradients descriptor and linear Support Vector Machines classifier and optimize the HOG component for faster speed. The experimental results show that the methods work well on our small robots with Raspberry Pi 3 embedded 1.2 GHz ARM CPUs to recognize the objects. Furthermore, we obtain valuable insights about the trade-offs between speed and accuracy.",https://ieeexplore.ieee.org/document/8581288/,"2018 15th International Conference on Control, Automation, Robotics and Vision (ICARCV)",18-21 Nov. 2018,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CRV50864.2020.00032,Real-time Motion Planning for Robotic Teleoperation Using Dynamic-goal Deep Reinforcement Learning,IEEE,Conferences,"We propose Dynamic-goal Deep Reinforcement Learning (DGDRL) method to address the problem of robot arm motion planning in telemanipulation applications. This method intuitively maps human hand motions to a robot arm in real-time, while avoiding collisions, joint limits and singularities. We further propose a novel hardware setup, based on the HTC VIVE VR system, that enables users to smoothly control the robot tool position and orientation with hand motions, while monitoring its movements in a 3D virtual reality environment. A VIVE controller captures 6D hand movements and gives them as reference trajectories to a deep neural policy network for controlling the robot’s joint movements. Our DGDRL method leverages the state-of-art Proximal Policy Optimization (PPO) algorithm for deep reinforcement learning to train the policy network with the robot joint values and reference trajectory observed at each iteration. Since training the network on a real robot is time-consuming and unsafe, we developed a simulation environment called RobotPath which provides kinematic modeling, collision analysis and a 3D VR graphical simulation of industrial robots. The deep neural network trained using RobotPath is then deployed on a physical robot (ABB IRB 120) to evaluate its performance. We show that the policies trained in the simulation environment can be successfully used for trajectory planning on a real robot. The the codes, data and video presenting our experiments are available at https://github.com/kavehkamali/ppoRobotPath.",https://ieeexplore.ieee.org/document/9108691/,2020 17th Conference on Computer and Robot Vision (CRV),13-15 May 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/LifeTech52111.2021.9391811,Real-time Object Detection with Deep Learning for Robot Vision on Mixed Reality Device,IEEE,Conferences,"Mixed reality device sensing capabilities are valuable for robots, for example, the inertial measurement unit (IMU) sensor and time-of-flight (TOF) depth sensor can support the robot in navigating its environment. This paper demonstrates a deep learning (YOLO model) background, realtime object detection system implemented on mixed reality device. The goal of the system is to create a real-time communication system between HoloLens and Ubuntu systems to enable real-time object detection using the YOLO model. The experimental results show that the proposed method has a fast speed to achieve real-time object detection using HoloLens. This enables Microsoft HoloLens as a device for robot vision. To enhance human-robot interaction, we will apply it to a wearable robot arm system to automatically grasp objects in the future.",https://ieeexplore.ieee.org/document/9391811/,2021 IEEE 3rd Global Conference on Life Sciences and Technologies (LifeTech),9-11 March 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CCA.2007.4389266,Real-time Obstacle Avoidance Strategy for Mobile Robot Based On Improved Coordinating Potential Field with Genetic Algorithm,IEEE,Conferences,"To overcome the problems during navigation of mobile robots in dynamic environment using the traditional artificial potential field (APF) method, a novel improved method called coordinating potential field (CPF) is proposed. The local potential field is constructed by using local subgoals, which obtained by updating dynamic windows. The questions of local minima, oscillation between multiple obstacles and real-time dynamic obstacle avoidance are solved. At last multi-objective parameter optimization is implemented by using adaptive genetic algorithm. Simulation results indicate that this strategy is practicable and effective.",https://ieeexplore.ieee.org/document/4389266/,2007 IEEE International Conference on Control Applications,1-3 Oct. 2007,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA48506.2021.9561861,Real-time Optimal Navigation Planning Using Learned Motion Costs,IEEE,Conferences,"Navigation on challenging terrain topographies requires the understanding of robots&#x2019; locomotion capabilities to produce optimal solutions. We present an integrated framework for real-time autonomous navigation of mobile robots based on elevation maps. The framework performs rapid global path planning and optimization that is aware of the locomotion capabilities of the robot. A GPU-aided, sampling-based path planner combined with a gradient-based path optimizer provides optimal paths by using a neural network-based locomotion cost predictor which is trained in simulation. We show that our approach is capable of planning and optimizing paths three orders of magnitude faster than RRT* on GPU-enabled hardware, enabling real-time deployment on mobile platforms. We successfully evaluate the framework on the ANYmal C quadrupedal robot in both simulations and real-world environments for path planning tasks on multiple complex terrains.",https://ieeexplore.ieee.org/document/9561861/,2021 IEEE International Conference on Robotics and Automation (ICRA),30 May-5 June 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/INTECH.2017.8102423,Real-time emotional state detection from facial expression on embedded devices,IEEE,Conferences,"From the last decade, researches on human facial emotion recognition disclosed that computing models built on regression modelling can produce applicable performance. However, many systems need extensive computing power to be run that prevents its wide applications such as robots and smart devices. In this proposed system, a real-time automatic facial expression system was designed, implemented and tested on an embedded device such as FPGA that can be a first step for a specific facial expression recognition chip for a social robot. The system was built and simulated in MATLAB and then was built on FPGA and it can carry out real time continuously emotional state recognition at 30 fps with 47.44% accuracy. The proposed graphic user interface is able to display the participant video and two dimensional predict labels of the emotion in real time together.",https://ieeexplore.ieee.org/document/8102423/,2017 Seventh International Conference on Innovative Computing Technology (INTECH),16-18 Aug. 2017,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IRIA53009.2021.9588681,Real-time gesture control UAV with a low resource framework,IEEE,Conferences,"This study showcases a low-resource framework that enables people with no technical know-how to interact with drones, it also explores the capabilities of 2D- computer vision and deep learning techniques for gesture based interface systems on a low-cost micro drone with an onboard RGB camera. This Human-Robot Interaction system processes the real-time human pose to allow a user to command the drone, i.e., by providing direction to move and execute actions. A linear PD controller and image processing techniques are implemented to track humans whilst maintaining a safe distance from the user by perceiving depth information through pose estimation. We incorporated the gesture recognition results into a drone using the Robot Operating System (ROS) and evaluated system performance indoor and outdoor. This low computation framework can be applied further to control robotic arms or mobile robots.",https://ieeexplore.ieee.org/document/9588681/,2021 International Symposium of Asian Control Association on Intelligent Robotics and Industrial Automation (IRIA),20-22 Sept. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ROBOT.1993.291973,Real-time implementation of neural network learning control of a flexible Space manipulator,IEEE,Conferences,"A neural network approach to online learning control and real-time implementation for a flexible space robot manipulator is presented. An overview of the motivation and system development of the self-mobile space modulator (SM/sup 2/) is given. The neural network learns control by updating feedforward dynamics based on feedback control input. Implementation issues associated with online training strategies are addressed and a single stochastic training scheme is presented. A recurrent neural network architecture with improved performance is proposed. Using the proposed learning scheme, the manipulator tracking error is reduced by 85% compared to that of conventional proportional-integral-derivative (PID) control. The approach possesses a high degree of generality and adaptability to various applications. It will be a valuable learning control method for robots working in unconstructed environments.<>",https://ieeexplore.ieee.org/document/291973/,[1993] Proceedings IEEE International Conference on Robotics and Automation,2-6 May 1993,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ETS.2017.7968218,Real-time self-learning for control law adaptation in nonlinear systems using encoded check states,IEEE,Conferences,"With the wide proliferation of autonomous sense-and-control real-time systems (such as robots and self-driven cars), a key research objective is rapid recovery from the effects of anomalies and impairments arising from performance degradation of sensors and actuators and electro-mechanical subsystems due to field wear and tear. This must be achieved with minimal impact on system performance while maintaining low implementation overhead and high coverage of multi-parameter failure mechanisms. In this work, we propose a reinforcement learning framework for on-line control law adaptation in autonomous nonlinear systems assisted by system state encodings. These encodings are exploited to generate time-varying error signals whose (transient) waveforms in relation to the input stimulus, contain root-cause diagnostic information. This establishes a statistical correlation between the transient waveforms and the parameters of the optimal nonlinear controller under arbitrary multi-parameter perturbations of sensor/actuator and subsystem performances. Consequently this correlation is tapped, using pre-deployment supervised learning algorithms, to predict near-optimal controller parameter values whenever sufficiently large parameter deviations are detected (due to non-zero error signals). From these near-optimal starting conditions, an actor-critic reinforcement learning controller for nonlinear systems quickly converges to the optimal control law for the parameter-perturbed system (up to 10× faster than for systems not assisted by the diagnostic information provided by the state encoding driven error signal above). We implement the proposed methodology on two nonlinear systems demonstrating fast performance recovery in real time.",https://ieeexplore.ieee.org/document/7968218/,2017 22nd IEEE European Test Symposium (ETS),22-26 May 2017,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICMCCE51767.2020.00092,Real-time virtual UR5 robot imitation of human motion based on 3D camera,IEEE,Conferences,"Human robot interaction is playing significant roles in many industries, in which humans are allowed to corporate with robots or control robots to complete some tasks. Achieving realtime control of robots by demonstrating human arm motion is one of the effective methods to improve the quality of human robot interaction. In this paper, a new method to accomplish human skeleton key points extraction is proposed and a system for controlling a virtual UR5 robot (which is modeling in Unity 3D) is developed to imitate human motions. A sequential architecture composed of convolutional networks is used to identify the 2D coordinates; the depth information from Kinect v2 sensor is used to obtain the actual 3D coordinates and the result of the proposed method is used for joint angles calculation. The appropriate mapping relationships between human arm and UR5 robots are built by transforming the arm angel to UR5's degrees of freedom. The performance of this method is measured by the degree of real-time simulation of human hand trajectory. Further, the comparison between the method in this paper and other method is implemented. The experimental results indicate that the developed system and proposed method are reliable and efficient to accomplish human motion imitation.",https://ieeexplore.ieee.org/document/9421649/,"2020 5th International Conference on Mechanical, Control and Computer Engineering (ICMCCE)",25-27 Dec. 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SII.2010.5708353,Realization and analysis of giant-swing motion using Q-Learning,IEEE,Conferences,"Many research papers have reported studies on sports robots that realize giant-swing motion. However, almost all these robots were controlled using trajectory planning methods, and few robots realized giant-swing motion by learning. Consequently, in this study, we attempted to construct a humanoid robot that realizes giant-swing motion by Q-learning, a reinforcement learning technique. The significant aspect of our study is that few robotic models were constructed beforehand; the robot learns giant-swing motion only by interaction with the environment during simulations. Our implementation faced several problems such as imperfect perception of the velocity state and robot posture issues caused by using only the arm angle. However, our real robot realized giant-swing motion by averaging the Q value and by using rewards - the absolute angle of the foot angle and the angular velocity of the arm angle-in the simulated learning data; the sampling time was 250 ms. Furthermore, the feasibility of generalization of learning for realizing selective motion in the forward and backward rotational directions was investigated; it was revealed that the generalization of learning is feasible as long as it does not interfere with the robot's motions.",https://ieeexplore.ieee.org/document/5708353/,2010 IEEE/SICE International Symposium on System Integration,21-22 Dec. 2010,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA.2019.8793627,Reinforcement Learning Meets Hybrid Zero Dynamics: A Case Study for RABBIT,IEEE,Conferences,"The design of feedback controllers for bipedal robots is challenging due to the hybrid nature of its dynamics and the complexity imposed by high-dimensional bipedal models. In this paper, we present a novel approach for the design of feedback controllers using Reinforcement Learning (RL) and Hybrid Zero Dynamics (HZD). Existing RL approaches for bipedal walking are inefficient as they do not consider the underlying physics, often requires substantial training, and the resulting controller may not be applicable to real robots. HZD is a powerful tool for bipedal control with local stability guarantees of the walking limit cycles. In this paper, we propose a non traditional RL structure that embeds the HZD framework into the policy learning. More specifically, we propose to use RL to find a control policy that maps from the robot’s reduced order states to a set of parameters that define the desired trajectories for the robot’s joints through the virtual constraints. Then, these trajectories are tracked using an adaptive PD controller. The method results in a stable and robust control policy that is able to track variable speed within a continuous interval. Robustness of the policy is evaluated by applying external forces to the torso of the robot. The proposed RL framework is implemented and demonstrated in OpenAI Gym with the MuJoCo physics engine based on the well-known RABBIT robot model.",https://ieeexplore.ieee.org/document/8793627/,2019 International Conference on Robotics and Automation (ICRA),20-24 May 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AI4I46381.2019.00027,Reinforcement Learning of a Robot Cell Control Logic using a Software-in-the-Loop Simulation as Environment,IEEE,Conferences,"This paper introduces a method for automatic robot programming of industrial robots using reinforcement learning on a Software-in-the-loop simulation. The focus of the the paper is on the higher levels of a hierarchical robot programming problem. While the lower levels the skills are stored as domain specific program code, the combination of the skills into a robot control program to solve a specific task is automated. The reinforcement learning learning approach allows the shopfloor workers and technicians just to define the end result of the manufacturing process through a reward function. The programming and process optimization is done within the learning procedure. The Software-in-the-loop simulation with the robot control software makes it possible to to interpret the real program code and generate the exact motion. The exact motion of the robot is needed in order to find not just an optimal but also a collision-free policy.",https://ieeexplore.ieee.org/document/9027783/,2019 Second International Conference on Artificial Intelligence for Industries (AI4I),25-27 Sept. 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS45743.2020.9340948,Reinforcement co-Learning of Deep and Spiking Neural Networks for Energy-Efficient Mapless Navigation with Neuromorphic Hardware,IEEE,Conferences,"Energy-efficient mapless navigation is crucial for mobile robots as they explore unknown environments with limited on-board resources. Although the recent deep rein-forcement learning (DRL) approaches have been successfully applied to navigation, their high energy consumption limits their use in several robotic applications. Here, we propose a neuromorphic approach that combines the energy-efficiency of spiking neural networks with the optimality of DRL and benchmark it in learning control policies for mapless navigation. Our hybrid framework, spiking deep deterministic policy gradient (SDDPG), consists of a spiking actor network (SAN) and a deep critic network, where the two networks were trained jointly using gradient descent. The co-learning enabled synergistic information exchange between the two networks, allowing them to overcome each other's limitations through a shared representation learning. To evaluate our approach, we deployed the trained SAN on Intel's Loihi neuromorphic processor. When validated on simulated and real-world complex environments, our method on Loihi consumed 75 times less energy per inference as compared to DDPG on Jetson TX2, and also exhibited a higher rate of successful navigation to the goal, which ranged from 1% to 4.2% and depended on the forward-propagation timestep size. These results reinforce our ongoing efforts to design brain-inspired algorithms for controlling autonomous robots with neuromorphic hardware.",https://ieeexplore.ieee.org/document/9340948/,2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),24 Oct.-24 Jan. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICMA.2005.1626784,Reinforcement learning based group navigation approach for multiple autonomous robotic system,IEEE,Conferences,"In several complex applications, the use of multiple autonomous robotic systems (ARS) becomes necessary to achieve different tasks such as foraging and transport of heavy and large objects with less cost and more efficiency. They have to achieve a high level of flexibility, adaptability and efficiency in real environments. In this paper, a reinforcement learning (RL) based group navigation approach for multiple ARS is suggested. Indeed, the robots must have the ability to form geometric figures and navigate without collisions while maintaining the formation. Thus, each robot must learn how to take its place in the formation and avoid obstacles and other ARS from its interaction with the environment. This approach must provide ARS with capability to acquire the group navigation approach among several ARS from elementary behaviors by learning with trial and error search. Then, simulation results display the ability of the suggested approach to provide ARS with capability to navigate in a group formation in dynamic environments.",https://ieeexplore.ieee.org/document/1626784/,"IEEE International Conference Mechatronics and Automation, 2005",29 July-1 Aug. 2005,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1049/cp.2012.1301,Relevance Vector Machine based multi-feature integration for semantic place recogntion,IET,Conferences,"In order to work in realistic scenarios, it is a desirable feature for autonomous robots to extract semantic concepts from environments. In this paper, A Relevance Vector Machine (RVM) based approach is presented for the task of visual semantic place recognition. The high sparsity and Bayesian property makes this approach capable of obtaining probabilistic confidence estimation, and computationally efficient during the online prediction stage. Meanwhile, in order to take advantage of discriminative powers of different feature descriptors, a multiple kernel technique is introduced in our system, resulting in a very flexible model where arbitrary feature descriptors can be integrated smoothly. In this paper we choose three popular descriptors for our implementation. Experiments carried out on real typical office environment datasets show the feasibility and robustness of our approach.",https://ieeexplore.ieee.org/document/6492908/,International Conference on Automatic Control and Artificial Intelligence (ACAI 2012),3-5 March 2012,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICMA.2019.8816557,Research on V-SLAM Methods,IEEE,Conferences,"With the development of intelligent mobile robots, SLAM, especially V-SLAM, as the basic technology of robot localization and navigation, has the advantages of strong adaptability, high precision and strong intelligence compared with the traditional localization technology. It is widely used in smart devices such as unmanned aerial vehicle, automatic driving and sweeping robots. According to different implementation methods, the visual SLAM is divided into: filter V-SLAM based on probability model, key frame BA-based V-SLAM using nonlinear optimization theory, direct tracking of V-SLAM under the assumption of luminosity invariance, space occupying V-SLAM that focuses on building three-dimensional dense maps. This paper focuses on representative systems of various V-SLAMs and gives their respective applicable scenarios and characteristics. Finally, this article forecasts the development of V-SLAM combining with multi-information fusion technology, semantic deep and learning technology.",https://ieeexplore.ieee.org/document/8816557/,2019 IEEE International Conference on Mechatronics and Automation (ICMA),4-7 Aug. 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICIME.2010.5477962,Research on the embedded system of facial expression recognition based on HMM,IEEE,Conferences,"With the rapid development of artificial intelligence, how to make robots have feelings has become a research highlight today. A portable device for facial expression recognition has been designed, based on the hardware platform of Intel embedded processor PXA270, and the software platform of embedded operation system Linux. The functions of video capture, image processing and face tracking have been achieved, through the application of the USB, Video4Linux programming technology, image processing technology, and artificial psychology theory. At the same time, an emotion model more consistent with the characteristics of human emotions is proposed under the theoretical framework of HMM according to the characteristics of human emotions. A new real-time facial expression recognition method is proposed based on the face detection and facial feature detection technology. Experimental results show that, the effect of face recognition is satisfactory, and the method is effective.",https://ieeexplore.ieee.org/document/5477962/,2010 2nd IEEE International Conference on Information Management and Engineering,16-18 April 2010,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA40945.2020.9197386,Residual Reactive Navigation: Combining Classical and Learned Navigation Strategies For Deployment in Unknown Environments,IEEE,Conferences,"In this work we focus on improving the efficiency and generalisation of learned navigation strategies when transferred from its training environment to previously unseen ones. We present an extension of the residual reinforcement learning framework from the robotic manipulation literature and adapt it to the vast and unstructured environments that mobile robots can operate in. The concept is based on learning a residual control effect to add to a typical sub-optimal classical controller in order to close the performance gap, whilst guiding the exploration process during training for improved data efficiency. We exploit this tight coupling and propose a novel deployment strategy, switching Residual Reactive Navigation (sRRN), which yields efficient trajectories whilst probabilistically switching to a classical controller in cases of high policy uncertainty. Our approach achieves improved performance over end-to-end alternatives and can be incorporated as part of a complete navigation stack for cluttered indoor navigation tasks in the real world. The code and training environment for this project is made publicly available at https://sites.google.com/view/srrn/home.",https://ieeexplore.ieee.org/document/9197386/,2020 IEEE International Conference on Robotics and Automation (ICRA),31 May-31 Aug. 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ACII.2013.88,Reversal Learning Based on Somatic Markers,IEEE,Conferences,"One of the main aspects in the field of Artificial Intelligence is the creation of agents with the ability to learn like human beings do. Based on made experiences humans are able to adapt their behaviour in order to solve tasks. Another important aspect of human decision making is the ability to discard learned behaviour when the usual decisions, concerning a stimulus, lead to a bad outcome. For robots intended to be embedded in a social environment, the adaptability of behaviour is an important factor. Research of human decision behaviour shows, that emotions play a decisive role, even for learning and reversal learning. In this paper, improvements and further results of a previously presented framework for decision making based on an emotional memory are presented. The improvements include the reduction of the amount of previous knowledge that has to be implemented and an evaluation concerning reversal learning. For evaluation purposes, a typical reversal learning task, performed by real subjects, has been used. The results show that this framework allows the adaption of behaviour comparable to human subjects and offers decisive improvements, which lead to better results in reversal learning tasks without the need to directly declare a task as such one.",https://ieeexplore.ieee.org/document/6681479/,2013 Humaine Association Conference on Affective Computing and Intelligent Interaction,2-5 Sept. 2013,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA.2015.7139395,RoboSherlock: Unstructured information processing for robot perception,IEEE,Conferences,"We present RoboSherlock, an open source software framework for implementing perception systems for robots performing human-scale everyday manipulation tasks. In RoboSherlock, perception and interpretation of realistic scenes is formulated as an unstructured information management (UIM) problem. The application of the UIM principle supports the implementation of perception systems that can answer task-relevant queries about objects in a scene, boost object recognition performance by combining the strengths of multiple perception algorithms, support knowledge-enabled reasoning about objects and enable automatic and knowledge-driven generation of processing pipelines. We demonstrate the potential of the proposed framework by three feasibility studies of systems for real-world scene perception that have been built on top of RoboSherlock.",https://ieeexplore.ieee.org/document/7139395/,2015 IEEE International Conference on Robotics and Automation (ICRA),26-30 May 2015,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ROBIO49542.2019.8961517,Robot Control in Human Environment using Deep Reinforcement Learning and Convolutional Neural Network,IEEE,Conferences,"Deep reinforcement learning (DRL) has been employed in numerous applications where complex decision-making is needed. Robot control in a human environment is an example. Such algorithm offers possibilities to achieve end-to-end training which learns from image directly. However, training on a physical robotic system under human environments using DRL is inefficient and even dangerous. Several recent works have used simulators for training models before implementing to physical robots. Although simulation provides efficiency to obtain DRL trained models, it poses challenges for the transformation from simulation to reality. Since a human environment is often cluttered, dynamic and complex, the policy trained with simulation images is not applicable for reality. Therefore, in this paper, we propose a DRL method to achieve end-to-end training in simulation, as well as to adapt to reality without any further finetune. Firstly, a Deep Deterministic Policy Gradient algorithm (DDPG) is employed to learn policy for robot control. Secondly, a pre-trained Convolutional Neural Network algorithm (CNN) is used to visually track the target in image. This technique provides the efficient and safe DRL training in simulation while offering robust application when a real robot is placed in dynamic human environment. Simulation and experiment are conducted for validation and can be seen in the attached video. The results have shown successful demonstration under various complex environments.",https://ieeexplore.ieee.org/document/8961517/,2019 IEEE International Conference on Robotics and Biomimetics (ROBIO),6-8 Dec. 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS40897.2019.8967847,Robot Localization in Floor Plans Using a Room Layout Edge Extraction Network,IEEE,Conferences,"Indoor localization is one of the crucial enablers for deployment of service robots. Although several successful techniques for indoor localization have been proposed, the majority of them relies on maps generated from data gathered with the same sensor modality used for localization. Typically, tedious labor by experts is needed to acquire this data, thus limiting the readiness of the system as well as its ease of installation for inexperienced operators. In this paper, we propose a memory and computationally efficient monocular camera-based localization system that allows a robot to estimate its pose given an architectural floor plan. Our method employs a convolutional neural network to predict room layout edges from a single camera image and estimates the robot pose using a particle filter that matches the extracted edges to the given floor plan. We evaluate our localization system using multiple real-world experiments and demonstrate that it has the robustness and accuracy required for reliable indoor navigation.",https://ieeexplore.ieee.org/document/8967847/,2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),3-8 Nov. 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA48506.2021.9560893,Robot Navigation in Constrained Pedestrian Environments using Reinforcement Learning,IEEE,Conferences,"Navigating fluently around pedestrians is a necessary capability for mobile robots deployed in human environments, such as buildings and homes. While research on social navigation has focused mainly on the scalability with the number of pedestrians in open spaces, typical indoor environments present the additional challenge of constrained spaces such as corridors and doorways that limit maneuverability and influence patterns of pedestrian interaction. We present an approach based on reinforcement learning (RL) to learn policies capable of dynamic adaptation to the presence of moving pedestrians while navigating between desired locations in constrained environments. The policy network receives guidance from a motion planner that provides waypoints to follow a globally planned trajectory, whereas RL handles the local interactions. We explore a compositional principle for multi-layout training and find that policies trained in a small set of geometrically simple layouts successfully generalize to more complex unseen layouts that exhibit composition of the structural elements available during training. Going beyond walls-world like domains, we show transfer of the learned policy to unseen 3D reconstructions of two real environments. These results support the applicability of the compositional principle to navigation in real-world buildings and indicate promising usage of multi-agent simulation within reconstructed environments for tasks that involve interaction. https://ai.stanford.edu/&#x223C;cdarpino/socialnavconstrained/",https://ieeexplore.ieee.org/document/9560893/,2021 IEEE International Conference on Robotics and Automation (ICRA),30 May-5 June 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ROBOT.2000.845355,Robot improv: using drama to create believable agents,IEEE,Conferences,"Believable agents usually depend upon explicit, model-based simulations of human emotions. This work appeals instead to the sensibilities of dramatic acting to create agents that are believable. The chosen task is that of comedy improvisation as it provides a solid demonstration of the agents' believability in the context of a high-level deliberative goal. Furthermore, this work employs physical robots as the actors, employing the real-time sensor values from the robots as inputs into the acting process. This paper describes the dramatic approach to acting that we used and describes the Java-based implementation on two Nomad Scout robots. Actual, improvised scripts created by the robots are included and analyzed.",https://ieeexplore.ieee.org/document/845355/,Proceedings 2000 ICRA. Millennium Conference. IEEE International Conference on Robotics and Automation. Symposia Proceedings (Cat. No.00CH37065),24-28 April 2000,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA48506.2021.9561545,Robot in a China Shop: Using Reinforcement Learning for Location-Specific Navigation Behaviour,IEEE,Conferences,"Robots need to be able to work in multiple different environments. Even when performing similar tasks, different behaviour should be deployed to best fit the current environment. In this paper, We propose a new approach to navigation, where it is treated as a multi-task learning problem. This enables the robot to learn to behave differently in visual navigation tasks for different environments while also learning shared expertise across environments. We evaluated our approach in both simulated environments as well as real-world data. Our method allows our system to converge with a 26% reduction in training time, while also increasing accuracy.",https://ieeexplore.ieee.org/document/9561545/,2021 IEEE International Conference on Robotics and Automation (ICRA),30 May-5 June 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IISA.2017.8316452,Robot painting recognition based on deep belief learning,IEEE,Conferences,"In a society where the number of elderly people is increasing rapidly, autonomous wheelchair robots are expected to be widely used for mobility of elderly people. In this paper we focus on how we can utilize wheelchair robots operating in museums. In this paper, we propose a deep learning based painting recognition and its application for the wheelchair robot. We consider the case when the user clicks on the painting he/she wants to see. The robot searches, recognizes and reaches the painting using deep learning. This is in difference from the most traditional methods where the robot explains the exhibited objects in a sequential order. The deep neural network generates a series of high dimensional features for each painting resulting in a high recognition rate. In our implementation, the wheelchair robot recognizes the painting in real time using the video stream.",https://ieeexplore.ieee.org/document/8316452/,"2017 8th International Conference on Information, Intelligence, Systems & Applications (IISA)",27-30 Aug. 2017,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ROBIO.2011.6181679,"Robot self-preservation and adaptation to user preferences in game play, a preliminary study",IEEE,Conferences,"It is expected that in a near future, personal robots will be endowed with enough autonomy to function and live in an individual's home. This is while commercial robots are designed with default configuration and factory settings which may often be different to an individual's operating preferences. This paper presents how reinforcement learning is applied and utilised towards personalisation of a robot's behaviour. Two-level reinforcement learning has been implemented: first level is in charge of energy autonomy, i.e. how to survive, and second level is involved in adapting robot's behaviour to user's preferences. In both levels Q-learning algorithm has been applied. First level actions have been learnt in a simulated environment and then the results have been transferred to the real robot. Second level has been fully implemented in the real robot and learnt by human-robot interaction. Finally, experiments showing the performance of the system are presented.",https://ieeexplore.ieee.org/document/6181679/,2011 IEEE International Conference on Robotics and Biomimetics,7-11 Dec. 2011,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CEC.2016.7743924,Robot-to-human handover with obstacle avoidance via continuous time Recurrent Neural Network,IEEE,Conferences,"Parallel with the development of service robots, it is vital for the robots to carry out handovers autonomously. Robot-to-human handover is a coordination in time and space for a robot to deliver an object to human. A good robot-to-human handover should consider human safety and preference, natural motion planning that mimics human and adaptability to the changes of the environment. Conventional handover motion mostly rely on sampling-based algorithms that emphasizes on kinematic and dynamic analysis. This kind of motion planning could become complicated and slow in response if the handover motion is implemented in a dynamic environment where real time motion planning is required. To simplify the implementation of robot-to-human handover, a motion learning and generation framework that based on Continuous Time Recurrent Neural Network(CTRNN) is proposed. The proposed framework is equipped with the capabilities of object recognition, motion generation based on past learning experience and obstacle adaptation. As compared with conventional method, the proposed framework could be easily extended to handover motion with high dimensional configuration spaces as the motion can be generated from the learnt experience. In the proposed framework, the handover behaviour can be learnt via human-guided motion teaching which provides an intuitive and visible solution for motion planning. The proposed framework has been experimentally evaluated on a customized design robot via robotto-human handover testing. Based on the testing, the feasibility of the proposed framework had been justified.",https://ieeexplore.ieee.org/document/7743924/,2016 IEEE Congress on Evolutionary Computation (CEC),24-29 July 2016,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AICAI.2019.8701333,Robust LQR Based ANFIS Control of x-z Inverted Pendulum,IEEE,Conferences,"Inverted pendulum is a highly unstable, nonlinear and an under-actuated system. Its dynamics resembles many real-time systems such as segways, self-balancing robots, vertical take-off and landing aircraft (VTOL) and crane lifting containers etc. These real-time applications demand the need of a robust controller. In literature, many different control strategies have been discussed to stabilize an inverted pendulum, out of them, the most robust being fuzzy control and sliding mode control. The former is difficult to tune and has a problem of rule explosion for multivariable system, whereas the latter has a problem of discontinuity and chattering. To address the issues in fuzzy controller, a novel robust linear quadratic regulator (LQR) based adaptive-network fuzzy inference system (ANFIS) controller is proposed and implemented on the stabilization of x-z inverted pendulum. The proposed controller is able to solve the problem of robustness in the LQR controller as well as the difficulty in tuning along with rule explosion in fuzzy controller. Furthermore, the designed controller is tested for different pendulum masses and the results show that as compared with conventional PID controller, the proposed controller gives better performance in achieving lesser overshoot and settling time along with better robustness properties.",https://ieeexplore.ieee.org/document/8701333/,2019 Amity International Conference on Artificial Intelligence (AICAI),4-6 Feb. 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SII.2011.6147636,Robust localization system using online / offline hybrid learning,IEEE,Conferences,"In this paper, we propose an online motion model parameter estimation method. To achieve accurate localization, accurate estimation of motion model parameters is needed. However, the true values of motion model parameters change sequentially according to alteration of surrounding environments. Therefore the online estimation is absolutely imperative. As a typical method to estimate motion model parameters sequentially, Augmented Kalman Filter (AKF) is there. AKF achieves parameter estimation through Kalman filtering algorithm. However, AKF has serious problems to be implemented in real robot operation. These problems are the accuracy of observation and the limitation to motion control of robots. To solve these problems and achieve accurate motion model parameter estimation, proposed method introduces discriminative training. The introduction of discriminative training increases the convergence performance and stability of parameter estimation through AKF. The proposal method achieves accurate motion model parameter estimation in real robot operation. This paper describes the efficiency of our technique through simulations and an outdoor experiment.",https://ieeexplore.ieee.org/document/6147636/,2011 IEEE/SICE International Symposium on System Integration (SII),20-22 Dec. 2011,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS.2017.8206604,Robust real-time visual tracking using dual-frame deep comparison network integrated with correlation filters,IEEE,Conferences,"In recent years, applications of visual tracking algorithms has seen a substantial growth with deployments in intelligent robots such as drones for human tracking. The algorithms for such tasks has to be efficient in terms of computational cost while been robust, accurate and fast. Object tracking algorithms based on handcrafted heuristics and constraints are widely used in uav applications. The handcrafted heuristics are mostly implemented for task-oriented applications which limits the extensions in uav's capability beyond the predefined functions. This paper considers the challenges of tracking and landing an autonomous uav on a speed high moving target, and presents a visual tracking algorithm that integrates correlation filters with deep comparison network for real-time tracking with state-of-the-art accuracy. The method first tracks the target upto translation using an online learnt model via local search technique. The changes in scale is estimated by a deep comparison network (DCN) instead of the commonly used pyramidal approach. In a single network evaluation, DCN can estimate the changes in scale as well as compensate the drifting of the tracker by refining the object region estimated by the correlation filters. The network is end-to-end trained which attempts to learn a powerful matching function for object localization using a known template. Generally, the integrated framework can be viewed as coarse-to-fine level motion estimation. Moreover, the framework can redetect the lost target without a need for a separate detector.",https://ieeexplore.ieee.org/document/8206604/,2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),24-28 Sept. 2017,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/EURCON.2007.4400663,Role Selection Mechanism for the Soccer Robot System using Petri Net,IEEE,Conferences,"Robot soccer is a challenging platform for multi-agent research, involving topics such as real-time image processing and control, robot path planning, obstacle avoidance and machine learning. The system consists of a supervisory controller, and controllers for defending and goalkeeping robots. These controllers are designed using Petri net. The robot soccer game presents an uncertain and dynamic environment for cooperating agents. Dynamic role switching and formation control are crucial for a successful game. A soccer robot has to take an appropriate decision based on environment situation. With the role of a robot fixed as goalkeeper, the supervisor, according to the game situation, assigns the role of attacking or defending to the other robots and then respective controllers control the robots. The Petri net model is implemented in Petri net toolbox under MATLAB environment.",https://ieeexplore.ieee.org/document/4400663/,"EUROCON 2007 - The International Conference on ""Computer as a Tool""",9-12 Sept. 2007,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SNPD.2013.86,RvGIST: A Holistic Road Feature for Real-Time Road-Scene Understanding,IEEE,Conferences,"Image-based road scene understanding is a critical issue for intelligent vehicles and autonomous mobile robots. It is challenging to deal with varying road conditions in a dynamic environment in real time. This paper presents an effective while simple approach to classify road types and locate the road-related elements through the analysis of a holistic visual road feature. The feature is abstracted from responses of Gabor-filter-set and grouped into super-pixel grids, consisting of road-scene textural context and dominant orientation distribution. From this feature we successively deduce the information of horizon line, road type and coarse locations of road surface, lanes, on-road obstacles and off-road regions. Experiments show that the proposed analyzing method based on the new holistic feature is beneficial to road estimation and vehicle detection in complex road scenes, achieving improvements in both accuracy and efficiency.",https://ieeexplore.ieee.org/document/6598534/,"2013 14th ACIS International Conference on Software Engineering, Artificial Intelligence, Networking and Parallel/Distributed Computing",1-3 July 2013,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS45743.2020.9340915,SQUIRL: Robust and Efficient Learning from Video Demonstration of Long-Horizon Robotic Manipulation Tasks,IEEE,Conferences,"Recent advances in deep reinforcement learning (RL) have demonstrated its potential to learn complex robotic manipulation tasks. However, RL still requires the robot to collect a large amount of real-world experience. To address this problem, recent works have proposed learning from expert demonstrations (LfD), particularly via inverse reinforcement learning (IRL), given its ability to achieve robust performance with only a small number of expert demonstrations. Nevertheless, deploying IRL on real robots is still challenging due to the large number of robot experiences it requires. This paper aims to address this scalability challenge with a robust, sample-efficient, and general meta-IRL algorithm, SQUIRL, that performs a new but related long-horizon task robustly given only a single video demonstration. First, this algorithm bootstraps the learning of a task encoder and a task-conditioned policy using behavioral cloning (BC). It then collects real-robot experiences and bypasses reward learning by directly recovering a Q-function from the combined robot and expert trajectories. Next, this algorithm uses the learned Q-function to re-evaluate all cumulative experiences collected by the robot to improve the policy quickly. In the end, the policy performs more robustly (90%+ success) than BC on new tasks while requiring no experiences at test time. Finally, our real-robot and simulated experiments demonstrate our algorithm's generality across different state spaces, action spaces, and vision-based manipulation tasks, e.g., pick-pour-place and pick-carry-drop.",https://ieeexplore.ieee.org/document/9340915/,2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),24 Oct.-24 Jan. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SSRR.2019.8848957,Sample Efficient Reinforcement Learning for Navigation in Complex Environments,IEEE,Conferences,"Navigation of mobile robots in unstructured, time-varying environments is challenging. It becomes even more complicated in disaster scenarios where logistical difficulties, as well as technical issues such as reactive and time-varying obstacles, exist. These scenarios are too complex for classical obstacle avoidance methods to navigate through successfully. This paper presents a sample efficient reinforcement learning algorithm for navigation in complex environments. The approach augments training data with randomly generated target location data to accelerate learning. A Q-learning approach is implemented, which is capable of quick training with limited episodes. The procedure is tested in four scenarios in Gazebo and one scenario in a real-world experiment. In the two simulation scenarios with no obstacles, the method can learn to navigate towards the target in fewer than 200 episodes. For environments with moving obstacles, training takes slightly longer, but the process is still able to learn an effective policy quickly.",https://ieeexplore.ieee.org/document/8848957/,"2019 IEEE International Symposium on Safety, Security, and Rescue Robotics (SSRR)",2-4 Sept. 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/RoboSoft48309.2020.9116004,Scalable sim-to-real transfer of soft robot designs,IEEE,Conferences,"The manual design of soft robots and their controllers is notoriously challenging, but it could be augmented-or, in some cases, entirely replaced-by automated design tools. Machine learning algorithms can automatically propose, test, and refine designs in simulation, and the most promising ones can then be manufactured in reality (sim2real). However, it is currently not known how to guarantee that behavior generated in simulation can be preserved when deployed in reality. Although many previous studies have devised training protocols that facilitate sim2real transfer of control polices, little to no work has investigated the simulation-reality gap as a function of morphology. This is due in part to an overall lack of tools capable of systematically designing and rapidly manufacturing robots. Here we introduce a low cost, open source, and modular soft robot design and construction kit, and use it to simulate, fabricate, and measure the simulation-reality gap of minimally complex yet soft, locomoting machines. We prove the scalability of this approach by transferring an order of magnitude more robot designs from simulation to reality than any other method. The kit and its instructions can be found here: github.com/skriegman/sim2real4designs.",https://ieeexplore.ieee.org/document/9116004/,2020 3rd IEEE International Conference on Soft Robotics (RoboSoft),15 May-15 July 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CCIOT45285.2018.9032441,Segmental Deployment of Neural Network in Cloud Robotic System,IEEE,Conferences,"In this paper, we describe a new method for ep neural networks in the field of computer vision, which can effectively solve the difficulty of applying deep learning in the cloud robotic system. By segmenting the trained network, most of the computing tasks can be cut out and offloaded to the cloud. By effective feature extraction and compression methods, the computing power of robot and cloud can be integrated and coordinated. A method of selecting the split points of the network model and a method of data transmission and compression in the communication between robots and cloud after segmenting are given based on the characteristics of machine vision tasks, and the theoretical analysis is carried out. In the experiment, the effectiveness of all the above methods is verified by comparing the compression capability, response time and network performance of the actual network model. The experimental results show that with the use of segmental methods in cloud robotic system, the task of deep network is processed in real time, while the performance is almost guaranteed.",https://ieeexplore.ieee.org/document/9032441/,2018 IEEE 3rd International Conference on Cloud Computing and Internet of Things (CCIOT),20-21 Oct. 2018,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ACCAI53970.2022.9752493,Segmentation of Lidar Point Cloud Data using SOM,IEEE,Conferences,"Sensors play an important role in detecting and perceiving the environment. Many applications of designing a system to understand the environment semantically include vision for navigation, reverse engineering, Simultaneous Localization and Mapping (SLAM), modelling, autonomous vehicles, change detection, and autonomous robots. Accuracy concern, the Lidar sensor is now used in stunning systematic investigations. Lidar sensors generate data in various file formats such as LAS and point cloud data from the sensed environment. The ability to form simple various applications with error-free environment perception from the Lidar point cloud is critical. Navigation of an autonomous vehicle without colliding with obstacles and locating the vehicle on its own may be a difficult task, which is commonly referred to as SLAM. Creating an accurate map for local features in SLAM problems can be a time-consuming process. Many studies are concerned with the creation of a local map for a structured environment. This study will look into the implementation of a self-organized map for an unknown environment. The self-organized map (SOM) learns about the environment and acquires semantic knowledge from a local expert. This is taken as an input and further segments the region, especially unstructured environments like agricultural land, nonindustrial environments. Heretofore, the developed works for clustering and segmentation only for the roadside habitat and industrial purpose. In this paper, we implemented the real-time SOM to perceive any world environment regardless of illumination and is experimented. Implementation outputs are analysed for further rectification.",https://ieeexplore.ieee.org/document/9752493/,"2022 International Conference on Advances in Computing, Communication and Applied Informatics (ACCAI)",28-29 Jan. 2022,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA48506.2021.9561411,Self-Imitation Learning by Planning,IEEE,Conferences,"Imitation learning (IL) enables robots to acquire skills quickly by transferring expert knowledge, which is widely adopted in reinforcement learning (RL) to initialize exploration. However, in long-horizon motion planning tasks, a challenging problem in deploying IL and RL methods is how to generate and collect massive, broadly distributed data such that these methods can generalize effectively. In this work, we solve this problem using our proposed approach called self-imitation learning by planning (SILP), where demonstration data are collected automatically by planning on the visited states from the current policy. SILP is inspired by the observation that successfully visited states in the early reinforcement learning stage are collision-free nodes in the graph-search based motion planner, so we can plan and relabel robot's own trials as demonstrations for policy learning. Due to these self-generated demonstrations, we relieve the human operator from the laborious data preparation process required by IL and RL methods in solving complex motion planning tasks. The evaluation results show that our SILP method achieves higher success rates and enhances sample efficiency compared to selected baselines, and the policy learned in simulation performs well in a real-world placement task with changing goals and obstacles.",https://ieeexplore.ieee.org/document/9561411/,2021 IEEE International Conference on Robotics and Automation (ICRA),30 May-5 June 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA.2018.8460655,Self-Supervised Deep Reinforcement Learning with Generalized Computation Graphs for Robot Navigation,IEEE,Conferences,"Enabling robots to autonomously navigate complex environments is essential for real-world deployment. Prior methods approach this problem by having the robot maintain an internal map of the world, and then use a localization and planning method to navigate through the internal map. However, these approaches often include a variety of assumptions, are computationally intensive, and do not learn from failures. In contrast, learning-based methods improve as the robot acts in the environment, but are difficult to deploy in the real-world due to their high sample complexity. To address the need to learn complex policies with few samples, we propose a generalized computation graph that subsumes value-based model-free methods and model-based methods, with specific instantiations interpolating between model-free and model-based. We then instantiate this graph to form a navigation model that learns from raw images and is sample efficient. Our simulated car experiments explore the design decisions of our navigation model, and show our approach outperforms single-step and N-step double Q-learning. We also evaluate our approach on a real-world RC car and show it can learn to navigate through a complex indoor environment with a few hours of fully autonomous, self-supervised training. Videos of the experiments and code can be found at github.com/gkahn13/gcg.",https://ieeexplore.ieee.org/document/8460655/,2018 IEEE International Conference on Robotics and Automation (ICRA),21-25 May 2018,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ROMAN.2008.4600739,Semantic category acquisition in dialogue for interactive object learning,IEEE,Conferences,"An important aspect of humanoid robots in a natural environment is the ability to acquire new knowledge through learning mechanisms, which enhances an artificial system with the ability to adapt to a changing or new environment. In contrast to most learning algorithms applied in machine learning today, which mainly work with offline learning on training samples, such learning mechanisms need to be performed autonomously and through interaction with the environment or with other agents/humans. In this paper we describe a learning algorithm as a dialogue approach for learning semantic categories and object description in object learning. New objects are introduced to the robot and learning dialogues are conducted as a means of information acquisition. In dialogue, the robot can acquire semantic categories, type and properties of objects, learn new words for object descriptions and learn and association to visual identification from object recognition. In contrast to existing work, this approach combines recognition of real objects, new words learning and semantic categories in one learning dialogue. The presented approach has been implemented in a dialogue system and evaluated on the humanoid robot Armar III.",https://ieeexplore.ieee.org/document/4600739/,RO-MAN 2008 - The 17th IEEE International Symposium on Robot and Human Interactive Communication,1-3 Aug. 2008,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS.2016.7759769,Sensor substitution for video-based action recognition,IEEE,Conferences,"There are many applications where domain-specific sensing, such as accelerometers, kinematics, or force sensing, provide unique and important information for control or for analysis of motion. However, it is not always the case that these sensors can be deployed or accessed beyond laboratory environments. For example, it is possible to instrument humans or robots to measure motion in the laboratory in ways that it is not possible to replicate in the wild. An alternative, which we explore in this paper, is to address situations where accurate sensing is available while training an algorithm, but for which only video is available for deployment. We present two examples of this sensory substitution methodology. The first variation trains a convolutional neural network to regress real-valued signals, including robot end-effector pose, from video. The second example regresses binary signals derived from accelerometer data which signifies when specific objects are in motion. We evaluate these on the JIGSAWS dataset for robotic surgery training assessment and the 50 Salads dataset for modeling complex structured cooking tasks. We evaluate the trained models for video-based action recognition and show that the trained models provide information that is comparable to the sensory signals they replace.",https://ieeexplore.ieee.org/document/7759769/,2016 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),9-14 Oct. 2016,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CVPR.2019.01165,Sim-Real Joint Reinforcement Transfer for 3D Indoor Navigation,IEEE,Conferences,"There has been an increasing interest in 3D indoor navigation, where a robot in an environment moves to a target according to an instruction. To deploy a robot for navigation in the physical world, lots of training data is required to learn an effective policy. It is quite labour intensive to obtain sufficient real environment data for training robots while synthetic data is much easier to construct by render-ing. Though it is promising to utilize the synthetic environments to facilitate navigation training in the real world, real environment are heterogeneous from synthetic environment in two aspects. First, the visual representation of the two environments have significant variances. Second, the houseplans of these two environments are quite different. There-fore two types of information,i.e. visual representation and policy behavior, need to be adapted in the reinforce mentmodel. The learning procedure of visual representation and that of policy behavior are presumably reciprocal. We pro-pose to jointly adapt visual representation and policy behavior to leverage the mutual impacts of environment and policy. Specifically, our method employs an adversarial feature adaptation model for visual representation transfer anda policy mimic strategy for policy behavior imitation. Experiment shows that our method outperforms the baseline by 19.47% without any additional human annotations.",https://ieeexplore.ieee.org/document/8953924/,2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),15-20 June 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IJCNN.2000.859462,"Simulating the evolution of 2D pattern recognition on the CAM-Brain Machine, an evolvable hardware tool for building a 75 million neuron artificial brain",IEEE,Conferences,"This paper presents some simulation results of the evolution of 2D visual pattern recognizers to be implemented very shortly on real hardware, namely the ""CAM-Brain Machine"" (CBM), an FPGA based piece of evolvable hardware which implements a genetic algorithm (GA) to evolve a 3D cellular automata (CA) based neural network circuit module, of approximately 1,000 neurons, in about a second, i.e. a complete run of a GA, with tens of thousands of circuit growths and performance evaluations. Up to 65,000 of these modules, each of which is evolved with a humanly specified function, can be downloaded into a large RAM space, and interconnected according to humanly specified artificial brain architectures. This RAM, containing an artificial brain with up to 75 million neurons, is then updated by the CBM at a rate of 130 billion CA cells per second. Such speeds will enable real time control of robots and hopefully the birth of a new research field that we call ""brain building"". The first such artificial brain, to be built at STARLAB in 2000 and beyond, will be used to control the behaviors of a life sized kitten robot called ""Robokitty"". This kitten robot will need 2D pattern recognizers in the visual section of its artificial brain. This paper presents simulation results on the evolvability and generalization properties of such recognizers.",https://ieeexplore.ieee.org/document/859462/,Proceedings of the IEEE-INNS-ENNS International Joint Conference on Neural Networks. IJCNN 2000. Neural Computing: New Challenges and Perspectives for the New Millennium,27-27 July 2000,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CCDC.2008.4597309,Simulation of robot localization based on virtual sensors,IEEE,Conferences,"A flexible simulation frame based on concept of component is presented. An indoor robots localization simulation environment based on virtual sensors RoboSimer is built with OpenGL. The parameters here are easily adjusted and controlled by customs and the simulation module is easy to integrate. It can integrate any sensors, environment and robot shape into the simulation software. The interfaces of simulation software are coincided with the real hardware platform. It provides a convenient condition for the further study on robot localization.",https://ieeexplore.ieee.org/document/4597309/,2008 Chinese Control and Decision Conference,2-4 July 2008,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS.2018.8593518,Simultaneous End-User Programming of Goals and Actions for Robotic Shelf Organization,IEEE,Conferences,"Arrangement of items on shelves in stores or warehouses is a tedious, repetitive task that can be feasible for robots to perform. The diversity of products that are available in stores and the different setups and preferences of each store makes pre-programming a robot for this task extremely challenging. Instead, our work argues for enabling end-users to customize the robot to their specific objects and setup at deployment time by programming it themselves. To that end, this paper contributes (i) a task representation for shelf arrangements based on a large dataset of grocery store shelf images, (ii) a method for inferring goal configurations from user inputs including demonstrations and direct parameter specifications, and (iii) a system implementation of the proposed approach that allows simultaneously learning task goals and actions. We evaluate our goal inference approach with ten different teaching strategies that combine alternative user inputs in different ways on the large dataset of grocery configurations, as well as with real human teachers through an online user study (N=32). We evaluate our full system implemented on a Fetch mobile manipulator on eight benchmark tasks that demonstrate end-to-end programming and execution of shelf arrangement tasks.",https://ieeexplore.ieee.org/document/8593518/,2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),1-5 Oct. 2018,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS51168.2021.9636012,Simultaneous Semantic and Collision Learning for 6-DoF Grasp Pose Estimation,IEEE,Conferences,"Grasping in cluttered scenes has always been a great challenge for robots, due to the requirement of the ability to well understand the scene and object information. Previous works usually assume that the geometry information of the objects is available, or utilize a step-wise, multi-stage strategy to predict the feasible 6-DoF grasp poses. In this work, we propose to formalize the 6-DoF grasp pose estimation as a simultaneous multi-task learning problem. In a unified framework, we jointly predict the feasible 6-DoF grasp poses, instance semantic segmentation, and collision information. The whole framework is jointly optimized and end-to-end differentiable. Our model is evaluated on large-scale benchmarks as well as the real robot system. On the public dataset, our method outperforms prior state-of-the-art methods by a large margin (+4.08 AP). We also demonstrate the implementation of our model on a real robotic platform and show that the robot can accurately grasp target objects in cluttered scenarios with a high success rate. Project link: https://openbyterobotics.github.io/sscl.",https://ieeexplore.ieee.org/document/9636012/,2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),27 Sept.-1 Oct. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICHR.2010.5686285,SkyAI: Highly modularized reinforcement learning library,IEEE,Conferences,"This paper introduces a software library of reinforcement learning (RL) methods, named SkyAI. SkyAI is a highly modularized RL library for real/simulated robots to learn behaviors. Our ultimate goal is to develop an artificial intelligence (AI) program with which the robots can learn to behave as their users' wish. In this paper, we describe the concepts, the requirements, and the current implementation of SkyAI. SkyAI provides two conflicting features: high execution-speed enough for real robot systems and high flexibility to design learning systems. We also demonstrate the applications to crawling tasks of both a humanoid robot in simulation and a real spider robot.",https://ieeexplore.ieee.org/document/5686285/,2010 10th IEEE-RAS International Conference on Humanoid Robots,6-8 Dec. 2010,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/BigComp51126.2021.00051,Smart Energy Management System based on Reconfigurable AI Chip and Electrical Vehicles,IEEE,Conferences,"Almost every larger city in Europe has ambitious smart city projects. This is particularly true for Hamburg, a Hanseatic city in the north of Germany. Hamburg is the smartest city in Germany according to a Federal Association for Information Technology. Although there are no megacities in the European Union (the largest city in the European Union is Berlin with 3.7 million inhabitants), the increasing urbanization is apparent and produces problems to be solved. At the same time rural depopulation creates conjugated problems.One category of these problems is mobility. Mobility can be regarded as the need to move persons and freight. In densely populated cities an increasing amount of transport users have to share a decreasing amount of space with conflicting needs. At the same time in rural areas, a dwindling supply of local public transport makes the mobility of the remaining residents more difficult. The same applies to parcel delivery or the supply of goods. Autonomous systems have great potential to create a sustainable and livable environment. The author has initiated a publicly funded project to investigate technologies of autonomous mobile systems which interact with a smart city. The test area intelligent urban mobility (Testfeld intelligente Quartiersmobilitat) at the campus of Hamburgs University of Applied Sciences is created to do research on connected and autonomous mobile systems like multipurpose robots and other mobility users like pedestrians with a smartphone. A particular focus is on neighborhood mobility. This means that distances of less than 3 kilometers usually have to be covered. The special type of needs in neighborhood mobility has two important aspects that affect development of autonomous mobile systems: It is slow mobility and the transport users are especially vulnerable. The acceptance of the residents of autonomous systems is equally important, as is the protection of privacy when collecting environmental data. They are expected to make decisions on their own in complex environments. The real world usually differs from a simulation or an experimental setup in a laboratory - a problem commonly referred to as Sim-2-Real gap. Active and non-destructive exploration is expected from an autonomous system to solve unexpected problems. Machine learning methods come into play which in turn have their own pitfalls. The author has built a specialized laboratory to investigate machine learning technology applied to autonomous systems. In this laboratory miniature autonomous vehicles are developed. The general idea of this experimental setup allows research on new methodologies for autonomous systems in a very small scale",https://ieeexplore.ieee.org/document/9373129/,2021 IEEE International Conference on Big Data and Smart Computing (BigComp),17-20 Jan. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA.2018.8460968,Socially Compliant Navigation Through Raw Depth Inputs with Generative Adversarial Imitation Learning,IEEE,Conferences,"We present an approach for mobile robots to learn to navigate in dynamic environments with pedestrians via raw depth inputs, in a socially compliant manner. To achieve this, we adopt a generative adversarial imitation learning (GAIL) strategy, which improves upon a pre-trained behavior cloning policy. Our approach overcomes the disadvantages of previous methods, as they heavily depend on the full knowledge of the location and velocity information of nearby pedestrians, which not only requires specific sensors, but also the extraction of such state information from raw sensory input could consume much computation time. In this paper, our proposed GAIL-based model performs directly on raw depth inputs and plans in real-time. Experiments show that our GAIL-based approach greatly improves the safety and efficiency of the behavior of mobile robots from pure behavior cloning. The real-world deployment also shows that our method is capable of guiding autonomous vehicles to navigate in a socially compliant manner directly through raw depth inputs. In addition, we release a simulation plugin for modeling pedestrian behaviors based on the social force model.",https://ieeexplore.ieee.org/document/8460968/,2018 IEEE International Conference on Robotics and Automation (ICRA),21-25 May 2018,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/UR55393.2022.9826285,Stereoscopic low-latency vision system via ethernet network for Humanoid Teleoperation,IEEE,Conferences,"Despite the recent rise of artificial intelligence, the automatic operation of the robot in an unstructured environment is still insufficient to perform complicated and detailed tasks and requires remote operation by a human. The most important thing to improve the efficiency of such Teleoperation is the vision system that enables the human operator to recognize the surrounding situation of the robot. While constructing the system, immersive experience for the operator was considered. Low latency and curved plane rendering are proposed to provide an immersive experience for operators. Moreover, considering the operator&#x2019;s exhaustion during the teleoperation that comes from Virtual Reality sickness, the reduction of Virtual Reality sickness is also suggested. Finally, to check the entire system&#x2019;s visual feedback latency, display-to-display latency is quantitatively measured. This paper presents an implementation of a vision system for the Teleoperation of humanoid robots through Virtual Reality.",https://ieeexplore.ieee.org/document/9826285/,2022 19th International Conference on Ubiquitous Robots (UR),4-6 July 2022,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CISIM.2008.21,Strategy Description and Modelling for Multi-Agent Systems,IEEE,Conferences,"The field of robot soccer provides numerous opportunities for the application of AI methods for game strategy development. Robot soccer is a part of standard applications of distributed system control in real time. The software part of a distributed control system is realized by decision making and executive agents. The algorithm of agents' cooperation was proposed with the control agent on a higher level. The algorithms for agents realized in robots are the same. Real-time dynamic simple strategy description and strategy learning possibility based on game observation is important for discovering opponent's strategies and searching for tactical group movements, simulation and synthesis of suitable counter-strategies. For the improvement of game strategy, we are developing an abstract description of the game and propose ways to use this description (e.g. for learning rules and adapting team strategies to every single opponent).",https://ieeexplore.ieee.org/document/4557834/,2008 7th Computer Information Systems and Industrial Management Applications,26-28 June 2008,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/UKSIM.2008.117,Strategy Description and Modelling for Multi-agent Systems (Invited Paper),IEEE,Conferences,"The field of robot soccer provides numerous opportunities for the application of AI methods for game strategy development. Robot soccer is a part of standard applications of distributed system control in real time. The software part of a distributed control system is realized by decision making and executive agents. The algorithm of agents’ cooperation was proposed with the control agent on a higher level. The algorithms for agents realized in robots are the same. Real-time dynamic simple strategy description and strategy learning possibility based on game observation is important for discovering opponents strategies and searching for tactical group movements, simulation and synthesis of suitable counter-strategies. For the improvement of game strategy, we are developing an abstract description of the game and propose ways to use this description (e.g. for learning rules and adapting team strategies to every single opponent).",https://ieeexplore.ieee.org/document/4489006/,Tenth International Conference on Computer Modeling and Simulation (uksim 2008),1-3 April 2008,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SACI.2016.7507375,Superior vision: Always on human-like vision for intelligent devices,IEEE,Conferences,"Summary form only given. We live in a world of intelligent robots, drones and mobile phones/assistants. The technologies behind these products are heralded by a new wave of deep learning applications ported on mobile devices. The robotic and vision technologies behind these products will shift the applications of electronic devices to a more superior level of intelligence that will change our world. From a hardware point of view, these devices will become a hub of learning sensors that will be able to capture and learn from their surrounding environment and their context. On top of the aforementioned deep learning technologies, the most important sense for these devices will be their vision capabilities. To make this sense as close as possible to human vision, will require advanced technology to capture, process, understand and provide analytics in real time to enable their applications make critical “educated: decisions”. “Always on” vision capabilities are envisaged as the paradigm for the next generation of mobile This Keynote will cover in this context the state of the art in the domain of vision with the latest image processing, computer vision and deep learning algorithms implemented on various low-power processing architectures.",https://ieeexplore.ieee.org/document/7507375/,2016 IEEE 11th International Symposium on Applied Computational Intelligence and Informatics (SACI),12-14 May 2016,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS51168.2021.9635867,SurRoL: An Open-source Reinforcement Learning Centered and dVRK Compatible Platform for Surgical Robot Learning,IEEE,Conferences,"Autonomous surgical execution relieves tedious routines and surgeon&#x2019;s fatigue. Recent learning-based methods, especially reinforcement learning (RL) based methods, achieve promising performance for dexterous manipulation, which usually requires the simulation to collect data efficiently and reduce the hardware cost. The existing learning-based simulation platforms for medical robots suffer from limited scenarios and simplified physical interactions, which degrades the real-world performance of learned policies. In this work, we designed SurRoL, an RL-centered simulation platform for surgical robot learning compatible with the da Vinci Research Kit (dVRK). The designed SurRoL integrates a user-friendly RL library for algorithm development and a real-time physics engine, which is able to support more PSM/ECM scenarios and more realistic physical interactions. Ten learning-based surgical tasks are built in the platform, which are common in the real autonomous surgical execution. We evaluate SurRoL using RL algorithms in simulation, provide in-depth analysis, deploy the trained policies on the real dVRK, and show that our SurRoL achieves better transferability in the real world.",https://ieeexplore.ieee.org/document/9635867/,2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),27 Sept.-1 Oct. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IEEECONF49454.2021.9382607,Teaching System for Multimodal Object Categorization by Human-Robot Interaction in Mixed Reality,IEEE,Conferences,"As service robots are becoming essential to support aging societies, teaching them how to perform general service tasks is still a major challenge preventing their deployment in daily-life environments. In addition, developing an artificial intelligence for general service tasks requires bottom-up, unsupervised approaches to let the robots learn from their own observations and interactions with the users. However, compared to the top-down, supervised approaches such as deep learning where the extent of the learning is directly related to the amount and variety of the pre-existing data provided to the robots, and thus relatively easy to understand from a human perspective, the learning status in bottom-up approaches is by their nature much harder to appreciate and visualize. To address these issues, we propose a teaching system for multimodal object categorization by human-robot interaction through Mixed Reality (MR) visualization. In particular, our proposed system enables a user to monitor and intervene in the robot&#x2019;s object categorization process based on Multimodal Latent Dirichlet Allocation (MLDA) to solve unexpected results and accelerate the learning. Our contribution is twofold by 1) describing the integration of a service robot, MR interactions, and MLDA object categorization in a unified system, and 2) proposing an MR user interface to teach robots through intuitive visualization and interactions.",https://ieeexplore.ieee.org/document/9382607/,2021 IEEE/SICE International Symposium on System Integration (SII),11-14 Jan. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SNPDWinter52325.2021.00048,Technology-driven Service Innovation in University Libraries,IEEE,Conferences,"Libraries are building a digital environment and preparing for the post-corona era. They attempt to increase operational efficiency and competitiveness by applying new technologies and strive to build a more intelligent and user-friendly environment using big data, the Internet of things (IoT), artificial intelligence (AI), virtual reality (VR), 3D printing, and automated robots. This study aims to present the direction for future university libraries by analyzing technology-based service innovation cases in line with the current era, centering on university libraries",https://ieeexplore.ieee.org/document/9403519/,"2021 21st ACIS International Winter Conference on Software Engineering, Artificial Intelligence, Networking and Parallel/Distributed Computing (SNPD-Winter)",28-30 Jan. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICISCAE48440.2019.221655,The Construction of Portrait Identification Tracking System Based on Mask R-CNN,IEEE,Conferences,"A portrait identification and tracking system with strong real-time performance, good flexibility and controllable cost is designed and implemented in the paper. Firstly, Mask R-CNN neural network is used to extract the features of the target, and the COCO dataset is used to train and establish the portrait data model. As a result, accuracy of the portrait recognition is improved. Then, a portrait tracking system including monocular camera, data acquisition module, data processing module, steering gear and control system is built. And the ""Raspberry Pi"" control method is used to control the MG955 steering gear group. Finally, the recognition and tracking of characters can be realized through wired, WIFI, Bluetooth and other ways, which improves the universality of the system. The designed system has simple structure, complete functions and can be used for automatic aiming and tracking of other objects. The modular system can also be used for unmanned aerial vehicles, robots and other platforms.",https://ieeexplore.ieee.org/document/9075573/,2019 2nd International Conference on Information Systems and Computer Aided Education (ICISCAE),28-30 Sept. 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CRC52766.2021.9620166,The Development of an Omnidirectional Mobile Robot Based on Hub Motor,IEEE,Conferences,"To enable the ability of moving in a narrow space, the robots are required to move in all directions. However, traditional robots with omnidirectional mobile function are easily wearing, with poor bearing capacity, and with complex structure. We designed and proposed an omnidirectional mobile robot (OMR) with the active split offset caster (ASoC) wheelset as the driving wheelset. Specifically, we first established the mathematical model of the robot based on the differential drive principle. Then the mathematical model is verified with synchronized real and simulated movement of the designed robot. A motion capture system is used to track the actual motion trajectory. Finally, a road test experiment is applied to test the stability of the designed robot in a real environment. The tiny differences between real and simulated trajectory show that the designed robot has the ability to move omnidirectionally in a narrow space. The road test experiment proves the advanced adaptability in real environment. Therefore, the designed robot can assist in a narrow space.",https://ieeexplore.ieee.org/document/9620166/,"2021 6th International Conference on Control, Robotics and Cybernetics (CRC)",9-11 Oct. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISDFS55398.2022.9800837,The Necessity of Emotion Recognition from Speech Signals for Natural and Effective Human-Robot Interaction in Society 5.0,IEEE,Conferences,"The history of humanity has reached Industry 4.0 that aims to the integration of information technologies and especially artificial intelligence with all life-sustaining mechanisms in the 21st century, and consecutively, the transformation of Society 5.0 has begun. Society 5.0 means a smart society in which humans share life with physical robots and software robots as well as smart devices based on augmented reality. Industry 4.0 contains main structures such as the internet of things, big data analytics, digital transformation, cyber-physical systems, artificial intelligence, and business processes optimization. It is impossible to consider the machines to be without emotions and emotional intelligence within the transformation of smart tools and artificial intelligence, in addition, while it is planned to give most of the commands with voice and speaking, it became more important to develop algorithms that can detect emotions. In the smart society, new and rapid methods are needed for speech recognition, emotion recognition, and speech emotion recognition areas to maximize human-computer (HCI) or human-robot interaction (HRI) and collaboration. In this study, speech recognition and speech emotion recognition studies in robot technology are investigated and developments are revealed.",https://ieeexplore.ieee.org/document/9800837/,2022 10th International Symposium on Digital Forensics and Security (ISDFS),6-7 June 2022,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICMLC.2002.1174408,The approach of extracting features from the local environment for mobile robot,IEEE,Conferences,"A new data fusion method to extract features from the local environment for a mobile robot's navigation has been developed and implemented. This method, named the obstacle group, compresses data in a series of levels in order to reduce the quantity of data for communication between modules in a distributed single-robot system, or between all the robots and the central station in a multi-robot system. The method based on a grid map and an active window has strong adaptability and is real-time in a crowded environment. Experimental results demonstrate that the robot can successfully avoid collisions and plan its path by using this method.",https://ieeexplore.ieee.org/document/1174408/,Proceedings. International Conference on Machine Learning and Cybernetics,4-5 Nov. 2002,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS.2010.5650765,The design of LEO: A 2D bipedal walking robot for online autonomous Reinforcement Learning,IEEE,Conferences,"Real robots demonstrating online Reinforcement Learning (RL) to learn new tasks are hard to find. The specific properties and limitations of real robots have a large impact on their suitability for RL experiments. In this work, we derive the main hardware and software requirements that a RL robot should fulfill, and present our biped robot LEO that was specifically designed to meet these requirements. We verify its aptitude in autonomous walking experiments using a pre-programmed controller. Although there is room for improvement in the design, the robot was able to walk, fall and stand up without human intervention for 8 hours, during which it made over 43; 000 footsteps.",https://ieeexplore.ieee.org/document/5650765/,2010 IEEE/RSJ International Conference on Intelligent Robots and Systems,18-22 Oct. 2010,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IVS.1994.639471,The development of a fully autonomous ground vehicle (FAGV),IEEE,Conferences,"As a first step toward the creation of a fully autonomous vehicle that operates in a real world environment, we are currently developing a prototype autonomous ground vehicle (AGV) for use in factories and other industrial/business sites based on behavior-based artificial intelligence (AI) control. This flexible and fully autonomous AGV (FAGV) is expected to operate efficiently in a normal industrial environment without any external guidance. The crucial technique employed is a non-Cartesian way of organizing software agents for the creation of a highly responsive control program. The resulting software is considerably reduced in size. Through numerous experiments using mobile robots we confirmed that these new control programs excel in functionality, efficiency, flexibility and robustness. The second key technique in the planning stage is evolutionary computation, of which genetic algorithms are a principal technique. An online, real-time evolution of the control program will be incorporated in later phases of the project to make FAGVs adaptable to any given operational environment after deployment. The first prototype FAGV has an active vision and behaviour-based control system.",https://ieeexplore.ieee.org/document/639471/,Proceedings of the Intelligent Vehicles '94 Symposium,24-26 Oct. 1994,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CEC.2002.1004464,The force model: reducing the complexity by reformulating the problem,IEEE,Conferences,"Most experiments in research on autonomous agents and mobile robots are performed either in simulation or on robots with static physical properties; evolvable hardware is hardly ever used. One of the very rare exceptions is the eyebot on which Lichtensteiger and Eggenberger have evolved simplified insect eyes. Even though substantially improved, the evolutionary models currently applied still lack both scalability and noise-resistance. To tackle these problems, this paper proposes a biologically-inspired force model for this class of real-world applications. The simulation results clearly indicate that this model provides a significant improvement over existing limitations. Furthermore, this paper argues that the force model is of more general utility.",https://ieeexplore.ieee.org/document/1004464/,Proceedings of the 2002 Congress on Evolutionary Computation. CEC'02 (Cat. No.02TH8600),12-17 May 2002,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS40897.2019.8968514,Timepix Radiation Detector for Autonomous Radiation Localization and Mapping by Micro Unmanned Vehicles,IEEE,Conferences,"A system for measuring radiation intensity and for radiation mapping by a micro unmanned robot using the Timepix detector is presented in this paper. Timepix detectors are extremely small, but powerful 14 × 14 mm, 256 × 256 px CMOS hybrid pixel detectors, capable of measuring ionizing alpha, beta, gamma radiation, and heaving ions. The detectors, developed at CERN, produce an image free of any digital noise thanks to per-pixel calibration and signal digitization. Traces of individual ionizing particles passing through the sensors can be resolved in the detector images. Particle type and energy estimates can be extracted automatically using machine learning algorithms. This opens unique possibilities in the task of flexible radiation detection by very small unmanned robotic platforms. The detectors are well suited for the use of mobile robots thanks to their small size, lightweight, and minimal power consumption. This sensor is especially appealing for micro aerial vehicles due to their high maneuverability, which can increase the range and resolution of such novel sensory system. We present a ROS-based readout software and real-time image processing pipeline and review options for 3-D localization of radiation sources using pixel detectors. The provided software supports off-the-shelf FITPix, USB Lite readout electronics with Timepix detectors.",https://ieeexplore.ieee.org/document/8968514/,2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),3-8 Nov. 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ETFA45728.2021.9613144,Towards User-Awareness in Human-Robot Collaboration for Future Cyber-Physical Systems,IEEE,Conferences,"Cyber-Physical Systems constitute one of the core concepts in Industry 4.0 aiming at realizing production systems that combine the efforts from human workers, robots and intelligent entities. This is particularly true in Human-Robot Collaboration manufacturing where a tight peer-to-peer interaction between humans and (intelligent) autonomous robots is necessary. Such production systems need a holistic integration along different levels of abstraction and coordination for deploying effective and safe control solutions. We propose the use of novel Artificial Intelligence technologies to enhance flexibility and adaptability of these collaborative systems. Our aim is to advance the classical human-aware paradigm that considers the worker as an anonymous acting entity, in favour of a user-aware paradigm, that considers a worker as profiled user characterized with a number of specific features influencing the &#x201C;shape&#x201D; of the collaboration.",https://ieeexplore.ieee.org/document/9613144/,2021 26th IEEE International Conference on Emerging Technologies and Factory Automation (ETFA ),7-10 Sept. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS40897.2019.8968166,Towards a Robot Architecture for Situated Lifelong Object Learning,IEEE,Conferences,"The ability to acquire knowledge incrementally and after deployment is of utmost importance for robots operating in the real world. Moreover, robots that have to operate alongside people need to be able to interact in a way that is intuitive for the users, e.g., by understanding and producing natural language. In this paper we present a first prototype of a robot architecture developed for situated lifelong object learning. The system is able to communicate with its users through natural language and perform object learning and recognition on the spot through situated interactions. In this first stage, we evaluate the system in terms of recognition accuracy which gives an indirect measure of the quality of the collected data with the proposed pipeline. Our results show that the robot can use this data for both learning and recognition with acceptable incremental performance. We also discuss limitations and steps that are necessary in order to improve performance as well as to shed some light on system usability.",https://ieeexplore.ieee.org/document/8968166/,2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),3-8 Nov. 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ROMAN.2002.1045641,Towards grounded human-robot communication,IEEE,Conferences,"Future robots are expected to communicate with humans using natural language. The naive human user will expect a robot to easily understand what he/she is meaning by instructions concerning robot's tasks. This implies that the robot will need to have a means of grounding, in its own sensors, the natural language terms and constructions used by the human user. This paper presents an approach to solve this problem that is based on the integration of a ""learning server"" in the software architecture of the robot. Such server should be capable of on-line, incremental learning from examples; it should handle multiple problems concurrently and it should have meta-learning capabilities. A learning server already developed by the authors is presented. Complementarily, the dimensionality reduction problem is also addressed, using a Blocked DCT approach. Experimental results are obtained in a scenario in which three concepts (corresponding to natural language expressions) are concurrently learned.",https://ieeexplore.ieee.org/document/1045641/,Proceedings. 11th IEEE International Workshop on Robot and Human Interactive Communication,27-27 Sept. 2002,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCAR55106.2022.9782656,Trade-off on Sim2Real Learning: Real-world Learning Faster than Simulations,IEEE,Conferences,"Deep Reinforcement Learning (DRL) experiments are commonly performed in simulated environments due to the tremendous training sample demands from deep neural networks. In contrast, model-based Bayesian Learning allows a robot to learn good policies within a few trials in the real world. Although it takes fewer iterations, Bayesian methods pay a relatively higher computational cost per trial, and the advantage of such methods is strongly tied to dimensionality and noise. In here, we compare a Deep Bayesian Learning algorithm with a model-free DRL algorithm while analyzing our results collected from both simulations and real-world experiments. While considering Sim and Real learning, our experiments show that the sample-efficient Deep Bayesian RL performance is better than DRL even when computation time (as opposed to number of iterations) is taken in consideration. Additionally, the difference in computation time between Deep Bayesian RL performed in simulation and in experiments point to a viable path to traverse the reality gap. We also show that a mix between Sim and Real does not outperform a purely Real approach, pointing to the possibility that reality can provide the best prior knowledge to a Bayesian Learning. Roboticists design and build robots every day, and our results show that a higher learning efficiency in the real-world will shorten the time between design and deployment by skipping simulations.",https://ieeexplore.ieee.org/document/9782656/,"2022 8th International Conference on Control, Automation and Robotics (ICCAR)",8-10 April 2022,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ROBIO.2017.8324512,Trajectory tracking control of a unicycle-type mobile robot with a new planning algorithm,IEEE,Conferences,"Trajectory tracking control is one of the core techniques that impacts the auto-driving performance of a mobile robot. Whereas, there lacks enough work on reference trajectory generation and controller design for practical usage. This paper considers mobile robots with unicycle vehicle model on which most of automatic guided vehicles (AGVs) in real world are built. A new trajectory planning algorithm is developed, and is applied along with a control law considering constraints of the unicycle model and limited motor capabilities. The proposed algorithm is easy to be implemented on real world AGVs, and it yields a fast, accurate and robust trajectory tracking performance. The effectiveness of the algorithm is validated by simulation tests.",https://ieeexplore.ieee.org/document/8324512/,2017 IEEE International Conference on Robotics and Biomimetics (ROBIO),5-8 Dec. 2017,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICACR51161.2020.9265509,Transfer of Inter-Robotic Inductive Classifier,IEEE,Conferences,"In multi-robot deployments, the robots need to share and integrate their own experience and perform transfer learning. Under the assumption that the robots have the same morphology and carry equivalent sensory equipment, the problem of transfer learning can be considered incremental learning. Thus, the transfer learning problem inherits the challenges of incremental learning, such as catastrophic forgetting and concept drift. In catastrophic forgetting, the model abruptly forgets the previously learned knowledge during the learning process. The concept drift arises with different experiences between consecutively sampled models. However, state-of-the-art robotic transfer learning approaches do not address both challenges at once. In this paper, we propose to use an incremental classifier on a transfer learning problem. The feasibility of the proposed approach is demonstrated in a real deployment. The robot consistently merges two classifiers learned on two different tasks into a classifier that performs well on both tasks.",https://ieeexplore.ieee.org/document/9265509/,"2020 4th International Conference on Automation, Control and Robots (ICACR)",11-13 Oct. 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/RO-MAN46459.2019.8956420,Trust Repair in Human-Swarm Teams+,IEEE,Conferences,"Swarm robots are coordinated via simple control laws to generate emergent behaviors such as flocking, rendezvous, and deployment. Human-swarm teaming has been widely proposed for scenarios, such as human-supervised teams of unmanned aerial vehicles (UAV) for disaster rescue, UAV and ground vehicle cooperation for building security, and soldier-UAV teaming in combat. Effective cooperation requires an appropriate level of trust, between a human and a swarm. When an UAV swarm is deployed in a real-world environment, its performance is subject to real-world factors, such as system reliability and wind disturbances. Degraded performance of a robot can cause undesired swarm behaviors, decreasing human trust. This loss of trust, in turn, can trigger human intervention in UAVs' task executions, decreasing cooperation effectiveness if inappropriate. Therefore, to promote effective cooperation we propose and test a trust-repairing method (Trust-repair) restoring performance and human trust in the swarm to an appropriate level by correcting undesired swarm behaviors. Faulty swarms caused by both external and internal factors were simulated to evaluate the performance of the Trust-repair algorithm in repairing swarm performance and restoring human trust. Results show that Trust-repair is effective in restoring trust to a level intermediate between normal and faulty conditions.",https://ieeexplore.ieee.org/document/8956420/,2019 28th IEEE International Conference on Robot and Human Interactive Communication (RO-MAN),14-18 Oct. 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ASID50160.2020.9271762,Use of LSTM Regression and Rotation Classification to Improve Camera Pose Localization Estimation,IEEE,Conferences,"More accurately estimating camera pose can be used to greatly improve localization in applications such as augmented reality, autonomous driving, and intelligent robots. Deep learning methods have achieved great progress to improve accuracy but still have limitations with respect to rotation, which results in angle regression errors. In this work, we combine a LSTM module with rotation classification loss to regress the camera pose. The algorithm uses a robust processing pipeline to supervise the pose estimation with dynamic, weighted, multi-losses in order to limit separate Euler angle (yaw, pitch, roll) losses, and common translation-quaternion losses. An empirical test on the 7Scenes benchmark dataset shows better results than when using common absolute pose regression methods.",https://ieeexplore.ieee.org/document/9271762/,"2020 IEEE 14th International Conference on Anti-counterfeiting, Security, and Identification (ASID)",30 Oct.-1 Nov. 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICEPDS.2018.8571820,Using Robot and Electric Drive in Fall Prediction,IEEE,Conferences,"The global aging phenomenon has motivated active research in human fall injuries. The fall prevention has hence become a popular topic in health informatics. An effective fall prevention paradigm could save millions of people from injury and avoid considerable casualties. Through comparison studies, detail-oriented simulations, and pragmatic field tests, an effective fall prediction method has been developed by authors. The finding is presented in this paper. Three techniques for fall prediction are discussed in this paper. A comparison technique to mimic the traditional stateless fall prediction techniques, along with an algorithm using artificial neural network, was first implemented in authors' previous paper. Then a robotic scheme was developed to simulate human fall by transplanting a proven fall prediction paradigm for humanoid robots with controlled electric drive systems to human subjects. Due to its simulation nature far from the human fall scenarios in reality, the robotic paradigm has obvious limits in real world applications. It was also used more like a reference framework for our last scheme. Eventually we built the third approach that eliminated the above limitation. The third approach is elaborated in this paper. Our test and simulation have proved its pragmatic superiority over other two approaches, along with vast majority of traditional paradigms.",https://ieeexplore.ieee.org/document/8571820/,2018 X International Conference on Electrical Power Drive Systems (ICEPDS),3-6 Oct. 2018,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AERO.2015.7119180,Utilizing Artificial Intelligence to achieve a robust architecture for future robotic spacecraft,IEEE,Conferences,"This paper presents a novel failure-tolerant architecture for future robotic spacecraft. It is based on the Time and Space Partitioning (TSP) principle as well as a combination of Artificial Intelligence (AI) and traditional concepts for system failure detection, isolation and recovery (FDIR). Contrary to classic payload that is separated from the platform, robotic devices attached onto a satellite become an integral part of the spacecraft itself. Hence, the robot needs to be integrated into the overall satellite FDIR concept in order to prevent fatal damage upon hardware or software failure. In addition, complex dexterous manipulators as required for onorbit servicing (OOS) tasks may reach unexpected failure states, where classic FDIR methods reach the edge of their capabilities with respect to successfully detecting and resolving them. Combining, and partly replacing traditional methods with flexible AI approaches aims to yield a control environment that features increased robustness, safety and reliability for space robots. The developed architecture is based on a modular on-board operational framework that features deterministic partition scheduling, an OS abstraction layer and a middleware for standardized inter-component and external communication. The supervisor (SUV) concept is utilized for exception and health management as well as deterministic system control and error management. In addition, a Kohonen self-organizing map (SOM) approach was implemented yielding a real-time robot sensor confidence analysis and failure detection. The SOM features nonsupervized training given a typical set of defined world states. By compiling a set of reviewable three-dimensional maps, alternative strategies in case of a failure can be found, increasing operational robustness. As demonstrator, a satellite simulator was set up featuring a client satellite that is to be captured by a servicing satellite with a 7-DoF dexterous manipulator. The avionics and robot control were integrated on an embedded, space-qualified Airbus e.Cube on-board computer. The experiments showed that the integration of SOM for robot failure detection positively complemented the capabilities of traditional FDIR methods.",https://ieeexplore.ieee.org/document/7119180/,2015 IEEE Aerospace Conference,7-14 March 2015,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SAMI.2016.7422985,Vehicle navigation by fuzzy cognitive maps using sonar and RFID technologies,IEEE,Conferences,"Emerging concept of the so-called intelligent space (IS) offers means for use of mobile autonomous devices like vehicles or robots in a very broad area without necessity for these devices to own all necessary sensors. From this reason also new navigation methods are developing, which utilize IS means, with the aim to offer maybe not so accurate but first of all cheep and reliable solutions for a wide variety of devices. Our paper deals with the examination of possibility to interconnect sparsely deployed RFID tags with sonars. As signals produced by these two technologies are often affected by uncertainty and incompleteness we use fuzzy logic for their processing as well as control of the entire navigation process. For this purpose a special type of a fuzzy cognitive map was proposed. The paper describes real navigation experiments with a simple vehicle and evaluates them by selected criteria. Based on obtained results their explanations and conclusions for potential future research are sketched.",https://ieeexplore.ieee.org/document/7422985/,2016 IEEE 14th International Symposium on Applied Machine Intelligence and Informatics (SAMI),21-23 Jan. 2016,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS.2018.8594234,Virtual Occupancy Grid Map for Submap-based Pose Graph SLAM and Planning in 3D Environments,IEEE,Conferences,"In this paper, we propose a mapping approach that constructs a globally deformable virtual occupancy grid map (VOG-map) based on local submaps. Such a representation allows pose graph SLAM systems to correct globally accumulated drift via loop closures while maintaining free space information for the purpose of path planning. We demonstrate use of such a representation for implementing an underwater SLAM system in which the robot actively plans paths to generate accurate 3D scene reconstructions. We evaluate performance on simulated as well as real-world experiments. Our work furthers capabilities of mobile robots actively mapping and exploring unstructured, three dimensional environments.",https://ieeexplore.ieee.org/document/8594234/,2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),1-5 Oct. 2018,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ITST.2017.7972192,Virtual assistants and self-driving cars,IEEE,Conferences,"Self-driving cars are technologically a reality and in the next decade they are expected to reach the highest level of automation. While there is general agreement that an advanced human-autonomous vehicle (HAV) interaction is key to achieve the benefits of self-driving cars, it is less clear what role artificial intelligence (AI) should play in this context. While the scientific community is debating on the role and intersections of AI, autonomous vehicles and related issues, above all ethics, the automotive industry is already presenting AI-based products and services that may influence, in a direction or in another, our technological and societal futures. This paper focuses on virtual assistants, the personification of the car intelligence incorporating, among others, an algorithmic “brain”, a synthetic human “voice” and powerful sensor-based “senses”. Should virtual assistants just assist humans or replace them whenever necessary? Should their scope of action be limited to safety-related driving tasks or to any activity performed in the car or controlled from the car? Although at a very early stage of commercial development, the paper will review the state-of-the-art of in-car virtual assistants underlining their role and functions in the connected and automated driving ecosystem. By drawing from earlier reflections on automation, robots and intelligent agents, it will then identify a series of issues to be addressed by the scientific community, policy-makers and the automotive industry stakeholders.",https://ieeexplore.ieee.org/document/7972192/,2017 15th International Conference on ITS Telecommunications (ITST),29-31 May 2017,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/OCEANS.1995.528724,Virtual world visualization for an autonomous underwater vehicle,IEEE,Conferences,"A critical bottleneck exists in autonomous underwater vehicle (AUV) design and development. It is tremendously difficult to observe, communicate with and test underwater robots, because they operate in a remote and hazardous environment where physical dynamics and sensing modalities are counterintuitive. An underwater virtual world can comprehensively model all necessary functional characteristics of the real world in real time. This virtual world is designed from the perspective of the robot, enabling realistic AUV evaluation and testing in the laboratory. 3D real-time graphics are our window into the virtual world, enabling multiple observers to visualize complex interactions. A networked architecture enables multiple world components to operate collectively in real time, and also permits world-wide observation and collaboration with other scientists interested in the robot and virtual world.",https://ieeexplore.ieee.org/document/528724/,'Challenges of Our Changing Global Environment'. Conference Proceedings. OCEANS '95 MTS/IEEE,9-12 Oct. 1995,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/i-Society.2016.7854189,Virtualized higher education: Where e-learning trends and new faculty roles converge towards personalization,IEEE,Conferences,"Virtual Higher Education is in sharp focus lately with onslaught of Online Learning Platforms (MOOC, Coursera, Khan Academy, Udacity) offering Free Courses in every discipline. Originally targeted at populations in underdeveloped countries not otherwise able to offer a world-class education, these courses have become popular and mainstream in developed countries being free, online and caters to most professionals to build skills and explore career changes. Current delivery format robs an academic the ability to express their teaching style. The PLErify Application seeks to address the need to support, through a `DIY approach', an Academic's teaching style and retain ownership of materials. PLErify is discussed in the paper relative to the Robots role vis a vis the traditional Professor's role and the prevailing thought of the best ways to energize and modernize current teaching methods.",https://ieeexplore.ieee.org/document/7854189/,2016 International Conference on Information Society (i-Society),10-13 Oct. 2016,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AQTR.2006.254650,Vision based algorithm for path planning of a mobile robot by using cellular neural networks,IEEE,Conferences,"The paper presents a new vision based algorithm for mobile robots path planning in an environment with obstacles. Cellular neural networks (CNNs) processing techniques are used here for real time motion planning to reach a fixed target. The CNN methods have been considered a solution for image processing in autonomous mobile robots guidance. The choice of CNNs for the visual processing is based on the possibility of their hardware implementation in large networks on a single VLSI chip (cellular neural networks -universal machine, CNN-UM (Roska and Chua, 1993 and Kim et al., 2002))",https://ieeexplore.ieee.org/document/4022973/,"2006 IEEE International Conference on Automation, Quality and Testing, Robotics",25-28 May 2006,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA.2019.8794123,Visual Guidance and Automatic Control for Robotic Personalized Stent Graft Manufacturing,IEEE,Conferences,"Personalized stent graft is designed to treat Abdominal Aortic Aneurysms (AAA). Due to the individual difference in arterial structures, stent graft has to be custom made for each AAA patient. Robotic platforms for autonomous personalized stent graft manufacturing have been proposed in recently which rely upon stereo vision systems for coordinating multiple robots for fabricating customized stent grafts. This paper proposes a novel hybrid vision system for real-time visual-sevoing for personalized stent-graft manufacturing. To coordinate the robotic arms, this system is based on projecting a dynamic stereo microscope coordinate system onto a static wide angle view stereo webcam coordinate system. The multiple stereo camera configuration enables accurate localization of the needle in 3D during the sewing process. The scale-invariant feature transform (SIFT) method and color filtering are implemented for stereo matching and feature identifications for object localization. To maintain the clear view of the sewing process, a visual-servoing system is developed for guiding the stereo microscopes for tracking the needle movements. The deep deterministic policy gradient (DDPG) reinforcement learning algorithm is developed for real-time intelligent robotic control. Experimental results have shown that the robotic arm can learn to reach the desired targets autonomously.",https://ieeexplore.ieee.org/document/8794123/,2019 International Conference on Robotics and Automation (ICRA),20-24 May 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICSESS52187.2021.9522158,Visual Loop Closure Detection Based on Lightweight Convolutional Neural Network and Product Quantization,IEEE,Conferences,"Mobile robots rely heavily on the creation of the scene map and the positioning in the map in an unknown environment, but no matter what type of map is created, it is inevitably affected by cumulative errors. This presents a huge challenge for loop closure detection technology. Using traditional loop closure detection methods to perform scene recognition is difficult to extract the appearance changes caused by time, weather, or seasonal conditions in the image and deep semantic information, and the speed of extracting image features is slow, which is difficult to meet the real-time performance of robots. Because of the success of deep convolutional neural networks(CNN), it is possible to enrich the information of image features. First of all, this paper uses the pre-trained CNN model SSE-Net to extract the deep visual appearance and semantic features of the image, and obtain the feature description vector. Then, after product quantization(PQ) and encoding, the final pair of candidate frames is quickly searched and matched to obtain the most similar pair of candidate frames and judged as a loop . After the verification of the New collage dataset and the City Center dataset, this algorithm has achieved a good Precision-Recall rate and a faster speed compared with the recently proposed large-scale convolution network VGG16 method and traditional feature extraction methods.",https://ieeexplore.ieee.org/document/9522158/,2021 IEEE 12th International Conference on Software Engineering and Service Science (ICSESS),20-22 Aug. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IECON.2019.8926916,Visual Subterranean Junction Recognition for MAVs based on Convolutional Neural Networks,IEEE,Conferences,"This article proposes a novel visual framework for detecting tunnel crossings/junctions in underground mine areas towards the autonomous navigation of Micro Aerial Vehicles (MAVs). Usually mine environments have complex geometries, including multiple crossings with different tunnels that challenge the autonomous planning of aerial robots. Towards the envisioned scenario of autonomous or semi-autonomous deployment of MAVs with limited Line-of-Sight in subterranean environments, the proposed module acknowledges the existence of junctions by providing crucial information to the autonomy and planning layers of the aerial vehicle. The capability for a junction detection is necessary in the majority of mission scenarios, including unknown area exploration, known area inspection and robot homing missions. The proposed novel method has the ability to feed the image stream from the vehicles on-board forward facing camera in a Convolutional Neural Network (CNN) classification architecture, expressed in four categories: 1) left junction, 2) right junction, 3) left & right junction, and 4) no junction in the local vicinity of the vehicle. The core contribution stems for the incorporation of AlexNet in a transfer learning scheme for detecting multiple branches in a subterranean environment. The validity of the proposed method has been validated through multiple data-sets collected from real underground environments, demonstrating the performance and merits of the proposed module.",https://ieeexplore.ieee.org/document/8926916/,IECON 2019 - 45th Annual Conference of the IEEE Industrial Electronics Society,14-17 Oct. 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS.1997.655065,Visual navigation in an open environment without map,IEEE,Conferences,We describe how a mobile robot controlled only by visual information can retrieve a particular goal location in an open environment. Our model does not need a precise map nor to learn all the possible positions in the environment. The system is a neural architecture inspired from neurobiological studies using the recognition of visual patterns called landmarks. The robot merges this visual information and its azimuth to build a plastic representation of its location. This representation is used to learn the best movement to reach the goal. A simple and fast online learning of a few places located near the goal allows the robot to reach the goal from anywhere in its neighborhood. The system uses only an egocentric representation of the robot environment and presents very high generalization capabilities. We describe an efficient implementation tested on our robot in two real indoor environments. We show the limitations of the model and its possible extensions to create autonomous robots only guided by visual information.,https://ieeexplore.ieee.org/document/655065/,Proceedings of the 1997 IEEE/RSJ International Conference on Intelligent Robot and Systems. Innovative Robotics for Real-World Applications. IROS '97,11-11 Sept. 1997,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCA.2009.5410442,Wheeled mobile robot control using virtual pheromones and neural networks,IEEE,Conferences,"This paper presents a novel approach on the implementation of the concept of ¿virtual pheromones¿ for use in controlling autonomous mobile robots. Rather than being deployed in the environment, the virtual pheromones are stored in a map of the environment maintained and updated by a ¿pheromone server¿. This map acts like a shared memory for all the agents, by means of a radio communication link between each agent and the pheromone server. No direct communication between agents is required. The pheromone server can be implemented on a regular computer, a handheld device, or an embedded controller carried by a leader robot. The technique described is equally applicable for guiding individual robot and robot swarms. The experiments, performed with mobile robot Pioneer 3-DX show that this method allows significant simplification and cost reduction of the autonomous agents. Several possible applications are discussed.",https://ieeexplore.ieee.org/document/5410442/,2009 IEEE International Conference on Control and Automation,9-11 Dec. 2009,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICPR48806.2021.9413066,Yolo+FPN: 2D and 3D Fused Object Detection With an RGB-D Camera,IEEE,Conferences,"In this paper we propose a new deep neural network system, called Yolo+FPN, which fuses both 2D and 3D object detection algorithms to achieve better real-time object detection results and faster inference speed, to be used on real robots. Finding an optimized fusion strategy to efficiently combine 3D object detection with 2D detection information is useful and challenging for both indoor and outdoor robots. In order to satisfy real-time requirements, a trade-off between accuracy and efficiency is needed. We not only have improved training and test accuracies and lower mean losses on the KITTI object detection benchmark comparing with our baseline method, but also achieve competitive average precision on 3D detection of all classes in three levels of difficulty comparing with other state-of-the-art methods. Also, we implemented Yolo+FPN system using an RGB-D camera, and compared the speed of object detection using different GPUs. For the real implementation of both indoor and outdoor scenes, we focus on person detection, which is the most challenging and important among the three classes.",https://ieeexplore.ieee.org/document/9413066/,2020 25th International Conference on Pattern Recognition (ICPR),10-15 Jan. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICAS.2009.2,[Title page iii],IEEE,Conferences,The following topics are dealt with: real-time chain-structured synchronous dataflow; memory requirement formal determination; linear singular descriptor differential system; execution-optimized paths; greedy strategy; load trend evaluation; self-managed P2P streaming; context-aware ambient assisted living application; self-adaptive distributed model; autonomic systems; wireless sensor networks; topology control; learning based method; self-recovery method; mobile data sharing; heterogeneous QoS resource manager; component-based self-healing; NGN mobility; interactive user activity; .NET Windows service agent technology; agent based Web browser; resource-definition policies; autonomic computing; autonomic system administration; automatic database performance tuning; knowledge management; adaptive reinforcement learning; VoIP services; autonomic RSS: distributed virtual reality simulations; virtual machines resources allocation; multi-lier distributed systems; network I/O extensibility; virtual keyboards; self-configuring smart homes; legged underwater vehicles; particle filters; reusable semantic components; multi-agent systems; fixed-wing unmanned aerial vehicles; fuzzy inference system; robot swarms; mobile robots; optimization architecture; autonomous unmanned helicopter landing system design; heterogeneous multi-database environments; autonomic software license management system; Web server crashes prediction; laser range finder; video quality; wireless networks; ITU-T G.1030; open IMS core; context-aware data mining methodology; supply chain finance cooperative systems; autonomous pervasive environments; distributed generic stress tool; dynamic adaptive systems; multisensory media effects and user preference.,https://ieeexplore.ieee.org/document/4976566/,2009 Fifth International Conference on Autonomic and Autonomous Systems,20-25 April 2009,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/RCAR47638.2019.9043946,libSmart: an Open-Source Tool for Simple Integration of Deep Learning into Intelligent Robotic Systems,IEEE,Conferences,"Intelligent robotic systems can be empowered by advanced deep learning techniques. Robotic operations such as object recognition are well investigated by researchers involved in machine learning. However, these solutions have often led to ad-hoc implementation in experimental settings. Less reported is systematic implementation of deep learning models in industrial robots. The lack of standard implementation platforms has impeded widespread use of deep learning modules in industrial robots. It is of great importance to have development platforms that can coordinate several deep learning modules of a complex system. In this paper, a scalable deep-learning friendly robot task organization system named libSmart is introduced. Similar to ROS, the architecture of the proposed system allows users to plug and play various devices but the proposed architecture is also highly compatible with deep learning modules. Specifically, the deployment of deep learning models is handled using a novel data graph method with distributed computing. In this way, the computationally expensive training and inferencing processes of deep learning models can be handled with isolated accelerating hardware to reduce the overall system latency. Successful implementation of simultaneous object recognition and pose estimation by an industrial robot has been presented as a case study. The proposed system is open source for all users to build their own intelligent systems with customized deep-learning models. (https://github.com/RustIron/libSmart.git).",https://ieeexplore.ieee.org/document/9043946/,2019 IEEE International Conference on Real-time Computing and Robotics (RCAR),4-9 Aug. 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/TAMD.2011.2112766,A Biologically Inspired Architecture for an Autonomous and Social Robot,IEEE,Journals,"Lately, lots of effort has been put into the construction of robots able to live among humans. This fact has favored the development of personal or social robots, which are expected to behave in a natural way. This implies that these robots could meet certain requirements, for example, to be able to decide their own actions (autonomy), to be able to make deliberative plans (reasoning), or to be able to have an emotional behavior in order to facilitate human-robot interaction. In this paper, the authors present a bioinspired control architecture for an autonomous and social robot, which tries to accomplish some of these features. In order to develop this new architecture, authors have used as a base a prior hybrid control architecture (AD) that is also biologically inspired. Nevertheless, in the later, the task to be accomplished at each moment is determined by a fix sequence processed by the Main Sequencer. Therefore, the main sequencer of the architecture coordinates the previously programmed sequence of skills that must be executed. In the new architecture, the main sequencer is substituted by a decision making system based on drives, motivations, emotions, and self-learning, which decides the proper action at every moment according to robot's state. Consequently, the robot improves its autonomy since the added decision making system will determine the goal and consequently the skills to be executed. A basic version of this new architecture has been implemented on a real robotic platform. Some experiments are shown at the end of the paper.",https://ieeexplore.ieee.org/document/5711644/,IEEE Transactions on Autonomous Mental Development,Sept. 2011,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/OJCAS.2022.3174632,A Low-Rank CNN Architecture for Real-Time Semantic Segmentation in Visual SLAM Applications,IEEE,Journals,"Real-time semantic segmentation on embedded devices has recently enjoyed significant gain in popularity, due to the increasing interest in smart vehicles and smart robots. In particular, with the emergence of autonomous driving, low latency and computation-intensive operations lead to new challenges for vehicles and robots, such as excessive computing power and energy consumption. The aim of this paper is to address semantic segmentation, one of the most critical tasks for the perception of the environment, and its implementation in a low power core, by preserving the required performance of accuracy and low complexity. To reach this goal a low-rank convolutional neural network (CNN) architecture for real-time semantic segmentation is proposed. The main contributions of this paper are: <i>i)</i> a tensor decomposition technique has been applied to the kernel of a generic convolutional layer, <i>ii)</i> three versions of an optimized architecture, that combines UNet and ResNet models, have been derived to explore the trade-off between model complexity and accuracy, <i>iii)</i> the low-rank CNN architectures have been implemented in a Raspberry Pi 4 and NVIDIA Jetson Nano 2 GB embedded platforms, as severe benchmarks to meet the low-power, low-cost requirements, and in the high-cost GPU NVIDIA Tesla P100 PCIe 16 GB to meet the best performance.",https://ieeexplore.ieee.org/document/9773325/,IEEE Open Journal of Circuits and Systems,2022,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ACCESS.2021.3124386,A Multiple Pheromone Communication System for Swarm Intelligence,IEEE,Journals,"Pheromones are chemical substances essential for communication among social insects. In the application of swarm intelligence to real micro mobile robots, the deployment of a single virtual pheromone has emerged recently as a powerful real-time method for indirect communication. However, these studies usually exploit only one kind of pheromones in their task, neglecting the crucial fact that in the world of real insects, multiple pheromones play important roles in shaping stigmergic behaviors such as foraging or nest building. To explore the multiple pheromones mechanism which enable robots to solve complex collective tasks efficiently, we introduce an artificial multiple pheromone system (ColCOS $\Phi $ ) to support swarm intelligence research by enabling multiple robots to deploy and react to multiple pheromones simultaneously. The proposed system ColCOS $\Phi $  uses optical signals to emulate different evaporating chemical substances i.e. pheromones. These emulated pheromones are represented by trails displayed on a wide LCD display screen positioned horizontally, on which multiple miniature robots can move freely. The color sensors beneath the robots can detect and identify lingering “pheromones” on the screen. Meanwhile, the release of any pheromone from each robot is enabled by monitoring its positional information over time with an overhead camera. No other communication methods apart from virtual pheromones are employed in this system. Two case studies have been carried out which have verified the feasibility and effectiveness of the proposed system in achieving complex swarm tasks as empowered by multiple pheromones. This novel platform is a timely and powerful tool for research into swarm intelligence.",https://ieeexplore.ieee.org/document/9594791/,IEEE Access,2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/TCIAIG.2012.2228483,A Neurally Controlled Computer Game Avatar With Humanlike Behavior,IEEE,Journals,"This paper describes the NeuroBot system, which uses a global workspace architecture, implemented in spiking neurons, to control an avatar within the Unreal Tournament 2004 (UT2004) computer game. This system is designed to display humanlike behavior within UT2004, which provides a good environment for comparing human and embodied AI behavior without the cost and difficulty of full humanoid robots. Using a biologically inspired approach, the architecture is loosely based on theories about the high-level control circuits in the brain, and it is the first neural implementation of a global workspace that has been embodied in a complex dynamic real-time environment. NeuroBot's humanlike behavior was tested by competing in the 2011 BotPrize competition, in which human judges play UT2004 and rate the humanness of other avatars that are controlled by a human or a bot. NeuroBot came a close second, achieving a humanness rating of 36%, while the most human human reached 67%. We also developed a humanness metric that combines a number of statistical measures of an avatar's behavior into a single number. In our experiments with this metric, NeuroBot was rated as 33% human, and the most human human achieved 73%.",https://ieeexplore.ieee.org/document/6357232/,IEEE Transactions on Computational Intelligence and AI in Games,March 2013,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ACCESS.2021.3105102,A Novel Maximin-Based Multi-Objective Evolutionary Algorithm Using One-by-One Update Scheme for Multi-Robot Scheduling Optimization,IEEE,Journals,"With the continuous development of E-commerce, warehouse logistics is also facing emerging challenges, including more batches of orders and shorter order processing cycles. When more orders need to be processed simultaneously, some existing task scheduling methods may not be able to give a suitable plan, which delays order processing and reduces the efficiency of the warehouse. Therefore, the intelligent warehouse system that uses autonomous robots for automated storage and intelligent order scheduling is becoming mainstream. Based on this concept, we propose a multi-robot cooperative scheduling system in the intelligent warehouse. The aim of the multi-robot cooperative scheduling system of the intelligent storage is to drive many robots in an intelligent warehouse to perform the distributed tasks in an optimal (e.g., time-saving and energy-conserved) way. In this paper, we propose a multi-robot cooperative task scheduling model in the intelligent warehouse. For this model, we design a maximin-based multi-objective algorithm, which uses a one-by-one update scheme to select individuals. In this algorithm, two indicators are devised to discriminate the equivalent individuals with the same maximin fitness value in the environmental selection process. The results on benchmark test suite show that our algorithm is indeed a useful optimizer. Then it is applied to settle the multi-robot scheduling problem in the intelligence warehouse. Simulation experiment results demonstrate the efficiency of the proposed algorithm on the real-world scheduling problem.",https://ieeexplore.ieee.org/document/9514575/,IEEE Access,2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/OJITS.2020.3027146,A Plausibility-Based Fault Detection Method for High-Level Fusion Perception Systems,IEEE,Journals,"Trustworthy environment perception is the fundamental basis for the safe deployment of automated agents such as self-driving vehicles or intelligent robots. The problem remains that such trust is notoriously difficult to guarantee in the presence of systematic faults, e.g., non-traceable errors caused by machine learning functions. One way to tackle this issue without making rather specific assumptions about the perception process is plausibility checking. Similar to the reasoning of human intuition, the final outcome of a complex black-box procedure is verified against given expectations of an object's behavior. In this article, we apply and evaluate collaborative, sensor-generic plausibility checking as a mean to detect empirical perception faults from their statistical fingerprints. Our real use case is next-generation automated driving that uses a roadside sensor infrastructure for perception augmentation, represented here by test scenarios at a German highway and a city intersection. The plausibilization analysis is integrated naturally in the object fusion process, and helps to diagnose known and possibly yet unknown faults in distributed sensing systems.",https://ieeexplore.ieee.org/document/9207739/,IEEE Open Journal of Intelligent Transportation Systems,2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/TSMC.2019.2917034,A Real-Time Robotic Grasping Approach With Oriented Anchor Box,IEEE,Journals,"Grasping is an essential skill for robots to interact with humans and the environment. In this paper, we build a vision-based, robust, and real-time robotic grasping approach with fully convolutional neural network. The main component of our approach is a grasp detection network with oriented anchor boxes as detection priors. Because the orientation of detected grasps is significant, which determines the rotation angle configuration of the gripper, we propose the orientation anchor box mechanism to regress grasp angle based on predefined assumption instead of classification or regression without any priors. With oriented anchor boxes, the grasps can be predicted more accurately and efficiently. Besides, to accelerate the network training and further improve the performance of angle regression, angle matching is proposed during training instead of Jaccard index matching. Fivefold cross-validation results demonstrate that our proposed algorithm achieves an accuracy of 98.8% and 97.8% in image-wise split and object-wise split, respectively, and the speed of our detection algorithm is 67 frames per second (FPS) with GTX 1080Ti, outperforming all the current state-of-the-art grasp detection algorithms on Cornell Dataset both in speed and accuracy. Robotic experiments demonstrate the robustness and generalization ability in unseen objects and real-world environment, with the average success rate of 90.0% and 84.2% of familiar things and unseen things, respectively, on Baxter robot platform.",https://ieeexplore.ieee.org/document/8734882/,"IEEE Transactions on Systems, Man, and Cybernetics: Systems",May 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/TMRB.2022.3155278,A Spiking Neural Network Mimics the Oculomotor System to Control a Biomimetic Robotic Head Without Learning on a Neuromorphic Hardware,IEEE,Journals,"Facilitated by the emergence of neuromorphic hardware, neuromorphic algorithms mimic the brain&#x2019;s asynchronous computation to improve energy efficiency, low latency, and robustness, which are crucial for a wide variety of real-time robotic applications. However, the limited on-chip learning abilities hinder the applicability of neuromorphic computing to real-world robotic tasks. Biomimetism can overcome this limitation by complementing or replacing training with the knowledge of the brain&#x2019;s connectome associated with the targeted behavior. By drawing inspiration from the human oculomotor network, we designed a spiking neural network (SNN) that tracked visual targets in real-time. We deployed the biomimetic controller on Intel&#x2019;s Loihi neuromorphic processor to control an in-house robotic head. The robot&#x2019;s behavior resembled the smooth pursuit and saccadic eye movements observed in humans, while the SNN on Loihi exhibited similar performance to a CPU-run PID controller. Interestingly, this behavior emerged from the SNN without training, which places the biomimetic design as an alternative to the energy- and data-greedy learning-based methods. This work reinforces our on-going efforts to devise energy-efficient autonomous robots that mimic the robustness and versatility of their biological counterparts.",https://ieeexplore.ieee.org/document/9722907/,IEEE Transactions on Medical Robotics and Bionics,May 2022,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ACCESS.2022.3157822,A Universal LiDAR SLAM Accelerator System on Low-Cost FPGA,IEEE,Journals,"LiDAR (Light Detection and Ranging) SLAM (Simultaneous Localization and Mapping) serves as a basis for indoor cleaning, navigation, and many other useful applications in both industry and household. From a series of LiDAR scans, it constructs an accurate, globally consistent model of the environment and estimates a robot position inside it. SLAM is inherently computationally intensive; it is a challenging problem to realize a fast and reliable SLAM system on mobile robots with a limited processing capability. To overcome such hurdles, in this paper, we propose a universal, low-power, and resource-efficient accelerator design for 2D LiDAR SLAM targeting resource-limited FPGAs. As scan matching is at the heart of SLAM, the proposed accelerator consists of dedicated scan matching cores on the programmable logic part, and provides software interfaces to facilitate the use. Our accelerator can be integrated to various SLAM methods including the ROS (Robot Operating System)-based ones, and users can switch to a different method without modifying and re-synthesizing the logic part. We integrate the accelerator into three widely-used methods, i.e., scan matching, particle filter, and graph-based SLAM. We evaluate the design in terms of resource utilization, speed, and quality of output results using real-world datasets. Experiment results on a Pynq-Z2 board demonstrate that our design accelerates scan matching and loop-closure detection tasks by up to  $14.84\times$  and  $18.92\times$ , yielding  $4.67\times$ ,  $4.00\times$ , and  $4.06\times$  overall performance improvement in the above methods, respectively. Our design enables the real-time performance while consuming only 2.4W and maintaining accuracy, which is comparable to the software counterparts and even the state-of-the-art methods.",https://ieeexplore.ieee.org/document/9730869/,IEEE Access,2022,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/TASE.2020.3032075,A Virtual Mechanism Approach for Exploiting Functional Redundancy in Finishing Operations,IEEE,Journals,"We propose a new approach to programming by the demonstration of finishing operations. Such operations can be carried out by industrial robots in multiple ways because an industrial robot is typically functionally redundant with respect to a finishing task. In the proposed system, a human expert demonstrates a finishing operation, and the demonstrated motion is recorded in the Cartesian space. The robot’s kinematic model is augmented with a virtual mechanism, which is defined according to the applied finishing tool. This way, the kinematic model is expanded with additional degrees of freedom that can be exploited to compute the optimal joint space motion of the robot without altering the essential aspects of the Cartesian space task execution as demonstrated by the human expert. Finishing operations, such as polishing and grinding, occur in contact with the treated workpiece. Since information about the contact point position is needed to control the robot during the operation, we have developed a novel approach for accurate estimation of contact points using the measured forces and torques. Finally, we applied iterative learning control to refine the demonstrated operations and compensate for inaccurate calibration and different dynamics of the robot and human demonstrator. The proposed method was verified on real robots and real polishing and grinding tasks. Note to Practitioners—This work was motivated by the need for automation of finishing operations, such as polishing and grinding, on contemporary industrial robots. Existing approaches are both too complex and too time-consuming to be applied in flexible and small-scale production, which often requires the frequent deployment of new applications. Our approach is based on programming by demonstration and enables the programming of finishing operations also for users who are not specialists in robot programming. Programming by demonstration is especially useful for teaching finishing operations because it enables the transfer of expert knowledge about finishing skills to robots without providing lengthy task descriptions or manual coding. Besides the human demonstration of the desired operation, the proposed approach also requires the availability of the kinematic model for the machine tool applied to carry out the finishing operation. We provide several practical examples of grinding and polishing tools and how to integrate them into our approach. Another feature of the proposed system is that user demonstrations of finishing operations can be transferred between different combinations of robots and machine tools.",https://ieeexplore.ieee.org/document/9246671/,IEEE Transactions on Automation Science and Engineering,Oct. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/TMECH.2015.2396114,Adaptive Neural Network Control of a Compact Bionic Handling Arm,IEEE,Journals,"In this paper, autonomous control problem of a class of bionic continuum robots named “Compact Bionic Handling Arm” (CBHA) is addressed. These robots can reproduce biological behaviors of trunks, tentacles, or snakes. The modeling problem associated with continuum robots includes nonlinearities, structured and unstructured uncertainties, and the hyperredundancy. In addition to these problems, the CBHA comprises the hysteresis behavior of its actuators and a memory phenomenon related to its structure made of polyamide materials. These undesirable effects make it difficult to design a control system based on quantitative models of the CBHA. Thus, two subcontrollers are proposed in this paper. One, encapsulated in the other, and both implemented in real time allow controlling of the CBHA's end-effector position. The first subcontroller controls the CBHA's kinematics based on a distal supervised learning scheme. The second subcontroller controls the CBHA's kinetics based on an adaptive neural control. These subcontrollers allow a better assessment of the stability of the control architecture while ensuring the convergence of Cartesian errors. The obtained experimental results using a CBHA robot show an accurate tracking of the CBHA's end-effector position.",https://ieeexplore.ieee.org/document/7057549/,IEEE/ASME Transactions on Mechatronics,Dec. 2015,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/LRA.2020.2967296,Aerial Single-View Depth Completion With Image-Guided Uncertainty Estimation,IEEE,Journals,"On the pursuit of autonomous flying robots, the scientific community has been developing onboard real-time algorithms for localisation, mapping and planning. Despite recent progress, the available solutions still lack accuracy and robustness in many aspects. While mapping for autonomous cars had a substantive boost using deep-learning techniques to enhance LIDAR measurements using image-based depth completion, the large viewpoint variations experienced by aerial vehicles are still posing major challenges for learning-based mapping approaches. In this letter, we propose a depth completion and uncertainty estimation approach that better handles the challenges of aerial platforms, such as large viewpoint and depth variations, and limited computing resources. The core of our method is a novel compact network that performs both depth completion and confidence estimation using an image-guided approach. Real-time performance onboard a GPU suitable for small flying robots is achieved by sharing deep features between both tasks. Experiments demonstrate that our network outperforms the state-of-the-art in depth completion and uncertainty estimation for single-view methods on mobile GPUs. We further present a new photorealistic aerial depth completion dataset that exhibits more challenging depth completion scenarios than the established indoor and car driving datasets. The dataset includes an open-source, visual-inertial UAV simulator for photo-realistic data generation. Our results show that our network trained on this dataset can be directly deployed on real-world outdoor aerial public datasets without fine-tuning or style transfer.",https://ieeexplore.ieee.org/document/8962227/,IEEE Robotics and Automation Letters,April 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/OJCS.2020.3001839,An Instrument for Remote Kissing and Engineering Measurement of Its Communication Effects Including Modified Turing Test,IEEE,Journals,"Various communication systems have been developed to integrate the haptic channel in digital communication. Future directions of such haptic technologies are moving towards realistic virtual reality applications and human-robot social interaction. With the digitisation of touch, robots equipped with touch sensors and actuators can communicate with humans on a more emotional and intimate level, such as sharing a hug or kiss just like humans do. This paper presents the design guideline, implementation and evaluations of a novel haptic kissing machine for smart phones - the Kissenger machine. The key novelties and contributions of the paper are: (i) A novel haptic kissing device for mobile phones, which uses dynamic perpendicular force stimulation to transmit realistic sensations of kissing in order to enhance intimacy and emotional connection of digital communication; (ii) Extensive evaluations of the Kissenger machine, including a lab experiment that compares mediated kissing with Kissenger to real kissing, a unique haptic Turing test that involves the first academic study of human-machine kiss, and a field study of the effects of Kissenger on long distance relationships. The first experiment showed that mediated kissing with Kissenger elicited similar ratings for pleasure, arousal and user experience as real kissing. Experiment 2 confirmed our hypothesis that interrogators have a higher chance of winning the Imitation Game (Turing test) when Kissenger is used during the game. Results from experiment 3 showed that long relationship couples who used Kissenger for a week experienced increased relationship satisfaction and decreased perceived stress.",https://ieeexplore.ieee.org/document/9119758/,IEEE Open Journal of the Computer Society,2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/LRA.2022.3142439,Anytime 3D Object Reconstruction Using Multi-Modal Variational Autoencoder,IEEE,Journals,"For effective human-robot teaming, it is important for the robots to be able to share their visual perception with the human operators. In a harsh remote collaboration setting, data compression techniques such as autoencoder can be utilized to obtain and transmit the data in terms of latent variables in a compact form. In addition, to ensure real-time runtime performance even under unstable environments, an anytime estimation approach is desired that can reconstruct the full contents from incomplete information. In this context, we propose a method for imputation of latent variables whose elements are partially lost. To achieve the anytime property with only a few dimensions of variables, exploiting prior information of the category-level is essential. A prior distribution used in variational autoencoders is simply assumed to be isotropic Gaussian regardless of the labels of each training datapoint. This type of flattened prior makes it difficult to perform imputation from the category-level distributions. We overcome this limitation by exploiting a category-specific multi-modal prior distribution in the latent space. The missing elements of the partially transferred data can be sampled, by finding a specific modal according to the remaining elements. Since the method is designed to use partial elements for anytime estimation, it can also be applied for data over-compression. Based on the experiments on the ModelNet and Pascal3D datasets, the proposed approach shows consistently superior performance over autoencoder and variational autoencoder up to 70% data loss. The software is open source and is available from our repository1.",https://ieeexplore.ieee.org/document/9681277/,IEEE Robotics and Automation Letters,April 2022,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/TASE.2004.840071,Artificial-intelligence approach for biomedical sample characterization using Raman spectroscopy,IEEE,Journals,"An artificial-intelligence approach is proposed to differentiate various biomedical samples via Raman spectroscopy technology to obtain accurate medical diagnosis and decision making. The complete process consists of noise filtering, fluorescence identification, optimization and elimination, spectral normalization, multivariate statistical analysis, and data clustering, as well as the final decision making. Numerous modeling, intelligent control, and system-identification schemes have been employed. By means of fuzzy control, genetic algorithms, and principal component analysis (PCA), as well as system identification, a systematic intelligent-control approach is formulated, which is capable of classifying diversified biomedical samples. Raman spectra are weak signals whose features are sensitive to a variety of noises, which have to be reduced to an acceptable level. Fuzzy logic has been known to interpret uncertainty, imprecision, and vague phenomena. Thus, a fuzzy controller is used for noise filtering. On the other hand, background fluorescence acts as a secondary intensity component within a raw Raman spectrograph, so its spectral baseline should be determined. By removing background fluorescence, intrinsic Raman spectrum can be extracted in consequence. To optimize this detrend process, genetic algorithms have been implemented for baseline-function global optimization by selecting an optimal combination of individual spectroscopic functions. Normalization is performed by standard normal variate (SNV) afterwards to compensate for scattering effects. Normalized intrinsic spectra can be used for sample differentiation, where the PCA approach distinguishes some signatures from different samples in terms of dominant principal components. Eventually, various principal components are accumulated for clustering using scatter plots. The long-term objective of this intelligent-control approach is to create a real-time technique for sample analysis, using a Raman spectrometer directly mounted at the end-effectors of medical robots, which is to enhance the robotic surgery.",https://ieeexplore.ieee.org/document/1381368/,IEEE Transactions on Automation Science and Engineering,Jan. 2005,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/TASE.2021.3077689,Attention-Driven Active Sensing With Hybrid Neural Network for Environmental Field Mapping,IEEE,Journals,"In environmental monitoring programs, mobile robots have been widely deployed for remote sensing, with the end objective of monitoring and mapping out environmental fields. Complex characteristics and correlations in natural phenomena make it challenging to establish a reliable framework for mobile sensing and field mapping. Furthermore, constraints of onboard resources will limit the ability of mobile robots to cover a large area. This article focuses on the active sensing problem in environmental field mapping and particularly exploits the use of intrinsic interactions among multivariate spatiotemporal data. A novel deep neural network of a hybrid CNN-RNN model is employed to learn the monitored multivariate spatiotemporal field. Specifically, a set of attention mechanisms is designed and embedded in the network, which is able to adaptively capture parameterwise dependencies among the monitored heterogeneous parameters and spatial correlations in geolocations of a surveyed field. The weights of inferred attention facilitate explicit interpretation of the driving parameters and geolocations. Some subregions of interest in the surveyed field are specified by their spatial attention distribution and are actively sensed by following the proposed coverage path planner. Experiments are carried out using a real-world dataset with multisource environmental imagery from a remote sensing program. Experimental results are obtained, which demonstrate the superior mapping performance of the proposed systematical methodology compared to baseline methods. Furthermore, the proposed model is able to quantitatively reveal the driving monitored parameters and geolocations in a regression process. <i>Note to Practitioners</i>&#x2014;This article was motivated by the need for a practical and systematic approach for reconstruction and planning to execute robotic active sensing (AS) in environmental field mapping. Field robotic applications are not maneuverable in comparison with indoor scenarios due to severe conflict between the need for long execution endurance in the field and the very limited onboard resources. Traditional AS planners normally use statistical model-based informative metrics, which may lead to model misspecification in real-world phenomena. The developed framework in this article yields a novel attention-driven metric to guide AS and mapping. It relies on an attention-based hybrid neural network that reveals the driving variables in terms of the heterogeneities and complexities in a natural environment. The high-priority regions are maximized in a coverage path depending on the inferred spatial attention distribution while maintaining the travel cost of the sensing robots within an available energy budget. Experiments using a remote sensing dataset validate the reliable performance of the proposed framework, in environmental field mapping.",https://ieeexplore.ieee.org/document/9431671/,IEEE Transactions on Automation Science and Engineering,July 2022,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/TCDS.2019.2928820,BND*-DDQN: Learn to Steer Autonomously Through Deep Reinforcement Learning,IEEE,Journals,"It is vital for mobile robots to achieve safe autonomous steering in various changing environments. In this paper, a novel end-to-end network architecture is proposed for mobile robots to learn steering autonomously through deep reinforcement learning. Specifically, two sets of feature representations are first extracted from the depth inputs through two different input streams. The acquired features are then merged together to derive both linear and angular actions simultaneously. Moreover, a new action selection strategy is also introduced to achieve motion filtering by taking the consistency in angular velocity into account. Besides, in addition to the extrinsic rewards, the intrinsic bonuses are also adopted during training to improve the exploration capability. Furthermore, it is worth noting the proposed model is readily transferable from the simple virtual training environment to much more complicated real-world scenarios so that no further fine-tuning is required for real deployment. Compared to the existing methods, the proposed method demonstrates significant superiority in terms of average reward, convergence speed, success rate, and generalization capability. In addition, it exhibits outstanding performance in various cluttered real-world environments containing both static and dynamic obstacles. A video of our experiments can be found at https://youtu.be/19jrQGG1oCU.",https://ieeexplore.ieee.org/document/8764461/,IEEE Transactions on Cognitive and Developmental Systems,June 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/LRA.2021.3062303,Bi-Directional Domain Adaptation for Sim2Real Transfer of Embodied Navigation Agents,IEEE,Journals,"Deep reinforcement learning models are notoriously data hungry, yet real-world data is expensive and time consuming to obtain. The solution that many have turned to is to use simulation for training before deploying the robot in a real environment. Simulation offers the ability to train large numbers of robots in parallel, and offers an abundance of data. However, no simulation is perfect, and robots trained solely in simulation fail to generalize to the real-world, resulting in a “sim-vs-real gap”. How can we overcome the trade-off between the abundance of less accurate, artificial data from simulators and the scarcity of reliable, real-world data? In this letter, we propose Bi-directional Domain Adaptation (BDA), a novel approach to bridge the sim-vs-real gap in both directions- real2sim to bridge the visual domain gap, and sim2real to bridge the dynamics domain gap. We demonstrate the benefits of BDA on the task of PointGoal Navigation. BDA with only 5 k real-world (state, action, next-state) samples matches the performance of a policy fine-tuned with ~ 600 k samples, resulting in a speed-up of ~ 120×.",https://ieeexplore.ieee.org/document/9363564/,IEEE Robotics and Automation Letters,April 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/LRA.2021.3111416,Binarized P-Network: Deep Reinforcement Learning of Robot Control from Raw Images on FPGA,IEEE,Journals,"This letter explores a deep reinforcement learning (DRL) approach for designing image-based control for edge robots to be implemented on Field Programmable Gate Arrays (FPGAs). Although FPGAs are more power-efficient than CPUs and GPUs, a typical DRL method cannot be applied since they are composed of many Logic Blocks (LBs) for high-speed logical operations but low-speed real-number operations. To cope with this problem, we propose a novel DRL algorithm called Binarized P-Network (BPN), which learns image-input control policies using Binarized Convolutional Neural Networks (BCNNs). To alleviate the instability of reinforcement learning caused by a BCNN with low function approximation accuracy, our BPN adopts a robust value update scheme called Conservative Value Iteration, which is tolerant of function approximation errors. We confirmed the BPN's effectiveness through applications to a visual tracking task in simulation and real-robot experiments with FPGA.",https://ieeexplore.ieee.org/document/9534708/,IEEE Robotics and Automation Letters,Oct. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/70.88080,CONDOR: an architecture for controlling the Utah-MIT dexterous hand,IEEE,Journals,"The authors describe a fully implemented computational architecture (CONDOR) that controls the Utah-MIT dexterous hand and other complex robots. The architecture derives its power from the highly efficient real-time environment provided for its control processors, coupled with a development host that allows flexible program development. By mapping the memory of a dedicated group of processors into the address space of a host computer, efficient sharing of system resources between them is possible. The software is characterized by a few simple design concepts but provides the facilities out of which more powerful utilities such as a multiprocessor pseudo-terminal emulator, a transparent and fast file server, and a flexible symbolic debugger could be constructed.<>",https://ieeexplore.ieee.org/document/88080/,IEEE Transactions on Robotics and Automation,Oct. 1989,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ACCESS.2018.2845855,Color Transfer Pulse-Coupled Neural Networks for Underwater Robotic Visual Systems,IEEE,Journals,"With rapid developments in cloud computing, artificial intelligence, and robotic systems, ever more complex tasks, such as space and ocean exploration, are being implemented by intelligent robots. Here, we propose an underwater image enhancement scheme for robotic visual systems. The proposed algorithm and its implementation enhances and outputs an image captured by an underwater robot in real time. In this scheme, pulse-coupled neural network (PCNN)-based image enhancement and color transfer algorithms are combined to enhance the underwater image. To avoid color imbalance in the underwater image and enhance details while suppressing noise, color correction is first carried out on the underwater image before converting it into the hue-saturation-intensity domain and enhancing it by PCNN. The enhanced result improves the color and contrast of the source image and enhances the details and edges of darker regions. Experiments are performed on real world data to demonstrate the effectiveness of the proposed scheme.",https://ieeexplore.ieee.org/document/8377996/,IEEE Access,2018,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/TEVC.2010.2058120,Compact Differential Evolution,IEEE,Journals,"This paper proposes the compact differential evolution (cDE) algorithm. cDE, like other compact evolutionary algorithms, does not process a population of solutions but its statistic description which evolves similarly to all the evolutionary algorithms. In addition, cDE employs the mutation and crossover typical of differential evolution (DE) thus reproducing its search logic. Unlike other compact evolutionary algorithms, in cDE, the survivor selection scheme of DE can be straightforwardly encoded. One important feature of the proposed cDE algorithm is the capability of efficiently performing an optimization process despite a limited memory requirement. This fact makes the cDE algorithm suitable for hardware contexts characterized by small computational power such as micro-controllers and commercial robots. In addition, due to its nature cDE uses an implicit randomization of the offspring generation which corrects and improves the DE search logic. An extensive numerical setup has been implemented in order to prove the viability of cDE and test its performance with respect to other modern compact evolutionary algorithms and state-of-the-art population-based DE algorithms. Test results show that cDE outperforms on a regular basis its corresponding population-based DE variant. Experiments have been repeated for four different mutation schemes. In addition cDE outperforms other modern compact algorithms and displays a competitive performance with respect to state-of-the-art population-based algorithms employing a DE logic. Finally, the cDE is applied to a challenging experimental case study regarding the on-line training of a nonlinear neural-network-based controller for a precise positioning system subject to changes of payload. The main peculiarity of this control application is that the control software is not implemented into a computer connected to the control system but directly on the micro-controller. Both numerical results on the test functions and experimental results on the real-world problem are very promising and allow us to think that cDE and future developments can be an efficient option for optimization in hardware environments characterized by limited memory.",https://ieeexplore.ieee.org/document/5675671/,IEEE Transactions on Evolutionary Computation,Feb. 2011,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/LRA.2022.3145517,Competence-Aware Path Planning Via Introspective Perception,IEEE,Journals,"Robots deployed in the real world over extendedperiods of time need to reason about unexpected failures, learn to predict them, and to proactively take actions to avoid future failures. Existing approaches for competence-aware planning are either model-based, requiring explicit enumeration of known failure sources, or purely statistical, using state- and location-specific failure statistics to infer competence. We instead propose a structured model-free approach to competence-aware planning by reasoning about plan execution failures due to errors in perception, without requiring a priori enumeration of failure sources or requiring location-specific failure statistics. We introduce competence-aware path planning via introspective perception (CPIP), a Bayesian framework to iteratively learn and exploit task-level competence in novel deployment environments. CPIP factorizes the competence-aware planning problem into two components. First, perception errors are learned in a model-free and location-agnostic setting via introspective perception prior to deployment in novel environments. Second, during actual deployments, the prediction of task-level failures is learned in a context-aware setting. Experiments in a simulation show that the proposed CPIP approach outperforms the frequentist baseline in multiple mobile robot tasks, and is further validated via real robot experiments in environments with perceptually challenging obstacles and terrain.",https://ieeexplore.ieee.org/document/9691872/,IEEE Robotics and Automation Letters,April 2022,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/TCSI.2021.3101296,"Dadu-Eye: A 5.3 TOPS/W, 30 fps/1080p High Accuracy Stereo Vision Accelerator",IEEE,Journals,"Stereo vision is widely deployed on robots and drones to enable depth estimation at a low cost. The combination of lightweight deep neural network (DNN) and cost volumes algorithm is proved to possess the advantages of both high depth estimation accuracy and speed. However, currently there is no accelerator architecture compatible with both efficient DNN inference and cost generation algorithms such as stereo matching. This work proposes a stereo vision accelerator called Dadu-eye, dedicated to real-time processing of high-resolution image streams. The proposed architecture adopts a pipelined hardware design with the techniques of operation approximation and scheduling-level optimization. First, a cost estimation block is designed to generate cost volumes from both luminance and color information. Second, a super pipelined multiplication and accumulation array with a row scan-based fused-layer convolution scheduling is proposed to perform the encoding and decoding neural network efficiently. Finally, an optical flow block is designed and cooperates with the array to approximately predict half of the frames’ depth to achieve real-time (30fps) processing on 1080p view. Based on the SMIC 40 nm CMOS process, this stereo vision accelerator achieves 5.3 TOPS/W power efficiency and significantly reduces 81% off-chip memory access.",https://ieeexplore.ieee.org/document/9511631/,IEEE Transactions on Circuits and Systems I: Regular Papers,Oct. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ACCESS.2022.3159785,Deep Reinforcement Learning Based Dynamic Proportional-Integral (PI) Gain Auto-Tuning Method for a Robot Driver System,IEEE,Journals,"To meet the growing trend of stringent fuel economy regulations, automakers around the world are designing modules such as engines, motors, transmissions and batteries to be as efficient as possible. In order to verify the effect of these designs on the overall fuel efficiency of the vehicle, the vehicle equipped with each module is placed on the chassis dynamometer, driven to follow the target vehicle speed, and actual fuel efficiency is measured. These tests are traditionally performed by human operators, but are now being replaced by robots (physical or software) to ensure the accuracy and reliability of test results. Although the conventionally proposed proportional integral (PI)-based controller has a simple structure and is easy to implement, it requires the process of finding the optimal gain whenever the test conditions such as vehicle or drive cycle change, which is difficult and time consuming. In this study, we propose a proportional integral controller gain adjustment algorithm using deep reinforcement learning. The reinforcement learning agent learns to dynamically modify the PI gain value of the acceleration/deceleration pedal to better follow the target vehicle in a simulation environment. The perturbation is used in each training episode to reduce the difference between the simulation and real testing environment. Upon completion of the training process, the trained agent performs an adjustment process that generates a reference gain table. We then use this reference gain table to perform a real test. The performance of the proposed system was evaluated using Hyundai Tucson HEV (NX4) on an AVL chassis dynamometer. We also compared the performance of our proposed algorithm to traditional fuzzy logic-based PI controllers. The obtained experimental results show that the proposed control system achieved a performance improvement of aounrd 46.8% compared to the conventional PI control system in terms of root mean square error.",https://ieeexplore.ieee.org/document/9737020/,IEEE Access,2022,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/TSMC.2019.2958094,Design and Analysis of a Human–Machine Interaction System for Researching Human’s Dynamic Emotion,IEEE,Journals,"Dynamic emotion is typically used to facilitate human–machine interactions. Conversational data from social media contain a considerable amount of useful information, and such data are the foundation for researching dynamic and artificial emotion. At present, most human–machine interaction systems focus on the complexity and accuracy of the dialog but neglect the emotional characteristics of the speaker. When generating a dialog considering the emotional personality of the interlocutor, controlling, and guiding the dialog to a specified direction are essential. This article presents a system for studying dynamic emotions in human-computer interaction from the perspective of emotional transfer and guidance. Based on the emotional state of the interlocutor and the distribution of emotional transfer, the process of emotional transfer is simulated and sampled, and the sequence of emotional guidance is generated. In this system, two algorithms are proposed. A generative Markov chain Monte Carlo (GEN-MCMC) algorithm is proposed to generate a variety of emotional transfer sequences that fit the talke’s personality dynamically based on the real-world dialog. Further, a guiding MCMC (GUI-MCMC) algorithm-based GEN-MCMC is proposed to generate the emotional guiding sequences. The generated emotional sequences by GEN-MCMC were evaluated in two aspects: 1) consistency and 2) diversity. The experimental results show that the GEN-MCMC algorithm performs better than the general sequence generation algorithm in terms of consistency and diversity in generating emotional states. The GUI-MCMC was able to generate a proper stimulus sequence when given the first and target emotions. An emotional stimulus sequence can simulate the emotional transfer of the interlocutor in the process of dialogue, and give the observer appropriate reference to guide and control the emotions of dialogue. The experimental results show that the proposed system can effectively model the dynamic emotion in emotional transfer and guidance, which can be further used to build chat robots, intelligent assistants, and human–machine interaction systems. The models can also be used for emotional induction and enhance the feel-good or feel-terrible factor in human–machine communication applications, such as medical treatment of mental diseases, interrogation, and psychological attack and defense.",https://ieeexplore.ieee.org/document/8943333/,"IEEE Transactions on Systems, Man, and Cybernetics: Systems",Oct. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ACCESS.2021.3135448,Device Light Fingerprints Identification Using MCU-Based Deep Learning Approach,IEEE,Journals,"We introduce device identification using the light fingerprint by a MCU-based deep learning approach. At first, we observe that minor differences exist for individual components of lighting equipment. The corresponding difference produces a unique phenomenon in the frequency spectrum. Therefore, we adopt deep learning approaches for developing a mobile phone light fingerprint identification system and implementing it on a low-cost microcontroller platform. The screen light of the mobile phone is analyzed to obtain the features of unique light fingerprints. We utilize the convolutional neural network, the improved multi-class greedy autoencoder and variational autoencoder with domain adaptation techniques to develop the identification algorithm. Finally, the Bayesian optimization technique is used to optimize the hyper-parameters of models for implementing in the microprocessor. The corresponding comparisons are introduced to demonstrate the performance. The multi-class greedy autoencoder algorithm produces results with an overall accuracy rate and abnormal sample detection rate of 99.67&#x0025; and 99.85&#x0025;, respectively. Only a single model needs to be added or deleted for updating new authentication data and this does not affect the identification ability of all models. This results in greater flexibility in real-life applications and potential for expansion to other fields, such as smart buildings and automated robots.",https://ieeexplore.ieee.org/document/9648329/,IEEE Access,2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/TCDS.2016.2562121,Emergence of Altruistic Behavior Through the Minimization of Prediction Error,IEEE,Journals,"The emergence of altruistic behavior in infants fosters their social development and supports their involvement in our society. Altruistic tendencies, intended to benefit others with no apparent rewards, are also very useful for social robots that are designed to be used in our households. Yet, to make robots capable of learning how to help others as infants do, it is important to understand the mechanisms and motives responsible for the development of altruistic behavior. Further, understanding the mechanisms behind the early development of pro-social behavior would be a great contribution to the field of developmental psychology. To these ends, we hypothesize that infants from 14 months of age help others to minimize the differences between predicted actions and observations, that is, to minimize prediction errors. To evaluate our hypothesis, we created a computational model based on psychological studies and implemented it in real and simulated robots. Our system first acquires its own sensory-motor representation by interacting with its environment. Then, using its experience, the system recognizes and predicts others' actions and uses this prediction to estimate a prediction error. Our experiments demonstrated that our robots could spontaneously generate helping behaviors by being motivated by the minimization of prediction errors.",https://ieeexplore.ieee.org/document/7479539/,IEEE Transactions on Cognitive and Developmental Systems,Sept. 2016,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/LRA.2022.3142433,Enabling Low-Cost Full Surface Tactile Skin for Human Robot Interaction,IEEE,Journals,"Realizing full coverage, low-maintenance, and low-cost tactile skin is a <i>de facto</i> design dream since the invention of robots. It ensures safety and enables collaborative work protocols for human robot interactions (HRI). The on-robot tactile capability is realized by deploying an array of external sensors or inferring from proprioceptive information that comes with the robot, such as motor torque. However, these methods may be cumbersome, introduce extra management cost, expensive, lack real-world robustness, or require special robot designs. In this letter, we present <i>SonicSkin</i>, a low-cost (&#x0024;2) and easy to deploy system that localizes the on-robot human touch and estimates the touch pressure without actually attaching sensors at potential touch locations. The system requires only a single pair of piezoelectric transducers (<i>i.e.</i> one transmitter and one receiver) attached on the target robot and turns the robot itself into a versatile sensor. We present a set of novel algorithms to progressively address the unique challenges posed by our system design. We put together an end-to-end <i>SonicSkin</i> system on a Jaco robot arm that runs in real-time, and conducted an extensive real-world study including 57019 actual evaluation datapoints under various challenging conditions from 12 human subjects. <i>SonicSkin</i> achieves less than 2 cm localization error for 96.4&#x0025; of touches, with more than 96.7&#x0025; cross-correlation similarity between the predicted touch pressure and the ground truth touch pressure.",https://ieeexplore.ieee.org/document/9681158/,IEEE Robotics and Automation Letters,April 2022,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/LRA.2020.3012951,End-to-End Tactile Feedback Loop: From Soft Sensor Skin Over Deep GRU-Autoencoders to Tactile Stimulation,IEEE,Journals,"Tactile feedback is a key sensory channel that contributes to our ability to perform precise manipulations. In this regard, sensor skin provides robots with the sense of touch making them increasingly capable of dexterous object manipulation. However, in applications like teleoperation, the complex sensory input of an infinite number of different textures must be projected to the human user's skin in a meaningful manner. In addressing this issue, a deep gated recurrent unit-based autoencoder (GRU-AE) that captured the perceptual dimensions of tactile textures in latent space was deployed to implicitly understand unseen textures. The expression of unknown textures in this latent space allowed for the definition of a control law to effectively drive tactile displays and to convey tactile feedback in a psycho-physically meaningful manner. The approach was experimentally verified by evaluating the prediction performance of the GRU-AE on seen and unseen data that were gathered during active tactile exploration of objects commonly encountered in daily living. A user study on a custom-made tactile display was conducted in which real tactile perceptions in response to active tactile object exploration were compared to the emulated tactile feedback using the proposed tactile feedback loop. The results suggest that the deep GRU-AE for tactile display control offers an effective and intuitive method for efficient end-to-end tactile feedback during active tactile texture exploration.",https://ieeexplore.ieee.org/document/9152113/,IEEE Robotics and Automation Letters,Oct. 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ACCESS.2021.3101397,Energy-Efficient Edge-Fog-Cloud Architecture for IoT-Based Smart Agriculture Environment,IEEE,Journals,"The current agriculture systems compete to take advantage of industry advanced technologies, including the internet of things (IoT), cloud/fog/edge computing, artificial intelligence, and agricultural robots to monitor, track, analyze and process various functions and services in real-time. Additionally, these technologies can make the agricultural processes smarter and more cost-efficient by using automated systems and eliminating any human interventions, hence enhancing agricultural production to meet future expectations. Although the current agriculture systems that adopt the traditional cloud-based architecture have provided powerful computing infrastructure to distributed IoT sensors. However, the cost of energy consumption associated with transferring heterogeneous data over the multiple network tiers to process, analyze and store the sensor's information in the cloud has created a huge load on information and communication infrastructure. Besides, the energy consumed by cloud data centers has an environmental impact associated with using non-clean fuels, which usually release carbon emissions (CO2) to produce electricity. Thus, to tackle these issues, we propose a new integrated edge-fog-cloud architectural paradigm that promises to enhance the energy-efficient of smart agriculture systems and corresponding carbon emissions. This architecture allows data collection from several sensors to process and analyze the agriculture data that require real-time operation (e.g., weather temperature, soil moisture, soil acidity, irrigation, etc.) in several layers (edge, fog, and cloud). Thus, the real-time processing could be held by the edge and fog layers to reduce the load on the cloud layer, which will help to enhance the overall energy consumption and process the agriculture applications/services efficiently. Mathematical modeling is conducted using mixed-integer linear programming (MILP) for a smart agriculture environment, where the proposed architecture is implemented, and results are analyzed and compared to the traditional implementation. According to the results of thousands of agriculture sensors, the proposed architecture outperforms the traditional cloud-based architecture in terms of reducing the overall energy consumption by 36% and the carbon emissions by 43%. In addition to these achievements, the results show that our proposed architecture can reduce network traffic by up to 86%, which can reduce network congestion. Finally, we develop a heuristic algorithm to validate and mimic the presented approach, and it shows comparable results to the MILP model.",https://ieeexplore.ieee.org/document/9502114/,IEEE Access,2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/TNNLS.2018.2830119,Enhanced Robot Speech Recognition Using Biomimetic Binaural Sound Source Localization,IEEE,Journals,"Inspired by the behavior of humans talking in noisy environments, we propose an embodied embedded cognition approach to improve automatic speech recognition (ASR) systems for robots in challenging environments, such as with ego noise, using binaural sound source localization (SSL). The approach is verified by measuring the impact of SSL with a humanoid robot head on the performance of an ASR system. More specifically, a robot orients itself toward the angle where the signal-to-noise ratio (SNR) of speech is maximized for one microphone before doing an ASR task. First, a spiking neural network inspired by the midbrain auditory system based on our previous work is applied to calculate the sound signal angle. Then, a feedforward neural network is used to handle high levels of ego noise and reverberation in the signal. Finally, the sound signal is fed into an ASR system. For ASR, we use a system developed by our group and compare its performance with and without the support from SSL. We test our SSL and ASR systems on two humanoid platforms with different structural and material properties. With our approach we halve the sentence error rate with respect to the common downmixing of both channels. Surprisingly, the ASR performance is more than two times better when the angle between the humanoid head and the sound source allows sound waves to be reflected most intensely from the pinna to the ear microphone, rather than when sound waves arrive perpendicularly to the membrane.",https://ieeexplore.ieee.org/document/8371531/,IEEE Transactions on Neural Networks and Learning Systems,Jan. 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/TSMCB.2010.2073702,Experimental Analysis of Mobile-Robot Teleoperation via Shared Impedance Control,IEEE,Journals,"In this paper, Internet-based teleoperation of mobile robots for obstacle avoidance is analyzed. A shared impedance-control scheme is presented, and the results of an experimental study for the evaluation of the effects of different teleoperation parameters are reported. In the experimental study, the effects of time delay, operator training, image-display alternatives (virtual model versus real images), viewpoint, and force-reflection method were studied. For this purpose, several hypotheses were formulated and tested through the experiments using the introduced quantitative and qualitative measures. A fuzzy force-reflection controller is also proposed as an alternative force-reflection technique, and its performance is compared with a conventional proportional-derivative-type force-reflection method. The experimental scheme was implemented using MATLAB XPC Target and Simulink. The results could serve as guidelines in the design of teleoperation systems for obstacle avoidance and could also provide directions for further investigations.",https://ieeexplore.ieee.org/document/5598539/,"IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics)",April 2011,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ACCESS.2022.3145969,FastMDE: A Fast CNN Architecture for Monocular Depth Estimation at High Resolution,IEEE,Journals,"A depth map helps robots and autonomous vehicles (AVs) visualize the three-dimensional world to navigate and localize neighboring obstacles. However, it is difficult to develop a deep learning model that can estimate the depth map from a single image in real-time. This study proposes a fast monocular depth estimation model named FastMDE by optimizing the deep convolutional neural network according to the encoder-decoder architecture. The decoder needs to obtain partial and semantic feature maps from the encoding phase to improve the depth estimation accuracy. Therefore, we designed FastMDE with two effective strategies. The first one involved redesigning the skip connection with the features of the squeeze-excitation module to obtain partial and semantic feature maps of the encoding phase. The second strategy involved redesigning the decoder by using the fusion dense block to permit the usage of high-resolution features that were learned earlier in the network before upsampling. The proposed FastMDE model utilizes only 4.1 M parameters, which is much lesser than the parameters utilized by state-of-art models. Thus, FastDME has a higher accuracy and lower latency than previous models. This study also demonstrates that MDE can leverage deep neural networks in real-time (i.e., 30 fps) with the Linux embedded board Nvidia Jetson Xavier NX. The model can facilitate the development and applications with superior performances and easy deployment on an embedded platform.",https://ieeexplore.ieee.org/document/9690863/,IEEE Access,2022,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/TOH.2017.2753233,Functional Contour-following via Haptic Perception and Reinforcement Learning,IEEE,Journals,"Many tasks involve the fine manipulation of objects despite limited visual feedback. In such scenarios, tactile and proprioceptive feedback can be leveraged for task completion. We present an approach for real-time haptic perception and decision-making for a haptics-driven, functional contour-following task: the closure of a ziplock bag. This task is challenging for robots because the bag is deformable, transparent, and visually occluded by artificial fingertip sensors that are also compliant. A deep neural net classifier was trained to estimate the state of a zipper within a robot's pinch grasp. A Contextual Multi-Armed Bandit (C-MAB) reinforcement learning algorithm was implemented to maximize cumulative rewards by balancing exploration versus exploitation of the state-action space. The C-MAB learner outperformed a benchmark Q-learner by more efficiently exploring the state-action space while learning a hard-to-code task. The learned C-MAB policy was tested with novel ziplock bag scenarios and contours (wire, rope). Importantly, this work contributes to the development of reinforcement learning approaches that account for limited resources such as hardware life and researcher time. As robots are used to perform complex, physically interactive tasks in unstructured or unmodeled environments, it becomes important to develop methods that enable efficient and effective learning with physical testbeds.",https://ieeexplore.ieee.org/document/8039205/,IEEE Transactions on Haptics,1 Jan.-March 2018,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/TSMCB.2011.2167679,Fuzzy Sliding-Mode Formation Control for Multirobot Systems: Design and Implementation,IEEE,Journals,"This paper mainly addresses the decentralized formation problems for multiple robots, where a fuzzy sliding-mode formation controller (FSMFC) is proposed. The directed networks of dynamic agents with external disturbances and system uncertainties are discussed in consensus problems. To perform a formation control and to guarantee system robustness, a novel formation algorithm combining the concepts of graph theory and fuzzy sliding-model control is presented. According to the communication topology, formation stability conditions can be determined so that an FSMFC can be derived. By Lyapunov stability theorem, not only the system stability can be guaranteed, but the desired formation pattern of a multirobot system can be also achieved. Simulation results are provided to demonstrate the effectiveness of the provided control scheme. Finally, an experimental setup for the e-puck multirobot system is built. Compared to first-order formation algorithm and fuzzy neural network formation algorithm, it shows that real-time experimental results empirically support the promising performance of desire.",https://ieeexplore.ieee.org/document/6046141/,"IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics)",April 2012,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/LRA.2022.3186635,Generalizable Task Planning Through Representation Pretraining,IEEE,Journals,"The ability to plan for multi-step manipulation tasks in unseen situations is crucial for future home robots. But collecting sufficient experience data for end-to-end learning is often infeasible in the real world, as deploying robots in many environments can be prohibitively expensive. On the other hand, large-scale scene understanding datasets contain diverse and rich semantic and geometric information. But how to leverage such information for manipulation remains an open problem. In this letter, we propose a learning-to-plan method that can generalize to new object instances by leveraging object-level representations extracted from a synthetic scene understanding dataset. We evaluate our method with a suite of challenging multi-step manipulation tasks inspired by household activities (Srivastava, <i>et al.</i>, 2022) and show that our model achieves measurably better success rate than state-of-the-art end-to-end approaches.",https://ieeexplore.ieee.org/document/9808141/,IEEE Robotics and Automation Letters,July 2022,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ACCESS.2020.3043662,Ground-Level Mapping and Navigating for Agriculture Based on IoT and Computer Vision,IEEE,Journals,"Autonomous agricultural systems are a promising solution to bridge the gap between labor shortage for agriculture tasks and the continuing needs for increasing productivity in agriculture. Automated mapping and navigation system will be a cornerstone of most autonomous agricultural system. Accordingly, we propose a ground-level mapping and navigating system based on computer vision technology (Mesh Simultaneous Localization and Mapping algorithm, Mesh-SLAM) and Internet of Things (IoT), to generate a 3D farm map on both the edge side and cloud. The innovation of this system includes three layers as sub-systems that are 1) ground-level robot vehicles' layer for conducting frames collection only with a monocular camera, 2) edge node layer for image feature data edge computing and communication, and 3) cloud layer for general management and deep computing. High efficiency and speed of mapping stage are enabled by making the robot vehicles directly stream continuous frames to their corresponding edge node. Then each edge node, that coordinate a certain range of robots, applies a new Mesh-SLAM frame by frame, whose core is reconstructing the features map by a mesh-based algorithm with scalable units and reduce the feature data size by a filtering algorithm. Additionally, the cloud-computing allows comprehensive arrangement and heavily deep computing. The system is scalable to larger-scale fields and more complex environment by taking advantage of dynamically distributing the computation power to edges. Our evaluation indicates that: 1) this Mesh-SLAM algorithm outperforms in mapping and localization precision, accuracy, and yield prediction error (resolution at centimeter); and 2) The scalability and flexibility of the IoT architecture make the system modularized, easy adding/removing new functional modules or IoT sensors. We conclude the trade-off between cost and performance widely augments the feasibility and practical implementation of this system in real farms.",https://ieeexplore.ieee.org/document/9288741/,IEEE Access,2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/TCDS.2020.2991470,Guest Editorial Special Issue on Multidisciplinary Perspectives on Mechanisms of Language Learning,IEEE,Journals,"Humans excel at learning from other humans [item 1) in the Appendix). Language facilitates such learning and plays a crucial role. On the one hand, it coordinates our interactions and cooperative behavior [item 2) in the Appendix). On the other hand, language and communication allow to directly incorporate novel knowledge gathered from social interaction or from reading [item 3) in the Appendix). It has been a long-standing goal of artificial intelligence to leverage such communicative abilities [item 4) in the Appendix) for robots and smart software agents which would, at first, simplify our interactions with machines through a more human-like way of coordinating between humans and robots. But furthermore, this would allow us to easily teach these machines, increasing their abilities and skills further which would allow them to become real partners and companions.",https://ieeexplore.ieee.org/document/9114350/,IEEE Transactions on Cognitive and Developmental Systems,June 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/3477.499796,Hidden state and reinforcement learning with instance-based state identification,IEEE,Journals,"Real robots with real sensors are not omniscient. When a robot's next course of action depends on information that is hidden from the sensors because of problems such as occlusion, restricted range, bounded field of view and limited attention, we say the robot suffers from the hidden state problem. State identification techniques use history information to uncover hidden state. Some previous approaches to encoding history include: finite state machines, recurrent neural networks and genetic programming with indexed memory. A chief disadvantage of all these techniques is their long training time. This paper presents instance-based state identification, a new approach to reinforcement learning with state identification that learns with much fewer training steps. Noting that learning with history and learning in continuous spaces both share the property that they begin without knowing the granularity of the state space, the approach applies instance-based (or ""memory-based"") learning to history sequences-instead of recording instances in a continuous geometrical space, we record instances in action-percept-reward sequence space. The first implementation of this approach, called Nearest Sequence Memory, learns with an order of magnitude fewer steps than several previous approaches.",https://ieeexplore.ieee.org/document/499796/,"IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics)",June 1996,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ACCESS.2021.3063782,Hierarchical Decomposed-Objective Model Predictive Control for Autonomous Casualty Extraction,IEEE,Journals,"In recent years, several robots have been developed and deployed to perform casualty extraction tasks. However, the majority of these robots are overly complex, and require teleoperation via either a skilled operator or a specialised device, and often the operator must be present at the scene to navigate safely around the casualty. Instead, improving the autonomy of such robots can reduce the reliance on expert operators and potentially unstable communication systems, while still extracting the casualty in a safe manner. There are several stages in the casualty extraction procedure, from navigating to the location of the emergency, safely approaching and loading the casualty, to finally navigating back to the medical assistance location. In this paper, we propose a Hierarchical Decomposed-Objective based Model Predictive Control (HiDO-MPC) method for safely approaching and manoeuvring around the casualty. We implement this controller on ResQbot — a proof-of-concept mobile rescue robot we previously developed — capable of safely rescuing an injured person lying on the ground, i.e. performing the casualty extraction procedure. HiDO-MPC achieves the desired casualty extraction behaviour by decomposing the main objective into multiple sub-objectives with a hierarchical structure. At every time step, the controller evaluates this hierarchical decomposed objective and generates the optimal control decision. We have conducted a number of experiments both in simulation and using the real robot to evaluate the proposed method’s performance, and compare it with baseline approaches. The results demonstrate that the proposed control strategy gives significantly better results than baseline approaches in terms of accuracy, robustness, and execution time, when applied to casualty extraction scenarios.",https://ieeexplore.ieee.org/document/9369351/,IEEE Access,2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/LRA.2016.2517213,Human-Inspired Neurorobotic System for Classifying Surface Textures by Touch,IEEE,Journals,"Giving robots the ability to classify surface textures requires appropriate sensors and algorithms. Inspired by the biology of human tactile perception, we implement a neurorobotic texture classifier with a recurrent spiking neural network, using a novel semisupervised approach for classifying dynamic stimuli. Input to the network is supplied by accelerometers mounted on a robotic arm. The sensor data are encoded by a heterogeneous population of neurons, modeled to match the spiking activity of mechanoreceptor cells. This activity is convolved by a hidden layer using bandpass filters to extract nonlinear frequency information from the spike trains. The resulting high-dimensional feature representation is then continuously classified using a neurally implemented support vector machine. We demonstrate that our system classifies 18 metal surface textures scanned in two opposite directions at a constant velocity. We also demonstrate that our approach significantly improves upon a baseline model that does not use the described feature extraction. This method can be performed in real-time using neuromorphic hardware, and can be extended to other applications that process dynamic stimuli online.",https://ieeexplore.ieee.org/document/7378880/,IEEE Robotics and Automation Letters,Jan. 2016,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ACCESS.2019.2894524,Hybrid Stochastic Exploration Using Grey Wolf Optimizer and Coordinated Multi-Robot Exploration Algorithms,IEEE,Journals,"Multi-robot exploration is a search of uncertainty in restricted space seeking to build a finite map by a group of robots. It has the main task to distribute the search assignments among robots in real time. In this paper, we proposed a stochastic optimization for multi-robot exploration that mimics the coordinated predatory behavior of grey wolves via simulation. Here, the robot movement is computed by the combined deterministic and metaheuristic techniques. It uses the Coordinated Multi-Robot Exploration and GreyWolf Optimizer algorithms as a new method called the hybrid stochastic exploration. Initially, the deterministic cost and utility determine the precedence of adjacent cells around a robot. Then, the stochastic optimization improves the overall solution. It implies that the robots evaluate the environment by the deterministic approach and move on using the metaheuristic algorithm. The proposed hybrid method was implemented on simple and complex maps and compared with the Coordinated Multi-Robot Exploration algorithm. The simulation results show that the stochastic optimization enhances the deterministic approach to completely explore and map out the areas.",https://ieeexplore.ieee.org/document/8631022/,IEEE Access,2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/TIE.2018.2864707,Incremental Updating Multirobot Formation Using Nonlinear Model Predictive Control Method With General Projection Neural Network,IEEE,Journals,"In this paper, an incremental centralized formation system is developed for controlling the multirobot formation with joining robots, and a nonlinear model predictive control (NMPC) method is implemented as the controller. The incremental updating method is used to update the system's state in real time, when there is a new robot joining during the formation process. Then, an NMPC approach is developed to reformulate the formation system into a convex nonlinear minimization problem, which can be further transformed into a quadratic programming (QP) with constraints. Then, a general projection neural network (GPNN) is implemented for solving this QP problem online to get the optimal inputs. In the end, two examples of incremental multirobot formation are demonstrated to verify the effectiveness of this method.",https://ieeexplore.ieee.org/document/8437254/,IEEE Transactions on Industrial Electronics,June 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/LRA.2021.3060712,Joint Plant Instance Detection and Leaf Count Estimation for In-Field Plant Phenotyping,IEEE,Journals,"Precision management of agricultural fields as well as plant breeding are central factors for keeping yields high and to provide food, feed, and fiber for our society. A key element in breeding trials but also for targeted management actions is to analyze the growth state of individual plants objectively and at a large scale. In this letter, we address the problem of analyzing crops in real agricultural fields based on camera data recorded with mobile robots and to derive information about the plant development, e.g., to monitor phenotypic traits such as growth stage. We propose a novel single-stage object detection approach that localizes crops and weeds in the field. At the same time, it detects plant-specific leaf keypoints intending to estimate leaf count at a plant level, which is a key trait for classifying the growth stage. We implemented and thoroughly tested our approach on real sugar beet fields. As our experiments show, it performs the required detections and shows superior performance with respect to a state-of-the-art two-stage approach based on Mask R-CNN.",https://ieeexplore.ieee.org/document/9359471/,IEEE Robotics and Automation Letters,April 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/TRO.2021.3137751,"Kimera-Multi: Robust, Distributed, Dense Metric-Semantic SLAM for Multi-Robot Systems",IEEE,Journals,"Multi-robot simultaneous localization and mapping (SLAM) is a crucial capability to obtain timely situational awareness over large areas. Real-world applications demand multi-robot SLAM systems to be robust to perceptual aliasing and to operate under limited communication bandwidth; moreover, it is desirable for these systems to capture semantic information to enable high-level decision-making and spatial artificial intelligence. This article presents <inline-formula><tex-math notation=""LaTeX"">$ \mathsf{{Kimera-Multi}} $</tex-math></inline-formula>, a multi-robot system that: 1) is robust and capable of identifying and rejecting incorrect inter- and intrarobot loop closures resulting from perceptual aliasing; 2) is fully distributed and only relies on local (peer-to-peer) communication to achieve distributed localization and mapping; and 3) builds a globally consistent metric-semantic 3-D mesh model of the environment in real time, where faces of the mesh are annotated with semantic labels. <inline-formula><tex-math notation=""LaTeX"">$ \mathsf{{Kimera-Multi}} $</tex-math></inline-formula> is implemented by a team of robots equipped with visual-inertial sensors. Each robot builds a local trajectory estimate and a local mesh using <inline-formula><tex-math notation=""LaTeX"">$ \mathsf{{Kimera}} $</tex-math></inline-formula>. When communication is available, robots initiate a distributed place recognition and robust pose graph optimization protocol based on a distributed graduated nonconvexity algorithm. The proposed protocol allows the robots to improve their local trajectory estimates by leveraging inter-robot loop closures while being robust to outliers. Finally, each robot uses its improved trajectory estimate to correct the local mesh using mesh deformation techniques. We demonstrate <inline-formula><tex-math notation=""LaTeX"">$ \mathsf{{Kimera-Multi}} $</tex-math></inline-formula> in photo-realistic simulations, SLAM benchmarking datasets, and challenging outdoor datasets collected using ground robots. Both real and simulated experiments involve long trajectories (e.g., up to 800 m per robot). The experiments show that <inline-formula><tex-math notation=""LaTeX"">$ \mathsf{{Kimera-Multi}} $</tex-math></inline-formula>: 1) outperforms the state of the art in terms of robustness and accuracy; 2) achieves estimation errors comparable to a centralized SLAM system while being fully distributed; 3) is parsimonious in terms of communication bandwidth; 4) produces accurate metric-semantic 3-D meshes; and 5) is modular and can also be used for standard 3-D reconstruction (i.e., without semantic labels) or for trajectory estimation (i.e., without reconstructing a 3-D mesh).",https://ieeexplore.ieee.org/document/9686955/,IEEE Transactions on Robotics,Aug. 2022,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/TRO.2021.3098239,Learning for Attitude Holding of a Robotic Fish: An End-to-End Approach With Sim-to-Real Transfer,IEEE,Journals,"Controlling biomimetic underwater robots in unknown flow fields remains a challenge due to the strong nonlinearity of the fluid. This article investigates the attitude holding task of a robotic fish swimming in reality. Such a typical sensing-based control task requires the fish to keep a desired angle of attack in an unknown and even varied incoming flow. To this end, we propose a learning-based approach by using a deep neural network directly maps the raw data of sensors equipped on the robot to the continuous control signals in an end-to-end manner. First, based on experimental data of the physical robot, we construct a data-driven simulation environment including three modules of dynamic, sensor, and control. The dynamic and sensor modules are established to model the dynamics of the fish and to generate its sensors&#x2019; data, based on which a deep reinforcement learning (DRL) algorithm in the control module is trained to get a control policy. Then, we directly deploy the trained policy to a physical robotic fish for attitude holding task. Experimental results demonstrate the robustness and effectiveness of the DRL policy and, thus, verify the success of our approach to achieving sim-to-real transfer.",https://ieeexplore.ieee.org/document/9509293/,IEEE Transactions on Robotics,April 2022,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/TIM.2022.3165828,Leveraging Tactile Sensors for Low Latency Embedded Smart Hands for Prosthetic and Robotic Applications,IEEE,Journals,"Tactile sensing is a crucial perception mode for robots and human amputees in need of controlling a prosthetic device. Today, robotic and prosthetic systems are still missing the important feature of accurate tactile sensing. This lack is mainly due to the fact that the existing tactile technologies have limited spatial and temporal resolution and are either expensive or not scalable. In this article, we present the design and implementation of a hardware&#x2013;software embedded system called SmartHand. It is specifically designed to enable the acquisition and real-time processing of high-resolution tactile information from a hand-shaped multisensor array for prosthetic and robotic applications. During data collection, our system can deliver a high throughput of 100 frames per second, which is <inline-formula> <tex-math notation=""LaTeX"">$13.7\times $ </tex-math></inline-formula> higher than previous related work. This has allowed the collection of a new tactile dataset consisting of 340 000 frames while interacting with 16 objects from everyday life during five different sessions. Together with the empty hand, the dataset presents a total of 17 classes. We propose a compact yet accurate convolutional neural network that requires one order of magnitude less memory and <inline-formula> <tex-math notation=""LaTeX"">$15.6\times $ </tex-math></inline-formula> fewer computations compared with related work without degrading classification accuracy. The top-1 and top-3 cross-validation accuracies on the collected dataset are, respectively, 98.86&#x0025; and 99.83&#x0025;. We further analyze the intersession variability and obtain the best top-3 leave-one-out-validation accuracy of 77.84&#x0025;. We deploy the trained model on a high-performance ARM Cortex-M7 microcontroller achieving an inference time of only 100 ms minimizing the response latency. The overall measured power consumption is 505 mW. Finally, we fabricate a new control sensor and perform additional experiments to provide analyses on sensor degradation and slip detection. This work is a step forward in giving robotic and prosthetic devices a sense of touch by demonstrating the practicality of a smart embedded system that uses a scalable tactile sensor with embedded tiny machine learning.",https://ieeexplore.ieee.org/document/9751605/,IEEE Transactions on Instrumentation and Measurement,2022,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ACCESS.2020.2970728,LoPECS: A Low-Power Edge Computing System for Real-Time Autonomous Driving Services,IEEE,Journals,"To simultaneously enable multiple autonomous driving services on affordable embedded systems, we designed and implemented LoPECS, a Low-Power Edge Computing System for real-time autonomous robots and vehicles services. The contributions of this paper are three-fold: first, we developed a Heterogeneity-Aware Runtime Layer to fully utilize vehicle's heterogeneous computing resources to fulfill the real-time requirement of autonomous driving applications; second, we developed a vehicle-edge Coordinator to dynamically offload vehicle tasks to edge cloudlet to further optimize user experience in the way of prolonged battery life; third, we successfully integrated these components into LoPECS system and implemented it on Nvidia Jetson TX1. To the best of our knowledge, this is the first complete edge computing system in a production autonomous vehicle. Our implementation on Nvidia Jetson demonstrated that it could successfully support multiple autonomous driving services with only 11 W of power consumption, and hence proves the effectiveness of the proposed LoPECS system.",https://ieeexplore.ieee.org/document/8977507/,IEEE Access,2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/JIOT.2022.3161050,Machine Learning in Real-Time Internet of Things (IoT) Systems: A Survey,IEEE,Journals,"Over the last decade, machine learning (ML) and deep learning (DL) algorithms have significantly evolved and been employed in diverse applications, such as computer vision, natural language processing, automated speech recognition, etc. Real-time safety-critical embedded and Internet of Things (IoT) systems, such as autonomous driving systems, UAVs, drones, security robots, etc., heavily rely on ML/DL-based technologies, accelerated with the improvement of hardware technologies. The cost of a deadline (required time constraint) missed by ML/DL algorithms would be catastrophic in these safety-critical systems. However, ML/DL algorithm-based applications have more concerns about accuracy than strict time requirements. Accordingly, researchers from the real-time systems (RTSs) community address the strict timing requirements of ML/DL technologies to include in RTSs. This article will rigorously explore the state-of-the-art results emphasizing the strengths and weaknesses in ML/DL-based scheduling techniques, accuracy versus execution time tradeoff policies of ML algorithms, and security and privacy of learning-based algorithms in real-time IoT systems.",https://ieeexplore.ieee.org/document/9739684/,IEEE Internet of Things Journal,"1 June1, 2022",ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/LRA.2020.2972819,Marker-Less Micro Aerial Vehicle Detection and Localization Using Convolutional Neural Networks,IEEE,Journals,"A relative localization system for micro aerial vehicles (MAVs), which is able to work without any markers or other specialized equipment, is presented in this letter. The system utilizes images from an onboard camera to detect nearby MAVs using a convolutional neural network. When compared to traditional computer vision-based relative localization systems, this approach removes the need for specialized markers to be placed on the MAVs, saving weight and space, while also enabling localization of noncooperating robots. The system is designed and implemented to run online, onboard an MAV platform in order to enable relative stabilization of several MAVs in a formation or swarm-like behavior, when operating in a closed feedback loop with the control system of the MAVs. We demonstrate the viability and robustness of the proposed method in real-world experiments. The method was also designed for the purpose of autonomous aerial interception and is a fitting complement to other MAV detection and relative localization methods for this purpose, as is shown in the experiments.",https://ieeexplore.ieee.org/document/8988144/,IEEE Robotics and Automation Letters,April 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/TIE.2007.903993,Multimodal Approach to Human-Face Detection and Tracking,IEEE,Journals,"The constructive need for robots to coexist with humans requires human-machine interaction. It is a challenge to operate these robots in such dynamic environments, which requires continuous decision-making and environment-attribute update in real-time. An autonomous robot guide is well suitable in places such as museums, libraries, schools, hospital, etc. This paper addresses a scenario where a robot tracks and follows a human. A neural network is utilized to learn the skin and nonskin colors. The skin-color probability map is utilized for skin classification and morphology-based preprocessing. Heuristic rule is used for face-ratio analysis and Bayesian cost analysis for label classification. A face-detection module, based on a 2D color model in the and YUV color space, is selected over the traditional skin-color model in a 3D color space. A modified continuously adaptive mean shift tracking mechanism in a 1D hue, saturation, and value color space is developed and implemented onto the mobile robot. In addition to the visual cues, the tracking process considers 16 sonar scan and tactile sensor readings from the robot to generate a robust measure of the person's distance from the robot. The robot thus decides an appropriate action, namely, to follow the human subject and perform obstacle avoidance. The proposed approach is orientation invariant under varying lighting conditions and invariant to natural transformations such as translation, rotation, and scaling. Such a multimodal solution is effective for face detection and tracking.",https://ieeexplore.ieee.org/document/4392479/,IEEE Transactions on Industrial Electronics,March 2008,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ACCESS.2022.3155661,Non-Contact Service Robot Development in Fast-Food Restaurants,IEEE,Journals,"Given the prospect of low birth rate and aging population among developed countries, which further resulted in the shortage of workforces, service robots have been gradually applied to real-life from past academic research. This paper introduces the development of a service robot that is designed for food service in fast-food restaurants with the innovative improvement of mapping, localization, and navigation. Moreover, this research took the initiative of integrating 3D point cloud map and 2D occupancy grid map (OGM) in order to build a PC-OGM. In another word, using the sensory fusion method allows the service robot to adapt to a more complicated environment as well as enhance its positioning. In terms of its navigation function, the adaptive motion controller is refined so the service robot could navigate through narrow aisles smoothly. Finally, friendly contact-free food service robots were evaluated at fast-food restaurants in order to gain feedback from diners and waiters. Their feedback was broken down into 3 categories, availability, reliability, and satisfaction, for further analysis. As COVID-19 assails the world, we also look into future possibilities of food service robot deployment among restaurants to keep food and surface free from the virus.",https://ieeexplore.ieee.org/document/9723055/,IEEE Access,2022,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/LRA.2021.3129136,OCRTOC: A Cloud-Based Competition and Benchmark for Robotic Grasping and Manipulation,IEEE,Journals,"In this paper, we propose a cloud-based benchmark for robotic grasping and manipulation, called the OCRTOC benchmark. The benchmark focuses on the object rearrangement problem, specifically table organization tasks. We provide a set of identical real robot setups and facilitate remote experiments of standardized table organization scenarios in varying difficulties. In this workflow, users upload their solutions to our remote server and their code is executed on the real robot setups and scored automatically. After each execution, the OCRTOC team resets the experimental setup manually. We also provide a simulation environment that researchers can use to develop and test their solutions. With the OCRTOC benchmark, we aim to lower the barrier of conducting reproducible research on robotic grasping and manipulation and accelerate progress in this field. Executing standardized scenarios on identical real robot setups allows us to quantify algorithm performances and achieve fair comparisons. Using this benchmark we held a competition in the 2020 International Conference on Intelligence Robots and Systems (IROS 2020). In total, 59 teams took part in this competition worldwide. We present the results and our observations of the 2020 competition, and discuss our adjustments and improvements for the upcoming OCRTOC 2021 competition. The homepage of the OCRTOC competition is www.ocrtoc.org, and the OCRTOC software package is available at https://github.com/OCRTOC/OCRTOC_software_package.",https://ieeexplore.ieee.org/document/9619915/,IEEE Robotics and Automation Letters,Jan. 2022,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/56.812,On terrain acquisition by a point robot amidst polyhedral obstacles,IEEE,Journals,"The authors consider the problem of terrain model acquisition by a roving point placed in an unknown terrain populated by stationary polyhedral obstacles in two/three dimensions. The motivation for this problem is that after the terrain model is completely acquired, navigation from a source point to a destination point can be achieved along the collision-free paths. This can be done without the usage of sensors by applying the existing techniques for the find-path problem. In the paper, the point robot autonomous machine (PRAM) is used as a simplified abstract model for real-life roving robots. An algorithm is presented that enables PRAM to autonomously acquire the model of an unexplored obstacle terrain composed of an unknown number of polyhedral obstacles in two/three dimensions. In this method, PRAM undertakes a systematic exploration of the obstacle terrain with its sensor that detects all the edges and vertices visible from the present location, and builds the complete obstacle terrain model.<>",https://ieeexplore.ieee.org/document/812/,IEEE Journal on Robotics and Automation,Aug. 1988,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/LRA.2021.3076955,On the Emergence of Whole-Body Strategies From Humanoid Robot Push-Recovery Learning,IEEE,Journals,"Balancing and push-recovery are essential capabilities enabling humanoid robots to solve complex locomotion tasks. In this context, classical control systems tend to be based on simplified physical models and hard-coded strategies. Although successful in specific scenarios, this approach requires demanding tuning of parameters and switching logic between specifically-designed controllers for handling more general perturbations. We apply model-free Deep Reinforcement Learning for training a general and robust humanoid push-recovery policy in a simulation environment. Our method targets high-dimensional whole-body humanoid control and is validated on the iCub humanoid. Reward components incorporating expert knowledge on humanoid control enable fast learning of several robust behaviors by the same policy, spanning the entire body. We validate our method with extensive quantitative analyses in simulation, including out-of-sample tasks which demonstrate policy robustness and generalization, both key requirements towards real-world robot deployment.",https://ieeexplore.ieee.org/document/9420230/,IEEE Robotics and Automation Letters,Oct. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/TNNLS.2014.2354400,Opportunistic Behavior in Motivated Learning Agents,IEEE,Journals,"This paper focuses on the novel motivated learning (ML) scheme and opportunistic behavior of an intelligent agent. It extends previously developed ML to opportunistic behavior in a multitask situation. Our paper describes the virtual world implementation of autonomous opportunistic agents learning in a dynamically changing environment, creating abstract goals, and taking advantage of arising opportunities to improve their performance. An opportunistic agent achieves better results than an agent based on ML only. It does so by minimizing the average value of all need signals rather than a dominating need. This paper applies to the design of autonomous embodied systems (robots) learning in real-time how to operate in a complex environment.",https://ieeexplore.ieee.org/document/6913540/,IEEE Transactions on Neural Networks and Learning Systems,Aug. 2015,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/TASE.2021.3077026,PLC-VIO: Visual&#x2013;Inertial Odometry Based on Point-Line Constraints,IEEE,Journals,"Visual&#x2013;inertial odometry (VIO) is widely studied and used in autonomous robots. This article proposes a novel tightly coupled monocular VIO system based on point-line constraints (PLC-VIO). In the front end, PLC-VIO presents a line segment extraction and merging algorithm based on the EDLines method and achieves real-time feature tracking based on the geometric constraints between feature points and lines. In the back end, PLC-VIO reconstructs new 3-D landmarks of feature lines through points on the line and optimizes the states by minimizing a cost function that combines the preintegrated inertial measurement unit (IMU) error term together with the point and line reprojection error terms in a sliding window optimization framework. A loop closure module is also integrated, which enables relocalization and drift elimination. The corresponding experimental evaluations are conducted using public datasets to validate the effectiveness and robustness of the proposed system, and the results show that PLC-VIO can achieve good performance when compared with other state-of-the-art systems and, at the same time, with no compromise to real-time performance. <i>Note to Practitioners</i>&#x2014;Visual&#x2013;inertial odometry (VIO) can estimate the states of the rigid body (including position, attitude, and velocity) that is widely used in robotic navigation, autonomous driving, virtual reality (VR), and augmented reality (AR). Aiming at the problem of estimating the states of autonomous robots in the GPS-denied environment, this article proposes a novel VIO system based on the point-line constraints (PLC-VIO). PLC-VIO can not only achieve accurate pose estimation for robots due to the introduction of the line features but also make no concession to real-time performance. Furthermore, PLC-VIO can also enrich the texture features of the environment during the 3-D mapping construction. The corresponding experiments are implemented in public datasets to evaluate the effectiveness, efficiency, and robustness of the proposed system. We believe that PLC-VIO can be widely used in robotic navigation and AR/VR fields to provide accurate position and environment information in real time.",https://ieeexplore.ieee.org/document/9431679/,IEEE Transactions on Automation Science and Engineering,July 2022,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/TOH.2020.2975555,Perception of Tactile Directionality via Artificial Fingerpad Deformation and Convolutional Neural Networks,IEEE,Journals,"Humans can perceive tactile directionality with angular perception thresholds of 14-40° via fingerpad skin displacement. Using deformable, artificial tactile sensors, the ability to perceive tactile directionality was developed for a robotic system to aid in object manipulation tasks. Two convolutional neural networks (CNNs) were trained on tactile images created from fingerpad deformation measurements during perturbations to a handheld object. A primary CNN regression model provided a point estimate of tactile directionality over a range of grip forces, perturbation angles, and perturbation speeds. A secondary CNN model provided a variance estimate that was used to determine uncertainty about the point estimate. A 5-fold cross-validation was performed to evaluate model performance. The primary CNN produced tactile directionality point estimates with an error rate of 4.3% for a 20° angular resolution and was benchmarked against an open-source force estimation network. The model was implemented in real-time for interactions with an external agent and the environment with different object shapes and widths. The perception of tactile directionality could be used to enhance the situational awareness of human operators of telerobotic systems and to develop decision-making algorithms for context-appropriate responses by semi-autonomous robots.",https://ieeexplore.ieee.org/document/9007491/,IEEE Transactions on Haptics,Oct.-Dec. 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ACCESS.2019.2900475,Pollution Source Localization Based on Multi-UAV Cooperative Communication,IEEE,Journals,"Harmful gas leakage accidents in chemical plants have occurred from time to time. The application of mobile robots to find odor source has become one of the hottest research topics. Compared to traditional robots, unmanned aerial vehicle (UAV) is more flexible and safer. Therefore, using multi-UAV to solve pollution source tracking is a meaningful study. In this paper, an air pollution source tracking algorithm based on artificial potential field and particle swarm optimization is proposed. The particle swarm optimization algorithm combined with artificial potential field method is used to guide the UAVs to track the plume and avoid the collisions among them. At the same time, adaptive inertia weights are used to help improve the convergence and the searchability of particles. We not only evaluated this algorithm in simulation experiments but also designed a multi-UAV pollution source tracking platform for real-world experiments. The experimental results show that the algorithm can accurately find the pollution source in a short time.",https://ieeexplore.ieee.org/document/8665856/,IEEE Access,2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/TNSRE.2017.2692520,Portable and Reconfigurable Wrist Robot Improves Hand Function for Post-Stroke Subjects,IEEE,Journals,"Rehabilitation robots have become increasingly popular for stroke rehabilitation. However, the high cost of robots hampers their implementation on a large scale. This paper implements the concept of a modular and reconfigurable robot, reducing its cost and size by adopting different therapeutic end effectors for different training movements using a single robot. The challenge is to increase the robot's portability and identify appropriate kinds of modular tools and configurations. Because literature on the effectiveness of this kind of rehabilitation robot is still scarce, this paper presents the design of a portable and reconfigurable rehabilitation robot and describes its use with a group of post-stroke patients for wrist and forearm training. Seven stroke subjects received training using a reconfigurable robot for 30 sessions, lasting 30 min per session. Post-training, statistical analysis showed significant improvement of 3.29 points (16.20%, p = 0.027) on the Fugl-Meyer assessment scale for forearm and wrist components. Significant improvement of active range of motion was detected in both pronation-supination (75.59%, p = 0.018) and wrist flexion-extension (56.12%, p = 0.018) after the training. These preliminary results demonstrate that the developed reconfigurable robot could improve subjects' wrist and forearm movement.",https://ieeexplore.ieee.org/document/7894193/,IEEE Transactions on Neural Systems and Rehabilitation Engineering,Oct. 2017,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ACCESS.2018.2887099,Power- and Time-Aware Deep Learning Inference for Mobile Embedded Devices,IEEE,Journals,"Deep learning is a state-of-the-art approach that provides highly accurate inference for many cyber-physical systems (CPS) such as autonomous cars and robots. Deep learning inference often needs to be performed locally on mobile and embedded devices, rather than in the cloud, to address concerns such as latency, power consumption, and limited bandwidth. However, existing approaches have focused on delivering “best-effort” performance to resource-constrained mobile embedded devices, resulting in unpredictable performance under highly variable environments of CPS. In this paper, we propose a novel deep learning inference runtime, called DeepRT, that supports multiple QoS objectives simultaneously against unpredictable workloads. In DeepRT, the multiple inputs/multiple outputs (MIMO) modeling and control methodology is proposed as a primary tool to support multiple QoS goals including the inference latency and power consumption. DeepRT’s MIMO controller coordinates multiple computing resources, such as CPUs and GPUs, by capturing their close interactions and effects on multiple QoS objectives. We demonstrate the viability of DeepRT’s QoS management architecture by implementing a prototype of DeepRT. The evaluation results demonstrate that, compared with baseline approaches, DeepRT can support the desired inference latency as well as power consumption for various deep learning models in a highly robust manner.",https://ieeexplore.ieee.org/document/8579137/,IEEE Access,2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/TNN.2006.877534,Prune-Able Fuzzy ART Neural Architecture for Robot Map Learning and Navigation in Dynamic Environments,IEEE,Journals,"Mobile robots must be able to build their own maps to navigate in unknown worlds. Expanding a previously proposed method based on the fuzzy ART neural architecture (FARTNA), this paper introduces a new online method for learning maps of unknown dynamic worlds. For this purpose the new Prune-able fuzzy adaptive resonance theory neural architecture (PAFARTNA) is introduced. It extends the FARTNA self-organizing neural network with novel mechanisms that provide important dynamic adaptation capabilities. Relevant PAFARTNA properties are formulated and demonstrated. A method is proposed for the perception of object removals, and then integrated with PAFARTNA. The proposed methods are integrated into a navigation architecture. With the new navigation architecture the mobile robot is able to navigate in changing worlds, and a degree of optimality is maintained, associated to a shortest path planning approach implemented in real-time over the underlying global world model. Experimental results obtained with a Nomad 200 robot are presented demonstrating the feasibility and effectiveness of the proposed methods.",https://ieeexplore.ieee.org/document/1687933/,IEEE Transactions on Neural Networks,Sept. 2006,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ACCESS.2021.3056149,READ-IoT: Reliable Event and Anomaly Detection Framework for the Internet of Things,IEEE,Journals,"Internet of Things (IoT) enables a myriad of applications by interconnecting software to physical objects. The objects range from wireless sensors to robots and include surveillance cameras. The applications are often critical (e.g. physical intrusion detection, fire fighting) and latency-sensitive. On the one hand, such applications rely on specific protocols (e.g. MQTT, COAP) and the network to communicate with the objects under very tight timeframe. On the other hand, anomalies (e.g. communication noise, sensors' failures, security attacks) are likely to occur in open IoT systems and can result by sending false alerts or the failure to properly detect critical events. To address that, IoT systems have to be equipped with anomaly detection processing in addition to the required event detection capability. This is a key feature that enables reliability and efficiency in IoT. However, anomaly detection systems can be themselves object of failures and attacks, and then can easily fall short to accomplish their mission. This paper introduces a Reliable Event and Anomaly Detection Framework for the Internet of Things (READ-IoT for short). The designed framework integrates events and anomalies detection into a single and common system that centralizes the management of both concepts. To enforce its reliability, the system relies on a reputation-aware provisioning of detection capabilities that takes into account the vulnerability of the deployment hosts. As for validation, READ-IoT was implemented and evaluated using two real life applications, i.e. a fire detection and an unauthorized person detection applications. Several scenarios of anomalies and events were conducted using NSL-KDD public dataset, as well as, generated data to simulate routing attacks. The obtained results and performance measurements show the efficiency of READ-IoT in terms of event detection accuracy and real-time processing.",https://ieeexplore.ieee.org/document/9343860/,IEEE Access,2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/LRA.2020.2998414,RILaaS: Robot Inference and Learning as a Service,IEEE,Journals,"Programming robots is complicated due to the lack of `plug-and-play' modules for skill acquisition. Virtualizing deployment of deep learning models can facilitate large-scale use/re-use of off-the-shelf functional behaviors. Deploying deep learning models on robots entails real-time, accurate and reliable inference service under varying query load. This letter introduces a novel Robot-Inference-and-Learning-as-a-Service (RILaaS) platform for low-latency and secure inference serving of deep models that can be deployed on robots. Unique features of RILaaS include: 1) low-latency and reliable serving with gRPC under dynamic loads by distributing queries over multiple servers on Edge and Cloud, 2) SSH based authentication coupled with SSL/TLS based encryption for security and privacy of the data, and 3) front-end REST API for sharing, monitoring and visualizing performance metrics of the available models. We report experiments to evaluate the RILaaS platform under varying loads of batch size, number of robots, and various model placement hosts on Cloud, Edge, and Fog for providing benchmark applications of object recognition and grasp planning as a service. We address the complexity of load balancing with a reinforcement learning algorithm that optimizes simulated profiles of networked robots; outperforming several baselines including round robin, least connections, and least model time with 68.30% and 14.04% decrease in round-trip latency time across models compared to the worst and the next best baseline respectively. Details and updates are available at: https://sites.google.com/view/rilaas.",https://ieeexplore.ieee.org/document/9103220/,IEEE Robotics and Automation Letters,July 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/81.747195,Reaction-diffusion CNN algorithms to generate and control artificial locomotion,IEEE,Journals,"In this paper a physiological-behavioral approach to neural processing is used to realize artificial locomotion in mechatronic devices. The task has been realized by using a particular model of reaction-diffusion cellular neural networks (RD-CNN's) generating autowave fronts as well as Turing patterns. Moreover a programmable hardware cellular neural network structure is presented in order to model, generate, and control in real time some biorobots. The programmable hardware implementation gives the possibility of generating locomotion in real time and also to control the transition among several types of locomotion, with particular attention to hexapodes. The approach proposed allows not only the design of walking robots, but also the ability to build structures able to efficiently solve typical problems in industrial automation, such as online routing of objects moved on conveyor belts.",https://ieeexplore.ieee.org/document/747195/,IEEE Transactions on Circuits and Systems I: Fundamental Theory and Applications,Feb. 1999,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/3477.809039,Reactive navigation in dynamic environment using a multisensor predictor,IEEE,Journals,"A reactive navigation system for an autonomous mobile robot in unstructured dynamic environments is presented. The motion of moving obstacles is estimated for robot motion planning and obstacle avoidance. A multisensor-based obstacle predictor is utilized to obtain obstacle-motion information. Sensory data from a CCD camera and multiple ultrasonic range finders are combined to predict obstacle positions at the next sampling instant. A neural network, which is trained off-line, provides the desired prediction on-line in real time. The predicted obstacle configuration is employed by the proposed virtual force based navigation method to prevent collision with moving obstacles. Simulation results are presented to verify the effectiveness of the proposed navigation system in an environment with multiple mobile robots or moving objects. This system was implemented and tested on an experimental mobile robot at our laboratory. Navigation results in real environment are presented and analyzed.",https://ieeexplore.ieee.org/document/809039/,"IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics)",Dec. 1999,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/LRA.2022.3187536,Recommending Fine-Grained Tool Consistent With Common Sense Knowledge for Robot,IEEE,Journals,"When robots carry out task, selecting an appropriate tool is necessary. The current research ignores the fine-grained characteristic of tasks, and mainly focuses on whether the task can be completed. Little consideration is paid for the object being manipulated, which affects the task completion quality. In order to support task oriented fine-grained tool recommendation, based on common sense knowledge, this paper proposes Fine-grained Tool-Task Graph (FTTG) to describe multi-granularity semantics of tasks, tools, objects being manipulated and relationships among them. According to FTTG, a Fine-grained Tool-Task (FTT) dataset is constructed by labeling images of tools and objects being manipulated with the defined semantics. A baseline method named Fine-grained Tool Recommendation Network (FTR-Net) is also proposed in this paper. FTR-Net gives coarse-grained and fine-grained semantic predictions by simultaneously learning the common and special features of the tools and objects being manipulated. At the same time, FTR-Net constrains the distance between features of the well matched tool and object more smaller than that of those unmatched. The constraint and the special feature ensure FTR-Net provide fine-grained tool recommendation. The constraint and the common feature ensure FTR-Net provide coarse-grained tool recommendation when the fine-grained tools are not available. Experiments show that FTR-Net can recommend tools consistent with common sense whether on test data sets or in real situations.",https://ieeexplore.ieee.org/document/9811343/,IEEE Robotics and Automation Letters,Oct. 2022,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/TASE.2014.2377791,RoboEarth Semantic Mapping: A Cloud Enabled Knowledge-Based Approach,IEEE,Journals,"The vision of the RoboEarth project is to design a knowledge-based system to provide web and cloud services that can transform a simple robot into an intelligent one. In this work, we describe the RoboEarth semantic mapping system. The semantic map is composed of: 1) an ontology to code the concepts and relations in maps and objects and 2) a SLAM map providing the scene geometry and the object locations with respect to the robot. We propose to ground the terminological knowledge in the robot perceptions by means of the SLAM map of objects. RoboEarth boosts mapping by providing: 1) a subdatabase of object models relevant for the task at hand, obtained by semantic reasoning, which improves recognition by reducing computation and the false positive rate; 2) the sharing of semantic maps between robots; and 3) software as a service to externalize in the cloud the more intensive mapping computations, while meeting the mandatory hard real time constraints of the robot. To demonstrate the RoboEarth cloud mapping system, we investigate two action recipes that embody semantic map building in a simple mobile robot. The first recipe enables semantic map building for a novel environment while exploiting available prior information about the environment. The second recipe searches for a novel object, with the efficiency boosted thanks to the reasoning on a semantically annotated map. Our experimental results demonstrate that, by using RoboEarth cloud services, a simple robot can reliably and efficiently build the semantic maps needed to perform its quotidian tasks. In addition, we show the synergetic relation of the SLAM map of objects that grounds the terminological knowledge coded in the ontology.",https://ieeexplore.ieee.org/document/7015601/,IEEE Transactions on Automation Science and Engineering,April 2015,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/LRA.2022.3153728,Robot Cooking With Stir-Fry: Bimanual Non-Prehensile Manipulation of Semi-Fluid Objects,IEEE,Journals,"This letter describes an approach to achieve well-known Chinese cooking art stir-fry on a bimanual robot system. Stir-fry requires a sequence of highly dynamic coordinated movements, which is usually difficult to learn for a chef, let alone transfer to robots. In this letter, we define a canonical stir-fry movement, and then propose a decoupled framework for learning this deformable object manipulation from human demonstration. First, dualarms of the robot are decoupled into different roles (a leader and follower) and learned with classical and neural network based methods separately, then the bimanual task is transformed into a coordination problem. To obtain general bimanual coordination, we secondly propose a Graph and Transformer based model&#x2014;<i>Structured-Transformer</i>, to capture the spatio-temporal relationship between dual-arm movements. Finally, by adding visual feedback of contents deformation, our framework can adjust the movements automatically to achieve the desired stir-fry effect. We verify the framework by a simulator and deploy it on a real bimanual Panda robot system. The experimental results validate our framework can realize the bimanual robot stir-fry motion and have the potential to extend to other deformable objects with bimanual coordination.",https://ieeexplore.ieee.org/document/9720489/,IEEE Robotics and Automation Letters,April 2022,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ACCESS.2020.2992701,Robot Formation Control Based on Internet of Things Technology Platform,IEEE,Journals,"The cooperative control technology of robot formation can sense all kinds of external environment in real time. It is a multi-functional control and management system including visual recognition, task management execution and distribution, behavior decision-making and so on. It can easily adapt to all kinds of harsh environment. In order to meet the efficient response requirements of robot formation control, a real-time transmission system of robot cooperative motion control is built based on the Internet of things platform, which collects and feeds back the trajectory of multiple robots. Through particle swarm optimization deep learning algorithm, more accurate identification, prediction and guidance of the robot's next action. Finally, the simulation of robot formation motion is established by MATLAB software, which verifies the feasibility of particle swarm optimization deep learning neural network algorithm under the Internet of things technology. Compared with the traditional robot formation control method, the optimized control method has faster convergence speed, smaller error and more accurate position, which provides method guidance for the accuracy and efficiency of robot formation control technology.",https://ieeexplore.ieee.org/document/9087868/,IEEE Access,2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/TMECH.2021.3100306,Robot-Assisted Object Detection for Construction Automation: Data and Information-Driven Approach,IEEE,Journals,"In construction automation, robotic solution is becoming an emerging technology with the advent of artificial intelligence and advancement in mechatronic systems. In construction buildings, regular inspections are carried out to ensure project completion as per approved plans and quality standards. Currently, expert human inspectors are deployed onsite to perform inspection tasks with the naked eye and conventional tools. This process is time-consuming, labor-intensive, dangerous, and repetitive and may yield subjective results. In this article, we propose a robotic system equipped with perception sensors and intelligent algorithms to help construction supervisors remotely identify the construction materials, detect component installations and defects, and generate report of their status and location information. The building information model (BIM) is used for mobile robot navigation and to retrieve building component’s location information. Unlike the current deep learning-based object detection, which depends heavily on training data, this article proposes a data and information-driven approach, which incorporates offline training data, sensor data, and BIM information to achieve BIM-based object coverage navigation, BIM-based false detection filtering, and a fine maneuver technique to improve on object detections during real-time automated task execution by robots. This allows the user to select building components to be inspected, and the mobile robot navigates autonomously to the target components using the BIM-generated navigation map. An object detector then detects the building components and materials and generates an inspection report. The proposed system is verified through laboratory and onsite experiments.",https://ieeexplore.ieee.org/document/9497744/,IEEE/ASME Transactions on Mechatronics,Dec. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/TMECH.2021.3072675,Rule-Based Reinforcement Learning for Efficient Robot Navigation With Space Reduction,IEEE,Journals,"For real-world deployments, it is critical to allow robots to navigate in complex environments autonomously. Traditional methods usually maintain an internal map of the environment, and then design several simple rules, in conjunction with a localization and planning approach, to navigate through the internal map. These approaches often involve a variety of assumptions and prior knowledge. In contrast, recent reinforcement learning (RL) methods can provide a model-free, self-learning mechanism as the robot interacts with an initially unknown environment, but are expensive to deploy in real-world scenarios due to inefficient exploration. In this article, we focus on efficient navigation with the RL technique and combine the advantages of these two kinds of methods into a rule-based RL (RuRL) algorithm for reducing the sample complexity and cost of time. First, we use the rule of wall-following to generate a closed-loop trajectory. Second, we employ a reduction rule to shrink the trajectory, which in turn effectively reduces the redundant exploration space. Besides, we give the detailed theoretical guarantee that the optimal navigation path is still in the reduced space. Third, in the reduced space, we utilize the Pledge rule to guide the exploration strategy for accelerating the RL process at the early stage. Experiments conducted on real robot navigation problems in hex-grid environments demonstrate that RuRL can achieve improved navigation performance.",https://ieeexplore.ieee.org/document/9403986/,IEEE/ASME Transactions on Mechatronics,April 2022,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/TMM.2021.3080076,Scene Recognition Mechanism for Service Robot Adapting Various Families: A CNN-Based Approach Using Multi-Type Cameras,IEEE,Journals,"The key challenges of scene recognition for service robots in various family environments are the view shortage of holistic scenes and poor adaptation. To address these problems, a family scene recognition mechanism for the service robot is proposed in this paper. A comprehensive application of fish-eye, pinhole, and depth cameras is provided to guarantee the sufficient view of robot. A selective CNN features fusion for the recognition of fish-eye scene images is designed to improve the training efficiency and the recognition accuracy. The mechanism is deployed in a designed hybrid cloud including public and private clouds. The proposed family scene recognition model is trained by large-scale datasets in the public cloud and runs in the private cloud. Besides, the recognition skill can be reinforced and increased by matching human guidance and CNN features to help the robot learn new scenes and improve the adaptation in different family environments. Extensive experiments are implemented to evaluate the proposed method using real scene images from six families. The experiment results show the validity and good performance of our method for the service robot scene recognition in various family environments.",https://ieeexplore.ieee.org/document/9431727/,IEEE Transactions on Multimedia,2022,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/LRA.2020.3013848,Sim2Real Predictivity: Does Evaluation in Simulation Predict Real-World Performance?,IEEE,Journals,"Does progress in simulation translate to progress on robots? If one method outperforms another in simulation, how likely is that trend to hold in reality on a robot? We examine this question for embodied PointGoal navigation - developing engineering tools and a research paradigm for evaluating a simulator by its sim2real predictivity. First, we develop Habitat-PyRobot Bridge (HaPy), a library for seamless execution of identical code on simulated agents and robots - transferring simulation-trained agents to a LoCoBot platform with a one-line code change. Second, we investigate the sim2real predictivity of Habitat-Sim M. Savva et al., for PointGoal navigation. We 3D-scan a physical lab space to create a virtualized replica, and run parallel tests of 9 different models in reality and simulation. We present a new metric called Sim-vs-Real Correlation Coefficient (SRCC) to quantify predictivity. We find that SRCC for Habitat as used for the CVPR19 challenge is low (0.18 for the success metric), suggesting that performance differences in this simulator-based challenge do not persist after physical deployment. This gap is largely due to AI agents learning to exploit simulator imperfections - abusing collision dynamics to `slide' along walls, leading to shortcuts through otherwise non-navigable space. Naturally, such exploits do not work in the real world. Our experiments show that it is possible to tune simulation parameters to improve sim2real predictivity (e.g. improving SRCCSucc from 0.18 to 0.844) - increasing confidence that in-simulation comparisons will translate to deployed systems in reality.",https://ieeexplore.ieee.org/document/9158349/,IEEE Robotics and Automation Letters,Oct. 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/TCDS.2016.2565542,Spatial Concept Acquisition for a Mobile Robot That Integrates Self-Localization and Unsupervised Word Discovery From Spoken Sentences,IEEE,Journals,"In this paper, we propose a novel unsupervised learning method for the lexical acquisition of words related to places visited by robots, from human continuous speech signals. We address the problem of learning novel words by a robot that has no prior knowledge of these words except for a primitive acoustic model. Furthermore, we propose a method that allows a robot to effectively use the learned words and their meanings for self-localization tasks. The proposed method is nonparametric Bayesian spatial concept acquisition method (SpCoA) that integrates the generative model for self-localization and the unsupervised word segmentation in uttered sentences via latent variables related to the spatial concept. We implemented the proposed method SpCoA on SIGVerse, which is a simulation environment, and TurtleBot2, which is a mobile robot in a real environment. Further, we conducted experiments for evaluating the performance of SpCoA. The experimental results showed that SpCoA enabled the robot to acquire the names of places from speech sentences. They also revealed that the robot could effectively utilize the acquired spatial concepts and reduce the uncertainty in self-localization.",https://ieeexplore.ieee.org/document/7467531/,IEEE Transactions on Cognitive and Developmental Systems,Dec. 2016,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/TCAD.2020.3012864,StereoEngine: An FPGA-Based Accelerator for Real-Time High-Quality Stereo Estimation With Binary Neural Network,IEEE,Journals,"Stereo estimation is essential to many applications such as mobile autonomous robots, most of which ask for real-time response, high energy, and storage efficiency. Deep neural networks (DNNs) have shown to yield significant gains in improving accuracy. However, these DNN-based algorithms are challenging to be deployed on energy and resource-constrained devices due to the high computational complexities of DNNs. In this article, we present StereoEngine, a fully pipelined end-to-end stereo vision accelerator that computes accurate dense depth in a real-time and energy-efficient manner. An efficient stereo algorithm is developed and optimized for a high-quality hardware-friendly implementation, that leverages binary neural network (BNN) to learn discriminative binary descriptors to improve the disparity. The design of StereoEngine is a standalone DNN-based stereo vision system where all processing procedures are implemented on a hardware platform. The effectiveness of StereoEngine is evaluated by comprehensive experiments. Compared with software-based implementations on the highend and embedded Nvidia GPUs, StereoEngine achieves up to 3×, 13×, and 50× speedups, as well as up to 211×, 58×, and 73× energy efficiency improvement, respectively. Furthermore, StereoEngine achieves leading accuracy when compared to state-of-the-art hardware implementations on the challenging KITTI dataset.",https://ieeexplore.ieee.org/document/9211569/,IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems,Nov. 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/TRO.2012.2228134,The Impact of Human–Robot Interfaces on the Learning of Visual Objects,IEEE,Journals,"This paper studies the impact of interfaces, allowing nonexpert users to efficiently and intuitively teach a robot to recognize new visual objects. We present challenges that need to be addressed for real-world deployment of robots capable of learning new visual objects in interaction with everyday users. We argue that in addition to robust machine learning and computer vision methods, well-designed interfaces are crucial for learning efficiency. In particular, we argue that interfaces can be key in helping nonexpert users to collect good learning examples and, thus, improve the performance of the overall learning system. Then, we present four alternative human-robot interfaces: Three are based on the use of a mediating artifact (smartphone, wiimote, wiimote and laser), and one is based on natural human gestures (with a Wizard-of-Oz recognition system). These interfaces mainly vary in the kind of feedback provided to the user, allowing him to understand more or less easily what the robot is perceiving and, thus, guide his way of providing training examples differently. We then evaluate the impact of these interfaces, in terms of learning efficiency, usability, and user's experience, through a real world and large-scale user study. In this experiment, we asked participants to teach a robot 12 different new visual objects in the context of a robotic game. This game happens in a home-like environment and was designed to motivate and engage users in an interaction where using the system was meaningful. We then discuss results that show significant differences among interfaces. In particular, we show that interfaces such as the smartphone interface allows nonexpert users to intuitively provide much better training examples to the robot, which is almost as good as expert users who are trained for this task and are aware of the different visual perception and machine learning issues. We also show that artifact-mediated teaching is significantly more efficient for robot learning, and equally good in terms of usability and user's experience, than teaching thanks to a gesture-based human-like interaction.",https://ieeexplore.ieee.org/document/6384810/,IEEE Transactions on Robotics,April 2013,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/TCDS.2017.2712712,Toward Brain-Inspired Learning With the Neuromorphic Snake-Like Robot and the Neurorobotic Platform,IEEE,Journals,"Neurorobotic mimics the structural and functional principles of living creature systems. Modeling a single system by robotic hardware and software has existed for decades. However, an integrated toolset studying the interaction of all systems has not been demonstrated yet. We present a hybrid neuromorphic computing paradigm to bridge this gap by combining the neurorobotics platform (NRP) with the neuromorphic snake-like robot (NeuroSnake). This paradigm encompasses the virtual models, neuromorphic sensing and computing capabilities, and physical bio-inspired bodies, with which an experimenter can design and execute both in-silico and in-vivo robotic experimentation easily. The NRP is a public Web-based platform for easily testing brain models with virtual bodies and environments. The NeuroSnake is a bio-inspired robot equipped with a silico-retina sensor and neuromorphic computer for power-efficiency applications. We illustrate the efficiencies of our paradigm with an easy designing of a visual pursuit experiment in the NRP. We study two automatic behavior learning tasks which are further integrated into a complex task of semi-autonomous pole climbing. The result shows that robots could build new learning rules in a less explicit manner inspired by living creatures. Our method gives an alternative way to efficiently develop complex behavior control of the ro As spiking neural network is a bio-inspired neural network and the NeuroSnake robot is equipped with a spike-based silicon retina camera, the control system can be easily implemented via spiking neurons simulated on neuromorphic hardware, such as SpiNNaker.bot.",https://ieeexplore.ieee.org/document/7945270/,IEEE Transactions on Cognitive and Developmental Systems,March 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/TASE.2017.2731371,Toward Socially Aware Robot Navigation in Dynamic and Crowded Environments: A Proactive Social Motion Model,IEEE,Journals,"Safe and social navigation is the key to deploying a mobile service robot in a human-centered environment. Widespread acceptability of mobile service robots in daily life is hindered by robot's inability to navigate in crowded and dynamic human environments in a socially acceptable way that would guarantee human safety and comfort. In this paper, we propose an effective proactive social motion model (PSMM) that enables a mobile service robot to navigate safely and socially in crowded and dynamic environments. The proposed method considers not only human states (position, orientation, motion, field of view, and hand poses) relative to the robot but also social interactive information about human-object and human group interactions. This allows development of the PSMM that consists of elements of an extended social force model and a hybrid reciprocal velocity obstacle technique. The PSMM is then combined with a path planning technique to generate a motion planning system that drives a mobile robot in a socially acceptable manner and produces respectful and polite behaviors akin to human movements. Note to Practitioners-In this paper, we validated the effectiveness and feasibility of the proposed proactive social motion model (PSMM) through both simulation and real-world experiments under the newly proposed human comfortable safety indices. To do that, we first implemented the entire navigation system using the open-source robot operating system. We then installed it in a simulated robot model and conducted experiments in a simulated shopping mall-like environment to verify its effectiveness. We also installed the proposed algorithm on our mobile robot platform and conducted experiments in our office-like laboratory environment. Our results show that the developed socially aware navigation framework allows a mobile robot to navigate safely, socially, and proactively while guaranteeing human safety and comfort in crowded and dynamic environments. In this paper, we examined the proposed PSMM with a set of predefined parameters selected based on our empirical experiences about the robot mechanism and selected social environment. However, in fact a mobile robot might need to adapt to various contextual and cultural situations in different social environments. Thus, it should be equipped with an online adaptive interactive learning mechanism allowing the robot to learn to auto-adjust their parameters according to such embedded environments. Using machine learning techniques, e.g., inverse reinforcement learning [1] to optimize the parameter set for the PSMM could be a promising research direction to improve adaptability of mobile service robots in different social environments. In the future, we will evaluate the proposed framework based on a wider variety of scenarios, particularly those with different social interaction situations and dynamic environments. Furthermore, various kinds of social cues and signals introduced in [2] and [3] will be applied to extend the proposed framework in more complicated social situations and contexts. Last but not least, we will investigate different machine learning techniques and incorporate them in the PSMM in order to allow the robot to automatically adapt to diverse social environments.",https://ieeexplore.ieee.org/document/8011466/,IEEE Transactions on Automation Science and Engineering,Oct. 2017,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ACCESS.2020.3046730,Tracking In-Cabin Astronauts Using Deep Learning and Head Motion Clues,IEEE,Journals,"A person-following robot is under development for astronaut assistance on the Chinese Space Station. Real-time astronaut detection and tracking are the most important prerequisites for in-cabin flying assistant robots so that they can follow a specific astronaut and offer him/her assistance. In the limited space in the space station cabin, astronauts stand close to each other when working collaboratively; thus, large regions of their bodies tend to overlap in the image. In addition, because astronauts wear the same clothes most of the time, it is difficult to distinguish an individual astronaut using human body features. In this paper, we distinguish the astronauts by tracking their heads in the image. A deep learning model trained using big data is proposed for effective head detection. In addition, a motion model based on spatial clues is combined with the head detection results to track astronauts in the scene. A complete pipeline of the algorithm has been implemented and run efficiently on the Tegra X2 embedded AI microprocessor. A set of experiments were carried out and successfully validated the effectiveness of the proposed tracking algorithm. This algorithm is a step toward the implementation of robot assistants, especially in resource-limited environments.",https://ieeexplore.ieee.org/document/9305234/,IEEE Access,2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/TRO.2021.3129972,Unsupervised Online Learning for Robotic Interestingness With Visual Memory,IEEE,Journals,"Autonomous robots frequently need to detect &#x201C;interesting&#x201D; scenes to decide on further exploration, or to decide which data to share for cooperation. These scenarios often require fast deployment with little or no training data. Prior work considers &#x201C;interestingness&#x201D; based on data from the same distribution. Instead, we propose to develop a method that automatically adapts online to the environment to report interesting scenes quickly. To address this problem, we develop a novel translation-invariant visual memory and design a three-stage architecture for long-term, short-term, and online learning, which enables the system to learn human-like experience, environmental knowledge, and online adaption, respectively. With this system, we achieve an average of 20&#x0025; higher accuracy than the state-of-the-art unsupervised methods in a subterranean tunnel environment. We show comparable performance to supervised methods for robot exploration scenarios showing the efficacy of our approach. We expect that the presented method will play an important role in the robotic interestingness recognition exploration tasks.",https://ieeexplore.ieee.org/document/9653144/,IEEE Transactions on Robotics,Aug. 2022,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/LRA.2022.3140793,Vision-Based Self-Adaptive Gripping in a Trimodal Robotic Sorting End-Effector,IEEE,Journals,"Recyclable waste management, which includes sorting as a key process, is a crucial component of maintaining a sustainable ecosystem. The use of robots in sorting could significantly facilitate the production of secondary raw materials from waste in the sense of a recycling economy. However, due to the complex and heterogeneous types of the recyclable items, the conventional robotic gripping end-effectors, which typically come with a fixed structure, are unlikely to hold onto the full range of items to enable separation and recycling. To this end, a trimodal adaptive end-effector is proposed that can be integrated with robotic manipulators to improve their gripping versatility. The end-effector can deploy effective modes of gripping to different objects in response to their size and porosity via gripping mechanisms based on Nano Polyurethane (PU) adhesive gels, pumpless vacuum suction, and radially deployable claws. While the end-effector's mechanical design allows the three gripping modes to be deployed independently or in conjunction with one another, this work aims at deploying modes that are effective for gripping onto the recyclable item. In order to decide on the suitable modes of gripping a real-time vision system is designed to measure the size and porosity of the recyclable items and advise on a suitable combination of gripping modes to be deployed. Integrated current sensors provide an indication of successful gripping and releasing of the recyclable items. The results of the experiments confirmed the ability of our vision-based approach in identifying suitable gripping modes in real-time, the deployment of the relevant mechanisms and successful gripping onto a maximum of 84.8% (single-mode), 90.9% (dual-mode) and 96.9% (triple-mode) of a specified set of recyclable items.",https://ieeexplore.ieee.org/document/9672720/,IEEE Robotics and Automation Letters,April 2022,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/LRA.2022.3143289,VisuoTactile 6D Pose Estimation of an In-Hand Object Using Vision and Tactile Sensor Data,IEEE,Journals,"Knowledge of the 6D pose of an object can benefit in-hand object manipulation. Existing 6D pose estimation methods use vision data. In-hand 6D object pose estimation is challenging because of heavy occlusion produced by the robot’s grippers, which can have an adverse effect on methods that rely on vision data only. Many robots are equipped with tactile sensors at their fingertips that could be used to complement vision data. In this letter, we present a method that uses both tactile and vision data to estimate the pose of an object grasped in a robot’s hand.The main challenges of this research include 1) lack of standard representation for tactile sensor data, 2) fusion of sensor data from heterogeneous sources—vision and tactile, and 3) a need for large training datasets. To address these challenges, first, we propose use of point clouds to represent object surfaces that are in contact with the tactile sensor. Second, we present a network architecture based on pixel-wise dense fusion to fuse vision and tactile data to estimate the 6D pose of an object. Third, we extend NVIDIA’s Deep Learning Dataset Synthesizer to produce synthetic photo-realistic vision data and the corresponding tactile point clouds for 11 objects from the YCB Object and Model Set in Unreal Engine 4. We present results of simulated experiments suggesting that using tactile data in addition to vision data improves the 6D pose estimate of an in-hand object. We also present qualitative results of experiments in which we deploy our network on real physical robots showing successful transfer of a network trained on synthetic data to a real system.",https://ieeexplore.ieee.org/document/9682507/,IEEE Robotics and Automation Letters,April 2022,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/TSMCB.2010.2089978,"Walking Motion Generation, Synthesis, and Control for Biped Robot by Using PGRL, LPI, and Fuzzy Logic",IEEE,Journals,"This paper proposes the implementation of fuzzy motion control based on reinforcement learning (RL) and Lagrange polynomial interpolation (LPI) for gait synthesis of biped robots. First, the procedure of a walking gait is redefined into three states, and the parameters of this designed walking gait are determined. Then, the machine learning approach applied to adjusting the walking parameters is policy gradient RL (PGRL), which can execute real-time performance and directly modify the policy without calculating the dynamic function. Given a parameterized walking motion designed for biped robots, the PGRL algorithm automatically searches the set of possible parameters and finds the fastest possible walking motion. The reward function mainly considered is first the walking speed, which can be estimated from the vision system. However, the experiment illustrates that there are some stability problems in this kind of learning process. To solve these problems, the desired zero moment point trajectory is added to the reward function. The results show that the robot not only has more stable walking but also increases its walking speed after learning. This is more effective and attractive than manual trial-and-error tuning. LPI, moreover, is employed to transform the existing motions to the motion which has a revised angle determined by the fuzzy motion controller. Then, the biped robot can continuously walk in any desired direction through this fuzzy motion control. Finally, the fuzzy-based gait synthesis control is demonstrated by tasks and point- and line-target tracking. The experiments show the feasibility and effectiveness of gait learning with PGRL and the practicability of the proposed fuzzy motion control scheme.",https://ieeexplore.ieee.org/document/5640679/,"IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics)",June 2011,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/JSEN.2020.3024094,k-Nearest Neighbor Classification for Pattern Recognition of a Reference Source Light for Machine Vision System,IEEE,Journals,"The design of machine vision applications allows automatic inspection, measuring systems, and robot guidance. Typical applications of industrial robots are based on no-contact sensors to give the robot information about the environment. Robot's machine vision requires photosensors or video cameras to make intelligent decisions about its localization. Video cameras used as image-capturing equipment are too costly in comparison with optical scanning systems (OSS). The OSS system provides spatial coordinates measurements that can be exploited to solve a wide variety of structural problems in real-time. Localization and guidance using machine learning (ML) techniques offer advantages due to signals captured can be transformed and be reduced for processing, storage, and displaying. The use of algorithms of ML enhances the performance of the optical system based on localization and guidance. Feature extraction represents an important part of ML techniques to transform the original raw data onto a low-dimensional subspace and holding relevant information. This work presents an improvement of an optical system based on k-nearest neighbor ( k-NN) technique to solve the object detection and localization problem. The utility of this improvement allows the optical system can discriminate between the reference source and the optical noise or interference. The OSS system presented in this article has been implemented in structural health monitoring to measure the angular position even under “lighting and weather conditions”. The feature extraction techniques used in this article were linear predictive coding (LPC), quartiles ( Qiquartile), and autocorrelation coefficients (ACC). The results of using k-NN and autocorrelation coefficients and quartiles predicted more than 98% of correct classification by using a reference source light as a class 1 and a light bulb as an optical noise and called class 2.",https://ieeexplore.ieee.org/document/9195874/,IEEE Sensors Journal,"15 May15, 2021",ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/UR52253.2021.9494682,3-Survivor: A Rough Terrain Negotiable Search and Surveillance Mobile Robot with Real-Time Object Detection,IEEE,Conferences,"This paper presents the design and integration of 3-Survivor: a rough terrain negotiable search and surveillance robot. In 3-Survivor, a modified double-tracked chained wheel with a caterpillar mechanism incorporates the body design. A passive adjustment is established in the body balance that enables the front and rear body to operate in excellent synchronization. The robot is remotely operated using the web portal, and the entire operation is telecast with a Raspberry Pi Sony IMX477R camera. An object detection module (ODM) is incorporated with live streaming to integrate the surveillance system. A learning-based EfficientDet-D7 network is optimized for precise and versatile target recognition and interaction. The D7 includes a computationally efficient bi-directional feature pyramid network (BiFPN) as the backbone network that allows multi-scale feature fusion. A custom dataset containing 5000 images of indoor-outdoor objects is developed to train and validate the performance of the SAR tasks. A very impressive 56.7 mAP is acquired from this proposed D7 model. The feasibility and efficacy of the proposed system are tested and validated by indoor and outdoor SAR trial operations.",https://ieeexplore.ieee.org/document/9494682/,2021 18th International Conference on Ubiquitous Robots (UR),12-14 July 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AIID51893.2021.9456574,3D scene geometry estimation method of substation inspection robot based on lightweight neural network,IEEE,Conferences,"Understanding 3D scene geometry from video is a basic subject of visual perception. It includes many classic computer vision tasks, such as depth recovery, traffic estimation, visual odometer. Recent work has proved that deep learning can be applied to scene understanding problems. But they all have some inherent limitations. For example, they need stereo cameras as additional devices for data acquisition, or can't explicitly deal with non-rigid and occlusion. The environment in the substation is complex, and there are many devices. In the working process of inspection robot, the target is very easy to be blocked, and it is difficult to deploy directly by traditional methods. In addition, the real-time performance of neural network is very important for electric inspection robot. In this paper, 3D scene geometry estimation method of substation inspection robot is proposed, which consists of two main parts: GeoNet module and pruning module. Experiments show that the proposed method can be effectively applied to electric inspection robot.",https://ieeexplore.ieee.org/document/9456574/,2021 IEEE International Conference on Artificial Intelligence and Industrial Design (AIID),28-30 May 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/iEECON53204.2022.9741676,A Development of Object Detection System based on Deep Learning Approach to Support the Laparoscope Manipulating Robot (LMR),IEEE,Conferences,"This paper presents the development of an object detection system based on the deep learning approach of computer vision to support the laparoscopic surgical robotic position control system. The system comprises two main phases, the training phase, and the real-time operating phase. In the training phase, a modified YOLOv4 algorithm is proposed to be adopted within the Darknet framework for training the weights of detection models. The best detection model is then selected to be assigned to the object detector in the real-time operating phase. The validation result of the detection model across the experiment datasets is 90% accuracy on average during the training phase. To validate the viability of real-time object detection, the proposed system was implemented in two test-cases of gynecologic surgery with the soft-body cadavers.",https://ieeexplore.ieee.org/document/9741676/,2022 International Electrical Engineering Congress (iEECON),9-11 March 2022,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.23919/ICCAS52745.2021.9650034,A Fast Real-time Facial Expression Classifier Deep Learning-based for Human-robot Interaction,IEEE,Conferences,"Human-robot interaction drives the need for vision technology to recognize user expressions. Convolutional Neural Networks (CNN) has been introduced as a robust facial feature extractor and can overcome classification task. However, it is not supported by efficient computation for real-time applications. The work proposes an efficient CNN architecture to recognize human facial expressions that consist of five stages containing a combination of lightweight convolution operations. It introduces the efficient contextual extractor with a partial transfer module to suppress computational compression. This technique is applied to the mid and high-level features by separating the channel-based input features into two parts. Then it applies sequential convolution to only one part and combines it with the previous separated part. A shuffle channel group is used to exchange the information extracted. The structure of the entire network generates less than a million parameters. The CK+ and KDEF datasets are used as training and test sets to evaluate the performance of the proposed architecture. As a result, the proposed classifier obtains an accuracy that is competitive with other methods. In addition, the efficiency of the classifier has strongly suitable for implementation to edge devices by achieving 43 FPS on a Jetson Nano.",https://ieeexplore.ieee.org/document/9650034/,"2021 21st International Conference on Control, Automation and Systems (ICCAS)",12-15 Oct. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ComPE49325.2020.9200082,A Mobile Robot for Hazardous Gas Sensing,IEEE,Conferences,"This paper reports development of a robot that can sense the presence of hazardous gases in the environment. The robot aims at detection of hazardous gases and mapping global positioning system (GPS) locations of the detected gases to the navigation terrain in real-time. These information was transmitted to a hand-held device in a remote location for exploration of gas types, which holds promise for disaster management. The robot was equipped with a module of gas sensors, human detection sensor, GPS module and obstacle detection sensors in a coherent system. While navigating with collision avoidance to obstacles, the robot can transmit information about the presence of hazardous gases and human being in the area of navigation. It was tested in an uneven terrain to recognize the presence of hazardous gases like carbon dioxide, liquefied petroleum gas, vaporized alcohol gas vis-a-vis ambient gases in real-time. Á neural network-based classifier was implemented to recognize the gases with an average accuracy of 98%.",https://ieeexplore.ieee.org/document/9200082/,2020 International Conference on Computational Performance Evaluation (ComPE),2-4 July 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CSICC52343.2021.9420614,A New Approach for Mapping of Soccer Robot Agents Position to Real Filed Based on Multi-Core Fuzzy Clustering,IEEE,Conferences,"Mapping the position of soccer robot agents to a real field, is one of the essential issues in the practical implementation of scientific contributions in this context. The lack of a proper assignment affects the scientific implementation of many subjects, such as routing, obstacle avoidance, and robot guidance. For this reason, the use of a clustering method is proposed in this article. Upon the entrance of a new agent, its position is mapped to the real field based on the clustering algorithm. After this mapping, the system begins to work according to the position of the agents, which is defined as the position of the centers of the clusters, as well as the rules defined in the knowledge-base. Considering the unknown and dynamic environment of the robot, some objects inherit common traits from multiple clusters. One reasonable solution for considering the cluster overlaps is to assign a set of membership degrees to each of them. Multiple membership degree assignments result from the fuzzy nature of the clusters. Due to the reduction of segmentations and the shrinkage of the search space, fuzzy clustering generally faces less computational overhead, while the identification and handling of vague, noisy, and outlier data also become much easier in them. The approach of the proposed method is based on the feasibility ideas and uses multi-core learning to identify clusters with complex data structures. The feasibility score of each data represents the percentages of the properties that data inherits from the clusters. Automatically adjusting the weights of the cores in an optimization framework, the proposed method avoids the damage caused by problems such as adopting inefficient cores, or irrelevant features.",https://ieeexplore.ieee.org/document/9420614/,"2021 26th International Computer Conference, Computer Society of Iran (CSICC)",3-4 March 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISIC.2007.4450948,A New Color Based Optical Flow Algorithm for Environment Mapping Using a Mobile Robot,IEEE,Conferences,"Environment mapping from a video sequence is considered to be one of the most important problems in computer vision because of its application in surveillance, virtual reality, autonomous navigation, multimedia communications, medical prognosis, etc. In this paper, we have presented an optical flow based method for environment mapping. It uses a new color based optical flow computation technique. The camera, which is mounted on a mobile robot, is kept perpendicular to the direction of motion, and the captured set of images is used to compute the dense depth map. We have used a Kalman filter to denoise the depth map.",https://ieeexplore.ieee.org/document/4450948/,2007 IEEE 22nd International Symposium on Intelligent Control,1-3 Oct. 2007,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/RAMECH.2011.6070484,A Q-learning based Cartesian model reference compliance controller implementation for a humanoid robot arm,IEEE,Conferences,This paper presents the implementation (real time and simulation) of a model-free Q-learning based discrete model reference compliance controller for a humanoid robot arm. The Reinforcement learning (RL) scheme uses a recently developed Q-learning scheme to develop an optimal policy on-line. The RL Cartesian (x and y) tracking controller with model reference compliance was implemented using two links (shoulder flexion and elbow flexion joints) of the right arm of the humanoid Bristol-Elumotion-Robotic-Torso II (BERT II) torso.,https://ieeexplore.ieee.org/document/6070484/,"2011 IEEE 5th International Conference on Robotics, Automation and Mechatronics (RAM)",17-19 Sept. 2011,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IJCNN.2003.1224077,A RAM-based neural network for collision avoidance in a mobile robot,IEEE,Conferences,"A RAM-based neural network is being developed for a mobile robot controlled by a simple microprocessor system. Conventional neural networks often require a powerful and sophisticated computer system. Training a multi-layer neural network requires repeated presentation of training data, which often results in very long learning time. The goal for this paper is to demonstrate that RAM-based neural networks are a suitable choice for embedded applications with few computational resources. This functionality is demonstrated in a simple robot powered by an 8051 microcontroller with 512 bytes of RAM. The RAM-based neural network allows the robot to detect and avoid obstacles in real time.",https://ieeexplore.ieee.org/document/1224077/,"Proceedings of the International Joint Conference on Neural Networks, 2003.",20-24 July 2003,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/EMBC46164.2021.9630765,A Single RGB Camera Based Gait Analysis With A Mobile Tele-Robot For Healthcare,IEEE,Conferences,"With the increasing awareness of high-quality life, there is a growing need for health monitoring devices running robust algorithms in home environment. Health monitoring technologies enable real-time analysis of users’ health status, offering long-term healthcare support and reducing hospitalization time. The propose of this work is twofold, the software focuses on the analysis of gait, which is widely adopted for joint correction and assessing any lower limb, or spinal problem. On the hardware side, a novel marker-less gait analysis device using a low-cost RGB camera mounted on a mobile tele-robot is designed. As gait analysis with a single camera is much more challenging compared to previous works utilizing multi-cameras, a RGB-D camera or wearable sensors, we propose using vision-based human pose estimation approaches. More specifically, based on the out-put of state-of-the-art human pose estimation models, we devise measurements for four bespoke gait parameters: inversion/eversion, dorsiflexion/plantarflexion, ankle and foot progression angles. We thereby classify walking patterns into normal, supination, pronation and limp. We also illustrate how to run the proposed machine learning models in low-resource environments such as a single entry-level CPU. Experiments show that our single RGB camera method achieves competitive performance compared to multi-camera motion capture systems, at smaller hardware costs.",https://ieeexplore.ieee.org/document/9630765/,2021 43rd Annual International Conference of the IEEE Engineering in Medicine & Biology Society (EMBC),1-5 Nov. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ROBIO.2006.340185,A Study of real-time EMG-driven Arm Wrestling Robot,IEEE,Conferences,"An EMG-driven arm wrestling robot (AWR) is being developed in our laboratories for the purposes of studying neuromuscular control of arm movements. The AWR arm have 2-DOF, integrated with mechanical arm, elbow/wrist force sensors, servo motor, encoder, 3-D MEMS accelerometer, and USB camera, is used to estimate tension developed by individual muscles based on recorded electromyograms (EMGs). The surface electromyographic signal form the upper limb is sampled from a real player in same conditions. By using the method of wavelet packet transformation (WPT) and auto regressive model (AR), the characteristics of EMG signals can be extracted. Artificial neural network is adopted to estimate the elbow joint torque. The effectiveness of the humanoid algorithm using torque control estimated via WRT and neural network is confirmed by experiments. The purpose of this paper is to describe the design objectives, fundamental components and implementation of our real-time, EMG-driven AWR arm.",https://ieeexplore.ieee.org/document/4142107/,2006 IEEE International Conference on Robotics and Biomimetics,17-20 Dec. 2006,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA.2012.6225245,A depth space approach to human-robot collision avoidance,IEEE,Conferences,"In this paper a real-time collision avoidance approach is presented for safe human-robot coexistence. The main contribution is a fast method to evaluate distances between the robot and possibly moving obstacles (including humans), based on the concept of depth space. The distances are used to generate repulsive vectors that are used to control the robot while executing a generic motion task. The repulsive vectors can also take advantage of an estimation of the obstacle velocity. In order to preserve the execution of a Cartesian task with a redundant manipulator, a simple collision avoidance algorithm has been implemented where different reaction behaviors are set up for the end-effector and for other control points along the robot structure. The complete collision avoidance framework, from perception of the environment to joint-level robot control, is presented for a 7-dof KUKA Light-Weight-Robot IV using the Microsoft Kinect sensor. Experimental results are reported for dynamic environments with obstacles and a human.",https://ieeexplore.ieee.org/document/6225245/,2012 IEEE International Conference on Robotics and Automation,14-18 May 2012,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AIM.2009.5229793,A friendly and intelligent human-robot interface system based on human face and hand gesture,IEEE,Conferences,"A friendly and intelligent human-robot interface (HRI) system based on machine vision is established in this paper. A user uses his/her face and hand gestures to communicate with the robot. Three kinds of face detection, including shape, similarity and lip detections, are adopted to improve the recognition accuracy. The recognition of hand gestures is implemented through PCA, BPANN and their combined classifier. Successive dynamic commands are processed by dynamic programming. Experimental results show that the devised natural HRI works well and the system can process 8 image frames per second for practical real time applications.",https://ieeexplore.ieee.org/document/5229793/,2009 IEEE/ASME International Conference on Advanced Intelligent Mechatronics,14-17 July 2009,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SSCI.2016.7849899,A fuzzy-based machine learning model for robot prediction of link quality,IEEE,Conferences,"With foresight into the state of the wireless channel, a robot can make various optimization decisions with regards to routing packets, planning mobility paths, or switching between diverse radios. However, the process of predicting link quality (LQ) is nontrivial due to the streaming and dynamic nature of radio wave propagation, which is complicated by robot mobility. Due to robot movement, the wireless propagation environment can change considerably in terms of distance, obstacles, noise, and interference. Therefore, LQ must be learned and regularly updated while the robot is online. However, the existing fuzzy-based models for assessing LQ are non-adaptable due to the absence of any learning mechanism. To address this issue, we introduce a fuzzy-based prediction model designed for the efficient online and incremental learning of LQ. The unique approach uses fuzzy logic to infer LQ based on the collective output from a series of offset classifiers and their posterior probabilities. In essence, the proposed model leverages machine learning for extracting the underlying functional relationship between the input and output variables, but deeper inferences are made from the output of the learning algorithms using fuzzy logic. Wireless link data from a real-world robot network was used to compare the model with the traditional linear regression approach. The results show statistically significant improvements in three out of the six real-world indoor and outdoor environments where the robot operated. Additionally, the novel approach offers a number of other benefits, including the flexibility to use fuzzy logic for model tuning, as well as the ability to make implementation efficiencies in terms of parallelization and the conservation of labeling resources.",https://ieeexplore.ieee.org/document/7849899/,2016 IEEE Symposium Series on Computational Intelligence (SSCI),6-9 Dec. 2016,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCE-Asia.2016.7804752,A hardware architecture of face detection for human-robot interaction and its implementation,IEEE,Conferences,"This paper presents hardware architecture with low-complexity face detection (FD) and parallel processing of local binary pattern (LBP) generation and adaptive boosting (AdaBoost) algorithm using Haar features for the intelligent service robot system. We designed a fully pipelined architecture implemented with the design techniques, such as variable image scaling and parallel processing multiple classifiers without integral image generation, on the FPGA platform. The proposed architecture enables a real-time FD processing for a VGA video at 30 frames per second.",https://ieeexplore.ieee.org/document/7804752/,2016 IEEE International Conference on Consumer Electronics-Asia (ICCE-Asia),26-28 Oct. 2016,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SKIMA.2016.7916262,"A hybrid real-time EMG intelligent rehabilitation robot motions control based on Kalman Filter, support vector machines and particle swarm optimization",IEEE,Conferences,"Intelligent Control of agent autonomous rehabilitation robot is a very complex problem, especially for stroke patients' treatments and dealing with real-time EMG sensors readings of muscles activity states and transfer between real-time Human motions to interface with rehabilitation robot agent or assisteddevice. The field of Artificial Intelligence and neural networks plays a critical role in modern intelligent control interfaces for robot devices. This paper presents a novel hybrid intelligent robot control that acts as human-robot interaction, where it depends on real-time EMG sensor patients data and extracted features along with estimated knee joint angles from Extended Kalman Filter method are used for training the intelligent controller using support vector machines trained with Adatron Learning algorithm for handling huge data values of sensors readings. Moreover, the proposed platform for rehabilitation robot agent is tested in the framework of the NAO Humanoid Robot agent along with Neurosolutions Toolkit and Matlab code. The average overall accuracy of the proposed intelligent motion SVM-EKF controller shows average high performance that approaches average 96% of knee motions classifications and also good performance for comparing Extended Kalman filter knee joint angles estimations and real EMG human knee joint angles in the framework of Human Walk Gait cycle. Also, the basic enhancement of proposing PSO optimization technique for robot knee motion is discussed for future improvements. The overall algorithm, methodology and experiments are presented in this paper along with future work.",https://ieeexplore.ieee.org/document/7916262/,"2016 10th International Conference on Software, Knowledge, Information Management & Applications (SKIMA)",15-17 Dec. 2016,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCAE.2010.5451340,A low cost microcontroller implementation of neural network based hurdle avoidance controller for a car-like robot,IEEE,Conferences,This paper describes the implementation of a neural network based hurdle avoidance controller for a car like robot using a low cost single chip 89C52 microcontroller. The neural network is the multilayer feed-forward network with back propagation training algorithm. The network is trained offline with tangent-sigmoid as activation function for neurons and is implemented in real time with piecewise linear approximation of tangent-sigmoid function. Results have shown that up-to twenty neurons in hidden layer can be deployed with the proposed technique using a single 89C52 microcontroller. The vehicle is tested in various environments containing obstacles and is found to avoid obstacles in its path successfully.,https://ieeexplore.ieee.org/document/5451340/,2010 The 2nd International Conference on Computer and Automation Engineering (ICCAE),26-28 Feb. 2010,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ROBOT.2001.933206,A method for obstacle avoidance and shooting action of the robot soccer,IEEE,Conferences,"A fuzzy-obstacle-avoidance-path algorithm for obstacle avoidance and a procedure for the shooting action of a soccer robot based on this algorithm are proposed. This algorithm contains a fuzzy system that is used to estimate the rotational velocity of the soccer robot. To demonstrate the effectiveness and applicability of the proposed method, two simulations are presented and a real-time implementation is developed.",https://ieeexplore.ieee.org/document/933206/,Proceedings 2001 ICRA. IEEE International Conference on Robotics and Automation (Cat. No.01CH37164),21-26 May 2001,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS.1997.649083,A mobile robot for service use: behaviour simulation system and intelligent control,IEEE,Conferences,"The structure of hardware and software of AI control system of a mobile robot for service use are described. Hardware of the mobile robot described include an autonomous wheel vehicle and a five degree of freedom manipulator. The software of the AI control system is based on soft computing including fuzzy control rules, fuzzy neural network and genetic algorithms. The intelligent control of cooperative motion between the autonomous vehicle and manipulator realises flexible operations such as navigation of a mobile robot in presence of static and dynamic obstacles, processes of opening door in rooms and pushing buttons of an elevator. New hierarchical structure of the AI control system includes direct human-robot communication line based on natural language and cognitive graphics, and a generator of virtual reality for simulation of artificial life conditions for the mobile service robot. Simulation and experimental results of navigation and technical operations with the manipulator mobile service robot used in office building are described.",https://ieeexplore.ieee.org/document/649083/,Proceedings of the 1997 IEEE/RSJ International Conference on Intelligent Robot and Systems. Innovative Robotics for Real-World Applications. IROS '97,11-11 Sept. 1997,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICIPS.1997.669208,A neural network approach to the elimination of road shadow for outdoor mobile robot,IEEE,Conferences,"A new method of road tracking oriented environmental noise elimination is presented for implementing navigation and control of land autonomous vehicles (ALV). The concept of vision based environmental noise is firstly introduced for the purpose of road and/or obstacle edge detection. Then, a representation of pyramid is proposed for vision processing. Furthermore, a fuzzy neural network is designed and implemented to recognize the environmental noises such as shadow and water prints on the road. With structure optimization by genetic algorithm and special training by classified samples, we use the network to guide our THMR-III (Tsinghua University Mobile Robot, Model 3) in the outdoor real world. Experiments have shown good properties for the ALV's ""perception-action"" behaviors, including obstacle avoidance, road following, wandering, etc. Although the work is still going on, we can see from the present results the better quality, adaptability and robustness of the above approach.",https://ieeexplore.ieee.org/document/669208/,1997 IEEE International Conference on Intelligent Processing Systems (Cat. No.97TH8335),28-31 Oct. 1997,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IACC.1995.465839,A neural network system that controls and plans paths for a robot,IEEE,Conferences,"Proposes to solve the problems of direct/inverse kinematics and control of trajectories by multilevel perceptrons. The authors' solution admits a parallel implementation in real time. It does not need either to solve kinematic equations or robot trajectories, because it learns gradually by examples adaptively. The control system consists of different networks each of which specialises in solving a particular problem. This structure enables a modular approach to the problem accelerating convergence. The system obtains an acceptable trajectory and gives a parallel solution that could be used in real-time applications.<>",https://ieeexplore.ieee.org/document/465839/,Proceedings of IEEE/IAS International Conference on Industrial Automation and Control,5-7 Jan. 1995,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICSMC.1995.538227,A neural network-based robot safety system,IEEE,Conferences,"This paper presents a new approach for real-time robot safety system based on artificial neural networks. This approach includes a neural network detection unit and a neural network decision unit, implemented at an intermediate and high level of sensory processing, respectively. Both the detection and decision units have been implemented and tested by simulation, both separately and as an integrated unit. The response time of the integrated system measured on the 90 MHz, P5 microprocessor is less than 11 ms, and the correctness of safety decisions is 97%.",https://ieeexplore.ieee.org/document/538227/,"1995 IEEE International Conference on Systems, Man and Cybernetics. Intelligent Systems for the 21st Century",22-25 Oct. 1995,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1049/cp:19990285,A neural vision based controller for a robot footballer,IET,Conferences,"Robot football is growing in popularity both as a research topic and as a sporting event. The football setting provides rich interaction possibilities and a ready source of competition in an environment containing both predictable and non-deterministic elements. Successful players must be able to react quickly in real time, exhibit multiple competences and choose between several possibly conflicting goals. Opportunities exist to explore reflexive behaviour, strategic behaviour and even communication and social behaviour in team events. At the same time, artificial neural networks are increasingly being used in robot controllers to explore new biologically-inspired ideas relating to perception, memory and motor control. The research described in this paper attempts to combine these two areas of study to produce a framework for a neurally based and visually guided football-playing controller. A controller architecture is proposed in which a small set of high-level features in the robot's environment are extracted from raw image data by using a feedforward neural network. These feature signals, collectively termed the ""feature bus"", are then available for use by other controller modules. The feature bus signals are sufficiently general and high-level to be used with many different controller strategies, and their low dimensionality compared to the raw visual input makes the implementation of learning controllers more feasible.",https://ieeexplore.ieee.org/document/791354/,"Image Processing And Its Applications, 1999. Seventh International Conference on (Conf. Publ. No. 465)",13-15 July 1999,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IJCNN.2015.7280750,A neurocomputational model implemented on humanoid robot for learning action selection,IEEE,Conferences,"Computational modeling of neural circuits enhances our comprehension of brain functions. In addition to the simulation of the models which helps to anticipate cognitive processes, embodiment of these models is essential. Such embodiment would provide the setting to explain neural functioning ongoing in real environments under oncoming sensory information besides giving opportunity of implementation of intelligent systems. Even studies pursued in neuroscience seem far from achieving all these aims in intelligent systems, the pre-results using cognitive models are faster than animal experiments in leading further the understanding of cognitive processes and designing related experiments. In this study, a computational model of basal ganglia, thalamus and cortex for action selection is extended with the point neuron approach to obtain a more realistic method to investigate the model in real time task on humanoid robot platform, Darwin-Op. The spiking neural network model of cortex consists of channels for each action to be elected and plastic alI-to-alI connections from the sensory stimuli to the basal ganglia structures which are modulated with reward. In the task, the sensory inputs, namely colors, are presented to the humanoid robot and it is expected that these sensory inputs would be associated with the predefined actions by modulating the connections. Furthermore, the rearrangement of these associations with reward is performed after learning is accomplished. In this way, the embodiment of computational-model provided more information on the evolution of connections through reward based learning in the action selection circuit.",https://ieeexplore.ieee.org/document/7280750/,2015 International Joint Conference on Neural Networks (IJCNN),12-17 July 2015,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICMLC.2011.6016883,A novel intelligent control system design for a humanoid robot,IEEE,Conferences,"This paper presents the design of an intelligent control system for a humanoid robot. A novel fuzzy cerebellar model articulation controller (FCMAC) is proposed; this controller incorporates the fuzzy system inference rule with a CMAC fast learning ability. This FCMAC is a generalization network; in some special cases it can be reduced to a fuzzy neural network or a CMAC. This FCMAC is used as the main controller for the trajectory tracking control of the robot. In this robotic system, an inertial navigation system (INS) including gyroscopes and accelerometers is used to measure the robot's attitude and acceleration for modifying the dynamic attitude of the robot. Moreover, a zero moment point (ZMP) compensator is used to on-line adjust the gait trajectories to improve the walking stability. The control system is implemented based on system on a programmable chip (SoPC) technology. Thus, this intelligent control system can achieve real-time on-line closed-loop feedback control of the humanoid robot. Experimental results show that the developed system can achieve favorable control performance for a high-order nonlinear humanoid robot.",https://ieeexplore.ieee.org/document/6016883/,2011 International Conference on Machine Learning and Cybernetics,10-13 July 2011,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/PEAM.2011.6135058,A novel on-line training solution using a Radial Basis Function Network to modify the inverse kinematic approximation of a robot-vision system,IEEE,Conferences,"This paper describes a new practical approach for approximating the inverse kinematics of a manipulator using an RBFN (Radial Basis Function Network). This neural network with its inherent learning ability can be an effective alternative solution for the inverse kinematics problem where traditional methods are impractical because the manipulator geometry cannot be easily determined, e.g. in a robot-vision system. However, sometimes a well-trained network cannot work effectively in the operational phase because the initial network training occurs in an environment that is not exactly the same as the environment where the system is actually deployed. In this paper, an on-line retraining solution using the Delta rule is presented for systems whose characteristics change due to environmental variations. Moreover, a “free interference rule” is also suggested to avoid learning interference where the training effect of a current training point may upset some of the weights which were trained with previous points. To verify the performance of the proposed approach, a practical experiment has been performed using a Mitsubishi PA10-6CE manipulator observed by a webcam. All application programmes, such as robot servo control, neural network, and image processing tool, were written in C/C++ and run in a real robotic system. The experimental results prove that the proposed approach is effective.",https://ieeexplore.ieee.org/document/6135058/,2011 IEEE Power Engineering and Automation Conference,8-9 Sept. 2011,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICSMC.1999.812484,A paradigm for intelligent motion planning of robot manipulators,IEEE,Conferences,"A paradigm for intelligent motion planning of robot manipulators is presented, and its implementation by some unconventional (AI, soft computing, computational intelligence) techniques is outlined. The article focuses on those aspects that lead, from an intelligent/unconventional point of view, to a unified framework for real-time motion planning, singularities prevention, and/or pseudoinverse robustness.",https://ieeexplore.ieee.org/document/812484/,"IEEE SMC'99 Conference Proceedings. 1999 IEEE International Conference on Systems, Man, and Cybernetics (Cat. No.99CH37028)",12-15 Oct. 1999,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS.1998.727453,A real-time library for the design of hybrid robot control architectures,IEEE,Conferences,"Describes a real-time library providing facilities useful in the design of robot control architectures. The library supports structured creation of reactive and deliberative modules, dynamic modification of relevant real-time parameters, generation of timing fault handlers, measurement and monitoring of execution times. This support enables adaptation of the rate of computation of real-time modules to the rate of change of the external world, and hence better tuning of robot behavior to the world uncertainty and dynamics. The real-time library has been put to work by designing a hybrid control architecture for a robot performing a kitting task. In the prototype experiment described in the paper, a Puma 560 robot manipulator is fed with parts by a small mobile robot. The control architecture governing Puma operations dynamically allocates computational resources to reactive and deliberative modules, according to the task level priorities.",https://ieeexplore.ieee.org/document/727453/,"Proceedings. 1998 IEEE/RSJ International Conference on Intelligent Robots and Systems. Innovations in Theory, Practice and Applications (Cat. No.98CH36190)",17-17 Oct. 1998,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/TENCON.2000.888756,A review of real-time software engineering methodologies for developing a wall-climbing robot control firmware,IEEE,Conferences,"Designing and developing software for autonomous robot control system is a challenging task. Issues related to real-time control, embedded system and artificial intelligence are involved in the software development process. This type of software must be developed with proper software methodology or well-defined development process in order to increase the productivity and quality of the software design and software products. This paper presents a review of two real-time software development methodologies and compares their suitability for developing real-time control software for a wall-climbing robot under development at Universiti Teknologi Malaysia (UTM).",https://ieeexplore.ieee.org/document/888756/,2000 TENCON Proceedings. Intelligent Systems and Technologies for the New Millennium (Cat. No.00CH37119),24-27 Sept. 2000,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1049/cp.2012.0975,A robot dance system based on real-time beat prediction,IET,Conferences,"In this paper, we present a robot with a system of tracking beats and downbeats from musical audio in real time as well as automatically synthesizing dance motions synchronized to the extracted musical events. And a real-time beat prediction approach improved by an off-line analysis is described, which has positive effects on the estimations of beat and downbeat. We also make an attempt to solve the problem of enabling a robot to understand the played music and accomplish dance compositions intelligently itself. Consequently, the experimental results are quite encouraging and show that our implemented robot, to some extent, has the ability of dancing in time to the music.",https://ieeexplore.ieee.org/document/6492582/,International Conference on Automatic Control and Artificial Intelligence (ACAI 2012),3-5 March 2012,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/MFI-2003.2003.1232590,A robust real time position and force (hybrid) control of a robot manipulator in presence of uncertainties,IEEE,Conferences,"We examine the living intelligent biological systems and model the computational system components. We consider the situation of a kind of ""blind-tracking"" with constant force/torque by a human hand. The problem involves hand kinematics, hand motor control, and an adaptive judgment method from the position and force/torque reflection of the uncertain hyper plane. In this study, these control levels were designed using neural networks and fuzzy logic technologies. The control levels are coordinated amongst themselves forming the distributed artificial intelligent (DAI) system. The conclusive characteristic of the proposed controller was a one-step-ahead feedback control. This DAI-based control systems was implemented in the RX-90 industrial robot. Certainly these types of control system will help an industry to be autonomous and increase the productivity as well.",https://ieeexplore.ieee.org/document/1232590/,"Proceedings of IEEE International Conference on Multisensor Fusion and Integration for Intelligent Systems, MFI2003.",1-1 Aug. 2003,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/WCICA.2000.863460,A study on a new robot simulation and monitoring system based on PC,IEEE,Conferences,"A new robot simulation and monitoring system has been developed. Running on Pentium II PCs with Windows 95 operation system and being developed with OpenGL, this system has the capacity of real-time simulating the motion of an industrial robot through 3D animation with ray tracing. Connected with the robot controller via network, users can monitor the behavior of the robot dynamically or even directly control the action of the robot if necessary. By comparing with the traditional off-line programming system, the framework and the functions of this system and the hardware and software platform of the system are described. The principle of 3D-motion simulation and both the geometry modeling and kinematics modeling are discussed in more detail. Finally, the paper summarizes the characteristics of the system and discusses the extension prospect of the system.",https://ieeexplore.ieee.org/document/863460/,Proceedings of the 3rd World Congress on Intelligent Control and Automation (Cat. No.00EX393),26 June-2 July 2000,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS.2004.1389844,Acquisition of human-robot joint attention through real-time natural interaction,IEEE,Conferences,"Joint attention, a process to attend to the object that the other attends to is supposed to be important for human-robot communication as well as for human-human communication. We propose an architecture for acquiring joint attention within a certain time period for realizing natural human-robot interaction. The architecture has two featured modules: a self-organizing map that makes the leaning time shorter and an automatic visual attention selector that let the agent communicate with a human synchronously. We implemented the proposed architecture in a real robot agent and found that 30 minutes was enough for acquiring joint attention with two objects. We can conclude from preliminary experiments that even if the gaze preference of the robot is different from that of the human caregiver, it can acquire joint attention.",https://ieeexplore.ieee.org/document/1389844/,2004 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) (IEEE Cat. No.04CH37566),28 Sept.-2 Oct. 2004,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/DEVLRN.2002.1011736,Action chaining by a developmental robot with a value system,IEEE,Conferences,"A developmental cognitive learning architecture with a value system is proposed for an artificial agent to learn composite behaviors upon the acquisition of basic ones. This work is motivated by researches on classical conditioning in animal learning areas. Compared to former works, the proposed architecture enables an agent to conduct learning in unknown environments through online realtime experiences. All possible perceptions and actions, including even the actual number of classes, are not available until the programming is finished and the robot starts to learn in the real world. Experiments with our SAIL (Self-organizing Autonomous Incremental Learner) robot are reported to show how a trainer instructed (or shaped) the behaviors of the agent through verbal commands.",https://ieeexplore.ieee.org/document/1011736/,Proceedings 2nd International Conference on Development and Learning. ICDL 2002,12-15 June 2002,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/FUZZ-IEEE.2014.6891705,Active interaction control of a rehabilitation robot based on motion recognition and adaptive impedance control,IEEE,Conferences,"Although electromyography (EMG) signals and interaction force have been widely used in patient cooperative or interactive training, the conventional EMG based control usually breaks the process into a patient-driven phase and a separate passive phase, which is not desirable. In this research, an active interaction controller based on motion recognition and adaptive impedance control is proposed and implemented on a six-DOFs parallel robot for lower limb rehabilitation. The root mean square (RMS) features of EMG signals integrating with the support vector machine (SVM) classifier were used to online predict the lower limb intention in advance and to trigger the robot assistance. The impedance control strategy was adopted to directly influence the robot assistance velocity and allow the exercise to follow a physiological trajectory. Moreover, an adaptive scheme learned the muscle activity level in real time and adapted the robot impedance in accordance with patient's voluntary participation efforts. Experimental results on several healthy subjects demonstrated that the lower limb motion intention can be precisely predicted in advance, and the robot assistance mode was also adjustable based on human-robot interaction and muscle activity level of subjects. Comparing with the conventional EMG-triggered assistance methods, such a strategy can increase patient's motivation because the subject's movement intention, active efforts as well as the muscle activity level changes can be directly reflected in the trajectory pattern and the robot assistance speeds.",https://ieeexplore.ieee.org/document/6891705/,2014 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE),6-11 July 2014,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRoM.2014.6990997,Actor-critic neural network reinforcement learning for walking control of a 5-link bipedal robot,IEEE,Conferences,"Today, researches on adaptive control have focused on bio-inspired learning techniques to deal with real-life applications. Reinforcement Learning (RL) is one of these major techniques, which has been widely used in robot control tasks recently. On the other hand, artificial neural networks are an accurate approximation tool in nonlinear robotic dynamic control tasks. In this paper, our main goal was to combine the advantages of the artificial neural networks and the RL to reduce the learning time length and enhance the control accuracy. Therefore, we have implemented one of the promising RL approaches, actor-critic RL to control the actuation torques of a planar five-link bipedal robot and retain the passive torso in the vertical position. Our control agent consists of two three-layered neural network units, known as the critic and the actor for learning prediction and learning control tasks. These units are synchronized by the temporal difference error, which implements the eligibility trace vector to assign credit or blame for the error. Moreover, since the neural networks are implemented in both of the actor and the critic sections, we have added a learning database to reduce the probability of inaccurate approximation of the nonlinear functions. Results of our presented control method reveal its perfect performance in stable walking control of the bipedal robot.",https://ieeexplore.ieee.org/document/6990997/,2014 Second RSI/ISM International Conference on Robotics and Mechatronics (ICRoM),15-17 Oct. 2014,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/M2VIP.2017.8211476,Actuation planning and modeling of a soft swallowing robot,IEEE,Conferences,"The paper presents a new methodology to solve the actuation and modelling problems of a soft-bodied swallowing robot (SR), developed for human swallow evaluation. To solve the actuation problem, a central pattern generator (CPG) based novel actuation scheme is developed and implemented to generate peristalsis in the robot. Machine learning based technique is used to determine the governing dynamics of the robot because presently the robot does not have any differential equation to describe its actuation principle or its physics. To profile and sense the peristaltic waveform, a flat version of the robot containing pneumatic chambers for actuation has been proposed to approximate the deformation of the original SR and the CPG actuation scheme is used to command the flat SR so that the pneumatic chambers can be inflated. The logic of actuation is motivated from the swallowing phenomenon in humans, have been implemented in real time. An optical motion detection system (Vicon) is used to track the displacement of the air chambers of the robot and hence, to generate time-series data for determining the governing differential equations of the robot by using l1 regularised machine learning technique. It is also concluded that the proposed method provides a promising new modelling technique for determining the governing dynamics of the robot where conventional modelling approaches are not applicable.",https://ieeexplore.ieee.org/document/8211476/,2017 24th International Conference on Mechatronics and Machine Vision in Practice (M2VIP),21-23 Nov. 2017,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICIT.2002.1189963,Adaptive linear robot control for tracking and grasping a dynamic object,IEEE,Conferences,"Versatile vision systems are being employed increasingly for automated visual feedback intelligent robotic control to perform complex manufacturing tasks especially for tracking and grasping dynamic objects a the conveyor by generating the optimum-tracking trajectory for the robot. Currently automated visual feedback robot systems use vision and robot system as separate tools. The best solution for these kind of problems still can not be implemented successfully with ease. Therefore, we tried to develop a new single adaptive prediction and execution algorithm for the picking and placing of dynamic objects in real time in a robot work-cell by integrating a CCD camera with frame grabber, artificial neural network, an optical flow technique and an industrial robot into a single application. The implementation of this proposed system is done only in one dimension because of the time constraint. After the robot learning process, it is shown that a KUKA robot is capable of adaptively tracking and intercepting dynamic objects at an optimal rendezvous point on the conveyor accurately in real time.",https://ieeexplore.ieee.org/document/1189963/,"2002 IEEE International Conference on Industrial Technology, 2002. IEEE ICIT '02.",11-14 Dec. 2002,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICIT.1996.601644,Adaptive robust robot control using BP-SMENs,IEEE,Conferences,"This paper presents the development of a new adaptive recurrent neural network for the control of a nonlinear system represented by a two-link SCARA type planar robot manipulator. The standard backpropagation algorithm is used to adjust the weights of the networks. The proposed control system consists of an inverse neural model of robot (INNM), an INNM-based neural controller, a robust controller, a conventional PI controller, and a second order linear filter. To evaluate the performance of the proposed control scheme and neural network, a simulated SCARA type robot was studied and the results showed how well the proposed controller can minimise the error between an actual and desired end-effector trajectory. From simulation examples, the robot trajectory tracking showed superior performance that is very attractive for real-time implementation and application in complex industrial tasks. For comparison, the standard computed torque method is employed for controlling the robot.",https://ieeexplore.ieee.org/document/601644/,Proceedings of the IEEE International Conference on Industrial Technology (ICIT'96),2-6 Dec. 1996,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/TAI.1994.346494,Advanced fuzzy control of a trailer type mobile robot-stability analysis and model-based fuzzy control,IEEE,Conferences,"In a previous paper (see ""A Robust Stabilization Problem of Fuzzy Controller Systems and Its Applications to Backing up Control of a Truck-Trailer"", IEEE Trans. on Fuzzy Systems, Vo1.2, no.2, p.119-34, 1994), we designed a control system for backing up a computer simulated trailer type mobile robot, which is non-linear and unstable, by applying a robust stabilization technique for fuzzy systems. Furthermore, we have shown that the designed fuzzy controller smoothly achieves backing up control of the computer simulated trailer type mobile robot from all initial positions. We control a real trailer type mobile robot by applying the design method proposed previously. The experimental results show that the designed fuzzy controller effectively realize backing up control of the real trailer type mobile robot.<>",https://ieeexplore.ieee.org/document/346494/,Proceedings Sixth International Conference on Tools with Artificial Intelligence. TAI 94,6-9 Nov. 1994,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA40945.2020.9196582,Adversarial Skill Networks: Unsupervised Robot Skill Learning from Video,IEEE,Conferences,"Key challenges for the deployment of reinforcement learning (RL) agents in the real world are the discovery, representation and reuse of skills in the absence of a reward function. To this end, we propose a novel approach to learn a task-agnostic skill embedding space from unlabeled multi-view videos. Our method learns a general skill embedding independently from the task context by using an adversarial loss. We combine a metric learning loss, which utilizes temporal video coherence to learn a state representation, with an entropy-regularized adversarial skill-transfer loss. The metric learning loss learns a disentangled representation by attracting simultaneous viewpoints of the same observations and repelling visually similar frames from temporal neighbors. The adversarial skill-transfer loss enhances re-usability of learned skill embeddings over multiple task domains. We show that the learned embedding enables training of continuous control policies to solve novel tasks that require the interpolation of previously seen skills. Our extensive evaluation with both simulation and real world data demonstrates the effectiveness of our method in learning transferable skills from unlabeled interaction videos and composing them for new tasks. Code, pretrained models and dataset are available at http://robotskills.cs.uni-freiburg.de.",https://ieeexplore.ieee.org/document/9196582/,2020 IEEE International Conference on Robotics and Automation (ICRA),31 May-31 Aug. 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA48506.2021.9562117,Agile Robot Navigation through Hallucinated Learning and Sober Deployment,IEEE,Conferences,"Learning from Hallucination (LfH) is a recent machine learning paradigm for autonomous navigation, which uses training data collected in completely safe environments and adds numerous imaginary obstacles to make the environment densely constrained, to learn navigation planners that produce feasible navigation even in highly constrained (more dangerous) spaces. However, LfH requires hallucinating the robot perception during deployment to match with the hallucinated training data, which creates a need for sometimes-infeasible prior knowledge and tends to generate very conservative planning. In this work, we propose a new LfH paradigm that does not require runtime hallucination—a feature we call ""sober deployment""—and can therefore adapt to more realistic navigation scenarios. This novel Hallucinated Learning and Sober Deployment (HLSD) paradigm is tested in a benchmark testbed of 300 simulated navigation environments with a wide range of difficulty levels, and in the real-world. In most cases, HLSD outperforms both the original LfH method and a classical navigation planner.",https://ieeexplore.ieee.org/document/9562117/,2021 IEEE International Conference on Robotics and Automation (ICRA),30 May-5 June 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISCSIC.2017.28,An Adaptive 2D Tracking Approach for Person Following Robot,IEEE,Conferences,"In this paper, we present a 2D appearance vision based tracking approach for human following robot. Generally, existing methods have high cost of computing and requirements of hardware which makes the difficulty of employing the function on service robot. Hence, minimizing the cost of tracking with slight loss of precision can benefit this area. We focus on approach based on 2D image data which reduce tracking into two dimensions and can minimize the cost of computation. Our approach presents a corporate strategy which utilizes central consensus of correspondence for 2D feature points pairwise to track and employ a semi-supervised learning detector to update appearance change. To overcome difficulties from environment change, we set an enhancing process for feature points and background segmentation through depth information. We carefully evaluate our approach with common challenges of visual tracking in static view and deploy a dynamic view a real-world following task. The experiment results illustrate that our tracking approach works against common risks at 2D appearance tracking and properly follows the user obtaining 25 fps performance on mobile platform.",https://ieeexplore.ieee.org/document/8294176/,2017 International Symposium on Computer Science and Intelligent Controls (ISCSIC),20-22 Oct. 2017,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ROMAN.2018.8525668,An Ontology-based Home Care Service Robot for Persons with Dementia,IEEE,Conferences,"In this paper, we introduce an ontology-based home care service robot that can provide personalized care for people who are in the early stage of dementia. The hardware and software framework encompassed in the proposed service robot was developed to carry out care services in their daily life at home. Specifically, to generate adaptive task plans in diverse caring situation, context reasoner and ontological model of dementia are included. Ontology includes various concepts that are related with the knowledge of caring dementia patient: dementia, dementia symptom, environment of around patient, and situation during patient's daily life. To evaluate if the proposed service robot could provide appropriate care service or not, experimental care scenario for helping a person with dementia take medicine was tried in the lab environment. Although tasks of the robot required for the experiment are rather simple, we have demonstrated that the robot could provide a personalized service that may be beneficial to dementia patient, family members and caregivers. In the future, we will add more care knowledge in the ontology and further develop a variety of care services. Additionally, we are going to test the care service robot in a real environment with actual dementia patient.",https://ieeexplore.ieee.org/document/8525668/,2018 27th IEEE International Symposium on Robot and Human Interactive Communication (RO-MAN),27-31 Aug. 2018,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ROMAN.2012.6343892,An active audition framework for auditory-driven HRI: Application to interactive robot dancing,IEEE,Conferences,"In this paper we propose a general active audition framework for auditory-driven Human-Robot Interaction (HRI). The proposed framework simultaneously processes speech and music on-the-fly, integrates perceptual models for robot audition, and supports verbal and non-verbal interactive communication by means of (pro)active behaviors. To ensure a reliable interaction, on top of the framework a behavior decision mechanism based on active audition policies the robot's actions according to the reliability of the acoustic signals for auditory processing. To validate the framework's application to general auditory-driven HRI, we propose the implementation of an interactive robot dancing system. This system integrates three preprocessing robot audition modules: sound source localization, sound source separation, and ego noise suppression; two modules for auditory perception: live audio beat tracking and automatic speech recognition; and multi-modal behaviors for verbal and non-verbal interaction: music-driven dancing and speech-driven dialoguing. To fully assess the system, we set up experimental and interactive real-world scenarios with highly dynamic acoustic conditions, and defined a set of evaluation criteria. The experimental tests revealed accurate and robust beat tracking and speech recognition, and convincing dance beat-synchrony. The interactive sessions confirmed the fundamental role of the behavior decision mechanism for actively maintaining a robust and natural human-robot interaction.",https://ieeexplore.ieee.org/document/6343892/,2012 IEEE RO-MAN: The 21st IEEE International Symposium on Robot and Human Interactive Communication,9-13 Sept. 2012,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICSMC.2009.5346800,An embedded interval type-2 neuro-fuzzy controller for mobile robot navigation,IEEE,Conferences,"This paper describes intelligent navigation using an embedded interval type-2 neuro-fuzzy controller. Weightless neural network (WNNs) strategy is used because fast learning, easy hardware implementation and well suited to microcontroller-based-real-time systems. The WNNs utilizes previous sensor data and analyzes the situation of the current environment and classifies geometric feature such as U-shape, corridor and left or right corner. The behavior of mobile robot is implemented by means of interval type-2 fuzzy control rules can be generated directly from the WNNs classifier. This functionality is demonstrated on a mobile robot using modular platform and containing several microcontrollers implies the implementation of a robust architecture. The proposed architecture implemented using low cost range sensor and low cost microprocessor. The experiment results show, using that technique the source code is efficient. The mobile robot can recognize the current environment and to be able successfully avoid obstacle in real time and achieve smother motion compare than logic function and fuzzy type-1 controller.",https://ieeexplore.ieee.org/document/5346800/,"2009 IEEE International Conference on Systems, Man and Cybernetics",11-14 Oct. 2009,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CIRA.1997.613874,An evolutionary method for active learning of mobile robot path planning,IEEE,Conferences,"Several evolutionary algorithms have been proposed for robot path planning. Most existing methods for evolutionary path planning require a number of generations for finding a satisfactory trajectory and thus are not efficient enough for real-time applications. In this paper we present a new method for evolutionary path planning which can be used online in real-time. We use an evolutionary algorithm as a means for active learning of a route map for the path planner. Given a source-destination pair, the path planner searches the map for a best matching route. If an acceptable match is not found, the planner uses another evolutionary algorithm to generate online a path for the source-destination pair. The overall system is an incremental learning planner that gradually expands its own knowledge suitable for path planning in real-time. Simulations have been performed in the domain of robotic soccer to demonstrate the effectiveness of the presented method.",https://ieeexplore.ieee.org/document/613874/,Proceedings 1997 IEEE International Symposium on Computational Intelligence in Robotics and Automation CIRA'97. 'Towards New Computational Principles for Robotics and Automation',10-11 July 1997,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CNNA.2010.5430339,An on-line test setup of CNN based real-time mobile robot navigation application,IEEE,Conferences,"In this demo, we introduced a mobile robot navigation test setup that is accessible via internet. This test setup has a Lego Mindstorms NXT differential drive mobile robot which is controlled by a computer with Bluetooth connection. The control computer, which is called Host Computer in Fig. 1, runs MATLAB to operate the system. The navigation of mobile robot is based on an algorithm which uses a Relaxation Oscillator Cellular Neural/Nonlinear Network to execute wave computing. This network is also simulated by the Host Computer. The significant property of this demo is that any Client Computer can establish a remote desktop connection with the Host Computer and use the test setup remotely. By this means, any comparative tests on this setup can be handled remotely by independent researchers.",https://ieeexplore.ieee.org/document/5430339/,2010 12th International Workshop on Cellular Nanoscale Networks and their Applications (CNNA 2010),3-5 Feb. 2010,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ROBOT.1992.220085,An optimal scheduling of pick place operations of a robot-vision-tracking system by using back-propagation and Hamming networks,IEEE,Conferences,"The authors present a neural network approach to solve the dynamic scheduling problem for pick-place operations of a robot-vision-tracking system. An optimal scheduling problem is formulated to minimize robot processing time without constraint violations. This is a real-time optimization problem which must be repeated for each group of objects. A scheme which uses neural networks to learn the mapping from object pattern space to optimal order space offline and to recall online what has been learned is presented. The idea was implemented in a real system to solve a problem in large commercial dishwashing operations. Experimental results have been shown that with four different objects, time savings of up to 21% are possible over first-come, first-served schemes currently used in industry.<>",https://ieeexplore.ieee.org/document/220085/,Proceedings 1992 IEEE International Conference on Robotics and Automation,12-14 May 1992,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICAR53236.2021.9659393,Analytical Solution for the Inverse Kinematics Problem of the Franka Emika Panda Seven-DOF Light-Weight Robot Arm,IEEE,Conferences,"The redundant light-weight robot Franka Emika Panda is omnipresent in modern research like human-robot interaction or machine learning. Much of that research requires to solve the inverse kinematics problem (IKP) properly. This paper presents an analytical solution for the IKP based on the geometry of the manipulator&#x0027;s kinematic structure. It also provides information on how the redundancy is resolved and how singularities are handled. The whole procedure is a concatenation of intersection and closest point problems of two or three dimensional objects, for which at least basic solutions already exist. Consequently, the approach is lightweight in calculation and, in addition, very robust when it comes to finding solutions for rare edge cases. On the top of that, the simple nature of calculations make the approach utilizable in a real-time safe fashion. Hence, an implementation in C++ is given as open source library, which also can be used to validate the results of this paper.",https://ieeexplore.ieee.org/document/9659393/,2021 20th International Conference on Advanced Robotics (ICAR),6-10 Dec. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.23919/IConAC.2019.8895095,Ant Colony Optimization Algorithm for Industrial Robot Programming in a Digital Twin,IEEE,Conferences,"Advanced manufacturing that is adaptable to constantly changing product designs often requires dynamic changes on the factory floor to enable manufacture. The integration of robotic manufacture with machine learning approaches offers the possibility to enable such dynamic changes on the factory floor. While ensuring safety and the possibility of losses of components and waste of material are against their usage. Furthermore, developments in design of virtual environments makes it possible to perform simulations in a virtual environment, to enable human-in-the-loop production of parts correctly the first time like never before. Such powerful simulation and control software provides the means to design a digital twin of manufacturing environment in which trials are completed at almost at no cost. In this paper, ant colony optimization is used to program an industrial robot to avoid obstacles and find its way to pick and place objects during an assembly task in an environment containing obstacles that must be avoided. The optimization is completed in a digital twin environment first and movements transferred to the real robot after human inspection. It is shown that the proposed methodology can find the optimal solution, in addition to avoiding collisions, for an assembly task with minimum human intervention.",https://ieeexplore.ieee.org/document/8895095/,2019 25th International Conference on Automation and Computing (ICAC),5-7 Sept. 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AICI.2009.305,Application of RBF Neural Network in Trajectory Planning of Robot,IEEE,Conferences,"Trajectory planning of robot is to control the robot in order to accurately follow the target track. And the target trajectory is always high-order and nonlinear. But RBF neural network can be achieved from the input to the output of arbitrary nonlinear mapping, through network learning and training to achieve the nonlinear function. This paper establishes a RBF neural network model firstly, and carries on the simulation through software MATLAB. The result confirms that the RBF neural network can keep the control of the robot's nonlinear trajectory planning in real time.",https://ieeexplore.ieee.org/document/5375883/,2009 International Conference on Artificial Intelligence and Computational Intelligence,7-8 Nov. 2009,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IJCNN.1990.137866,Application of neural networks on robot grippers,IEEE,Conferences,"A new-generation general-purpose robot gripper system which applies an artificial neural network to guide a three-finger gripper has been designed. The simulation of the core part of the whole system, i.e. optimally placing three fingers for a stable grasp using the Hopfield net, has been conducted. The results obtained show that this scheme behaves in a promising fashion. The actual computation time is usually within several seconds if implemented in an analog neural net, making the real application attractive",https://ieeexplore.ieee.org/document/5726824/,1990 IJCNN International Joint Conference on Neural Networks,17-21 June 1990,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CDC.1999.833361,Application of reinforcement learning control to a nonlinear dexterous robot,IEEE,Conferences,"In this paper, the effects of basic parameters in reinforcement learning control such as eligibility, action and critic network weights, system nonlinearities, gradient information, state-space partitioning, variance of exploration were studied in detail. We attempt to increase feasibility for practical applications, implementation, learning efficiency, and performance. Reinforcement learning is then applied for control of a nonlinear dexterous robot. This control problem dictates that the learning is performed online, based on binary and real valued reinforcement signal from a critic network, without knowing the system model nonlinearity. The learning algorithm consists of an action and critic networks that learn to keep the multifinger hand of the dexterous robot within desired limits.",https://ieeexplore.ieee.org/document/833361/,Proceedings of the 38th IEEE Conference on Decision and Control (Cat. No.99CH36304),7-10 Dec. 1999,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IJCNN.2004.1380179,Applying KIV dynamic neural network model for real time navigation by mobile robot EMMA,IEEE,Conferences,"We use a biologically inspired dynamic neural network model to accomplish goal-oriented navigation by a mobile robot in a real environment with obstacles. This model is the KIV model of the brain. Real time navigation is a challenging task, especially when there is no a priori information about the environment. Our robot EMMA is designed to be autonomous using various sensory inputs, which are integrated to achieve an efficient navigation task. This paper focuses on the design, implementation, and evaluation of the performance of EMMA and gives a proof-of-principle in a real environment.",https://ieeexplore.ieee.org/document/1380179/,2004 IEEE International Joint Conference on Neural Networks (IEEE Cat. No.04CH37541),25-29 July 2004,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICONIP.1999.845675,Artificial neural networks for autonomous robot control: reflective navigation and adaptive sensor calibration,IEEE,Conferences,"The authors present the application of artificial neural networks to the control of a mobile autonomous robot, which is acting in a totally unknown and-most importantly-dynamically changing environment. In particular, the employment of interacting 'simple', i.e. hand-designed, neural networks for navigation purposes is investigated as well as a variation of self-organizing maps for adaptive sensor calibration. We take a pragmatic point of view as the minimal condition imposed on the developed algorithms: that they do well on a real system acting in a real environment. Hence, the design of all of the implemented neural networks is clearly motivated by their applicability. In this context, special considerations are dedicated to ensure robustness, real-time capability and memory resourcefulness. In order to practically demonstrate the obtained results, the mini-robot Khepera is utilized as an experimentational platform, which is (due to its small size), a versatile tool for scientific investigation.",https://ieeexplore.ieee.org/document/845675/,ICONIP'99. ANZIIS'99 & ANNES'99 & ACNN'99. 6th International Conference on Neural Information Processing. Proceedings (Cat. No.99EX378),16-20 Nov. 1999,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SIU.2015.7130068,Audio-visual human tracking for active robot perception,IEEE,Conferences,"In this paper, a multimodal system is designed in the form of an active audio-vision in order to improve the perceptual capability of a robot in a noisy environment. The system running in real-time consists of 1) audition modality, 2) a complementary vision modality and 3) motion modality incorporating intelligent behaviors based on the data obtained from both modalities. The tasks of audition and vision are to detect, localize and track a speaker independently. The aim of motion modality is to enable a robot to have intelligent and human-like behaviors by using localization results from the sensor fusion. The system is implemented on a mobile robot platform in a real-time environment and the speaker tracking performance of the fusion is confirmed to be improved compared to each of sensory modalities.",https://ieeexplore.ieee.org/document/7130068/,2015 23nd Signal Processing and Communications Applications Conference (SIU),16-19 May 2015,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IJCNN.2003.1223997,Automatic language acquisition by an autonomous robot,IEEE,Conferences,"There is no such thing as a disembodied mind. We posit that cognitive development can only occur through interaction with the physical world. To this end, we are developing a robotic platform for the purpose of studying cognition. We suggest that the central component of cognition is a memory which is primarily associative, one where learning occurs as the correlation of events from diverse inputs. We also posit that human-like cognition requires a well-integrated sensory-motor system, to provide these diverse inputs. As implemented in our robot, this system includes binaural hearing, stereo vision, tactile sense, and basic proprioceptive control. On top of these abilities, we are implementing and studying various models of processing, learning and decision making. Our goal is to produce a robot that will learn to carry out simple tasks in response to natural language requests. The robot's understanding of language will be learned concurrently with its other cognitive abilities. We have already developed a robust system and conducted a number or experiments on the way to this goal, some details of which appear in this paper. This is a first progress report of what we believe will be a long term project with significant implications.",https://ieeexplore.ieee.org/document/1223997/,"Proceedings of the International Joint Conference on Neural Networks, 2003.",20-24 July 2003,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS.2003.1250752,Autonomous behavior control architecture of entertainment humanoid robot SDR-4X,IEEE,Conferences,"In this paper we describe the autonomous behavior control architecture of SDR-4X, which serves to integrate multi-modal recognition and motion control technologies. We overview the entire software architecture of SDR-4X, which is composed of perception, short and long term memory, behavior control, and motion control parts. Regarding autonomous behavior control, we further focus on issues such as spontaneous behavior generation using a homeostasis regulation mechanism, and a behavior control/selection mechanism with tree-structured situated behavior modules. In the autonomous behavior control architecture, we achieve three basic requirements, which are the concurrent evaluation of the situation of each behavior module, concurrent execution of multiple behavior modules, and preemption (behavior interruption/resume capability). Using the autonomous behavior control architecture described, we demonstrate that SDR-4X can spontaneously and passively interact with a human.",https://ieeexplore.ieee.org/document/1250752/,Proceedings 2003 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2003) (Cat. No.03CH37453),27-31 Oct. 2003,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SYSOSE.2008.4724191,Autonomous navigation based on a Q-learning algorithm for a robot in a real environment,IEEE,Conferences,"This paper explores autonomous navigation and obstacle avoidance techniques based on Q-learning for a mobile robot in a real environment. The implemented algorithm focuses on simplicity and efficiency. The learning process takes place in both simulation and real world allowing the combination of a longer learning time in the simulator with a more accurate knowledge from the real world. After learning is completed in simulation and in the real world, the robot was able to navigate without hitting obstacles and able to generate control law for complex situations such as corners and small objects.",https://ieeexplore.ieee.org/document/4724191/,2008 IEEE International Conference on System of Systems Engineering,2-4 June 2008,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CARPI.2010.5624410,Autonomous navigation for underground energy line inspection robot,IEEE,Conferences,"This work proposes architecture of an inspection robot's navigation system, aiming at monitoring underground energy lines. This architecture is composed of two modules: i. feature extraction from environment; ii navigation approach. The feature extraction module is based on the use of the edge detector by Canny algorithm and Hough transform for identification of lines from images of environment to monitoring. The lines identified correspond to cable conformation inside the duct. This information will serve to help the navigation system. For the implementation of the navigation system two approaches were proposed: navigation based on artificial neural network and navigation based on PID control. The navigation architecture can be used in real or simulated scenarios, and it was tested in a simulated environment.",https://ieeexplore.ieee.org/document/5624410/,2010 1st International Conference on Applied Robotics for the Power Industry,5-7 Oct. 2010,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISIC.2002.1157759,Autonomous robot navigation based on fuzzy sensor fusion and reinforcement learning,IEEE,Conferences,"This paper presents the design and implementation of an autonomous robot navigation system for intelligent target collection in dynamic environments. A feature-based multi-stage fuzzy logic (MSFL) sensor fusion system is developed for target recognition, which is capable of mapping noisy sensor inputs into reliable decisions. The robot exploration and path planning are based on a grid map oriented reinforcement path learning system (GMRPL), which allows for long-term predictions and path adaptation via dynamic interactions with physical environments. In our implementation, the MSFL and GMRPL are integrated into a subsumption architecture for intelligent target-collecting applications. The subsumption architecture is a layered reactive agent structure that enables the robot to implement higher-layer functions including path learning and target recognition regardless of lower-layer functions such as obstacle detection and avoidance. Real-world application using a Khepera robot shows the robustness and flexibility of the developed system in dealing with robotic behavior such as target collecting in an ever-changing physical environment.",https://ieeexplore.ieee.org/document/1157759/,Proceedings of the IEEE Internatinal Symposium on Intelligent Control,30-30 Oct. 2002,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICARCV.2018.8581174,Bi-Manual Articulated Robot Teleoperation using an External RGB-D Range Sensor,IEEE,Conferences,"In this paper, we present an implementation of a bi-manual teleoperation system, controlled by a human through three-dimensional (3D) skeleton extraction. The input data is given from a cheap RGB-D range sensor, such as the ASUS Xtion PRO. To achieve this, we have implemented a 3D version of the impressive OpenPose package, which was recently developed. The first stage of our method contains the execution of the OpenPose Convolutional Neural Network (CNN), using a sequence of RGB images as input. The extracted human skeleton pose localisation in two-dimensions (2D) is followed by the mapping of the extracted joint location estimations into their 3D pose in the camera frame. The output of this process is then used as input to drive the end-pose of the robotic hands relative to the human hand movements, through a whole-body inverse kinematics process in the Cartesian space. Finally, we implement the method as a ROS wrapper package and we test it on the centaur-like CENTAURO robot. Our demonstrated task is of a box and lever manipulation in real-time, as a result of a human task demonstration.",https://ieeexplore.ieee.org/document/8581174/,"2018 15th International Conference on Control, Automation, Robotics and Vision (ICARCV)",18-21 Nov. 2018,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICMLC.2008.4620720,Bi-criteria joint torque minimization of redundant robot arms using LVI-based primal-dual neural network,IEEE,Conferences,"To diminish the discontinuity and divergence of infinity norm torque minimization scheme, a bi-criteria weighting scheme is proposed for online redundancy resolution of redundant robot arms. Such a scheme can easily be reformulated into a quadratic program (QP) subject to equality, inequality and bound constraints. To solve this QP problem online, a primal-dual dynamical-system solver is further presented based on linear variational inequalities (LVI). Compared to previous research work, the adopted QP-solver has simple piecewise-linear dynamics, and does not entail real-time matrix inversion. Computer simulations are performed based on a PUMA560 manipulator to verify the performance and effectiveness of the proposed torque-optimization method.",https://ieeexplore.ieee.org/document/4620720/,2008 International Conference on Machine Learning and Cybernetics,12-15 July 2008,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/BIOROB.2018.8487202,Bioinspired Adaptive Spiking Neural Network to Control NAO Robot in a Pavlovian Conditioning Task,IEEE,Conferences,"The cerebellum has a central role in fine motor control and in various neural processes, as in associative paradigms. In this work, a bioinspired adaptive model, developed by means of a spiking neural network made of thousands of artificial neurons, has been leveraged to control a humanoid NAO robot in real-time. The learning properties of the system have been challenged in a classic cerebellum-driven paradigm, the Pavlovian timing association between two provided stimuli, here implemented as a laser-avoidance task. The neurophysiological principles used to develop the model, succeeded in driving an adaptive motor control protocol with acquisition and extinction phases. The spiking neural network model showed learning behaviors similar to the ones experimentally measured with human subjects in the same conditioning task. The model processed in real-time external inputs, encoded as spikes, and the generated spiking activity of its output neurons was decoded, in order to trigger the proper response with a correct timing. Three long-term plasticity rules have been embedded for different connections and with different time-scales. The plasticities shaped the firing activity of the output layer neurons of the network. In the Pavlovian protocol, the neurorobot successfully learned the correct timing association, generating appropriate responses. Therefore, the spiking cerebellar model was able to reproduce in the robotic platform how biological systems acquire and extinguish associative responses, dealing with noise and uncertainties of a real-world environment.",https://ieeexplore.ieee.org/document/8487202/,2018 7th IEEE International Conference on Biomedical Robotics and Biomechatronics (Biorob),26-29 Aug. 2018,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IJCNN.2000.859467,Biologically inspired neural controllers for motor control in a quadruped robot,IEEE,Conferences,"This paper presents biologically inspired neural controllers for generating motor patterns in a quadruped robot. Sets of artificial neural networks are presented which provide 1) pattern generation and gait control, allowing continuous passage from walking to trotting to galloping, 2) control of sitting and lying down behaviors, and 3) control of scratching. The neural controllers consist of sets of oscillators composed of leaky-integrator neurons, which control pairs of flexor-extensor muscles attached to each joint. The networks receive sensory feedback proportional to the contraction of simulated muscles and to joint flexion. Similarly to what is observed in cats, locomotion can be initiated by either applying tonic (i.e. non-oscillating) input to the locomotion network or by sensory feedback from extending the legs. The networks are implemented in a quadruped robot. It is shown that computation can be carried out in real time and that the networks can generate the above mentioned motor behaviors.",https://ieeexplore.ieee.org/document/859467/,Proceedings of the IEEE-INNS-ENNS International Joint Conference on Neural Networks. IJCNN 2000. Neural Computing: New Challenges and Perspectives for the New Millennium,27-27 July 2000,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS.2006.281655,Brush Footprint Acquisition and Preliminary Analysis for Chinese Calligraphy using a Robot Drawing Platform,IEEE,Conferences,"A robot drawing platform supporting four degrees of freedom (x, y, z and z-rotation) of a brush-pen motion for studying Chinese painting and calligraphy has been operational in our laboratory. This paper describes the real-time capturing and data analysis of the brush footprint using the new hardware and software capabilities in the platform. They include a transparent drawing plate and an underneath camera system, together with projective rectification and video segmentation algorithms. Preliminary result of the footprint analysis and nonparametric modeling, and their applications to well-known Chinese calligraphy are demonstrated",https://ieeexplore.ieee.org/document/4059247/,2006 IEEE/RSJ International Conference on Intelligent Robots and Systems,9-15 Oct. 2006,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS40897.2019.8967592,Can a Robot Become a Movie Director? Learning Artistic Principles for Aerial Cinematography,IEEE,Conferences,"Aerial filming is constantly gaining importance due to the recent advances in drone technology. It invites many intriguing, unsolved problems at the intersection of aesthetical and scientific challenges. In this work, we propose a deep reinforcement learning agent which supervises motion planning of a filming drone by making desirable shot mode selections based on aesthetical values of video shots. Unlike most of the current state-of-the-art approaches that require explicit guidance by a human expert, our drone learns how to make favorable viewpoint selections by experience. We propose a learning scheme that exploits aesthetical features of retrospective shots in order to extract a desirable policy for better prospective shots. We train our agent in realistic AirSim simulations using both a hand-crafted reward function as well as reward from direct human input. We then deploy the same agent on a real DJI M210 drone in order to test the generalization capability of our approach to real world conditions. To evaluate the success of our approach in the end, we conduct a comprehensive user study in which participants rate the shot quality of our methods. Videos of the system in action can be seen at https://youtu.be/qmVw6mfyEmw.",https://ieeexplore.ieee.org/document/8967592/,2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),3-8 Nov. 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ROBIO.2017.8324803,Classification-lock tracking approach applied on person following robot,IEEE,Conferences,"The task of following a person in the real complex environment by camera still keeps at risk even the visual tracking technologies have been well studied in the last decade. Currently, most approaches only utilize single-shot initialization in the first frame and update their tracking models according to the result of the last frame. However, it leads to an uncorrected target selection once the inner appearance changes, i.e., a feature-rich object is moved out of the human. In this paper, we reveal a classification-lock tracking framework and apply our approach on a mobile platform. A pairwise cluster tracker is used to locate the person. A positive & negative classifier is utilized to verify the tracker's result and update tracking model. In addition, a pre-trained CPU optimized neural network is employed to lock the tracking result to only be human. In the experiment, we deploy the common challenges of visual tracking both on the static scene and a real-following task. Furthermore, our approach is compared with other state-of-art approaches on common datasets. Results prove the tracking quality of our approach in both the static and the dynamic scenes. Our approach achieves the best average score on the common dataset.",https://ieeexplore.ieee.org/document/8324803/,2017 IEEE International Conference on Robotics and Biomimetics (ROBIO),5-8 Dec. 2017,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCE46568.2020.9042997,Cliff-sensor-based Low-level Obstacle Detection for a Wheeled Robot in an Indoor Environment,IEEE,Conferences,"A ramp and uneven ground formed by a low-level obstacle – whose height is too low from the ground – often stalls a robot’s navigation in an indoor environment. Few-centimeter differences between a low-level obstacle and a low-level non-obstacle are very difficult to be precisely measured in a constant distance at a mobile robot. In this paper, a wheeled mobile robot thus makes physical contact onto a low-level object, in order to measure such subtle differences. We use one or more cliff sensors typically in place for a mobile robot in order to avoid a drop-off. A wheeled robot climbs over a low-level object, classifies an obstacle vs. a non-obstacle using a cliff sensor’s timeseries data, and rapidly backs up before getting stuck onto an obstacle. While adopting a simplified deep-learning architecture, we suggest a rapid and accurate obstacle detection technique in real-time. We implemented our technique on an embedded robot platform of LG Hom-Bot. The supplementary video on the physical robot experiment can be accessed at https://youtu.be/yK57S857_II.",https://ieeexplore.ieee.org/document/9042997/,2020 IEEE International Conference on Consumer Electronics (ICCE),4-6 Jan. 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/INDIN.2011.6034840,Cognitive decision unit applied to autonomous biped robot NAO,IEEE,Conferences,"The novel approach to use meta-psychology - the theoretic foundation of psychoanalysis - as archetype for a decision making framework for autonomous agents was realized in simulations recently. In addition, multiple studies showed the capability of a robot to sense and interact in its environment. This work fills the gap between sensing, environmental interaction and decision making by grounding these topics with an agents internal needs using the concepts of meta-psychology. The bodies of typical agents are equipped with internal systems which can generate bodily needs - for example the urgent need for food. As proof-of-concept we implemented this concept on a simulated agent as well as on a physical real humanoid biped robot to additionally proof the concept within a fully controlled simulated environment. The use of the common humanoid robot platform NAO, which has 25 degrees of freedom and biped locomotion, enforced us to deal with complex situations and disturbed sensor readings. NAO provides various internal sensors like engine temperature or battery level as well as external sensors like sonar or cameras. An implemented visual marker detecting system allowed us to detect objects in the surrounding environmental, representing food or energy sources. We show, how it is possible to use the psychoanalytically inspired framework ARS to control a real world application, the robot NAO.",https://ieeexplore.ieee.org/document/6034840/,2011 9th IEEE International Conference on Industrial Informatics,26-29 July 2011,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CARE.2013.6733739,Cognitive learning enabled real time object search robot,IEEE,Conferences,"Object Tracking is usually performed in the context of higher-level applications that require the location and/or shape of the object in every frame. Most works are focused on a specific application, such as tracking human, car, or pre-learned objects. All these require database and considerable amount of training time to detect the current object and to track it. In this paper we propose a method to track objects where a pre-stored database is not a requirement. The proposed method uses a combination of Scale Invariant Feature Transform (SIFT) based feature extraction, Kalman filter and Cognitive learning. The algorithm has the ability to make its own database of the objects in the due course of time by interacting with the user through text based communication. This algorithm is deployed on a search robot which does the operation of searching an object in real time upon a command from the user. The search operation of robot is made more flexible using Bluetooth wireless communication protocol.",https://ieeexplore.ieee.org/document/6733739/,"2013 International Conference on Control, Automation, Robotics and Embedded Systems (CARE)",16-18 Dec. 2013,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICTAI.2019.00023,Collision-Free Path Finding for Dynamic Gaming and Real Time Robot Navigation,IEEE,Conferences,"Collision-free path finding is crucial for multi-agent traversing environments like gaming systems. An efficient and accurate technique is proposed for avoiding collisions with potential obstacles in virtual and real time environments. Potential field is a coherent technique but it eventuates with various problems like static map usage and pre-calculated potential field map of the environment. It is unsuitable for dynamically changing or unknown environments. Agents can get stuck inside a local minima incompetent in escaping without a workaround implementation. This paper presents efficient and accurate solutions to find collision free path using potential field for dynamic gaming and real time robot navigation. A surfing game in two testing environments with a Gamecar and a physical robot called Robocar is created with dynamic and solid obstacles. Sensor like proximity, line and ultrasonic are used along with the camera as different agents for path finding. The proposed intelligent agent (IA) technique is compared with other path planing algorithms and games in terms of time complexity, cost metrics, decision making complexity, action repertoire, interagent communication, reactivity and temporally continuous. It traverses for 135 meters(m) in 55.8 seconds(s) covering 20 goals and 419.3 m in 8.7 minutes while avoiding 10 local minimas successfully. Proposed technique shows comparable results to path finding with techniques using neural networks and A* algorithm. Experimental results prove the efficiency with run time overload, time complexity and resource consumption of the proposed technique.",https://ieeexplore.ieee.org/document/8995276/,2019 IEEE 31st International Conference on Tools with Artificial Intelligence (ICTAI),4-6 Nov. 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/HUMANOIDS.2017.8246935,Combining deep learning for visuomotor coordination with object identification to realize a high-level interface for robot object-picking,IEEE,Conferences,"We present a proof of concept to show how a deep network for end-to-end visuomotor learning to grasp is coupled with an attention focus mechanism for state-of-the-art object detection with convolutional neural networks. The cognitively motivated integration of both methods in a single robotic system allows us to realize a high-level interface to use the visuomotor network in environments with several objects, which otherwise would only be usable in environments with a single object. The resulting system is deployed on a humanoid robot, and we perform several real-world grasping experiments that demonstrate the feasibility of our approach.",https://ieeexplore.ieee.org/document/8246935/,2017 IEEE-RAS 17th International Conference on Humanoid Robotics (Humanoids),15-17 Nov. 2017,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.23919/ICCAS47443.2019.8971680,Comparison of Object Recognition Approaches using Traditional Machine Vision and Modern Deep Learning Techniques for Mobile Robot,IEEE,Conferences,"In this paper, we consider the problem of object recognition for a mobile robot in an indoor environment using two different vision approaches. Our first approach uses HOG descriptor with SVM classifier as traditional machine vision model while the second approach uses Tiny-YOLOv3 as modern deep learning model. The purpose of this study is to gain intuitive insight of both approaches for understanding the principles behind these techniques through their practical implementation in real world. We train both approaches with our own dataset for doors. The proposed work is assessed through the real-world implementation of both approaches using mobile robot with Zed camera in real world indoor environment and the robustness has been evaluated by comparing and analyzing the experimental results of both models on same dataset.",https://ieeexplore.ieee.org/document/8971680/,"2019 19th International Conference on Control, Automation and Systems (ICCAS)",15-18 Oct. 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ITNT52450.2021.9649145,Comparison of Reinforcement Learning Algorithms for Motion Control of an Autonomous Robot in Gazebo Simulator,IEEE,Conferences,"This article compares various implementations of deep Q learning as it is one of the most efficient reinforcement learning algorithms for discrete action space systems. The efficiency of the implementations for the classical Cartpole problem ported to the Gazebo environment is investigated. Then, these algorithms are compared for a self-created bipedal robot problem. Since the creation and configuration of a real robotic system is a laborious process, the initial debugging of the robot can be performed using the appropriate software that simulates the real environment. In our case, the Gazebo simulator was used. Using the simulator allows you to conduct research without having a real robotic system. In this case, it is possible to transfer the results from the simulator to the real system. The result of the study is the conclusion about the greatest efficiency of deep Q-learning with the experience reproduction mechanism. Also, the conclusion is that even for a robot with two degrees of freedom, Q-learning algorithms are not effective enough, and a comparative study with other families of reinforcement learning algorithms is needed.",https://ieeexplore.ieee.org/document/9649145/,2021 International Conference on Information Technology and Nanotechnology (ITNT),20-24 Sept. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS.2006.282163,Conceptual Design and Implementation of Arm Wrestling Robot,IEEE,Conferences,"In this paper, we develop a novel robotic arm wrestling system integrated with mechanical arm, elbow/wrist force sensors, servo motor, encoder, 3-D MEMS accelerometer, and USB camera. The arm wrestling robot (AWR) is intended to play arm wrestling game with real human on a table for entertainment. The designing scenario of the prototype model's hardware is performed. Elbow/wrist force sensors, as a crucial device in the force sensing system, are described in details. Software is developed for device driven and interface. The surface electromyographic (EMG) signals from the upper limb are sampled when a real player competes with the force testing system. By using the method of wavelet packet transformation (WPT), the high-frequency noises can be eliminated effectively and the characteristics of EMG signals can be extracted. Artificial neural network is adopted to estimate the elbow joint torque. The effectiveness of the humanoid algorithm using torque control estimated via WRT and neural network is confirmed by experiments",https://ieeexplore.ieee.org/document/4059139/,2006 IEEE/RSJ International Conference on Intelligent Robots and Systems,9-15 Oct. 2006,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SAUPEC/RobMech/PRASA48453.2020.9041114,Context-Aware Action with a Small Mobile Robot,IEEE,Conferences,"Simultaneous advances in mobile GPU computing and real-time object recognition now enable machines to make decisions and take actions based on the detection of objects of interest in the environment. An implementation of a mobile robot system that combines autonomous exploration and mapping capabilities with a real-time object recognition method based on a deep neural network running on a mobile GPU, is described. The system is able to detect objects of interest and then take real-time actions to interact with the objects, in this case, by moving to acquire inspection-style images of the object, from multiple angles. The robot system is small, self-contained and runs on battery power. The system shows the potential for the development of robotic systems with context awareness, permitting advanced autonomy.",https://ieeexplore.ieee.org/document/9041114/,2020 International SAUPEC/RobMech/PRASA Conference,29-31 Jan. 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICEEE.2011.6106626,Continuous-time neural control for a 2 DOF vertical robot manipulator,IEEE,Conferences,"This paper presents a continuous-time neural control scheme for identification and control of a two degrees of freedom (DOF) direct drive vertical robot manipulator model, on which effects due to friction and gravitational forces are both considered. A recurrent high-order neural network (RHONN) structure is proposed in order to identify the plant model to then, based on this neural structure, derive a neural controller using the backstepping design methodology. The trajectory tracking performance of the neural controller is illustrated via simulations results, which suggest the validity of the proposed approach for its implementation in real-time.",https://ieeexplore.ieee.org/document/6106626/,"2011 8th International Conference on Electrical Engineering, Computing Science and Automatic Control",26-28 Oct. 2011,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS.2018.8593374,Cost of Transport Estimation for Legged Robot Based on Terrain Features Inference from Aerial Scan,IEEE,Conferences,"The effectiveness of the robot locomotion can be measured using the cost of transport (CoT) which represents the amount of energy that is needed for traversing from one place to another. Terrains excerpt different mechanical properties when crawled by a multi-legged robot, and thus different values of the CoT. It is therefore desirable to estimate the CoT in advance and plan the robot motion accordingly. However, the CoT might not be known prior the robot deployment, e.g., in extraterrestrial missions; hence, a robot has to learn different terrains as it crawls through the environment incrementally. In this work, we focus on estimating the CoT from visual and geometrical data of the crawled terrain. A thorough analysis of different terrain descriptors within the context of incremental learning is presented to select the best performing approach. We report on the achieved results and experimental verification of the selected approaches with a real hexapod robot crawling over six different terrains.",https://ieeexplore.ieee.org/document/8593374/,2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),1-5 Oct. 2018,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICAIBD55127.2022.9820142,D-Leader Control Algorithm for Multi-Robot Formation Transformation,IEEE,Conferences,"The Leader-Follower algorithm stands out among many multi-robot formation control algorithms due to its simple operation and easy implementation. However, the Leader-Follower algorithm has a strong dependence on the pilot robot and poor information feedback. To solve this problem, the multivariable D-leader algorithm based on multi-robot formation transformation is proposed in this paper. Based on the Leader-Follower algorithm, the idea of dynamic pilot is introduced to make the multi-robot formation flexibly, and realize the dynamic adjustment of the pilot robot while changing the multi-robot formation. At the same time, the Runge-Kutta algorithm and the transformation method of particle model and difference model are combined to realize the flexible adjustment of path and position during formation transformation. The cross-mode experimental simulation shows that compared with the Leader-Follower algorithm, the formation adjustment rate of multi-robot is increased by an average of 66.7&#x0025; and the dynamic navigation rate of pilot robot is increased by an average of 53.35&#x0025; under the D-Leader algorithm. The D-Leader algorithm not only realizes the formation adjustment of multi-robot but also effectively improves the performance of multi-robot formation.",https://ieeexplore.ieee.org/document/9820142/,2022 5th International Conference on Artificial Intelligence and Big Data (ICAIBD),27-30 May 2022,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA.2019.8794187,Deep Learning based Motion Prediction for Exoskeleton Robot Control in Upper Limb Rehabilitation,IEEE,Conferences,"The synchronization of the movement between exoskeleton robot and human arm is crucial for Robot-assisted training (RAT) in upper limb rehabilitation. In this paper, we propose a deep learning based motion prediction model which is applied to our recently developed 8 degrees-of-freedom (DoFs) upper limb rehabilitation exoskeleton, named NTUH-II. The human arm dynamics and surface electromyography (sEMG) can be first measured by two wireless sensors and used as input of deep learning model to predict user's motion. Then, the prediction can be used as desired motion trajectory of the exoskeleton. As a result, the robot arm can follow the movement on either side of the user's arm in real-time. Various experiments have been conducted to verify the performance of the proposed motion prediction model, and the results show that the proposed motion prediction implementation can reduce the mean absolute error and the average delay time of movement between human arm and robot arm.",https://ieeexplore.ieee.org/document/8794187/,2019 International Conference on Robotics and Automation (ICRA),20-24 May 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.23919/ASCC56756.2022.9828060,Deep Learning-based Real-time Object Detection for Empty-Dish Recycling Robot,IEEE,Conferences,"The world is facing a shrinking workforce by the sagging birth rate and an aging population. Robot techniques are one of the best solutions for taking place of humans and overcoming this emergency issue. This paper introduces a deep learning-based empty-dish recycling robot for realizing the automatic empty-dish recycling after breakfast, dinner, or lunch in a restaurant, canteen, or cafeteria. A deep learning model&#x2013;You Only Look Once (YOLO)&#x2013;is equipped for dish detection such as cups, bowls, chopsticks, towels et al., and catch points are calculated for controlling the robot arm to recycle the target dishes. Finally, the YOLOv4 model is quantized by TensorRT and deployed on Jetson Nano. The real-time dish detection YOLO is focused on this paper, the experimental results show that after the YOLO model quantization, the detection time of a single image is increased from 3.93s to 0.44s, with more than 96.00% high accuracy on Precision, Recall, and F1 values. The functions of empty-dish recycling are realized, which will lead to further development toward practical use.",https://ieeexplore.ieee.org/document/9828060/,2022 13th Asian Control Conference (ASCC),4-7 May 2022,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CDC.2018.8619843,Deep Reinforcement Learning Based Self-Configuring Integral Sliding Mode Control Scheme for Robot Manipulators,IEEE,Conferences,"This paper deals with the design of an intelligent self-configuring control scheme for robot manipulators. The scheme features two control structures: one of centralized type, implementing the inverse dynamics approach, the other of decentralized type. In both control structures, the controller is based on Integral Sliding Mode (ISM), so that matched disturbances and uncertain terms, due to unmodeled dynamics or couplings effects, are suitably compensated. The use of the ISM control also enables the exploitation of its capability of acting as a “perturbation estimator” which, in the considered case, allows us to design a Deep Reinforcement Learning (DRL) based decision making mechanism. It implements a switching rule, based on an appropriate reward function, in order to choose one of the two control structures present in the scheme, depending on the requested robot performances. The proposed scheme can accommodate a variety of velocity and acceleration requirements, in contrast with the genuine decentralized or centralized control structures taken individually. The assessment of our proposal has been carried out relying on a model of the industrial robot manipulator COMAU SMART3-S2, identified on the basis of real data and with realistic sensor noise.",https://ieeexplore.ieee.org/document/8619843/,2018 IEEE Conference on Decision and Control (CDC),17-19 Dec. 2018,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCCI54379.2022.9740974,Design and Implementation of Arduino Insect Robot for Real Time Applications,IEEE,Conferences,"A robotic project that mimic the insect motions is the Arduino-based Insect Robot. This Arduino Insect robot&#x0027;s characteristics are part of the current advancement of Artificial Intelligence. The characteristics include obstacle avoidance, light attraction, and random resting abilities, which are quite like insect behavior. The project includes an Arduino product, the Arduino-Pro Mini, that serves as the project&#x0027;s principal controller. It has five servo motors that move its front right leg, front left leg, center, or shaft, which is coupled to the two back servos, back right leg and back left leg. The shaft servo used in this project aids robot operation by allowing the rear servos to move at a higher angle. The project also includes three ultrasonic sensors that measure obstacle distance and have configurable parameters that control how many steps the insect robot will take to escape the barrier. Photodiodes with pull-up resistors are also included in the project, which are used to detect light. Two USB connections are also included, with channel 1 rated at 1A, channel 2 rated at 2.1A, and a toggle switch for power supply. A solar panel is also utilized in this project, which absorbs the sun&#x0027;s rays and converts them to electricity. Simply said, they are utilized to produce power through the photovoltaic effect.",https://ieeexplore.ieee.org/document/9740974/,2022 International Conference on Computer Communication and Informatics (ICCCI),25-27 Jan. 2022,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AINL-ISMW-FRUCT.2015.7382967,Design and implementation Raspberry Pi-based omni-wheel mobile robot,IEEE,Conferences,Nowadays simultaneous localization and mapping (SLAM) algorithms are being tested at least in two phases: software simulation and real hardware platform testing. This paper describes hardware design and control software for small size omni-directional wheels robot implemented for indoor testing SLAM algorithms.,https://ieeexplore.ieee.org/document/7382967/,"2015 Artificial Intelligence and Natural Language and Information Extraction, Social Media and Web Search FRUCT Conference (AINL-ISMW FRUCT)",9-14 Nov. 2015,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ITAIC54216.2022.9836962,Design of Robot Motion Grasping System Based on Machine Vision,IEEE,Conferences,"This paper proposes a vision-based control strategy for performing high-speed sorting tasks in automated product lines, and develops the associated control software. Utilize a Cartesian robot to control suction cups to grab unordered objects from one moving conveyor and place them in sequence on another conveyor. A CCD camera takes an image every fixed period. After the contour extraction algorithm and the image deduplication algorithm are processed, the shape and position of the object are obtained. The target positioning and tracking method based on &#x0027;minimum external contour &#x002B; image deduplication &#x002B; conveyor belt dynamic speed detection&#x2019; is adopted to complete high-speed handling operations in real time. Experiments on the Cartesian coordinate robot sorting system verify the effectiveness of the proposed vision control strategy.",https://ieeexplore.ieee.org/document/9836962/,2022 IEEE 10th Joint International Information Technology and Artificial Intelligence Conference (ITAIC),17-19 June 2022,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AIEA53260.2021.00013,Design of a Real-time Robot Control System oriented for Human-Robot Cooperation,IEEE,Conferences,"An open real-time control system based on the EtherCAT fieldbus communication technology is proposed to fulfill the high real-time requirement of the human-robot cooperation controller in this paper. An open source real-time kernel of Xenomai is employed as the real-time software platform of the robot control system. Based on this, four-layer interfaces architecture are accomplished, which are human-machine cooperation control layer, motion control layer, robot axis control layer and hardware abstraction layer, through the corresponding four real-time tasks to meet the demand of human-robot cooperation operations. In addition, the scheduling task is developed to manage the 4 real-time tasks. The dual buffer communication mechanisms and priority-based scheduling strategy between layers was exploited to synchronize these real-time tasks. The underlying hardware abstract interface and the human-robot collaborative control algorithm interface are opened in the control system as the quadric exploitation interfaces to meet the need of developing application tasks in real-time space. Experiment results which are conducted on a self-developed 6-DOF collaborative robot show that the proposed control system is effective in real-time control applications of human-robot cooperative control at the control cycle of 5 milliseconds.",https://ieeexplore.ieee.org/document/9525600/,2021 International Conference on Artificial Intelligence and Electromechanical Automation (AIEA),14-16 May 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CMCE.2010.5609659,Design of mobile robot system with remote control based on CAN-bus,IEEE,Conferences,"In order to realizing remote control and information collection quickly and reliably, the mobile robot with remote control is designed. In the paper, according to analysis of the overall structure, hardware circuit of the robot system is designed. Because the CAN2.0 standard only makes physical layer protocol and data link layer protocol, application layer protocol is ruled according to robot control system. In the last part of this paper, the software of master/slave computer is introduced in detail. The experiment shows that running performance of robot control system is balanced, efficient and has satisfied the practical demand.",https://ieeexplore.ieee.org/document/5609659/,"2010 International Conference on Computer, Mechatronics, Control and Electronic Engineering",24-26 Aug. 2010,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICSMC.1998.726514,"Designing, making, and using a mobile robot",IEEE,Conferences,"Describes the building and control of a mobile robot which is capable of navigating in a well defined workspace by means of generating an optimal trajectory. The basic control architecture of the mobile robot is implemented with a combination of an MC68HC11 microcontroller and a personal computer. The kinematics of the proposed differential impulse is analyzed which allow us to select the appropriate steering DC motors and speed measurement requirements of the system. The motor control is performed by a PWM scheme and PI controllers. A path planning stage finds the optimal trajectory, taking a graphical description of the workspace and using potential fields and dynamic programming to solve the optimization problem and avoid the obstacles. Clearly there are two specific problems: building a complete device that will allow having an electric powered robot and the use of these resources to obtain controlled and collision-free movement in a real workspace.",https://ieeexplore.ieee.org/document/726514/,"SMC'98 Conference Proceedings. 1998 IEEE International Conference on Systems, Man, and Cybernetics (Cat. No.98CH36218)",14-14 Oct. 1998,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICoSTA48221.2020.1570615971,Detecting Features of Middle Size Soccer Field using Omnidirectional Camera for Robot Soccer ERSOW,IEEE,Conferences,"ERSOW (EEPIS Robot Soccer on Wheeled) is robot soccer developed by Politeknik Elektronika Negeri Surabaya that is designed and implemented on a Middle Size League division by following the rules of RoboCup, an international robot competition. One of the most famous division is a soccer robot, that is divided into two divisions: (1) SSL (Small Size League) and (2) MSL (Middle Size League). There are many research fields related to soccer robot which must be developed in robot ERSOW such as Artificial Intelligence (AI), Computer Vision, Embedded System, Mechanic Systems, and Hardware. This paper focuses on computer vision research for robot ERSOW, especially detecting features of the middle size soccer field, so that specific features of the field like X-junction, T-junction and L-junction can be detected to help robot positioning task where the result is represented into x and y in real-world coordinate. By knowing the position of the features, the robot position can be calculated. The localization system at robot ERSOW uses odometry, which has a large percentage of data errors. Therefore, we attempt to extract the feature of X-junction that is done to find its x and y coordinates and then the obtained coordinate can be used as a reference for correcting odometry data by AI.",https://ieeexplore.ieee.org/document/9079260/,2020 International Conference on Smart Technology and Applications (ICoSTA),20-20 Feb. 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1049/cp.2012.1127,Distributed parallel processing of mobile robot PF-SLAM,IET,Conferences,"Real-time property is a fundamental requirement for a practical robot system. For this purpose, this article proposes an implementation architecture of robot SLAM by adopting two parallel threads processing. Since the dominant factor which determines the computational complexity is the employed particle number, two distributed threads with different particle set size are executed simultaneously. Conventional PF-SLAM algorithm occupies one of threads, and the other thread which hires more particles is activated whenever robot has significant motion changes. Advantages of this presented idea are validated by experiment carried on Pioneer robot.",https://ieeexplore.ieee.org/document/6492734/,International Conference on Automatic Control and Artificial Intelligence (ACAI 2012),3-5 March 2012,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA.2019.8793748,Effect of Mechanical Resistance on Cognitive Conflict in Physical Human-Robot Collaboration,IEEE,Conferences,"Physical Human-Robot Collaboration (pHRC) is about the interaction between one or more human operator(s) and one or more robot(s) in direct contact and voluntarily exchanging forces to accomplish a common task. In any pHRC, the intuitiveness of the interaction has always been a priority, so that the operator can comfortably and safely interact with the robot. So far, the intuitiveness has always been described in a qualitative way. In this paper, we suggest an objective way to evaluate intuitiveness, known as prediction error negativity (PEN) using electroencephalogram (EEG). PEN is defined as a negative deflection in event related potential (ERP) due to cognitive conflict, as a consequence of a mismatch between perception and reality. Experimental results showed that the forces exchanged between robot and human during pHRC modulate the amplitude of PEN, representing different levels of cognitive conflict. We also found that PEN amplitude significantly decreases (p <; 0.05) when a mechanical resistance is being applied smoothly and more time in advance before an invisible obstacle, when compared to a scenario in which the resistance is applied abruptly before the obstacle. These results indicate that an earlier and smoother resistance reduces the conflict level. Consequently, this suggests that smoother changes in resistance make the interaction more intuitive.",https://ieeexplore.ieee.org/document/8793748/,2019 International Conference on Robotics and Automation (ICRA),20-24 May 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ROMAN.2009.5326159,Efficient parsing of spoken inputs for human-robot interaction,IEEE,Conferences,"The use of deep parsers in spoken dialogue systems is usually subject to strong performance requirements. This is particularly the case in human-robot interaction, where the computing resources are limited and must be shared by many components in parallel. A real-time dialogue system must be capable of responding quickly to any given utterance, even in the presence of noisy, ambiguous or distorted input. The parser must therefore ensure that the number of analyses remains bounded at every processing step. The paper presents a practical approach to addressing this issue in the context of deep parsers designed for spoken dialogue. The approach is based on a word lattice parser combined with a statistical model for parse selection. Each word lattice is parsed incrementally, word by word, and a discriminative model is applied at each incremental step to prune the set of resulting partial analyses. The model incorporates a wide range of linguistic and contextual features and can be trained with a simple perceptron. The approach is fully implemented as part of a spoken dialogue system for human-robot interaction. Evaluation results on a Wizard-of-Oz test suite demonstrate significant improvements in parsing time.",https://ieeexplore.ieee.org/document/5326159/,RO-MAN 2009 - The 18th IEEE International Symposium on Robot and Human Interactive Communication,27 Sept.-2 Oct. 2009,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICE2T.2017.8215992,Elevator button and floor number recognition through hybrid image classification approach for navigation of service robot in buildings,IEEE,Conferences,"To successfully move a robot into the building, the elevator button and elevator floor number detection and recognition can play an important role. It can help a robot move in the building, just as it also can help a visually impaired person who wants to move another floor in the building. Due to vision-based approach, the difference in lighting condition and the complex background are the main obstacles in this research. A hybrid image classification model is presented in this research to overcome all these difficulties. This hybrid model is the combination of histogram of oriented gradients and bag of words models, which later reduces the dimension of image features by using the feature selection algorithm. An artificial neural network has been implemented to get the experimental result by training and testing. In order to get training performance, 1000 training image samples have been used and additional 1000 image samples also been used to get the testing performance. The experimental results of this research indicate that this proposed framework is important for real-time implementation to implement the elevator button and elevator floor number recognition framework.",https://ieeexplore.ieee.org/document/8215992/,2017 International Conference on Engineering Technology and Technopreneurship (ICE2T),18-20 Sept. 2017,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICIEV.2018.8641023,Embedded System based Bangla Intelligent Social Virtual Robot with Sentiment Analysis,IEEE,Conferences,"Bangla is the mother tongue of millions of people all over the world. Despite being a very popular language, any social virtual robot that can intelligently communicate in Bangla is a fairytale till now. One of the main reason of this is lack of rich text corpus and previous research on Bangla language. The proposed Bangla Intelligent Social Virtual Robot can communicate in Bangla intelligently and can express its reflective emotion virtually with the help of machine learning algorithms and sentiment analysis. In this paper, we discuss the approached system, design methodology and implementation details of first ever Bangla virtual embedded robot followed by the methodology of building a rich Bangla text corpus. The proposed embedded virtual robot turns out better performer when compared with only known Bangla intelligent chatbot named ‘Golpo’ and the embedded system performance efficiency has been upgraded with the help CPU over-clocking technique.",https://ieeexplore.ieee.org/document/8641023/,"2018 Joint 7th International Conference on Informatics, Electronics & Vision (ICIEV) and 2018 2nd International Conference on Imaging, Vision & Pattern Recognition (icIVPR)",25-29 June 2018,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA48506.2021.9562114,Enhancing Robot Perception in Grasping and Dexterous Manipulation through Crowdsourcing and Gamification,IEEE,Conferences,"Robot grasping and manipulation planning in unstructured and dynamic environments is heavily dependent on the attributes of manipulated objects. Although deep learning approaches have delivered exceptional performance in robot perception, human perception and reasoning are still superior in processing novel object classes. Moreover, training such models requires large datasets that are generally expensive to obtain. This work combines crowdsourcing and gamification to leverage human intelligence, enhancing the object recognition and attribute estimation aspects of robot perception. The framework employs an attribute matching system that encodes visual information into an online puzzle game, utilizing the collective intelligence of players to expand an initial attribute database and react to real-time perception conflicts. The framework is deployed and evaluated in a proof-of-concept application for enhancing object recognition in autonomous robot grasping and a model for estimating the response time is proposed. The obtained results demonstrate that given enough players, the framework can offer near real-time labeling of novel objects, based purely on visual information and human experience.",https://ieeexplore.ieee.org/document/9562114/,2021 IEEE International Conference on Robotics and Automation (ICRA),30 May-5 June 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA40945.2020.9197510,Episodic Koopman Learning of Nonlinear Robot Dynamics with Application to Fast Multirotor Landing,IEEE,Conferences,"This paper presents a novel episodic method to learn a robot's nonlinear dynamics model and an increasingly optimal control sequence for a set of tasks. The method is based on the Koopman operator approach to nonlinear dynamical systems analysis, which models the flow of observables in a function space, rather than a flow in a state space. Practically, this method estimates a nonlinear diffeomorphism that lifts the dynamics to a higher dimensional space where they are linear. Efficient Model Predictive Control methods can then be applied to the lifted model. This approach allows for real time implementation in on-board hardware, with rigorous incorporation of both input and state constraints during learning. We demonstrate the method in a real-time implementation of fast multirotor landing, where the nonlinear ground effect is learned and used to improve landing speed and quality.",https://ieeexplore.ieee.org/document/9197510/,2020 IEEE International Conference on Robotics and Automation (ICRA),31 May-31 Aug. 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/WCICA.2000.863468,Estimated force emulation for space robot using neural networks,IEEE,Conferences,"This paper introduces the telerobotic system estimated force emulation using neural networks. A delay-compensating 3D stereo-graphic simulator is implemented in SGI ONYX/4 RE/sup 2/. The estimated force emulation can protect the real robot in time from being damaged in collision. The neural network is used to learn the mapping between the contact force error and the accommodated position command to the controller of the space robot. Finally, the controller can feel the emulated force with a two-hand 6-DOF master arm using the force feedback interface.",https://ieeexplore.ieee.org/document/863468/,Proceedings of the 3rd World Congress on Intelligent Control and Automation (Cat. No.00EX393),26 June-2 July 2000,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ROMAN.2007.4415203,Evolving Personality of a Genetic Robot in Ubiquitous Environment,IEEE,Conferences,"This paper discusses the personality of genetic robot and its evolving algorithm within the purview of the broader ubiquitous robot framework. Ubiquitous robot systems blends mobile robot technology (Mobot) with distributed sensor systems (Embot) and overseeing software intelligence (Sobot), for various integrated services. The Sobot is a critical question since it performs the dual purpose of overseeing intelligence as well as user interface. The Sobot is hence modelled as an artificial creature with autonomously driven behavior. The artificial creature has its own genome and in which each chromosome consists of many genes that contribute to defining its personality. This paper proposes evolving the personality of an artificial creature. A genome population is evolved such that it customized the genome satisfying a set of personality traits desired by the user. Evaluation procedure for each genome of the population is carried out in a virtual environment. Effectiveness of this scheme is demonstrated by using an artificial creature, Rity in the virtual 3D world created in a PC.",https://ieeexplore.ieee.org/document/4415203/,RO-MAN 2007 - The 16th IEEE International Symposium on Robot and Human Interactive Communication,26-29 Aug. 2007,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/INDIN.2017.8104924,Experiences in integrating Internet of Things and cloud services with the robot operating system,IEEE,Conferences,"New Internet of Things open source technologies, middlewares, and programming languages, make the quick integration of devices, systems and cloud services easier than never before. With their utilization, complex tasks such as object detection, tracking and tracing, can be easily realized, even by embedded devices in a fraction of time. The interplay of highly heterogeneous IoT devices and open source software, has been utilized in this work as a learning tool, in order to train developers and enhance their IoT skills. By designing, implementing, testing and deploying a rapid prototype, new knowledge is acquired, assessment of technologies and concepts is carried out, and the end-result, although developed in a constraint timeframe, is technologically promising, cost-effective and feature-rich. This work sheds some light on the prototype implemented and discusses the developer experiences and benefits of this IoT integration hands-on approach.",https://ieeexplore.ieee.org/document/8104924/,2017 IEEE 15th International Conference on Industrial Informatics (INDIN),24-26 July 2017,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICITA.2005.135,Experiences with simulated robot soccer as a teaching tool,IEEE,Conferences,"The development of assignments for undergraduate teaching typically requires a compromise between what is achievable by an average student and what engages the interest of a more advanced member of the class. Selecting a suitable compromise is particularly problematic for undergraduate artificial intelligence (AI) courses which typically attempt to cover a very broad range of topics, without delving too deeply into the details. Ideally, a single problem would be selected whose solution could be approached with more than one technique covered in the course, enabling students to carry out a comparative analysis of performance. Robot soccer simulation has provided an interesting platform for artificial intelligence research and is increasingly being used as a teaching apparatus. There are a number of limitations with existing simulation methodologies for this purpose. Current robot soccer simulators are aimed at research groups where accuracy is paramount and all facets of the real system must be emulated. However, many of the intricacies of a real robot soccer player are inappropriate for a teaching environment, as they detract from desired learning outcomes. Consequently, there is a need for a simulation that employs a simplified set of game rules and dynamics. This paper describes the design and implementation of such a framework and presents experiences gained from its use as a third year practical.",https://ieeexplore.ieee.org/document/1488833/,Third International Conference on Information Technology and Applications (ICITA'05),4-7 July 2005,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICSEC53205.2021.9684633,Experimental Piano &#x2013; Playing Robot Hand,IEEE,Conferences,"The robotic and Artificial Intelligent (AI) have been introduced as a key factor for industry revolution4.0. Many industries such as manufacturing, agriculture, logistic and supply chain and so on, are transformed and applied robotic and AI in order to enhance the productivities and reduce cost. In addition, the automation system based on modern AI are highly adopted according to lacking of labors in ageing society era. The AI in creative work is very challenging, especially in music. This paper presents the experiments of making processing unit to understand music source separation, automatic music transcription and optical music recognition is our first experiments. Music source separation (MSS) is the separation of music into different sound sources (for example, vocal, drum, or piano). Automatic music transcription (AMT) is a transcription of instrument sound as a musical note. Optical music recognition (OMR) is a task that involves reading and interpreting music notes and creating a machine-readable version of the sheet music. From these three basic knowledges, our first prototype of a robotic hand is implemented successfully and is able to play piano from musical notes, songs and piano sound properly in real-time. The design and experimental results are explained in this paper.",https://ieeexplore.ieee.org/document/9684633/,2021 25th International Computer Science and Engineering Conference (ICSEC),18-20 Nov. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA48506.2021.9561040,Extendable Navigation Network based Reinforcement Learning for Indoor Robot Exploration,IEEE,Conferences,This paper presents a navigation network based deep reinforcement learning framework for autonomous indoor robot exploration. The presented method features a pattern cognitive non-myopic exploration strategy that can better reflect universal preferences for structure. We propose the Extendable Navigation Network (ENN) to encode the partially observed high-dimensional indoor Euclidean space to a sparse graph representation. The robot&#x2019;s motion is generated by a learned Q-network whose input is the ENN. The proposed framework is applied to a robot equipped with a 2D LIDAR sensor in the GAZEBO simulation where floor plans of real buildings are implemented. The experiments demonstrate the efficiency of the framework in terms of exploration time.,https://ieeexplore.ieee.org/document/9561040/,2021 IEEE International Conference on Robotics and Automation (ICRA),30 May-5 June 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/EEI48997.2019.00073,FPGA Implementation of Family Service Robot Based on Neural Network PID Motion Control System,IEEE,Conferences,"For the fixed invariability of control parameters in the PID closed-loop control algorithm of existing mobile robot, and the poor real-time response and stability of robot chassis to the upper machine motion control command, a robot motion control system is designed based on BP neural network PID motion control algorithm. Firstly, according to the three-wheel omni-directional mobile robot motion characteristics and the principle of neural network PID control algorithm, the control system is modeled and simulated on simulink, it theoretically demonstrates that the BP neural network PID closed-loop control algorithm is superior to the traditional PID control algorithm. The simulation results show that the overshoot is small and the real-time performance is good, which can greatly improve the flexibility and stability of the system. Then, through the top-down design method by Verilog language, the FPGA design of BP neural network PID closed-loop control system is carried out. The three-wheel omni-directional mobile robot chassis is used as the experimental platform, which is controlled by the robot upper machine to follow and avoid obstacles. The test results show that the control system improves the robot's running speed by 11.6% and accuracy by 13.4%. Compared with the open-loop control system, which effectively verifies the feasibility and practicability of the closed-loop control system.",https://ieeexplore.ieee.org/document/8990996/,2019 International Conference on Electronic Engineering and Informatics (EEI),8-10 Nov. 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/FPT.2009.5377635,FPGA implementation of mixed integer quadratic programming solver for mobile robot control,IEEE,Conferences,"We propose a high-speed mixed integer quadratic programming (MIQP) solver on an FPGA. The MIQP solver can be applied to various optimizing applications including real-time robot control. In order to rapidly solve the MIQP problem, we implement reusing a first solution (first point), pipeline architecture, and multi-core architecture on the single FPGA. By making use of them, we confirmed that 79.5% of the cycle times are reduced, compared with straightforward sequential processing. The operating frequency is 67 MHz, although a core 2 duo PC requires 3.16 GHz in processing the same size problem. The power consumption of the MIQP solver is 4.2 W.",https://ieeexplore.ieee.org/document/5377635/,2009 International Conference on Field-Programmable Technology,9-11 Dec. 2009,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICMLC.2006.258689,Facial Tracking for an Emotion-Diagnosis Robot to Support E-Learning,IEEE,Conferences,"There have been a lot of researches on the detection/estimation of human emotions from facial expressions. However, most of them have extracted facial features for some specific emotions from the still pictures of artificial actions or performances. This paper describes facial tracking for e-learning support robot which can estimate a emotion of e-learning user from his/her facial expression in real-time; (1) the criteria of the facial expression to classify the eight emotions was obtained by the time sequential subjective evaluation on the emotions as well as the time sequential analysis of a facial expression by image processing. (2) The coincidence ratio between the discriminated emotions based upon the criteria of emotion diagnosis and the time sequential subjective evaluation on emotions for ten e-learning subjects was 69%. (3) Then, the possibility of the real time emotion diagnosis robot to support e-learning was confirmed by the facial image processing at the 15 frame/sec. rate as well as the simple emotion diagnosis algorithm based upon the Mahalonobis distance",https://ieeexplore.ieee.org/document/4028735/,2006 International Conference on Machine Learning and Cybernetics,13-16 Aug. 2006,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICSMC.1997.633250,Facial interaction between animated 3D face robot and human beings,IEEE,Conferences,"We study the realization of a realistic human-like response of an animated 3D face robot in communicative interaction with human beings. The face robot can produce human-like facial expressions and recognize human facial expressions using facial image data obtained by a CCD camera mounted inside the left eyeball. We developed the real time machine recognition of facial expressions by using a layered neural network and achieved a high correct recognition ratio of 85% with respect to 6 typical facial expressions of 15 subjects in 55 ms. We also developed a new small-size actuator for display of facial expressions on the face robot, giving the same speed in dynamic facial expressions as in human even in the case of a high-speed expression of ""surprise"". For facial interactive communication between the face robot and human beings, we integrated these two technologies to produce the facial expression in respond to the recognition result of the human facial expression in real time. This implies a high technological potential for the animated face robot to undertake interactive communication with human when an artificial emotion being implemented.",https://ieeexplore.ieee.org/document/633250/,"1997 IEEE International Conference on Systems, Man, and Cybernetics. Computational Cybernetics and Simulation",12-15 Oct. 1997,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICPS51978.2022.9816965,Fast Person Detector with Efficient Multi-level Contextual Block for Supporting Assistive Robot,IEEE,Conferences,"The robotic demand a vision method to work in real-time on embedded devices. Besides, an assistive robot requires person detection, which is widely used to help automatically interact with the user. This work presents a fast real-time person detection (Fast-PdNet) to localize human areas implemented on a Jetson Nano. This device has been commonly used as an embedded system and is suitable for synchronizing sensors and actuators. The proposed architecture contains layers of Convolutional Neural Network consisting of two main modules: backbone and detection. An efficient extractor module with a multi-level contextual block is employed to extract the spatial features quickly. It avoids high-cost computing to distinguish interest features of the human body and background features. The lightweight learning attention selects suspected specific features area without generating excessive parameters. The end-to-end training was conducted on MS COCO 2017 to generate efficiently weighted models. The Fast-PdNet achieves competitive performance with other light detectors evaluated on the MS COCO 2017, PASCAL VOC 2007, and 2012 datasets. Moreover, this detector can run 35 frames per second when working in real-time on Jetson Nano.",https://ieeexplore.ieee.org/document/9816965/,2022 IEEE 5th International Conference on Industrial Cyber-Physical Systems (ICPS),24-26 May 2022,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISPACS.2018.8923369,Fast Recognition and Control of Walking Mode for Humanoid Robot Based on Pressure Sensors and Nearest Neighbor Search,IEEE,Conferences,"In this paper, we propose a nearest-neighbor multi-reference learning system for control of humanoid-robot movements, using real-time data from pressure sensors embedded in the robot feet, which is processed with parallelized pipeline architecture for high-speed recognition of actual surface conditions. A first nearest-neighbor (1-NN) classifier is used to recognize the most similar reference pattern in terms of the smallest Euclidean distance. Our proposed architecture achieves a classification time of about 2.4μ s with a total power consumption of 8.53mW at 100 MHz operating frequency when implemented on a low-cost FPGA (Cyclone-V GX-Series). The analysis results are further useful for a next-generation-ASIC-based AI-chip design for a robust real-time robot-learning system.",https://ieeexplore.ieee.org/document/8923369/,2018 International Symposium on Intelligent Signal Processing and Communication Systems (ISPACS),27-30 Nov. 2018,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ROBOT.1996.506991,Fast connectionist learning for trailer backing using a real robot,IEEE,Conferences,This paper presents the application of a connectionist control-learning system to an autonomous mini-robot. The system's design is severely constrained by the computing power and memory available on board the mini-robot and the on-board training time is greatly limited by the short life of the battery. The system is capable of rapid unsupervised learning of output responses in temporal domains through the use of eligibility traces and data sharing within topologically defined neighborhoods.,https://ieeexplore.ieee.org/document/506991/,Proceedings of IEEE International Conference on Robotics and Automation,22-28 April 1996,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/RCAR.2018.8621723,Fault-Tolerant and Self-Adaptive Market-Based Coordination Using Hoplites Framework for Multi-Robot Patrolling Tasks,IEEE,Conferences,"An autonomous robot team can be employed for continuous coverage of a dynamic environment. In this paper, we propose a novel approach for creating multi-robot patrolling policies, which is fault-tolerant and self-adaptive. A dynamic priority queue and time-out replanning mechanism are maintained by each robot to schedule the tasks fault-tolerantly in the context of the market-based method. Hoplites framework is adapted by introducing a self-adaptive threshold adjustment and sharing mechanism to provide a high-level coordination. This work is demonstrated by a multi-robot patrolling task implemented on Robot Operating System (ROS). A flexible tool, Stage, is leveraged to provide the simulated environment. The experimental results validate the effectiveness and availability.",https://ieeexplore.ieee.org/document/8621723/,2018 IEEE International Conference on Real-time Computing and Robotics (RCAR),1-5 Aug. 2018,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SII.2011.6147520,Forming an artificial pheromone potential field using mobile robot and RFID tags,IEEE,Conferences,"In the biological world, social insects such as ants and bees use a volatile substance called pheromone for their foraging or homing tasks. This study deals with how to utilize the concept of the chemical pheromone as an artificial potential field for robotic purposes. This paper first models a pheromone-based potential field, which is constructed through the interaction between mobile robot and RFID tags. The emphasis in the modeling of the system is on the possibility of the practical implementable ideas. The stability analysis of the pheromone potential field is carried out with the aim of implementing the model on a real robotic system. The comprehensive analysis on stability provides the criteria for how the parameters are to be set for the proper potential field, and has also led to a new filter design scheme called pheromone filter. The designed filter satisfies both the stability and accuracy of the field, and facilitates a more straightforward and practical implementation for building and shaping the potential field. The effectiveness of the proposed algorithm is validated through both computer simulation and real experiment.",https://ieeexplore.ieee.org/document/6147520/,2011 IEEE/SICE International Symposium on System Integration (SII),20-22 Dec. 2011,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICSCT53883.2021.9642700,Fruits Detection and Distance Estimation using RGB-D camera for Harvesting Robot,IEEE,Conferences,"In this paper, fruit detection and distance estimation from detected fruits are processed in parallel based on a low-cost and compact RGB-D camera. Fruits are detected in RGB images and surrounded by bounding boxes using modified deep-learning models, which are optimized for high accuracy based on limited computational resources. Distances from the detected fruits to the camera as well as the size of those fruits are estimated based on their respective boxes in depth images, which provided third-dimensional knowledge or 3D localization of the fruits. The proposed method is implemented on an embedded computer connected to an RGB-D camera and validated in the real environment. Experimental results show that high accuracy in detection, distance, and fruit’s size estimation has been achieved. The promising results enable the further grasping action for a harvesting robot.",https://ieeexplore.ieee.org/document/9642700/,2021 International Conference on Science & Contemporary Technologies (ICSCT),5-7 Aug. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ROMAN.2017.8172498,Functional imitation task in the context of robot-assisted Autism Spectrum Disorder diagnostics: Preliminary investigations,IEEE,Conferences,"This paper presents a functional imitation task aimed at facilitating Autism Spectrum Disorder (ASD) diagnostics in children. Imitation plays a key role in the development of social skills at a young age, and studies have shown that the ability to imitate is impaired in children with ASD. Therefore, we expect imitation-based tasks to have diagnostic value. In this paper, we introduce two novel elements of human-robot interaction in the context of autism diagnostics. Instead of pure motoric imitation, we propose imitation tasks involving real objects in the environment. The introduction of physical objects strongly emphasizes joint attention skills, another area that is typically impaired in children with ASD. Furthermore, we present simple object detection, manipulation, tracking and gesture recognition algorithms, suitable for real-time, onboard execution on the small-scale humanoid robot NAO. The proposed system paves the way for fully autonomous execution of diagnostic tasks, which would simplify the deployment of robotic assistants in clinical settings. The source code for all described functionalities has been made publicly available as open-source software. We present a preliminary evaluation of the proposed system with a control group of typically developing preschool children and a group of seven children diagnosed with ASD.",https://ieeexplore.ieee.org/document/8172498/,2017 26th IEEE International Symposium on Robot and Human Interactive Communication (RO-MAN),28 Aug.-1 Sept. 2017,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/MMAR.2019.8864671,Fusion of Gesture and Speech for Increased Accuracy in Human Robot Interaction,IEEE,Conferences,"An approach for decision-level fusion for gesture and speech based human-robot interaction (HRI) is proposed. A rule-based method is compared with several machine learning approaches. Gestures and speech signals are initially classified using hidden Markov models, reaching accuracies of 89.6% and 84% respectively. The rule-based approach reached 91.6% while SVM, which was the best of all evaluated machine learning algorithms, reached an accuracy of 98.2% on the test data. A complete framework is deployed in real time humanoid robot (NAO) which proves the efficacy of the system.",https://ieeexplore.ieee.org/document/8864671/,2019 24th International Conference on Methods and Models in Automation and Robotics (MMAR),26-29 Aug. 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AIM.2009.5229761,Fuzzy and Neural controllers for acute obstacle avoidance in mobile robot navigation,IEEE,Conferences,"Robot navigation is the technique to guide the mobile robot move towards the desired goal where dynamic and unknown environment is involved. The environment is distinguished by variable terrain and also certain objects which are known as obstacles that may block the movement of the robot in reaching the desired destination. Fuzzy Logic (FL) and Artificial Neural Network (ANN) are used to assist autonomous mobile robot move, learn the environment and reach the desired goal. This research study is focused on exploring the four combinations of training algorithms composed of FL and ANN that avoid acute obstacles in the environment. Path Remembering algorithm proposed in this paper will assist the mobile robot to come out from acute obstacles. Virtual wall building method also is proposed in order to prevent the mobile robot reentering the same acute obstacle once it has been turned away from the wall. MATLAB simulation is developed to verify and validate the algorithms before they are implemented in real time on Team AmigoBot™ robot. The results obtained from both simulation and actual application confirmed the flexibility and robustness of the controllers designed in avoiding acute obstacles and a comparison of all the four combinations of algorithms is done to find the best combination of algorithms to perform the required navigation to avoid acute obstacles.",https://ieeexplore.ieee.org/document/5229761/,2009 IEEE/ASME International Conference on Advanced Intelligent Mechatronics,14-17 July 2009,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA40945.2020.9196608,Gershgorin Loss Stabilizes the Recurrent Neural Network Compartment of an End-to-end Robot Learning Scheme,IEEE,Conferences,"Traditional robotic control suits require profound task-specific knowledge for designing, building and testing control software. The rise of Deep Learning has enabled end-to-end solutions to be learned entirely from data, requiring minimal knowledge about the application area. We design a learning scheme to train end-to-end linear dynamical systems (LDS)s by gradient descent in imitation learning robotic domains. We introduce a new regularization loss component together with a learning algorithm that improves the stability of the learned autonomous system, by forcing the eigenvalues of the internal state updates of an LDS to be negative reals. We evaluate our approach on a series of real-life and simulated robotic experiments, in comparison to linear and nonlinear Recurrent Neural Network (RNN) architectures. Our results show that our stabilizing method significantly improves test performance of LDS, enabling such linear models to match the performance of contemporary nonlinear RNN architectures. A video of the obstacle avoidance performance of our method on a mobile robot, in unseen environments, compared to other methods can be viewed at https://youtu.be/mhEsCoNao5E.",https://ieeexplore.ieee.org/document/9196608/,2020 IEEE International Conference on Robotics and Automation (ICRA),31 May-31 Aug. 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ROMAN.2009.5326235,Gestural teleoperation of a mobile robot based on visual recognition of sign language static handshapes,IEEE,Conferences,"This paper presents results achieved in the frames of a national research project (titled ldquoDIANOEMArdquo), where visual analysis and sign recognition techniques have been explored on Greek Sign Language (GSL) data. Besides GSL modelling, the aim was to develop a pilot application for teleoperating a mobile robot using natural hand signs. A small vocabulary of hand signs has been designed to enable desktopbased teleoperation at a high-level of supervisory telerobotic control. Real-time visual recognition of the hand images is performed by training a multi-layer perceptron (MLP) neural network. Various shape descriptors of the segmented hand posture images have been explored as inputs to the MLP network. These include Fourier shape descriptors on the contour of the segmented hand sign images, moments, compactness, eccentricity, and histogram of the curvature. We have examined which of these shape descriptors are best suited for real-time recognition of hand signs, in relation to the number and choice of hand postures, in order to achieve maximum recognition performance. The hand-sign recognizer has been integrated in a graphical user interface, and has been implemented with success on a pilot application for real-time desktop-based gestural teleoperation of a mobile robot vehicle.",https://ieeexplore.ieee.org/document/5326235/,RO-MAN 2009 - The 18th IEEE International Symposium on Robot and Human Interactive Communication,27 Sept.-2 Oct. 2009,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CogInfoCom.2017.8268243,Guidelines for improving task-based natural language understanding in human-robot rescue teams,IEEE,Conferences,"Mixed human-robot teams are increasingly considered for accomplishing complex mission due to their complementary capabilities. A major barrier for deploying such heterogeneous teams in real-world settings, is the current lack of natural skills in robotic team members, such as the understanding and interpretation of natural language instructions that include referential descriptions of entities in the world. In this paper we report the results of an empirical study in which humans tend to use referring expressions. We show how the received results and ideas can be used as guidelines to improve dialogue systems. By integrating and extending our system with these results, we will show how complex natural language instructions can be easily translated by robotic systems.",https://ieeexplore.ieee.org/document/8268243/,2017 8th IEEE International Conference on Cognitive Infocommunications (CogInfoCom),11-14 Sept. 2017,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS51168.2021.9635831,Guiding Robot Model Construction with Prior Features,IEEE,Conferences,"Virtually all robot control methods benefit from the availability of an accurate mathematical model of the robot. However, obtaining a sufficient amount of informative data for constructing dynamic models can be difficult, especially when the models are to be learned during robot deployment. Under such circumstances, standard data-driven model learning techniques often yield models that do not comply with the physics of the robot. We extend a symbolic regression algorithm based on Single Node Genetic Programming by including the prior model information into the model construction process. In this way, symbolic regression automatically builds models that compensate for theoretical or empirical model deficiencies. We experimentally demonstrate the approach on two real-world systems: the TurtleBot 2 mobile robot and the Parrot Bebop 2 drone. The results show that the proposed model-learning algorithm produces realistic models that fit well the training data even when using small training sets. Passing the prior model information to the algorithm significantly improves the model accuracy while speeding up the search.",https://ieeexplore.ieee.org/document/9635831/,2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),27 Sept.-1 Oct. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA48506.2021.9561034,High-Speed Robot Navigation using Predicted Occupancy Maps,IEEE,Conferences,"Safe and high-speed navigation is a key enabling capability for real world deployment of robotic systems. A significant limitation of existing approaches is the computational bottleneck associated with explicit mapping and the limited field of view (FOV) of existing sensor technologies. In this paper, we study algorithmic approaches that allow the robot to predict spaces extending beyond the sensor horizon for robust planning at high speeds. We accomplish this using a generative neural network trained from real-world data without requiring human annotated labels. Further, we extend our existing control algorithms to support leveraging the predicted spaces to improve collision-free planning and navigation at high speeds. Our experiments are conducted on a physical robot based on the MIT race car using an RGBD sensor where were able to demonstrate improved performance at 4 m/s compared to a controller not operating on predicted regions of the map.",https://ieeexplore.ieee.org/document/9561034/,2021 IEEE International Conference on Robotics and Automation (ICRA),30 May-5 June 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CRV.2010.55,Human Upper Body Pose Recognition Using Adaboost Template for Natural Human Robot Interaction,IEEE,Conferences,"In this paper, we propose a novel Adaboost template to recognize human upper body poses from disparity images for natural human robot interaction (HRI). First, the upper body poses of standing persons are classified into seven categories of views. For each category, a mean template, variance template, and percentage template are generated. Then, the template region is divided into positive and negative regions, corresponding to the region of bodies and surrounding open space. A weak classifier is designed for each pixel in the template. A new EM-like Adaboost learning algorithm is designed to learn the Adaboost template. Different from existing Adaboost classifiers, we show that the Adaboost template can be used not only for recognition but also for adaptive top-down segmentation. By using Adaboost template, only a few positive samples for each category are required for learning. Comparison with conventional template matching techniques has been made. Experimental results show that significant improvements can be achieved in both cases. The method has been deployed in a social robot to estimate human attentions to the robot in real-time human robot interaction.",https://ieeexplore.ieee.org/document/5479162/,2010 Canadian Conference on Computer and Robot Vision,31 May-2 June 2010,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/YAC53711.2021.9486647,Human-Robot Interaction System Design for Manipulator Control Using Reinforcement Learning,IEEE,Conferences,"In this article, a novel human-robot interaction (HRI) system is presented and applied in the robotic arm coordinated operation control task. The presented HRI system includes two parts, the impedance model controller and the robotic arm controller, which allows the operator to manipulate the robotic arm to accomplish the given task with minimal human effort. First, the model-based reinforcement learning (RL) method is applied in the impedance model for operator adaptation. The impedance model controller can transform human input into the specific signal for the manipulator. Second, a novel adaptive manipulator controller is designed. In contrast to existing controllers, a velocity-free filter is implemented in our controller, which is developed to replace the manipulator actuator's speed signal. The effectiveness of the presented HRI system is verified by the simulation based on real manipulator parameters.",https://ieeexplore.ieee.org/document/9486647/,2021 36th Youth Academic Annual Conference of Chinese Association of Automation (YAC),28-30 May 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/EPIA.2005.341221,Hybrid State Machines with Timed Synchronization for Multi-Robot System Specification,IEEE,Conferences,"In multi-robot systems, the need for precise modeling or specification of agent behaviors arises due to the high complexity of the robot agent interactions and the dynamics of the environment. Since the behavior of agents usually can be understood as driven by external events and internal states, it is obvious to model multiagent systems by state transition diagrams. The corresponding formalisms come equipped with a formal semantics which is advantageous. In this paper, a combination of UML statecharts and hybrid automata is proposed, allowing formal system specification on different levels on abstraction on the one hand, and expressing real-time system behavior with continuous variables on the other hand. One important aspect of multi-robot systems is the need of coordination and hence synchronization of behavior. For both, statecharts and hybrid automata, it is assumed that synchronization takes zero time. This is sometimes unrealistic. Therefore, a new notation and implementation of synchronization is proposed here, which overcomes this problem. The proposed method is illustrated with a case study from the robotic soccer domain",https://ieeexplore.ieee.org/document/4145962/,2005 portuguese conference on artificial intelligence,5-8 Dec. 2005,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICSESS.2011.5982262,Implement the TABLE TENNIS game by robot arm with forward-backward neural network,IEEE,Conferences,"In this paper, implementing the table tennis game by robot arm with forward-backward neural network has been investigated. The main contributions have been made as the followings, (1) Based on 2-dimensions (2-D), both ideal model and improved model have been developed, creating a Ping-pong game by employing Visual C++. The example provide by this paper has shown that the two new model operate very well. (2) The 2-D approach for the game has been proposed and the calculation formulas of this approach have been strictly proofed by employing the small amplitude disturbed theory. (3) A novel 3-D approach has also been put up forwarded. By using 3-D approach, one can implement the game in 3 dimensions; it acts as a real game. Finally, the proposed 2-D and 3-D approaches are not only used in the Ping-pong game, can also be exploited to a motion target tracking such as a Ballistics Missile Defense System.",https://ieeexplore.ieee.org/document/5982262/,2011 IEEE 2nd International Conference on Software Engineering and Service Science,15-17 July 2011,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/iFuzzy.2013.6825409,Implementation of human following mission by using fuzzy head motion control and Q-learning wheel motion control for home service robot,IEEE,Conferences,"This paper mainly implements human following function for home service robot, May, developed in our laboratory. In order to follow the operator accurately, visual tracking is composed by Tracing-Learning-Detection (TLD) and Kinect skeleton, where TLD plays the role as re-detecting the situation that operator is occluded or disappeared, and Kinect skeleton is adopted to track all other situations while TLD is learning how to enlarge operator image patterns in order to enhance recognition rates. For the sake of improving tracking capability, fuzzy head motion control is added in the visual tracking system to compensate the constraints that the mobile platform of May cannot react rapidly. Every instant movement of the operator can be captured by fuzzy head motion control in real time. Q-learning is applied to discover the pose switching of the mobile platform such that May possesses more robust following ability. By Q-learning, states setting are based on three dimensional position, actions are created by the pose of four wheel independent steering and four wheel independent driven (4WIS4WID) platform, and rewards are established on state transitions. Finally, both the experimental results in the laboratory and competition consequents of Follow Me Mission in robot@home league at RoboCup Japan Open 2013 Tokyo demonstrate that our robot May can fluently switch its poses to follow operator by utilizing the proposed schemes.",https://ieeexplore.ieee.org/document/6825409/,2013 International Conference on Fuzzy Theory and Its Applications (iFUZZY),6-8 Dec. 2013,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/INES.1997.632397,Implementation of neural network sliding-mode controller for DD robot,IEEE,Conferences,"The experimental development of a trajectory tracking neural network controller based on the theory of continuous sliding-mode controllers is shown in the paper. The neural network control law was verified on a real direct drive 3 DOF PUMA mechanism. The new neural network sliding-mode controller was successfully tested for trajectory tracking sudden changes in the manipulator dynamics (load). The comparision between the neural network sliding mode controller, a computer torque method controller and a continuous sliding mode controller with PI-estimator for sudden load changes on the real robot mechanism is shown.",https://ieeexplore.ieee.org/document/632397/,Proceedings of IEEE International Conference on Intelligent Engineering Systems,17-17 Sept. 1997,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/I2CT.2014.7092212,Implementation of synthetic brain concept in humanoid robot,IEEE,Conferences,"This paper is elaborate the model of humanoid robot interacts with human being and perform various operation as per the command given by the human being. A humanoid robot having Synthetic brain can able to do Interaction, communication, Object detection, information acquisition about any object, response to voice command, chatting logically with human beings. Object detection will be done by this robot for that purpose there is use image processing concept (HAAR Technique), And to make the system intelligent that is whenever system interact, communicate, chat with human it gives proper response, question / answers there is integrates artificial intelligence and DFA / NFA automata and Prolog language concept for answering logically over the complex and relevant strings or data.",https://ieeexplore.ieee.org/document/7092212/,International Conference for Convergence for Technology-2014,6-8 April 2014,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS.2004.1389758,Implementing reinforcement learning in the chaotic KIV model using mobile robot AIBO,IEEE,Conferences,"We use the biologically inspired dynamic neural network architecture KIV to achieve robust goal-oriented navigation in a physical environment with obstacles. KIV operates on the principle of chaotic neurodynamics, in the style of brains. It performs the task of multi-sensory fusion, recognition, and decision-making in real time. We use the Sony AIBO robot to demonstrate the operation of our algorithm. AIBO's video camera and infra sensors have been complemented with an external camera for monitoring of the robot's position. The performance of the autonomous system is evaluated using goal-oriented navigation.",https://ieeexplore.ieee.org/document/1389758/,2004 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) (IEEE Cat. No.04CH37566),28 Sept.-2 Oct. 2004,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/COGINF.2011.6016164,Improved mobile robot's Corridor-Scene Classifier based on probabilistic Spiking Neuron Model,IEEE,Conferences,"The ability of cognition and recognition for complex environment is very important for a real autonomous robot. A improved Corridor-Scene-Classifier based on probabilistic Spiking Neuron Model(pSNM) for mobile robot is designed. In the SNN classifier, the model pSNM is used. As network's training, Thorpe's learning rule is used. The experimental results show that the improved Classifier is more effective and it also has stronger robustness than the previous classifier based on Integrated-and-Fire (IAF) spiking neuron model for the structural corridor-scene. It also has better robustness than the traditional kernel-pca and the BP Corridor-Scene-classifier.",https://ieeexplore.ieee.org/document/6016164/,IEEE 10th International Conference on Cognitive Informatics and Cognitive Computing (ICCI-CC'11),18-20 Aug. 2011,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ELECSYM.2018.8615506,Improving Field and Ball Detector for Humanoid Robot Soccer EROS Platform,IEEE,Conferences,"Humanoid robot soccer perceives environment mostly through cameras. The performance decrement in our humanoid soccer platform (EROS) is primarily due to the visual perception that is less robust to the RoboCup new rule which specifically reducing color coding in the field. Notable works favorably employ simple color segmentation, image morphology, and blob detector due to simplicity in the implementation and run in real-time for most embedded hardware, while some employ a more advanced supervised learning running in sophisticated hardware to boost detection accuracy. In this paper, a visual perception system consisting of field and ball detection is developed in our platform EROS to address the RoboCup new rule. Color segmentation and image morphology are stacked with a more advanced supervised learning cascade classifier. In this way, the favorable color segmentation and image morphology help to reduce the number of object candidates while the cascade classifier helps to boost the accuracy of detection. Experiments show encouraging result for detecting field and ball position. Our approach has successfully been implemented in practice and achieves remarkably result in Indonesian humanoid robot soccer competition.",https://ieeexplore.ieee.org/document/8615506/,2018 International Electronics Symposium on Engineering Technology and Applications (IES-ETA),29-30 Oct. 2018,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/VRW55335.2022.00107,Indirect Robot Manipulation using Eye Gazing and Head Movement For Future of Work in Mixed Reality,IEEE,Conferences,"Most robot manipulations are guided by the worker&#x0027;s both hands using a teach-pendant or other gadgets. However, manipulating the robot is more difficult when both hands are occupied, which requires a new manipulation method. This study proposes a novel indirect robot interaction and manipulation method using eye gaze and head movement in mixed reality. The proposed approach has been implemented to provide hands-free robot interaction for pick-and-place tasks. The results show a promising direction for the future of work in human-robot collaboration.",https://ieeexplore.ieee.org/document/9757386/,2022 IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops (VRW),12-16 March 2022,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ROBOT.1992.219999,Integrated planning and execution control of autonomous robot actions,IEEE,Conferences,"The authors describe an implemented integrated system allowing a mobile robot to plan its actions, taking into account temporal constraints, and to control their execution in real time. The general architecture has three levels, and the approach is related to hierarchical planning: the plan produced by the temporal planner is further refined at the control level, which in turn supervises its execution by a functional level. The framework of the French Mars Rover project VAP is used as an illustration of the various aspects discussed.<>",https://ieeexplore.ieee.org/document/219999/,Proceedings 1992 IEEE International Conference on Robotics and Automation,12-14 May 1992,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CIRA.1997.613877,Intelligence computing for direct human-robot communication using natural language and cognitive graphics,IEEE,Conferences,"A direct human-robot communication system based on natural language (NL) and cognitive graphics (CG) as a part of a new hierarchical structure of an AI control system of a mobile robot for service use (MRSU) is developed. The software of the simulation system for direct human-robot communication and MRSU behavior based on NL and CG is described. The NL and CG are used for description and representation of possible external worlds in the robot artificial life. This system allows us to evaluate the control algorithms of real time robot behavior and to reduce difficulties connected with such troubles as robot collisions with real objects and robot hardware damage. The mathematical background of direct NL human-robot communication and robot behavior simulation system is knowledge engineering based on spatio-temporal and action logics, default reasoning, cognitive graphics and soft computing. The main concepts, structure, conceptual model of the simulation system for description of the artificial life of the MRSU are discussed. A simulation example and real experimental results are described.",https://ieeexplore.ieee.org/document/613877/,Proceedings 1997 IEEE International Symposium on Computational Intelligence in Robotics and Automation CIRA'97. 'Towards New Computational Principles for Robotics and Automation',10-11 July 1997,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AERO53065.2022.9843528,Intelligent Anomaly Detection of Robot Manipulator based on Energy Consumption Auditing,IEEE,Conferences,"Robot manipulators have become a key component in smart manufacturing, and play instrumental roles in automated production lines. It is of paramount importance of monitoring their operational health and security. Nevertheless, manufacturing systems are vulnerable to both cyber and physical threats/attacks, necessitating a side-channel monitoring mechanism. Recently, the machine learning techniques to detect and identify threats in Internet of Things by auditing energy consumption have been developed. This paper presents a holistic framework that combines the deep autoencoder (DAE) and energy consumption auditing for real-time health and security monitoring and diagnosis of manufacturing systems. The key idea is to train DAEs with energy consumption data of normal operation only without presence of threats/anomaly, which is then employed to reconstruct energy consumption online. This semi-supervised approach allows to detect faults by evaluating the magnitude of reconstruction errors. That is, a larger error indicates anomalous events. The method is demonstrated using a delicately designed experiment, in which a robot manipulator is programmed to perform different tasks in a circular pattern. Cyber and physical threats are implemented to trigger anomalous events. For all anomalies, the path of the robot manipulator remains unchanged, which makes impossible health and security monitoring through simple inspection of completed products/tasks. By auditing the DAE reconstruction error of energy consumption during task execution, most of the anomalies are detected with the accuracy of greater than 80&#x0025;. Having relatively small false positives, the precision is found to be above 73&#x0025;. Unfortunately, threats are not detected every time instance during the entire period of a single attack, causing many false negatives and a low (23&#x0025;) recall rate. However, having a high recall rate is not necessary because often anomalies are counted as events, which does not need to be detected for every instance. Our study shows that energy consumption auditing in conjunction with DAE is a promising approach for independent, side-channel anomaly detection.",https://ieeexplore.ieee.org/document/9843528/,2022 IEEE Aerospace Conference (AERO),5-12 March 2022,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SKIMA.2014.7083542,Intelligent Appearance and shape based facial emotion recognition for a humanoid robot,IEEE,Conferences,"In this paper, we present an intelligent facial emotion recognition system with real-time face tracking for a humanoid robot. The system is able to detect facial actions and emotions from images with up to 60 degrees of pose variations. We employ the Active Appearance Model to perform real-time face tracking and extract both texture and geometric representations of images. A POSIT algorithm is also used to identify head rotations. The extracted texture and shape features are employed to detect 18 facial actions and seven basic emotions. The overall system is integrated with a humanoid robot platform to further extend its vision APIs. The system is proved to be able to deal with challenging facial emotion recognition tasks with various pose variations.",https://ieeexplore.ieee.org/document/7083542/,"The 8th International Conference on Software, Knowledge, Information Management and Applications (SKIMA 2014)",18-20 Dec. 2014,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ARIS50834.2020.9205772,Intelligent Robot for Worker Safety Surveillance: Deep Learning Perception and Visual Navigation,IEEE,Conferences,"The fatal injury rate for the construction industry is higher than the average for all industries. Recently, researchers have shown an increased interest in occupational safety in the construction industry. However, all the current methods using conventional machine learning with stationary cameras suffer from some severe limitations, perceptual aliasing (e.g., different places/objects can appear identical), occlusion (e.g., place/object appearance changes between visits), seasonal / illumination changes, significant viewpoint changes, etc. This paper proposes a perception module using end-to-end deep-learning and visual SLAM (Simultaneous Localization and Mapping) for an effective and efficient object recognition and navigation using a differential-drive mobile robot. Various deep-learning frameworks and visual navigation strategies with evaluation metrics are implemented and validated for the selection of the best model. The deep-learning model's predictions are evaluated via the metrics (model speed, accuracy, complexity, precision, recall, P-R curve, F1 score). The YOLOv3 shows the best trade-off among all algorithms, 57.9% mean average precision (mAP), in real-world settings, and can process 45 frames per second (FPS) on NVIDIA Jetson TX2 which makes it suitable for real-time detection, as well as a right candidate for deploying the neural network on a mobile robot. The evaluation metrics used for the comparison of laser SLAM are Root Mean Square Error (RMSE). The Google Cartographer SLAM shows the lowest RMSE and acceptable processing time. The experimental results demonstrate that the perception module can meet the requirements of head protection criteria in Occupational Safety and Health Administration (OSHA) standards for construction. To be more precise, this module can effectively detect construction worker's non-hardhat-use in different construction site conditions and can facilitate improved safety inspection and supervision.",https://ieeexplore.ieee.org/document/9205772/,2020 International Conference on Advanced Robotics and Intelligent Systems (ARIS),19-21 Aug. 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISIC.1988.65458,Intelligent navigation for an autonomous mobile robot,IEEE,Conferences,"The authors present a navigational algorithm for solving the problem of collision-free path planning and real-time control of an autonomous mobile robot in an environment cluttered with moving obstacles. The proposed approach is based on geometric representation/multiobjective A* search and the path smoothing/steering control techniques. An attempt was made to introduce a geometric structure into a paradigm that lacked any previous structure, as well as a multiobject search technique. The proposed navigational system has the following three-level structure: identifier, goal selector, and adapter. The authors are currently implementing the algorithm on a Sun 4 artificial intelligence workstation.<>",https://ieeexplore.ieee.org/document/65458/,Proceedings IEEE International Symposium on Intelligent Control 1988,24-26 Aug. 1988,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS.1994.407388,Intelligent robot hand control system using a tailorable parallel computer concept,IEEE,Conferences,"Presents a control system for the intelligent force control of multifingered robot grips. The multilevel system architecture combines both a fuzzy-based adaptation level and a neural-based one with a conventional PID-controller that allows autonomous performance of fine manipulations of an object. Two kinds of fuzzy controllers are presented. They use a decision making logic which expresses the a priori knowledge about the grasp force behaviour inside the friction cones and the necessary reactions regarding the criterion for grip stability. A neural controller, based on a Hooke-Jeeves optimisation approach, has been developed as well. The neural control algorithm is implemented by a three-layered backpropagation neural network. The neural and fuzzy controllers can be used separately or in parallel. In the last case the neural controller can be on-line trained using the input-output information from the fuzzy one. A computer based simulation system for the peg-in-hole insertion task is developed to analyse and to compare the capabilities of both control algorithms. The demands of flexibility and real-time control of the implementation of the control system are suited by a tailorable parallel computer concept. The two basic ideas of the concept are to set up each processing element individually for its application and connect these elements with different communicational methods according to the applicational demands. As this happens before runtime the concept is called static flexibility and is implemented using a new mechanical computer structure.<>",https://ieeexplore.ieee.org/document/407388/,Proceedings of IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS'94),12-16 Sept. 1994,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS.2005.1545188,Interactive evolution of human-robot communication in real world,IEEE,Conferences,"This paper describes how to implement interactive evolutionary computation (IEC) into a human-robot communication system. IEC is an evolutionary computation (EC) in which the fitness function is performed by human assessors. We used IEC to configure the human-robot communication system. We have already simulated IEC's application. In this paper, we implemented IEC into a real robot. Since this experiment leads considerable burdens on both the robot and experimental subjects, we propose the human-machine hybrid evaluation (HMHE) to increase the diversity within the genetic pool without increasing the number of interactions. We used a communication robot, WAMOEBA-3 (Waseda artificial mind on emotion base), which is appropriate for this experiment. In the experiment, human assessors interacted with WAMOEBA-3 in various ways. The fitness values increased gradually, and assessors felt the robot learnt the motions they desired. Therefore, it was confirmed that the IEC is most suitable as the communication learning system.",https://ieeexplore.ieee.org/document/1545188/,2005 IEEE/RSJ International Conference on Intelligent Robots and Systems,2-6 Aug. 2005,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IJCNN.2003.1223995,Investigating models of social development using a humanoid robot,IEEE,Conferences,"Human social dynamics rely upon the ability to correctly attribute beliefs, goals, and percepts to other people. The set of abilities that allow an individual to infer these hidden mental states based on observed actions and behavior has been called a ""theory of mind"". Drawing from the models of Baron-Cohen (1995) and Leslie (1994), a novel architecture called embodied theory of mind was developed to link high-level cognitive skills to the low-level perceptual abilities of a humanoid robot. The implemented system determines visual saliency based on inherent object attributes, high-level task constraints, and the attentional states of others. Objects of interest are tracked in real-time to produce motion trajectories which are analyzed by a set of naive physical laws designed to discriminate animate from inanimate movement. Animate objects can be the source of attentional states (detected by finding faces and head orientation) as well as intentional states (determined by motion trajectories between objects). Individual components are evaluated by comparisons to human performance on similar tasks, and the complete system is evaluated in the context of a basic social learning mechanism that allows the robot to mimic observed movements.",https://ieeexplore.ieee.org/document/1223995/,"Proceedings of the International Joint Conference on Neural Networks, 2003.",20-24 July 2003,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AIMSEC.2011.6009874,Kinematics simulation of upper limb rehabilitant robot based on virtual reality techniques,IEEE,Conferences,"The wearable exoskeletal robot for upper extremity rehabilitation is taken as the research object. According to D-H method, an accurate three-dimensional mechanism model for the robot system is established by SolidWorks software. The virtual set was generated in Simulink/VRML to carry out dynamic simulation. The variable parameters were set based on robotic practical joint range movement. The simulation of all joints and terminal trajectory and space motion area provided theoretical basis for position control, remote control and trajectory planning, realizing the rehabilitation robot visualizations and system interaction.",https://ieeexplore.ieee.org/document/6009874/,"2011 2nd International Conference on Artificial Intelligence, Management Science and Electronic Commerce (AIMSEC)",8-10 Aug. 2011,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ROBIO.2007.4522258,Layered omnidirectional walking controller for the humanoid soccer robot,IEEE,Conferences,"This paper proposes the layered omnidirectional walking controller for the humanoid soccer robot. The gait of the robot can be parameterized using the destination posititon and the desired direction while reaching the destination. Its implementation in our RoboCup simulation team — SEU-3D is detailed in this paper. Our approach generates smooth robot trajectories without stop before changing direction or turning, and is fast enough to meet the real-time requirements. The proposed approach has been tested in the RoboCup soccer 3D server platform. The results showed that omnidirectional walking has advantages in dynamic environments.",https://ieeexplore.ieee.org/document/4522258/,2007 IEEE International Conference on Robotics and Biomimetics (ROBIO),15-18 Dec. 2007,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICTC49870.2020.9289214,Learning Control Policy with Previous Experiences from Robot Simulator,IEEE,Conferences,"Advances in deep reinforcement learning enabled cost-efficient training of control policy of physical robot actions from robot simulators. Learning control policy in a simulated environment is cost-efficient over learning in a real environment. Reward engineering is one of the key components to train efficient control policy. For tasks with long horizons such as navigation and manipulation, a sparse reward is providing limited information. The robot simulator for a physical engine of physical robot manipulation has made it easy for researchers in the field of deep reinforcement learning to simulate complicated robot manipulation environments. In this paper, A robot manipulation simulator and a deep RL framework are utilized for implement a training control policy by utilizing previous experiences. For implementation, Recent innovation Hindsight Experience Replay (HER) algorithms with previous experiences to calculate dense rewards from a sparse reward is leveraged . Proposed implementation showed an approach to investigate the reward engineering method to formulate dense reward in robot manipulator tasks.",https://ieeexplore.ieee.org/document/9289214/,2020 International Conference on Information and Communication Technology Convergence (ICTC),21-23 Oct. 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA46639.2022.9812011,Learning Crowd-Aware Robot Navigation from Challenging Environments via Distributed Deep Reinforcement Learning,IEEE,Conferences,"This paper presents a deep reinforcement learning (DRL) sframework for safe and efficient navigation in crowded environments. Here, the robot learns cooperative behavior using a new reward function that penalizes robot actions interfering with the pedestrian&#x0027;s movement. Also, we propose a simulated pedestrian policy reflecting data from actual pedestrian movements. Furthermore, we introduce a collision detection that considers the pedestrian&#x0027;s personal space to generate affinity robot behavior. To efficiently explore this simulation environment, we propose distributed learning using Ape-X [1]. We deployed the robot in a real environment and verified its crowd-aware navigation performance compared with an actual human in terms of path length, travel time, and the number of abrupt avoidances.",https://ieeexplore.ieee.org/document/9812011/,2022 International Conference on Robotics and Automation (ICRA),23-27 May 2022,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ROBIO.2017.8324818,Learning complex assembly skills from kinect based human robot interaction,IEEE,Conferences,"Acquiring complex assembly skills is still a challenging task for robot programming. Because of the sensory and body structure differences, the human knowledge has to be demonstrated, recorded, converted and finally learned by the robot, in an inexplicit and indirect way. During this process, “how to demonstrate”, “how to convert” and “how to learn” are the key problems. In this paper, Kinect sensor is utilized to provide the behavior information of the human demonstrator. Through natural human robot interaction, body skeleton and joint 3D coordinates are provided in real-time, which can fully describe the human intension and task related skills. To overcome the structural and individual differences, a Cartesian level unified mapping method is proposed to convert the human motion and match the specified robot. The recorded data set are modeled using Gaussian mixture model(GMM) and Gaussian mixture regression(GMR), which can extract redundancies across multiple demonstrations and build robust models to regenerate the dynamics of the recorded movements. The proposed methodologies are implemented in the imNEU humanoid robot platform. Experimental results verify the effectiveness.",https://ieeexplore.ieee.org/document/8324818/,2017 IEEE International Conference on Robotics and Biomimetics (ROBIO),5-8 Dec. 2017,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICNN.1994.374669,Learning dynamic balance of a biped walking robot,IEEE,Conferences,"This paper discusses the application of CMAC (cerebellar model arithmetic computer) neural networks to the problem of biped walking with dynamic balance. The project goal is to develop biped control strategies based on a hierarchy of simple gait oscillators, PID controllers and neural network learning, but requiring no detailed dynamic models. The focus of this report is on real-time control studies using a ten axis biped robot with joint position, foot force and body acceleration sensors. While efficient walking has not yet been achieved, the experimental biped has learned the closed chain kinematics necessary to shift body weight from side-to-side while maintaining good foot contact and has learned the dynamic balance required in order to lift a foot off the floor for a desired length of time, during which the foot can be moved to a new location relative to the body. Using these skills, the biped is able to link short steps without falling.<>",https://ieeexplore.ieee.org/document/374669/,Proceedings of 1994 IEEE International Conference on Neural Networks (ICNN'94),28 June-2 July 1994,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IJCNN.1993.716991,Learning goal-directed navigation as attractor dynamics for a sensory motor system. (An experiment by the mobile robot YAMABICO),IEEE,Conferences,"This paper describes experimental results based on the authors' prior-proposed scheme: learning of sensory-based, goal-directed behavior. The scheme was implemented on the mobile robot ""YAMABICO"" and learning of a set of goal-directed navigations were conducted. The experiment assumed that the robot receives no global information such as position nor prior environment model. Instead, the robot was trained to learn adequate maneuvering in the adopted workspace by building a correct mapping between a spatio-temporal sequence of sensory inputs and maneuvering outputs on a neural structure. The experimental results showed that sufficient training generated rigid dynamical structure of a fixed point and limit cycling in the sensory-based state space, which realized robust navigations of homing and cyclic routing even against certain changes of environment as well as miscellaneous noises in the real world.",https://ieeexplore.ieee.org/document/716991/,"Proceedings of 1993 International Conference on Neural Networks (IJCNN-93-Nagoya, Japan)",25-29 Oct. 1993,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/DevLrn.2013.6652572,Learning the rules of a game: Neural conditioning in human-robot interaction with delayed rewards,IEEE,Conferences,"Learning in human-robot interaction, as well as in human-to-human situations, is characterised by noisy stimuli, variable timing of stimuli and actions, and delayed rewards. A recent model of neural learning, based on modulated plasticity, suggested the use of rare correlations and eligibility traces to model conditioning in real-world situations with uncertain timing. The current study tests neural learning with rare correlations in a human-robot realistic teaching scenario. The humanoid robot iCub learns the rules of the game rock-paper-scissors while playing with a human tutor. The feedback of the tutor is often delayed, missing, or at times even incorrect. Nevertheless, the neural system learns with great robustness and similar performance both in simulation and in robotic experiments. The results demonstrate the efficacy of the plasticity rule based on rare correlations in implementing robotic neural conditioning.",https://ieeexplore.ieee.org/document/6652572/,2013 IEEE Third Joint International Conference on Development and Learning and Epigenetic Robotics (ICDL),18-22 Aug. 2013,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ROBIO.2009.5420526,Learning-based action planning for real-time robot telecontrol with binocular vision in enhanced reality environment,IEEE,Conferences,"Action planning is one of the pivot issues in robot telecontrol, in which the action instructions are often given by the controller from remote site with the help of vision systems. In this paper, we present a learning-based strategy for action planning in robot telecontrol, in which the parameters of sophisticated actions of the remote robot equipped with a binocular vision system could be pre-scheduled with a virtual robot at the control terminal. The remote robot will then be 'taught' with the scheduled action plan with a series of parameter sets obtained form try-outs with the virtual robot and object in the enhanced environment, thus implementing dedicated actions assigned correctly. The action planning process is implemented within a enhanced reality environment, in which both the virtual and the real robot will be displayed simultaneously for the purpose of being deeply immersed. Experiment results demonstrate that the proposed method is capable of promoting the action precision of the remote robot, and effective and valid to designated applications, where action precision plays a critical role.",https://ieeexplore.ieee.org/document/5420526/,2009 IEEE International Conference on Robotics and Biomimetics (ROBIO),19-23 Dec. 2009,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.23919/ChiCC.2018.8483251,Local Gaussian Processes for Identifying Complex Mobile robot System,IEEE,Conferences,"Nonparametric Gaussian processes regression (GPR) is an important tool in machine learning, can be applied in identifying nonlinear models from experimental data, especially, the prediction of mean and variance present the useful advantage. However, when dealing with the large number of training data for the prediction of a complex dynamics system, GPR is not suitable to implement in real-time learning systems. To reduce the computation effort, local learning algorithm is introduced to improve the global Gaussian processes (GP) model in this paper. In this paper, a convenient and effective method for building local model network is proposed and then local GP for weighted regression is performed. The proposed local GPR method is implemented on a simulated example of online identification and prediction fast for a complex dynamic system of wheeled mobile robot.",https://ieeexplore.ieee.org/document/8483251/,2018 37th Chinese Control Conference (CCC),25-27 July 2018,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCSE.2014.6926425,MKL-SVM-based human detection for autonomous navigation of a robot,IEEE,Conferences,"This paper presents a classifier trained by a multiple kernel-learning support vector machine (MKL-SVM) to detect a human in sequential images from a video stream. The developed method consists of two aspects: multiple features consisting of HOG features and HOF features suitable for moving objects, and combined nonlinear kernels for SVM. For the purpose of real time application in autonomous navigation, the SimpleMKL algorithm is implemented into the proposed MKL-SVM classifier. It is able to converge rapidly with comparable efficiency through a weighted 2-norm regularization formulation with an additional constraint on the weights. The classifier is compared with the state-of-the-art linear SVM using a dataset called TUD-Brussels, which is available on line. The results show that the proposed classifier outperforms the Linear SVM with respect to accuracy.",https://ieeexplore.ieee.org/document/6926425/,2014 9th International Conference on Computer Science & Education,22-24 Aug. 2014,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/WACV.2009.5403083,ML-fusion based multi-model human detection and tracking for robust human-robot interfaces,IEEE,Conferences,"A novel stereo vision system for real-time human detection and tracking on a mobile service robot is presented in this paper. The system integrates the individually enhanced stereo-based human detection, HOG-based human detection, color-based tracking, and motion estimation for the robust detection and tracking of humans with large appearance and scale variations in real-world environments. A new framework of maximum likelihood based multi-model fusion is proposed to fuse these four human detection and tracking models according to the detection-track associations in 3D space, which is robust to the possible missed detections, false detections, and duplicated responses from the individual models. Multi-person tracking is implemented in a sequential near-to-far way, which well alleviates the difficulties caused by human-over-human occlusions. Extensive experimental results demonstrate the robustness of the proposed system under real-world scenarios with large variations in lighting conditions, cluttered backgrounds, human clothes and postures, and complex occlusion situations. Significant improvements in human detection and tracking have been achieved. The system has been deployed on six robot butlers to serve drinks, and showed encouraging performance in open ceremony events.",https://ieeexplore.ieee.org/document/5403083/,2009 Workshop on Applications of Computer Vision (WACV),7-8 Dec. 2009,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/MFI-2003.2003.1232635,Map generation based on the interaction between robot body and its surrounding environment,IEEE,Conferences,"This paper presents a method for map generation based on the interaction between a robot body and its surrounding environment. While a robot moves in the environment, the robot interacts with its surrounding environment. If the effect of the environment on the robot changes, such interactions also change. By observing the robot's body, our method detects such change of the interaction and generates a description representing the type of change and the location where such change is observed. In the current implementation, we assume that there are two types of the change in the interaction. The real robot experiments are conducted in order to show the validity of our method.",https://ieeexplore.ieee.org/document/1232635/,"Proceedings of IEEE International Conference on Multisensor Fusion and Integration for Intelligent Systems, MFI2003.",1-1 Aug. 2003,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SSCI.2016.7850169,Memetic robot control evolution and adaption to reality,IEEE,Conferences,"Inspired by animals' ability to learn and adapt to changes in their environment during life, hybrid evolutionary algorithms have been developed and successfully applied in a number of research areas. This paper explores the effects of learning combined with a genetic algorithm to evolve control system parameters for a four-legged robot. Here, learning corresponds to the application of a local search algorithm on individuals during evolution. Two types of learning were implemented and tested, i.e. Baldwinian and Lamarckian learning. On the direct results from evolution in simulation, Lamarckian learning showed promising results, with a significant increase in final fitness compared with the results from evolution without learning. Further experiments with learning on the real robot demonstrated an efficient adaptation of the robot gait to the real world environment, and increased the performance to the level measured in simulation. This paper demonstrates that Lamarckian evolution is effective in improving the performance of robot controller evolution, and that the same learning process on the physical robot efficiently reduces the negative impact of the simulation-reality gap.",https://ieeexplore.ieee.org/document/7850169/,2016 IEEE Symposium Series on Computational Intelligence (SSCI),6-9 Dec. 2016,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SIBCON.2019.8729597,Method of Maximum Permitted Learning Rate Calculation for Neural Controller of Balancing Robot,IEEE,Conferences,"This research is to solve a problem of sustainability of a balancing robot controlled by an artificial neural network. The mentioned network acts as a regulator and calculates at its output layer a control action for the plant. Online training of such a network is necessary to improve the quality of the robot control since it changes its parameters or a mode of functioning in the course of operation. Implementing such training, the question of the learning rate limitation arises sharply. It is directly related to the assessment of sustainability of the control system under consideration. That is why a method based on the second Lyapunov approach is proposed to calculate the upper allowable limit of the online learning rate for the neural network controller under various conditions at each moment of its functioning. This method does not require the plant mathematical model. The efficiency of the approach is proved by experiments with a real balancing robot based on the EV3 platform.",https://ieeexplore.ieee.org/document/8729597/,2019 International Siberian Conference on Control and Communications (SIBCON),18-20 April 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CINTI.2011.6108528,Mobile robot control using combined neural-fuzzy and neural network,IEEE,Conferences,"This paper describes the concept of the navigation system for a mobile robot. The system is using a combination of two navigation algorithms: self-learning neural network, necessary to form a movement plan for a robot, and a collision-free control algorithm based on heuristic neuro-fuzzy approach. The basic task of neural network is to generate initial path. Heuristic base of rules for collision free algorithm are limited and does not cover all situations. Main contribution of proposed navigation is related to neural network property to supplements special cases that are not covered by present heuristic rules from the data base. Both algorithms are adapted and implemented to navigate real platform of a mobile robot equipped by two independent wheel drives, encoders and a set of short-range sonars. The combined reactive algorithm (using two control methods) is used in real time for obstacle navigation in robotized system. Navigation algorithms are placed into a PC, which is connected to mobile robot by wireless and wired links. Experiments have shown ability of collision-free navigation of mobile robot in real time.",https://ieeexplore.ieee.org/document/6108528/,2011 IEEE 12th International Symposium on Computational Intelligence and Informatics (CINTI),21-22 Nov. 2011,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SISY.2009.5291131,Mobile robot control using self-learning neural network,IEEE,Conferences,"The paper describes the concept of the navigation system for a mobile robot. The system is using a navigation algorithm based on self-learning neural network, necessary to form a movement plan for a robot. The algorithm is adapted and implemented to navigate real platform of a mobile robot equipped by two independent wheel drives, encoders and a set of short-range sonars. Navigation algorithm is placed into a PC, which is connected to mobile robot by wireless and wired links. Experiments have shown ability of collision-free navigation of mobile robot in real time.",https://ieeexplore.ieee.org/document/5291131/,2009 7th International Symposium on Intelligent Systems and Informatics,25-26 Sept. 2009,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISIC.1998.713694,Mobile robot localization using the Hough transform and neural networks,IEEE,Conferences,"For an autonomous mobile robot to navigate in an unknown environment it is essential to know the location of the robot on a real-time basis. Finding position and orientation of a mobile robot in a world coordinate system is a problem in localization. Dead-reckoning is commonly used for localization, but position and orientation errors from dead-reckoning tend to accumulate over time. The objective of the paper is to develop a feature-based localization method that uses the Hough transform to detect wall-like features in the environment based on sonar range data. Although the Hough transform is an effective method for detecting lines and curves from noisy data it has the drawback of being sensitive to discretization resolution. Results of line detection using various discretization resolutions and using a winner-take-all neural network are presented and discussed. The results indicate that the Hough transform method is able to reliably recognize wall-like features from noisy sonar data, but the accuracy of detected features is dependent heavily on the choice of resolution of parameter discretization.",https://ieeexplore.ieee.org/document/713694/,Proceedings of the 1998 IEEE International Symposium on Intelligent Control (ISIC) held jointly with IEEE International Symposium on Computational Intelligence in Robotics and Automation (CIRA) Intell,17-17 Sept. 1998,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AMC.2006.1631674,Mobile robot navigation in an unknown environment,IEEE,Conferences,"This paper discusses application of an intelligent system in order to navigate in real-time a small size, four wheeled, indoor mobile robot accurately using ultra-light (160 gr), inexpensive laser range finder without prior information of the environment. A recurrent neural network is used to find the best path to the target of the robot. An accurate grid-based map is generated using a laser range finder scene and location found by a modified dead reckoning system. Finally a motion control method is presented. These approaches are implemented and tested in Resquake mobile robot",https://ieeexplore.ieee.org/document/1631674/,"9th IEEE International Workshop on Advanced Motion Control, 2006.",27-29 March 2006,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ROMAN.2009.5326151,"Motion capture and classification for real-time interaction with a bipedal robot using on-body, fully wireless, motion capture specknets",IEEE,Conferences,"This paper presents, to the best of our knowledge, the first instance of real-time human-robot interaction using motion capture (mocap) data obtained from fully wireless, on-body sensor networks. During the learning phase, data for motion such as waving of the hands, standing on a leg, performing sit-ups and squats is captured from a human strapped with the orient motion capture specks. Key features are extracted from the captured motion data using unsupervised learning algorithms. During subsequent interactions with the robot, the motion of the operator, speckled with orients, is classified and the robot selects to play the closest motion. This approach is particularly useful in situations where the robot operates a well defined vocabulary of motion, and the advantages are the real-time interaction and the rapidity (in a matter of minutes) in programming new behaviour compared to a heuristics-based approach. This paper compares the performances of three unsupervised learning algorithms: c-means, k-means and expectation maximisation (EM) for the four motion scenarios. Nine best candidates for the three learning algorithms for each of the four motion scenarios were selected in the Webots robot simulator and then transferred to the real robot. Metrics were defined for each motion scenario and their performances compared for the three learning algorithms. In all the cases the motions were able to be imitated; c-means was the best, followed closely by the k-means algorithms, and the reasons have been analysed.",https://ieeexplore.ieee.org/document/5326151/,RO-MAN 2009 - The 18th IEEE International Symposium on Robot and Human Interactive Communication,27 Sept.-2 Oct. 2009,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ROBOT.1990.125999,Motion planning for a whole-sensitive robot arm manipulator,IEEE,Conferences,"A sensor-based motion planning system for a robot arm manipulator must include these four basic components: sensor hardware; real-time signal/sensory data processing hardware/software a local step planning subsystem that works at the basic sample rate of the arm; and a subsystem for global planning. The objective of this work is to develop the fourth component, a real-time implementable algorithm that realizes the upper, global level of planning. Based on the current collection of local normals, the algorithm generates preferable directions of motion around obstacles, in order to guarantee reaching the target position if it is reachable. Experimental results from testing the developed system are also discussed.<>",https://ieeexplore.ieee.org/document/125999/,"Proceedings., IEEE International Conference on Robotics and Automation",13-18 May 1990,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AIAM54119.2021.00121,Multi-Robot Cooperative Hunting Based on Virtual Reality Technology,IEEE,Conferences,"Multi-robot cooperative rounding up is one of the important methods to test the intelligence level of multi-robot system. In order to test the intelligence of multi-robot system, a multi-robot cooperative rounding up system is established in this paper. In order to realize the rounding up system, this paper uses finite state machine to simplify the rounding up behavior of multi-robot. At the same time, in order to observe the multi-robot system more intuitively, this paper uses Vega software to realize virtual reality technology and applies it to the rounding up system established in this paper. Experiments show that the virtual reality technology based on VEGA can effectively simulate the multi-robot roundup, so that observers can observe the system more intuitively and effectively, so as to find out whether there are problems in the system.",https://ieeexplore.ieee.org/document/9724756/,2021 3rd International Conference on Artificial Intelligence and Advanced Manufacture (AIAM),23-25 Oct. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/INDIN45523.2021.9557363,Multi-Robot Multiple Camera People Detection and Tracking in Automated Warehouses,IEEE,Conferences,"In this work a multi-robot system is presented for people detection and tracking in automated warehouses. Each Automated Guided Vehicle (AGV) is equipped with multiple RGB cameras that can track the workers’ current locations on the floor thanks to a neural network that provides human pose estimation. Based on the local perception of the environment each AGV can exploit information about the tracked people for self-motion planning or collision avoidance.Additionally, data collected from each robot contributes to a global people detection and tracking system. A warehouse central management software fuses information received from all AGVs into a map of the current locations of workers. The estimated locations of workers are sent back to the AGVs to prevent potential collision. The proposed method is based on two-level hierarchy of Kalman filters. Experiments performed in a real warehouse show the viability of the proposed approach.",https://ieeexplore.ieee.org/document/9557363/,2021 IEEE 19th International Conference on Industrial Informatics (INDIN),21-23 July 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISKE.2010.5680764,Multilevel fuzzy navigation control scheme applied to a monitoring mobile robot,IEEE,Conferences,"The design, construction and real time performance of a mobile monitoring system based on a Khepera mobile robot are presented. The functions performed by the system are: (a) line following, (b) obstacle avoidance, (c) identification of test points along the path, (d) recognition of the mark (bar code) located at each test point and, (e) measuring of a physical parameter. For the navigation, an innovative multilevel fuzzy control scheme is implemented in which the fuzzy sensor fusion, related to the perception of the environment, reduces the complexity of the navigation function. Other distinctive characteristics are the identification of test points by means of a Kohonen's neural network and the processing of a one-dimensional video signal for recognition of landmarks located at each test point.",https://ieeexplore.ieee.org/document/5680764/,2010 IEEE International Conference on Intelligent Systems and Knowledge Engineering,15-16 Nov. 2010,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ROMAN.1995.531939,Multimedia sensing system for robot,IEEE,Conferences,"The purpose of this study is to realize a multimedia sensing system for robot. Using both image and sound processing, the system makes a robot track the person who is speaking. The sound direction is calculated from the phase difference between the sounds arriving at the right and left microphones (ears) of the robot. Then by detecting the synchronization between the sound and image changes, the system identifies the speaker. Furthermore, by introducing a multi-level synchronization checking and context analysis, the action pattern of the robot can be regulated to make the robot perform in a complicated environment with plural speakers. All the processes are performed in real-time. The proposed system is implemented in the information assistant robot ""Hadaly"".",https://ieeexplore.ieee.org/document/531939/,Proceedings 4th IEEE International Workshop on Robot and Human Communication,5-7 July 1995,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CEC.2007.4424774,Multiple sensors data integration using MFAM for mobile robot navigation,IEEE,Conferences,"The mobile robot navigation with complex environment needs more input space to match the environmental data into robot outputs in order to perform realistic task. At the same time, the number of rules at the rule base needs to be optimized to reduce the computing time and to provide the possibilities for real time operation. In this paper, the optimization of fuzzy rules using a modified fuzzy associative memory (MFAM) is designed and implemented. MFAM provides good flexibility to use multiple input space and reduction of rule base for robot navigation. This paper presents the MFAM model to generate the rule base for robot navigation. The behavior rules obtained from MFAM model are tested using simulation and real world experiments, and the results are discussed in the paper and compared with the existing methods.",https://ieeexplore.ieee.org/document/4424774/,2007 IEEE Congress on Evolutionary Computation,25-28 Sept. 2007,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICMLC.2016.7872960,Muscle-gesture robot hand control based on sEMG signals with wavelet transform features and neural network classifier,IEEE,Conferences,"In this paper, we propose a muscle gesture-computer interface (MGCI) system for a five-fingered robotic hand control employing a commercial wearable MYO gesture armband. Eight channels of surface EMG (sEMG) signals were acquired and segmented. Then four levels of Daubechies 5 Wavelet family were performed to analyze the EMG signal. Totally 72 features were extracted from the EMG raw data for 16 hand motions recognition utilizing artificial Neural Networks. The average of best overall classification rate during off-line training is 87.8%. Consequently, real-time hand gesture recognition was implemented to evaluate the performance of the proposed system and the average recognition accuracy was 89.38%. Finally, it was applied to control a five-fingered robot hand.",https://ieeexplore.ieee.org/document/7872960/,2016 International Conference on Machine Learning and Cybernetics (ICMLC),10-13 July 2016,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IECON.2000.972604,NN controller of the constrained robot under unknown constraint,IEEE,Conferences,"In this paper, the problems faced in the constrained force control is studied (uncertainties in dynamic model and the unknown constraints). A neural network (NN) controller is proposed based on the derived dynamic model of robot in the task space. The feed-forward neural network is used to adaptively compensate for the uncertainties in the robot dynamics. Training signals are proposed for the feed-forward neural network controller. The NN weights are tuned online, with no off-line learning phase required. An online estimation algorithm is developed to estimate the local shape of the constraint surface by using measured data on the force and position of the end-effector. The suggested controller is simple in structure and can be implemented easily. Real-time experiments are conducted using the five-bar robot to demonstrate the effectiveness of the proposed controller.",https://ieeexplore.ieee.org/document/972604/,"2000 26th Annual Conference of the IEEE Industrial Electronics Society. IECON 2000. 2000 IEEE International Conference on Industrial Electronics, Control and Instrumentation. 21st Century Technologies",22-28 Oct. 2000,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICMA52036.2021.9512587,Natural Residual Reinforcement Learning for Bicycle Robot Control,IEEE,Conferences,"This work focuses on motion control of the bicycle robot by using the proposed NRRL algorithm. Unlike the traditional RL algorithm, decomposing the main tasks into subtasks manually and introducing qualitative prior knowledge to the agent have been applied in the NRRL algorithm. Simulation results show that better performance and better sample efficiency of the proposed NRRL algorithm have been achieved in terms of balance control and path tracking of bicycle robot. It's believed that the NRRL algorithm is available on the real physical bicycle robot, and the deployment of the algorithm will be realized soon, as the real physical bicycle robot has been constructed currently.",https://ieeexplore.ieee.org/document/9512587/,2021 IEEE International Conference on Mechatronics and Automation (ICMA),8-11 Aug. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS.1991.174585,Neural network approach to path planning for two dimensional robot motion,IEEE,Conferences,"A method for robot obstacle avoidance and path planning is proposed. The algorithm is based on a camera image feedback loop utilizing a neural network for image processing. The method can successfully generate collision-free paths in a 2D robot workspace containing randomly-placed polygonal obstacles. The control algorithm is simple and robust and has low computational requirements. Controller simulation implemented on a personal computer can generate collision-free robot paths in real time, requiring approximately 1 sec. per robot move.<>",https://ieeexplore.ieee.org/document/174585/,Proceedings IROS '91:IEEE/RSJ International Workshop on Intelligent Robots and Systems '91,3-5 Nov. 1991,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ROBOT.1995.525344,Neural network based iterative learning controller for robot manipulators,IEEE,Conferences,"An efficient neural network based learning control scheme is proposed to solve the trajectory tracking controI problem of robot manipulators. The proposed approach has four distinctive characteristics: 1) good tracking performance can be achieved during the first learning trial; 2) learning algorithm for adjusting neural network weights is independent of the manipulator dynamic model, thus displays strong robustness to torque disturbances and model parameter uncertainty; 3) no acceleration measurement or estimation is needed; and 4) real-time implementation with a higher sampling rate is readily possible. Simulation results on a 3 degree-of-freedom manipulator are presented to show its validity.",https://ieeexplore.ieee.org/document/525344/,Proceedings of 1995 IEEE International Conference on Robotics and Automation,21-27 May 1995,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS.2003.1250614,Non-learning artificial neural network approach to motion planning for the Pioneer robot,IEEE,Conferences,This paper describes the implementation of a biologically-inspired non-learning artificial neural network for robot motion planning on the Pioneer 2DX robot. This motion planner fits within the Saphira operating system. The deliberative ANN motion planner is able to respond to changing situations in real time and complements the reactive behaviours.,https://ieeexplore.ieee.org/document/1250614/,Proceedings 2003 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2003) (Cat. No.03CH37453),27-31 Oct. 2003,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IECON.2006.347441,Obstacle avoidance algorithm based on biological patterns for anthropomorphic robot manipulator,IEEE,Conferences,"This study addresses the problem of collision-free controlling of 3-DOF (degree of freedom) anthropomorphic manipulators with given a priori unrestricted trajectory. The robot constraints resulting from the physical robot's actuators are also taken into account during the robot movement. Obstacle avoidance algorithm is based on penalty function, which is minimized when collision is predicted. Mathematical construction of penalty function and minimization process allows modeling of variety behaviors of robot elusion moves. Implementation of artificial neural network (ANN) inside the control process gives the additional flexibility needed to remember most important robot behaviors based on biological pattern of human arm moves. Thanks to the fast collisions' detection, the presented algorithm appears to be applicable to the industrial real-time implementations. Numerical simulations of the anthropomorphic manipulator operating in three dimensional space with obstacles is also presented",https://ieeexplore.ieee.org/document/4152937/,IECON 2006 - 32nd Annual Conference on IEEE Industrial Electronics,6-10 Nov. 2006,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA48506.2021.9561790,Occupancy Map Inpainting for Online Robot Navigation,IEEE,Conferences,"In this work, we focus on mobile robot navigation in indoor environments where occlusions and field-of-view limitations hinder onboard sensing capabilities. We show that the footprint of a camera mounted on a robot can be drastically improved using learning-based approaches. Specifically, we consider the task of building an occupancy map for autonomous navigation of a robot equipped with a depth camera. In our approach, a local occupancy map is first computed using measurements from the camera directly. Afterwards, an inpainting network adds further information, the occupancy probabilities of unseen grid cells, to the map. A novel aspect of our approach is that rather than direct supervision from ground truth, we combine the information from a second camera with a better field-of-view for supervision. The training focuses on predicting extensions of the sensed data. To test the effectiveness of our approach, we use a robot setup with a single camera placed at 0.5m above the ground. We compare the navigation performance using raw maps from only this camera’s input (baseline) versus using inpainted maps augmented with our network. Our method outperforms the baseline approach even in completely new environments not included in the training set and can yield 21% shorter paths than the baseline approach. A real-time implementation of our method on a mobile robot is also tested in home and office environments.",https://ieeexplore.ieee.org/document/9561790/,2021 IEEE International Conference on Robotics and Automation (ICRA),30 May-5 June 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA.2013.6630740,Off-line path integral reinforcement learning using stochastic robot dynamics approximated by sparse pseudo-input Gaussian processes: Application to humanoid robot motor learning in the real environment,IEEE,Conferences,"We develop fast reinforcement learning (RL) framework using the approximated dynamics of a humanoid robot. Although RL is a useful non-linear optimizer, applying it to real robotic systems is usually difficult due to the large number of iterations required to acquire suitable policies. In this study, we approximate the dynamics using data from a real robot with sparse pseudo-input Gaussian processes (SPGPs). By using SPGPs, we estimated the probability distribution considering both the input vector and output signal variances. In real environments, since the observations from robotic sensors include large noise, SPGPs can suitably approximate the stochastic dynamics of a real humanoid robot. We use the approximated dynamics to improve the performance of a movement task in a path integral RL framework, which updates a policy from the sampled trajectories of the state and action vectors and the cost. We implemented our proposed method on a real humanoid robot and tested on a via-point reaching task. The robot achieved successful performance with fewer number of interactions with the real environment by using the proposed method than a conventional approach which dose not use the simulated dynamics.",https://ieeexplore.ieee.org/document/6630740/,2013 IEEE International Conference on Robotics and Automation,6-10 May 2013,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICAS.2006.40,On the conception of an autonomous and modular robot based on an Event Driven OSEK System with deterministic real-time behavior,IEEE,Conferences,"In this paper, we are interested in the design of an autonomous and modular self-reconfigurable robot having self-assembly capability and deterministic behavior. The ability of a modular robot to meet its mission strongly depends on the artificial intelligence software and on the underlying hardware and software architecture. The artificial intelligence software of a robot is mapped into several elementary tasks with different real-time constraints. We propose in this paper a real-time analysis taking into account kernel overheads for the validation of the real-time behavior of an artificial intelligence software. We study the OSEK operating system that requires few hardware resources and is cost effective. The overheads are due to the context switching mechanism which activates, terminates, and reschedules tasks, and to the periodic timer used to create the time base which is necessary for the periodic tasks model. We show how to take into account those overheads in the feasibility conditions. We compare the theoretical worst case response time obtained with kernel overheads to the response time obtained on a task set, on a real robot, based on the event driven OSEK implementation. We show that the kernel overheads cannot be neglected and that the theoretical results are valid and can be used to ensure a deterministic behavior of the robot",https://ieeexplore.ieee.org/document/1690225/,International Conference on Autonomic and Autonomous Systems (ICAS'06),19-21 July 2006,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/MWSCAS.1991.251981,On the fast robot dynamic parameters learning,IEEE,Conferences,"A computationally efficient solution to the problem of identifying the dynamic parameters of a robot manipulator is presented. The identification procedure is based on a simplified form of the dynamics. The approach has three important characteristics. First, being based on the Lagrangian representation, the equations are linear in the dynamic parameters, which makes possible the application of linear identification techniques. Second, the dynamic parameters are easily recognized, extracted, and grouped. Third, the equations are amenable to the implementation of parallel processing schemes. For the identification a recursive least squares algorithm is used. The algorithm is distributed over a network of transputers. Real-time results have been produced to demonstrate the speedup and efficiency of the proposed technique. A case study is given for the first three links of the Stanford Arm positioning system.<>",https://ieeexplore.ieee.org/document/251981/,[1991] Proceedings of the 34th Midwest Symposium on Circuits and Systems,14-17 May 1992,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ROMAN.2007.4415153,Online Affect Detection and Adaptation in Robot Assisted Rehabilitation for Children with Autism,IEEE,Conferences,"This paper presents a novel affect-sensitive human-robot interaction framework for rehabilitation of children with autism spectrum disorder (ASD) where the robot can detect the affective cues of the children implicitly and response to them appropriately. Psychophysiological analysis is performed that uses subjective reports of the affective states from a clinical observer. Comprehensive physiological indices are investigated that may correlate with the affective states of children with ASD. A robot uses a support vector machines based affect model to detect the affective cues. A reinforcement learning based adaptation mechanism is employed to allow the robot to adjust its behaviors autonomously as a function of the predicted children's affective state. Four adolescents diagnosed with high-functioning autism participated in the experiments. This is the first time, to our knowledge, that the affective states of children with ASD have been detected via physiology-based affective modeling technique in real-time. This is also the first time that impact of affect-sensitive interaction between a robot and children with ASD in closed loop has been demonstrated experimentally.",https://ieeexplore.ieee.org/document/4415153/,RO-MAN 2007 - The 16th IEEE International Symposium on Robot and Human Interactive Communication,26-29 Aug. 2007,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICMA.2019.8816298,Online Learning of the Inverse Dynamics with Parallel Drifting Gaussian Processes: Implementation of an Approach for Feedforward Control of a Parallel Kinematic Industrial Robot,IEEE,Conferences,"The present paper deals with an online approach to learn the inverse dynamics of any robot. This is realized by the use of Gaussian Processes drifting parallel along the system data. An extension by a database enables the efficient use of data points from the past. The central component of this work is the implementation of such a method in a controller in order to achieve the actual goal: the feedforward control of an industrial robot by means of machine learning. This is done by splitting the procedure into two threads running parallel so that the prediction is decoupled from the computing-intensive training of the models. Experiments show that the method reduces the tracking errors more clearly than an elaborately identified rigid body model including friction. For a defined trajectory, the squared areas of the tracking errors of all axes are reduced by more than 54% compared to motion without pre-control. In addition, a highly dynamic pick-and-place experiment is used to investigate the possible changes in system dynamics. Compared to an offline trained model, the approximation error of the proposed online approach is smaller for the remaining time of the experiment after an initial phase. Furthermore, this error is smaller throughout the experiment for online learning with parallel drifting Gaussian Processes than when using a single one.",https://ieeexplore.ieee.org/document/8816298/,2019 IEEE International Conference on Mechatronics and Automation (ICMA),4-7 Aug. 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA.2012.6224998,Online audio beat tracking for a dancing robot in the presence of ego-motion noise in a real environment,IEEE,Conferences,"This paper presents the design and implementation of a real-time real-world beat tracking system which runs on a dancing robot. The main problem of such a robot is that, while it is moving, ego noise is generated due to its motors, and this directly degrades the quality of the audio signal features used for beat tracking. Therefore, we propose to incorporate ego noise reduction as a pre-processing stage prior to our tempo induction and beat tracking system. The beat tracking algorithm is based on an online strategy of competing agents sequentially processing a continuous musical input, while considering parallel hypotheses regarding tempo and beats. This system is applied to a humanoid robot processing the audio from its embedded microphones on-the-fly, while performing simplistic dancing motions. A detailed and multi-criteria based evaluation of the system across different music genres and varying stationary/non-stationary noise conditions is presented. It shows improved performance and noise robustness, outperforming our conventional beat tracker (i.e., without ego noise suppression) by 15.2 points in tempo estimation and 15.0 points in beat-times prediction.",https://ieeexplore.ieee.org/document/6224998/,2012 IEEE International Conference on Robotics and Automation,14-18 May 2012,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICSMC.1998.726546,Online learning neural network controller for pneumatic robot position control,IEEE,Conferences,"This paper presents the implementation of online learning neural network controller in the pneumatic robot position servo control. The advantages of this design include: the ability to compensate for nonlinearities, and it is insensitive to system parameter time-varying. The traditional PID controller is replaced by neural network controller trained online to learn the inverse model of the pneumatic manipulator by backpropagation of the performance error. The simulation studies and experimental results on the PID controller, online learning neural network controller and off-line training neural network controller, are presented and discussed.",https://ieeexplore.ieee.org/document/726546/,"SMC'98 Conference Proceedings. 1998 IEEE International Conference on Systems, Man, and Cybernetics (Cat. No.98CH36218)",14-14 Oct. 1998,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICMA.2012.6285128,Online learning of COM trajectory for humanoid robot locomotion,IEEE,Conferences,"Center Of Mass (COM) trajectory is an essential factor for stable and natural robot locomotion. Unlike previous research in which COM trajectory either restricted by ZMP trajectory or directly predefined by simple function such as sinusoid, this research aims to establish the COM trajectory by online autonomous learning under the objective of locomotion stability and naturalness, which is expressed as a self-consistent measure in this paper. It provides an alternative that may avoid or weaken the mismatch between theoretical planning and practical implementation. The experimental results on a real humanoid robot PKU-HR4 show its effectiveness and promising future.",https://ieeexplore.ieee.org/document/6285128/,2012 IEEE International Conference on Mechatronics and Automation,5-8 Aug. 2012,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/INDIN.2006.275808,"Ontology for Cognitics, Closed-Loop Agility Constraint, and Case Study - a Mobile Robot with Industrial-Grade Components",IEEE,Conferences,"The paper refers to intelligent industrial automation. The objective is to present key elements and methods for best practice, as well as some results obtained. The first part presents an ontology for automated cognition (cognitics), where, based on information and time, the main cognitive concepts, including those of complexity, knowledge, expertise, learning, intelligence abstraction, and concretization are rigorously defined, along with corresponding metrics and specific units. Among important conclusions at this point are the fact that reality is much too complex to be approached better than through much simplified models, in very restricted contexts. Another conclusion is the necessity to be focused on goal. Extensions are made here for group behavior. The second part briefly presents a basic law governing the choice of overall control architecture: achievable performance level of control system in terms of agility, relative to process dynamics, dictates the type of approaches which is suitable, in a spectrum which ranges from simple threshold-based switching, to classical closed-loop calculus (PID, state space multivariable systems, etc.), up to ""impossible"" cases where additional controllers must be considered, leading to cascaded, hierarchical control structures. For complex cases such as latter ones, new tools and methodologies must be designed, as is typical in O3NEIDA initiative, at least for software components. Finally, a large part of the paper presents a case study, a mobile robot, i.e. an embedded autonomous system with distributed, networked control, featuring industry-grade components, designed with the main goal of robust functionality. The case illustrates several of the concepts introduced earlier in the paper.",https://ieeexplore.ieee.org/document/4053562/,2006 4th IEEE International Conference on Industrial Informatics,16-18 Aug. 2006,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/UR49135.2020.9144838,Outdoor Robot Navigation System using Game-Based DQN and Augmented Reality,IEEE,Conferences,"This paper presents a deep reinforcement learning based robot outdoor navigation method using visual information. The deep q network (DQN) maps the visual data to robot action in a goal location reaching task. An advantage of the proposed method is that the implemented DQN is trained in the first-person shooter (FPS) game-based simulated environment provided by ViZDoom. The FPS simulated environment reduces the differences between the training and the real environments resulting in a good performance of trained DQNs. In our implementation a marker-based augmented reality algorithm with a simple object detection method is used to train the DQN. The proposed outdoor navigation system is tested in the simulation and real robot implementation, with no additional training. Experimental results showed that the navigation system trained inside the game-based simulation can guide the real robot in outdoor goal directed navigation tasks.",https://ieeexplore.ieee.org/document/9144838/,2020 17th International Conference on Ubiquitous Robots (UR),22-26 June 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ROBOT.1991.131722,Parallel robot motion planning,IEEE,Conferences,"A fast, parallel method for computing configuration space maps is presented. The method is made possible by recognizing that one can compute a family of primitive maps which can be combined by superposition based on the distribution of real obstacles. This motion planner has been implemented for the first three degrees-of-freedom of a Puma robot in *Lisp on a Connection Machine with 8 K processors. A six degree-of-freedom version of the algorithm which performs a sequential search of the six-dimensional configuration space, building three-dimensional cross sections in parallel, has also been implemented.<>",https://ieeexplore.ieee.org/document/131722/,Proceedings. 1991 IEEE International Conference on Robotics and Automation,9-11 April 1991,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/URAI.2012.6463006,Path planning through maze routing for a mobile robot with nonholonomic constraints,IEEE,Conferences,"A comprehensive technique to plan path for a mobile robot with nonholonomic constraints through maze routing technique has been presented. Our robot uses a stereo vision based approach to detect the obstacles by creating dense 3D point clouds from the stereo images. ROS packages have been implemented on the robot for specific tasks of providing: i) Linear and angular velocity commands, ii) Calibration and rectification of the stereo images for generating point clouds, iii) Simulating the URDF (Unified Robot Description Format) module in real time, with respect to the real robot and iv) For visualizing the sensor data. For efficient path planning a hybrid technique using Lee's algorithm, modified by Hadlock and Soukup's algorithm has been implemented. Different path planning results have been shown using the maze routing algorithms. Preliminary results shows that Lee's algorithm is more time consuming in comparison with other algorithms. A hybrid of Lee's with Soukup's algorithm is more efficient but unpredictable for minimal path. A hybrid of Lee's with Hadlock's algorithm is the most efficient and least time consuming.",https://ieeexplore.ieee.org/document/6463006/,2012 9th International Conference on Ubiquitous Robots and Ambient Intelligence (URAI),26-28 Nov. 2012,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICSMC.1996.571370,Perception-action method for mobile robot plan and control based on driving experience,IEEE,Conferences,"A method of navigation and control for mobile robot is introduced. It combines task planning and path tracking together with the principle of ""perception-action"" under the guidance of ""goal planning"". First, we discuss the behavior of the mobile robot in an outdoor real world for the purpose of setting up a mixed layered architecture with ""perception-action"" and ""goal planning"". Then a simple but effective approach of sensor based navigation and control is described for the implementation of the architecture. Finally, we give some improvements based on the human-driving experience concerning path tracking and control for the mobile robot moving on outdoor semistructured roads. The experiments carried out on our THMR-III (Tsinghua Mobile Robot III) mobile robot navigating in the real world showed the method described was effective and robust.",https://ieeexplore.ieee.org/document/571370/,"1996 IEEE International Conference on Systems, Man and Cybernetics. Information Intelligence and Systems (Cat. No.96CH35929)",14-17 Oct. 1996,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.23919/ICCAS52745.2021.9649792,Performance Evaluation of YOLOv3 and YOLOv4 Detectors on Elevator Button Dataset for Mobile Robot,IEEE,Conferences,"The performance evaluation of an AI network model is the important part for building an effective solution before its deployment in real-world on the robot. In our study, we have implemented YOLOv3-tiny and YOLOv4-tiny darknet based frameworks for performance evaluation of the elevator button recognition task and tested both variants on image and video datasets. The objective of our study is two-fold: First, to overcome the limitation of elevator buttons dataset by creating new dataset and increasing its quantity without compromising the quality; Second, to provide a comparative analysis through experimental results and the performance evaluation of both detectors using four machine learning metrics. The purpose of our work is to assist the researchers and developers in decision making of suitable detector selection for deployment in the elevator robot towards button recognition application. The results show that YOLOv4-tiny outperforms YOLOv3-tiny with an overall accuracy of 98.60% compared to 97.91% at 0.5 IoU.",https://ieeexplore.ieee.org/document/9649792/,"2021 21st International Conference on Control, Automation and Systems (ICCAS)",12-15 Oct. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISIE51582.2022.9831515,Person Re-Identification on a Mobile Robot Using a Depth Camera,IEEE,Conferences,"In this paper, we designed and implemented a real-time person re-identification API on a mobile robot, for a closed-and open-world setting, using only the IR gray value image of a depth camera. Since common datasets are not usable we created our own dataset using the IR gray value images, the pose and image processing techniques. Then we trained the state-of-the-art neural network for person re-identification with common parameters and methods. For running it in real-time, we sped up the model as well as the application. It is possible to re-identify three persons at once, at around 10 FPS. Our model reaches as closed-world setting a rank-1-accuracy of 95.5 &#x0025;. With an additional threshold, coming from rank-1-accuracy of closed-world setting, our real-time application reaches as open-world setting a f&#x0027;l-score of 79.44 &#x0025; and a recall of 68.44 &#x0025;.",https://ieeexplore.ieee.org/document/9831515/,2022 IEEE 31st International Symposium on Industrial Electronics (ISIE),1-3 June 2022,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA.2011.5979792,Physical human robot interaction in imitation learning,IEEE,Conferences,"This video presents our recent research on the integration of physical human-robot interaction (pHRI) into imitation learning. First, a marker control approach for real time human motion imitation is shown. Secondly, physical coaching in addition to observational learning is applied for the incremental learning of motion primitives. Last, we extend imitation learning to learning pHRI which includes the establishment of intended physical contacts. The proposed methods were implemented and tested using the IRT humanoid robot and DLR's humanoid upper-body robot Justin.",https://ieeexplore.ieee.org/document/5979792/,2011 IEEE International Conference on Robotics and Automation,9-13 May 2011,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA.2014.6907470,Posture control of a three-segmented tracked robot with torque minimization during step climbing,IEEE,Conferences,"In this paper, we present a posture control scheme for step climbing by an in-house developed three-segmented tracked robot, miniUGV. The posture control scheme results in minimum torque at the actuated joints of the segments. Non-linear optimization is carried out offline for progressively decreasing distance of the robot from the step with torque minimization as objective function and force balance, motor torque limits, slippage avoidance and interference avoidance constraints. The resulting angles of the joints are fitted to a third degree polynomial as a function of the robot distance from the step and the step height. It is shown that a single set of polynomial functions is sufficient for climbing steps of all permissible heights and angles of attack of the front segment. The methodology has been verified through simulation followed by implementation on the real robot. As a consequence of this optimization we find that the average current reduced by more than thirty percent, reducing power consumption and confirming the efficacy of the optimization framework.",https://ieeexplore.ieee.org/document/6907470/,2014 IEEE International Conference on Robotics and Automation (ICRA),31 May-7 June 2014,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IJCNN.2014.6889830,Predictive Hebbian association of time-delayed inputs with actions in a developmental robot platform,IEEE,Conferences,"The work described here explores a neural network architecture that can be embedded directly in the realtime sensorimotor coordination loop of a developmental robot platform. We take inspiration from the way children are able to learn while interacting with a teacher, in particular the use of prediction of the teacher actions to improve own learning. The architecture is based on two neural networks that operate online, and in parallel, one for learning and one for prediction. A Hebbian learning rule is used to associate the high-dimensional afferent sensor input at different time-delays with the current efferent motor commands corresponding to the teacher demonstration. The predictions of future motor commands are used to limit the growth of the neural network weights, and to enable the robot to smoothly continue movements the teacher has begun. Results on a simulated iCub robot learning object interaction tasks are presented, including an analysis of the sensitivity to changes in the task setup. We also outline the first implementation on the real iCub platform.",https://ieeexplore.ieee.org/document/6889830/,2014 International Joint Conference on Neural Networks (IJCNN),6-11 July 2014,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/TAAI54685.2021.00062,Preliminary Implementation of Grasping Operation by a Collaborative Robot Arm: Using a Ball as Example,IEEE,Conferences,"Grasping objects is one of the basic functions of a robot arm. This study completed the implementation of the process in which a collaborative robot (cobot) arm grasps an object. Hardware components included a depth camera, cobot arm, and artificial intelligence equipment for edge computing. Software components included computer visualization techniques, deep learning, and robot operating system. To complete the preliminary implementation of the system, the grasping operation of the robot arm was set to target a ball. This system implementation sheds light on how robot arms and deep learning techniques are applied to real-life problems. Experiments verified that the preliminary system implemented was able to correctly complete the ball-grasping operation and achieve the pragmatic goal.",https://ieeexplore.ieee.org/document/9778057/,2021 International Conference on Technologies and Applications of Artificial Intelligence (TAAI),18-20 Nov. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IMTC.1999.776982,Problems and solutions in acquisition and interpretation of sensorial data on a mobile robot,IEEE,Conferences,"We discuss some guidelines to cope with problems that arise when using cheap and simple sensors on mobile, autonomous, robotic agents. In particular we focus on the perceptual aliasing problem and on the possibility to perform active sensor data acquisition. We present a robotic architecture that we have implemented on a real robot following the proposed guidelines. The obtained mobile robot satisfies the design specifications, navigating autonomously in an unstructured environment.",https://ieeexplore.ieee.org/document/776982/,IMTC/99. Proceedings of the 16th IEEE Instrumentation and Measurement Technology Conference (Cat. No.99CH36309),24-26 May 1999,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICEC.1996.542714,Propagating learned behaviors from a virtual agent to a physical robot in reinforcement learning,IEEE,Conferences,"For a physical robot to acquire behaviors, it is important for it to learn in the physical environment. Since reinforcement learning requires large computation costs as well as a lot of time in the physical environment, most research has performed learning by simulation. However, this does not work well in the real world. Realizing reinforcement learning of a physical robot in a physical environment requires both an adaptation for the diversity of possible situations and a high-speed learning method that can learn from fewer trials. This paper describes cooperative reinforcement learning based on propagating the learned behaviors of a virtual agent to a physical robot in order to accelerate learning in a physical environment. The method consists of two parts: (1) preparation learning in a virtual environment to accelerate initial learning, which accounts for most of the learning cost; and, (2) refinement learning in a physical environment by using the virtual learning results as an initial behavior set of a physical robot. Experimental results are given for a ball-pushing task with the physical robot and a virtual agent.",https://ieeexplore.ieee.org/document/542714/,Proceedings of IEEE International Conference on Evolutionary Computation,20-22 May 1996,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICICIP47338.2019.9012202,RBF Neural Network-Sliding Model Control Approach for Lower Limb Rehabilitation Robot Based on Gait Trajectories of SEMG Estimation,IEEE,Conferences,"This paper designed and developed a new RBF neural network-sliding model controller for patients with stroke and lower extremity motor dysfunction, and applied it to a 3 degrees of freedom (3-DOF) lower limb rehabilitation robot (LLRR) for passive rehabilitation of patients. At first, a simple LLRR structure is designed that can be adjusted to fit the patient at the hip, knee, and ankle joints. Then, the patient's sEMG signal is obtained to predict the expected trajectory of the LLRR system, where the EMG signal is detected by BIOPAC software. Moreover, a RBF neural network-sliding model approach is designed for the dynamics model of the LLRR, and the asymptotic stability of the controller is verified via a Lyapunov theorem. Finally, LLRR system is experimentally verified by the MATLAB software, which exploit that the proposed control approach is feasible and effective for the lower extremity patients. Thereby, the developed control approach has illustrated high efficiency and robustness for the patient's passive rehabilitation training in real-time.",https://ieeexplore.ieee.org/document/9012202/,2019 Tenth International Conference on Intelligent Control and Information Processing (ICICIP),14-19 Dec. 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISIE.2005.1529114,Real time implementation of a selective attention model for the intelligent robot with autonomous mental development,IEEE,Conferences,"We propose a biologically motivated selective attention model to find an object based on context free search for an intelligent robot with an autonomous mental development (AMD) mechanism. For real-time operation of the selective attention model in the robot system, we have considered a way to reduce the computational load of the selective attention model, which uses a simplified symmetry operation with retina-topic sampling and look-up table in the localized candidate attention region. As a result, our model can perform within 270 ms at Pentium-4 2.8Ghz, and obtain a plausible human-like visual scan path in order to pay attention to an object preferentially. Then, we implemented an intelligent mobile robot with selective attention for an AMD mechanism.",https://ieeexplore.ieee.org/document/1529114/,"Proceedings of the IEEE International Symposium on Industrial Electronics, 2005. ISIE 2005.",20-23 June 2005,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/WCICA.2000.863455,Real time tracking in robot teleoperation system,IEEE,Conferences,"The robot teleoperation system based on stereo vision was developed by the State Key Lab of Intelligent Technology and System of Tsinghua University. The paper presents the design frame of the whole system, and describes in detail some of the key design and implementation problems. Finally, the paper analyses the difficulty of applying this technology to virtual reality and augmented reality systems, and some suggestions are provided. The success of this system can contribute to further research on augmented reality.",https://ieeexplore.ieee.org/document/863455/,Proceedings of the 3rd World Congress on Intelligent Control and Automation (Cat. No.00EX393),26 June-2 July 2000,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICMLC.2006.259112,Real-Time Implementation of a PD+DFNNS Controller for Compliance Robot,IEEE,Conferences,"This paper presents the compliance control of a robot manipulator under a constrained environment. The controller design proposed herein is based on the intelligence adaptive control scheme. In this design, the DFNNs (dynamic fuzzy neural networks) and PD feedback controllers control the position and the contact force of robot end-effector. The DFNNs controller is employed to compensate for environmental variations such as payload mass and disturbance torque during the operation process; PD feedback controllers control the position and the contact force of end-effector. Applying these controllers allows us to adapt the manipulator to the unknown surface of the surrounding environment and to have close contact with the curved surface",https://ieeexplore.ieee.org/document/4028106/,2006 International Conference on Machine Learning and Cybernetics,13-16 Aug. 2006,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/DEVLRN.2005.1490961,Real-Time Multi-View Face Tracking for Human-Robot Interaction,IEEE,Conferences,"For face tracking in a video sequence, various face tracking algorithms have been proposed. However, most of them have difficulty in finding the initial position and size of a face automatically. In this paper, we present a fast and robust method for fully automatic multi-view face detection and tracking. Using a small number of critical rectangle features selected and trained by the Adaboost learning algorithm, we can detect the initial position, size and view of a face correctly. Once a face is reliably detected, we can extract face and upper body color distribution from the detected facial regions and upper body regions for building robust color modeling respectively. Simultaneously, each color modeling is performed by using k-means clustering and multiple Gaussian models. Then, fast and efficient multi-view face tracking is executed by using several critical features. Our proposed algorithm is robust to rotation, partial occlusions, and scale changes in front of dynamic, unstructured background. In addition, our proposed method is computationally efficient. Therefore, it can be executed in real-time",https://ieeexplore.ieee.org/document/1490961/,"Proceedings. The 4th International Conference on Development and Learning, 2005",19-21 July 2005,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SYSOSE.2015.7151922,Real-time FPGA decentralized inverse optimal neural control for a Shrimp robot,IEEE,Conferences,"This paper presents a field programmable gate array (FPGA) implementation for a decentralized inverse optimal neural controller for unknown nonlinear systems, in presence of external disturbances and parameter uncertainties. This controller is based on two techniques: first, an identifier using a discrete-time recurrent high order neural network (RHONN) trained with an extended Kalman filter (EKF) algorithm; second, on the basis of the neural identifier a controller which uses inverse optimal control, is designed to avoid solving the Hamilton Jacobi Bellman (HJB) equation. The proposed scheme is implemented in real-time to control a Shrimp robot.",https://ieeexplore.ieee.org/document/7151922/,2015 10th System of Systems Engineering Conference (SoSE),17-20 May 2015,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CCA.1994.381367,Real-time control of a robot using neural networks,IEEE,Conferences,"The real-time computation of the robot kinematics is very important. The basic transformations are the direct kinematic transformation (DKT) and the inverse kinematic transformation (IKT). The DKT can be computed in a straightforward way using the Denavit-Hartenberg notation. No such general method yet exists for the IKT, although this transformation is of major interest for control purposes. In this paper a neural network is presented that maps the IKT independent of the type of robot. After training, the network achieves very good accuracy and may easily be implemented in real-time. The performance of the algorithm is tested an the RTX robot, a SCARA-type robot with six degrees of freedom. This robot is controlled by a distributed control system. A host computer realizes the continuous path control and a network of 5 slave-transputers is used to compute the local controls and to drive the DC servomotors.<>",https://ieeexplore.ieee.org/document/381367/,1994 Proceedings of IEEE International Conference on Control and Applications,24-26 Aug. 1994,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IJCNN.2013.6706785,Real-time decentralized inverse optimal neural control for a Shrimp robot,IEEE,Conferences,"This paper deals with a decentralized inverse optimal neural controller for MIMO discrete-time unknown nonlinear systems, in a presence of external disturbances and parameter uncertainties. It uses two techniques: first, an identifier based on a discrete-time recurrent high order neural network (RHONN) trained with an extended Kalman filter (EKF) algorithm; second, on the basis of the real identifier a controller which uses inverse optimal control, is designed to avoid solving the Hamilton Jacobi Bellman (HJB) equation. The proposed scheme is implemented in real-time to control a Shrimp robot.",https://ieeexplore.ieee.org/document/6706785/,The 2013 International Joint Conference on Neural Networks (IJCNN),4-9 Aug. 2013,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CCA.2009.5280998,Real-time decentralized neural backstepping controller for a robot manipulator,IEEE,Conferences,This paper deals with adaptive trajectory tracking for discrete-time MIMO nonlinear systems. A high order neural network (HONN) is used to approximate a decentralized control law designed by the backstepping technique as applied to a block strict feedback form (BSFF). The HONN learning is performed online by an Extended Kalman Filter (EKF) algorithm. The proposed scheme is implemented in real-time to control a two DOF robot manipulator.,https://ieeexplore.ieee.org/document/5280998/,"2009 IEEE Control Applications, (CCA) & Intelligent Control, (ISIC)",8-10 July 2009,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS.2009.5354338,Real-time decentralized neural block controller for a robot manipulator,IEEE,Conferences,"This paper presents a discrete-time decentralized control scheme for identification and trajectory tracking of a two degrees of freedom (DOF) robot manipulator. A recurrent high order neural network (RHONN) structure is used to identify the plant model and based on this model, a discrete-time control law is derived, which combines discrete-time block control and sliding modes techniques. The neural network learning is performed online by Kalman filtering. A controller is designed for each joint, using only local angular position and velocity measurements. These simple local joint controllers allow trajectory tracking with reduced computations. The proposed scheme is implemented in real-time to control a two DOF robot manipulator.",https://ieeexplore.ieee.org/document/5354338/,2009 IEEE/RSJ International Conference on Intelligent Robots and Systems,10-15 Oct. 2009,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISIC.2010.5612924,Real-time five DOF robot control using a decentralized neural backstepping scheme,IEEE,Conferences,This paper presents a discrete-time decentralized control scheme for trajectory tracking of a five degrees of freedom (DOF) redundant robot. A high order neural network (HONN) is used to approximate a decentralized control law designed by the backstepping technique as applied to a block strict feedback form (BSFF). The neural network learning is performed on-line by Kalman filtering. The controllers are designed for each joint using only local angular position and velocity measurements. These simple local joint controllers allow trajectory tracking with reduced computations. The applicability of the proposed scheme is illustrated via real-time implementation.,https://ieeexplore.ieee.org/document/5612924/,2010 IEEE International Symposium on Intelligent Control,8-10 Sept. 2010,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS.2005.1545240,Real-time human motion analysis for human-robot interaction,IEEE,Conferences,"This paper introduces a novel real-time human motion analysis system based on hierarchical tracking and inverse kinematics. This work constitutes a first step towards our goal of implementing a mechanism of human-machine interaction that allows a robot to provide feedback to a teacher in an imitation learning framework. In particular, we have developed a computer-vision based, upper-body motion analysis system that works without special devices or markers. Since such system is unstable and can only acquire partial information because of self-occlusions and depth ambiguity, we have employed a model-based pose estimation method based on inverse kinematics. The resulting system can estimate upper-body human postures with limited perceptual cues, such as centroid coordinates and disparity of head and hands.",https://ieeexplore.ieee.org/document/1545240/,2005 IEEE/RSJ International Conference on Intelligent Robots and Systems,2-6 Aug. 2005,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICAR.1997.620222,Real-time navigation of a mobile robot using Kohonen's topology conserving neural network,IEEE,Conferences,"This paper proposes a real-time sensor based navigation method using Kohonen's topology conserving network for navigation of a mobile robot in any uncertain environment. The sensory information including target location with respect to current location of the mobile robot, have been discretely conserved using a two dimensional Kohonen lattice. Reinforcement learning based on a stochastic real valued technique have been implemented to compute the action space for this Kohonen lattice. The proposed scheme learns the input and output weight space of the Kohonen lattice which is generalized to any workspace. The effectiveness of the proposed scheme has been established by simulation where the complete domain of the input-space is quantized based on experience on sensory data encountered in real-time. The input-output mapping conserved by the Kohonen lattice during simulation was used to guide a mobile robot in a real-time environment. Successful navigation of the mobile robot without further training confirms the robustness of the proposed scheme.",https://ieeexplore.ieee.org/document/620222/,1997 8th International Conference on Advanced Robotics. Proceedings. ICAR'97,7-9 July 1997,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ROBOT.2001.932598,Real-time robot learning,IEEE,Conferences,"This paper presents the design, implementation and testing of a real-time system using computer vision and machine learning techniques to demonstrate learning behavior in a miniature mobile robot. The miniature robot, through environmental sensing, learns to navigate a maze choosing the optimum route. Several reinforcement learning based algorithms, such as the Q-learning, Q(/spl lambda/)-learning, fast online Q(/spl lambda/)-learning and DYNA structure, are considered. Experimental results based on simulation and an integrated real-time system are presented for varying density of obstacles in a 15/spl times/15 maze.",https://ieeexplore.ieee.org/document/932598/,Proceedings 2001 ICRA. IEEE International Conference on Robotics and Automation (Cat. No.01CH37164),21-26 May 2001,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICACEH51803.2020.9366217,Realization of Human and Fish Robot Interaction with Artificial Intelligence Using Hand Gesture,IEEE,Conferences,This study mimicked a real fish movement in the aquarium which was controlled by hand signals. The main idea to develop an aquarium robotic fish with hand gestures. Control actions include directions and stop and go of the fish. The inputs are given by human hands known as bio-mimetic ornamental. We implemented control algorithms to recognize hand gestures. The experimental results showed the effective control of robot fish with hand gestures.,https://ieeexplore.ieee.org/document/9366217/,"2020 IEEE 2nd International Conference on Architecture, Construction, Environment and Hydraulics (ICACEH)",25-27 Dec. 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/GTSD50082.2020.9303087,Receptionist and Security Robot Using Face Recognition with Standardized Data Collecting Method,IEEE,Conferences,"Face recognition has become the front runner for deep learning applications in the real world and this paper focuses on its implementation in a human-robot interaction and security system. For this specific project, it is inherent that restraints are created to allow the system to produce greater performance within the requirements of a receptionist and security robot. A k-nearest neighbors classifier is applied to further enhance the accuracy of face recognition. By sequencing images from videos, we create large datasets to train our own classifier in various conditions to increase its accuracy and lower false-positive rates in poor lighting environments. With the goal of creating a service robot, we have standardized our method of data collection for new inputs that will assist the recognition process in variable conditions of operation. The resulting product is a system that can accurately predict known and unknown faces with Asian features.",https://ieeexplore.ieee.org/document/9303087/,2020 5th International Conference on Green Technology and Sustainable Development (GTSD),27-28 Nov. 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IRDS.2002.1043897,Recognizing and remembering individuals: online and unsupervised face recognition for humanoid robot,IEEE,Conferences,"Individual recognition is a widely reported phenomenon in the animal world, where it contributes to successful maternal interaction, parental care, group breeding, cooperation, mate choice, etc. This work addresses the question of how one may implement such social competence in a humanoid robot. We argue that the robot must be able to recognize people and learn about their various characteristics through embodied social interaction. Thus, we proposed an initial implementation of an online and unsupervised face recognition system for Kismet, our sociable robotic platform. We show how specific features of this particular application drove our decision and implementation process, challenged by the difficulty of the face recognition problem, which has so far been explored in the supervised manner. Experimental results are reported to illustrate what was solved and the lessons learned from the current implementation.",https://ieeexplore.ieee.org/document/1043897/,IEEE/RSJ International Conference on Intelligent Robots and Systems,30 Sept.-4 Oct. 2002,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/BioRob49111.2020.9224392,Reinforcement Learning Assist-as-needed Control for Robot Assisted Gait Training,IEEE,Conferences,"The primary goal of an assist-as-needed (AAN) controller is to maximize subjects&#x2019; active participation during motor training tasks while allowing moderate tracking errors to encourage human learning of a target movement. Impedance control is typically employed by AAN controllers to create a compliant force-field around the desired motion trajectory. To accommodate different individuals with varying motor abilities, most of the existing AAN controllers require extensive manual tuning of the control parameters, resulting in a tedious and time-consuming process. In this paper, we propose a reinforcement learning AAN controller that can autonomously reshape the force-field in real-time based on subjects&#x2019; training performances. The use of action-dependent heuristic dynamic programming enables a model-free implementation of the proposed controller. To experimentally validate the controller, a group of healthy individuals participated in a gait training session wherein they were asked to learn a modified gait pattern with the help of a powered ankle-foot orthosis. Results indicated the potential of the proposed control strategy for robot-assisted gait training.",https://ieeexplore.ieee.org/document/9224392/,2020 8th IEEE RAS/EMBS International Conference for Biomedical Robotics and Biomechatronics (BioRob),29 Nov.-1 Dec. 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SII52469.2022.9708882,Reinforcement Learning based Hierarchical Control for Path Tracking of a Wheeled Bipedal Robot with Sim-to-Real Framework,IEEE,Conferences,"We propose a reinforcement learning (RL) based hierarchical control framework for path tracking of a wheeled bipedal robot. The framework consists of three control levels. 1) The high-level RL is used to obtain an optimal policy through trial and error in a simulated environment. 2) The middle-level Lyapunov-based non-linear controller is utilized to track a desired line with strong robustness and high stability. 3) The low-level PID-based controller is implemented to simultaneously achieve both balancing and velocity tracking for a physical wheeled bipedal robot in real world. Thanks to the middle-level controller, the offline trained policy in simulation can be directly employed on the physical robot in real time without tuning any parameters. Moreover, the high-level policy network is able to improve optimality and generality for the task of path tracking, as well to avoid the cumbersome process of manually tuning control gains. The experiment results in both simulation and real world demonstrate that the proposed hierarchical control framework can achieve quick, robust, and stable path tracking for a wheeled bipedal robot.",https://ieeexplore.ieee.org/document/9708882/,2022 IEEE/SICE International Symposium on System Integration (SII),9-12 Jan. 2022,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CIRA.2007.382878,Reinforcement Learning with a Supervisor for a Mobile Robot in a Real-world Environment,IEEE,Conferences,"This paper describes two experiments with supervised reinforcement learning (RL) on a real, mobile robot. Two types of experiments were preformed. One tests the robot's reliability in implementing a navigation task it has been taught by a supervisor. The other, in which new obstacles are placed along the previously learned path to the goal, measures the robot's robustness to changes in environment. Supervision consisted of human-guided, remote-controlled runs through a navigation task during the initial stages of reinforcement learning. The RL algorithms deployed enabled the robot to learn a path to a goal yet retain the ability to explore different solutions when confronted with a new obstacle. Experimental analysis was based on measurements of average time to reach the goal, the number of failed states encountered during an episode, and how closely the RL learner matched the supervisor's actions.",https://ieeexplore.ieee.org/document/4269878/,2007 International Symposium on Computational Intelligence in Robotics and Automation,20-23 June 2007,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICSMC.2012.6377767,Reinforcement learning-based tracking control for wheeled mobile robot,IEEE,Conferences,This paper proposes a new method to design a reinforcement learning-based integrated kinematic and dynamic tracking control scheme for a nonholonomic wheeled mobile robot. The scheme uses just only one neural network to design an online adaptive synchronous policy iteration algorithm implemented as an actor critic structure. Our tuning law for the single neural network not only learns online a tracking-HJB equation to approximate both the optimal cost and the optimal adaptive control law but also guarantees closed-loop stability in real-time. The convergence and stability of the overall system are proven by Lyapunov theory. The simulation results for wheeled mobile robot verify the effectiveness of the proposed controller.,https://ieeexplore.ieee.org/document/6377767/,"2012 IEEE International Conference on Systems, Man, and Cybernetics (SMC)",14-17 Oct. 2012,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CYBER.2012.6392582,Reinforecement learning-based optimal tracking control for wheeled mobile robot,IEEE,Conferences,This paper proposes a new method to design a reinforcement learning-based integrated kinematic and dynamic tracking control scheme for a nonholonomic wheeled mobile robot. The scheme uses just only one neural network to design an online adaptive synchronous policy iteration algorithm implemented as an actor critic structure. Our tuning law for the single neural network not only learns online a tracking-HJB equation to approximate both the optimal cost and the optimal control law but also guarantees closed-loop stability in real-time. The convergence and stability of the overall system are proven by Lyapunov theory. The simulation results for wheeled mobile robot verify the effectiveness of the proposed controller.,https://ieeexplore.ieee.org/document/6392582/,"2012 IEEE International Conference on Cyber Technology in Automation, Control, and Intelligent Systems (CYBER)",27-31 May 2012,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/WCICA.2000.863464,Related value set algorithm for robot to distinguish image,IEEE,Conferences,"The image recognition from image database is different from robot vision in real world because the robot concerned is movable. The size of the image that captured by the robot varies with the distance between robot's camera and the real picture, or object. We analyzed the process of how human remembers and identifies an image. We derived eight properties of the key feature in an image. We then developed a related value set algorithm. With the algorithm, we defined a set to represent the key features of an image. The elements of the set are represented with the related value. The key features retrieved from an image with the algorithm satisfied the eight properties, thus the algorithm can be used for robot to distinguish images or object. The key features can also be used in image knowledge representation.",https://ieeexplore.ieee.org/document/863464/,Proceedings of the 3rd World Congress on Intelligent Control and Automation (Cat. No.00EX393),26 June-2 July 2000,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/RCAR47638.2019.9044114,Research on omnidirectional mobile robot motion control based on integration of traction and steering wheel,IEEE,Conferences,"In order to solve the automatic transportation of heavy materials under the limited working space of production workshops and warehouses, two sets of heavy-duty omnidirectional mobile robot motion control systems with steering wheel drive units were designed. The steering wheel combination drive unit of the “walking + steering” set is used to build the mobile robot chassis, and the mechatronics servo system and mathematical model of multi-motor coordinated motion are constructed. The communication between the controller and the steering wheel combination drive unit is established through the CAN bus. The specific implementation is to capture and analyze the control signal through the controller to obtain the desired motion mode, to obtain the motion of each set of steering wheel unit through the mathematical model, and to realize the desired motion through the synthesis of each set of steering wheel unit motion. It has been verified by experiments that the two sets of steering wheel unit-driven mobile robot control system realizes the zero turning radius, 360-degree omnidirectional movement of the robot and rotation during the movement. It can be used for flexible work in tight spaces.",https://ieeexplore.ieee.org/document/9044114/,2019 IEEE International Conference on Real-time Computing and Robotics (RCAR),4-9 Aug. 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/WCICA.2004.1343675,Research on remote controlled robot motion control system based on agent theory,IEEE,Conferences,"This paper introduces remote controlled robot motion control system based on agent theory. Task planning and reactive behavior control are discussed and implemented. This paper describes design of manager agent and motion control agent in detail. Agent theory are implemented in the robot control system to realize distributed intelligence based on M/A/R(Man/Agent/Robot) architecture. Thereby autonomy, reliability and real-time operation are improved.",https://ieeexplore.ieee.org/document/1343675/,Fifth World Congress on Intelligent Control and Automation (IEEE Cat. No.04EX788),15-19 June 2004,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA.2019.8794127,Residual Reinforcement Learning for Robot Control,IEEE,Conferences,"Conventional feedback control methods can solve various types of robot control problems very efficiently by capturing the structure with explicit models, such as rigid body equations of motion. However, many control problems in modern manufacturing deal with contacts and friction, which are difficult to capture with first-order physical modeling. Hence, applying control design methodologies to these kinds of problems often results in brittle and inaccurate controllers, which have to be manually tuned for deployment. Reinforcement learning (RL) methods have been demonstrated to be capable of learning continuous robot controllers from interactions with the environment, even for problems that include friction and contacts. In this paper, we study how we can solve difficult control problems in the real world by decomposing them into a part that is solved efficiently by conventional feedback control methods, and the residual which is solved with RL. The final control policy is a superposition of both control signals. We demonstrate our approach by training an agent to successfully perform a real-world block assembly task involving contacts and unstable objects.",https://ieeexplore.ieee.org/document/8794127/,2019 International Conference on Robotics and Automation (ICRA),20-24 May 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SAIS53221.2021.9483964,Robot First Aid: Autonomous Vehicles Could Help in Emergencies,IEEE,Conferences,"Safety is of critical importance in designing autonomous vehicles (AVs) that will be able to perform effectively in complex, mixed-traffic, real-world urban environments. Some prior research has looked at how to proactively avoid accidents with safe distancing and driver monitoring, but currently little research has explored strategies to recover afterwards from emergencies, from crime to natural disasters. The current short paper reports on our ongoing work using a speculative prototyping approach to explore this expansive design space, in the context of how a robot inside an AV could be deployed to support first aid. As a result, we present some proposals for how to detect emergencies, and examine and help victims, as well as lessons learned in prototyping. Thereby, our aim is to stimulate discussion and ideation that-by considering the prevalence of Murphy's law in our complex world, and the various technical, ethical, and practical concerns raised-could potentially lead to useful safety innovations.",https://ieeexplore.ieee.org/document/9483964/,2021 Swedish Artificial Intelligence Society Workshop (SAIS),14-15 June 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICNSC48988.2020.9238090,Robot Navigation with Map-Based Deep Reinforcement Learning,IEEE,Conferences,"This paper proposes an end-to-end deep reinforcement learning approach for mobile robot navigation with dynamic obstacles avoidance. Using experience collected in a simulation environment, a convolutional neural network (CNN) is trained to predict proper steering actions of a robot from its egocentric local occupancy maps, which accommodate various sensors and fusion algorithms. The trained neural network is then transferred and executed on a real-world mobile robot to guide its local path planning. The new approach is evaluated both qualitatively and quantitatively in simulation and realworld robot experiments. The results show that the map-based end-to-end navigation model is easy to be deployed to a robotic platform, robust to sensor noise and outperforms other existing DRL-based models in many indicators.",https://ieeexplore.ieee.org/document/9238090/,"2020 IEEE International Conference on Networking, Sensing and Control (ICNSC)",30 Oct.-2 Nov. 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICBASE53849.2021.00131,"Robot Swarm Navigation: Methods, Analysis, and Applications",IEEE,Conferences,"Swarm navigation is one of the possible collective behaviours a swarm robotic system can possess. Research in multiple robot navigation has gained more attention given their potential real-world applications, such as search and rescue, transportation, precision farming, and environmental monitoring. In this paper, we analyze the recent advances in the field of swarm navigation, focusing mainly on design and analysis methods that contribute to collective exploration, coordinated motion, and collective transport. Moreover, various experimental and real-world applications of swarm navigation, instead of works that involve simulations, are described. Several challenges that restrict the implementation of successful laboratory works of swarm systems to industrial applications are also identified and described. To tackle these challenges, some interesting future research directions are proposed and discussed.",https://ieeexplore.ieee.org/document/9696121/,2021 2nd International Conference on Big Data & Artificial Intelligence & Software Engineering (ICBASE),24-26 Sept. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/MECHATRONIKA.2014.7018286,Robot imitation of human arm via Artificial Neural Network,IEEE,Conferences,"In this study, a robot arm that can imitate human arm is designed and presented. The potentiometers are located to the joints of the human arm in order to detect movements of human gestures, and data were collected by this way. The collected data named as “movement of human arm” are classified by the help of Artificial Neural Network (ANN). The robot performs its movements according to the classified movements of the human. Real robot and real data are used in this study. Obtained results show that the learning application of imitating human action via the robot was successfully implemented. With this application, the platforms of robot arm in an industrial environment can be controlled more easily; on the other hand, robotic automation systems which have the capability of making a standard movements of a human can become more resistant to the errors.",https://ieeexplore.ieee.org/document/7018286/,Proceedings of the 16th International Conference on Mechatronics - Mechatronika 2014,3-5 Dec. 2014,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ASAMA.1999.805407,Robot media communication: an interactive real-world guide agent,IEEE,Conferences,"Describes a guide system and the software architecture for an autonomous, interactive robot based on a multi-agent system. A robot navigation system has been developed allowing the robot to guide people through halls in various types of exhibitions. Our approach uses an infrared location system in the hallway ceilings, making the environment part of a sensor-distributed robot system. The real-world guide agent is composed of a guide agent on a hand-held mobile computer and a robot agent on an autonomous mobile robot. The guide agent plays the role of ""robot media"" in order to integrate information in the information space of the mobile computer and the physical space of the exhibits in order to guide visitors through the physical space. This research aims to develop a cooperative adaptive system using two-way communication among spaces, media and human beings to construct transparent knowledge boundaries between the real space and the virtual space. The virtual space is generated from computer data using shared space technology and it creates a distributed intelligence in order to manage the communication and control the guide in a laboratory. We have experimented with and verified this software architecture using a prototype autonomous mobile robot equipped with a compass.",https://ieeexplore.ieee.org/document/805407/,"Proceedings. First and Third International Symposium on Agent Systems Applications, and Mobile Agents",6-6 Oct. 1999,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICSMC.2008.4811760,Robot navigation using KFLANN place field,IEEE,Conferences,"This paper presents an implementation of place cells for a robot navigation using the K-iterations fast learning artificial neural networks (KFLANN) clustering algorithm. The KFLANN possesses several desirable properties suitable for place cell robot navigation tasks. The technique proposed is able to autonomously adjust the resolution of cells according to the complexity of the environment. This is achieved through two parameters known as the tolerance and the vigilance of the network. In addition, a navigation system consisting of a topological map building and a place cell path planning strategy is presented. A physical implementation of the system was developed on an autonomous platform and actual results were obtained. The experimental results obtained indicate that the system was able to navigate successfully through the experimental space and also tolerate unexpected discrepancies arising from motor and sensor errors present in a real environment. Furthermore, despite abrupt changes in an environment due to the deliberate introduction of obstacles, the system was still able to cope without changes to the program. The experiment was also extended to include a kidnapped robot scenario and the results were favorable, indicating a positive use of allothetic cue recognition capabilities.",https://ieeexplore.ieee.org/document/4811760/,"2008 IEEE International Conference on Systems, Man and Cybernetics",12-15 Oct. 2008,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SMC.2016.7844958,Robot position control in pipes using Q Learning,IEEE,Conferences,"In the most critical hydro crisis in Brazil, 37 percent of the whole amount of treated water is wasted before reaching consumers. A robot with a position control to travel inside a pipe is an important step in the pursuit of an autonomous solution to detect and correct pipes failures. This paper shows a Q Learning controller algorithm implemented using a microcontroller in a mechanical body of a commercial pipe inspection robot. Using only the measurements of a gyroscope, and controlling the wheels' motors on the left and right sides, the controller learned the best set of movements to ride inside a 300mm sewer pipe, in the tested conditions. Real tests in a 300mm pipe were performed using the developed algorithm and it was compared to a random movement and to a straight forward movement.",https://ieeexplore.ieee.org/document/7844958/,"2016 IEEE International Conference on Systems, Man, and Cybernetics (SMC)",9-12 Oct. 2016,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1145/1957656.1957814,Robot self-initiative and personalization by learning through repeated interactions,IEEE,Conferences,"We have developed a robotic system that interacts with the user, and through repeated interactions, adapts to the user so that the system becomes semi-autonomous and acts proactively. In this work we show how to design a system to meet a user's preferences, show how robot pro-activity can be learned and provide an integrated system using verbal instructions. All these behaviors are implemented in a real platform that achieves all these behaviors and is evaluated in terms of user acceptability and efficiency of interaction.",https://ieeexplore.ieee.org/document/6281377/,2011 6th ACM/IEEE International Conference on Human-Robot Interaction (HRI),8-11 March 2011,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/RO-MAN47096.2020.9223566,Robust Real-Time Hand Gestural Recognition for Non-Verbal Communication with Tabletop Robot Haru,IEEE,Conferences,"In this paper, we present our work in close-distance non-verbal communication with tabletop robot Haru through hand gestural interaction. We implemented a novel hand gestural understanding system by training a machine-learning architecture for real-time hand gesture recognition with the Leap Motion. The proposed system is activated based on the velocity of a user's palm and index finger movement, and subsequently labels the detected movement segments under an early classification scheme. Our system is able to combine multiple gesture labels for recognition of consecutive gestures without clear movement boundaries. System evaluation is conducted on data simulating real human-robot interaction conditions, taking into account relevant performance variables such as movement style, timing and posture. Our results show robustness in hand gesture classification performance under variant conditions. We furthermore examine system behavior under sequential data input, paving the way towards seamless and natural real-time close-distance hand-gestural communication in the future.",https://ieeexplore.ieee.org/document/9223566/,2020 29th IEEE International Conference on Robot and Human Interactive Communication (RO-MAN),31 Aug.-4 Sept. 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CCDC.2019.8832613,Robust Zhang Neural Network for Tracking Control of Parallel Robot Manipulators With Unknown Parameters,IEEE,Conferences,"Under the situation of parameter uncertainty, the tracking control of parallel robot manipulators is a challenging problem in robotic research. Unlike conventional Zhang neural network (ZNN) relying on the assumption that the robot parameter information is fully and accurately known, this paper proposes a robust Zhang neural network (RZNN) for tracking control problems solving of parallel robot manipulators in the absence of parameter information. The proposed RZNN features the full utilization of effector feedback information, and shows a robust tracking performance even with unknown robot parameter information. Then, the continuous-time model of the RZNN is discretized via Euler forward formula (EFF) for numerical implementation. Finally, comprehensive simulative experiments including robustness test verify the effectiveness of the RZNN model for the real-time tracking control of parallel robot manipulators with unknown parameters.",https://ieeexplore.ieee.org/document/8832613/,2019 Chinese Control And Decision Conference (CCDC),3-5 June 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/HUMANOIDS.2014.7041487,Robust fall detection with an assistive humanoid robot,IEEE,Conferences,"Summary form only given. In this video we introduce a robot assistant that monitors a person in a household environment to promptly detect fall events. In contrast to the use of a fixed sensor, the humanoid robot will track and keep the moving person in the scene while performing daily activities. For this purpose, we extended the humanoid Nao1 with a depth sensor2 attached to its head. The tracking framework implemented with OpenNI3 segments and tracks the person's position and body posture. We use a learning neural framework for processing the extracted body features and detecting abnormal behaviors, e.g. a fall event [1]. The neural architecture consists of a hierarchy of self-organizing neural networks for attenuating noise caused by tracking errors and detecting fall events from video stream in real time. The tracking application, the neural framework, and the humanoid actuators communicate over Robot Operating System (ROS)4. We use communication over the ROS network implemented with publisher-subscriber nodes. When a fall event is detected, Nao will approach the person and ask whether assistance is needed. In any case, Nao will take a picture of the scene that can be sent to the caregiver or a relative for further human evaluation and agile intervention. The combination of this sensor technology with our neural network approach allows to tailor the robust detection of falls independently from the background surroundings and in the presence of noise (tracking errors and occlusions) introduced by a real-world scenario. The video shows experiments run in a home-like environment.",https://ieeexplore.ieee.org/document/7041487/,2014 IEEE-RAS International Conference on Humanoid Robots,18-20 Nov. 2014,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICIAS.2012.6306173,SCARA robot control using neural networks,IEEE,Conferences,"A SCARA industrial robot model is identified based on a 4-axis structure using Lagrangian mechanics, also the dynamic model for the electromechanical actuator and motion transmission systems is identified. A conventional PD controller is implemented and compared to neural networks control system to achieve precise position control of SCARA manipulator. The performance of the modeled system is simulated using several desired tracking motion for each joint. Neural networks control method has shown a remarkable improvement of tracking capabilities for the SCARA robot over conventional PD controller. The proposed neural network controller has the potential to accurately control real-time manipulator applications.",https://ieeexplore.ieee.org/document/6306173/,2012 4th International Conference on Intelligent and Advanced Systems (ICIAS2012),12-14 June 2012,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA48506.2021.9561020,SQRP: Sensing Quality-aware Robot Programming System for Non-expert Programmers,IEEE,Conferences,"Robot programming typically makes use of a set of mechanical skills that is acquired by machine learning. Because there is in general no guarantee that machine learning produces robot programs that are free of surprising behavior, the safe execution of a robot program must utilize monitoring modules that take sensor data as inputs in real time to ensure the correctness of the skill execution. Owing to the fact that sensors and monitoring algorithms are usually subject to physical restrictions and that effective robot programming is sensitive to the selection of skill parameters, these considerations may lead to different sensor input qualities such as the view coverage of a vision system that determines whether a skill can be successfully deployed in performing a task. Choosing improper skill parameters may cause the monitoring modules to delay or miss the detection of important events such as a mechanical failure. These failures may reduce the throughput in robotic manufacturing and could even cause a destructive system crash. To address above issues, we propose a sensing quality-aware robot programming system that automatically computes the sensing qualities as a function of the robot&#x2019;s environment and uses the information to guide non-expert users to select proper skill parameters in the programming phase. We demonstrate our system framework on a 6DOF robot arm for an object pick-up task.",https://ieeexplore.ieee.org/document/9561020/,2021 IEEE International Conference on Robotics and Automation (ICRA),30 May-5 June 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICSMC.2001.969840,STNS-R: a learning method for seamless transplantation from a virtual agent to a physical robot,IEEE,Conferences,"In this paper, we are concerned with the problem of how a physical robot can get an appropriate internal representation to its task and environment. Learning from experience is effective for the problem, but it is very time-consuming to learn a representation from the beginning in a real environment. On the other hand, the representation learned only in a simulated environment has the risk of not serving the purpose in a real environment because of the uncertainty in sensors, actuators, and the environment. In, order to have the best of both worlds, it is effective to transplant the learned state representation of a virtual agent to a physical robot. For this purpose., we improved our developed incremental learning architecture for use in the real environment and developed a new architecture, called STNS-R. In this architecture, inappropriate negative instances caused by uncertainties are found on the basis of the distribution of instances and removed in order to correct the distorted shapes of the states. The effectiveness of STNS-R is shown in the experimental results.",https://ieeexplore.ieee.org/document/969840/,"2001 IEEE International Conference on Systems, Man and Cybernetics. e-Systems and e-Man for Cybernetics in Cyberspace (Cat.No.01CH37236)",7-10 Oct. 2001,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA.2019.8794176,Safe Human Robot Cooperation in Task Performed on the Shared Load,IEEE,Conferences,"Human-robot collaboration in industrial settings calls for implementing safety measures to ensure there is no risk to humans working in such an environment. In human-robot physical collaboration, an object or a load is handled by both human and the robot. Developing a safety framework for the robot is a requirement for preventing collisions during performing a task. In this paper, force myography (FMG) data are used to develop a control scheme for the robot such that it can work with the human worker while avoiding collisions. Force myography quantifies the activities of human muscles when applying forces to handle an object. A neural network-based approach is then used to select the most informative features of the FMG signal. The developed control scheme incorporates the FMG data and the robot dynamics to obtain a prediction about the next step of the cooperation task and to plan the robot motion accordingly. The proposed approach is evaluated experimentally in real time in a moving objects task which requires appropriate complementary actions from the robot and the human user. The results of this study show that the proposed scheme can successfully plan the robot motion based on the actions of the human user.",https://ieeexplore.ieee.org/document/8794176/,2019 International Conference on Robotics and Automation (ICRA),20-24 May 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CIMCA.2005.1631415,Self-Organization of Spiking Neural Network Generating Autonomous Behavior in a Real Mobile Robot,IEEE,Conferences,"In this paper, we study the relation between neural dynamics and robot behavior to develop self-organization algorithm of spiking neural network applicable to autonomous robot. We first formulated a spiking neural network model whose inputs and outputs were analog. We then implemented it into a miniature mobile robot Khepera. In order to see whether or not a solution(s) for the given task exists with the spiking neural network, the robot was evolved with the genetic algorithm (GA) in an environment. The robot acquired the obstacle avoidance and navigation task successfully, exhibiting the presence of the solution. Then, a self-organization algorithm based on the use-dependent synaptic potentiation and depotentiation was formulated and implemented into the robot. In the environment, the robot gradually organized the network and the obstacle avoidance behavior was formed. The time needed for the training was much less than with genetic evolution, approximately one fifth (1/5)",https://ieeexplore.ieee.org/document/1631415/,"International Conference on Computational Intelligence for Modelling, Control and Automation and International Conference on Intelligent Agents, Web Technologies and Internet Commerce (CIMCA-IAWTIC'06)",28-30 Nov. 2005,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISSCAA.2008.4776238,Self-motion planning of redundant robot manipulators based on quadratic program and shown via PA10 example,IEEE,Conferences,"In this paper, a criterion is proposed in the form of a quadratic function for the purpose of self-motion planning of redundant robot arms. The proposed self-motion scheme with joint physical limits considered could be formulated as a quadratic programming (QP) problem subject to equality, (inequality) and bound constraints. A primal-dual neural network based on linear variational inequalities (LVI) is developed as the real-time solver for the resultant quadratic-program. The so-called LVI-based primal-dual neural network has a simple piecewise-linear dynamics and a global exponential convergence to optimal solutions of QP problems. Computer-simulations performed based on PA10 robot arm substantiate the efficacy of the proposed QP-based neural self-motion-planning scheme.",https://ieeexplore.ieee.org/document/4776238/,2008 2nd International Symposium on Systems and Control in Aerospace and Astronautics,10-12 Dec. 2008,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICSMC.1997.625763,Self-organized learning and its implementation of robot movements,IEEE,Conferences,The self-organizing map algorithm using an artificial neural network originally developed by Kohonen and extended and modified later provides a distributed and autonomous learning procedure in engineering modeling of the human sensory-motor mapping mechanism. Its extension and adaptation to a control problem of a robot manipulator has been intensively discussed in past years. In this article the application of the self-organizing map algorithm to the generation of a visuo-motor map is focused on. A task-oriented inverse kinematic solution to a redundant manipulator is formed and real-time implementation of the map on a mechanical manipulator is performed.,https://ieeexplore.ieee.org/document/625763/,"1997 IEEE International Conference on Systems, Man, and Cybernetics. Computational Cybernetics and Simulation",12-15 Oct. 1997,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ROBOT.2006.1642213,Self-organizing approach for robot's behavior imitation,IEEE,Conferences,"In this paper, an approach for behavior imitation using visual information was introduced. The imitation process is done by a self organizing neural network module. From several demonstrations of task operation, a vision system captures movement of the demonstrator mobile robot and associated objects in an operation field. Then, the movement features are extracted to present to an imitation engine. Finally, skill or decision policy from teacher's demonstration is extracted and embedded into a self organizing neural network without explicit external supervisory signals. A simple action selection algorithm for choosing action from learned network is proposed. The algorithm was implemented and tested on a simulated robot and a real mobile robot to imitate two simple robot soccer behaviors: approaching the target and obstacle avoidance. Furthermore, the concept of similarity measure is introduced to evaluate imitation performance from the demonstrator",https://ieeexplore.ieee.org/document/1642213/,"Proceedings 2006 IEEE International Conference on Robotics and Automation, 2006. ICRA 2006.",15-19 May 2006,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ROBOT.1996.506506,Semantic learning by an autonomous mobile robot,IEEE,Conferences,Describes the design and implementation of a learning system for control of an autonomous mobile robot. The robot learns reactive behaviors that allow it to retreat from potential collisions and to explore its environment by seeking out nearby objects. No external teaching input is required. Results from experiments with a real robot are presented. The learned reactive behaviors become the basis for the acquisition of more complex behaviors. Sensory/motor states are classified and then associated with lexical items to form a simple command language which is then used to direct the robot.,https://ieeexplore.ieee.org/document/506506/,Proceedings of IEEE International Conference on Robotics and Automation,22-28 April 1996,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS.2017.8206048,Sensor fusion for robot control through deep reinforcement learning,IEEE,Conferences,"Deep reinforcement learning is becoming increasingly popular for robot control algorithms, with the aim for a robot to self-learn useful feature representations from unstructured sensory input leading to the optimal actuation policy. In addition to sensors mounted on the robot, sensors might also be deployed in the environment, although these might need to be accessed via an unreliable wireless connection. In this paper, we demonstrate deep neural network architectures that are able to fuse information generated by multiple sensors and are robust to sensor failures at runtime. We evaluate our method on a search and pick task for a robot both in simulation and the real world.",https://ieeexplore.ieee.org/document/8206048/,2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),24-28 Sept. 2017,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICAR.2015.7251437,Simultaneous human-robot adaptation for effective skill transfer,IEEE,Conferences,"In this paper, we propose and implement a human-in-the loop robot skill synthesis framework that involves simultaneous adaptation of the human and the robot. In this framework, the human demonstrator learns to control the robot in real-time to make it perform a given task. At the same time, the robot learns from the human guided control creating a non-trivial coupled dynamical system. The research question we address is how this system can be tuned to facilitate faster skill transfer or improve the performance level of the transferred skill. In the current paper we report our initial work for the latter. At the beginning of the skill transfer session, the human demonstrator controls the robot exclusively as in teleoperation. As the task performance improves the robot takes increasingly more share in control, eventually reaching full autonomy. The proposed framework is implemented and shown to work on a physical cart-pole setup. To assess whether simultaneous learning has advantage over the standard sequential learning (where the robot learns from the human observation but does not interfere with the control) experiments with two groups of subjects were performed. The results indicate that the final autonomous controller obtained via simultaneous learning has a higher performance measured as the average deviation from the upright posture of the pole.",https://ieeexplore.ieee.org/document/7251437/,2015 International Conference on Advanced Robotics (ICAR),27-31 July 2015,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICICIP53388.2021.9642219,Sliding Mode Control Algorithm of Upper Limb Exoskeleton Rehabilitation Robot Based on RBF Neural Network,IEEE,Conferences,"Aiming at the nonlinear and uncertain problems of upper limb exoskeleton rehabilitation robot (ULERR) during passive training, a sliding model controller based on radial basis neural network is designed in this paper. Firstly, a four-degree-of-freedom ULERR is designed for stroke patients in soft paralysis and spasticity, and a kinetic model was established. Secondly, RBF neural network is used to approximate the uncertainty caused by spastic disturbance of patients in the system. The weight in the neural network is replaced by a single parameter, and the adaptive algorithm is easy to adjust and has strong real-time performance. The asymptotic stability of the controller is verified by Lyapunov theorem. Finally, the desired training trajectory of the upper limb is obtained by a three-dimensional motion capture system, and the simulation experiments are carried out with Matlab software to prove that the proposed control method solves the chattering problem of traditional sliding mode control, to meet the control requirements of real-time rehabilitation training.",https://ieeexplore.ieee.org/document/9642219/,2021 11th International Conference on Intelligent Control and Information Processing (ICICIP),3-7 Dec. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SaCoNeT.2018.8585616,Smart Navigation of Mobile Robot Using Neural Network Controller,IEEE,Conferences,"The field of autonomous navigation of mobile robot is advancing so fast especially with the development of machine learning algorithms. This study aims to introduce a neural network controller that controls the trajectory and the obstacle avoidance of a non-holonomic mobile robot.We train the robot in environment containing multiple obstacles with different places. This paper includes both a kinematic and a dynamic study of a mobile robot. Different training schemes have been studied that tackle the learning objectives differently. The trained controller is producing the Pulse Width Modulation (PWM) signals that could be implemented in a microprocessor and validated by simulations. Unlike some other recent approaches, this work was validated by a 3D simulation which is similar to the real model.",https://ieeexplore.ieee.org/document/8585616/,2018 International Conference on Smart Communications in Network Technologies (SaCoNeT),27-31 Oct. 2018,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ROBOT.2004.1308781,Software approach for the autonomous inspection robot MAKRO,IEEE,Conferences,"The sewer inspection robot MAKRO is an autonomous multi-segment robot with worm-like shape driven by wheels. It is currently under development in the project MAKRO-PLUS. The robot has to navigate autonomously within sewer systems. Its first tasks is to take water probes, analyze them onboard, and measure positions of manholes and pipes to detect pollution loaded sewage and to improve current maps of sewer systems. One of the challenging problems is the control software, which should enable the robot to navigate in the sewer system and perform the inspection tasks autonomously, while always taking care of its own safety. Tests in our test environment and in a real sewer system show promising results. This paper focuses on the software approach. To manage the complexity a layered architecture has been chosen, each layer defining a different level of abstraction. After determining the abstraction levels, we use different methods for implementation. For the highest abstraction level a standard AI-planning algorithm is used. For the next level, finite state automata has been chosen. For ""simple"" task implementation we use a modular C++ based method (MCA2), which is also used on the lowest software level.",https://ieeexplore.ieee.org/document/1308781/,"IEEE International Conference on Robotics and Automation, 2004. Proceedings. ICRA '04. 2004",26 April-1 May 2004,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCE46568.2020.9042995,Stroke Signs Detection System by SNS Agency Robot,IEEE,Conferences,"This paper proposes a system which implements the Cincinnati Prehospital Stroke Scale (CPSS), the widely used screening method for the initial symptoms of a stroke, in a communication robot. AI on cloud analyses an acquired video through a conversation with the robot in real time and automatically determines the abnormalities. The judgement result is informed to his/her families by SNS. This study implemented two of the three CPSS scales such as “Arms” and “Speech”, we confirmed that the system enables to acquire, analyze and notify the information in real time.",https://ieeexplore.ieee.org/document/9042995/,2020 IEEE International Conference on Consumer Electronics (ICCE),4-6 Jan. 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISIC.1991.187403,Symbolic feature-based representation and planning for an agent based robot controller,IEEE,Conferences,"A prototype system has been developed to input 3-D computer-aided-design (CAD) data to automatically generate and implement robot trajectories for such application as waterjet cutting, surface finishing and polishing. The system can drive various robot configurations and handle contingencies such as collisions and singularities. The system provides a framework for integration of high-level reasoning, real-time path and trajectory planning, and various levels of feedback based on contingency detection algorithms or sensor data. Although initial CAD data have been in the form of IGES geometric entities, a higher-level CAD description based on manufacturing features which incorporates both geometric and process information is being developed. This feature-based CAD representation provides a direct interface between the CAD design data and the planning system for robot control. An agent actor paradigm is proposed as the representation for software/hardware system specifications and associated software and hardware modules.<>",https://ieeexplore.ieee.org/document/187403/,Proceedings of the 1991 IEEE International Symposium on Intelligent Control,13-15 Aug. 1991,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICSICT.2018.8565654,System Simulation for Robot Control Based on AI Approach,IEEE,Conferences,"In this paper, we focus on real robot development and control for different surface conditions using an AI based approach. The core components of our existing robot are pressure sensor, servomotor and software-driven microcontroller. We performed robot system simulation for various ground surface conditions to control the robot with respect to pressure-sensing data that incorporates the two-way interactions between robot and ground. We have used an artificial neural network (ANN) approach for pressure-sensor-data analysis. The analysis result shows that above 25 hidden neurons and an increasing number of training cycles will deliver better performance in terms of mean square error (mse), learning time and improved nearest surface-pattern recognition. The analysis results are useful for next generation AI-chip development for real-time robot control and movement.",https://ieeexplore.ieee.org/document/8565654/,2018 14th IEEE International Conference on Solid-State and Integrated Circuit Technology (ICSICT),31 Oct.-3 Nov. 2018,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICMA52036.2021.9512666,Target Detection and Tracking of Ground Mobile Robot Based on Improved Single Shot Multibox Detector Network,IEEE,Conferences,"To solve the problems of slow labeling speed of the traditional labellmg data set establishment method, and slow running speed of target classification and detection algorithm based on Single Shot Multibox Detector(SSD) deep learning network, this paper proposes a fast data set labeling algorithm and a fast SSD network for target real-time detection and tracking research. First, a data set is established quickly by using TLD target detection and tracking algorithms, cropping and mirroring methods are used to strengthen the data set. Then, SSD backbone network is improved based on depth-wise separable convolution to establish a fast SSD network. Finally, the ground mobile robot in RoboMasters(RM) competition is used as the detection and tracking target indoors and outdoors, as well as with other different scenarios with shield to test the real-time performance, accuracy and effectiveness of the algorithm. The results show that compared with traditional SSD network research, in terms of the analysis and processing system deployed on low-performance hardware, the improved fast SSD network can better meet the real-time requirements of target detection and tracking.",https://ieeexplore.ieee.org/document/9512666/,2021 IEEE International Conference on Mechatronics and Automation (ICMA),8-11 Aug. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS.1999.812762,Task-model based human robot cooperation using vision,IEEE,Conferences,"In order to assist a human, the robot must recognize human motion in real time by vision, and must plan and execute the needed assistance motion based on the task purpose and the context. In this research, we tried to solve such problems. We defined the abstract task model, analyzed the human demonstration by using events and an event stack, and automatically generated the task models needed in the assistance by the robot. The robot planned and executed the appropriate assistance motions based on the task: models according to the human motions in the cooperation with the human. We implemented a 3D object recognition system and a human grasp recognition system by using trinocular stereo color cameras and a real time range finder. The effectiveness of these methods was tested through an experiment in which the human and the robotic hand assembled toy parts in cooperation.",https://ieeexplore.ieee.org/document/812762/,Proceedings 1999 IEEE/RSJ International Conference on Intelligent Robots and Systems. Human and Environment Friendly Robots with High Intelligence and Emotional Quotients (Cat. No.99CH36289),17-21 Oct. 1999,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/MICAI46078.2018.00022,Teleoperation and Control of a Humanoid Robot NAO through Body Gestures,IEEE,Conferences,"The present project focuses on controlling the humanoid robot, NAO, in real time through the gestural interpretation of the Kinect sensor and implementing linear regression techniques to adjust the movements. The system was developed under the Python programming language and the Naoqi operating system. For the teleoperation part, seven pre-established operation commands were integrated by means of corporal gestures. The system was validated by a user performing random movements that control the NAO and verifying how similar those movements were with respect to those executed by the robot. For the teleoperation part, the user navigated the NAO through a structured environment by viewing streaming video obtained directly from the camera integrated into the robot. As a result, it was observed that the robot imitated movements of the user's full body. The proposed method offers the advantage of requiring a smaller number of arithmetic operations and a smaller amount of processing compared to related works in the literature.",https://ieeexplore.ieee.org/document/9046510/,2018 Seventeenth Mexican International Conference on Artificial Intelligence (MICAI),27 Oct.-2 Nov. 2018,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICAE47758.2019.9221734,The Real-Time Object Detection System on Mobile Soccer Robot using YOLO v3,IEEE,Conferences,"In a Soccer robot, each robot must be able to detect the object such as ball, goal and circle line in the game field through the vision system using a camera. The object detection in the past decade was used color filtering method, then it's methods was develop into neural network based. Neural network detection has also a lot of progress, start from R-CNN, fast R-CNN, and new method is YOLOv3. In this paper we will explain the implementation of YOLO V3 to detect object at Barelang mobile soccer robot. This system is run on SHUTTLE Xl MINI PC has Octa Core Intel Core i7-7700HQ processor, 16 GB of RAM and with 3GB of NVIDIA GeForce GTX 1060 graphics card has cuda core score 1152. The experiments result show the object detection get 28.3 fps. Datasets performance of the proposed method is IOU 71.76%, recall 0.92, precision 0.92 and mAP 87.07%. YOLO v3 capable to detect and distinguish objects in the different lighting condition, with max distance 3 m for ball object and 8 m for goal object.",https://ieeexplore.ieee.org/document/9221734/,2019 2nd International Conference on Applied Engineering (ICAE),2-3 Oct. 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/BEC.2010.5631008,Timed Automata based provably correct robot control,IEEE,Conferences,"This paper presents a feasibility study on the usage of Uppaal Timed Automata (UPTA) for deliberative level robotic control. The study is based on the Scrub Nurse Robot case-study. Our experience confirms that UPTA model based control enables the control loop to be defined and maintained during the robot operation autonomously with minimum human intervention. Specifically, in our robot architecture the control model is constructed automatically using unsupervised learning. Correctness of the model is verified on-the-fly against safety, reachability, and performance requirements. Finally, it is demonstrated that UPTA model based robot control, action planning and model updates have natural implementation based on existing model execution and conformance testing tool Uppaal Tron.",https://ieeexplore.ieee.org/document/5631008/,2010 12th Biennial Baltic Electronics Conference,4-6 Oct. 2010,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/RO-MAN50785.2021.9515348,Towards Out-of-Sight Predictive Tracking for Long-Term Indoor Navigation of Non-Holonomic Person Following Robot,IEEE,Conferences,"The ability to predict the movements of the target person allows a person following robot (PFR) to coexist with the person while still complying with the social norms. In human-robot collaboration, this is an essential requisite for long-term time-dependent navigation and not losing sight of the person during momentary occlusions that may arise from a crowd due to static or dynamic obstacles, other human beings, or intersections in the local surrounding. The PFR must not only traverse to the previously unknown goal position but also relocate the target person after the miss, and resume following. In this paper, we try to solve this as a coupled motion-planning and control problem by formulating a model predictive control (MPC) controller with non-linear constraints for a wheeled differential-drive robot. And, using a human motion prediction strategy based on the recorded pose and trajectory information of both the moving target person and the PFR, add additional constraints to the same MPC, to recompute the optimal controls to the wheels. We make comparisons with RNNs like LSTM and Early Relocation for learning the best-predicted reference path.MPC is best suited for complex constrained problems because it allows the PFR to periodically update the tracking information, as well as to adapt to the moving person’s stride. We show the results using a simulated indoor environment and lay the foundation for its implementation on a real robot. Our proposed method offers a robust person following behaviour without the explicit need for policy learning or offline computation, allowing us to design a generalized framework.",https://ieeexplore.ieee.org/document/9515348/,2021 30th IEEE International Conference on Robot & Human Interactive Communication (RO-MAN),8-12 Aug. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AIVR.2018.00060,Towards a Music Visualization on Robot (MVR) Prototype,IEEE,Conferences,"This paper presents a Music Visualization on Robot (MVR) prototype system which automatically links the flashlight, color and emotion of a robot through music. The MVR system is divided into three portions. Firstly, the system calculates the waiting time for a flashlight by beat tracking. Secondly, the system calculates the emotion correlated with music mood. Thirdly, the system links the color with emotion. To illustrate the prototype on a robot, the prototype implementation is based on a programmable robot called Zenbo because Zenbo has 8 LED light colors on 2 wheels and 24 face emotions to support various compositions.",https://ieeexplore.ieee.org/document/8613679/,2018 IEEE International Conference on Artificial Intelligence and Virtual Reality (AIVR),10-12 Dec. 2018,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SIPROCESS.2016.7888345,Towards robust ego-centric hand gesture analysis for robot control,IEEE,Conferences,"Wearable device with an ego-centric camera would be the next generation device for human-computer interaction such as robot control. Hand gesture is a natural way of ego-centric human-computer interaction. In this paper, we present an ego-centric multi-stage hand gesture analysis pipeline for robot control which works robustly in the unconstrained environment with varying luminance. In particular, we first propose an adaptive color and contour based hand segmentation method to segment hand region from the ego-centric viewpoint. We then propose a convex U-shaped curve detection algorithm to precisely detect positions of fingertips. And parallelly, we utilize the convolutional neural networks to recognize hand gestures. Based on these techniques, we combine most information of hand to control the robot and develop a hand gesture analysis system on an iPhone and a robot arm platform to validate its effectiveness. Experimental result demonstrates that our method works perfectly on controlling the robot arm by hand gesture in real time.",https://ieeexplore.ieee.org/document/7888345/,2016 IEEE International Conference on Signal and Image Processing (ICSIP),13-15 Aug. 2016,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCICT.2012.6398104,Tracking of a target person using face recognition by surveillance robot,IEEE,Conferences,"In this paper we designed an experimental setup in order to have human-robot interaction i.e. first we are going to detect the face and after that we recognise the detected face. Afterwards we get the persons upper body torso color as a key feature. As we extracted the color feature we can compute the moments and also evaluate the motion parameters so that the surveillance robot can track the person accordingly. We also had introduced Speech module in order to have a interaction between the remote and base station. Surveillance robot must track the targeted person in a robust manner in indoor and outdoor environment in different light and dynamic varying conditions. In our proposed setup we use PCA which is going to recognise the person in a real time environment and should communicate to the person via speech module deployed in the surveillance robot, as face recognition works on real time environment we are getting average recognition rate of 98%. Experiment demonstration validates the efficient performance of the approach.",https://ieeexplore.ieee.org/document/6398104/,"2012 International Conference on Communication, Information & Computing Technology (ICCICT)",19-20 Oct. 2012,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ROBOT.2010.5509160,Transfer of skills between human operators through haptic training with robot coordination,IEEE,Conferences,"In this paper, we discuss a coordinated haptic training architecture useful for transferring expertise in teleoperation-based manipulation between two human users. The objective is to construct a reality-based haptic interaction system for knowledge transfer by linking an expert's skill with robotic movement in real time. The benefits from this approach include 1) a representation of an expert's knowledge into a more compact and general form by learning from a minimized set of training samples, and 2) an increase in the capability of a novice user by coupling learned skills absorbed by a robotic system with haptic feedback. In order to evaluate our ideas and present the effectiveness of our paradigm, human handwriting is selected as our experiment of interest. For the learning algorithms, artificial neural network (ANN) and support vector machine (SVM) are utilized and their performances are compared. For the evaluation of the performance of the output of the learning modules, a modified Longest Common Subsequence (LCSS) algorithm is implemented. Results show that one or two experts' samples are sufficient for the generation of haptic training knowledge, which can successfully recreate manipulation motion with a robotic system and transfer haptic forces to an untrained user with a haptic device. Also in the case of handwriting comparison, the similarity measures result in up to an 88% match even with a minimized set of training samples.",https://ieeexplore.ieee.org/document/5509160/,2010 IEEE International Conference on Robotics and Automation,3-7 May 2010,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ROBOT.1998.680515,Unsupervised learning to recognize environments from behavior sequences in a mobile robot,IEEE,Conferences,"We describe the development of a mobile robot which does unsupervised learning for recognizing environments from behavior sequences. Most studies on recognizing an environment have tried to build precise geometric maps with high sensitive and global sensors. However such precise and global information may not be obtained in real environments. Furthermore unsupervised-learning is necessary for recognition in unknown environments without help of a teacher. Thus we attempt to build a mobile robot which does unsupervised-learning to recognize environments with low sensitivity and local sensors. The mobile robot is behavior-based and does wall-following in enclosures. Then the sequences of behaviors executed in each enclosure are transformed into input vectors for a self-organizing network. Learning without a teacher is done, and the robot becomes able to identify enclosures. Moreover we developed a method to identify environments independent of a start point using a partial sequence. We have fully implemented the system with a real mobile robot, and made experiments for evaluating the ability. As a result, we found out that the environment recognition was done well and our method was adaptive to noisy environments.",https://ieeexplore.ieee.org/document/680515/,Proceedings. 1998 IEEE International Conference on Robotics and Automation (Cat. No.98CH36146),20-20 May 1998,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SPW50608.2020.00045,Using Taint Analysis and Reinforcement Learning (TARL) to Repair Autonomous Robot Software,IEEE,Conferences,"It is important to be able to establish formal performance bounds for autonomous systems. However, formal verification techniques require a model of the environment in which the system operates; a challenge for autonomous systems, especially those expected to operate over longer timescales. This paper describes work in progress to automate the monitor and repair of ROS-based autonomous robot software written for an apriori partially known and possibly incorrect environment model. A taint analysis method is used to automatically extract the dataflow sequence from input topic to publish topic, and instrument that code. A unique reinforcement learning approximation of MDP utility is calculated, an empirical and non-invasive characterization of the inherent objectives of the software designers. By comparing design (a-priori) utility with deploy (deployed system) utility, we show, using a small but real ROS example, that it's possible to monitor a performance criterion and relate violations of the criterion to parts of the software. The software is then patched using automated software repair techniques and evaluated against the original off-line utility.",https://ieeexplore.ieee.org/document/9283859/,2020 IEEE Security and Privacy Workshops (SPW),21-21 May 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/HNICEM.2018.8666242,Utilization of Fuzzy Logic Control in a Waste Robot,IEEE,Conferences,"This research aimed to design and develop an autonomous robot to feasibly address waste disposal issues in common indoor places. The researchers explored opportunities to improve path planning using Fuzzy Logic Control (FLC). The researchers utilized a Microcontroller Unit (MCU) to control input proximity, sound, and infrared sensors, and output geared Direct Current (DC) motors through machine learning and electromechanical interface. The researchers simulated an adaptive algorithm using Mamdani-type FLC model, implemented using C programming language, then downloaded as machine code to a real prototype. Based on significant test results, the waste robot accurately detected human interference, a feature that would be pivotal in overcoming individual indifferences on waste management.",https://ieeexplore.ieee.org/document/8666242/,"2018 IEEE 10th International Conference on Humanoid, Nanotechnology, Information Technology,Communication and Control, Environment and Management (HNICEM)",29 Nov.-2 Dec. 2018,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICMLC.2018.8526952,Vision-Based Line-Following Control of a Two-Wheel Self-Balancing Robot,IEEE,Conferences,"This paper presents a vision-based two-wheel self-balancing (TWSB) robot to follow a black line using visual feedback. We use MATLAB software to connect to the URL of IP camera and use image processing toolbox to process the image from the IP camera. After image processing, this paper sets 10 coordinates to detect if the black line is straight or the black line is in different kind of situation. This paper considers the black line including straight line, curve line, intersection and inconsecutive line. Thus, a cascade intelligent motion control system is proposed to control the balancing and moment of the vision-based TWSB robot with tracking the position and direction commands from MATLAB software. Finally, it shows that the vision-based TWSB robot can trace the black line on the map very well from the real-time experimental results.",https://ieeexplore.ieee.org/document/8526952/,2018 International Conference on Machine Learning and Cybernetics (ICMLC),15-18 July 2018,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICOVET50258.2020.9230275,Vision-Based Robot Hand Using Open Source Software,IEEE,Conferences,"The robot hand can plan grasp movements based on the position of the target object and the motion of the robot hand. The position of the target object is recognized from the image captured by a camera mounted on the robot arm, and the motion of the robot hand is estimated from the inertial measurement unit (IMU). We also adopt a variety of mechanisms to determine the target object among the objects detected in the camera scene. Experiments are conducted to verify the validity of control system. The experiments proved that the developed system can support the user to grasp the target object.",https://ieeexplore.ieee.org/document/9230275/,2020 4th International Conference on Vocational Education and Training (ICOVET),19-19 Sept. 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/FSKD.2017.8393254,Visual control system design of wheeled inverted pendulum robot based on Beaglebone Black,IEEE,Conferences,"The wheeled inverted pendulum robot has broad prospects of applications in real life. It can use two coaxial wheels to achieve the body self-balancing, forward moving and turning. But the general wheeled inverted pendulum robot seldom has vision function to perceive enviromental change. In order to realize the robust visual control, a wheeled inverted-pendulum vision robot with attitude sensors, photoelectric encoders, ultrasonic sensors and so on is designed based on Beaglebone Black board. The moving object is separated in the space domain by obtaining the image sequence which is sent by a robot-mounted camera, and the modeling, identification and tracking of target sequence are implemented in the time domain. The balance PD, speed PI and steering PD controllers are designed to realize the dynamic balance, forward and steering function of the robot. To satisfy the functional requirements of the visual tracking system, an improved tracking-learning-detection algorithm based on kernelized correlation filtering is used, and a tracking anomaly based on spatial context is detected to determine the tracking state and reduce the error rate. Experimental results show that the robot reaches the requirement of design and achieves better visual control effectiveness.",https://ieeexplore.ieee.org/document/8393254/,"2017 13th International Conference on Natural Computation, Fuzzy Systems and Knowledge Discovery (ICNC-FSKD)",29-31 July 2017,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICMLA.2006.53,Web Robot Learning Powered by Bluetooth Communication System,IEEE,Conferences,"This paper presents a web robot web-robot learning powered by Bluetooth communication system. The web-robot system is used as the virtual robot laboratory integrating a number of disciplines in engineering. This virtual laboratory is a valuable teaching tool for engineering education used at any time and from any location through Internet. The mobile robot was controlled with robot server named as control center. The server can be connected to mobile robot via Bluetooth adapter. The mobile robot system focuses on vision sensing. Real time image processing techniques are realized by the web robot system. This system can also realize monitoring, tele-controlling, parameter adjusting and reprogramming through Internet exclusively with a standard Web browser without the need of any additional software",https://ieeexplore.ieee.org/document/4041484/,2006 5th International Conference on Machine Learning and Applications (ICMLA'06),14-16 Dec. 2006,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICMLA.2007.19,Web-based maze robot learning using fuzzy motion control system,IEEE,Conferences,"In this study, a Web based maze robot system has been designed and implemented for solving different maze algorithms with the help of machine learning approaches. The robot system has a map-based heuristic maze solving algorithm. The algorithm used for solving the maze is based on map creation and produces a control signal for robot direction. Robot motions were controlled by a fuzzy motion control system running on a chip. The control algorithm can be easily changed with the help of an algorithm via web interface controlled by the control center. The control center program powered by MATLAB functions and special libraries (image and control) in DELPHI manage all robotic activities. These activities are: command interpreter, image capturing, processing and serving, machine learning techniques, Web serving, database management, communication with robot, and compiling microcontroller programs. The results have shown that the proposed, designed and implemented system provides amazing new features to the applicants doing their real-time programming exercises on Web.",https://ieeexplore.ieee.org/document/4457243/,Sixth International Conference on Machine Learning and Applications (ICMLA 2007),13-15 Dec. 2007,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICAE50557.2020.9350386,XNOR-YOLO: The High Precision of The Ball and Goal Detecting on The Barelang-FC Robot Soccer,IEEE,Conferences,"The essential part in developing the humanoid robot which is able to play soccer is the vision system. The vision system needs to be fast and accurate in detecting the surrounding objects on the field such as the ball, goal, teammates, or even the opponent. One of the powerful methods which was able to generate the object detection quickly with high accuracy was the deep learning. However, this method proceeded a huge computation. Even if it was generated on the GPU, it would still generat a low speed of detecting. Therefore, the high precision and fast detecting of the object method need to be considered in this area. In order to overcome this problem, we proposed the combination of the XNOR-Network (XNOR-Net) towards YOLOv3 running on the GPU with the same layer configuration as the tinyYOLO. To testify the performance of this method, some experiments has been carried out in real-time application by implementing it in the NVDIA Jetson TX1 GPU. From the experiment results, this method is able to detect the object faster than other object detection in detecting the ball and goal colored by white and generated 30 FPS in detecting each object.",https://ieeexplore.ieee.org/document/9350386/,2020 3rd International Conference on Applied Engineering (ICAE),7-8 Oct. 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/TSMCC.2004.840063,"""Sticky Hands"": learning and generalization for cooperative physical interactions with a humanoid robot",IEEE,Journals,"""Sticky Hands"" is a physical game for two people involving gentle contact with the hands. The aim is to develop relaxed and elegant motion together, achieve physical sensitivity-improving reactions, and experience an interaction at an intimate yet comfortable level for spiritual development and physical relaxation. We developed a control system for a humanoid robot allowing it to play Sticky Hands with a human partner. We present a real implementation including a physical system, robot control, and a motion learning algorithm based on a generalizable intelligent system capable itself of generalizing observed trajectories' translation, orientation, scale and velocity to new data, operating with scalable speed and storage efficiency bounds, and coping with contact trajectories that evolve over time. Our robot control is capable of physical cooperation in a force domain, using minimal sensor input. We analyze robot-human interaction and relate characteristics of our motion learning algorithm with recorded motion profiles. We discuss our results in the context of realistic motion generation and present a theoretical discussion of stylistic and affective motion generation based on, and motivating cross-disciplinary research in computer graphics, human motion production and motion perception.",https://ieeexplore.ieee.org/document/1522534/,"IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews)",Nov. 2005,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ACCESS.2020.3042439,A Fuzzy Ensemble Method With Deep Learning for Multi-Robot System,IEEE,Journals,"In a multi-robot system, situation assessment evaluates the current situation quantitatively to help decision-makers make the best decision. Conventional situation assessment methods ignore the initiative of each robot, so it often encounters bottlenecks. Collaborative intelligence shows better performance than a single global decision. To address this problem, this work introduces a deep learning-based fuzzy adaptive method (DLFA) to achieve the real-time situation assessment for a multi-robot system. The proposed method employs the shortest path faster algorithm to achieve information sharing between agents. The shortest path faster algorithm ensures that the agent distributes its state information to its teammates in the fastest way. Each agent gets the information from teammates and treats their state as the observation of the scene. Deep neural network maps current observations into a local situation assessment result by combining a large number of nonlinear processing layers. Finally, each local assessment result is regarded as a brick to construct the final situation assessment via a fuzzy ensemble method. Experimental results show that the proposed method outperforms competitors.",https://ieeexplore.ieee.org/document/9279196/,IEEE Access,2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ACCESS.2021.3091642,A Human-Robot Interaction System Calculating Visual Focus of Human’s Attention Level,IEEE,Journals,"Attention is the mental awareness of human on a particular object or a piece of information. The level of attention indicates how intense the focus is on an object or an instance. In this study, several types of human attention level have been observed. After introducing image segmentation and detection technique for facial features, eyeball movement and gaze estimation were measured. Eye movement were assessed using the video data, and a total of 10197 data instances were manually labelled for the attention level. Then Artificial Neural Network (ANN) and Recurrent Neural Network-Long Short Term Memory (LSTM) based Deep learning (DL) architectures have been proposed for analysing the data. Next, the trained DL model has been implanted into a robotic system that is capable of detecting various features; ultimately leading to the calculation of visual attention for reading, browsing, and writing purposes. This system is capable of checking the attention level of the participants and also can detect if participants are present or not. Based on a certain level of visual focus of attention (VFOA), this system interacts with the person, generates awareness and establishes verbal or visual communication with that person. The proposed ML techniques have achieved almost 99.24% validation accuracy and 99.43% test accuracy. It is also shown in the comparative study that, since the dataset volumes are limited, ANN is more suitable for attention level calculation than RNN-LSTM. We hope that the implemented robotic structure manifests the real-world implication of the proposed method.",https://ieeexplore.ieee.org/document/9462086/,IEEE Access,2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/TAMD.2011.2164404,A Multiple Context Brain for Experiments With Robot Consciousness,IEEE,Journals,"The PURR-PUSS system (PP) is a versatile model of a human-like brain, designed to be implemented in parallel hardware and embodied in the head of a robot moving in the real world. The aim of the research with PP is to try out mechanisms for learning, intelligence and consciousness. Limitations of resources have dictated that the experiments with PP are made on a personal computer by simulating the brain and robot body in a microworld. The unique features of PP are multiple context and novelty-seeking. In this paper, a squash-pop microworld is described first, so that concrete examples can be given for a brief review of the PP system, followed by two new features called trail memory, to realize Baars' global workspace, and belief memory, to realize Rosenthal's higher order thoughts and Johnson-Laird's conscious reasoning. The extended system, PP*, is designed to give consciousness to the subconscious PP, but higher order thoughts and conscious reasoning prove to be elusive. A definition of a conscious robot provides a measure of progress.",https://ieeexplore.ieee.org/document/5986693/,IEEE Transactions on Autonomous Mental Development,Dec. 2011,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/TSMC.2013.2297398,A Multiple-Feature and Multiple-Kernel Scene Segmentation Algorithm for Humanoid Robot,IEEE,Journals,"This paper presents a multiple-feature and multiple-kernel support vector machine (MFMK-SVM) methodology to achieve a more reliable and robust segmentation performance for humanoid robot. The pixel wise intensity, gradient, and C1 SMF features are extracted via the local homogeneity model and Gabor filter, which would be used as inputs of MFMK-SVM model. It may provide multiple features of the samples for easier implementation and efficient computation of MFMK-SVM model. A new clustering method, which is called feature validity-interval type-2 fuzzy C-means (FV-IT2FCM) clustering algorithm, is proposed by integrating a type-2 fuzzy criterion in the clustering optimization process to improve the robustness and reliability of clustering results by the iterative optimization. Furthermore, the clustering validity is employed to select the training samples for the learning of the MFMKSVM model. The MFMK-SVM scene segmentation method is able to fully take advantage of the multiple features of scene image and the ability of multiple kernels. Experiments on the BSDS dataset and real natural socene images demonstrate the superior performance of our proposed method.",https://ieeexplore.ieee.org/document/6717184/,IEEE Transactions on Cybernetics,Nov. 2014,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/TSMC.2019.2956321,A Novel Approach to Image-Sequence-Based Mobile Robot Place Recognition,IEEE,Journals,"Visual place recognition is a challenging problem in simultaneous localization and mapping (SLAM) due to a large variability of the scene appearance. A place is usually described by a single-frame image in conventional place recognition algorithms. However, it is unlikely to completely describe the place appearance using a single frame image. Moreover, it is more sensitive to the change of environments. In this article, a novel image-sequence-based framework for place detection and recognition is proposed. Rather than a single frame image, a place is represented by an image sequence in this article. Position invariant robust feature (PIRF) descriptors are extracted from images and processed by the incremental bag-of-words (BoWs) for feature extraction. The robot automatically partitions the sequentially acquired images into different image sequences according to the change of the environmental appearance. Then, the echo state network (ESN) is applied to model each image sequence. The resultant states of the ESN are used as features of the corresponding image sequence for place recognition. The proposed method is evaluated on two public datasets. Experimental comparisons with the FAB-MAP 2.0 and SeqSLAM are conducted. Finally, a real-world experiment on place recognition with a mobile robot is performed to further verify the proposed method.",https://ieeexplore.ieee.org/document/8931657/,"IEEE Transactions on Systems, Man, and Cybernetics: Systems",Sept. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/JSEN.2020.3042665,A Searching Space Constrained Partial to Full Registration Approach With Applications in Airport Trolley Deployment Robot,IEEE,Journals,"For airports with high passenger and luggage flows, a large number of staff members have to be hired to deploy the scattered passenger luggage trolleys. To release humans from the repetitive and laborious job, we develop an autonomous trolley deployment robot to detect, transport and collect the scattered idle trolleys to recycling points. This paper will firstly illustrate the entire collection pipeline of the deployment robot system and then address the key challenge: partial to full point set registration. With the perception framework, the robot can detect the idle trolleys and acquire the pose of the trolleys on the ground, and then capture the trolley from behind, along the same direction for subsequent grasping and manipulation. With RGB-D camera and a segmentation Convolutional Neural Network, the robot can generate a partial surface point cloud of the detected trolley. The resulting point cloud, data and a pre-scanned full trolley point cloud, model, are matched by an implicit pose. To tackle the low accuracy and long computation time issues, a novel searching space-constrained point set registration algorithm is proposed to register the two overlapping point sets. Based on Branch-and-Bound (BnB) mechanism, the error between data and model is iteratively optimized. The constraint of searching space speeds up the global searching of the optimal pose, by pruning the candidate spaces which is impossible to contain the optimal result. To evaluate the performance, an airport trolley segmentation dataset and a point cloud dataset for registration are constructed. Experimental results on the datasets and synthetic dataset show that our method achieves higher accuracy and success rate than the previous methods. The experiments demonstrated in video clips validate the developed system works in real-world applications.",https://ieeexplore.ieee.org/document/9281085/,IEEE Sensors Journal,"15 May15, 2021",ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/JPROC.2018.2840045,A Value-Driven Eldercare Robot: Virtual and Physical Instantiations of a Case-Supported Principle-Based Behavior Paradigm,IEEE,Journals,"In this paper, a case-supported principle-based behavior paradigm is proposed to help ensure ethical behavior of autonomous machines. We argue that ethically significant behavior of autonomous systems should be guided by explicit ethical principles determined through a consensus of ethicists. Such a consensus is likely to emerge in many areas in which autonomous systems are apt to be deployed and for the actions they are liable to undertake. We believe that this is the case since we are more likely to agree on how machines ought to treat us than on how human beings ought to treat one another. Given such a consensus, particular cases of ethical dilemmas where ethicists agree on the ethically relevant features and the right course of action can be used to help discover principles that balance these features when they are in conflict. Such principles not only help ensure ethical behavior of complex and dynamic systems but also can serve as a basis for justification of this behavior. The requirements, methods, implementation, and evaluation components of the paradigm are detailed as well as its instantiation in both a simulated and real robot functioning in the domain of eldercare.",https://ieeexplore.ieee.org/document/8500162/,Proceedings of the IEEE,March 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/TNSRE.2020.3038175,AI Therapist Realizing Expert Verbal Cues for Effective Robot-Assisted Gait Training,IEEE,Journals,"Repetitive and specific verbal cues by a therapist are essential in aiding a patient's motivation and improving the motor learning process. The verbal cues comprise various expressions, sentences, volumes, and timings, depending on the therapist's proficiency. This paper proposes an AI therapist (AI-T) that implements the verbal cues of professional therapists having extensive experience with robot-assisted gait training using the SUBAR for stroke patients. The AI-T was developed using a neuro-fuzzy system, a machine learning technique leveraging the benefits of fuzzy logic and artificial neural networks. The AI-T was trained with the professional therapist's verbal cue data, as well as clinical and robotic data collected from robot-assisted gait training with real stroke patients. Ten clinical data and 16 robotic data are input variables, and six verbal cues are output variables. Fifty-eight stroke patients wore the SUBAR, a gait training robot, and participated in the robot-assisted gait training. A total of 9059 verbal cue data, 580 clinical data of stroke patients, and 144 944 robotic data were collected from 693 training sessions. Test results show that the trained AI-T can implement six types of verbal cues with 93.7% accuracy for the 1812 verbal cue data of the professional therapist. Currently, the trained AI-T is deployed in the SUBAR and provides six verbal cues to stroke patients in robot-assisted gait training.",https://ieeexplore.ieee.org/document/9260225/,IEEE Transactions on Neural Systems and Rehabilitation Engineering,Dec. 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/JIOT.2020.2979413,Adversarial Learning-Enabled Automatic WiFi Indoor Radio Map Construction and Adaptation With Mobile Robot,IEEE,Journals,"Location-based service (LBS) has become an indispensable part of our daily lives. Realizing accurate LBS in indoor environments is still a challenging task. WiFi fingerprinting-based indoor positioning system (IPS) has achieved encouraging results recently, but the time and labor overhead of constructing a dense WiFi radio map remains the key bottleneck that hinders it for real-world large-scale implementation. In this article, we propose WiGAN an automatic fine-grained indoor ratio map construction and the adaptation scheme empowered by the Gaussian process regression conditioned least-squares generative adversarial networks (GPR-GANs) with a mobile robot. First, we develop a mobile robotic platform that constructs the spatial map and radio map simultaneously in the easily accessed free space. GPR-GAN first establishes a Gaussian process regression (GPR) model using the real received signal strength (RSS) measurements collected by our robotic platform via LiDAR SLAM in the free space. Then, the outputs of the GPR are adopted as the input of GAN's generator. The learning objective of GAN is to synthesize realistic RSS data in a constrained space where it has not been covered and model the irregular RSS distributions in complex indoor environments. Real-world experiments were conducted in a real-world indoor environment, which confirms the feasibility, high accuracy, and superiority of WiGAN over existing solutions in terms of both RSS estimation accuracy and localization accuracy.",https://ieeexplore.ieee.org/document/9031749/,IEEE Internet of Things Journal,Aug. 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/21.3461,An approach to an expert robot welding system,IEEE,Journals,"Adaptive control and sensory processing techniques in robotic arc welding are discussed. The gas metal arc welding and gas tungsten arc welding processes are considered, along with a literature review of aspects of welding automation. Topics covered include process modeling, detection and measurement of process features, real-time control, and implementation considerations. An approach for an adaptive welding system is presented. The proposed architecture fits within the scope of an ambitious project to develop an expert welding robot. Different levels of automation are discussed, from the decision level to the closed-loop control of process variables and torch trajectory.<>",https://ieeexplore.ieee.org/document/3461/,"IEEE Transactions on Systems, Man, and Cybernetics",March-April 1988,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ACCESS.2021.3079427,Autonomous Endoscope Robot Positioning Using Instrument Segmentation With Virtual Reality Visualization,IEEE,Journals,"This paper presents a method for endoscope's autonomous positioning by a robotic endoscope holder for minimally invasive surgery. The method improves human-robot cooperation in robot-assisted surgery by allowing the endoscope holder to acknowledge the surgeon's view projection and navigate the camera without manual control. The real-time prediction of next desired camera location is estimated using segmented instrument's tip locations from endoscope video and surgeon's attention focus given by tracked virtual reality headset. To tackle the issue of real-time surgical instrument segmentation for more precise instrument tip localization, we propose the YOLOv3 and ResNet Combined Neural Network. The method showed an 86.6% IoU across MICCAI'17 Endovis datasets with 30 frames per second processing speed. The proposed pipeline was implemented in ROS on Ubuntu with visualization running under Windows operating system in Unity3D. The simulation demonstrates the robotic arm, endoscope, and surgical environment visualized in 3D in the virtual reality headset to provide a stable view of the endoscope and improve the surgeon's perception of the operating environment.",https://ieeexplore.ieee.org/document/9429186/,IEEE Access,2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ACCESS.2021.3093340,Ball Motion Control in the Table Tennis Robot System Using Time-Series Deep Reinforcement Learning,IEEE,Journals,"One of the biggest challenges hindering a table tennis robot to play as well as a professional player is the ball’s accurate motion control, which depends on various factors such as the incoming ball’s position, linear, spin velocity and so forth. Unfortunately, some factors are almost impossible to be directly measured in real practice, such as the ball’s spin velocity, which is difficult to be estimated from vision due to the little texture on the ball’s surface. To perform accurate motion control in table tennis, this study proposes to learn a ball stroke strategy to guarantee desirable “target landing location” and the “over-net height” which are two key indicators to evaluate the quality of a stroke. To overcome the spin velocity challenge, a deep reinforcement learning (DRL) based stroke approach is developed with the spin velocity estimation capability, through which the system can predict the relative spin velocity of the ball and stroke it back accurately by iteratively learning from the robot-environment interactions. To pre-train the DRL-based strategy effectively, this paper develops a virtual table tennis playing environment, through which various simulated data can be collected. For the real table tennis robot implementation, experimental results demonstrate the superior performance of the proposed control strategy compared to that of the traditional aerodynamics-based method with an average landing error around 80mm and the landing-within-table probability higher than 70%.",https://ieeexplore.ieee.org/document/9467347/,IEEE Access,2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ACCESS.2021.3062329,Biologically-Inspired Legged Robot Locomotion Controlled With a BCI by Means of Cognitive Monitoring,IEEE,Journals,"Brain-computer interfaces (BCI) are a mechanism to record the electrical signals of the brain and translate them into commands to operate an output device like a robotic system. This article presents the development of a real-time locomotion system of a hexapod robot with bio-inspired movement dynamics inspired in the stick insect and tele-operated by cognitive activities of motor imagination. Brain signals are acquired using only four electrodes from a BCI device and sent to computer equipment for processing and classification by the iQSA method based on quaternion algebra. A structure consisting of three main stages are proposed: (1) signal acquisition, (2) data analysis and processing by the iQSA method, and (3) bio-inspired locomotion system using a Spiking Neural Network (SNN) with twelve neurons. An off-line training stage was carried out with data from 120 users to create the necessary decision rules for the iQSA method, obtaining an average performance of 97.72%. Finally, the experiment was implemented in real-time to evaluate the performance of the entire system. The recognition rate to achieve the corresponding gait pattern is greater than 90% for BCI, and the time delay is approximately from 1 to 1.5 seconds. The results show that all the subjects could generate their desired mental activities, and the robotic system could replicate the gait pattern in line with a slight delay.",https://ieeexplore.ieee.org/document/9363897/,IEEE Access,2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/LRA.2022.3146589,Bottom-Up Skill Discovery From Unsegmented Demonstrations for Long-Horizon Robot Manipulation,IEEE,Journals,"We tackle real-world long-horizon robot manipulation tasks through skill discovery. We present a bottom-up approach to learning a library of reusable skills from unsegmented demonstrations and use these skills to synthesize prolonged robot behaviors. Our method starts with constructing a hierarchical task structure from each demonstration through agglomerative clustering. From the task structures of multi-task demonstrations, we identify skills based on the recurring patterns and train goal-conditioned sensorimotor policies with hierarchical imitation learning. Finally, we train a meta controller to compose these skills to solve long-horizon manipulation tasks. The entire model can be trained on a small set of human demonstrations collected within 30 minutes without further annotations, making it amendable to real-world deployment. We systematically evaluated our method in simulation environments and on a real robot. Our method has shown superior performance over state-of-the-art imitation learning methods in multi-stage manipulation tasks. Furthermore, skills discovered from multi-task demonstrations boost the average task success by 8% compared to those discovered from individual tasks.",https://ieeexplore.ieee.org/document/9695333/,IEEE Robotics and Automation Letters,April 2022,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/TSMCB.2006.874131,Control Architecture for Human–Robot Integration: Application to a Robotic Wheelchair,IEEE,Journals,"Completely autonomous performance of a mobile robot within noncontrolled and dynamic environments is not possible yet due to different reasons including environment uncertainty, sensor/software robustness, limited robotic abilities, etc. But in assistant applications in which a human is always present, she/he can make up for the lack of robot autonomy by helping it when needed. In this paper, the authors propose human-robot integration as a mechanism to augment/improve the robot autonomy in daily scenarios. Through the human-robot-integration concept, the authors take a further step in the typical human-robot relation, since they consider her/him as a constituent part of the human-robot system, which takes full advantage of the sum of their abilities. In order to materialize this human integration into the system, they present a control architecture, called architecture for human-robot integration, which enables her/him from a high decisional level, i.e., deliberating a plan, to a physical low level, i.e., opening a door. The presented control architecture has been implemented to test the human-robot integration on a real robotic application. In particular, several real experiences have been conducted on a robotic wheelchair aimed to provide mobility to elderly people",https://ieeexplore.ieee.org/document/1703648/,"IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics)",Oct. 2006,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ACCESS.2020.3033550,Developing a Lightweight Rock-Paper-Scissors Framework for Human-Robot Collaborative Gaming,IEEE,Journals,"We present a novel implementation of a Rock-Paper-Scissors (RPS) game interaction with a social robot. The framework is tailored to be computationally lightweight, as well as entertaining and visually appealing through collaboration with designers and animators. The fundamental gesture recognition pipeline employs a Leap motion device and two separate machine learning architectures to evaluate kinematic hand data on-the-fly. The first architecture is used to recognize and segment human motion activity in order to initialize the RPS play, and the second architecture is used to classify hand gestures into rock, paper or scissors. The employed tabletop robot interacts in the RPS play through unique animated gestural movements and vocalizations designed by animators which communicate the robot's choices as well as cognitive reflection on winning, losing and draw states. Performance of both learning architectures is carefully evaluated with respect to accuracy, reliability and run time performance under different feature and classifier types. Moreover, we assess our system during an interactive RPS play between robot and human. Experimental results show that the proposed system is robust to user variations and play style in real environment conditions. As such, it offers a powerful application for the subsequent exploration of social human-machine interaction.",https://ieeexplore.ieee.org/document/9239276/,IEEE Access,2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/3516.537045,Development and integration of generic components for a teachable vision-based mobile robot,IEEE,Journals,"This paper presents a mobile robotic system for human assistance in navigation-the robot navigates by receiving visual instructions from a human being and is able to replicate them autonomously. We describe three generic components defined as the HOST, the VISION, and the CONTROL components as well as their integration in our teachable mobile robot. These components are connected to each other via a transputer serial link, namely they are loosely coupled, they work in parallel and are asynchronous with each other. Each component is described with a peculiar feature of extensibility. Especially in the VISION component, there are two major features. The first one is a correlator which each vision board possesses. The correlator does block-matching between the template and the grabbed images in real-time. The other is the PIM library which manages the visual tasks over limited parallel visual resources of the mobile robot. These features of our design enable the system to be real-time and allow for efficient and extensible software development. In order to show the feasibility of our system design, we present a preliminary experiment of the route teaching on our mobile robot.",https://ieeexplore.ieee.org/document/537045/,IEEE/ASME Transactions on Mechatronics,Sept. 1996,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/TMECH.2013.2294180,Development of a Laser-Range-Finder-Based Human Tracking and Control Algorithm for a Marathoner Service Robot,IEEE,Journals,"This paper presents a human detection algorithm and an obstacle avoidance algorithm for a marathoner service robot (MSR) that provides a service to a marathoner while training. To be used as a MSR, the mobile robot should have the abilities to follow a running human and avoid dynamically moving obstacles in an unstructured outdoor environment. To detect a human by a laser range finder (LRF), we defined features of the human body in LRF data and employed a support vector data description method. In order to avoid moving obstacles while tracking a running person, we defined a weighted radius for each obstacle using the relative velocity between the robot and an obstacle. For smoothly bypassing obstacles without collision, a dynamic obstacle avoidance algorithm for the MSR is implemented, which directly employed a real-time position vector between the robot and the shortest path around the obstacle. We verified the feasibility of these proposed algorithms through experimentation in different outdoor environments.",https://ieeexplore.ieee.org/document/6690173/,IEEE/ASME Transactions on Mechatronics,Dec. 2014,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/70.650165,Environment prediction for a mobile robot in a dynamic environment,IEEE,Journals,"The problem of navigating a mobile robot among moving obstacles is usually solved on the condition of knowing the velocity of obstacles. However, it is difficult to provide such information to a robot in real time. In this paper, we present an environment predictor that provides an estimate of future environment configuration by fusing multisensor data in real time. The predictor is implemented by an artificial neural network (ANN) trained using a relative-error-backpropagation (REBP) algorithm. The REBP algorithm enables the ANN to provide output data with a minimum relative error, which is better than conventional backpropagation (BP) algorithms in this prediction application. The mobile robot can, therefore, respond to anticipated changes in the environment. The performance is verified by prediction simulation and navigation experiments.",https://ieeexplore.ieee.org/document/650165/,IEEE Transactions on Robotics and Automation,Dec. 1997,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/LRA.2022.3178473,External and Internal Sensor Fusion Based Localization Strategy for 6-DOF Pose Estimation of a Magnetic Capsule Robot,IEEE,Journals,"This paper introduces a novel localization approach for active capsule endoscopy that, for the first time, combines external magnetic field sensing and internal inertial sensing to realize 6-DOF pose estimation of a magnetic capsule robot. It utilizes an inertial measurement unit embedded in the capsule with an external magnetic sensor array to estimate the 6-DOF pose of the capsule, which does not require complicated structures of the capsule and the actuator or the implementation of specific motions of the magnets, and can achieve accurate and real-time localization of the capsule in a large workspace. We formulate the localization model and analyze the singularities of the method, and present the design approach to determine the configuration of the localization system for efficient and accurate localization in a 0.5 &#x00D7; 0.5 &#x00D7; 0.2 m<inline-formula><tex-math notation=""LaTeX"">$^{3}$</tex-math></inline-formula> workspace. Simulation and real-wold experiments are conducted to validate the effectiveness of the proposed localization strategy. Our results show that the proposed method can achieve a localization accuracy of 5.35 <inline-formula><tex-math notation=""LaTeX"">$\pm$</tex-math></inline-formula> 2.89 mm and 1.46 <inline-formula><tex-math notation=""LaTeX"">$\pm$</tex-math></inline-formula> 1.09<inline-formula><tex-math notation=""LaTeX"">$^\circ$</tex-math></inline-formula> in position and orientation in the real-time tracking task at an update rate of <inline-formula><tex-math notation=""LaTeX"">$60 \,\mathrm{Hz}$</tex-math></inline-formula>. The presented method can be integrated with any magnetic actuation method to achieve closed-loop control of a magnetic capsule robot in the human body.",https://ieeexplore.ieee.org/document/9783101/,IEEE Robotics and Automation Letters,July 2022,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/LRA.2020.2979656,Guided Constrained Policy Optimization for Dynamic Quadrupedal Robot Locomotion,IEEE,Journals,"Deep reinforcement learning (RL) uses model-free techniques to optimize task-specific control policies. Despite having emerged as a promising approach for complex problems, RL is still hard to use reliably for real-world applications. Apart from challenges such as precise reward function tuning, inaccurate sensing and actuation, and non-deterministic response, existing RL methods do not guarantee behavior within required safety constraints that are crucial for real robot scenarios. In this regard, we introduce guided constrained policy optimization (GCPO), an RL framework based upon our implementation of constrained proximal policy optimization (CPPO) for tracking base velocity commands while following the defined constraints. We introduce schemes which encourage state recovery into constrained regions in case of constraint violations. We present experimental results of our training method and test it on the real ANYmal quadruped robot. We compare our approach against the unconstrained RL method and show that guided constrained RL offers faster convergence close to the desired optimum resulting in an optimal, yet physically feasible, robotic control behavior without the need for precise reward function tuning.",https://ieeexplore.ieee.org/document/9028178/,IEEE Robotics and Automation Letters,April 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/TOH.2020.3029043,Human-Inspired Haptic Perception and Control in Robot-Assisted Milling Surgery,IEEE,Journals,"Bone milling is one of the most widely used and high-risk procedures in various types of surgeries, and it is important to be noted that the experienced surgeon can perform such an operation safely. The objective of this article is to enhance the safety of the robot-assisted milling operation with the inspiration of human haptic perception. The emergence, coding and perception of the human haptic are introduced. Following this, a single axis accelerometer that measures the vibration of the surgical power tool is mounted in the robot arm, and the recorded acceleration signal is encoded as parallel stream of binary data. The data are subsequently inputted to the Hopfield network so as to identify the milling state. Inspired by human inference procedure, the fuzzy logic controller is introduced to control the robot to track the desired state when performing bone milling operations. A real-time implementation of the proposed method on a digital signal processing is also described. The experimental results in milling porcine spines prove that the robot accurately discriminates different milling states even when the additive noise is serious, and the safe motion control of the robot is also realized.",https://ieeexplore.ieee.org/document/9220848/,IEEE Transactions on Haptics,1 April-June 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/TMECH.2013.2245337,Image-Based Visual Servoing of a 7-DOF Robot Manipulator Using an Adaptive Distributed Fuzzy PD Controller,IEEE,Journals,"This paper is concerned with the design and implementation of a distributed proportional-derivative (PD) controller of a 7-degrees of freedom (DOF) robot manipulator using the Takagi-Sugeno (T-S) fuzzy framework. Existing machine learning approaches to visual servoing involve system identification of image and kinematic Jacobians. In contrast, the proposed approach actuates a control signal primarily as a function of the error and derivative of the error in the desired visual feature space. This approach leads to a significant reduction in the computational burden as compared to model-based approaches, as well as existing learning approaches to model inverse kinematics. The simplicity of the controller structure will make it attractive in industrial implementations where PD/PID type schemes are in common use. While the initial values of PD gain are learned with the help of model-based controller, an online adaptation scheme has been proposed that is capable of compensating for local uncertainties associated with the system and its environment. Rigorous experiments have been performed to show that visual servoing tasks such as reaching a static target and tracking of a moving target can be achieved using the proposed distributed PD controller. It is shown that the proposed adaptive scheme can dynamically tune the controller parameters during visual servoing, so as to improve its initial performance based on parameters obtained while mimicking the model-based controller. The proposed control scheme is applied and assessed in real-time experiments using an uncalibrated eye-in-hand robotic system with a 7-DOF PowerCube robot manipulator.",https://ieeexplore.ieee.org/document/6471828/,IEEE/ASME Transactions on Mechatronics,April 2014,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/TMECH.2021.3071295,Integral-Free Spatial Orientation Estimation Method and Wearable Rotation Measurement Device for Robot-Assisted Catheter Intervention,IEEE,Journals,"The spatial orientation of rotating objects is typically measured by utilizing inertial measurement units and requires temporal integration of angular velocities. The integration of the angular velocities accumulates the measurement noise and results in erroneous orientation estimation, thus, necessitates real-time error compensation. In this article, an integral-free 3-D orientation estimation framework based on stereo-accelerometry and sensor fusion is proposed and validated. Afterward, a wearable device, MiCarp for robot-assisted interventional surgery was designed, prototyped, and investigated for the accuracy and real-time performance. To achieve the real-time performance, an artificial neural network was trained and implemented in the wearable device. The comparison of the results of the proposed method with a representative complementary filtering method showed the superior performance. The proposed method had a mean-absolute-error of <inline-formula><tex-math notation=""LaTeX"">$1.7^\circ \pm 2.4^\circ$</tex-math></inline-formula>, a measurement range of <inline-formula><tex-math notation=""LaTeX"">$\pm 180^\circ$</tex-math></inline-formula>, and a real-time sample rate of up to 117 Hz. In the end, the feasibility of integrating the proposed device with a representative robotic intervention system was investigated. The proposed wearable device showed the capability of robust capturing of multiple successive rotations for an arterial cannulation task on a vascular model.",https://ieeexplore.ieee.org/document/9397346/,IEEE/ASME Transactions on Mechatronics,April 2022,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ACCESS.2021.3057808,Intuitive Robot Teleoperation Through Multi-Sensor Informed Mixed Reality Visual Aids,IEEE,Journals,"Mobile robotic systems have evolved to include sensors capable of truthfully describing robot status and operating environment as accurately and reliably as never before. This possibility is challenged by effective sensor data exploitation, because of the cognitive load an operator is exposed to, due to the large amount of data and time-dependency constraints. This paper addresses this challenge in remote-vehicle teleoperation by proposing an intuitive way to present sensor data to users by means of using mixed reality and visual aids within the user interface. We propose a method for organizing information presentation and a set of visual aids to facilitate visual communication of data in teleoperation control panels. The resulting sensor-information presentation appears coherent and intuitive, making it easier for an operator to catch and comprehend information meaning. This increases situational awareness and speeds up decision-making. Our method is implemented on a real mobile robotic system operating outdoor equipped with on-board internal and external sensors, GPS, and a reconstructed 3D graphical model provided by an assistant drone. Experimentation verified feasibility while intuitive and comprehensive visual communication was confirmed through an assessment, which encourages further developments.",https://ieeexplore.ieee.org/document/9349454/,IEEE Access,2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/3468.952717,Learning and communication via imitation: an autonomous robot perspective,IEEE,Journals,"This paper proposes a neural network architecture designed to exhibit learning and communication capabilities via imitation. Our architecture allows a ""protoimitation"" behavior using the ""perception ambiguity"" inherent in real environments. In the perspective of turn-taking and gestural communication between two agents, new experiments on movement synchronization in an interaction game are presented. Synchronization is obtained as a global attractor depending on the coupling between agents' dynamics. We also discuss the unsupervised context of the imitation process and present new experiments in which the same architecture is able to learn perception-action associations without any explicit reinforcement. The learning is based on the ability to detect novelty or irregularities in the communication rhythm.",https://ieeexplore.ieee.org/document/952717/,"IEEE Transactions on Systems, Man, and Cybernetics - Part A: Systems and Humans",Sept. 2001,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/TASE.2017.2783342,MASD: A Multimodal Assembly Skill Decoding System for Robot Programming by Demonstration,IEEE,Journals,"Programming by demonstration (PBD) transforms the robot programming from the code level to automated interface between robot and human, promoting the flexibility of robotized automation. In this paper, we focus on programming the industrial robot for assembly tasks by parsing the human demonstration into a series of assembly skills and compiling the skill to the robot executables. To achieve this goal, an identification system using multimodal information to recognize the assembly skill, called MASD, is proposed including: 1) an initial learning stage using a hierarchical model to recognize the action by considering the features from action-object effect, gesture, and trajectory and 2) a retrospective thinking stage using a segmentation method to cut the continuous demonstrations into multiple assembly skills optimally. Using MASD, the demonstration of assembly tasks can be explained with high accuracy in real time, driving a hypothesis that a PBD system on the top of MASD can be extended to more realistic assembly tasks beyond pure positional moving and picking. In experiments, the skill identification module is used to recognize the five kinds of assembly skills in demonstrations of both single and multiple assembly skills, and outperforms the comparative action identification methods. Besides integrated with the MASD, the PBD system can generate the program based on the demonstration and successfully enable an ABB industrial robotic arm simulator to assemble a flashlight and a switch, verifying the initial hypothesis. Note to Practitioners-In the conventional robotized automation, the key role of the robot mainly owes to its capacity for repeating a wide variety of tasks with high speed and accuracy in long term, with a cost of days to months of programming for deployment. On the other hand, the new trend of customization brings the new characteristics: production in short cycle and small volume. This irreversible momentum urges the robot to switch from task to task efficiently. The biggest bottleneck here is the tedious programming, which also has high prerequisites for most practitioners in manufacturing. This situation motivates the development of a PBD system that can understand the assembly skills performed by the human experts in the demonstration and accordingly generate the program for robot's execution of the taught task. In this paper, we present a skill decoding system to parse the observational raw demonstration into symbolic sequences, which is the crucial bridge to enable the automatic programming. The system achieves high performance in recognition and is tailored for the PBD in assembly tasks by considering both advantages and disadvantages in the background of assembly, such as controllable environment and limited computational resources. It is particularly useful for assembly tasks with modularized actions based on a set of standard parts. At the perspective of industrial application, the PBD upon the proposed system is a promising solution to improve the flexibility of manufacture, which is expected to be true in midterm but an important step toward this goal.",https://ieeexplore.ieee.org/document/8263146/,IEEE Transactions on Automation Science and Engineering,Oct. 2018,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/TCYB.2017.2718037,Multiobjective Evolution of Biped Robot Gaits Using Advanced Continuous Ant-Colony Optimized Recurrent Neural Networks,IEEE,Journals,"This paper proposes the optimization of a fully connected recurrent neural network (FCRNN) using advanced multiobjective continuous ant colony optimization (AMO-CACO) for the multiobjective gait generation of a biped robot (the NAO). The FCRNN functions as a central pattern generator and is optimized to generate angles of the hip roll and pitch, the knee pitch, and the ankle pitch and roll. The performance of the FCRNN-generated gait is evaluated according to the walking speed, trajectory straightness, oscillations of the body in the pitch and yaw directions, and walking posture, subject to the basic constraints that the robot cannot fall down and must walk forward. This paper formulates this gait generation task as a constrained multiobjective optimization problem and solves this problem through an AMO-CACO-based evolutionary learning approach. The AMO-CACO finds Pareto optimal solutions through ant-path selection and sampling operations by introducing an accumulated rank for the solutions in each single-objective function into solution sorting to improve learning performance. Simulations are conducted to verify the AMO-CACO-based FCRNN gait generation performance through comparisons with different multiobjective optimization algorithms. Selected software-designed Pareto optimal FCRNNs are then applied to control the gait of a real NAO robot.",https://ieeexplore.ieee.org/document/7964700/,IEEE Transactions on Cybernetics,June 2018,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/TIE.2005.847576,Obstacle avoidance of a mobile robot using hybrid learning approach,IEEE,Journals,"in this paper, a hybrid learning approach for obstacle avoidance of a mobile robot is presented. the key features of the approach are, firstly, innate hardwired behaviors which are used to bootstrap learning in the mobile robot system. a neuro-fuzzy controller is developed from a pre-wired or innate controller based on supervised learning in a simulation environment. the fuzzy inference system has been constructed based on the generalized dynamic fuzzy neural networks learning algorithm of Wu and Er, whereby structure and parameters identification are carried out automatically and simultaneously. Secondly, the neuro-fuzzy controller is capable of re-adapting in a new environment. After carrying out the learning phase on a simulated robot, the controller is implemented on a real robot. A reinforcement learning method based on the fuzzy actor-critic learning algorithm is employed so that the system can re-adapt to a new environment without human intervention. In this phase, the structure of the fuzzy inference system and the parameters of the antecedent parts of fuzzy rules are frozen, and reinforcement learning is applied to further tune the parameters in the consequent parts of the fuzzy rules. Through the hybrid learning approach, an efficient and compact neuro-fuzzy system is generated for obstacle avoidance of a mobile robot in the real world.",https://ieeexplore.ieee.org/document/1435700/,IEEE Transactions on Industrial Electronics,June 2005,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ACCESS.2020.3048877,"Online Measuring of Robot Positions Using Inertial Measurement Units, Sensor Fusion and Artificial Intelligence",IEEE,Journals,"This research introduces a new method to estimate the position of a robot's Tool Center Point (TCP) using Inertial Measurement Units (IMUs), sensor fusion and Artificial Neural Networks (ANNs). The objective is to make an accurate estimate of TCP navigation, using the signals from an IMU as resources of a neural network capable of predicting the position. Considering that the IMU sensors suffer noise in the measurements and the noise progresses over time, this proposal employs a technique that eliminates the filtering step, and the process is done internally by the network. The work employs a non-parametric approach to reset the reference dynamically, minimize noise from sensors, and converge positioning to a nominal result. This method offers a solution for fast, cheap, and efficient robot calibration. The work does not want to replace current techniques but to introduce a new design to the literature. The concept does not require sophisticated mechanical parts and the production line to be idle during the calibration process, and the results show that the developed technique can accurately predict the TCP position with millimeter errors and in real-time. The study also implemented the concept with other neural networks, for which it used a smaller set of data in an attempt to reduce training time. The research used the Multilayer Perceptron and XGBRegressor networks to test the approach introduced with others algorithms. Different applications that need real-time positioning can benefit from the proposal.",https://ieeexplore.ieee.org/document/9312193/,IEEE Access,2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/LRA.2022.3146900,Open Simulation Environment for Learning and Practice of Robot-Assisted Surgical Suturing,IEEE,Journals,"Automation has the potential to improve the standard of care but is difficult to realize due to perceptual challenges, especially in soft-tissue surgery. Machine learning can provide solutions, but typically requires large amounts of training data, which is time-consuming to collect. Even with shared platforms, hardware differences can prevent effective sharing of data between institutions. This letter proposes a standardized simulation platform for training and testing algorithms to control surgical robotic systems, which is built upon an open-source simulator, the Asynchronous Multi-Body Framework (AMBF), to enable quick prototyping of different scenes. An illustrative example of a suturing task on a phantom is presented and has formed the basis of a challenge, released to the community. The top-level contribution is the open-source release of a dynamic simulation environment that enables realistic suturing on a phantom, but supporting contributions include its extendable architectural design and a series of algorithmic optimizations to achieve real-time control and collision detection, realistic behavior of the needle and suture, and generation of multi-modal ground-truth data, including labeled depth data. These capabilities enable simulation-based surgical training and support research in machine learning for surgical scene perception and autonomous action.",https://ieeexplore.ieee.org/document/9697399/,IEEE Robotics and Automation Letters,April 2022,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/LRA.2022.3188904,PD-FAC: Probability Density Factorized Multi-Agent Distributional Reinforcement Learning for Multi-Robot Reliable Search,IEEE,Journals,"This letter presents a new range of multi-robot search for a non-adversarial moving target problems, namely multi-robot reliable search (MuRRS). The term &#x2018;reliability&#x2019; in MuRRS is defined as the expectation of a predefined utility function over the probability density function (PDF) of the target&#x2019;s capture time. We argue that MuRRS subsumes the canonical multi-robot efficient search (MuRES) problem, which minimizes the target&#x2019;s expected capture time, as its special case, and offers the end user with a wide range of objective selection options. Since state-of-the-art algorithms are usually targeting the MuRES problem, and cannot offer up-to-standard performance to the various MuRRS objectives, we, thereby, propose a probability density factorized multi-agent distributional reinforcement learning method, namely PD-FAC, as a unified solution to the MuRRS problem. PD-FAC decomposes the PDF of the multi-robot system&#x2019;s overall value distribution into a set of individual value distributions and guarantees that any reliability objective defined as a function of the overall system&#x2019;s value distribution can be linearly approximated by the same reliability metric defined over the agent&#x2019;s individual value distribution. In this way, the individual global maximum (IGM) principle is satisfied for all the pre-defined reliability metrics. It means that when each reinforcement learning agent is executing the individual policy, which maximizes its own reliability metric, the system&#x2019;s overall reliability performance is also maximized. We evaluate and compare the performance of PD-FAC with state of the arts in a range of canonical multi-robot search environments with satisfying results, and also deploy PD-FAC to a real multi-robot system for non-adversarial moving target search.",https://ieeexplore.ieee.org/document/9816134/,IEEE Robotics and Automation Letters,Oct. 2022,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/TCYB.2013.2275291,Real-Time Multiple Human Perception With Color-Depth Cameras on a Mobile Robot,IEEE,Journals,"The ability to perceive humans is an essential requirement for safe and efficient human-robot interaction. In real-world applications, the need for a robot to interact in real time with multiple humans in a dynamic, 3-D environment presents a significant challenge. The recent availability of commercial color-depth cameras allow for the creation of a system that makes use of the depth dimension, thus enabling a robot to observe its environment and perceive in the 3-D space. Here we present a system for 3-D multiple human perception in real time from a moving robot equipped with a color-depth camera and a consumer-grade computer. Our approach reduces computation time to achieve real-time performance through a unique combination of new ideas and established techniques. We remove the ground and ceiling planes from the 3-D point cloud input to separate candidate point clusters. We introduce the novel information concept, depth of interest, which we use to identify candidates for detection, and that avoids the computationally expensive scanning-window methods of other approaches. We utilize a cascade of detectors to distinguish humans from objects, in which we make intelligent reuse of intermediary features in successive detectors to improve computation. Because of the high computational cost of some methods, we represent our candidate tracking algorithm with a decision directed acyclic graph, which allows us to use the most computationally intense techniques only where necessary. We detail the successful implementation of our novel approach on a mobile robot and examine its performance in scenarios with real-world challenges, including occlusion, robot motion, nonupright humans, humans leaving and reentering the field of view (i.e., the reidentification challenge), human-object and human-human interaction. We conclude with the observation that the incorporation of the depth information, together with the use of modern techniques in new ways, we are able to create an accurate system for real-time 3-D perception of humans by a mobile robot.",https://ieeexplore.ieee.org/document/6583249/,IEEE Transactions on Cybernetics,Oct. 2013,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/LRA.2021.3102318,Real-Time Obstacle Avoidance Using Dual-Type Proximity Sensor for Safe Human-Robot Interaction,IEEE,Journals,"This letter introduces a dual-type proximity sensor and a control strategy for a robot manipulator to realize safe human-robot interactions (HRI) by using the sensor. Safety is an essential condition for HRI in practical scenarios. To achieve this condition, information about the relationship between an external objects and the robot is required. To obtain this information, we employ a dual-type proximity sensor, which consists of capacitive and inductive transducers and can detect the distance between a robot and external objects. Further, we propose a real-time trajectory planning method to deal with obstacles by using admittance control and distance measurements. To update the motion of the manipulator according to our control strategy, a Weight-Prioritized solution based on a QP (quadratic programming) formalism was applied. Further, the problem of self-sensing is solved via machine learning using a training dataset consisting of data corresponding to random joint positions. The proposed method was implemented on a collaborate robot (UR10). Experiments were conducted considering realistic human-robot interactions, and safety improvement was validated.",https://ieeexplore.ieee.org/document/9508896/,IEEE Robotics and Automation Letters,Oct. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/70.265922,Real-time vision-based robot localization,IEEE,Journals,"This paper describes an algorithm for determining robot location from visual landmarks. This algorithm determines both the correspondence between observed landmarks (in this case vertical edges in the environment) and a stored map, and computes the location of the robot using those correspondences. The primary advantages of this algorithm are its use of a single geometric tolerance to describe observation error, its ability to recognize ambiguous sets of correspondences, its ability to compute bounds on the error in localization, and fast execution. The algorithm has been implemented and tested on a mobile robot system. In several hundred trials it has never failed, and computes location accurate to within a centimeter in less than 0.5 s.<>",https://ieeexplore.ieee.org/document/265922/,IEEE Transactions on Robotics and Automation,Dec. 1993,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ACCESS.2020.3003376,Reconfiguration During Locomotion by Pavement Sweeping Robot With Feedback Control From Vision System,IEEE,Journals,"Routine cleaning the pavement is an essential requirement to maintain a sustainable environment for social life. The different width and type of pavements raise the challenges for autonomous vehicles with fixed shape to operate effectively. In this paper, we introduce the vision based reconfiguration of self-reconfigurable pavement sweeping robot called Panthera, which can adjust its frame width to ease the cleaning tasks to become friendly with different pavement geometry. The expansion and compression operations of the Panthera width are implemented by rotating one high torque motor connecting with the lead screw rod to change the opening angle of linkage hinges. The Panthera cleaning and locomotion operations are synchronized with changing the robot width according to the output of detected pavement width. To this end, the segmented pavement leveraged on the masked based deep convolutional neural network (DCNN) is used as input for the proposed closed-loop feedback control method, enabling the robot to adjust the requirement of changing the width during locomotion accurately. The proposed PID scheme takes into account the robot kinematic design with the flexibility of width changing modes. The experiments were carried out in real environments demonstrated the autonomous reconfiguration robot width with various locomotion scenarios on pavements of varying width.",https://ieeexplore.ieee.org/document/9120028/,IEEE Access,2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ACCESS.2020.3018026,Reinforcement Learning for Position Control Problem of a Mobile Robot,IEEE,Journals,"Due to the increase in complexity in autonomous vehicles, most of the existing control systems are proving to be inadequate. Reinforcement Learning is gaining traction as it is posed to overcome these difficulties in a natural way. This approach allows an agent that interacts with the environment to get rewards for appropriate actions, learning to improve its performance continuously. The article describes the design and development of an algorithm to control the position of a wheeled mobile robot using Reinforcement Learning. One main advantage of this approach concerning traditional control algorithms is that the learning process is carried out automatically with a recursive procedure forward in time. Moreover, given the fidelity of the model for the particular implementation described in this work, the whole learning process can be carried out in simulation. This fact avoids damages to the actual robot during the learning stage. It shows that the position control of the robot (or similar specific tasks) can be done without the need to know the dynamic model of the system explicitly. Its main drawback is that the learning stage can take a long time to finish and that it depends on the complexity of the task and the availability of adequate hardware resources. This work provides a comparison between the proposed approach and traditional existing control laws in simulation and real environments. The article also discusses the main effects of using different controlled variables in the performance of the developed control law.",https://ieeexplore.ieee.org/document/9171241/,IEEE Access,2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ACCESS.2022.3181989,TVENet: Transformer-Based Visual Exploration Network for Mobile Robot in Unseen Environment,IEEE,Journals,"This paper presents a Transformer-based Visual Exploration Network (TVENet) that capably serves as a solution for active perception problems, especially the visual exploration problem: How could a robot that is equipped with a camera explore an unknown 3D environment? The TVENet consists of a Mapper, a Global Policy and a Local Policy. The mapper is trained by supervised learning to take the visual observation as input and generate an occupancy grid map for the explored environment. The Global Policy and the Local Policy are trained by reinforcement learning in order to make navigation decision. Most state-of-the-art methods in visual exploration domain use ResNet as feature extractor, and few of them pay attention to the extraction capability of the extractor. Therefore, this paper focuses on enhancing the extraction capability, and proposes a Transformer-based Feature Pyramid Module (TFPM). Moreover, two tricks for training process are introduced to improve the performance (M.F. and Aux.) Our experiments in photo-realistic simulated environment (Habitat) demonstrate the higher-accuracy mapping of TVENet. Experimental results prove that the TFPM and tricks have positive impacts on the mapping accuracy of the visual exploration and increase it by 5.31&#x0025; compared with the state-of-the-art. Most importantly, the TVENet is deployed on a real robot (NVIDIA Jetbot) to prove the feasibility of Embodied AI approaches. To the authors&#x2019; knowledge, this paper is the first one that proves the viability of the Embodied AI style approach for visual exploration tasks and deploys the pre-trained model on the NVIDIA Jetson robot.",https://ieeexplore.ieee.org/document/9795177/,IEEE Access,2022,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/LRA.2022.3195195,Tactile Gym 2.0: Sim-to-Real Deep Reinforcement Learning for Comparing Low-Cost High-Resolution Robot Touch,IEEE,Journals,"High-resolution optical tactile sensors are increasingly used in robotic learning environments due to their ability to capture large amounts of data directly relating to agent-environment interaction. However, there is a high barrier of entry to research in this area due to the high cost of tactile robot platforms, specialised simulation software, and sim-to-real methods that lack generality across different sensors. In this letter we extend the Tactile Gym simulator to include three new optical tactile sensors (TacTip, DIGIT and DigiTac) of the two most popular types, Gelsight-style (image-shading based) and TacTip-style (marker based). We demonstrate that a single sim-to-real approach can be used with these three different sensors to achieve strong real-world performance despite the significant differences between real tactile images. Additionally, we lower the barrier of entry to the proposed tasks by adapting them to an inexpensive 4-DoF robot arm, further enabling the dissemination of this benchmark. We validate the extended environment on three physically-interactive tasks requiring a sense of touch: object pushing, edge following and surface following. The results of our experimental validation highlight some differences between these sensors, which may help future researchers select and customize the physical characteristics of tactile sensors for different manipulations scenarios.",https://ieeexplore.ieee.org/document/9847020/,IEEE Robotics and Automation Letters,Oct. 2022,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ACCESS.2021.3080517,Towards Open and Expandable Cognitive AI Architectures for Large-Scale Multi-Agent Human-Robot Collaborative Learning,IEEE,Journals,"Learning from Demonstration (LfD) constitutes one of the most robust methodologies for constructing efficient cognitive robotic systems. Despite the large body of research works already reported, current key technological challenges include those of multi-agent learning and long-term autonomy. Towards this direction, a novel cognitive architecture for multi-agent LfD robotic learning is introduced in this paper, targeting to enable the reliable deployment of open, scalable and expandable robotic systems in large-scale and complex environments. In particular, the designed architecture capitalizes on the recent advances in the Artificial Intelligence (AI) (and especially the Deep Learning (DL)) field, by establishing a Federated Learning (FL)-based framework for incarnating a multi-human multi-robot collaborative learning environment. The fundamental conceptualization relies on employing multiple AI-empowered cognitive processes (implementing various robotic tasks) that operate at the edge nodes of a network of robotic platforms, while global AI models (underpinning the aforementioned robotic tasks) are collectively created and shared among the network, by elegantly combining information from a large number of human-robot interaction instances. Regarding pivotal novelties, the designed cognitive architecture a) introduces a new FL-based formalism that extends the conventional LfD learning paradigm to support large-scale multi-agent operational settings, b) elaborates previous FL-based self-learning robotic schemes so as to incorporate the human in the learning loop and c) consolidates the fundamental principles of FL with additional sophisticated AI-enabled learning methodologies for modelling the multi-level inter-dependencies among the robotic tasks. The applicability of the proposed framework is explained using an example of a real-world industrial case study (subject to ongoing research activities) for agile production-based Critical Raw Materials (CRM) recovery.",https://ieeexplore.ieee.org/document/9431107/,IEEE Access,2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/LRA.2019.2961598,When Your Robot Breaks: Active Learning During Plant Failure,IEEE,Journals,"Detecting and adapting to catastrophic failures in robotic systems requires a robot to learn its new dynamics quickly and safely to best accomplish its goals. To address this challenging problem, we propose probabilistically-safe, online learning techniques to infer the altered dynamics of a robot at the moment a failure (e.g., physical damage) occurs. We combine model predictive control and active learning within a chance-constrained optimization framework to safely and efficiently learn the new plant model of the robot. We leverage a neural network for function approximation in learning the latent dynamics of the robot under failure conditions. Our framework generalizes to various damage conditions while being computationally light-weight to advance real-time deployment. We empirically validate within a virtual environment that we can regain control of a severely damaged aircraft in seconds and require only 0.1 seconds to find safe, information-rich trajectories, outperforming state-of-the-art approaches.",https://ieeexplore.ieee.org/document/8938725/,IEEE Robotics and Automation Letters,April 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISSCC.2019.8662455,2.5 A 40×40 Four-Neighbor Time-Based In-Memory Computing Graph ASIC Chip Featuring Wavefront Expansion and 2D Gradient Control,IEEE,Conferences,"Single-source shortest path (SSP) problems have a rich history of algorithm development [1-3]. SSP has many applications including AI decision making, robot navigation, VLSI signal routing, autonomous vehicles and many other classes of problems that can be mapped onto graphs. Conventional algorithms rely on sequentially traversing the search space, which is inherently limited by traditional computer architecture. In graphs which become very large, this slow processing time can become a bottleneck in real world applications. We propose a time-based ASIC to address this issue. Our design leverages a dedicated hardware implementation to solve these problems in linear time complexity with superior energy efficiency. A $40\times40$ four-neighbor grid implements a wavefront (WF) expansion with a first-in lockout mechanism to enable traceback. Outside the array, a programmable resistive ladder provides bias voltages to the edge cells, which enables pulse shaping reminiscent of the A* algorithm [3].",https://ieeexplore.ieee.org/document/8662455/,2019 IEEE International Solid- State Circuits Conference - (ISSCC),17-21 Feb. 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICTAI.2015.60,A 3D Frontier-Based Exploration Tool for MAVs,IEEE,Conferences,"This paper presents a 3D frontier-based exploration tool named 3D-FBET. Our tool runs onboard the MAV equipped with a 3D sensor. The 3D map of the environment explored is constructed incrementally from two consecutive point clouds obtained. Considering the computation and memory limitations of MAVs, the OctoMap is utilized to represent 3D models. A novel approach is designed to extract the 3D frontiers from the OctoMap. Different from existing extraction method, only state-changed space in the 3D map is processed in each iteration. We implement our approach on top of the well-known robot operating system (ROS) and demonstrate the effectiveness of our tool in real scenarios.",https://ieeexplore.ieee.org/document/7372156/,2015 IEEE 27th International Conference on Tools with Artificial Intelligence (ICTAI),9-11 Nov. 2015,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CIG.2011.6032006,A Bayesian model for RTS units control applied to StarCraft,IEEE,Conferences,"In real-time strategy games (RTS), the player must reason about high-level strategy and planning while having effective tactics and even individual units micro-management. Enabling an artificial agent to deal with such a task entails breaking down the complexity of this environment. For that, we propose to control units locally in the Bayesian sensory motor robot fashion, with higher level orders integrated as perceptions. As complete inference encompassing global strategy down to individual unit needs is intractable, we embrace incompleteness through a hierarchical model able to deal with uncertainty. We developed and applied our approach on a StarCraft1 AI.",https://ieeexplore.ieee.org/document/6032006/,2011 IEEE Conference on Computational Intelligence and Games (CIG'11),31 Aug.-3 Sept. 2011,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS40897.2019.8968233,A Behavior Tree Cognitive Assistant System for Emergency Medical Services,IEEE,Conferences,"This paper presents a cognitive assistant system for emergency medical services (EMS) that can serve as a rescue robot or virtual assistant, helping with improving situational awareness of the first responders through automated collection and analysis of data from the incident scene and providing suggestions to them. The proposed system relies on a Behavior Tree (BT) framework that combines the knowledge of EMS protocol guidelines with speech recognition, natural language processing, and machine learning methods to (i) extract critical information from responders' conversations and verbalized observations, (ii) infer the incident context, and (iii) decide on safe and effective response interventions to perform. We use a data-set of 8302 real EMS call records from an urban, high volume regional ambulance agency in the U.S. to evaluate the responsiveness and cognitive ability of the system and assess the safety of the suggestions provided to the responders. The experimental results show that the developed cognitive assistant achieves an average top-3 accuracy of 89% in selecting the correct EMS protocols and an average F1-score of 71% in suggesting the protocol specific interventions while providing transparency and evidence for the suggestions.",https://ieeexplore.ieee.org/document/8968233/,2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),3-8 Nov. 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SICE.2008.4655167,A CMAC-Q-Learning based Dyna agent,IEEE,Conferences,"In the this paper, a CMAC-Q-Learning based Dyna agent is presented to relieve the problem of learning speed in reinforcement learning, in order to achieve the goals of shortening training process and increasing the learning, speed. We combine CMAC, Q-learning, and Prioritized sweeping techniques to construct the Dyna agent in which a Q-learning is trained for policy learning; meanwhile, model approximators, called CMAC-Model and CMAC-R-Model, are in charge of approximating the environment model. The approximated model provides the Q-learning with virtual interaction experience to further update the policy within the time gap when there is no interplay between the agent and the real environment. The Dyna agent switches seamlessly between the real environment and the virtual environment model for the objective of policy learning. A simulation for controlling a differential-drive mobile robot has been conducted to demonstrate that the proposed method can preliminarily achieve the design goal.",https://ieeexplore.ieee.org/document/4655167/,2008 SICE Annual Conference,20-22 Aug. 2008,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICHR.2006.321300,A Cognitive Architecture for Flexible Imitative Interaction Using Tools and Objects,IEEE,Conferences,"Imitative responses constitute an important basis of rich interaction between an infant and a care-giver. This paper proposes a cognitive robotic architecture for generating a variety of flexible imitative responses in situations where a robot interacts with humans using various toy objects. The architecture is based on a cognitive scientific hypothesis that ""true imitation"" capability consists of a set of sub-functions called ""pseudo-imitation"". Examples of pseudo-imitation are: (1) assimilation of bodily movements, (2) evoking stereotyped actions associated with a viewed object (tool). We propose that the synergy of these pseudo-imitations leads to higher level imitation. The above hypothesis is realized by implementing the individual pseudo-imitation components and the synergy function as parallel real-time modules. The robot flexibly exhibits various pseudo-imitation as well as their synergy, called ""action unit imitation"", by the dynamic collective workings of components and the function",https://ieeexplore.ieee.org/document/4115630/,2006 6th IEEE-RAS International Conference on Humanoid Robots,4-6 Dec. 2006,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICUS50048.2020.9275007,A Collaborative Decision Making Approach for Multi-Unmanned Combat Vehicles based on the Behaviour Tree,IEEE,Conferences,"Aiming at the difficulty of effective behaviour decision-making in multi-unmanned combat vehicle systems under complex environments and multi-task conditions, based on analyzing existing unmanned vehicle behaviour decision systems, this paper proposes a multi-unmanned combat vehicle cooperative behaviour decision method based on the behaviour tree. In this paper, we describe the behaviour tree modeling method for a multi-unmanned combat vehicle collaborative behavioural decision system, analyze the general process of behavioural tree modeling, and demonstrate the effectiveness of the method by implementing a multi-unmanned combat vehicle collaborative behavioural decision based on the Robomaster AI robot.",https://ieeexplore.ieee.org/document/9275007/,2020 3rd International Conference on Unmanned Systems (ICUS),27-28 Nov. 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AIRC56195.2022.9836455,A Gender Recognition System Based on Facial Image,IEEE,Conferences,"A gender recognition system (GRS) based on facial images can be embedded in different areas such as surveillance, human-robot interaction, targeted advertising, etc. Traditional facial feature-based recognition systems extract and analyze textures on a face. Although such approaches perform well under certain controlled situations, they may fail due to variations of faces in images, which is very common in real-life images. To overcome such problems, we need to have an effective combination of a feature descriptor, representation, and classifier providing better accuracy. Recently, many recognition problems are tackled by using the Deep Neural Network (DNN) such as Convolutional Neural Network (CNN). However, deep learning needs a large number of images, which is not usually available, to work as expected. We propose a model that combines handcrafted features with CNN to overcome the shortcomings including handling of variations in imaging, such as the illumination and pose variations, and the necessity of voluminous training sets. Experimental results also show that the proposed method performs better than the available gender recognition approaches.",https://ieeexplore.ieee.org/document/9836455/,"2022 3rd International Conference on Artificial Intelligence, Robotics and Control (AIRC)",10-12 May 2022,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS51168.2021.9636757,A Hierarchical Framework for Quadruped Locomotion Based on Reinforcement Learning,IEEE,Conferences,"Quadruped locomotion is a challenging task for learning-based algorithms. It requires tedious manual tuning and is difficult to deploy in reality due to the reality gap. In this paper, we propose a quadruped robot learning system for agile locomotion which does not require any pre-training and works well in various real-world terrains. We introduce a hierarchical learning framework that uses reinforcement learning as the high-level policy to adjust the low-level trajectory generator for better adaptability to the terrain. We compact the observation and action space of the reinforcement learning to deploy it on a host computer in reality. Besides, we design a trajectory generator guided by robot posture, which can generate adaptive foot trajectory to interact with the environment. Experimental results show that our system can be easily deployed in reality while only trained in simulation, and also has the advantages of fast convergence and good terrain adaptability. The supplementary video demonstration is available at https://vsislab.github.io/hfql/.",https://ieeexplore.ieee.org/document/9636757/,2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),27 Sept.-1 Oct. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.23919/DATE54114.2022.9774609,A Middleware Journey from Microcontrollers to Microprocessors,IEEE,Conferences,"This paper discusses some of the challenges we encountered when developing Apex.OS, an automotive grade version of the Robot Operating System (ROS)2. To better understand these challenges, we look back at the best practices used for data communication and software execution in OSEK-based systems. Finally we describe the extensions made in ROS2, Apex.OS and Apex.Middleware to meet the real-time constraints of the targeted automotive systems.",https://ieeexplore.ieee.org/document/9774609/,"2022 Design, Automation & Test in Europe Conference & Exhibition (DATE)",14-23 March 2022,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA40945.2020.9196677,A Mobile Manipulation System for One-Shot Teaching of Complex Tasks in Homes,IEEE,Conferences,"We describe a mobile manipulation hardware and software system capable of autonomously performing complex human-level tasks in real homes, after being taught the task with a single demonstration from a person in virtual reality. This is enabled by a highly capable mobile manipulation robot, whole-body task space hybrid position/force control, teaching of parameterized primitives linked to a robust learned dense visual embeddings representation of the scene, and a task graph of the taught behaviors. We demonstrate the robustness of the approach by presenting results for performing a variety of tasks, under different environmental conditions, in multiple real homes. Our approach achieves 85% overall success rate on three tasks that consist of an average of 45 behaviors each. The video is available at: https://youtu.be/HSyAGMGikLk.",https://ieeexplore.ieee.org/document/9196677/,2020 IEEE International Conference on Robotics and Automation (ICRA),31 May-31 Aug. 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IJCNN.2006.246870,A Mobile Vision System with Reconfigurable Intelligent Agents,IEEE,Conferences,"Performing face detection and tracking on a mobile robot in a dynamic environment is a challenging task with the real-time constraints. To realize a natural reactive behavior of the mobile robot, some efficient algorithms are applied for face detection and tracking due to the limited onboard computation power. Instead of pushing the limit of software development and the computational resources by implementing these algorithms on a software platform, a reconfigurable System-on-Chip (SoC) platform is adopted to achieve the higher real-time performance. A BDI intelligent agent model is proposed as a unified structure for both hardware and software. The real-world experimental results on a mobile vision system for face detection and tracking demonstrate the feasibility and efficiency of the reconfigurable intelligent agent model.",https://ieeexplore.ieee.org/document/1716281/,The 2006 IEEE International Joint Conference on Neural Network Proceedings,16-21 July 2006,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SNPD.2012.115,A Model-Based Approach to Constructing Safe Soft Real-Time Programs for Non-Real-Time Environments,IEEE,Conferences,"The primary goal of this work is to provide an easy and systematic way of developing safe soft real-time systems. To achieve this goal, we propose a method of generating real-time programs from formally verified models written as systems of timed automata. The models are verified using UPPAAL model checker prior to be processed by our code generators. A characteristic of our code generator is that the generated code runs in a non-real-time environment, i.e., a runtime environment without inherent real-time schedulers. To realize this, the code generator weaves timing checking code fragments within the generated programs. The generated code explicitly checks the real-time clock of its runtime to obey the timing constraints specified in the model. In this paper, we describe how to generate Java/C programs from UPPAAL timed automata and show the benefits of our method using a robot controller case study.",https://ieeexplore.ieee.org/document/6299291/,"2012 13th ACIS International Conference on Software Engineering, Artificial Intelligence, Networking and Parallel/Distributed Computing",8-10 Aug. 2012,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/NAECON.2018.8556769,A Rapid Situational Awareness Development Framework for Heterogeneous Manned-Unmanned Teams,IEEE,Conferences,"This paper presents a robust framework for configuring and deploying a heterogeneous team of smart unmanned systems and human agents in dynamic and un-modeled environments to rapidly build mission critical situational awareness with selective details of potential areas of interest, especially focusing on minimized cognitive loading of the human agents. Five key components, namely control, communication, artificial intelligence (AI), platform, and visualization, merge seamlessly into a holistic framework to deliver this rapid situational awareness development capability to the heterogeneous manned unmanned team (MUM-T). In this framework, the overall control is seen as a combination of agent level control and mission level control. A common software, Robot Operating System (ROS), is used to establish communication, and consequently consensus, among the heterogeneous swarm of unmanned systems. These unmanned platforms are customized with co-processing hardware that can execute advanced artificial intelligence machine learning (AI/ML) modules to not only deliver stable and cooperative performance of these unmanned platforms in the swarm but also support human-centric human robot interaction (HRI). Finally, to reduce the cognitive burden on the human agents, a triaged visualization scheme, enabled through mixed reality (MR) technology, is implemented. This paper presents a preliminary proof of concept study for the presented hybrid map (i.e. 2D mapping with 3D detailing) construction framework, tested with a heterogeneous swarm of unmanned aerial vehicles (UAVs) of varying capabilities, teamed with a human operator.",https://ieeexplore.ieee.org/document/8556769/,NAECON 2018 - IEEE National Aerospace and Electronics Conference,23-26 July 2018,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ECAI.2017.8166446,A control approach for a robotic ground walking platform,IEEE,Conferences,"This study presents a control approach for a Ground Walking Platform (GWP). The control approach was developed to be a part of a future lower limb rehabilitation robot that aims to simulate the walking on different terrains (e.g. walking, on plane ground, on stairs, or on a hill). In addition the system aims to simulate for the user not only classic hard ground walking trajectories, but also to simulate different values of stiffens of as well. This may involve walking on solid ground, muddy land, sand and water too. The objective of this paper is to present a dynamic control study of a GWP system allowing us to predict the relationship between the walker's foot and a virtual environment. More specifically, his involves defining the equations describing the relation between the contact forces (foot/platform) and the position of the foot.",https://ieeexplore.ieee.org/document/8166446/,"2017 9th International Conference on Electronics, Computers and Artificial Intelligence (ECAI)",29 June-1 July 2017,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISIC.2003.1253933,A dynamically sized Radial Basis Function neural network for joint control of a PUMA 500 manipulator,IEEE,Conferences,"We present the design and analysis of a neural control structure for joint control of a PUMA 500 robot manipulator. We lay out the design considerations and steps to build an experimental electronic control system to control the shoulder joint of the manipulator. We review the use of neural networks for on-line learning closed-loop control applications. The 'curse of dimensionality', a problem encountered when using Radial Basis Function (RBF) neural networks, is addressed and a neuron-node resource-allocating algorithm is investigated to overcome this problem. An on-line learning neural-control structure, employing this resource-allocating algorithm, is proposed, implemented and successfully tested to improve the position accuracy of the robot manipulator. All the implementations are executed on a 16-bit microcontroller in real-time, developed using integer arithmetic in the programming language C. The program listings are available upon email request.",https://ieeexplore.ieee.org/document/1253933/,Proceedings of the 2003 IEEE International Symposium on Intelligent Control,8-8 Oct. 2003,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/INDIN.2013.6622897,A genetic algorithm for optimizing vector-based paths of industrial manipulators,IEEE,Conferences,"Nowadays there is a vast amount of IT tools specialized in vector graphics. The data generated by those tools could be used to describe the path of industrial manipulators as a set of vectors. The main problem is that the sequence/direction of those vectors is not meant to be executed by a robot and attempting to do it, would result in inefficient cycle times of the robot. Therefore it is necessary to generate an execution plan that minimizes the cost of carrying out the vector-based path. The number of possible execution actions has a factorial growth and it is unfeasible to evaluate each of them. This paper proposes the use of a genetic algorithm to optimize this task. The main contribution of this work is a chromosome encoding structure and modifications to the Partially Mapped Crossover operator in order to comply with the constraints of this optimization problem. The algorithm was implemented and tested in a real industrial manipulator.",https://ieeexplore.ieee.org/document/6622897/,2013 11th IEEE International Conference on Industrial Informatics (INDIN),29-31 July 2013,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ECBS.2003.1194801,A hybrid architecture for visualization and decision making in battlespace environments,IEEE,Conferences,"This article presents a hybrid software/hardware architecture for commander's decision support in tactical operations. The architecture builds on the symbolic, object-oriented visualization software called Advanced Tactical Architecture for Combat Knowledge System (ATACKS). The extension discussed here is the design of a real-time robot agent layer that interacts wirelessly with ATACKS. This layer enacts decisions made by software agents (wargamers), continuously relays the execution states back to ATACKS, and updates its actions as advocated by replanning algorithms. The software layer is briefly described followed by the specification of the real-time requirements for the robotic architecture. The design and implementation are given with a small example that illustrates the hybrid system's operation.",https://ieeexplore.ieee.org/document/1194801/,"10th IEEE International Conference and Workshop on the Engineering of Computer-Based Systems, 2003. Proceedings.",7-10 April 2003,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/FUZZY.1992.258618,A learning method of fuzzy inference rules by descent method,IEEE,Conferences,"The authors propose a learning method for fuzzy inference rules by a descent method. From input-output data gathered from specialists, the inference rules expressing the input-output relation of the data are obtained automatically. The membership functions in the antecedent part and the real number in the consequent part of the inference rules are tuned by means of the descent method. The learning speed and the generalization capability of this method are higher than those of a conventional backpropagation type neural network. This method has the capability to express the knowledge acquired from input-output data in the form of fuzzy inference rules. Some numerical examples are described to show these advantages over the conventional neural network. An application of the method to a mobile robot that avoids a moving obstacle and its computer simulation are reported.<>",https://ieeexplore.ieee.org/document/258618/,[1992 Proceedings] IEEE International Conference on Fuzzy Systems,8-12 March 1992,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ROBIO54168.2021.9739440,A learning-based control approach for blind quadrupedal locomotion with guided-DRL and hierarchical-DRL,IEEE,Conferences,"Control parameters play an important role on the locomotion performance of quadruped robot system. In this paper, a learning-based control method is proposed, where the parameters of controller are learned by deep reinforcement learning (DRL). The proposed control system consists of a hierarchical controller and an agent, in which the agent learns the parameters in the upper layer of the controller. In the learning process, the guided-DRL and the hierarchical-DRL were used to solve the exploration problem and reward sparse problem, respectively. The former learns good expert trajectories with controller, which can realize the transition from supervised learning to self-learning, whereas the latter divides the task into a series of sub-goals to ensure that the difficulty of each subtask matches the decision-making ability of the agent in the learning stage. Finally, the controller with the trained policy is deployed to a real robot without manual tuning, and experimental results prove the effectiveness of the proposed method.",https://ieeexplore.ieee.org/document/9739440/,2021 IEEE International Conference on Robotics and Biomimetics (ROBIO),27-31 Dec. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS.1996.571054,A method for expecting the features of objects and enabling real-time vision processing,IEEE,Conferences,"This paper presents a mathematical analysis of image processing, algorithms designed according to the results of this analysis, and their implementation. We prove that the search of objects features can be accelerated without loss of precision by using an inhomogeneous density of the sensitive cells the parameters space is composed of. In other words, the visual analysis should be concentrated in the region of the features space around the expected object position. The improvement relative to an uniform cell density is quantified using a cost function corresponding to time and precision optimisation. We show that a Kohonen neural network can be used for efficient image processing, and simulate this strategy. We introduce a simpler algorithm for the case that the object positions are Gauss-distributed around the expected position. This algorithm has been implemented it on a robot guided by a vision system. The robot learned to process images efficiently during the manoeuvres and after that was able to track objects moving in a fast and unpredictable manner.",https://ieeexplore.ieee.org/document/571054/,Proceedings of IEEE/RSJ International Conference on Intelligent Robots and Systems. IROS '96,8-8 Nov. 1996,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/RAMECH.2004.1438939,A model-based humanoid perception system for real-time human motion imitation,IEEE,Conferences,"This paper presents real-time human motion analysis based on hierarchical tracking and inverse kinematics. Our goal is to implement a mechanism of human-machine interaction that permits a robot to learn from human gestures, and, as a first stage, we have developed a computer-vision based human upperbody motion analysis system. This application requires developing a real-time human motion capturing system that works without special devices or markers. Since such a system is unstable and can only acquire, partial information because of self-occlusions, we have introduced a pose estimation method based on inverse kinematics. This system can estimate upper-body human postures with limited perceptual cues such as position of head and hands. The method has been tested using a HOAP-I humanoid robot.",https://ieeexplore.ieee.org/document/1438939/,"IEEE Conference on Robotics, Automation and Mechatronics, 2004.",1-3 Dec. 2004,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.23919/ECC.2001.7076440,A neural network implementation of real-time fuzzy predictive control,IEEE,Conferences,"Fuzzy predictive controllers have been applied to several applications with good control performance. However, this methodology often leads to nonconvex optimization problems, which are difficult to solve for fast processes, i.e. processes with small sampling times. This paper proposes a new methodology to apply a fuzzy predictive controller in real-time by using a neural network architecture, which receives data from the process and computes the control actions. Thus, the neural network is learned off-line, and its final structure guarantees that control actions are computed very rapidly. An internal model control structure is used to cope with model-plant mismatches and disturbances. The proposed methodology is tested in a realistic simulation of an experimental robot manipulator, where force and position are both controlled. The proposed scheme reveals very good control performance.",https://ieeexplore.ieee.org/document/7076440/,2001 European Control Conference (ECC),4-7 Sept. 2001,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRoM.2015.7367862,A new adaptive neural network based observer for robotic manipulators,IEEE,Conferences,"In this paper, a new neural network based observer is proposed for a class of nonlinear systems. The proposed observer can applied to estimate nonlinear systems with a high nonlinearity without any prior knowledge about system. This features help the proposed neuro-observer for real implementation and to use it in practice. The Lyapunov's direct method employed to show the stability and estimating performance of the proposed scheme. Simulation results on a two DOF robot manipulator are presented to show the efficiency of the proposed neural network based observer.",https://ieeexplore.ieee.org/document/7367862/,2015 3rd RSI International Conference on Robotics and Mechatronics (ICROM),7-9 Oct. 2015,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IJCNN.2003.1223697,A performance comparison of TRACA - an incremental on-line learning algorithm,IEEE,Conferences,"TRACA (Temporal Reinforcement-learning and Classification Architecture) is a learning system intended for robot-navigation tasks. One problem in this area is input-generalisation. Input generalisation requires learning a small set of internal states which represent useful abstractions of the much larger set of actual states. As such, the input-generalisation problem is fundamentally similar to the classical problems of classification, concept learning and discrimination. The priorities when evaluating a system for on-line robot learning include a small number of trials, predictive accuracy and minimal parameter tuning. Other requirements are the ability to learn without predefined classes (i.e. classes must be learned during training) and an efficient and adaptable representation. This paper evaluates the performance of TRACA, a new learning algorithm, on a number of common classification tasks. The same set of parameters is used to obtain all TRACA's results, which are then compared to the results obtained by other well-known algorithms. On most tasks, TRACA's predictive accuracy is within a few percent of the best performing systems compared. Furthermore, TRACA's result is often achieved with less training experience. In a final experiment TRACA is trialled on a robot navigation task that requires discrimination of a number of discrete locations.",https://ieeexplore.ieee.org/document/1223697/,"Proceedings of the International Joint Conference on Neural Networks, 2003.",20-24 July 2003,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/EMWRTS.1994.336860,A software architecture for telerobotics,IEEE,Conferences,"Describes a currently developed software architecture for telerobotics, based on a high-level and interpreted language, PILOT (Programming and Interpreted Language Of actions for Telerobotics), allowing one to plan the actions of a robot. Such a language allows the operator to modify a mission during its execution. A software architecture is developed for this language, and is composed of four modules which manage different aspects of plan execution.<>",https://ieeexplore.ieee.org/document/336860/,Proceedings Sixth Euromicro Workshop on Real-Time Systems,15-17 June 1994,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISCAS.2008.4541600,AER-based robotic closed-loop control system,IEEE,Conferences,"Address-Event-Representation (AER) is an asynchronous protocol for transferring the information of spiking neuro-inspired systems. Actually AER systems are able to see, to ear, to process information, and to learn. Regarding to the actuation step, the AER has been used for implementing Central Pattern Generator algorithms, but not for controlling the actuators in a closed-loop spike-based way. In this paper we analyze an AER based model for a real-time neuro-inspired closed-loop control system. We demonstrate it into a differential control system for a two-wheel vehicle using feedback AER information. PFM modulation has been used to power the DC motors of the vehicle and translation into AER of encoder information is also presented for the close-loop. A codesign platform (called AER-Robot), based into a Xilinx Spartan 3 FPGA and an 8051 USB microcontroller, with power stages for four DC motors has been used for the demonstrator.",https://ieeexplore.ieee.org/document/4541600/,2008 IEEE International Symposium on Circuits and Systems (ISCAS),18-21 May 2008,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISCAS.2019.8702353,AR-C3D: Action Recognition Accelerator for Human-Computer Interaction on FPGA,IEEE,Conferences,"In recent years, action recognition has been widely explored and attains significant performance improvement. In this paper, we propose a real-time action recognition specified convolutional 3D (AR-C3D) neural network for human-computer interaction. The CNN structure is optimized to decrease the complexity. Furthermore, Winograd algorithm is adopted to accelerate computation. It achieves 89.9% accuracy in the application which refers to the robot classifies the video captured by itself and would either imitate human's action or give verbal feedback. The Artix-7 FPGA implementation result outperforms previous work in terms of resource utilization and no external storage is consumed. One video can be processed in 6.6ms, and the power consumption is only 2.7W.",https://ieeexplore.ieee.org/document/8702353/,2019 IEEE International Symposium on Circuits and Systems (ISCAS),26-29 May 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/WRCSARA53879.2021.9612673,ATFVO: An Attentive Tensor-compressed LSTM Model with Optical Flow Features for Monocular Visual Odometry,IEEE,Conferences,"This paper proposes a new framework called ATFVO which can be deployed on the edge device to resolve monocular visual odometry problem. The vast majority of visual odometry algorithms using deep learning are equivalent to or beyond the traditional visual odometry algorithms in performance, however they do not consider the computing capability of edge equipment. In this paper, convolution neural network (CNN) and attentive tensor-compressed compression LSTM (A-T-LSTM) are used, with optical flow feature as input and a 6-DoF absolute-scale pose as output. The framework is fused with the spatio-temporal feature and deal with the overfitting problem of over-parameterized LSTM with high-dimensional inputs, and utilizes attention mechanism to get poses from the sequence output of T-LSTM. The poses are estimated from the original RGB images sequence without depending on any prior knowledge. The experimental outcomes at the KITTI dataset display that, in compared with the performance of the most advanced methods, the single T-LSTM model is 141&#x00D7; smaller than the original LSTM model, and the entire model is nearly one-seventh of DeepVO with a speed 23&#x00D7; faster than Flowdometry. The proposed VO is deployed to the robot based on raspberry pi, which can achieve real-time inference and navigate a cruise.",https://ieeexplore.ieee.org/document/9612673/,2021 WRC Symposium on Advanced Robotics and Automation (WRC SARA),11-11 Sept. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICTAI.2019.00249,Accurate and Robust RGB-D Dense Mapping with Inertial Fusion and Deformation-Graph Optimization,IEEE,Conferences,"RGB-D dense mapping has become more and more popular, however, when encountering rapid movement or shake, the robustness and accuracy of most RGB-D dense mapping methods are degraded and the generated maps are overlapped or distorted, due to the drift of pose estimation. In this paper, we present a novel RGB-D dense mapping method, which can obtain accurate, robust and global consistency map even in the above complex conditions. Firstly, the improved ORBSLAM method, which tightly-couples RGB-D information and inertial information to estimate the current pose of robot, is firstly introduced for accurate pose estimation rather than traditional frame-to-frame method in most RGB-D dense mapping methods. Besides, the TSDF (Truncated Signed Distance Function) method is used to effectively fuse depth frame into a global model, and to keep the global consistency of the generated map. Furthermore, since the drift error is inevitable, a deformation graph is constructed to minimize the consistent error in global model, to further improve the mapping performance. The performance of the proposed RGB-D dense mapping method was validated by extensive localization and mapping experiments on public datasets and real scene datasets, and it showed strongly accuracy and robustness over other state-of-the-art methods. What's more, the proposed method can achieve real-time performance implemented on GPU.",https://ieeexplore.ieee.org/document/8995400/,2019 IEEE 31st International Conference on Tools with Artificial Intelligence (ICTAI),4-6 Nov. 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AIM.2018.8452226,Active Path Clearing Navigation through Environment Reconfiguration in Presence of Movable Obstacles,IEEE,Conferences,"Geometrical map based path planning is a well adopted approach for robot navigation tasks. Although straightforward, it can only deal with the environment changes in a reactive way (passive re-planning) and cannot handle planning failures. In this paper, we try to tackle this problem with a smarter approach - using local environment reconfiguration strategy to actively create clearances through manipulation of movable obstacles. Basically, we implement our previously developed deep Convolutional Neural Network (CNN) based perception module to update local environment knowledge and conduct online space reconfiguration planning for local path clearing. We develop a novel path clearing algorithm capable of dealing with ordered manipulations of multiple movable obstacles to provide locally optimal navigation solution. We illustrate the pipeline implementation of the overall system and verify the effectiveness with simulations as well as real-world scenarios, regarding different objects and manipulation actions.",https://ieeexplore.ieee.org/document/8452226/,2018 IEEE/ASME International Conference on Advanced Intelligent Mechatronics (AIM),9-12 July 2018,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISCIS.2009.5291811,Active humanoid vision and object classification,IEEE,Conferences,"In this paper we study object learning and recognition on a humanoid robot with foveated vision. The developed approach is view-based and can learn viewpoint-independent representations for object recognition. The training data is collected statistically and in an interactive way where a human instructor freely shows the object from a number of different viewpoints. The proposed system was fully implemented and runs in real-time, which is essential for meaningful interaction with a humanoid robot.",https://ieeexplore.ieee.org/document/5291811/,2009 24th International Symposium on Computer and Information Sciences,14-16 Sept. 2009,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ROBOT.2009.5152428,Active-learning assisted self-reconfigurable activity recognition in a dynamic environment,IEEE,Conferences,"It is desirable to know a resident's on-going activities before a robot or a smart system can provide attentive services to meet real human needs. This work addresses the problem of learning and recognizing human daily activities in a dynamic environment. Most currently available approaches learn offline activity models and recognize activities of interest on a real time basis. However, the activity models become outdated when human behaviors or device deployment have changed. It is a tedious and error-prone job to recollect data for retraining the activity models. In such a case, it is important to adapt the learnt activity models to the changes without much human supervision. In this work, we present a self-reconfigurable approach for activity recognition which reconfigures previously learnt activity models and infers multiple activities under a dynamic environment meanwhile pursuing minimal human efforts in relabeling training data by utilizing active-learning assistance.",https://ieeexplore.ieee.org/document/5152428/,2009 IEEE International Conference on Robotics and Automation,12-17 May 2009,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ROMAN.2002.1045593,Advanced autonomous action elements in combination control of remote operation and autonomous control,IEEE,Conferences,"This paper examines the combination control in which remote operation is combined with autonomous behaviors with the aim to realize the remote operation of mobile robot which moves in human-coexisting environment. We consider the distance and direction to an obstacle and the speed of motion of the mobile robot for revolution, following, and slowdown, which we have proposed as the autonomous action element in combination control. Fuzzy reasoning and vector components are used. From the experiment by three subject persons, almost the same result has been obtained. When the distance and direction to an obstacle and the speed of motion of the mobile robot are considered, there doesn't seem to be a great difference in following, but mileage becomes shorter in revolution and transit time is reduced in slowdown.",https://ieeexplore.ieee.org/document/1045593/,Proceedings. 11th IEEE International Workshop on Robot and Human Interactive Communication,27-27 Sept. 2002,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICPR.2010.843,Aesthetic Image Classification for Autonomous Agents,IEEE,Conferences,"Computational aesthetics is the study of applying machine learning techniques to identify aesthetically pleasing imagery. Prior work used online datasets scraped from large user communities like Flikr to get labeled data. However, online imagery represents results late in the media generation process, as the photographer has already framed the shot and then picked the best results to upload. Thus, this technique can only identify quality imagery once it has been taken. In contrast, automatically creating pleasing imagery requires understanding the imagery present earlier in the process. This paper applies computational aesthetics techniques to a novel dataset from earlier in that process in order to understand how the problem changes when an autonomous agent, like a robot or a real-time camera aid, creates pleasing imagery instead of simply identifying it.",https://ieeexplore.ieee.org/document/5597531/,2010 20th International Conference on Pattern Recognition,23-26 Aug. 2010,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISCAS.2019.8702113,An Energy Efficient System for Touch Modality Classification in Electronic Skin Applications,IEEE,Conferences,"Electronic-skin aiming to mimic human skin is becoming a reality and systems able to process data close to the sensors are required to reduce latency and power consumption. This paper presents the design and implementation of an energy efficient smart system for tactile sensing based on a RISC-V parallel ultra-low power platform (PULP). The PULP processor, called Mr. Wolf, performs the on-board classification of different touch modalities. This demonstrates the promising use of on-board classification for emerging robot and prosthetic applications. Experimental results demonstrate the effectiveness of the platform on improving the energy efficiency of the online classification. In our experiments, Mr. Wolf runs 3.6 times faster than an ARM Cortex M4F (STM32F40), consuming only 28 mW. The proposed platform achieves 15× better energy efficiency, than the classification done on the STM32F40, consuming only 81mJ per classification.",https://ieeexplore.ieee.org/document/8702113/,2019 IEEE International Symposium on Circuits and Systems (ISCAS),26-29 May 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICTAI.2015.111,An Environment Visual Awareness Approach in Cognitive Model ABGP,IEEE,Conferences,"ABGP is a special cognitive model, which consists of awareness, beliefs, goals and plans. As most agent architectures, ABGP agents obtain knowledge from the natural scenes only through single preestablished rules as well, don't directly capture the natural scenes information like human visual. Inspired by the biological visual cortex (V1) and the higher brain areas perceiving visual features, we propose a novel deep network model convolutional generative stochastic model (CGSM) used to visual feature representation, and firstly introduce it into the awareness module of the cognitive model ABGP to construct a state-of-the-art cognitive model ABGP-CGSM. For the novel cognitive model ABGP-CGSM, we construct a rat-robot maze search simulation platform to show the validity recognizing natural scenes. According to the simulation results on the noise and noiseless natural scenes, the rat-robot implemented by ABGP-CGSM has an excellent success rate when passing through the maze. The simulation shows that the ABGP-CGSM model proposed in our work can directly enhance the capability of communication between agent and natural scenes, improve the ability to cognize the real world as human being and conduct the agent to plan independently its path in terms of the visual information from the natural scenes.",https://ieeexplore.ieee.org/document/7372207/,2015 IEEE 27th International Conference on Tools with Artificial Intelligence (ICTAI),9-11 Nov. 2015,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/WCICA.2006.1713159,An Improved Q-learning Algorithm Based on Exploration Region Expansion Strategy,IEEE,Conferences,"In order to find a good solution to one of the key problems in Q-learning algorithm - keeping the balance between exploration and exploitation, an improved Q-learning algorithm based on exploration region expansion strategy is proposed on the base of Metropolis criterion-based Q-learning. With this strategy, the exploration blindness in the entire environment is eliminated, and the learning efficiency is increased. Meanwhile, other feasible path is sought where agent encounters obstacles, which makes the implementation of the algorithm on real robot easy. An automatic termination condition is also put forward, therefore, the redundant learning after finding optimal path is avoided, and the time of learning is reduced. The validity of the algorithm is proved by simulation experiments",https://ieeexplore.ieee.org/document/1713159/,2006 6th World Congress on Intelligent Control and Automation,21-23 June 2006,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/KCIC.2018.8628468,An Incremental Episodic Memory Framework for Topological Map Building,IEEE,Conferences,"In this paper, an episodic memory learning framework is proposed for categorizing and encoding sensory information that acquired from a robot for environment adaptation and sensorimotor map building. The proposed learning model termed as Incremental Episodic Memory Adaptive Resonance Theory (In-EMART), consists two layers of ART networks which used to detect novel event encountered by the robot and learn the spatio-temporal relationship by creating neurons incrementally. A set of connected episodes forms a sensorimotor map that can be used for path planning and goal navigation autonomously. The experimental results for a mobile robot show that: (i) In-EMART can learn sensory data in real time which is important for robot implementation; (ii) the model solves the perceptual aliasing issue by recalling the connected episode neurons; (iii) compared with previous works, the proposed method further generates a sensorimotor map for connecting episodes together to navigate from one place to another continuously.",https://ieeexplore.ieee.org/document/8628468/,2018 International Electronics Symposium on Knowledge Creation and Intelligent Computing (IES-KCIC),29-30 Oct. 2018,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CCDC49329.2020.9164059,An Integration-Enhanced Noise-Resistant RNN Model with Superior Performance Illustrated via Time-Varying Sylvester Equation Solving,IEEE,Conferences,"The Sylvester equation plays a fundamental role in the control system, e.g, which can be applied to inverse-kinematic motion control problem in robot manipulators through a certain conversion. Considering the incompatibility of most existing Sylvester equation solving schemes on noise and the inevitability of noise in real life, by defining a new matrix-valued error function, an integration-enhanced noise-resistant recurrent neural network (IENRRNN) is generalize to the time-varying Sylvester equation solving in this paper. The convergence of the IENRRNN model under both the model implementation error and differential error of coefficient matrices are investigated. What’s more, from both convergence speed and convergence quality, effects of three activation functions on the computational errors achieved by the IENRRNN are evaluated. The influences of different design parameters on them are also discussed. Finally, with MATLAB, the effectiveness of the IENRRNN model for online solving the considered equation and its superiority compared to the traditional zeroing neural network (ZNN) model are demonstrated by simulative results.",https://ieeexplore.ieee.org/document/9164059/,2020 Chinese Control And Decision Conference (CCDC),22-24 Aug. 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/INMIC.2004.1492904,An adaptive clustering method for model-free reinforcement learning,IEEE,Conferences,Machine learning for real world applications is a complex task due to the huge state and action sets they deal with and the a priori unknown dynamics of the environment involved. Reinforcement learning offers very efficient model-free methods which are often combined with approximation architectures to overcome these problems. We present a Q-learning implementation that uses a new adaptive clustering method to approximate state and actions sets. Experimental results for an obstacle avoidance behavior with the mobile robot Khepera are given.,https://ieeexplore.ieee.org/document/1492904/,"8th International Multitopic Conference, 2004. Proceedings of INMIC 2004.",24-26 Dec. 2004,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ETFA46521.2020.9212163,An adaptive robotic grasping with a 2-finger gripper based on deep learning network,IEEE,Conferences,"In this paper, an adaptive and versatile robotic grasping system is presented that is able to manipulate manufactured objects in production factories with a 2-finger gripper. A pick and place scenario based on deep learning framework is implemented and is achieved based on the following main steps: detection of the manufactured objects in the global scene observed by a first RGB-D camera using a first deep learning network, estimation of the object pose using 2D bounding box coordinates and depth information, motion of the arm above the object in an approach pose using Kinematics and Dynamics Library (KDL), recognition of the object’s face using a second deep learning network and information coming from a second RGB-D camera setup on the arm wrist, decision on the optimal grasping mode (opening or closing the fingers), execution of the grasping action. The developed system is validated practically by experiments in real world settings using a mobile manipulator platform consisting of 6 DoF robot arm with a 2-finger gripper setup on a mobile robot equipped by two RGB-D cameras.",https://ieeexplore.ieee.org/document/9212163/,2020 25th IEEE International Conference on Emerging Technologies and Factory Automation (ETFA),8-11 Sept. 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/INDIN.2005.1560420,An analogue recurrent neural network for trajectory learning and other industrial applications,IEEE,Conferences,"A real-time analogue recurrent neural network (RNN) can extract and learn the unknown dynamics (and features) of a typical control system such as a robot manipulator. The task at hand is a tracking problem in the presence of disturbances. With reference to the tasks assigned to an industrial robot, one important issue is to determine the motion of the joints and the effector of the robot. In order to model robot dynamics we use a neural network that can be implemented in hardware. The synaptic weights are modelled as variable gain cells that can be implemented with a few MOS transistors. The network output signals portray the periodicity and other characteristics of the input signal in unsupervised mode. For the specific purpose of demonstrating the trajectory learning capabilities, a periodic signal with varying characteristics is used. The developed architecture, however, allows for more general learning tasks typical in applications of identification and control. The periodicity of the input signal ensures convergence of the output to a limit cycle. Online versions of the synaptic update can be formulated using simple CMOS circuits. Because the architecture depends on the network generating a stable limit cycle, and consequently a periodic solution which is robust over an interval of parameter uncertainties, we currently place the restriction of a periodic format for the input signals. The simulated network contains interconnected recurrent neurons with continuous-time dynamics. The system emulates random-direction descent of the error as a multidimensional extension to the stochastic approximation. To achieve unsupervised learning in recurrent dynamical systems we propose a synapse circuit which has a very simple structure and is suitable for implementation in VLSI.",https://ieeexplore.ieee.org/document/1560420/,"INDIN '05. 2005 3rd IEEE International Conference on Industrial Informatics, 2005.",10-12 Aug. 2005,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICAICA54878.2022.9844610,An application case of object detection model based on Yolov3-SPP model pruning,IEEE,Conferences,"In order to reduce environmental pollution and accelerate the sustainable utilization of resources, it is very important to classify municipal solid waste. The traditional manual sorting method not only has high labor costs, but also can cause physical harm and threaten the health of practitioners when it comes into contact with hazardous waste. Therefore, it is of great research significance to improve the autonomy, intelligence and practical application of waste sorting. In the specific application of garbage classification, it is restricted by the hardware of edge devices, and it is necessary to meet the requirements of detection accuracy and reduction of computing power consumption at the same time. Therefore, this paper adopts the light YOLOv3-SPP network to classify garbage. Specifically, this paper first builds a garbage detection dataset, and then uses the dataset to train the YOLOv3-SPP network to form a YOLOv3-SPP garbage detection model. Next, model compression is performed on the YOLOv3-SPP garbage detection model using channel pruning and layer pruning, and the compressed model can be directly deployed on the low-computing edge computing device carried in the robot that automatically sorts and recycles garbage, so as to realize the miniaturization and large-scale practical application of the garbage sorting robot. The experimental results show that after model compression, the size and inference time of the model drop by 90.77&#x0025; and 72.93&#x0025;, respectively, while the mAP of the model only drops by 3.92&#x0025;. It shows that the constructed garbage detection model meets the application requirements of real-time detection on edge computing devices with low computing power, and has practical application potential.",https://ieeexplore.ieee.org/document/9844610/,2022 IEEE International Conference on Artificial Intelligence and Computer Applications (ICAICA),24-26 June 2022,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICPHYS.2018.8390779,An approach for implementing key performance indicators of a discrete manufacturing simulator based on the ISO 22400 standard,IEEE,Conferences,"Performance measurement tools and techniques have become very significant in today's industries for increasing the efficiency of their processes in order to face the competitive market. The first step towards performance measurement is the real-time monitoring and gathering of the data from the manufacturing system. Applying these performance measurement techniques on real-world industry in a way that is more general and efficient is the next challenge. This paper presents a methodology for implementing the key performance indicators defined in the ISO 22400 standard-Automation systems and integration, Key performance indicators (KPIs) for manufacturing operations management. The proposed methodology is implemented on a multi robot line simulator for measuring its performance at runtime. The approach implements a knowledge-based system within an ontology model which describes the environment, the system and the KPIs. In fact, the KPIs semantic descriptions are based on the data models presented in the Key Performance Indicators Markup Language (KPIML), which is an XML implementation of models developed by the Manufacturing Enterprise Solutions Association (MESA) international organization.",https://ieeexplore.ieee.org/document/8390779/,2018 IEEE Industrial Cyber-Physical Systems (ICPS),15-18 May 2018,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICSMC.1999.812440,An artificial retina with adaptive mechanisms and its application to retinal prosthesis,IEEE,Conferences,"The retina is a paradigm of novel image processing systems in which a real time computation with low power dissipation and a compact hardware is required. The basic structure underlying the computation of outer retinal neural network was elucidated with physiological experiments and computer simulations. We describe the spatio-temporal properties of outer retinal circuit with an equivalent resistive circuit model. Based on the model, an artificial retina was fabricated using analog CMOS VLSI circuit technology. The output of artificial retina emulates the center-surround antagonistic receptive field. The chip was incorporated with light-adaptive functions, which are inspired by physiological observations, to extract edges of a given image effectively. Preliminary experiments of applying the artificial retina for robot vision as well as for retinal prosthesis are performed.",https://ieeexplore.ieee.org/document/812440/,"IEEE SMC'99 Conference Proceedings. 1999 IEEE International Conference on Systems, Man, and Cybernetics (Cat. No.99CH37028)",12-15 Oct. 1999,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/INDIN.2017.8104950,An autopilot system based on ROS distributed architecture and deep learning,IEEE,Conferences,"An autopilot system includes several modules, and the software architecture has a variety of programs. As we all know, it is necessary that there exists one brand with a compatible sensor system till now, owing to complexity and variety of sensors before. In this paper, we apply (Robot Operating System) ROS-based distributed architecture. Deep learning methods also adopted by perception modules. Experimental results demonstrate that the system can reduce the dependence on the hardware effectively, and the sensor involved is convenient to achieve well the expected functionalities. The system adapts well to some specific driving scenes, relatively fixed and simple driving environment, such as the inner factories, bus lines, parks, highways, etc. This paper presents the case study of autopilot system based on ROS and deep learning, especially convolution neural network (CNN), from the perspective of system implementation. And we also introduce the algorithm and realization process including the core module of perception, decision, control and system management emphatically.",https://ieeexplore.ieee.org/document/8104950/,2017 IEEE 15th International Conference on Industrial Informatics (INDIN),24-26 July 2017,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICSMC.2002.1175718,An evolutionary visual landmark recognition system,IEEE,Conferences,A vision-based landmark recognition system by using the evolutionary principle for robot navigation tasks is implemented in this study. The research is aimed at using the GA to do pattern matching. The basic idea is to use genetic algorithms to find the best matching between nodes of the two patterns. The evaluation function can be defined in terms of total differences in magnitudes of nodes between the desired pattern and the real pattern. A search method based on genetic algorithms for pattern recognition in digital images is implemented as the vision layer for a behavior based mobile robot. The vision layer can recognize artificial landmarks by searching all the pro-defined patterns using the GA. Then it generates the desired behavior corresponding to various landmarks. The results of the algorithm is promising and has a high accuracy in classifying the input patterns. The effectiveness of the developed system is demonstrated by simulation and experimental studies.,https://ieeexplore.ieee.org/document/1175718/,"IEEE International Conference on Systems, Man and Cybernetics",6-9 Oct. 2002,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICAICA52286.2021.9497930,An improved semantic segmentation and fusion method for semantic SLAM,IEEE,Conferences,"Perception of the environment is an important part of robot intelligence. In order to better interact with the environment, the robot should not only know the shape of objects but also their semantics. In order to meet diversified needs, robot products are becoming more and more miniaturized, and related technologies have become research hotspots in the field. In response to this situation, this paper focuses on speed optimization based on the existing semantic map construction method to make it suitable for operation in embedded systems. This paper makes improvements to semantic segmentation and uses TensorRT to build a fast inference engine to accelerate target detection and speed up its inference speed on embedded devices. This paper uses Bayesian fusion method to fuse the semantic information of different locations to build an accurate map. Finally, in order to evaluate the real-time performance and effectiveness of this method, a test on the ADE20K data set was carried out, and the experimental results were analyzed to prove the effectiveness of the optimization of this algorithm.",https://ieeexplore.ieee.org/document/9497930/,2021 IEEE International Conference on Artificial Intelligence and Computer Applications (ICAICA),28-30 June 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/URTC.2016.8284091,An indoor positioning system facilitated by computer vision,IEEE,Conferences,"The purpose of this paper is to present our work on a novel method which allows for previously unattainable accuracy in indoor positioning. Global positioning has changed the way in which we interact with our specific locations on a real time basis, as can be seen most prominently in mapping applications. However, global positioning is severely limited indoors where location is equally important, and further, requires greater accuracy. While there have been attempts to implement indoor positioning, these methods are severely lacking, prompting us to take a completely new approach. We use low cost webcams and a series of algorithms to detect people in a video frame, and then identify and position them. Accuracy for identification is upwards of 95% and positioning accuracy is within a half-meter for the majority of the frame of view, all while running in real time on mobile CPUs. Such a system can be implemented on large scales to allow for exciting new applications; indoor directions in malls and public transportation hubs, new forms of human-robot interactions and consumer habit analysis in stores are all now possible.",https://ieeexplore.ieee.org/document/8284091/,2016 IEEE MIT Undergraduate Research Technology Conference (URTC),4-6 Nov. 2016,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICMA.2013.6618173,An intelligent object manipulation framework for industrial tasks,IEEE,Conferences,"This paper presents an intelligent object manipulation framework for industrial tasks, which integrates a sensor-rich multi-fingered robot hand, an industrial robot manipulator, a conveyor belt and employs machine learning algorithms. The framework software architecture is implemented using a Windows 7 operating system with RTX real-time extension for synchronous handling of peripheral devices. The framework uses Scale Invariant Feature Transform (SIFT) image processing algorithm, Support Vector Machine (SVM) machine learning algorithm and 3D point cloud techniques for intelligent object recognition based on RGB camera and laser rangefinder information from the robot hand end effector. The objective is automated manipulation of objects with different shapes and poses with minimum programming effort applied by a user.",https://ieeexplore.ieee.org/document/6618173/,2013 IEEE International Conference on Mechatronics and Automation,4-7 Aug. 2013,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IJCNN.2010.5596696,An open-source real-time system for remote robotic control using Neuroblastoma cultures,IEEE,Conferences,"This paper introduces an open-source real-time system that controls remotelly a robot using Human Neuroblastoma cultures and basic Braitenberg principles. Multielectrode Arrays Setups have been designed for direct culturing neural cells over silicon or glass substrates, providing the capability to stimulate and record simultaneously populations of neural cells. The main objective of this research is to modulate the natural physiologic responses of human neural cells by tetanic stimulation of the culture. If the system is able to modify the selective responses of some cells with a external pattern stimuli provided by a robot over different time scales, the neuroblastoma-cultured structure could be trained to process pre-programmed spatio-temporal patterns, controlling in this way the robotic behaviour.",https://ieeexplore.ieee.org/document/5596696/,The 2010 International Joint Conference on Neural Networks (IJCNN),18-23 July 2010,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ROBOT.1991.131636,Analogue computation of collision-free paths,IEEE,Conferences,"A method for robot path planning that uses a 2D scalar electric potential subject to Neumann boundary conditions is presented. Obstacles are modeled as nonconducting solids in a conducting medium. The starting point is modeled as a current source and the goal as an equal and opposite current sink. It is shown that this formulation is considerably more powerful than the recent potential-field algorithm of C.I. Connolly et al. (1990), particularly when navigating long, narrow corridors. Feasible paths for navigation are current streamlines, as demonstrated by the results of software simulations in a 2D Euclidean plane. One of the principal advantages of the method is that it can be implemented with parallel analog hardware in the form of a resistive grid. With analog VLSI chips, it will be possible to plan paths for realistic environments in real time.<>",https://ieeexplore.ieee.org/document/131636/,Proceedings. 1991 IEEE International Conference on Robotics and Automation,9-11 April 1991,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SNPD.2007.118,Applying Multiple Classifier Systems to SoftMan's Perception System,IEEE,Conferences,"SoftMan, the virtual robot in network environment, is a kind of software Artificial Life living in computer networks. It has a humanoid structure, and simulating human, its perception system should be able to recognize perceptive objects. Enlightened by human's perception system and the ""disassemble-integration"" method in analyzing large systems, a cooperative classification model in SoftMan's perception system is proposed. In the model, different humanoid senses are simulated by multiple classifier systems (MCS). Consequently, SoftMan can perform multi-sense cooperative classification on its perceptive objects. A simulated experiment validates the basic feasibility of the model.",https://ieeexplore.ieee.org/document/4287524/,"Eighth ACIS International Conference on Software Engineering, Artificial Intelligence, Networking, and Parallel/Distributed Computing (SNPD 2007)",30 July-1 Aug. 2007,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/HUMANOIDS.2012.6651500,Applying statistical generalization to determine search direction for reinforcement learning of movement primitives,IEEE,Conferences,"In this paper we present a new methodology for robot learning that combines ideas from statistical generalization and reinforcement learning. First we apply statistical generalization to compute an approximation for the optimal control policy as defined by training movements that solve the given task in a number of specific situations. This way we obtain a manifold of movements, which dimensionality is usually much smaller than the dimensionality of a full space of movement primitives. Next we refine the policy by means of reinforcement learning on the approximating manifold, which results in a learning problem constrained to the low dimensional manifold. We show that in some situations, learning on the low dimensional manifold can be implemented as an error learning algorithm. We apply golden section search to refine the control policy. Furthermore, we propose a reinforcement learning algorithm with an extended parameter set, which combines learning in constrained domain with learning in full space of parametric movement primitives, which makes it possible to explore actions outside of the initial approximating manifold. The proposed approach was tested for learning of pouring action both in simulation and on a real robot.",https://ieeexplore.ieee.org/document/6651500/,2012 12th IEEE-RAS International Conference on Humanoid Robots (Humanoids 2012),29 Nov.-1 Dec. 2012,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ROBOT.1986.1087518,Architecture and early experience with planning for the ALV,IEEE,Conferences,"This paper describes the software architecture and the initial algorithms that have proved to be effective for a real time robot planning system. The architecture is designed to incorporate planning technology from research on artificial intelligence while at the same time supporting the high performance decision making needed to control a fast-moving autonomous vehicle. The symbolic representation of the vehicle's plan is a key element in this architecture. Our initial algorithms use an especially efficient version of dynamic programming to find the best routes. The route is then translated into a symbolic plan. Replanning happens at several levels with the cost of replanning proportionate to the scope of the changes. This software is currently running in an environment which simulates the vehicle and perception systems, but it will be transferred to the DARPA Autonomous Land Vehicle built by Martin Marietta Denver Aerospace [Lowrie 86].",https://ieeexplore.ieee.org/document/1087518/,Proceedings. 1986 IEEE International Conference on Robotics and Automation,7-10 April 1986,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IDAACS.2013.6663049,Artificial curiosity driven autonomous knowledge discovery based on learning by interaction,IEEE,Conferences,"In this work we investigate the development of a real-time intelligent system allowing a humanoid robot to discover its surrounding world and to learn autonomously new knowledge about it by semantically interacting with human. The learning is performed by observation and by interaction with a human tutor. We describe the system in a general manner, and then we apply it to autonomous learning of objects and their colors. We provide experimental results as well using simulated environment as implementing the approach on a humanoid robot in a real-world environment including every-day objects. We show, that our approach allows a humanoid robot to learn without negative input and from small number of samples.",https://ieeexplore.ieee.org/document/6663049/,2013 IEEE 7th International Conference on Intelligent Data Acquisition and Advanced Computing Systems (IDAACS),12-14 Sept. 2013,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA48506.2021.9561671,Assumption Monitoring Using Runtime Verification for UAV Temporal Task Plan Executions,IEEE,Conferences,"Temporal task planning guarantees a robot will succeed in its task as long as certain explicit and implicit assumptions about the robot’s operating environment, sensors, and capabilities hold. A robot executing a plan can silently fail to fulfill the task if the assumptions are violated at runtime. Monitoring assumption violations at runtime can flag silent failures and also provide mitigation and remediation opportunities. However, this requires means for describing assumptions combining temporal and quantitative data, automatic construction of correct monitors and ensuring a correct interplay between the planning execution and monitors. In this paper we propose combining temporal planning with stream runtime verification, which offers a high-level language to describe monitors together with guarantees on execution time and memory usage. We demonstrate our approach both in real and simulated flights for some typical mission scenarios.",https://ieeexplore.ieee.org/document/9561671/,2021 IEEE International Conference on Robotics and Automation (ICRA),30 May-5 June 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IJCNN.2011.6033367,Attention driven computational model of the auditory midbrain for sound localization in reverberant environments,IEEE,Conferences,"In this paper, an auditory attention driven computational model of the auditory midbrain is proposed based on a spiking neural network [17] in order to localize attended sound sources in reverberant environments. Both bottom-up attention driven by sensors and top-down attention driven by the cortex are modeled at the level of an auditory midbrain nucleus - the inferior colliculus (IC). Improvements of the model in [17] is made to increase biological plausibility. First, inter-neuron inhibitions are modeled among the IC neurons which have the same characteristic frequency but different spatial response. This is designed to mimic the precedence effect [15] to produce localization results in reverberate environments. Secondly, descending projections from the auditory cortex (AC) to the IC are model to simulate the top-down attention so that focused sound sources can be better sensed in noise or multiple sound source situations. Our model is implemented on a mobile robot with a manikin head equipped with binaural microphones and tested in a real environment. The results shows that our attention driven model can give more accurate localization results than prior models.",https://ieeexplore.ieee.org/document/6033367/,The 2011 International Joint Conference on Neural Networks,31 July-5 Aug. 2011,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/RIOS.2015.7270741,Attitude control and trajectory tracking of an autonomous miniature aerial vehicle,IEEE,Conferences,"This paper introduces a Miniature Aerial Vehicle (MAV) which is Autonomous in outdoor environment. Main contributions of this research are both new trajectory tracking and attitude control scheme in real flight mode. This MAV is based on a traditional quadrotor. For stabilization of the quadrotor's attitude a PID controller is utilized. The proposed controller is designed such that to be able to attenuate effect of external wind disturbance and guarantee stability in this condition. For autonomous trajectory tracking, it is necessary to have a fixed altitude. Also an ARM cortex M4 microcontroller performs processing activities. Then, a trajectory is determined by a GPS in Mission Planner software for the outdoor environment. For real time communication between robot and ground station, HMTR module is used. Flight data is saved in Memory SD card and converts to MATLAB code for real time implementation. Experimental results of the proposed controller on the Autonomous Quadrotor in real conditions show the effectiveness of our approach.",https://ieeexplore.ieee.org/document/7270741/,2015 AI & Robotics (IRANOPEN),12-12 April 2015,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/COMPSAC54236.2022.00116,Automatic Accuracy versus Complexity Characterization for Embedded Emotion-Sensing Platforms in Healthcare Applications,IEEE,Conferences,"In this paper, we present a novel framework to realize an automatic &#x201C;accuracy versus complexity&#x201D; characterization for embeddable emotion recognition systems in healthcare applications. This framework is based on a series of grid searches able to identify highly descriptive features from the input signals, while taking into account implementation ease, memory usage, and classification performance. This paper is articulated on a proof of concept, in which the proposed framework is used to extract an emotion recognition architecture able to discriminate up to 8 emotions, by employing the users&#x0027; cortical activity and a 3D model for the emotion characterization. The implemented series of grid searches lead to a final low-memory, low-resources and low-complexity emotions recognition processing chain suitable for embedded platforms. The processing chain extracted by the proposed framework has been implemented as a real-time task on a personal care robot processing core. The extracted emotion recognition system tested on cortical signals from a publicly available dataset, showed an accuracy of &#x007E;75 &#x0025; (average) in discriminating 8 emotions and a memory reduction of 70&#x0025; from a canonical implementation of the same feature extraction step.",https://ieeexplore.ieee.org/document/9842581/,"2022 IEEE 46th Annual Computers, Software, and Applications Conference (COMPSAC)",27 June-1 July 2022,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS45743.2020.9341657,Autonomous Exploration Under Uncertainty via Deep Reinforcement Learning on Graphs,IEEE,Conferences,"We consider an autonomous exploration problem in which a range-sensing mobile robot is tasked with accurately mapping the landmarks in an a priori unknown environment efficiently in real-time; it must choose sensing actions that both curb localization uncertainty and achieve information gain. For this problem, belief space planning methods that forward- simulate robot sensing and estimation may often fail in real-time implementation, scaling poorly with increasing size of the state, belief and action spaces. We propose a novel approach that uses graph neural networks (GNNs) in conjunction with deep reinforcement learning (DRL), enabling decision-making over graphs containing exploration information to predict a robot's optimal sensing action in belief space. The policy, which is trained in different random environments without human intervention, offers a real-time, scalable decision-making process whose high-performance exploratory sensing actions yield accurate maps and high rates of information gain.",https://ieeexplore.ieee.org/document/9341657/,2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),24 Oct.-24 Jan. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AERO47225.2020.9172808,Autonomous UAV Navigation for Active Perception of Targets in Uncertain and Cluttered Environments,IEEE,Conferences,"The use of Small Unmanned Aerial Vehicles (sUAVs) has grown exponentially owing to an increasing number of autonomous capabilities. Automated functions include the return to home at critical energy levels, collision avoidance, take-off and landing, and target tracking. However, sUAVs applications in real-world and time-critical scenarios, such as Search and Rescue (SAR) is still limited. In SAR applications, the overarching aim of autonomous sUAV navigation is the quick localisation, identification and quantification of victims to prioritise emergency response in affected zones. Traditionally, sUAV pilots are exposed to prolonged use of visual systems to interact with the environment, which causes fatigue and sensory overloads. Nevertheless, the search for victims onboard a sUAV is challenging because of noise in the data, low image resolution, illumination conditions, and partial (or full) occlusion between the victims and surrounding structures. This paper presents an autonomous Sequential Decision Process (SDP) for sUAV navigation that incorporates target detection uncertainty from vision-based cameras. The SDP is modelled as a Partially Observable Markov Decision Process (POMDP) and solved online using the Adaptive Belief Tree (ABT) algorithm. In particular, a detailed model of target detection uncertainty from deep learning-based models is shown. The presented formulation is tested under Software in the Loop (SITL) through Gazebo, Robot Operating System (ROS), and PX4 firmware. A Hardware in the Loop (HITL) implementation is also presented using an Intel Myriad Vision Processing Unit (VPU) device and ROS. Tests are conducted in a simulated SAR GPS-denied scenario, aimed to find a person at different levels of location and pose uncertainty.",https://ieeexplore.ieee.org/document/9172808/,2020 IEEE Aerospace Conference,7-14 March 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SIMPAR.2016.7862403,Autonomous exploration by expected information gain from probabilistic occupancy grid mapping,IEEE,Conferences,"Occupancy grid maps are spatial representations of environments, where the space of interest is decomposed into a number of cells that are considered either occupied or free. This paper focuses on exploring occupancy grid maps by predicting the uncertainty of the map. Based on recent improvements in computing occupancy probability, this paper presents a novel approach for selecting robot poses designed to maximize expected map information gain represented by the change in entropy. This result is simplified with several approximations to develop an algorithm suitable for real-time implementation. The predicted information gain proposed in this paper governs an effective autonomous exploration strategy when applied in conjunction with an existing motion planner to avoid obstacles, which is illustrated by numerical examples.",https://ieeexplore.ieee.org/document/7862403/,"2016 IEEE International Conference on Simulation, Modeling, and Programming for Autonomous Robots (SIMPAR)",13-16 Dec. 2016,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IJCNN.2013.6706877,Autonomous reinforcement of behavioral sequences in neural dynamics,IEEE,Conferences,"We introduce a dynamic neural algorithm called Dynamic Neural (DN) SARSA(λ) for learning a behavioral sequence from delayed reward. DN-SARSA(λ) combines Dynamic Field Theory models of behavioral sequence representation, classical reinforcement learning, and a computational neuroscience model of working memory, called Item and Order working memory, which serves as an eligibility trace. DN-SARSA(λ) is implemented on both a simulated and real robot that must learn a specific rewarding sequence of elementary behaviors from exploration. Results show DN-SARSA(λ) performs on the level of the discrete SARSA(λ), validating the feasibility of general reinforcement learning without compromising neural dynamics.",https://ieeexplore.ieee.org/document/6706877/,The 2013 International Joint Conference on Neural Networks (IJCNN),4-9 Aug. 2013,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IST.2012.6295593,Autonomous robotic ground penetrating radar surveys of ice sheets; Using machine learning to identify hidden crevasses,IEEE,Conferences,"This paper presents methods to continue development of a completely autonomous robotic system employing ground penetrating radar imaging of the glacier sub-surface. We use well established machine learning algorithms and appropriate un-biased processing, particularly those which are also suitable for real-time image analysis and detection. We tested and evaluated three processing schemes in conjunction with a Support Vector Machine (SVM) trained on 15 examples of Antarctic GPR imagery, collected by our robot and a Pisten Bully tractor in 2010 in the shear zone near McMurdo Station. Using a modified cross validation technique, we correctly classified all examples with a radial basis kernel SVM trained and evaluated on down-sampled and texture-mapped GPR images of crevasses, compared to 60% classification rate using raw data. We also test the most successful processing scheme on a larger dataset, comprised of 94 GPR images of crevasse crossings recorded in the same deployment. Our experiments demonstrate the promise and reliability of real-time object detection and classification with robotic GPR imaging surveys.",https://ieeexplore.ieee.org/document/6295593/,2012 IEEE International Conference on Imaging Systems and Techniques Proceedings,16-17 July 2012,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA.2013.6631235,Autonomous robotic valve turning: A hierarchical learning approach,IEEE,Conferences,"Autonomous valve turning is an extremely challenging task for an Autonomous Underwater Vehicle (AUV). To resolve this challenge, this paper proposes a set of different computational techniques integrated in a three-layer hierarchical scheme. Each layer realizes specific subtasks to improve the persistent autonomy of the system. In the first layer, the robot acquires the motor skills of approaching and grasping the valve by kinesthetic teaching. A Reactive Fuzzy Decision Maker (RFDM) is devised in the second layer which reacts to the relative movement between the valve and the AUV, and alters the robot's movement accordingly. Apprenticeship learning method, implemented in the third layer, performs tuning of the RFDM based on expert knowledge. Although the long-term goal is to perform the valve turning task on a real AUV, as a first step the proposed approach is tested in a laboratory environment.",https://ieeexplore.ieee.org/document/6631235/,2013 IEEE International Conference on Robotics and Automation,6-10 May 2013,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IEMBS.2008.4649235,BMI cyberworkstation: Enabling dynamic data-driven brain-machine interface research through cyberinfrastructure,IEEE,Conferences,"Dynamic data-driven brain-machine interfaces (DDDBMI) have great potential to advance the understanding of neural systems and improve the design of brain-inspired rehabilitative systems. This paper presents a novel cyberinfrastructure that couples in vivo neurophysiology experimentation with massive computational resources to provide seamless and efficient support of DDDBMI research. Closed-loop experiments can be conducted with in vivo data acquisition, reliable network transfer, parallel model computation, and real-time robot control. Behavioral experiments with live animals are supported with real-time guarantees. Offline studies can be performed with various configurations for extensive analysis and training. A Web-based portal is also provided to allow users to conveniently interact with the cyberinfrastructure, conducting both experimentation and analysis. New motor control models are developed based on this approach, which include recursive least square based (RLS) and reinforcement learning based (RLBMI) algorithms. The results from an online RLBMI experiment shows that the cyberinfrastructure can successfully support DDDBMI experiments and meet the desired real-time requirements.",https://ieeexplore.ieee.org/document/4649235/,2008 30th Annual International Conference of the IEEE Engineering in Medicine and Biology Society,20-25 Aug. 2008,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICMLC.2012.6359467,BMP: A self-balancing mobile platform,IEEE,Conferences,"The development of two wheels self-balancing robot has gained more and more attention over the past years. It is suitable for working in outdoor environment especially where the ground is not flat. This paper describes our works on building a self-Balancing Mobile Platform named BMP. We build the dynamic model to find the relation between the robot posture and output voltages applying on the motors. An accelerometer and gyroscope are used to estimate the posture through Kalman filter algorithm. Then a multi-segment PID controller changes the motor voltage according to the platform motion mode and posture. The experiments are carried out in simulation, then on the real platform. The results of the experiments show that BMP works effectively and robustly.",https://ieeexplore.ieee.org/document/6359467/,2012 International Conference on Machine Learning and Cybernetics,15-17 July 2012,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/EMWRTS.1996.557846,Behaviour-oriented commands: from distributed knowledge representation to real-time implementation,IEEE,Conferences,"This paper presents a general methodology to model and implement real time control of complex systems with high reactivity. It is based on an original concept called ""behaviour oriented commands"" (BOCs). This methodology has been applied successfully in our mobile robot. BOCs incorporate mechanisms to model the set of rules (knowledge) which describes the restrictions and actions to achieve a goal. Basic rules are well encapsulated by entities called ""behaviours"", while global co-operating rules are explicited by the association link managed by the BOC's control unit. The model is easily translated into a real time implementation. This fusion between knowledge and real time is the main contribution of our work to the RT area.",https://ieeexplore.ieee.org/document/557846/,Proceedings of the Eighth Euromicro Workshop on Real-Time Systems,12-14 June 1996,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA40945.2020.9197470,"Benchmark for Skill Learning from Demonstration: Impact of User Experience, Task Complexity, and Start Configuration on Performance",IEEE,Conferences,"We contribute a study benchmarking the performance of multiple motion-based learning from demonstration approaches. Given the number and diversity of existing methods, it is critical that comprehensive empirical studies be performed comparing the relative strengths of these techniques. In particular, we evaluate four approaches based on properties an end user may desire for real-world tasks. To perform this evaluation, we collected data from nine participants, across four manipulation tasks. The resulting demonstrations were used to train 180 task models and evaluated on 720 task reproductions on a physical robot. Our results detail how i) complexity of the task, ii) the expertise of the human demonstrator, and iii) the starting configuration of the robot affect task performance. The collected dataset of demonstrations, robot executions, and evaluations are publicly available. Research insights and guidelines are also provided to guide future research and deployment choices about these approaches.",https://ieeexplore.ieee.org/document/9197470/,2020 IEEE International Conference on Robotics and Automation (ICRA),31 May-31 Aug. 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CRV52889.2021.00009,Building Facades to Normal Maps: Adversarial Learning from Single View Images,IEEE,Conferences,"Surface normal estimation is an essential component of several computer and robot vision pipelines. While this problem has been extensively studied, most approaches are geared towards indoor scenes and often rely on multiple modalities (depth, multiple views) for accurate estimation of normal maps. Outdoor scenes pose a greater challenge as they exhibit significant lighting variation, often contain occluders, and structures like building facades are often ridden with numerous windows and protrusions. Conventional supervised learning schemes excel in indoor scenes, but do not exhibit competitive performance when trained and deployed in outdoor environments. Furthermore, they involve complex network architectures and require many more trainable parameters. To tackle these challenges, we present an adversarial learning scheme that regularizes the output normal maps from a neural network to appear more realistic, by using a small number of precisely annotated examples. Our method presents a lightweight and simpler architecture, while improving performance by at least 1.5x across most metrics. We evaluate our approaches against the state-of-the-art on normal map estimation, on a synthetic and a real outdoor dataset, and observe significant performance enhancements.",https://ieeexplore.ieee.org/document/9469450/,2021 18th Conference on Robots and Vision (CRV),26-28 May 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IJCNN.2005.1555934,Building a cheaper artificial brain,IEEE,Conferences,"This paper presents a methodology for building artificial brains that is much cheaper than the first author's earlier attempt. What initially cost $500,000, now costs about $3000. The much cheaper approach uses a Celoxica(.com) programmable board containing a Xilinx Virtex II FPGA chip with 3 million programmable logic gates, to evolve neural networks at electronic speeds. The genetic algorithm (GA) and the neural network model are programmed using a high level language called Handel-C, whose code is (silicon) compiled into the chip. The elite circuit is downloaded from the board into the memory of a PC. This process occurs up to several 10,000 s of times, once for each neural net circuit module having a unique function. Special software in the PC is used to specify the connections between the modules, according to the designs of human BAs (brain architects). The PC is then used to execute the neural signaling of the artificial brain (A-brain) in real time, defined to be 25 Hz per neuron. At this speed, the PC can handle several 10,000 s of modules. We would use our A-brain to control the behaviors of a small, four wheeled radio controlled robot with a CCD camera and gripper. The robot's task is to detect and collect unexploded cluster bomblets and deposit them in some central place. The total price of the PC, Celoxica board, and robot is less than $3000, making it affordable to virtually any research group interested in building artificial brains.",https://ieeexplore.ieee.org/document/1555934/,"Proceedings. 2005 IEEE International Joint Conference on Neural Networks, 2005.",31 July-4 Aug. 2005,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS51168.2021.9635991,BundleTrack: 6D Pose Tracking for Novel Objects without Instance or Category-Level 3D Models,IEEE,Conferences,"Tracking the 6D pose of objects in video sequences is important for robot manipulation. Most prior efforts, however, often assume that the target object's CAD model, at least at a category-level, is available for offline training or during online template matching. This work proposes BundleTrack, a general framework for 6D pose tracking of novel objects, which does not depend upon 3D models, either at the instance or category-level. It leverages the complementary attributes of recent advances in deep learning for segmentation and robust feature extraction, as well as memory-augmented pose graph optimization for spatiotemporal consistency. This enables long-term, low-drift tracking under various challenging scenarios, including significant occlusions and object motions. Comprehensive experiments given two public benchmarks demonstrate that the proposed approach significantly outperforms state-of-art, category-level 6D tracking or dynamic SLAM methods. When compared against state-of-art methods that rely on an object instance CAD model, comparable performance is achieved, despite the proposed method’s reduced information requirements. An efficient implementation in CUDA provides a real-time performance of 10Hz for the entire framework. Code is available at: https://github.com/wenbowen123/BundleTrack",https://ieeexplore.ieee.org/document/9635991/,2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),27 Sept.-1 Oct. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ROBOT.1990.126097,CSL: a cost-sensitive learning system for sensing and grasping objects,IEEE,Conferences,"The goal of the research reported is to build a learning robot which can survive in an unknown environment for a long time. Such a robot must learn which sensors to use, where to use them, and how to generate an inexpensive and reliable robot control procedure to accomplish its task. This is beyond machine learning methods because they usually ignore robot execution costs and are ill-prepared to handle failures. A cost-sensitive, noise-tolerant and inductive robot learning system, CSL, that represents the first steps toward achieving this goal is described, emphasizing the cost and noise issues in learning. CSL has been implemented in a real-world robot for sensing objects and selecting their grasping procedures.<>",https://ieeexplore.ieee.org/document/126097/,"Proceedings., IEEE International Conference on Robotics and Automation",13-18 May 1990,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCE-Berlin50680.2020.9352201,Camera-LIDAR Object Detection and Distance Estimation with Application in Collision Avoidance System,IEEE,Conferences,"Nowadays we are aware of accelerated development of automotive software. Numerous of ADAS (Advanced Driver Assistance Systems) systems are being developed these days. One such system is the forward CAS (Collision Avoidance System). In order to implement such a system, this paper presents one solution for detecting an object located directly in front of the vehicle and estimating its distance. The solution is based on the use of camera and LIDAR (Light Detection and Ranging) sensor fusion. The camera was used for object detection and classification, while 3D data obtained from LIDAR sensor were used for distance estimation. In order to map the 3D data from the LIDAR to the 2D image space, a spatial calibration was used. The solution was developed as a prototype using the ROS (Robot Operating System) based Autoware open source platform. This platform is essentially a framework intended for the development and testing of automotive software. ROS as the framework on which the Autoware platform is based, provides a library for the Python and C++ programming languages, intended for creating new applications. For the reason that this is a prototype project, and it is popular for application in machine learning, we decided to use the Python programming language. The solution was tested inside the CARLA simulator, where the estimation of the obstacle distance obtained at the output of our algorithm was compared with the ground truth values obtained from the simulator itself. Measurements were performed under different weather conditions, where this algorithm showed satisfactory results, with real-time processing.",https://ieeexplore.ieee.org/document/9352201/,2020 IEEE 10th International Conference on Consumer Electronics (ICCE-Berlin),9-11 Nov. 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS45743.2020.9341134,Catch the Ball: Accurate High-Speed Motions for Mobile Manipulators via Inverse Dynamics Learning,IEEE,Conferences,"Mobile manipulators consist of a mobile platform equipped with one or more robot arms and are of interest for a wide array of challenging tasks because of their extended workspace and dexterity. Typically, mobile manipulators are deployed in slow-motion collaborative robot scenarios. In this paper, we consider scenarios where accurate high-speed motions are required. We introduce a framework for this regime of tasks including two main components: (i) a bi-level motion optimization algorithm for real-time trajectory generation, which relies on Sequential Quadratic Programming (SQP) and Quadratic Programming (QP), respectively; and (ii) a learning-based controller optimized for precise tracking of high-speed motions via a learned inverse dynamics model. We evaluate our framework with a mobile manipulator platform through numerous high-speed ball catching experiments, where we show a success rate of 85.33%. To the best of our knowledge, this success rate exceeds the reported performance of existing related systems [1], [2] and sets a new state of the art.",https://ieeexplore.ieee.org/document/9341134/,2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),24 Oct.-24 Jan. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CRC.2019.00021,Category Classification of Deformable Object using Hybrid Dynamic Model for Robotic Grasping,IEEE,Conferences,"This work studies the problem of classification of a hung garment in the unfolding procedure by a home service robot. The sheer number of unpredictable configurations that the deformable object can end up in makes the visual identification of the object shape and size difficult. In this paper, we propose a hybrid dynamic model to recognize the pose of hung garment using a single manipulator. A dataset of hung garment is generated by capturing the depth images of real garments at the robotic platform (real images) and also the images of garment mesh model from offline simulation (synthetic images) respectively. Deep convolutional neural network is implemented to classify the category and estimate the pose of garment. Experiment results show that the proposed method performs well and is applicable to different garments in robotic manipulation.",https://ieeexplore.ieee.org/document/9058831/,"2019 4th International Conference on Control, Robotics and Cybernetics (CRC)",27-30 Sept. 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CIFEr.2019.8759121,Chatbot Application on Cryptocurrency,IEEE,Conferences,"Many chatbots have been developed that provide a multitude of services through a wide range of methods. A chatbot is a brand-new conversational agent in the highspeed changing technology world. With the advance of Artificial Intelligence and machine learning, chatbots are becoming more and more popular. A chatbot is the extension of human interface mediums such as the phone and social platforms. Similarly, Cryptocurrency is a new extension of digital or virtual currency designed to work as a medium of exchange. In the current digital exchanging world, investors and interested parties are eager to know more information about, and the capabilites of, this new type of currency. One of the potential paths to retrieve the info automatically and quickly is through a chatbot. We explored the open source python library, Chatterbot, to apply Itchat API (a WeChat interface) with the aim of building a robot chatting application, I&#x0026;C Chat, on the topic of cryptocurrency. First, we collected question and answer pairs datasets from Quora websites. Furthermore, we also created API calls to query the real time quote for the top 25 cryptocurrencies. Then we used the collected data to train our chatbot and implemented a logic adapter to receive the price quote of cryptocurrencies based on the incoming question. The Itchat API method will return the best matched answer to the asking party automatically. The response time of different questions has been investigated. The results imply that this application is quite useful, feasible and beneficial to the digital currency world.",https://ieeexplore.ieee.org/document/8759121/,2019 IEEE Conference on Computational Intelligence for Financial Engineering & Economics (CIFEr),4-5 May 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IEEE.ICCC.2017.20,Cognitive Acoustic Analytics Service for Internet of Things,IEEE,Conferences,"The rapid development of the Internet of Things (IoT) has brought great changes for non-contact and non-destructive sensing and diagnosis. For every inanimate object can tell us something by the sound it makes, acoustic sensor demonstrates great advantages comparing to conventional electronic and mechanic sensors in such cases: overcoming environmental obstacles, mapping to existing use cases of detecting problems with human ears, low cost for deployment, etc. It could be widely applied to various domains, such as predictive maintenance of machinery, robot sensory, elderly and baby care in smart home, etc. Whether we can use the acoustic sensor data to understand what is happening and to predict what will happen relies heavily on the analytics capabilities we apply to the acoustic data, which has to overcome the obstacles of noise, disturbance and errors, and has to meet the requirement of real-time processing of high volume signals with large number of sensors. In this paper, we propose a scalable cognitive acoustics analytics service for IoT that provides the user an incremental learning approach to evolve their analytics capability on non-intuitive and unstructured acoustic data through the combination of acoustic signal processing and machine learning technology. It first performs acoustic signal processing and denoising, enables acoustic signal based abnormal detection based on sound intensity, spectral centroid, etc. Then based on the accumulated abnormal data, a supervised learning method is performed as baseline and a neural network based classifier is used to recognize acoustic events in different scenarios with various volume of sample data and requirement of accuracy. In addition, acoustic sensor arrays processing is supported for localization of moving acoustic source in more complex scenario. In this paper, we designed a hybrid computing structure. Finally, we conduct experiments on acoustic event recognition for machinery diagnosis, and show that the proposed system can achieve high accuracy.",https://ieeexplore.ieee.org/document/8029228/,2017 IEEE International Conference on Cognitive Computing (ICCC),25-30 June 2017,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICARA51699.2021.9376455,Coinbot: Intelligent Robotic Coin Bag Manipulation Using Artificial Brain,IEEE,Conferences,"Given the laborious difficulty of moving heavy bags of physical currency in the cash center of the bank, there is a large demand for training and deploying safe autonomous systems capable of conducting such tasks in a collaborative workspace. In this paper, we apply deep reinforcement learning and machine learning techniques to the task of controlling a collaborative robot to automate the unloading of coin bags from a trolley. To accomplish the task-specific process of gripping coin bags where the center of the mass changes during manipulation, a special gripper was designed in physical hardware. Leveraging a depth camera and deep learning, a bag detection and pose estimation has been done for choosing the optimal point of grasping. An intelligent approach based on deep reinforcement learning has been introduced to propose the best configuration of the robot end-effector to maximize successful grasping. A boosted motion planning is utilized to speed up the robot operation. Real-world trials with the proposed pipeline have demonstrated success rates over 96% in a real-world setting.",https://ieeexplore.ieee.org/document/9376455/,"2021 7th International Conference on Automation, Robotics and Applications (ICARA)",4-6 Feb. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IDAP.2017.8090332,Color based moving object tracking with an active camera using motion information,IEEE,Conferences,"In this study, a real-time system design, which can track (RGB) targets in dynamic environments with an active camera, was implemented. Object tracking applications are quite important for military, surveillance system and operational robot applications and getting more important day by day. This design allows us tracking an object using fewer cameras. The design consists of three main parts that are object detection, mapping, tracking the object. When the camera catches the object, first it detects the shape of the object and creates bounding box, according to bounding box information, the algorithm calculates the centroid of the object. Object coordinates are determined using centroid of the object then tracking process works by activating motors via Arduino-MATLAB communication. The motors placed on a platform called pan-tilt platform. The platform can turn 270 and 180 degrees on×and y-axis respectively.",https://ieeexplore.ieee.org/document/8090332/,2017 International Artificial Intelligence and Data Processing Symposium (IDAP),16-17 Sept. 2017,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS.1996.571056,Combining probabilistic map and dialog for robust life-long office navigation,IEEE,Conferences,A design of mobile robot for robust life-long navigation in office environment is proposed and evaluated. The key idea is combining probabilistic map and dialog with humans for reducing the location uncertainty. Bayesian inference with the map represented by probabilistic automata is used in order to reduce the number of queries and to evaluate the success rate of planned paths. We experimentally implemented the design using a simple Bayesian network with continuous nodes and demonstrated its effectiveness in a real environment.,https://ieeexplore.ieee.org/document/571056/,Proceedings of IEEE/RSJ International Conference on Intelligent Robots and Systems. IROS '96,8-8 Nov. 1996,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS.2009.5354270,Consideration on robotic giant-swing motion generated by reinforcement learning,IEEE,Conferences,"This study attempts to make a compact humanoid robot acquire a giant-swing motion without any robotic models by using reinforcement learning; only the interaction with environment is available. Generally, it is widely said that this type of learning method is not appropriated to obtain dynamic motions because Markov property is not necessarily guaranteed during the dynamic task. However, in this study, we try to avoid this problem by embedding the dynamic information in the robotic state space; the applicability of the proposed method is considered using both the real robot and dynamic simulator. This paper, in particular, discusses how the robot with 5-DOF, in which the Q-Learning algorithm is implemented, acquires a giant-swing motion. Further, we describe the reward effects on the Q-Learning. Finally, this paper demonstrates that the application of the Q-Learning enable the robot to perform a very attractive giant-swing motion.",https://ieeexplore.ieee.org/document/5354270/,2009 IEEE/RSJ International Conference on Intelligent Robots and Systems,10-15 Oct. 2009,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IVS.2000.898390,Coordination strategies for the goal-keeper of a RoboCup mid-size team,IEEE,Conferences,"In robot soccer, as well as in real soccer and in every team effort, coordination between members of a team is a key issue. We describe the coordination strategies that were designed to achieve effective cooperation between a robot goal-keeper and the rest of the ART (Azzurra Robot Team) team that participates in the RoboCup F-2000 (midsize) competitions.",https://ieeexplore.ieee.org/document/898390/,Proceedings of the IEEE Intelligent Vehicles Symposium 2000 (Cat. No.00TH8511),5-5 Oct. 2000,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ROMAN.2018.8525717,Data-driven development of Virtual Sign Language Communication Agents,IEEE,Conferences,"Engaging deaf and hearing people in common discussions requires interfaces to help them understand each other, such as robot agents that translate spoken language into Sign Language (SL) expressions and vice-versa. However, the recognition and generation of signed sentences is a complex task of high dimensionality that cannot be solved in sufficient quality yet. Thus, it is necessary to develop new technologies of improved performances. The sequence to sequence neural network model, traditionally used for machine translation, is adapted to the above two tasks by treating a SL sequence as a multi-dimensional sentence. We defined an encoding of the SL annotations and conducted experiments on the network structure to define a most accurate translation model. This study proves the network trainable and possibly applicable in real-life with an extended dataset, which shall be tested for deployment in virtual translation assistants in the following.",https://ieeexplore.ieee.org/document/8525717/,2018 27th IEEE International Symposium on Robot and Human Interactive Communication (RO-MAN),27-31 Aug. 2018,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRoM48714.2019.9071857,Deep Learning Approach For Object Tracking Of RoboEye,IEEE,Conferences,"RoboEye is a spherical 3RRR parallel robot which has been developed for its high precision. It can provide high speeds, so can be used for fast tracking tasks. To this end, in this paper proper deep learning approaches are combined with classical control methods. Deep learning algorithms are employed to detect an object of interest among various ones in a monocular image, and then obtain an estimatation of the distance to the camera. So, simultaneous depth estimation, and object detection with a monocular camera for real time implementation is proposed here. For fast calculations, also to overcome manufacturing uncertainties, inverse kinematic equations are computed by a multi-layer perceptron (MLP) network based on real data. Finally, a classical PID controller can perform a fast tracking of the object.",https://ieeexplore.ieee.org/document/9071857/,2019 7th International Conference on Robotics and Mechatronics (ICRoM),20-21 Nov. 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/UR52253.2021.9494704,Deep Learning based Food Instance Segmentation using Synthetic Data,IEEE,Conferences,"In the process of intelligently segmenting foods in images using deep neural networks for diet management, data collection and labeling for network training are very important but labor-intensive tasks. In order to solve the difficulties of data collection and annotations, this paper proposes a food segmentation method applicable to real-world through synthetic data. To perform food segmentation on healthcare robot systems, such as meal assistance robot arm, we generate synthetic data using the open-source 3D graphics software Blender placing multiple objects on meal plate and train Mask R-CNN for instance segmentation. Also, we build a data collection system and verify our segmentation model on real-world food data. As a result, on our real-world dataset, the model trained only synthetic data is available to segment food instances that are not trained with 52.2% mask AP@all, and improve performance by +6.4%p after fine-tuning comparing to the model trained from scratch. In addition, we also confirm the possibility and performance improvement on the public dataset for fair analysis.",https://ieeexplore.ieee.org/document/9494704/,2021 18th International Conference on Ubiquitous Robots (UR),12-14 July 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CASE48305.2020.9216881,Deep Learning for Early Damage Detection of Tailing Pipes Joints with a Robotic Device,IEEE,Conferences,"In the mining industry, it is usual to employ several kilometers of pipes to carry tailing from the plant to a dam. Only in the Salobo Mine, a copper operation in the Amazon forest from Vale S.A., there are more than three and a half kilometers of tailing pipes. Since the material passing through the tailing pipe causes an abrasion effect that could lead to failures, regular inspections are needed. However, given the risky environment to perform manual inspections, a teleoperated or autonomous robot is a crucial tool to keep track of the pipe health. In this work, we propose a deep-learning methodology to process the data stream of images from the robot, aiming to detect early failures directly on the onboard computer of the device in real-time. Multiple architectures of deep-learning image classification were evaluated to detect the anomalies. We validated the early damage detection accuracy and pinpointed the approximate location of the anomalies using the Class Activation Mapping of the networks. Then, we tested the runtime for the network architectures that obtained the best results on different hardware to analyze the need for a GPU onboard in the robot. Moreover, we also trained a Single Shot object Detector to find the boundaries of the pipe joints, which means that the anomaly classification is performed only when a joint is detected. Our results show that it is possible to build an automatic anomaly detection system in the software of the robot.",https://ieeexplore.ieee.org/document/9216881/,2020 IEEE 16th International Conference on Automation Science and Engineering (CASE),20-21 Aug. 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/R10-HTC.2018.8629836,Deep Learning-Based Eye Gaze Controlled Robotic Car,IEEE,Conferences,"In recent years Eye gaze tracking (EGT) has emerged as an attractive alternative to conventional communication modes. Gaze estimation can be effectively used in human-computer interaction, assistive devices for motor-disabled persons, autonomous robot control systems, safe car driving, diagnosis of diseases and even in human sentiment assessment. Implementation in any of these areas however mostly depends on the efficiency of detection algorithm along with usability and robustness of detection process. In this context we have proposed a Convolutional Neural Network (CNN) architecture to estimate the eye gaze direction from detected eyes which outperforms all other state of the art results for Eye-Chimera dataset. The overall accuracies are 90.21&#x0025; and 99.19&#x0025; for Eye-Chimera and HPEG datasets respectively. This paper also introduces a new dataset EGDC for which proposed algorithm finds 86.93&#x0025; accuracy. We have developed a real-time eye gaze controlled robotic car as a prototype for possible implementations of our algorithm.",https://ieeexplore.ieee.org/document/8629836/,2018 IEEE Region 10 Humanitarian Technology Conference (R10-HTC),6-8 Dec. 2018,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.23919/ASCC56756.2022.9828057,Deep Reinforcement Learning Based Tracking Control of Unmanned Vehicle with Safety Guarantee,IEEE,Conferences,"It is well known that the development of efficient real-time path following strategy and collision avoidance mechanism is critical to the practical implementation of autonomous driving technique. Within this context, this paper presents a new kind of hybrid control strategy consisting of the robot Stanley's trajectory tracking algorithm [1] and deep reinforcement learning (DRL) technique to achieve the goal of tracking control of unmanned vehicle with safety guarantee. By introducing the DRL technique, the tracking accuracy of the robot Stanley's trajectory tracking algorithm is improved and a safe control algorithm with collision avoidance is obtained. Furthermore, the complexity of the learning algorithm involved in the tracking controller is significantly reduced by using the Stanley's trajectory tracking algorithm, which makes the learning converge fast. Finally, numerical simulations are performed to verify that the proposed tracking algorithm has obviously advantages on tracking accuracy and training efficiency over some existing ones.",https://ieeexplore.ieee.org/document/9828057/,2022 13th Asian Control Conference (ASCC),4-7 May 2022,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IJCNN48605.2020.9207332,Deep Reinforcement Learning Control of Hand-Eye Coordination with a Software Retina,IEEE,Conferences,"Deep Reinforcement Learning (DRL) has gained much attention for solving robotic hand-eye coordination tasks from raw pixel values. Despite promising results, training agents using images is hardware intensive often requiring millions of training steps to converge incurring long training times and increased risk of wear and tear on the robot. To speed up training, images are often cropped and downscaled resulting in a smaller field of view and loss of valuable high-frequency data. In this paper, we propose training the vision system using supervised learning prior to training robotic actuation using Deep Deterministic Policy Gradient (DDPG). The vision system uses a software retina, based on the mammalian retino-cortical transform, to preprocess full-size images to compress image data while preserving the full field of view and high-frequency visual information around the fixation point prior to processing by a Deep Convolutional Neural Network (DCNN) to extract visual state information. Using the vision system to preprocess the environment improves the agent's sample complexity and network update speed leading to significantly faster training with reduced image data loss. Our method is used to train a DRL system to control a real Baxter robot's arm, processing full-size images captured by an in-wrist camera to locate an object on a table and centre the camera over it by actuating the robot arm.",https://ieeexplore.ieee.org/document/9207332/,2020 International Joint Conference on Neural Networks (IJCNN),19-24 July 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.23919/ICCAS50221.2020.9268370,Deep Reinforcement Learning-based ROS-Controlled RC Car for Autonomous Path Exploration in the Unknown Environment,IEEE,Conferences,"Nowadays, Deep reinforcement learning has become the front runner to solve problems in the field of robot navigation and avoidance. This paper presents a LiDAR-equipped RC car trained in the GAZEBO environment using the deep reinforcement learning method. This paper uses reshaped LiDAR data as the data input of the neural architecture of the training network. This paper also presents a unique way to convert the LiDAR data into a 2D grid map for the input of training neural architecture. It also presents the test result from the training network in different GAZEBO environment. It also shows the development of hardware and software systems of embedded RC car. The hardware system includes-Jetson AGX Xavier, teensyduino and Hokuyo LiDAR; the software system includes-ROS and Arduino C. Finally, this paper presents the test result in the real world using the model generated from training simulation.",https://ieeexplore.ieee.org/document/9268370/,"2020 20th International Conference on Control, Automation and Systems (ICCAS)",13-16 Oct. 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ASCC.2017.8287420,Deep learning for picking point detection in dense cluster,IEEE,Conferences,"This paper considers the problem of picking objects in cluster. This requires the robot to reliably detect the picking point for the known or unseen objects under the environment with occlusion, disorder and a variety of objects. We present a novel pipeline to detect picking point based on deep convolutional neural network (CNN). A two-dimensional picking configuration is proposed, thus an extensive data augmentation strategy is enabled and a labeled dataset is established quickly and easily. At last, we demonstrate the implementation of our method on a real robot and show that our method can accurately detect picking point of unseen objects and achieve a pick success of 91% in cluster bin-picking scenario.",https://ieeexplore.ieee.org/document/8287420/,2017 11th Asian Control Conference (ASCC),17-20 Dec. 2017,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS.2017.8206046,Deep predictive policy training using reinforcement learning,IEEE,Conferences,"Skilled robot task learning is best implemented by predictive action policies due to the inherent latency of sensorimotor processes. However, training such predictive policies is challenging as it involves finding a trajectory of motor activations for the full duration of the action. We propose a data-efficient deep predictive policy training (DPPT) framework with a deep neural network policy architecture which maps an image observation to a sequence of motor activations. The architecture consists of three sub-networks referred to as the perception, policy and behavior super-layers. The perception and behavior super-layers force an abstraction of visual and motor data trained with synthetic and simulated training samples, respectively. The policy super-layer is a small subnetwork with fewer parameters that maps data in-between the abstracted manifolds. It is trained for each task using methods for policy search reinforcement learning. We demonstrate the suitability of the proposed architecture and learning framework by training predictive policies for skilled object grasping and ball throwing on a PR2 robot. The effectiveness of the method is illustrated by the fact that these tasks are trained using only about 180 real robot attempts with qualitative terminal rewards.",https://ieeexplore.ieee.org/document/8206046/,2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),24-28 Sept. 2017,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/RCAR49640.2020.9303294,Deep-Learning Based Robotic Manipulation of Flexible PCBs,IEEE,Conferences,"In the past 10 years, due to the fast development of 3C industries such as mobile phones and computers, people have higher requirements for the automatic soldering technology of flexible PCBs. However, the deformation and the small size of flexible PCBs open up significant challenges to robotic soldering. This paper proposes a deep-learning based manipulation scheme for automatic soldering of flexible PCBs. The proposed controller can enable the robot to automatically contact the flexible PCB first, then actively control the flexible PCB to the desired position with the visual feedback, and finally, the soldering machine will solder the flexible PCBs smoothly. First, the approach of deep learning is used to detect the position of the solder pad (feature). Then, the vision-based controller drives the robot to manipulate the solder pad to the desired position, such that the soldering machine can work to solder two pieces of flexible PCBs together. The use of a deep learning approach can explore the human’s experience to improve the accuracy of detection and hence deals with the issues of clustered environment, change of illumination, and different initial position, etc. The proposed detection approach and control scheme is implemented in a soldering robot for flexible PCBs and the results validate the performance of the proposed methods.",https://ieeexplore.ieee.org/document/9303294/,2020 IEEE International Conference on Real-time Computing and Robotics (RCAR),28-29 Sept. 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CVPR.2019.00346,DenseFusion: 6D Object Pose Estimation by Iterative Dense Fusion,IEEE,Conferences,"A key technical challenge in performing 6D object pose estimation from RGB-D image is to fully leverage the two complementary data sources. Prior works either extract information from the RGB image and depth separately or use costly post-processing steps, limiting their performances in highly cluttered scenes and real-time applications. In this work, we present DenseFusion, a generic framework for estimating 6D pose of a set of known objects from RGB-D images. DenseFusion is a heterogeneous architecture that processes the two data sources individually and uses a novel dense fusion network to extract pixel-wise dense feature embedding, from which the pose is estimated. Furthermore, we integrate an end-to-end iterative pose refinement procedure that further improves the pose estimation while achieving near real-time inference. Our experiments show that our method outperforms state-of-the-art approaches in two datasets, YCB-Video and LineMOD. We also deploy our proposed method to a real robot to grasp and manipulate objects based on the estimated pose.",https://ieeexplore.ieee.org/document/8953386/,2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),15-20 June 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA.2013.6631325,Deploying artificial landmarks to foster data association in simultaneous localization and mapping,IEEE,Conferences,"Data association is an essential problem in simultaneous localization and mapping. It is hard to solve correctly, especially in ambiguous environments. We consider a scenario where the robot can ease the data association problem by deploying a limited number of uniquely identifiable artificial landmarks along its path and use them afterwards as fixed anchors. Obviously, the choice of the positions where the robot should drop these markers is crucial as poor choices might prevent the robot from establishing accurate data associations. In this paper, we present a novel approach for learning when to drop the landmarks so as to optimize the data association performance. We use Monte Carlo reinforcement learning for computing an optimal policy and apply a statistical convergence test to decide if the policy is converged and the learning process can be stopped. Extensive experiments also carried out with a real robot demonstrate that the data association performance using landmarks deployed according to our learned policies is significantly higher compared to other strategies.",https://ieeexplore.ieee.org/document/6631325/,2013 IEEE International Conference on Robotics and Automation,6-10 May 2013,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICSSIT48917.2020.9214125,Design & implementation of real time autonomous car by using image processing & IoT,IEEE,Conferences,"Because of the inaccessibility of Vehicle-to-Infrastructure correspondence in the present delivering frameworks, (TLD), Traffic Sign Detection and path identification are as yet thought to be a significant task in self-governing vehicles and Driver Assistance Systems (DAS) or Self Driving Car. For progressively exact outcome, businesses are moving to profound Neural Network Models Like Convolutional Neural Network (CNN) as opposed to Traditional models like HOG and so forth. Profound neural Network can remove and take in increasingly unadulterated highlights from the Raw RGB picture got from nature. In any case, profound neural systems like CNN have a highly complex calculation. This paper proposes an Autonomous vehicle or robot that can identify the diverse article in condition and group them utilizing CNN model and through this information can take some continuous choice which can be utilized in the Self Driving vehicle or Autonomous Car or Driving Assistant System (DAS).",https://ieeexplore.ieee.org/document/9214125/,2020 Third International Conference on Smart Systems and Inventive Technology (ICSSIT),20-22 Aug. 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/FPL.2005.1515790,Design and FPGA implementation of an embedded real-time biologically plausible spiking neural network processor,IEEE,Conferences,"The implementation of a large scale, leaky-integrate-and-fire neural network processor using the Xilinx Virtex-II family of field programmable gate array (FPGA) is presented. The processor has been designed to model biologically plausible networks of spiking neurons in real-time to assist with the control of a mobile robot. The real-time constraint has led to a re-evaluation of some of the established architectural and algorithmic features of previous spiking neural network based hardware. The design was coded and simulated using Handel-C hardware description language (HDL) and the DK3 design suite from Celoxica. The processor has been physically implemented and tested on a RC200 development board, also from Celoxica.",https://ieeexplore.ieee.org/document/1515790/,"International Conference on Field Programmable Logic and Applications, 2005.",24-26 Aug. 2005,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ITAIC.2019.8785457,Design and implementation of a cooperative dual body vehicle,IEEE,Conferences,"In order to reduce the over-dependence of low-cost underwater vehicles on cables, and considering that it is difficult to transmit radio signals underwater, proposeding a design scheme of vehicle combining unmanned ship with underwater vehicles on the basis of the ROS platform (robot operating system).The work shows that under the condition of reasonable utilization of ROS compatibility, advantages of USV (unmanned surface vessel) and ROV (remote operating vehicle) can be complemented greatly. The USV shares part of the functions of the underwater working module, while providing auxiliary means for inertial navigation devices, it maintains better endurance and maneuverability. ROV which focuses on underwater operation has practical functions and low volume, thus saving production costs. This system has navigation performance, intelligent level, remote control ability and expansion ability, which can provide reference for the design of some low cost underwater vehicles in a way.",https://ieeexplore.ieee.org/document/8785457/,2019 IEEE 8th Joint International Information Technology and Artificial Intelligence Conference (ITAIC),24-26 May 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICMLC.2009.5212155,Design and implementation of a visual servo system,IEEE,Conferences,"Robot visual servo system has an important status in robotic research and application, also has a decisive influence on robot intelligence. According to robotic kinematics and dynamics, the method of visual servo controller based on position is adopted in this paper. Binocular camera stereo vision technology and a kind of real-time control card Q8 based on MATLAB are used in motion control. The paper describes the control structure, hardware, software, designs a visual servo system platform and implements the real-time mission that track, capture or access moving targets on the platform.",https://ieeexplore.ieee.org/document/5212155/,2009 International Conference on Machine Learning and Cybernetics,12-15 July 2009,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/TENCON.2016.7848000,Design of hardware circuit based on a neural network model for rapid detection of center of gravity position,IEEE,Conferences,"This paper proposes a rapid detection method for the center of gravity based on a neural network model. It is suitable for the rapid response requirement such as attitude control of a gait robot or real time torque control of a running car. The proposed method detects the center of gravity position on a straight line by using only the hardware circuit composing of common electronic devices instead of software, microprocessor and AD converter. The circuit employs some neural based comparators without the learning function to simplify the circuit structure. The detection circuit using some parallel processing neural comparators rapidly detects the center of gravity position on a straight line. In this paper, the circuit is designed and fabricated with electronic devices, and the circuit experiment shows the performance of the position detection.",https://ieeexplore.ieee.org/document/7848000/,2016 IEEE Region 10 Conference (TENCON),22-25 Nov. 2016,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS.1999.813052,Designing rhythmic motions using neural oscillators,IEEE,Conferences,"Neural oscillators offer simple and robust solutions to problems such as locomotion and dynamic manipulation. However, the parameters of these systems are notoriously difficult to tune. This paper presents an analysis technique which alleviates the difficulty of tuning. The method is based on describing function analysis, and can predict the steady state motion of the system, analyze stability, and be used to determine robustness to system changes. The method is illustrated using a number of design examples including an implementation of juggling on a real robot.",https://ieeexplore.ieee.org/document/813052/,Proceedings 1999 IEEE/RSJ International Conference on Intelligent Robots and Systems. Human and Environment Friendly Robots with High Intelligence and Emotional Quotients (Cat. No.99CH36289),17-21 Oct. 1999,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/DEVLRN.2018.8761037,Developmental Bayesian Optimization of Black-Box with Visual Similarity-Based Transfer Learning,IEEE,Conferences,"We present a developmental framework based on a long-term memory and reasoning mechanisms (Vision Similarity and Bayesian Optimisation). This architecture allows a robot to optimize autonomously hyper-parameters that need to be tuned from any action and/or vision module, treated as a black-box. The learning can take advantage of past experiences (stored in the episodic and procedural memories) in order to warm-start the exploration using a set of hyper-parameters previously optimized from objects similar to the new unknown one (stored in a semantic memory). As example, the system has been used to optimized 9 continuous hyper-parameters of a professional software (Kamido) both in simulation and with a real robot (industrial robotic arm Fanuc) with a total of 13 different objects. The robot is able to find a good object-specific optimization in 68 (simulation) or 40 (real) trials. In simulation, we demonstrate the benefit of the transfer learning based on visual similarity, as opposed to an amnesic learning (i.e. learning from scratch all the time). Moreover, with the real robot, we show that the method consistently outperforms the manual optimization from an expert with less than 2 hours of training time to achieve more than 88% of success.",https://ieeexplore.ieee.org/document/8761037/,2018 Joint IEEE 8th International Conference on Development and Learning and Epigenetic Robotics (ICDL-EpiRob),17-20 Sept. 2018,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA48506.2021.9562061,Dexterous Manoeuvre through Touch in a Cluttered Scene,IEEE,Conferences,"Manipulation in a densely cluttered environment creates complex challenges in perception to close the control loop, many of which are due to the sophisticated physical interaction between the environment and the manipulator. Drawing from biological sensory-motor control, to handle the task in such a scenario, tactile sensing can be used to provide an additional dimension of the rich contact information from the interaction for decision making and action selection to manoeuvre towards a target. In this paper, a new tactile-based motion planning and control framework based on bioinspiration is proposed and developed for a robot manipulator to manoeuvre in a cluttered environment. An iterative two-stage machine learning approach is used in this framework: an autoencoder is used to extract important cues from tactile sensory readings while a reinforcement learning technique is used to generate optimal motion sequence to efficiently reach the given target. The framework is implemented on a KUKA LBR iiwa robot mounted with a SynTouch BioTac tactile sensor and tested with real-life experiments. The results show that the system is able to move the end-effector through the cluttered environment to reach the target effectively.",https://ieeexplore.ieee.org/document/9562061/,2021 IEEE International Conference on Robotics and Automation (ICRA),30 May-5 June 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CCTA41146.2020.9206279,Direct Force Feedback using Gaussian Process based Model Predictive Control,IEEE,Conferences,"Many robotic applications require control of the applied forces or moments. Model predictive control allows for the direct or indirect control of forces, while taking constraints into account. However, challenges arise when the robot environment that affects the force is highly variable, uncertain and difficult to model. Learning supported model predictive control makes it possible to combine the advantages of optimal control, such as the explicit consideration of constraints, with the advantages of machine learning, such as adaptive data-based modeling. In this paper Gaussian processes are used to model the contact forces that are applied in model predictive force control. The Gaussian process learns the static output mapping describing the interaction of the robot with the environment. It is shown that stability guarantees can be derived in a similar way as in classical predictive control. A proof-of-concept experimental implementation of a direct hybrid position force controller for a lightweight robot shows real-time feasibility.",https://ieeexplore.ieee.org/document/9206279/,2020 IEEE Conference on Control Technology and Applications (CCTA),24-26 Aug. 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/Agro-Geoinformatics.2017.8047016,Disease detection on the leaves of the tomato plants by using deep learning,IEEE,Conferences,"The aim of this work is to detect diseases that occur on plants in tomato fields or in their greenhouses. For this purpose, deep learning was used to detect the various diseases on the leaves of tomato plants. In the study, it was aimed that the deep learning algorithm should be run in real time on the robot. So the robot will be able to detect the diseases of the plants while wandering manually or autonomously on the field or in the greenhouse. Likewise, diseases can also be detected from close-up photographs taken from plants by sensors built in fabricated greenhouses. The examined diseases in this study cause physical changes in the leaves of the tomato plant. These changes on the leaves can be seen with RGB cameras. In the previous studies, standard feature extraction methods on plant leaf images to detect diseases have been used. In this study, deep learning methods were used to detect diseases. Deep learning architecture selection was the key issue for the implementation. So that, two different deep learning network architectures were tested first AlexNet and then SqueezeNet. For both of these deep learning networks training and validation were done on the Nvidia Jetson TX1. Tomato leaf images from the PlantVillage dataset has been used for the training. Ten different classes including healthy images are used. Trained networks are also tested on the images from the internet.",https://ieeexplore.ieee.org/document/8047016/,2017 6th International Conference on Agro-Geoinformatics,7-10 Aug. 2017,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCAR.2017.7942676,Door and cabinet recognition using Convolutional Neural Nets and real-time method fusion for handle detection and grasping,IEEE,Conferences,"In this paper we present a new method that robustly identifies doors, cabinets and their respective handles, with special emphasis on extracting useful features from handles to be then manipulated. The novelty of this system relies on the combination of a Convolutional Neural Net (CNN), as a form of reducing the search space, several methods to extract point cloud data and a mobile robot to interact with the objects. The framework consists of the following components: The implementation of a CNN to extract a Region of Interest (ROI) from an image corresponding to a door or cabinet. Several vision based techniques to detect handles inside the ROI and its 3D positioning. A complementary plane segmentation method to differentiate door/cabinet from the handle. An algorithm to fuse both approaches robustly and extract essential information from the handle for robotic grasping (i.e. handle point cloud, door plane model, grasping locations, turning orientation, orthogonal vector to door). A mobile robot for grasping the handle. The system assumes no prior knowledge of the environment.",https://ieeexplore.ieee.org/document/7942676/,"2017 3rd International Conference on Control, Automation and Robotics (ICCAR)",24-26 April 2017,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/RoboSoft51838.2021.9479353,DroneTrap: Drone Catching in Midair by Soft Robotic Hand with Color-Based Force Detection and Hand Gesture Recognition,IEEE,Conferences,"The paper proposes a novel concept of docking drones to make this process as safe and fast as possible. The idea behind the project is that a robot with a soft gripper grasps the drone in midair. The human operator navigates the robotic arm with the ML-based gesture recognition interface. The 3-finger robot hand with soft fingers is equipped with touch sensors, making it possible to achieve safe drone catching and avoid inadvertent damage to the drone's propellers and motors. Additionally, the soft hand is featured with a unique color-based force estimation technology based on a computer vision (CV) system. Moreover, the visual color-changing system makes it easier for the human operator to interpret the applied forces.Without any additional programming, the operator has full real-time control of robot's motion and task execution by wearing a mocap glove with gesture recognition, which was developed and applied for the high-level control of DroneTrap.The experimental results revealed that the developed color-based force estimation can be applied for rigid object capturing with high precision (95.3%). The proposed technology can potentially revolutionize the landing and deployment of drones for parcel delivery on uneven ground, structure maintenance and inspection, risque operations, and etc.",https://ieeexplore.ieee.org/document/9479353/,2021 IEEE 4th International Conference on Soft Robotics (RoboSoft),12-16 April 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICAR46387.2019.8981552,Dynamic Movement Primitives: Volumetric Obstacle Avoidance,IEEE,Conferences,"Dynamic Movement Primitives (DMPs) are a framework for learning a trajectory from a demonstration. The trajectory can be learned efficiently after only one demonstration, and it is immediate to adapt it to new goal positions and time duration. Moreover, the trajectory is also robust against perturbations. However, obstacle avoidance for DMPs is still an open problem. In this work, we propose an extension of DMPs to support volumetric obstacle avoidance based on the use of superquadric potentials. We show the advantages of this approach when obstacles have known shape, and we extend it to unknown objects using minimal enclosing ellipsoids. A simulation and experiments with a real robot validate the framework, and we make freely available our implementation.",https://ieeexplore.ieee.org/document/8981552/,2019 19th International Conference on Advanced Robotics (ICAR),2-6 Dec. 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/DEVLRN.2008.4640818,Dynamic field theory of sequential action: A model and its implementation on an embodied agent,IEEE,Conferences,"How sequences of actions are learned, remembered, and generated is a core problem of cognition. Despite considerable theoretical work on serial order, it typically remains unexamined how physical agents may direct sequential actions at the environment within which they are embedded. Situated physical agents face a key problem - the need to accommodate variable amounts of time it takes to terminate each individual action within the sequence. Here we examine how Dynamic Field Theory (DFT), a neuronally grounded dynamical systems approach to embodied cognition, may address sequence learning and sequence generation. To demonstrate that the proposed DFT solution works with real and potentially noisy sensory systems as well as with real physical action systems, we implement the approach on a simple autonomous robot. We demonstrate how the robot acquires sequences from experiencing the associated sensory information and how the robot generates sequences based on visual information from its environment using low-level visual features.",https://ieeexplore.ieee.org/document/4640818/,2008 7th IEEE International Conference on Development and Learning,9-12 Aug. 2008,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICSMC.1995.537949,Dynamic path planning,IEEE,Conferences,"Path planning is dynamic when the path is continually recomputed as more information becomes available. A computational framework for dynamic path planning is proposed which has the ability to provide navigational directions during the computation of the plan. Path planning is performed using a potential field approach. We use a specific type of potential function-a harmonic function-which has no local minima. The implementation is parallel and consists of a collection of communicating processes, across a network of SPARC & SGI workstations using a message passing software package called PVM. The computation of the plan is performed independently of the execution of the plan. A hierarchical coarse-to-fine procedure is used to guarantee a correct control strategy at the expense of accuracy. We have successfully navigated a Nomad robot around our lab space with no a priori map in real-time. The result of the described approach is a parallel implementation which permits dynamic path planning using available processor resources.",https://ieeexplore.ieee.org/document/537949/,"1995 IEEE International Conference on Systems, Man and Cybernetics. Intelligent Systems for the 21st Century",22-25 Oct. 1995,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS45743.2020.9341406,EU Long-term Dataset with Multiple Sensors for Autonomous Driving,IEEE,Conferences,"The field of autonomous driving has grown tremendously over the past few years, along with the rapid progress in sensor technology. One of the major purposes of using sensors is to provide environment perception for vehicle understanding, learning and reasoning, and ultimately interacting with the environment. In this paper, we first introduce a multisensor platform allowing vehicle to perceive its surroundings and locate itself in a more efficient and accurate way. The platform integrates eleven heterogeneous sensors including various cameras and lidars, a radar, an IMU (Inertial Measurement Unit), and a GPS-RTK (Global Positioning System / Real-Time Kinematic), while exploits a ROS (Robot Operating System) based software to process the sensory data. Then, we present a new dataset (https://epan-utbm.github.io/utbm_robocar_dataset/) for autonomous driving captured many new research challenges (e.g. highly dynamic environment), and especially for long-term autonomy (e.g. creating and maintaining maps), collected with our instrumented vehicle, publicly available to the community.",https://ieeexplore.ieee.org/document/9341406/,2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),24 Oct.-24 Jan. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ROMAN.2011.6005223,Effect of human guidance and state space size on Interactive Reinforcement Learning,IEEE,Conferences,"The Interactive Reinforcement Learning algorithm enables a human user to train a robot by providing rewards in response to past actions and anticipatory guidance to guide the selection of future actions. Past work with software agents has shown that incorporating user guidance into the policy learning process through Interactive Reinforcement Learning significantly improves the policy learning time by reducing the number of states the agent explores. We present the first study of Interactive Reinforcement Learning in real-world robotic systems. We report on four experiments that study the effects that teacher guidance and state space size have on policy learning performance. We discuss modifications made to apply Interactive Reinforcement Learning to a real-world system and show that guidance significantly reduces the learning rate, and that its positive effects increase with state space size.",https://ieeexplore.ieee.org/document/6005223/,2011 RO-MAN,31 July-3 Aug. 2011,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SSIAI.2018.8470351,Efficient Face And Gesture Recognition For Time Sensitive Application,IEEE,Conferences,"Face recognition systems are used in various fields such as biometric authentication, security enhancement, automobile control and user detection. This research is focused on developing a model to control a system using gestures, while simultaneously implementing continuous facial recognition to avoid unauthorized access. An effective face recognition system is developed and applied in conjunction with a gesture recognition system to control a wireless robot in real-time. The facial recognition system extracts the face using the Viola-Jones algorithm which utilizes Haar like features along with Adaboost training. This is followed by a Convolution Neural Network (CNN) based feature extractor and Support Vector Machine (SVM) to recognize the face. The gesture recognition is facilitated by using color segmentation, which involves extracting the skin tone of the detected face and using this to detect the position of hand. The gesture is obtained by tracking the hand using the Kanade-Lucas-Tomasi (KLT) algorithm. In this research, we additionally utilize a background subtraction model so as to extract the foreground and reduce the misclassifications. Such a technique highly improves the performance of the facial and gesture detector in complex and cluttered environments. The performance of the face detector was tested on different databases including the ORL, Caltech and Faces96 database. The efficacy of this system in controlling a robot in real-time has also been demonstrated in this research. It provides an accuracy of 94.44% for recognizing faces and greater than 90.8% for recognizing gestures in real-time applications. Such a system is seen to have superior performance coupled with a relatively lower computation requirement in comparison to existing techniques.",https://ieeexplore.ieee.org/document/8470351/,2018 IEEE Southwest Symposium on Image Analysis and Interpretation (SSIAI),8-10 April 2018,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA40945.2020.9197211,Efficient Pig Counting in Crowds with Keypoints Tracking and Spatial-aware Temporal Response Filtering,IEEE,Conferences,"Pig counting is a crucial task for large-scale pig farming. Pigs are usually visually counted by human. But this process is very time-consuming and error-prone. Few studies in literature developed automated pig counting method. The existing works only focused on pig counting using single image, and its level of accuracy faced challenges due to pig movements, occlusion and overlapping. Especially, the field of view of a single image is very limited, and could not meet the needs of pig counting for large pig grouping houses. Towards addressing these challenges, we presented a real-time automated pig counting system in crowds using only one monocular fisheye camera with an inspection robot. Our system showed that it achieved performance superior to human. Our pipeline began with a novel bottom-up pig detection algorithm to avoid false negatives due to overlapping, occlusion and deformable pig shapes. This detection included a deep convolution neural network (CNN) for pig body part keypoints detection and the keypoints association method to identify individual pigs. It then employed an efficient on-line tracking method to associate pigs across image frames. Finally, pig counts were estimated by a novel spatial-aware temporal response filtering (STRF) method to suppress false positives caused by pig or camera movements or tracking failures. The whole pipeline has been deployed in an edge computing device, and demonstrated the effectiveness.",https://ieeexplore.ieee.org/document/9197211/,2020 IEEE International Conference on Robotics and Automation (ICRA),31 May-31 Aug. 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/OCEANS.2014.7003085,Efficient multi-AUV cooperation using semantic knowledge representation for underwater archaeology missions,IEEE,Conferences,"Advances in the fields of communication technology and software, electrical and mechanical engineering enable the replacement of a single robot by cooperative robotic team in highly demanding applications, such as search and rescue. A robotic team could perform better than a single robot, if certain challenges, such as action planning, coordination, and decision making, are successfully tackled. One key factor for the successful performance of a robotic team is the multi-robot task allocation. Specifically, the challenge is to define which robot executes which task, considering an efficient solution for the successful completion of the complex mission. The task allocation could be even more challenging when real-world communication constraints and uncertainties are presented, such as limited bandwidth, high latency and high packet loss. In the current study, we attempt to resolve the issue of a cooperative robotic team under communication constraints. To reach this goal, the use of a distributed world model for multi-robot task allocation is proposed. This ontology based distributed world model is capable of successfully handling to a great extent the aforementioned communications limitations, thus allowing successful mission execution even under harsh communication conditions. An efficient centralised task allocation mechanism, using k-means clustering, is described, and its performance is compared to a greedy centralised task allocation method. Experimental simulation results indicate that the efficient method performs better on average than the greedy one, without extra time requirements.",https://ieeexplore.ieee.org/document/7003085/,2014 Oceans - St. John's,14-19 Sept. 2014,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IHMSC.2013.225,Embedded Motion Controller Design Based on RTEX Network,IEEE,Conferences,"Adopting embedded framework and network communication mode, a kind of multi-axis embedded motion controller hardware platform design for the Panasonic A5N drive is proposed, which is Based on the modular control core (ARM + FPGA) and can adapt to the new real-time network RTEX. The processes of controller's functional design, hardware design and software design are explained in detail. Up to now, the motion controller's hardware platform have been completed and verified by communication experiments, position and velocity control experiments, and the results of which show that the controller have good scalability, reliability, flexibility and openness and could well meet the needs of the multi-axis robot's motion control.",https://ieeexplore.ieee.org/document/6642753/,2013 5th International Conference on Intelligent Human-Machine Systems and Cybernetics,26-27 Aug. 2013,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/RCAR47638.2019.9043931,Embedded UAV Real-Time Visual Object Detection and Tracking,IEEE,Conferences,"The use of camera-equipped Unmanned Aerial Vehicles (UAVs, or “drones”) for a wide range of aerial video capturing applications, including media production, surveillance, search and rescue operations, etc., has exploded in recent years. Technological progress has led to commercially available UAVs with a degree of cognitive autonomy and perceptual capabilities, such as automated, on-line detection and tracking of target objects upon the captured footage. However, the limited computational hardware, the possibly high camera-to-target distance and the fact that both the UAV/camera and the target(s) are moving, makes it challenging to achieve both high accuracy and stable real-time performance. In this paper, the current state-of-the-art on real-time object detection/tracking is overviewed. Additionally, a relevant, modular implementation suitable for on-drone execution (running on top of the popular Robot Operating System) is presented and empirically evaluated on a number of relevant datasets. The results indicate that a sophisticated, neural network-based detection and tracking system can be deployed at real-time even on embedded devices.",https://ieeexplore.ieee.org/document/9043931/,2019 IEEE International Conference on Real-time Computing and Robotics (RCAR),4-9 Aug. 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SEAMS51251.2021.00015,Enhancing Human-in-the-Loop Adaptive Systems through Digital Twins and VR Interfaces,IEEE,Conferences,"Self-adaptation approaches usually rely on closed-loop controllers that avoid human intervention from adaptation. While such fully automated approaches have proven successful in many application domains, there are situations where human involvement in the adaptation process is beneficial or even necessary. For such “human-in-the-loop” adaptive systems, two major challenges, namely transparency and controllability, have to be addressed to include the human in the self-adaptation loop. Transparency means that relevant context information about the adaptive systems and its context is represented based on a digital twin enabling the human an immersive and realistic view. Concerning controllability, the decision-making and adaptation operations should be managed in a natural and interactive way. As existing human-in-the-loop adaptation approaches do not fully cover these aspects, we investigate alternative human-in-the-loop strategies by using a combination of digital twins and virtual reality (VR) interfaces. Based on the concept of the digital twin, we represent a self-adaptive system and its respective context in a virtual environment. With the help of a VR interface, we support an immersive and realistic human involvement in the self-adaptation loop by mirroring the physical entities of the real world to the VR interface. For integrating the human in the decision-making and adaptation process, we have implemented and analyzed two different human-in-the-loop strategies in VR: a procedural control where the human can control the decision making-process and adaptations through VR interactions (human-controlled) and a declarative control where the human specifies the goal state and the configuration is delegated to an AI planner (mixed-initiative). We illustrate and evaluate our approach based on an autonomic robot system that is accessible and controlled through a VR interface.",https://ieeexplore.ieee.org/document/9462035/,2021 International Symposium on Software Engineering for Adaptive and Self-Managing Systems (SEAMS),18-24 May 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISIC.2001.971506,Enhancing control architectures using CORBA,IEEE,Conferences,"The nature of applied research in intelligent robot controllers makes having a versatile software architecture a real need for exploring alternative designs in robotic minds construction. The paper presents an experiment in the adaptation of a multi-robot cooperation architecture to a CORBA-based schema. The paper demonstrates not only the feasibility but the convenience of using, state-of-the-art, modular software technologies for the construction of advanced intelligent controllers.",https://ieeexplore.ieee.org/document/971506/,Proceeding of the 2001 IEEE International Symposium on Intelligent Control (ISIC '01) (Cat. No.01CH37206),5-7 Sept. 2001,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/EHB52898.2021.9657712,Experimental Testing and Implementation of a Force &#x2013; Torque Sensor in Automated Percutaneous Needle Insertion Instruments,IEEE,Conferences,"The paper aims to describe the forces and torques that appear during the percutaneous needle insertion (through in vivo laboratory experiments) and investigate the benefits of a force/torque sensor integrated on an automated needle insertion instrument used as a robot end-effector. Based on the experimental results, several events occurring during the needle insertion were identified and described. Furthermore, a solution to implement a force/torque sensor into a needle insertion instrument was proposed.",https://ieeexplore.ieee.org/document/9657712/,2021 International Conference on e-Health and Bioengineering (EHB),18-19 Nov. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/RIOS.2016.7529488,Experimental stability study of an Octorotor using an intelligent controller,IEEE,Conferences,"In this paper, the attitude stabilization problem of an Octorotor with coaxial motors is studied. To this end, the new method of intelligent adaptive control is presented. The designed controller which includes fuzzy and PID controllers, is completed by resistant adaptive function of approximate external disturbance and changing in the dynamic model. In fact, the regulation factor of PID controller is done by the fuzzy logic system. At first, the Fuzzy-PID and PID controllers are simulated in MATLAB/Simulink. Then, the Fuzzy-PID controller is implemented on the Octorotor with coaxial motors as online auto-tuning. Also, LabVIEW software has been used for tests and the performance analysis of the controllers. All of this experimental operation is done in indoor environment in the presence of wind as disturbance in the hovering operation. All of these operations are real-time and telemetry wireless is done by network connection between the robot and ground station in the LABVIEW software. Finally, the controller efficiency and results are studied.",https://ieeexplore.ieee.org/document/7529488/,2016 Artificial Intelligence and Robotics (IRANOPEN),9-9 April 2016,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICVRIS.2018.00078,Exploration of Computer Emotion Decision Based on Artificial Intelligence,IEEE,Conferences,"To carry out the discussion of computer emotion decision based on artificial intelligence, first of all, based on the psychological experiment paradigm of children's game task, and the test process of the artificial emotion generating engine was completed on the emotional spontaneous transfer and the stimulus transfer model. Secondly, the reasoning method and analytic hierarchy process (AHP) were introduced into the multi system, and a kind of multi emotion decision model based on the emotion reasoning was constructed. The hierarchical structure method was used to solve the complex decision problem of the humanoid robot in the intelligent home environment. Then, based on the emotion energy theory, a mood state regulation algorithm based on the combination of HMM-based spontaneous transfer and stimulus transfer was established. In addition, on this basis, the design and implementation of humanoid robot associative memory model was realized. Finally, the theory and algorithm were integrated into the interactive platform of human-computer expression, and the validity of the model was analysed and verified. The results showed that, on this robot platform, the process of human-computer interaction and cooperation which integrated emotion evaluation, emotional decision, associative memory and emotion regulation was realized. As a result, the computer emotion decision based on artificial intelligence can be well applied in many fields.",https://ieeexplore.ieee.org/document/8531405/,2018 International Conference on Virtual Reality and Intelligent Systems (ICVRIS),10-11 Aug. 2018,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICORR.2019.8779424,Exploring the Impact of Machine-Learned Predictions on Feedback from an Artificial Limb,IEEE,Conferences,"Learning to get by without an arm or hand can be very challenging, and existing prostheses do not yet fill the needs of individuals with amputations. One promising solution is to improve the feedback from the device to the user. Towards this end, we present a simple machine learning interface to supplement the control of a robotic limb with feedback to the user about what the limb will be experiencing in the near future. A real-time prediction learner was implemented to predict impact-related electrical load experienced by a robot limb; the learning system's predictions were then communicated to the device's user to aid in their interactions with a workspace. We tested this system with five able-bodied subjects. Each subject manipulated the robot arm while receiving different forms of vibrotactile feedback regarding the arm's contact with its workspace. Our trials showed that using machine-learned predictions as a basis for feedback led to a statistically significant improvement in task performance when compared to purely reactive feedback from the device. Our study therefore contributes initial evidence that prediction learning and machine intelligence can benefit not just control, but also feedback from an artificial limb. We expect that a greater level of acceptance and ownership can be achieved if the prosthesis itself takes an active role in transmitting learned knowledge about its state and its situation of use.",https://ieeexplore.ieee.org/document/8779424/,2019 IEEE 16th International Conference on Rehabilitation Robotics (ICORR),24-28 June 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/RCAR52367.2021.9517666,FPGA-based Deep Learning Acceleration for Visual Grasping Control of Manipulator,IEEE,Conferences,"The vision-based robotic arm control system is an important solution for intelligent production, and the robotic arm visual grasping system based on deep learning is an important branch. Aiming at the requirements of fast visual recognition speed, low power consumption and high precision of mobile visual grasping robot, a deep learning target detection scheme based on FPGA hardware acceleration is proposed. Use Vivado and Petalinux development kit to build the software and hardware system, then deploy YOLOv3 model in the system. Experiments show that the solution meets the demand of robotic arm visual grasping, and the real-time performance is better. The recognition speed is 18 times that of the CPU, the power consumption is 1/13 of the GPU, and the cost is lower.",https://ieeexplore.ieee.org/document/9517666/,2021 IEEE International Conference on Real-time Computing and Robotics (RCAR),15-19 July 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA40945.2020.9197159,Fast Adaptation of Deep Reinforcement Learning-Based Navigation Skills to Human Preference,IEEE,Conferences,"Deep reinforcement learning (RL) is being actively studied for robot navigation due to its promise of superior performance and robustness. However, most existing deep RL navigation agents are trained using fixed parameters, such as maximum velocities and weightings of reward components. Since the optimal choice of parameters depends on the use-case, it can be difficult to deploy such existing methods in a variety of real-world service scenarios. In this paper, we propose a novel deep RL navigation method that can adapt its policy to a wide range of parameters and reward functions without expensive retraining. Additionally, we explore a Bayesian deep learning method to optimize these parameters that requires only a small amount of preference data. We empirically show that our method can learn diverse navigation skills and quickly adapt its policy to a given performance metric or to human preference. We also demonstrate our method in real-world scenarios.",https://ieeexplore.ieee.org/document/9197159/,2020 IEEE International Conference on Robotics and Automation (ICRA),31 May-31 Aug. 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS40897.2019.8967966,Fast and Safe Policy Adaptation via Alignment-based Transfer,IEEE,Conferences,"Applying deep reinforcement learning to physical systems, as opposed to learning in simulation, presents additional challenges in terms of sample efficiency and safety. Collecting large amounts of hardware demonstration data is time-consuming and the exploratory behavior of reinforcement learning algorithms may lead the system into dangerous states, especially during the early stages of training. To address these challenges, we apply transfer learning to reuse a previously learned policy instead of learning from scratch. In this paper, we propose a method where given a source policy, policy adaptation is performed via transfer learning to produce a target policy suitable for real-world deployment. For policy adaptation, alignment-based transfer learning is applied to trajectories generated by the source policy and their corresponding safe target trajectories. We apply this method to manipulators and show that the proposed method is applicable to both inter-task and inter-robot transfer whilst considering safety. We also show that the resulting target policy is robust and can be further improved with reinforcement learning.",https://ieeexplore.ieee.org/document/8967966/,2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),3-8 Nov. 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CIRA.2005.1554338,Faster learning in embodied systems through characteristic attitudes,IEEE,Conferences,"Classical reinforcement learning is a general learning paradigm with wide applicability in many problem domains. Where embodied agents are concerned, however, it is unable to take advantage of the structured, regular nature of the physical world to maximise learning efficiency. Here, using a model of a three joint robot arm, we show initial learning accelerated by an order of magnitude using simple constraints to produce characteristic attitudes, implemented as part of the learning algorithm. We point out possible parallels with constraints on the movement of natural organisms owing to their detailed mechanical structure. The work forms part of our EMBER framework for reinforcement learning in embodied agents introduced and developed in 2004.",https://ieeexplore.ieee.org/document/1554338/,2005 International Symposium on Computational Intelligence in Robotics and Automation,27-30 June 2005,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ROBOT.1993.292135,Fixed computation real-time sonar fusion for local navigation,IEEE,Conferences,"A system is described for the spatial and temporal fusion of multiple sonars into a dynamic model which serves as a basis for local navigation. In order to guarantee that the robot's actions remain in synchrony with the changing state of the world, there is a continuous mapping from sonar readings to local model to navigation plan and, finally, to actuator commands. Because of this tight coupling of sensing and action, the responsiveness of the system is limited by the computational power of the processor and the required fidelity of the fused model. To address this tradeoff, the system is implemented using the GAPPS/REX circuit-based language. Analysis of the resulting fixed sized circuits allows computation tradeoffs to be made between the system fidelity and the responsiveness required by the operating environment. A description is presented of the sensor fusion and navigation algorithms, as well as the results of the system controlling MITRE's mobile robot Uncle Bob.<>",https://ieeexplore.ieee.org/document/292135/,[1993] Proceedings IEEE International Conference on Robotics and Automation,2-6 May 1993,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/HUMANOIDS.2014.7041373,Footstep planning on uneven terrain with mixed-integer convex optimization,IEEE,Conferences,"We present a new method for planning footstep placements for a robot walking on uneven terrain with obstacles, using a mixed-integer quadratically-constrained quadratic program (MIQCQP). Our approach is unique in that it handles obstacle avoidance, kinematic reachability, and rotation of footstep placements, which typically have required non-convex constraints, in a single mixed-integer optimization that can be efficiently solved to its global optimum. Reachability is enforced through a convex inner approximation of the reachable space for the robot's feet. Rotation of the footsteps is handled by a piecewise linear approximation of sine and cosine, designed to ensure that the approximation never overestimates the robot's reachability. Obstacle avoidance is ensured by decomposing the environment into convex regions of obstacle-free configuration space and assigning each footstep to one such safe region. We demonstrate this technique in simple 2D and 3D environments and with real environments sensed by a humanoid robot. We also discuss computational performance of the algorithm, which is currently capable of planning short sequences of a few steps in under one second or longer sequences of 10-30 footsteps in tens of seconds to minutes on common laptop computer hardware. Our implementation is available within the Drake MATLAB toolbox [1].",https://ieeexplore.ieee.org/document/7041373/,2014 IEEE-RAS International Conference on Humanoid Robots,18-20 Nov. 2014,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IECON.2000.973216,Force control in robotic assembly under extreme uncertainty using ANN,IEEE,Conferences,"Robotic assembly operations can be performed by specifying an exact model of the operation. However, the uncertainties involved during assembly make it difficult to conceive such a model In these cases, the use of a connectionist model may be advantageous. In this paper, the design of a robotic cell based on the adaptive resonance theory artificial neural network and a PC host-slave architecture that overcame these uncertainties is presented. Different sources of uncertainty under real conditions are identified and their contribution in a typical assembly operation evaluated. The robotic system is implemented using a PUMA 761 industrial robot with six degrees of freedom (DOF) and a force/torque (F/T) sensor attached to its wrist which conveys force information to the neural network controller (NNC). Results during assembly operations are presented which validate the approach. Furthermore, the method is generic and can be implemented onto other manipulators.",https://ieeexplore.ieee.org/document/973216/,"2000 26th Annual Conference of the IEEE Industrial Electronics Society. IECON 2000. 2000 IEEE International Conference on Industrial Electronics, Control and Instrumentation. 21st Century Technologies",22-28 Oct. 2000,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AiDAS47888.2019.8970881,"Framework Of Malay Intelligent Autonomous Helper (Min@H): Text, Speech And Knowledge Dimension Towards Artificial Wisdom For Future Military Training System",IEEE,Conferences,"Industrial Revolution 4.0 is expected to improve the way of military training system. Most of the assistant systems use English for their Human Machine Interaction (HMI) such `SARA' a virtual socially aware robot assistant which exclude Malay socio-emotional aspects. This scenario opens a suggestion, to internalize socio-emotional aspects based on Malay culture, custom and beliefs to military autonomous training systems (i.e. MIN@H) that can improve the `collaborative' skills between Malaysian military personnel and the systems. Therefore, to increase the wisdom of the systems, they must have feature to capture information for their human users or helping human users to learn new knowledge and ensure the interaction is comfortable and engaging. For that reason, the systems must understand Malay language and be able to interpret emotion and expression behavior according to the Malay culture and custom, furthermore, the systems able to differentiate the level of user's understanding and build a good rapport or feeling of harmony that makes communication possible or easy between the systems and users. This concept of the systems is referred as Malay Artificial Wisdom System (AWS). There are three fundamental aspects to achieve the AWS. First, to computationally model the conversational strategies and rapport between the system and human users based-on user's understanding and system's articulation. Second, to computationally model, recognize and synthesize the emotion and expression behavior according to the Malay culture, custom and beliefs. Third, the AWS can do analytical reasoning and responding in relation to falsehood analysis and users' understanding level. Knowledge discovery and inference technique as well as HMI that cater the inputs and output of the MIN@H will be developed to accomplish the AWS concept. This program could embrace military training system in Malaysia to enhance military personnel skills and experts in various areas.",https://ieeexplore.ieee.org/document/8970881/,2019 1st International Conference on Artificial Intelligence and Data Sciences (AiDAS),19-19 Sept. 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS40897.2019.8967568,From Pixels to Buildings: End-to-end Probabilistic Deep Networks for Large-scale Semantic Mapping,IEEE,Conferences,"We introduce TopoNets, end-to-end probabilistic deep networks for modeling semantic maps with structure reflecting the topology of large-scale environments. TopoNets build a unified deep network spanning multiple levels of abstraction and spatial scales, from pixels representing geometry of local places to high-level descriptions of semantics of buildings. To this end, TopoNets leverage complex spatial relations expressed in terms of arbitrary, dynamic graphs. We demonstrate how TopoNets can be used to perform end-to-end semantic mapping from partial sensory observations and noisy topological relations discovered by a robot exploring large-scale office spaces. Thanks to their probabilistic nature and generative properties, TopoNets extend the problem of semantic mapping beyond classification. We show that TopoNets successfully perform uncertain reasoning about yet unexplored space and detect novel and incongruent environment configurations unknown to the robot. Our implementation of TopoNets achieves real-time, tractable and exact inference, which makes these new deep models a promising, practical solution to mobile robot spatial understanding at scale.",https://ieeexplore.ieee.org/document/8967568/,2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),3-8 Nov. 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/DEVLRN.2005.1490934,From Unknown Sensors and Actuators to Visually Guided Movement,IEEE,Conferences,"This paper describes a developmental system implemented on a real robot that learns a model of its own sensory and actuator apparatuses. There is no innate knowledge regarding the modality or representation of the sensoric input and the actuators, and the system relies on generic properties of the robot's world such as piecewise smooth effects of movement on sensory changes. The robot develops the model of its sensorimotor system by first performing random movements to create an informational map of the sensors. Using this map the robot then learns what effects the different possible actions have on the sensors. After this developmental process the robot can perform simple motion tracking",https://ieeexplore.ieee.org/document/1490934/,"Proceedings. The 4th International Conference on Development and Learning, 2005",19-21 July 2005,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISSE.2015.7248059,Fully integrated artificial intelligence solution for real time route tracking,IEEE,Conferences,"In this paper the authors propose a solution in which an intelligent algorithm - genetic algorithm in our case - is used to generate commands for a robot in real time, so that the robot can determine the optimal moves considering several aspects: route tracking and low power consumption. Genetic algorithms are intelligent solutions for multi-criteria optimization and using them to find solutions to the optimization problems containing constrictions. However, they were designed as algorithms running on computer and therefore cannot ensure rapid generation of the solutions. On the other hand, the problem of determining the optimal response to command a robot requires real time response. The paper presents a method for hardware implementation and integration in a FPGA circuit of a genetic algorithm, in order to accelerate the convergence and to generate solutions in real time.",https://ieeexplore.ieee.org/document/7248059/,2015 38th International Spring Seminar on Electronics Technology (ISSE),6-10 May 2015,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISIC.1994.367848,Fuzzy neural network implementation of self tuning PID control systems,IEEE,Conferences,"The fuzzy cognitive map (FCM) is a powerful universal method for representation of knowledge in various domains. The fuzzy inference engine can be implemented in the form of a network of FCMs. FCM implementation of the inference engine provides a suitable mechanism for expert control systems and information engineers to embed acquired human expertise, which is often imprecise, vague, or incomplete. The exploitation of an online learning algorithm empowers the fuzzy inference engine with the ability to modify its incomplete or possibly inconsistent knowledge base resulting in continuous improvement of the embedded knowledge. The fact that learning is an inherent feature of neural networks has inspired several researchers with the idea of using neural networks to implement fuzzy inference engines capable of learning. This paper presents a method for neural network FCM implementation of the fuzzy inference engine using the fuzzy columnar neural network architecture (FCNA). In this method the available human expertise is mapped first into an initial set of weights for the neurons. A new learning algorithm is then used to enhance the embedded knowledge in the neural network as a result of real time experience. The fuzzy inference engine (the neural network FCM) is used in computer simulations to control the speed of an underwater autonomous mobile robot. Results and computer simulation experiments are presented along with an evaluation of the new approach.<>",https://ieeexplore.ieee.org/document/367848/,Proceedings of 1994 9th IEEE International Symposium on Intelligent Control,16-18 Aug. 1994,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ROBIO.2017.8324476,Generalized framework for the parallel semantic segmentation of multiple objects and posterior manipulation,IEEE,Conferences,"The end-to-end approach presented in this paper deals with the recognition, detection, segmentation and grasping of objects, assuming no prior knowledge of the environment nor objects. The proposed pipeline is as follows: 1) Usage of a trained Convolutional Neural Net (CNN) that recognizes up to 80 different classes of objects in real time and generates bounding boxes around them. 2) An algorithm to derive in parallel the pointclouds of said regions of interest (ROI). 3) Eight different segmentation methods to remove background data and noise from the pointclouds and obtain a precise result of the semantically segmented objects. 4) Registration of the object's pointclouds over time to generate the best possible model. 5) Utilization of an algorithm to detect an array of grasping positions and orientations based mainly on the geometry of the object's model. 6) Implementation of the system on the humanoid robot MyBot, developed in the RIT Lab at KAIST. 7) An algorithm to find the bounding box of the object's model in 3D to then create a collision object and add it to the octomap. The collision checking between robot's hand and the object is removed to allow grasping using the MoveIt libraries. 8) Selection of the best grasping pose for a certain object, plus execution of the grasping movement. 9) Retrieval of the object and moving it to a desired final position.",https://ieeexplore.ieee.org/document/8324476/,2017 IEEE International Conference on Robotics and Biomimetics (ROBIO),5-8 Dec. 2017,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS.1997.655074,Generation of behavior automaton on neural network,IEEE,Conferences,"To plan behavior procedures, it is necessary for an agent to have a world model concerning the temporal sequences information. In this paper, a temporal information learning algorithm is proposed with a three layer neural network implementing the ""effectiveness of simulation accumulation"" algorithm. This algorithm can construct a ""behavior automaton"" in the neural network. From the results of some learning experiments using a mobile robot simulation, the generated automaton expresses the complexity of the simulation environments. The robot agent acquires a behavior automaton for obstacle avoidance behavior which is influenced by the simulation environment.",https://ieeexplore.ieee.org/document/655074/,Proceedings of the 1997 IEEE/RSJ International Conference on Intelligent Robot and Systems. Innovative Robotics for Real-World Applications. IROS '97,11-11 Sept. 1997,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS.1994.407570,Generation of optimal configuration for a redundant manipulator with a trained neural network,IEEE,Conferences,"Redundant manipulators have more degrees of freedom than what is absolutely necessary for performing a task. The extra degrees of freedom can be used for avoiding obstacles or to optimize certain performance indices like manipulability or task compatibility. Maximizing manipulability keeps the manipulator away from singularities and provides more velocity transmission ratios in all directions. Optimizing task compatibility improves the force/velocity transmission ratios in the specified directions. However, the real time implementation of various optimizing algorithms is difficult because of the need of large computing time. In the present work, robot configurations for an optimum performance index are computed throughout the workspace. These configurations are then used to train a layered feed forward neural network (FFNN). During operation of the robot, the trained neural net outputs optimal configurations in real-time. The neural net captures the gross behaviour of the training data rather than memorizing the individual data, as in a lookup table. Thus its output is smooth and ideally suited for control purposes. We have simulated this approach on a 3-DOF redundant planar manipulator and the results are discussed in this paper.<>",https://ieeexplore.ieee.org/document/407570/,Proceedings of IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS'94),12-16 Sept. 1994,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ROBOT.2001.933128,Generation of optimal trajectory for real system of an underactuated manipulator,IEEE,Conferences,"Trajectory of an underactuated manipulator is usually generated according to both kinematics and dynamics of the manipulator, different to that of conventional manipulator. The real trajectory by a real system may differ greatly from the trajectory generated from the dynamics model because there always exist errors in the dynamic model, and the feedback control is less effective in an underactuated manipulator. A method for generation of optimal trajectory for the real system of an underactuated manipulator with nonholonomic constraints is proposed. By using this method, the dynamics model of a real system can be improved by learning, and an optimal trajectory is generated according to the model improved sequentially. The effectiveness of the method is confirmed by experiment with a golf swinging robot. The implementation and experimental results obtained of the control method are described.",https://ieeexplore.ieee.org/document/933128/,Proceedings 2001 ICRA. IEEE International Conference on Robotics and Automation (Cat. No.01CH37164),21-26 May 2001,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IRIS.2016.8066077,Gesture based robotic arm control for meal time care using a wearable sensory jacket,IEEE,Conferences,"This work presents the development of a wireless, low cost, wearable sensor jacket for the purpose of controlling a robot arm by mimicking the motion and behaviour of a humans arm. The intended use of our system is to provide remote daily nursing care by using our system from a distant place such as a nursing home or hospital, to control a stationed robotic arm placed in an elderly or patient's home. The final system is comprised of a wearable jacket which is embedded with IMU and flex sensors to detect and track the wearers arm movements and behaviour. The system is capable of detecting up to 5 degrees of freedom of the human arm and replicate these motions using a 6 DOF robot arm. The usability, accuracy and precision of our jacket system is evaluated through a user study and the results demonstrated that our system was more accurate and easier to use for operators than a conventional robotic arm joystick controller. In a water bottle transfer task our developed wearable jacket system demonstrated an average error distance of 29.36mm from the target point, while the results using the conventional joystick demonstrated an average error distance of 37.48mm. Furthermore subjects using our system were able to complete the transfer task in an average time of 44.1s per trial which was more efficient than the joystick method in which subjects averaged 55.55s per trial. Finally, we report a feasibility study with the jacket and a subject to demonstrate the capability of this system of giving a patient water to drink. The feasibility experiment showed an 86.66% success rate in giving a patient water via video stream teleoperation control.",https://ieeexplore.ieee.org/document/8066077/,2016 IEEE International Symposium on Robotics and Intelligent Sensors (IRIS),17-20 Dec. 2016,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA46639.2022.9811874,Graph-based Cluttered Scene Generation and Interactive Exploration using Deep Reinforcement Learning,IEEE,Conferences,"We introduce a novel method to teach a robotic agent to interactively explore cluttered yet structured scenes, such as kitchen pantries and grocery shelves, by leveraging the physical plausibility of the scene. We propose a novel learning framework to train an effective scene exploration policy to discover hidden objects with minimal interactions. First, we define a novel scene grammar to represent structured clutter. Then we train a Graph Neural Network (GNN) based Scene Generation agent using deep reinforcement learning (deep RL), to manipulate this Scene Grammar to create a diverse set of stable scenes, each containing multiple hidden objects. Given such cluttered scenes, we then train a Scene Exploration agent, using deep RL, to uncover hidden objects by interactively rearranging the scene. We show that our learned agents hide and discover significantly more objects than the baselines. We present quantitative results that prove the generalization capabilities of our agents. We also demonstrate sim-to-real transfer by successfully deploying the learned policy on a real UR10 robot to explore real-world cluttered scenes. The supplemental video can be found at: https://www.youtube.com/watch?v&#x003D;T2Jo7wwaXss.",https://ieeexplore.ieee.org/document/9811874/,2022 International Conference on Robotics and Automation (ICRA),23-27 May 2022,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS.2018.8594070,HARK-Bird-Box: A Portable Real-time Bird Song Scene Analysis System,IEEE,Conferences,"This paper addresses real-time bird song scene analysis. Observation of animal behavior such as communication of wild birds would be aided by a portable device implementing a real-time system that can localize sound sources, measure their timing, classify their sources, and visualize these factors of sources. The difficulty of such a system is an integration of these functions considering the real-time requirement. To realize such a system, we propose a cascaded approach, cascading sound source detection, localization, separation, feature extraction, classification, and visualization for bird song analysis. Our system is constructed by combining an open source software for robot audition called HARK and a deep learning library to implement a bird song classifier based on a convolutional neural network (CNN). Considering portability, we implemented this system on a single-board computer, Jetson TX2, with a microphone array and developed a prototype device for bird song scene analysis. A preliminary experiment confirms a computational time for the whole system to realize a real-time system. Also, an additional experiment with a bird song dataset revealed a trade-off relationship between classification accuracy and time consuming and the effectiveness of our classifier.",https://ieeexplore.ieee.org/document/8594070/,2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),1-5 Oct. 2018,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ITSC.2014.6958165,HOG based multi-object detection for urban navigation,IEEE,Conferences,"A necessary condition to perform a fully autonomous driving system in urban environment is to detect object types in real scenes. Visual object recognition is a key solution, but multi-object detection still remain unsolved. In this paper, we present a fast and efficient multi-object detection system built to recognize, at the same time, pedestrians cars and bicycles. For each target type, we construct a holistic detector in a cascade manner, using a dense overlapping grid based on histograms of oriented gradients (HOG). The selection of HOG features is obtained through a learning process using AdaBoost algorithm. Experiments have been conducted on the car-like robot Robucar, where the single detectors are combined and implemented on its embedded computer, which is endowed with a modular software platform. Results are promising as the system can process up to 20 fps with VGA images.",https://ieeexplore.ieee.org/document/6958165/,17th International IEEE Conference on Intelligent Transportation Systems (ITSC),8-11 Oct. 2014,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS.2018.8593709,"Hands and Faces, Fast: Mono-Camera User Detection Robust Enough to Directly Control a UAV in Flight",IEEE,Conferences,"We present a robust real-time system for simultaneous detection of hands and faces in RGB and gray-scale images, and a novel dataset used for training. Our goal is to provide a robust sensor front-end suitable for real-time human-robot interaction using face-engagement and gestures. Using hand-labelled videos obtained from real human-UAV interaction experiments, we re-trained the YOLOv2 Deep Convolutional Neural Network to detect only hands and faces. This model was then used to automatically label several much larger third-party datasets. After manual correction of these results, we modified and re-trained the model on all this labelled data. We obtain qualitatively good detection results at 60Hz on a commodity GPU: our simultaneous hand-and-face detector gives state of the art accuracy and speed in a hand detection benchmark and competitive results in a face detection benchmark. To demonstrate its effectiveness for human-robot interaction we describe its use as the input to a simple but practical gestural human-UAV interface for entertainment or industrial applications. All software, training and test data are freely available.",https://ieeexplore.ieee.org/document/8593709/,2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),1-5 Oct. 2018,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISVLSI.2016.101,Hardware Design Automation of Convolutional Neural Networks,IEEE,Conferences,"Convolutional Neural Networks (CNNs) are a variation of feed-forward Neural Networks inspired by the biological process in the visual cortex of animals. The interest in this supervised learning algorithm has rapidly grown in many fields like image and video recognition and natural language processing. Nowadays they have become the state of the art in various applications like mobile robot vision, video surveillance and Big Data analytics. The specific computation pattern of CNNs results to be highly suitable for hardware acceleration, in fact different types of accelerators have been proposed based on GPU, Field Programmable Gate Array (FPGA) and ASIC. In particular, in the embedded systems context, due to real time and power consumption challenges, it is crucial to find the right tradeoff between performance, energy efficiency, fast development round and cost. This work proposes a framework meant as a tool for the user to accelerate and simplify the design and the implementation of CNNs on FPGAs by leveraging High Level Synthesis, still providing a certain level of customization of the hardware design.",https://ieeexplore.ieee.org/document/7560201/,2016 IEEE Computer Society Annual Symposium on VLSI (ISVLSI),11-13 July 2016,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICAEE.2017.8255341,Hardware and software implementation of real time electrooculogram (EOG) acquisition system to control computer cursor with eyeball movement,IEEE,Conferences,"Human computer interface (HCI) is an emerging technology of neuroscience and artificial intelligence. Development of HCI system using bio signal e.g. Electrooculogram (EOG), Electromyogram (EMG), Electroencephalogram (EEG), Functional near-infrared spectroscopy (fNIRS) etc. are attracted more and more attention of researchers all over the world in recent years because through this it is possible to get acquainted with advanced technologies of artificial intelligence. This paper presents the design and implementation of a fully functional Electrooculogram (EOG) based human computer interface. In this work we have designed and implemented necessary hardware and software for EOG signal acquisition along with controlling hardware such as wheelchair, robotic arm, mobile robot etc., and move computer mouse cursor simultaneously using EOG signal. This interface has three portion: EOG signal acquisition and amplification, analog to digital conversion, and real time hardware and mouse cursor movement. Eye movement is detected by measuring potential difference between cornea and retina using five Ag-Agcl disposable electrodes. Frequency range of EOG signal is considered as 0.3 to 15Hz, so this frequency range is taken using an active high and low pass filter so that accurate EOG signal can be achieved. The analog output of the EOG signal from filter is converted into digital signal by using an Arduino. Arduino serialize the EOG data for calibration and provides a threshold reference point which is used for controlling Hardware. The Classification module e.g. Support Vector machine (SVM) and Linear Discriminant Analysis (LDA) classify live data with respect to the horizontal and vertical data. This works as a binary classifier and choose optimal hyper-plane between two variables. According to each update on the eye position, cursor automatically accelerated in particular direction. PyMouse module in python is used for this task. Eye gesture based Hardware like robot, wheelchair etc. control and mouse cursor movement are the principle outcome of this research work.",https://ieeexplore.ieee.org/document/8255341/,2017 4th International Conference on Advances in Electrical Engineering (ICAEE),28-30 Sept. 2017,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ROBOT.1993.292013,Hidden Markov model approach to skill learning and its application in telerobotics,IEEE,Conferences,"The problem of how human skill can be represented as a parametric model using a hidden Markov (HMM), and how an HMM-based skill model can be used to learn human skill, is discussed. The HMM is feasible for characterizing two stochastic processes, measurable action and immeasurable mental states that are involved in the skill learning. Based on the most likely performance criterion, the best action sequence can be selected from previously measured action data by modeling the skill as an HMM. This selection process can be updated in real-time by feeding new action data and modifying HMM parameters. The implementation of the proposed method in a teleoperation-controlled space robot is discussed. The results demonstrate the feasibility of the method.<>",https://ieeexplore.ieee.org/document/292013/,[1993] Proceedings IEEE International Conference on Robotics and Automation,2-6 May 1993,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SMC.2018.00176,Hierarchical Control Architecture Regulating Competition between Model-Based and Context-Dependent Model-Free Reinforcement Learning Strategies,IEEE,Conferences,"Recent evidence in neuroscience and psychology suggests that a single reinforcement learning (RL) algorithm only accounts for less than 60% of the variance of human choice behavior in an uncertain and dynamic environment, where the amount of uncertainty in state-action-state transitions drift over time. The prediction performance further decreases when the size of the state space increases. We proposed a hierarchical context-dependent RL control framework that dynamically exerted control weights on model-based (MB) and multiple model-free (MF) RL strategies associated with different task goals. To properly assess the validity of the proposed method, we considered a two-stage Markov decision task (MDT) in which the three different types of context changed over time. We trained 57 different RL control models on a Caltech MDT data set; then, we assessed their prediction performance using a Bayesian model comparison. This large-scale computer simulation analysis revealed that the model providing the most accurate prediction was the version that implemented the competition between the MB and multiple goal-dependent MF RL strategies. The present study demonstrates the applicability of the goal-driven RL control to a variety of real-world human-robot interaction scenarios.",https://ieeexplore.ieee.org/document/8616172/,"2018 IEEE International Conference on Systems, Man, and Cybernetics (SMC)",7-10 Oct. 2018,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IJCNN.2007.4371289,Hierarchical MMC Networks as a manipulable body model,IEEE,Conferences,"A cognitive control system for a walking robot should be able to solve from simple reactive tasks up to complex tasks, including tasks which need cognitive capabilities and setting up plans. Planning ahead involves some kind of internal representation: most important a model of the own body. Considering planning as mental simulation, this model must be fully functional: it is constrained in the same way as the body itself and it can move and be used in the same way as the body. This model can then be used to try out movements mentally without doing the action in reality. For this purpose it must be possible to decouple the body itself from the action controlling modules to use the original controllers for control of the internal representations. In this publication we introduce a hierarchical model, implemented as an recurrent neural network based on the MMC principle.",https://ieeexplore.ieee.org/document/4371289/,2007 International Joint Conference on Neural Networks,12-17 Aug. 2007,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ETFA.2018.8502527,Holo Pick'n'Place,IEEE,Conferences,"In this paper we contribute to the research on facilitating industrial robot programming by presenting a concept for intuitive drag and drop like programming of pick and place tasks with Augmented Reality (AR). We propose a service-oriented architecture to achieve easy exchangeability of components and scalability with respect to AR devices and robot workplaces. Our implementation uses a HoloLens and a UR5 robot, which are integrated into a framework of RESTful web services. The user can drag recognized objects and drop them at a desired position to initiate a pick and place task. Although the positioning accuracy is unsatisfactory yet, our implemented prototype achieves most of the desired advantages to proof the concept.",https://ieeexplore.ieee.org/document/8502527/,2018 IEEE 23rd International Conference on Emerging Technologies and Factory Automation (ETFA),4-7 Sept. 2018,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IV51971.2022.9827271,How to build and validate a safe and reliable Autonomous Driving stack? A ROS based software modular architecture baseline,IEEE,Conferences,"The implementation of Autonomous Driving stacks (ADS) is one of the most challenging engineering tasks of our era. Autonomous Vehicles (AVs) are expected to be driven in highly dynamic environments with a reliability greater than human beings and full autonomy. Furthermore, one of the most important topics is the way to democratize and accelerate the development and research of holistic validation to ensure the robustness of the vehicle. In this paper we present a powerful ROS (Robot Operating System) based modular ADS that achieves state-of-the-art results in challenging scenarios based on the CARLA (Car Learning to Act) simulator, outperforming several strong baselines in a novel evaluation setting which involves non-trivial traffic scenarios and adverse environmental conditions (Qualitative results). Our proposal ranks in second position in the CARLA Autonomous Driving Leaderboard (Map Track) and gets the best score considering modular pipelines, as a preliminary stage before implementing it in our real-world autonomous electric car. To encourage the use research in holistic development and testing, our code is publicly available at https://github.com/RobeSafe-UAH/CARLA_Leaderboard.",https://ieeexplore.ieee.org/document/9827271/,2022 IEEE Intelligent Vehicles Symposium (IV),4-9 June 2022,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/HAPTIC.2010.5444617,IN-HAPTICS: Interactive navigation using haptics,IEEE,Conferences,"We present a computational framework and experimental platform for robot navigation that allows for a user-friendly, graphical and haptic interaction with the human operator during the deployment process. The operator can see, feel, and manipulate the artificial potential field that drives the robot through an environment cluttered with obstacles. We present a case study in which the operator rescues a robot trapped in a local minimum of a navigation potential field.",https://ieeexplore.ieee.org/document/5444617/,2010 IEEE Haptics Symposium,25-26 March 2010,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS51168.2021.9636439,Image-Based Joint State Estimation Pipeline for Sensorless Manipulators,IEEE,Conferences,"Motion planning is a largely solved problem for robot arms with joint state feedback, but remains an area of research for sensorless manipulators such as toy robot arms and heavy equipment such as excavators and cranes. A promising approach to this problem is deep learning, which employs a pre-trained convolutional neural network to identify manipulator links and estimate joint states from a monocular camera video feed. Whereas manual labeling of training image sets is tedious and non-transferable, a simulation environment can automatically generate labeled training image sets of any size. The issue is the gap between simulated and real-world images. This paper solves this problem by implementing a Generative Adversarial Network. The complete joint state estimation pipeline is implemented and tested in hardware experiments to validate our proposed approach.",https://ieeexplore.ieee.org/document/9636439/,2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),27 Sept.-1 Oct. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CISP-BMEI48845.2019.8965907,Implementation and Verification of a Virtual Testing System Based on ROS and Unity for Computer Vision Algorithms,IEEE,Conferences,"With the development of artificial intelligence technology, Computer vision algorithm is playing an increasingly important role. Computer vision algorithm testing is a vital link to ensure the safe and reliable operation of agents. However, the traditional testing methods based on real scenes provide single samples and are challenging to obtain ground truth, which makes it inefficient to test computer vision algorithms. To solve this problem, the testing method of computer vision algorithms using virtual scenes instead of real scenes has been applied. In this paper, Unity and robot operating system (ROS) are selected to build a virtual testing system named URCV for computer vision algorithms. The feasibility of the system and the influence of virtual scene elements on the testing of the monocular ORB_SLAM2 algorithm are verified, including the rendering path of Unity's RGB camera, texture accuracy, and illumination model.",https://ieeexplore.ieee.org/document/8965907/,"2019 12th International Congress on Image and Signal Processing, BioMedical Engineering and Informatics (CISP-BMEI)",19-21 Oct. 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ELECSYM.2018.8615503,Implementation of Victims Detection Framework on Post Disaster Scenario,IEEE,Conferences,"Disasters are prone to occur in Indonesia due to geographical factors, such as tectonic plate movements, which can cause an earthquake. Earthquakes are one of the most frequent disasters, they have broad impacts in a short time and are unpredictable. Thus, an extensive search process in a short time is highly critical to determine the victims location. In this paper, a victims detection framework is developed starting from acquiring images using an unmanned aerial vehicle and further processing using convolutional neural network (CNN) to locate victims robustly on post-disaster. Input images are then sent to victim detector dedicated ground station server for further high processing robustly locating the possibility of victims. A simulation system mimicking a real environment is developed to test our framework in real time. A transmission protocol is also developed for effectively transmitting data between the robot and the server. The treatment on the detection process of the victim is different from the normal human detection, some pre-processing stages are applied to increase the variation of the given dataset. An embedded system is used for taking images and additional sensors data, such as location and time using Global Navigation Satellite System.",https://ieeexplore.ieee.org/document/8615503/,2018 International Electronics Symposium on Engineering Technology and Applications (IES-ETA),29-30 Oct. 2018,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SSST.1998.660084,Implementation of a navigational neural network on a parallel DSP board,IEEE,Conferences,"This work presents a neural network architecture that is motivated by the learning and memory characteristics of a part of the brain known as hippocampus, which is important in navigational behavior in humans and animals. Neural networks perform nonlinear transformations on data to yield suitable classification or control actions. In our case, the navigation network takes the distance information as data and maps it to control actions by the mobile robot. Navigation is a very important engineering problem for unknown or hazardous environments to ensure the safety of equipment and human life. Hardware implementation can benefit applications in real time where speed is the major concern. Our objective is to implement such a navigational neural network in parallel so that real time performance can be achieved by using a parallel DSP board system. Supplementary studies are also being carried out on the IBM SP2 supercomputer to understand the design and scaling properties of the parallel algorithm.",https://ieeexplore.ieee.org/document/660084/,Proceedings of Thirtieth Southeastern Symposium on System Theory,10-10 March 1998,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IJCNN.2008.4633972,Implementation of a neural network based visual motor control algorithm for A 7 DOF redundant manipulator,IEEE,Conferences,"This paper deals with visual-motor coordination of a 7 dof robot manipulator for pick and place applications. Three issues are dealt with in this paper - finding a feasible inverse kinematic solution without using any orientation information, resolving redundancy at position level and finally maintaining the fidelity of information during clustering process thereby increasing accuracy of inverse kinematic solution. A 3-dimensional KSOM lattice is used to locally linearize the inverse kinematic relationship. The joint angle vector is divided into two groups and their effect on end-effector position is decoupled using a concept called function decomposition. It is shown that function decomposition leads to significant improvement in accuracy of inverse kinematic solution. However, this method yields a unique inverse kinematic solution for a given target point. A concept called sub-clustering in configuration space is suggested to preserve redundancy during learning process and redundancy is resolved at position level using several criteria. Even though the training is carried out off-line, the trained network is used online to compute the required joint angle vector in only one step. The accuracy attained is better than the current state of art. The experiment is implemented in real-time and the results are found to corroborate theoretical findings.",https://ieeexplore.ieee.org/document/4633972/,2008 IEEE International Joint Conference on Neural Networks (IEEE World Congress on Computational Intelligence),1-8 June 2008,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICSCCN.2011.6024574,Implementation of neural network based controller using Verilog,IEEE,Conferences,"A simple and robust real time controller that works very well for linear systems with optimal gain tunings is the PID controller. But, PID controllers do not work properly if plant dynamics are changing fast or when the plant is highly nonlinear. However, many of the industries still rely on it. Hence, in most of the plants an auxiliary controller coexists to help the primary PID controller to work better by compensating for uncertainties present during control operation. Neural network has proven to be a good candidate as this auxiliary nonlinear controller. Neural network can effectively compensate for unknown uncertainties and also act as an intelligent control. The success of the neural network as an auxiliary controller has been reported in practical applications such as motion control system, signal processing and controlling robot manipulators. Nowadays, parallel programmable logic devices, such as the field programmable gate array (FPGA), have become powerful hardware options, offering low cost, high execution speed, reconfigurability and parallelism. This work intends to exploit the current available resources in commercial FPGAs to implement servo control for hard disk drive system. Simulation and experimental results included in this paper show the viability of exploiting the parallelism and modularity of a Virtex 6 FPGA to implement a high sampling rate neural network-RBF based controller. This control system platform will allow fast prototyping of new control concepts and evaluation of non-linear control.",https://ieeexplore.ieee.org/document/6024574/,"2011 International Conference on Signal Processing, Communication, Computing and Networking Technologies",21-22 July 2011,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS.1995.525808,Implementation of real time spatial mapping in robotic systems through self-organizing neural networks,IEEE,Conferences,"Presents a methodology which allows an autonomous agent i.e., a mobile robot, to learn and build maps of its operating environment by relying only on its range sensors. The maps, described with respect to the robot's inertial frame, are developed in real time by correlating robot position and sensory data. This latter feature characterizes part of the uniqueness of the authors' approach. These maps are topologically isomorphic to the maps created for the same room(s) by humans. The methodology exploits the principle of self-organization, implemented as an artificial neural network module which processes incoming sensor range data. The generation of environmental maps can be visualized as an elastic string of neurons whereby every neuron represents a finite portion of the physical world. This elastic string stretches dynamically so as to take on the shape of the environment, a unique characteristic of the authors' methodology. In this respect, the neural net provides a discretized representation of the ""continuous"" physical environment as the latter is seen through the robot's own sensors. Experiments, focused on indoor applications, have successfully demonstrated the ability of a robot to build maps of geometrically complex environments. The results presented in this paper, compared with the authors' earlier efforts, show significant improvement in that every single sensor data point contributes equally to the location of the neurons of the spatial map at the end of the learning process. This is important because the authors wish to minimize the effect of the order in which data points are processed.",https://ieeexplore.ieee.org/document/525808/,Proceedings 1995 IEEE/RSJ International Conference on Intelligent Robots and Systems. Human Robot Interaction and Cooperative Robots,5-9 Aug. 1995,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICMTMA.2018.00075,Implementing Multi-DOF Trajectory Tracking Control System for Robotic Arm Experimental Platform,IEEE,Conferences,"To implement the control system of a multi-DOF robotic manipulator (Dobot), the robot dynamics, trajectory planning algorithm and motion control strategy are studied for designing the trajectory tracking control system. In this paper, the hardware and software of Dobot magician control system are designed. The hardware mainly includes STM32 controller. The software part mainly builds the host computer display interface, completes the protocol communication between the robot manipulator and the PC, so as to realize the trajectory tracking control of the robot manipulator and implement the track-following in real time. The experimental results show that the control system can accurately track the trajectory of robotic manipulator with a certain degree of real-time and stability.",https://ieeexplore.ieee.org/document/8337386/,2018 10th International Conference on Measuring Technology and Mechatronics Automation (ICMTMA),10-11 Feb. 2018,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICPR.1996.547231,Incremental learning for vision-based navigation,IEEE,Conferences,"In this paper, we explore the issue of incremental learning for autonomous navigation of a mobile robot. The autonomous navigation problem is regarded as a content-based retrieval problem where the robot learns the navigation experience using a hierarchical recursive partition tree (RPT). During real navigation, each time a new image is grabbed to retrieve the learned tree. The associated control signals of the retrieved are used to control the new action of the robot. Use of RPT can achieve efficient retrieval. In the proposed incremental learning scheme, a new image with the associated control signals is learned or rejected according to whether its retrieved output control signals are within tolerance of the desired control signals of the input query image. We use the eigen-subspace method for feature extraction in our incremental learning. The proposed algorithm has a real-time implementation for both learning and performance phases. Experimental results are shown to confirm the effectiveness of proposed method.",https://ieeexplore.ieee.org/document/547231/,Proceedings of 13th International Conference on Pattern Recognition,25-29 Aug. 1996,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS.2010.5650519,Incremental motion primitive learning by physical coaching using impedance control,IEEE,Conferences,"We present an approach for kinesthetic teaching of motion primitives for a humanoid robot. The proposed teaching method allows for iterative execution and motion refinement using a forgetting factor. During the iterative motion refinement, a confidence value specifies an area of allowed refinement around the nominal trajectory. A novel method for continuous generation of motions from a hidden Markov model (HMM) representation of motion primitives is proposed, which incorporates relative time information for each state. On the real-time control level, the kinesthetic teaching is handled by a customized impedance controller, which combines tracking performance with soft physical interaction and allows to implement soft boundaries for the motion refinement. The proposed methods were implemented and tested using DLR's humanoid upper-body robot Justin.",https://ieeexplore.ieee.org/document/5650519/,2010 IEEE/RSJ International Conference on Intelligent Robots and Systems,18-22 Oct. 2010,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ROBOT.2010.5509682,Indoor scene recognition through object detection,IEEE,Conferences,"Scene recognition is a highly valuable perceptual ability for an indoor mobile robot, however, current approaches for scene recognition present a significant drop in performance for the case of indoor scenes. We believe that this can be explained by the high appearance variability of indoor environments. This stresses the need to include high-level semantic information in the recognition process. In this work we propose a new approach for indoor scene recognition based on a generative probabilistic hierarchical model that uses common objects as an intermediate semantic representation. Under this model, we use object classifiers to associate low-level visual features to objects, and at the same time, we use contextual relations to associate objects to scenes. As a further contribution, we improve the performance of current state-of-the-art category-level object classifiers by including geometrical information obtained from a 3D range sensor that facilitates the implementation of a focus of attention mechanism within a Monte Carlo sampling scheme. We test our approach using real data, showing significant advantages with respect to previous state-of-the-art methods.",https://ieeexplore.ieee.org/document/5509682/,2010 IEEE International Conference on Robotics and Automation,3-7 May 2010,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICALT.2006.1652652,Instruction Through The Ages: Building Pervasive Virtual Instructors for Life Long Learning,IEEE,Conferences,"A pervasive virtual instructor is an artificially intelligent instructor that may appear transparent to the learner or appear in the form of a three-dimensional graphical character, digital toy, or robot with capabilities to inhabit mixed reality environments, and provide personalized instruction anytime, anywhere, and at an-pace. Similarly to pedagogical agents, or traditional virtual instructors, pervasive virtual instructors are expected to behave autonomously, respond to human verbal/non-verbal input, and deliver information to human users. Unique to pervasive virtual instructors in this paper are capabilities to provide instruction across distributed networks, interact with human learners using context-aware intelligence, and apply empirically researched pedagogical/andragogical techniques. Technology challenges remain for building pervasive virtual instructors to achieve aforementioned capabilities. This paper summarizes technical challenges and an approach for building pervasive instructors that provide life long instructional services",https://ieeexplore.ieee.org/document/1652652/,Sixth IEEE International Conference on Advanced Learning Technologies (ICALT'06),5-7 July 2006,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISCAS.2007.378811,Integrating high-level sensor features via STDP for bio-inspired navigation,IEEE,Conferences,"Correlation based algorithms have been found to explain many basic behaviors in simple animals. In this paper the authors investigate the problem of navigation control of a robot from the viewpoint of bio-inspired perception. In this paper the authors study how to go up, through learning, from the implementation of a reactive system, towards behaviors of increasing complexity. The whole control system is based on networks of spiking neurons. A correlation based rule, namely the spike timing dependent plasticity (STDP), is implemented for an efficient learning. The main interesting consequence is that the system is able to learn high-level sensor features, based on a set of basic reflexes, depending on some low-level sensor inputs. The whole methodology is presented through simulation results and also through its implementation on an FPGA based system for real time working on a roving robot.",https://ieeexplore.ieee.org/document/4252708/,2007 IEEE International Symposium on Circuits and Systems,27-30 May 2007,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/LARS-SBR.2016.49,Integration of People Detection and Simultaneous Localization and Mapping Systems for an Autonomous Robotic Platform,IEEE,Conferences,"This paper presents the implementation of a people detection system for a robotic platform able to perform Simultaneous Localization and Mapping (SLAM), allowing the exploration and navigation of the robot considering people detection interaction. The robotic platform consists of a Pioneer 3DX robot equipped with an RGB-D camera, a Sick Lms200 sensor laser and a computer using the robot operating system ROS. The idea is to integrate the people detection system to the simultaneous localization and mapping (SLAM) system of the robot using ROS. Furthermore, this paper presents an evaluation of two different approaches for the people detection system. The first one uses a manual feature extraction technique, and the other one is based on deep learning methods. The manual feature extraction method in the first approach is based on HOG (Histogram of Oriented Gradients) detectors. The accuracy of the techniques was evaluated using two different libraries. The PCL library (Point Cloud Library) implemented in C ++ and the VLFeat MatLab library with two HOG variants, the original one, and the DPM (Deformable Part Model) variant. The second approaches are based on a Deep Convolutional Neural Network (CNN), and it was implemented using the MatLab MatConvNet library. Tests were made objecting the evaluation of losses and false positives in the people's detection process in both approaches. It allowed us to evaluate the people detection system during the navigation and exploration of the robot, considering the real time interaction of people recognition in a semi-structured environment.",https://ieeexplore.ieee.org/document/7783535/,2016 XIII Latin American Robotics Symposium and IV Brazilian Robotics Symposium (LARS/SBR),8-12 Oct. 2016,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SII52469.2022.9708896,Integration of a reconfigurable robotic workcell for assembly operations in automotive industry,IEEE,Conferences,"This paper deals with the integration of a flexible, reconfigurable work cell performing assembly of parts in the automotive industry. The unique feature of the developed cell is that it can function in two modes: a) entirely autonomously or b) in cooperation with a human, where the operation of the robot dynamically adapts to human actions. We have implemented technologies for online recognition of human intention and for real-time learning of robust assembly policies to achieve the desired outcome. This challenging goals dictate the integration of modern deep learning algorithms, statistical learning, and compliant robot control into a unique ROS-based robot control system.",https://ieeexplore.ieee.org/document/9708896/,2022 IEEE/SICE International Symposium on System Integration (SII),9-12 Jan. 2022,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IECON.2011.6119682,Integration of grey model and neural network for robotic application,IEEE,Conferences,"This paper proposes an intelligent forecasting system based on a feedforward neural network aided grey model (FNAGM), integrating a first-order single variable grey model (GM(1,1)) and a feedforward neural network. The system includes three phases: initialization phase, GM(1,1) prediction phase, and FNAGM prediction phase. A number of parameters required for the FNAGM are selected in the initialization phase. A one-step ahead predictive value is generated in the GM(1,1) prediction phase, followed by the implementation of a feedforward neural network used to determine the prediction error of the GM(1,1) and compensate for it in the FNAGM prediction phase. We also adopted on-line batch training to adjust the network according to the Levenberg-Marquardt algorithm in real-time. According to the experimental results of a robot, the proposed intelligent forecasting system can provide high accuracy for both trajectory prediction and target tracking.",https://ieeexplore.ieee.org/document/6119682/,IECON 2011 - 37th Annual Conference of the IEEE Industrial Electronics Society,7-10 Nov. 2011,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/WOCC.2018.8372718,Integration of open source platform duckietown and gesture recognition as an interactive interface for the museum robotic guide,IEEE,Conferences,"In recent years, population aging becomes a serious problem. To decrease the demand for labor when navigating visitors in museums, exhibitions, or libraries, this research designs an automatic museum robotic guide which integrates image and gesture recognition technologies to enhance the guided tour quality of visitors. The robot is a self-propelled vehicle developed by ROS (Robot Operating System), in which we achieve the automatic driving based on the function of lane-following via image recognition. This enables the robot to lead guests to visit artworks following the preplanned route. In conjunction with the vocal service about each artwork, the robot can convey the detailed description of the artwork to the guest. We also design a simple wearable device to perform gesture recognition. As a human machine interface, the guest is allowed to interact with the robot by his or her hand gestures. To improve the accuracy of gesture recognition, we design a two phase hybrid machine learning-based framework. In the first phase (or training phase), k-means algorithm is used to train historical data and filter outlier samples to prevent future interference in the recognition phase. Then, in the second phase (or recognition phase), we apply KNN (k-nearest neighboring) algorithm to recognize the hand gesture of users in real time. Experiments show that our method can work in real time and get better accuracy than other methods.",https://ieeexplore.ieee.org/document/8372718/,2018 27th Wireless and Optical Communication Conference (WOCC),30 April-1 May 2018,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICAMechS.2016.7813486,Intelligent adaptive precrash control for autonmous vehicle agents (CBR Engine & hybrid A path planner),IEEE,Conferences,"PreCrash problem of Intelligent Control of autonomous vehicles robot is a very complex problem, especially vehicle pre-crash scenariws and at points of intersections in real-time environmenta. This Paper presents a novel architecture of Intelligent adaptive control for autonomous vehicle agent that depends on Artificial Intelligence Techniques that applies case-based reasoning techniques, where Parallel CBR Engines are implemented for different scenarios' of PreCrash problem and sub-problems of intersection safety and collision avoidance, in the higher level of the controller and A∗ path planner for path planning and at lower-levels it also uses some features of autonomous vehicle dynamics. Moreover, the planner is enhanced by combination of Case-Based Planner. All modules are presented and discussed. Experimental results are conducted in the framework of Webots autonomous vehicle tool and overall results are good for the CBR Engine for Adaptive control and also for the hybrid Case-Based Planner, A∗ and D∗ motion planner along with conclusion and future work.",https://ieeexplore.ieee.org/document/7813486/,2016 International Conference on Advanced Mechatronic Systems (ICAMechS),30 Nov.-3 Dec. 2016,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISIC.2003.1253948,Internet-based remote control by using Adaline neural networks,IEEE,Conferences,"In this paper, we present a remote control scheme for Internet-based teleoperation. This control scheme relies on the real-time estimation of concurrent roundtrip delays in order to optimally assign tasks between the user and the robot. For this purpose, we employ an adaptive linear (Adaline) neural network for which most conventional learning algorithms are infeasible since the computation is usually too intensive to be practical. To get around this problem, we introduce a novel learning algorithm that is based on the maximum entropy principle. Compared to traditional learning algorithms, the computing cost of this algorithm is very low, which makes it possible for the proposed neural network to be implemented on-line in real-time.",https://ieeexplore.ieee.org/document/1253948/,Proceedings of the 2003 IEEE International Symposium on Intelligent Control,8-8 Oct. 2003,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISSCS52333.2021.9497411,Inverted Pendulum Control with a Robotic Arm using Deep Reinforcement Learning,IEEE,Conferences,"Inverted pendulum control is a benchmark control problem that researchers have used to test the new control strategies over the past 50 years. Deep Reinforcement Learning Algorithm is used recently on the inverted pendulum on a straightforward form. The inverted pendulum had only one degree of freedom and was moving on a plane. This paper demonstrates a successful implementation of a deep reinforcement learning algorithm on an inverted pendulum that rotates freely on a spherical joint with an industrial 6 degrees freedom robot arm. This research used the Deep Reinforcement Learning algorithm in Robot Operating System (ROS) and Gazebo Simulation. Experimental results show that the proposed method achieved promising outputs and reaches the control objectives. We were able to control the inverted pendulum upward for 30 and 20 seconds in two case studies. Two other significant novelties in this research are using an inertial measurement unit (IMU) on the tip of the pendulum, that will facilitate implementation on the real robot for future work and different reward functions in comparing to past publications that enable continuous learning and mastering control in a vertical position",https://ieeexplore.ieee.org/document/9497411/,"2021 International Symposium on Signals, Circuits and Systems (ISSCS)",15-16 July 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ACC.1994.735001,Investigation of kinematics and inverse dynamics algorithm with a DSP implementation of a neural network,IEEE,Conferences,"An investigation is described to demonstrate the benefits which can be gained by using a digital signal processor (DSP) to implement robot related control schemes, kinematics, and inverse dynamics with a neural network. A neural network adaptive controller is given and applied to a robot manipulator having a closed kinematic chain, a configuration which is not well suited to the popular serial link algorithms. The Lyapunov's stability approach is used to develop a learning rule for the neural network controller that would guarantee the stability of the training process under mild conditions. The controller hardware consists of a PC-386, a fixed point DSP, and a floating point DSP. The software installed on each of these processors has the requirements of satisfying the specific responsibility assigned to that processor and of communicating with other processors so that necessary data is passed on in a timely manner. A computational software package has been built to further enhance the speed of the general control scheme and the neural network algorithm. The techniques used in the DSP implementation of the adaptive control algorithm in real-time are also discussed.",https://ieeexplore.ieee.org/document/735001/,Proceedings of 1994 American Control Conference - ACC '94,29 June-1 July 1994,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISIE.2009.5221750,Joint control of ROBOKER arm using a neural chip embedded on FPGA,IEEE,Conferences,"This paper presents implementation of a neural chip to proceed neural processing of the radial basis function (RBF) network. RBF network along with a primary PD controller is trained in on-line fashion. Radial basis function network processing is embedded on a field programmable gate array(FPGA) chip to achieve real-time control. To enable nonlinear function calculation, a floating point processor is designed to allow assembly programming for learning algorithm. Other necessary hardware modules for control purposes are also designed and implemented. A humanoid robot called the ROBOKER with two arms of 6 degrees-of-freedom each is controlled. Joint angles of the ROBOKER arms are controlled and tracking performances by the neural chip are compared with those by PD controllers.",https://ieeexplore.ieee.org/document/5221750/,2009 IEEE International Symposium on Industrial Electronics,5-8 July 2009,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICALT.2003.1215101,Kana-input navigation system for kids based on the cyber assistant,IEEE,Conferences,"In Japan, it has increased the opportunity for young children to experience the personal computer in elementary schools. However, in order to use computer, many domestic barriers have confronted young children (kids) because they cannot read Kanji characters and had not learnt Roman alphabet yet. As a result, they cannot input text strings by JIS keyboard. We developed Kana-input navigation system for kids (KINVS) based on the cyber assistant system (CAS). CAS is a human-style software robot based on the 3D-CG real-time animation and voice synthesis technology. KINVS enables to input Hiragana/Katakana characters by mouse operation only (without keyboard) and CAS supports them by using speaking, facial expression, body action and sound effects. KINVS displays the 3D-stage like a classroom. In this room, blackboard, interactive parts to input Kana-characters, and CAS are placed. Mouse input method of KINVS are designed to use only single click and wheeler rotation. To input characters, kids clicks or rotates the interactive parts. KINVS reports all information by voice speaking and Kana subtitles instead of Kanji text. Furthermore, to verify the functional feature of KINVS, we measured how long kids had taken to input long text by using KINVS.",https://ieeexplore.ieee.org/document/1215101/,Proceedings 3rd IEEE International Conference on Advanced Technologies,9-11 July 2003,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA40945.2020.9197378,Learning Camera Miscalibration Detection,IEEE,Conferences,"Self-diagnosis and self-repair are some of the key challenges in deploying robotic platforms for long-term real-world applications. One of the issues that can occur to a robot is miscalibration of its sensors due to aging, environmental transients, or external disturbances. Precise calibration lies at the core of a variety of applications, due to the need to accurately perceive the world. However, while a lot of work has focused on calibrating the sensors, not much has been done towards identifying when a sensor needs to be recalibrated. This paper focuses on a data-driven approach to learn the detection of miscalibration in vision sensors, specifically RGB cameras. Our contributions include a proposed miscalibration metric for RGB cameras and a novel semi-synthetic dataset generation pipeline based on this metric. Additionally, by training a deep convolutional neural network, we demonstrate the effectiveness of our pipeline to identify whether a recalibration of the camera's intrinsic parameters is required or not. The code is available at http://github.com/ethz-asl/camera_miscalib_detection.",https://ieeexplore.ieee.org/document/9197378/,2020 IEEE International Conference on Robotics and Automation (ICRA),31 May-31 Aug. 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS51168.2021.9636547,Learning Contact-Rich Assembly Skills Using Residual Admittance Policy,IEEE,Conferences,"Contact-rich assembly tasks may result in large and unpredictable forces and torques when the locations of the contacting parts are uncertain. The ability to correct the trajectory in response to haptic feedback and accomplish the task despite location uncertainties is an important skill. We hypothesize that this skill would facilitate generalization and support direct transfer from simulations to real world. To reduce sample complexity, we propose to learn a residual admittance policy (RAP). RAP is learned to correct the movements generated by a baseline policy in the framework of dynamic movement primitives. Given the reference trajectories generated by the baseline policy, the action space of RAP is limited to the admittance parameters. Using deep reinforcement learning, a deep neural network is trained to map task specifications to proper admittance parameters. We demonstrate that RAP handles uncertainties in board location, generalizes well over space, size and shape, and facilitates quick transfer learning. Most impressively, we demonstrate that the policy learned in simulations achieves similar robustness to uncertainties, generalization and performance when deployed on an industrial robot (UR5e) without further training. See accompanying video for demonstrations.",https://ieeexplore.ieee.org/document/9636547/,2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),27 Sept.-1 Oct. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA46639.2022.9811624,Learning Design and Construction with Varying-Sized Materials via Prioritized Memory Resets,IEEE,Conferences,"Can a robot autonomously learn to design and construct a bridge from varying-sized blocks without a blueprint? It is a challenging task with long horizon and sparse reward - the robot has to figure out physically stable design schemes and feasible actions to manipulate and transport blocks. Due to diverse block sizes, the state space and action trajectories are vast to explore. In this paper, we propose a hierarchical approach for this problem. It consists of a reinforcement-learning designer to propose high-level building instructions and a motion-planning-based action generator to manipulate blocks at the low level. For high-level learning, we develop a novel technique, prioritized memory resetting (PMR) to improve exploration. PMR adaptively resets the state to those most critical configurations from a replay buffer so that the robot can resume training on partial architectures instead of from scratch. Furthermore, we augment PMR with auxiliary training objectives and fine-tune the designer with the locomotion generator. Our experiments in simulation and on a real deployed robotic system demonstrate that it is able to effectively construct bridges with blocks of varying sizes at a high success rate. Demos can be found at https://sites.google.com/view/bridge-pmr.",https://ieeexplore.ieee.org/document/9811624/,2022 International Conference on Robotics and Automation (ICRA),23-27 May 2022,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA40945.2020.9196785,Learning Resilient Behaviors for Navigation Under Uncertainty,IEEE,Conferences,"Deep reinforcement learning has great potential to acquire complex, adaptive behaviors for autonomous agents automatically. However, the underlying neural network polices have not been widely deployed in real-world applications, especially in these safety-critical tasks (e.g., autonomous driving). One of the reasons is that the learned policy cannot perform flexible and resilient behaviors as traditional methods to adapt to diverse environments. In this paper, we consider the problem that a mobile robot learns adaptive and resilient behaviors for navigating in unseen uncertain environments while avoiding collisions. We present a novel approach for uncertainty-aware navigation by introducing an uncertainty-aware predictor to model the environmental uncertainty, and we propose a novel uncertainty-aware navigation network to learn resilient behaviors in the prior unknown environments. To train the proposed uncertainty-aware network more stably and efficiently, we present the temperature decay training paradigm, which balances exploration and exploitation during the training process. Our experimental evaluation demonstrates that our approach can learn resilient behaviors in diverse environments and generate adaptive trajectories according to environmental uncertainties.",https://ieeexplore.ieee.org/document/9196785/,2020 IEEE International Conference on Robotics and Automation (ICRA),31 May-31 Aug. 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IIAI-AAI.2013.74,Learning Which Features to Imitate in a Painting Task,IEEE,Conferences,"Learning is essential for an autonomous agent to adapt to an environment. One method of learning is through trial and error, however, this method is impractical in a complex environment because of the long learning time required by the agent. Therefore, guidelines are necessary in order to expedite the learning process in such environments, and imitation is one such guideline. Sakato, Ozeki, and Oka (2012) recently proposed a computational model of imitation and autonomous behavior by which an agent can reduce its learning time through imitation. In this paper, we apply the model to a real robot, Nao, and evaluate the model using simple features in a simple environment. We also report on the progress of implementation of the model, and evaluations of the performance of imitation using the implemented model. Our experimental results indicate that the model adapted to the experimental environment by imitation.",https://ieeexplore.ieee.org/document/6630378/,2013 Second IIAI International Conference on Advanced Applied Informatics,31 Aug.-4 Sept. 2013,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISMR48346.2021.9661514,Learning from Demonstrations for Autonomous Soft-tissue Retraction,IEEE,Conferences,"The current research focus in Robot-Assisted Minimally Invasive Surgery (RAMIS) is directed towards increasing the level of robot autonomy, to place surgeons in a supervisory position. Although Learning from Demonstrations (LfD) approaches are among the preferred ways for an autonomous surgical system to learn expert gestures, they require a high number of demonstrations and show poor generalization to the variable conditions of the surgical environment. In this work, we propose an LfD methodology based on Generative Adversarial Imitation Learning (GAIL) that is built on a Deep Reinforcement Learning (DRL) setting. GAIL combines generative adversarial networks to learn the distribution of expert trajectories with a DRL setting to ensure generalisation of trajectories providing human-like behaviour. We consider automation of tissue retraction, a common RAMIS task that involves soft tissues manipulation to expose a region of interest. In our proposed methodology, a small set of expert trajectories can be acquired through the da Vinci Research Kit (dVRK) and used to train the proposed LfD method inside a simulated environment. Results indicate that our methodology can accomplish the tissue retraction task with human-like behaviour while being more sample-efficient than the baseline DRL method. Towards the end, we show that the learnt policies can be successfully transferred to the real robotic platform and deployed for soft tissue retraction on a synthetic phantom.",https://ieeexplore.ieee.org/document/9661514/,2021 International Symposium on Medical Robotics (ISMR),17-19 Nov. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IS.2018.8710525,Learning from Virtual Experience: Mapless Navigation with Neuro-Fuzzy Intelligence,IEEE,Conferences,"Traditional robot navigation approaches normally rely on creating a precise map of the environment which is a computationally expensive procedure and highly depends on an accurate sensory system. Even for motion planning in similar terrains, the planner needs to prepare or obtain a map beforehand. In this paper, this issue is addressed, and a neurofuzzy motion planner is presented for mobile robot navigation without a map. We show that, by means of a virtual experience model and a neuro-fuzzy system, a mapless motion planning approach can learn basic navigation primitives in simple obstacle arrangements without any prior demonstration. The virtual experience model creates a large number of test environments with a random set of arbitrarily shaped obstacles and places the robot in a random pose with different start and goal positions in different instances. Then, based on the readings of the robot's sensors and a collection of predefined general linguistic rules, a set of control commands including the robot's linear and angular velocity is calculated as the outputs of the virtual experience. The resulting dataset is then loaded into an adaptive neuro-fuzzy inference system to create and optimize a fuzzy motion planner using the subtractive clustering method and a hybrid technique combining the back-propagation algorithm and the least square adaptation method respectively, which guides the robot in simple unknown environments without requiring a global obstacle map. To validate the effectiveness of the proposed model, the motion planner was implemented on a nonholonomic differential drive robot to test its performance in two real navigation tasks. Experimental studies show that the proposed mapless motion planner can efficiently guide the robot in similar arrangements of convex obstacles.",https://ieeexplore.ieee.org/document/8710525/,2018 International Conference on Intelligent Systems (IS),25-27 Sept. 2018,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICSMC.2010.5641727,Learning from conflicts in real world environments for the realization of Cognitive Technical Systems,IEEE,Conferences,"In this contribution, a novel learning method realizing the refinement of a Cognitive Technical System's pattern recognition and attention capabilities is presented. The method is implemented within a cognitive architecture with a representational level based on Situation-Operator-Modeling and high-level Petri Nets. Through the representational level, it is possible to realize a mental model mapping the complex structure of the real world internally in a compact format reduced to the relevant aspects. The mental model can be created and modified automatically by learning from interaction. If the perceived real world does not correspond to the system's mental model, the system detects ambiguities (or conflicts) inevitably. Then, the system tries to solve the conflicts by a more detailed view to the measured sensor inputs. Thus, new significant features (on a high abstraction level) can be derived from the measurements and taken into account to distinguish different (before apparently equal) situations. The contribution describes the proposed method and its fundamentals in detail. Furthermore, the realization of a cognitive mobile robot is presented as an application example illustrating the proposed method.",https://ieeexplore.ieee.org/document/5641727/,"2010 IEEE International Conference on Systems, Man and Cybernetics",10-13 Oct. 2010,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CDC.1998.761747,"Learning helicopter control through ""teaching by showing""",IEEE,Conferences,"A model-free ""teaching by showing"" methodology is developed to train a fuzzy-neural controller for an autonomous robot helicopter. The controller is generated and tuned using training data gathered while a teacher operates the helicopter. A hierarchical behavior-based control architecture is used, with each behavior implemented as a hybrid fuzzy logic controller (FLC) and general regression neural network controller (GRNNC). The FLCs and GRNNCs are generated through ""teaching by showing"". The FLCs are built during initial controller generation, remain static once created, and provide coarse control of the helicopter. The GRNNCs are incrementally built and modified whenever the controller does not meet performance criteria, are dynamic, and provide fine control, enhancing the control of the FLCs. The methodology has been successfully applied in simulation and, in the future, will be applied on a radio control model helicopter for real world validation.",https://ieeexplore.ieee.org/document/761747/,Proceedings of the 37th IEEE Conference on Decision and Control (Cat. No.98CH36171),18-18 Dec. 1998,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS.2010.5651719,Learning interaction protocols using Augmented Baysian Networks applied to guided navigation,IEEE,Conferences,"Research in robot navigation usually concentrates on implementing navigation algorithms that allow the robot to navigate without human aid. In many real world situations, it is desirable that the robot is able to understand natural gestures from its user or partner and use this understanding to guide its navigation. Some algorithms already exist for learning natural gestures and/or their associated actions but most of these systems does not allow the robot to automatically generate the associated controller that allows it to actually navigate in the real environment. Furthermore, a technique is needed to combine the gestures/actions learned from interacting with multiple users or partners. This paper resolves these two issues and provides a complete system that allows the robot to learn interaction protocols and act upon them using only unsupervised learning techniques and enables it to combine the protocols learned from multiple users/partners. The proposed approach is general and can be applied to other interactive tasks as well. This paper also provides a real world experiment involving 18 subjects and 72 sessions that supports the ability of the proposed system to learn the needed gestures and to improve its knowledge of different gestures and their associations to actions over time.",https://ieeexplore.ieee.org/document/5651719/,2010 IEEE/RSJ International Conference on Intelligent Robots and Systems,18-22 Oct. 2010,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA.2012.6224553,Learning organizational principles in human environments,IEEE,Conferences,"In the context of robotic assistants in human everyday environments, pick and place tasks are beginning to be competently solved at the technical level. The question of where to place objects or where to pick them up from, among other higher-level reasoning tasks, is therefore gaining practical relevance. In this work, we consider the problem of identifying the organizational structure within an environment, i.e. the problem of determining organizational principles that would allow a robot to infer where to best place a particular, previously unseen object or where to reasonably search for a particular type of object given past observations about the allocation of objects to locations in the environment. This problem can be reasonably formulated as a classification task. We claim that organizational principles are governed by the notion of similarity and provide an empirical analysis of the importance of various features in datasets describing the organizational structure of kitchens. For the aforementioned classification tasks, we compare standard classification methods, reaching average accuracies of at least 79% in all scenarios. We thereby show that, in particular, ontology-based similarity measures are well-suited as highly discriminative features. We demonstrate the use of learned models of organizational principles in a kitchen environment on a real robot system, where the robot identifies a newly acquired item, determines a suitable location and then stores the item accordingly.",https://ieeexplore.ieee.org/document/6224553/,2012 IEEE International Conference on Robotics and Automation,14-18 May 2012,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IIAI-AAI.2014.174,Learning through Imitation and Reinforcement Learning: Toward the Acquisition of Painting Motions,IEEE,Conferences,"Learning is essential for an autonomous agent to adapt to an environment. One method of learning is through trial and error, however, this method is impractical in a complex environment because of the long learning time required by the agent. Therefore, guidelines are necessary in order to expedite the learning process in such environments, and imitation is one such guideline. Sakato, Ozeki, and Oka (2012-2013) recently proposed a computational model of imitation and autonomous behavior by which an agent can reduce its learning time through imitation. They evaluate the model in discrete and continuous spaces, and apply the model to a real robot in order to acquire painting skills. Their experimental results indicate that the model adapted to the experimental environment by imitation. In this paper, we introduce the model and discuss what are needed to improve the model.",https://ieeexplore.ieee.org/document/6913418/,2014 IIAI 3rd International Conference on Advanced Applied Informatics,31 Aug.-4 Sept. 2014,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA.2016.7487506,Learning time series models for pedestrian motion prediction,IEEE,Conferences,"Robot systems deployed in real-world environments often need to interact with other dynamic objects, such as pedestrians, cars, bicycles or other vehicles. In such cases, it is useful to have a good predictive model of the object's motion to factor in when optimizing the robot's own behaviour. In this paper we consider motion models cast in the Predictive Linear Gaussian (PLG) model, and propose two learning approaches for this framework: one based on the method of moments and the other on a least-squares criteria. We evaluate the approaches on several synthetic datasets, and deploy the system on a wheelchair robot, to improve its ability to follow a walking companion.",https://ieeexplore.ieee.org/document/7487506/,2016 IEEE International Conference on Robotics and Automation (ICRA),16-21 May 2016,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS.2018.8594204,Learning to Fly by MySelf: A Self-Supervised CNN-Based Approach for Autonomous Navigation,IEEE,Conferences,"Nowadays, Unmanned Aerial Vehicles (UAVs)are becoming increasingly popular facilitated by their extensive availability. Autonomous navigation methods can act as an enabler for the safe deployment of drones on a wide range of real-world civilian applications. In this work, we introduce a self-supervised CNN-based approach for indoor robot navigation. Our method addresses the problem of real-time obstacle avoidance, by employing a regression CNN that predicts the agent's distance-to-collision in view of the raw visual input of its on-board monocular camera. The proposed CNN is trained on our custom indoor-flight dataset which is collected and annotated with real-distance labels, in a self-supervised manner using external sensors mounted on an UAV. By simultaneously processing the current and previous input frame, the proposed CNN extracts spatio-temporal features that encapsulate both static appearance and motion information to estimate the robot's distance to its closest obstacle towards multiple directions. These predictions are used to modulate the yaw and linear velocity of the UAV, in order to navigate autonomously and avoid collisions. Experimental evaluation demonstrates that the proposed approach learns a navigation policy that achieves high accuracy on real-world indoor flights, outperforming previously proposed methods from the literature.",https://ieeexplore.ieee.org/document/8594204/,2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),1-5 Oct. 2018,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA46639.2022.9811554,"Learning to Rock-and-Walk: Dynamic, Non-Prehensile, and Underactuated Object Locomotion Through Reinforcement Learning",IEEE,Conferences,"When moving objects that are too bulky or heavy to be grasped or lifted, robotic manipulation can benefit from the object&#x0027;s interaction with the support surface and its natural dynamics under gravity. In this work, we show that such dynamic, underactuated manipulation capability can be acquired through reinforcement learning and deployed on real robot systems. First, we present a framework to learn a control policy for object transport in a dynamic simulation environment, featuring the object and the support surface. We then demonstrate successful object locomotion with the learned policy through a set of simulated and real-world experiments, performed with a robot arm and an aerial robot interacting with the object in a non-prehensile manner. While the object, which is in contact with the support surface, oscillates sideways passively under gravity, the robot uses the learned policy to move the object forward with a steady gait by regulating the mechanical energy and the posture of the object. Our experiment results show that the learned policy can transport the object through unmodeled effects of terrain and perturbation.",https://ieeexplore.ieee.org/document/9811554/,2022 International Conference on Robotics and Automation (ICRA),23-27 May 2022,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA46639.2022.9812363,Learning-based Ellipse Detection for Robotic Grasps of Cylinders and Ellipsoids,IEEE,Conferences,"In our daily life, there are many objects represented by cylindrical shapes and ellipsoids. The tops of these objects are formed by elliptic shape primitives. Thus, it is available for a robot to manipulate these objects by ellipse detection. In this work, we propose a novel approach to generating ground truth for training the model based on domain randomization. Using synthetic data generated in this manner, we build an end-to-end deep neural network with a detection backbone and then, combine multiple branches archived from the backbone for sharing the multiple-scale features; further, after employing active rotation filters, the features pass through the region proposal net to form the prediction branches of the box, orientation regression, and object classification; finally, these branches are fused to do ellipse detection, allowing robotic manipulations of cylinders and ellipsoids. To demonstrate the capabilities of the proposed detector, we show the comparison results with the state-of-the-art detector on synthetic and public datasets. The proposed model for ellipse detection and data generation pipeline based on domain randomization in a simulation are evaluated by a series of robotic manipulations implemented in real application scenarios. The results illustrate a high success rate on real-world grasp attempts despite having only been trained on a synthetic dataset. (A video of some robotic experiments is available on YouTube: https://youtu.be/Ueg1XSI2S98).",https://ieeexplore.ieee.org/document/9812363/,2022 International Conference on Robotics and Automation (ICRA),23-27 May 2022,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISCAS.2015.7169038,Live demonstration: Spiking neural circuit based navigation inspired by C. elegans thermotaxis,IEEE,Conferences,"We demonstrate a Spiking Neural Network (SNN) driven autonomous navigation system implemented on a robot. The neural architecture is inspired by those in nematode Caenorhabditis elegans used for thermotaxis, the behavior of tracking thermal isotherms. Our network uses light intensity as the sensor input, instead of temperature in the worm. The network is able to detect the gradations in sensor-input based on local information, and to make decisions in real time. This enables the robot to do a random search and to track specific intensity regions.",https://ieeexplore.ieee.org/document/7169038/,2015 IEEE International Symposium on Circuits and Systems (ISCAS),24-27 May 2015,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS40897.2019.8968004,Long Range Neural Navigation Policies for the Real World,IEEE,Conferences,"Learned Neural Network based policies have shown promising results for robot navigation. However, most of these approaches fall short of being used on a real robot due to the extensive simulated training they require. These simulations lack the visuals and dynamics of the real world, which makes it infeasible to deploy on a real robot. We present a novel Neural Net based policy, NavNet, which allows for easy deployment on a real robot. It consists of two sub policies - a high level policy which can understand real images and perform long range planning expressed in high level commands; a low level policy that can translate the long range plan into low level commands on a specific platform in a safe and robust manner. For every new deployment, the high level policy is trained on an easily obtainable scan of the environment modeling its visuals and layout. We detail the design of such an environment and how one can use it for training a final navigation policy. Further, we demonstrate a learned low-level policy. We deploy the model in a large office building and test it extensively, achieving 0.80 success rate over long navigation runs and outperforming SLAM-based models in the same settings.",https://ieeexplore.ieee.org/document/8968004/,2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),3-8 Nov. 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SOCC.2016.7905478,Low-power real-time intelligent SoCs for smart machines,IEEE,Conferences,"In this paper, we introduce low-power and real-time intelligent SoCs aimed at smart machines. To implement intelligent functions under low-power consumption, machine learning methods are tightly integrated with the traditional algorithms. At first, an object recognition processor (ORP) accelerating scale-invariant feature transform (SIFT) is presented with a visual attention based on convolutional neural network (CNN). For user interface (UI), a speech and gesture recognition processor (SGRP) based on convolutional deep belief network (CDBN) is presented with a voice activity detection (VAD) and a hand segmentation. At last, an artificial intelligence processor (AIP) for autonomous navigation is presented using A* tree search for path planning and reinforcement learning (RL) for dynamic obstacle avoidance. As a result, a prototype robot system integrating the presented SoCs is implemented and successfully demonstrated in the indoor environment.",https://ieeexplore.ieee.org/document/7905478/,2016 29th IEEE International System-on-Chip Conference (SOCC),6-9 Sept. 2016,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CVPRW.2019.00020,M2U-Net: Effective and Efficient Retinal Vessel Segmentation for Real-World Applications,IEEE,Conferences,"In this paper, we present a novel neural network architecture for retinal vessel segmentation that improves over the state of the art on two benchmark datasets, is the first to run in real time on high resolution images, and its small memory and processing requirements make it deployable in mobile and embedded systems. The M2U-Net has a new encoder-decoder architecture that is inspired by the U-Net. It adds pretrained components of MobileNetV2 in the encoder part and novel contractive bottleneck blocks in the decoder part that, combined with bilinear upsampling, drastically reduce the parameter count to 0.55M compared to 31.03M in the original U-Net. We have evaluated its performance against a wide body of previously published results on three public datasets. On two of them, the M2U-Net achieves new state-of-the-art performance by a considerable margin. When implemented on a GPU, our method is the first to achieve real-time inference speeds on high-resolution fundus images. We also implemented our proposed network on an ARM-based embedded system where it segments images in between 0.6 and 15 sec, depending on the resolution. Thus, the M2U-Net enables a number of applications of retinal vessel structure extraction, such as early diagnosis of eye diseases, retinal biometric authentication systems, and robot assisted microsurgery.",https://ieeexplore.ieee.org/document/9025339/,2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW),16-17 June 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS45743.2020.9340876,MAPPER: Multi-Agent Path Planning with Evolutionary Reinforcement Learning in Mixed Dynamic Environments,IEEE,Conferences,"Multi-agent navigation in dynamic environments is of great industrial value when deploying a large scale fleet of robot to real-world applications. This paper proposes a decentralized partially observable multi-agent path planning with evolutionary reinforcement learning (MAPPER) method to learn an effective local planning policy in mixed dynamic environments. Reinforcement learning-based methods usually suffer performance degradation on long-horizon tasks with goal-conditioned sparse rewards, so we decompose the long-range navigation task into many easier sub-tasks under the guidance of a global planner, which increases agents' performance in large environments. Moreover, most existing multi-agent planning approaches assume either perfect information of the surrounding environment or homogeneity of nearby dynamic agents, which may not hold in practice. Our approach models dynamic obstacles' behavior with an image-based representation and trains a policy in mixed dynamic environments without homogeneity assumption. To ensure multi-agent training stability and performance, we propose an evolutionary training approach that can be easily scaled to large and complex environments. Experiments show that MAPPER is able to achieve higher success rates and more stable performance when exposed to a large number of non-cooperative dynamic obstacles compared with traditional reaction-based planner LRA* and the state-of-the-art learning-based method.",https://ieeexplore.ieee.org/document/9340876/,2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),24 Oct.-24 Jan. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA48506.2021.9561878,MFPN-6D : Real-time One-stage Pose Estimation of Objects on RGB Images,IEEE,Conferences,"6D pose estimation of objects is an important part of robot grasping. The latest research trend on 6D pose estimation is to train a deep neural network to directly predict the 2D projection position of the 3D key points from the image, establish the corresponding relationship, and finally use Pespective-n-Point (PnP) algorithm performs pose estimation. The current challenge of pose estimation is that when the object texture-less, occluded and scene clutter, the detection accuracy will be reduced, and most of the existing algorithm models are large and cannot take the real-time requirements. In this paper, we introduce a Multi-directional Feature Pyramid Network, MFPN, which can efficiently integrate and utilize features. We combined the Cross Stage Partial Network (CSPNet) with MFPN to design a new network for 6D pose estimation, MFPN-6D. At the same time, we propose a new confidence calculation method for object pose estimation, which can fully consider spatial information and plane information. At last, we tested our method on the LINEMOD and Occluded-LINEMOD datasets. The experimental results demonstrate that our algorithm is robust to textureless materials and occlusion, while running more efficiently compared to other methods.",https://ieeexplore.ieee.org/document/9561878/,2021 IEEE International Conference on Robotics and Automation (ICRA),30 May-5 June 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ComPE49325.2020.9200129,Machine Learning Algorithm based Disease Detection in Tomato with Automated Image Telemetry for Vertical Farming,IEEE,Conferences,"This paper is highlighting an outline of disease detection in tomato using computer vision and machine learning algorithms. Readily available hardware is used to build a system where a camera mounted system can detect and identify spot disease in tomatoes in real-time. As an initial prototype only spot disease can be detected. The complete development can be divided into two parts. The first part is the software and algorithm which aimed to detect and identify disease in crops and generate a report for the user. It is successful in building the algorithm and GUI (graphical user interface) for the user which can detect spot disease in tomatoes. Using the Viola-Jones algorithm and Haar like feature extraction method for the machine learning process in MATLAB, an XML (an image trained file) file for spot disease in tomatoes is designed using 377 images of infected tomatoes. The second part is the hardware implementation which consists of a simple robot rig that carries the camera and the system scans the tomatoes for the disease. For the vast majority of the time, spot detection is accurate. Many other diseases which exist for the animal, human and crops can easily be added to the system. In terms of reliability, the system is a success with acceptable false positives.",https://ieeexplore.ieee.org/document/9200129/,2020 International Conference on Computational Performance Evaluation (ComPE),2-4 July 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AIKE.2018.00051,Management of Subdivided Dynamic Indoor Environments by Autonomous Scanning System,IEEE,Conferences,"With the development of sensing technologies, various spatial applications have been expanding into indoor spaces. For smooth spatial services, grasping indoor space information is most essential task. However, the indoor spaces is not only becoming increasingly complex, but also frequently changed than outdoor spaces. This makes it hard to provide an accurate location based service in an indoor space. This paper propose a way of managing a dynamic indoor environment by defining a multi-layered indoor model in terms of an object mobility. It allows an indoor space to be managed more elaborate and realistic than up-to-date indoor models which only consider an indoor floor plan. We firstly define a classification of indoor objects based on their characteristic to frequently change location, and propose three-layers indoor model followed by the classified objects with its mobility. Secondly, we design and implement an autonomous scanning system to understand changes of indoor situation quickly and automatically. The system is made up of a combination of IoT devices, including a programmable robot, lidar scanner and single-board computer. Finally, we demonstrate an implementation of the system with constructing the proposed model from a real indoor environment.",https://ieeexplore.ieee.org/document/8527483/,2018 IEEE First International Conference on Artificial Intelligence and Knowledge Engineering (AIKE),26-28 Sept. 2018,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICMLA51294.2020.00021,MaskedFusion: Mask-based 6D Object Pose Estimation,IEEE,Conferences,"MaskedFusion is a framework to estimate the 6D pose of objects using RGB-D data, with an architecture that leverages multiple sub-tasks in a pipeline to achieve accurate 6D poses. 6D pose estimation is an open challenge due to complex world objects and many possible problems when capturing data from the real world, e.g., occlusions, truncations, and noise in the data. Achieving accurate 6D poses will improve results in other open problems like robot grasping or positioning objects in augmented reality. MaskedFusion improves the state-of-the-art by using object masks to eliminate non-relevant data. With the inclusion of the masks on the neural network that estimates the 6D pose of an object we also have features that represent the object shape. MaskedFusion is a modular pipeline where each sub-task can have different methods that achieve the objective. MaskedFusion achieved 97.3% on average using the ADD metric on the LineMOD dataset and 93.3% using the ADD-S AUC metric on YCB-Video Dataset, which is an improvement, compared to the state-of-the-art methods.",https://ieeexplore.ieee.org/document/9356139/,2020 19th IEEE International Conference on Machine Learning and Applications (ICMLA),14-17 Dec. 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA40945.2020.9196540,Meta Reinforcement Learning for Sim-to-real Domain Adaptation,IEEE,Conferences,"Modern reinforcement learning methods suffer from low sample efficiency and unsafe exploration, making it infeasible to train robotic policies entirely on real hardware. In this work, we propose to address the problem of sim-to-real domain transfer by using meta learning to train a policy that can adapt to a variety of dynamic conditions, and using a task-specific trajectory generation model to provide an action space that facilitates quick exploration. We evaluate the method by performing domain adaptation in simulation and analyzing the structure of the latent space during adaptation. We then deploy this policy on a KUKA LBR 4+ robot and evaluate its performance on a task of hitting a hockey puck to a target. Our method shows more consistent and stable domain adaptation than the baseline, resulting in better overall performance.",https://ieeexplore.ieee.org/document/9196540/,2020 IEEE International Conference on Robotics and Automation (ICRA),31 May-31 Aug. 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICMLC.2010.71,Microcontroller Based Neural Network Controlled Low Cost Autonomous Vehicle,IEEE,Conferences,"In this paper, design of a low cost autonomous vehicle based on neural network for navigation in unknown environments is presented. The vehicle is equipped with four ultrasonic sensors for hurdle distance measurement, a wheel encoder for measuring distance traveled, a compass for heading information, a GPS receiver for goal position information, a GSM modem for changing destination place on run time and a nonvolatile RAM for storing waypoint data; all interfaced to a low cost AT89C52 microcontroller. The microcontroller processes the information acquired from the sensors and generates robot motion commands accordingly through neural network. The neural network running inside the microcontroller is a multilayer feed-forward network with back-propagation training algorithm. The network is trained offline with tangent-sigmoid as activation function for neurons and is implemented in real time with piecewise linear approximation of tangent-sigmoid function. Results have shown that upto twenty neurons can be implemented in hidden layer with this technique. The vehicle is tested with varying destination places in outdoor environments containing stationary as well as moving obstacles and is found to reach the set targets successfully.",https://ieeexplore.ieee.org/document/5460762/,2010 Second International Conference on Machine Learning and Computing,9-11 Feb. 2010,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AIVR50618.2020.00017,Mirrorlabs - creating accessible Digital Twins of robotic production environment with Mixed Reality,IEEE,Conferences,How to visualize recorded production data in Virtual Reality? How to use state of the art Augmented Reality displays that can show robot data? This paper introduces an opensource ICT framework approach for combining Unity-based Mixed Reality applications with robotic production equipment using ROS Industrial. This publication gives details on the implementation and demonstrates the use as a data analysis tool in the context of scientific exchange within the area of Mixed Reality enabled human-robot co-production.,https://ieeexplore.ieee.org/document/9319071/,2020 IEEE International Conference on Artificial Intelligence and Virtual Reality (AIVR),14-18 Dec. 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IWRSP.1999.779052,Model based multi-level prototyping,IEEE,Conferences,"In this paper, we present our approach to rapid prototyping of robot software. We propose model based multi-level prototyping using UML in combination with a refinement design flow to synchronize development of an early virtual prototype, detailed simulation models and the final real prototype. This is achieved by a core model which is the common reference for model based multi-level prototyping. We demonstrate our methodology at hand of the design of a motor control software for the RoboCup robot platform of GMD. We show that parameters obtained with the virtual prototype and tested in the simulation model are well suited estimations for the final real prototype and therefore allow to reduce time-consuming experiments with the real prototype to a minimum.",https://ieeexplore.ieee.org/document/779052/,Proceedings Tenth IEEE International Workshop on Rapid System Prototyping. Shortening the Path from Specification to Prototype (Cat. No.PR00246),16-18 June 1999,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ROBOT.1991.131806,"Model based, sensor directed remediation of underground storage tanks",IEEE,Conferences,"Experimental investigations into the application of intelligent robot control technology to the problem of removing waste stored in tanks are described. The authors discuss the experimental environment employed, with particular attention to the computing and software control environment. Intelligent system control is achieved through the integration of extensive geometric and kinematic world models with real-time sensor-based control. All operator interactions with the system are through fully animated, graphical representations which validate all operator commands before execution to provide for safe operation. Sensing is used to add information to the robot system's world model and to allow sensor-based servo control during selected operations. Initial test results are reported, and the potential for applying advanced intelligent control concepts to the removal of waste in storage tanks is discussed.<>",https://ieeexplore.ieee.org/document/131806/,Proceedings. 1991 IEEE International Conference on Robotics and Automation,9-11 April 1991,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ROBOT.1992.220046,Model-driven pose correction,IEEE,Conferences,"Pose determination for robot navigation is discussed. The problem is to maintain the system's instantaneous precept of its position and orientation in space for performing various tasks. The authors describe a system in which models were used to guide the sensory interpretation and to correct expectations. In this system, simulated images were used to analyze the real images and to correct the pose parameters. The reported techniques have been implemented and experiments with real images in a real environment have been performed.<>",https://ieeexplore.ieee.org/document/220046/,Proceedings 1992 IEEE International Conference on Robotics and Automation,12-14 May 1992,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/MLSP.2015.7324313,Modelling LGMD2 visual neuron system,IEEE,Conferences,"Two Lobula Giant Movement Detectors (LGMDs) have been identified in the lobula region of the locust visual system: LGMD1 and LGMD2. LGMD1 had been successfully used in robot navigation to avoid impending collision. LGMD2 also responds to looming stimuli in depth, and shares most the same properties with LGMD1; however, LGMD2 has its specific collision selective responds when dealing with different visual stimulus. Therefore, in this paper, we propose a novel way to model LGMD2, in order to emulate its predicted bio-functions, moreover, to solve some defects of previous LGMD1 computational models. The mechanism of ON and OFF cells, as well as bio-inspired nonlinear functions, are introduced in our model, to achieve LGMD2's collision selectivity. Our model has been tested by a miniature mobile robot in real time. The results suggested this model has an ideal performance in both software and hardware for collision recognition.",https://ieeexplore.ieee.org/document/7324313/,2015 IEEE 25th International Workshop on Machine Learning for Signal Processing (MLSP),17-20 Sept. 2015,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/iCMLDE.2018.00023,Monocular SLAM and Obstacle Removal for Indoor Navigation,IEEE,Conferences,"Visual Simultaneous Localization and Mapping (SLAM) is one of the hot topics in computer vision. For the past few years, the AI and deep learning technology research have been widespread used in self-driving technology and surveillance system etc., gaining more and more attention from researchers and public media. The combination of AI technology and robot perception is inevitably going to be a trend. This paper aims at removing the obstacle to enhance the SLAM system performance that based on popular open source framework ORB-SLAM2 in dynamic environment. Moving objects will bring noise in camera pose estimation, besides, when in re-localization, the robot returns to the previous place finding the previous landmark mismatches because of its movement. The system will be confused and misdirected. A novel approach is proposed to remove the obstacle in real environment by using convolutional neural network (CNN) to generate a segmentation mask of obstacle object so as to eliminate the interference by moving object. Our experiment result shows an impressive outcome of practical use and benchmark dataset test.",https://ieeexplore.ieee.org/document/8614006/,2018 International Conference on Machine Learning and Data Engineering (iCMLDE),3-7 Dec. 2018,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS51168.2021.9635912,Monocular Teach-and-Repeat Navigation using a Deep Steering Network with Scale Estimation,IEEE,Conferences,"This paper proposes a novel monocular teach-and-repeat navigation system with the capability of scale awareness, i.e. the absolute distance between observation and goal images. It decomposes the navigation task into a sequence of visual servoing sub-tasks to approach consecutive goal/node images in a topological map. To be specific, a novel hybrid model, named deep steering network is proposed to infer the navigation primitives according to the learned local feature and scale for each visual servoing sub-task. A novel architecture, Scale-Transformer, is developed to estimate the absolute scale between the observation and goal image pair from a set of matched deep representations to assist repeating navigation. The experiments demonstrate that our scale-aware teach-and-repeat method achieves satisfying navigation accuracy, and converges faster than the monocular methods without scale correction given an inaccurate initial pose. The proposed network is integrated into an onboard system deployed on a real robot to achieve real-time navigation in a real environment. A demonstration video can be found online: https://youtu.be/ctlwDaMKnHw",https://ieeexplore.ieee.org/document/9635912/,2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),27 Sept.-1 Oct. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CBS46900.2019.9114416,"Motion Prediction of Virtual Patterns, Human Hand Motions, and a simplified Hand Manipulation Task with Hierarchical Temporal Memory",IEEE,Conferences,"In this paper we utilize Numenta's Hierarchical Temporal Memory implementation NuPIC for online visual motion pattern prediction and test its performance on virtual animations as well as real world human motion data. For evaluation we run a series of progressively more complex experiments testing specific capabilities: Prediction of fixed-time noise-free motion animations, prediction of protocol-directed tasks with real-world camera captured human motion data, and lastly prediction of repetitive tasks performed without a strict protocol. Results show that the presented setup is able to predict time sequenced images as well as highly variable human motions increasingly well over several iterations. Limits are faced for non sequential variable hand motion execution: Here, predictions are made but do not improve in quality over time. The network runs online in real time and can be transferred to different tasks without expert knowledge. These characteristics qualify the setup for human robot interaction scenarios without the need for verified prediction accuracy.",https://ieeexplore.ieee.org/document/9114416/,2019 IEEE International Conference on Cyborg and Bionic Systems (CBS),18-20 Sept. 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CIMSA.2009.5069917,Motion planning in unknown environment using an interval fuzzy type-2 and neural network classifier,IEEE,Conferences,"This paper describes environmental recognition and motion control using weightless neural network classifier and interval type-2 fuzzy logic controller. The weightless neural network classifies geometric feature such as U-shape, corridor and left or right corner using ultrasonic sensors. The neural network utilizes previous sensor data and analyzes the situation of the current environment. The behavior of mobile robot is implemented by means of fuzzy control rules. Based on the performance criteria the quality of controller is evaluated to make navigation decisions. This functionality is demonstrated on a mobile robot using modular platform and containing several microcontrollers implies the implementation of a robust architecture. The proposed architecture implemented using low cost range sensor and low cost microprocessor. The experiment results show that the mobile robot can recognize the current environment and was able to successfully avoid obstacle in real time.",https://ieeexplore.ieee.org/document/5069917/,2009 IEEE International Conference on Computational Intelligence for Measurement Systems and Applications,11-13 May 2009,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS51168.2021.9636777,Multi-Object Grasping – Estimating the Number of Objects in a Robotic Grasp,IEEE,Conferences,"A human hand can grasp a desired number of objects at once from a pile based solely on tactile sensing. To do so, a robot needs to make a grasp in a pile, sense the number of objects in the grasp before lifting, and predict how many will remain in the grasp after lifting. It is a very challenging problem because when making the prediction, the robotic hand is still in the pile and the objects in the grasp are not observable to vision systems. Moreover, some objects in the hand before lifting may fall out the grasp when the lifting starts because they were supported by other objects in the pile instead of the fingers. A robotic hand should sense how many objects are in a grasp using its tactile sensors before lifting. This paper presents novel multi-object grasping analyzing methods to solve this problem. They include a grasp volume calculation, tactile force analysis, and a data-driven deep learning approach. The methods have been implemented on a Barrett hand and then evaluated in simulations and a real setup with a robotic system. The evaluation results conclude that once the Barrett hand grasps multiple objects in the pile, the data-driven models can make a good prediction before lifting on how many objects will remain in the hand after lifting. The root-mean-square errors are 0.74 for balls and 0.58 for cubes in simulations, and 1.06 for balls and 1.45 for cubes in the real system.",https://ieeexplore.ieee.org/document/9636777/,2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),27 Sept.-1 Oct. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICAICE54393.2021.00059,Multi-Rotor UAV Autonomous Tracking and Obstacle Avoidance Based on Improved DDPG,IEEE,Conferences,"To solve the problem of multi-rotor UAV autonomous tracking dynamic ground targets in obstacles environment, we used Markov decision process (MDP) to establish an autonomous maneuvering model of multi-rotor. Considering the obstacle avoidance requirements of UAV during the tracking process, we integrated the Long Short-Term Memory (LSTM) neural network with memory unit and time series data processing characteristics into the Deep Deterministic Policy Gradient (DDPG) algorithm framework, so that the Actor network can fully refer to the prior state information when making decisions. Finally, the performance test was implemented on the UAV 3D simulation platform based on Robot Operating System (ROS). The results show that the method proposed in this paper can enable the UAV to complete the whole process of autonomous tracking of the ground dynamic target. Compared with the traditional DDPG algorithm, the DDPG algorithm combined with LSTM has stronger accuracy and real-time performance, and can better meet the tracking and obstacle avoidance mission requirements of the multi-rotor UAV.",https://ieeexplore.ieee.org/document/9797591/,2021 2nd International Conference on Artificial Intelligence and Computer Engineering (ICAICE),5-7 Nov. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CVPR46437.2021.00816,Multi-view Depth Estimation using Epipolar Spatio-Temporal Networks,IEEE,Conferences,"We present a novel method for multi-view depth estimation from a single video, which is a critical task in various applications, such as perception, reconstruction and robot navigation. Although previous learning-based methods have demonstrated compelling results, most works estimate depth maps of individual video frames independently, without taking into consideration the strong geometric and temporal coherence among the frames. Moreover, current state-of-the-art (SOTA) models mostly adopt a fully 3D convolution network for cost regularization and therefore require high computational cost, thus limiting their deployment in real-world applications. Our method achieves temporally coherent depth estimation results by using a novel Epipolar Spatio-Temporal (EST) transformer to explicitly associate geometric and temporal correlation with multiple estimated depth maps. Furthermore, to reduce the computational cost, inspired by recent Mixture-of-Experts models, we design a compact hybrid network consisting of a 2D context-aware network and a 3D matching network which learn 2D context information and 3D disparity cues separately. Extensive experiments demonstrate that our method achieves higher accuracy in depth estimation and significant speedup than the SOTA methods.",https://ieeexplore.ieee.org/document/9577311/,2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),20-25 June 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/MILTECHS.2017.7988861,Multiple people detection and identification system integrated with a dynamic simultaneous localization and mapping system for an autonomous mobile robotic platform,IEEE,Conferences,"This paper presents the integration of a multiple people detection and identification system with a dynamic simultaneous localization and mapping system for an autonomous robotic platform. This integration allows the exploration and navigation of the robot considering people identification. The robotic platform consists of a Pioneer 3DX robot equipped with an RGBD camera, a Sick Lms200 sensor laser and a computer using the robot operating system (ROS). The idea is to integrate the people detection and identification system to the simultaneous localization and mapping (SLAM) system of the robot using ROS. The people detection and identification system is performed in two steps. The first one is for detecting multiple people on scene and the other one is for an individual person identification. Both steps are implemented as ROS nodes that works integrated with the SLAM ROS node. The multiple people detection's node uses a manual feature extraction technique based on HOG (Histogram of Oriented Gradients) detectors, implemented using the PCL library (Point Cloud Library) in C ++. The person's identification node is based on a Deep Convolutional Neural Network (CNN) that are implemented using the MatLab MatConvNet library. This step receives the detected people centroid from the previous step and performs the classification of a specific person. After that, the desired person centroid is send to the SLAM node, that consider it during the mapping process. Tests were made objecting the evaluation of accurateness in the people's detection and identification process. It allowed us to evaluate the people detection system during the navigation and exploration of the robot, considering the real time interaction of people recognition in a semi-structured environment.",https://ieeexplore.ieee.org/document/7988861/,2017 International Conference on Military Technologies (ICMT),31 May-2 June 2017,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/LARS.2010.16,Navigation System for an Underground Distribution Inspection Platform Using Simulation,IEEE,Conferences,"This work proposes an architecture of an inspection robot's navigation system, aiming at monitoring of power cables in underground distribution lines. This architecture is composed of two modules: i. feature extraction from environment, ii navigation approach. The feature extraction module is based on the use of the edge detector by Canny algorithm and Hough transform for identification of lines from images of environment to monitoring. The lines identified correspond to cable conformation inside the duct. This information will serve to help the navigation system. For the implementation of the navigation system two approaches were proposed: navigation based on artificial neural network and navigation based on PID control. The navigation architecture can be used in real or simulated scenarios, and it was tested in a simulated environment.",https://ieeexplore.ieee.org/document/5702190/,2010 Latin American Robotics Symposium and Intelligent Robotics Meeting,23-28 Oct. 2010,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IJCNN.1999.832713,Nonlinear system adaptive trajectory tracking by dynamic neural control,IEEE,Conferences,"In this article, new nonlinear control techniques based on dynamic neural networks are presented. The authors discuss the implementation of a modified identification algorithm using dynamic neural networks as well as a control law, based on the neural identifier, which eliminates modeling error effects via sliding mode techniques. Simulation and real time results are presented for systems like an inverted pendulum and a full actuated robot manipulator.",https://ieeexplore.ieee.org/document/832713/,IJCNN'99. International Joint Conference on Neural Networks. Proceedings (Cat. No.99CH36339),10-16 July 1999,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISMVL.2015.39,Novel VLSI Architectures for Real-World Intelligent Systems,IEEE,Conferences,"A real-world intelligent systems platform based on novel architectures is presented in this article. As real-world applications, we consider advanced intelligent systems such as a highly-safe system and an intelligent robot system which make our daily life safe and comfortable.",https://ieeexplore.ieee.org/document/7238146/,2015 IEEE International Symposium on Multiple-Valued Logic,18-20 May 2015,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/INDIN45523.2021.9557354,Novelty Detection for Iterative Learning of MIMO Fuzzy Systems,IEEE,Conferences,"This paper proposes a methodology for iterative learning of multi-input multi-output (MIMO) fuzzy models focusing on dynamic system identification. The first step of the proposed method is the learning of the antecedent part of the fuzzy system, which is learned iteratively, where fuzzy rules can be added or merged based on the presented novelty detection and similarity criteria defined by a recursive extension of the Gath-Geva clustering algorithm. Then, the consequent part consists in the direct implementation of a non-recursive fuzzy approach that uses global least squares, Observer Kalman Filter Identification (OKID) and the Eigensystem Realization Algorithm (ERA). The proposed method is validated using experimental data from a real quadrotor aerial robot, a nonlinear dynamic system. Using quantitative performance metrics, the proposed method is compared with Hammerstein-Wiener models (H.-W.), nonlinear autoregressive models with exogenous input (NARX), and state-space models using subspace method with time-domain data (N4SID), other MIMO system identification techniques. The proposed method achieved better results compared to other techniques, showing the importance and versatility of learning based on novelty detection for MIMO problems.",https://ieeexplore.ieee.org/document/9557354/,2021 IEEE 19th International Conference on Industrial Informatics (INDIN),21-23 July 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CISCT.2019.8777441,Object Recognition for Dental Instruments Using SSD-MobileNet,IEEE,Conferences,"In recent technological developments, robot-assisted surgery has become popular due to its tremendous prospects in enhancing the capabilities of surgeons performing open surgery, yet very little effort has been made to make these tools available to dental surgeons. This paper addresses the problem of identifying the problem of real-time object recognition of dental instruments by utilizing deep learning techniques. For this reason, the Single Shot MultiBox Detector (SSD) network was considered as the meta structure and joined with the base Convolutional Neural Network (CNN) MobileNet to shape SSD-MobileNet. The task of object recognition for dental instruments like spatula, elevator, mouth mirror etc is performed, in order to constitute a robotic arm; that works with voice commands using speech recognition, and assists the dentist in surgery. Our method can recognize instruments more precisely and quickly as contrast with other lightweight system strategies and conventional machine learning techniques. We have achieved the precision and accuracy of 87.3% and 98.8% respectively.",https://ieeexplore.ieee.org/document/8777441/,2019 International Conference on Information Science and Communication Technology (ICISCT),9-10 March 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ROBOT.2004.1307138,Obstacle avoidance through incremental learning with attention selection,IEEE,Conferences,"This work presents a learning-based approach to the task of generating local reactive obstacle avoidance. The learning is performed online in real-time by a mobile robot. The robot operated in an unknown bounded 2-D environment populated by static or moving obstacles (with slow speeds) of arbitrary shape. The sensory perception was based on a laser range finder. To greatly reduce the number of training samples needed, an attentional mechanism was used. An efficient, real-time implementation of the approach had been tested, demonstrating smooth obstacle-avoidance behaviors in a corridor with a crowd of moving students as well as static obstacles.",https://ieeexplore.ieee.org/document/1307138/,"IEEE International Conference on Robotics and Automation, 2004. Proceedings. ICRA '04. 2004",26 April-1 May 2004,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ROBOT.2008.4543481,Online constraint network optimization for efficient maximum likelihood map learning,IEEE,Conferences,"In this paper, we address the problem of incrementally optimizing constraint networks for maximum likelihood map learning. Our approach allows a robot to efficiently compute configurations of the network with small errors while the robot moves through the environment. We apply a variant of stochastic gradient descent and use a tree-based parameterization of the nodes in the network. By integrating adaptive learning rates in the parameterization of the network, our algorithm can use previously computed solutions to determine the result of the next optimization run. Additionally, our approach updates only the parts of the network which are affected by the newly incorporated measurements and starts the optimization approach only if the new data reveals inconsistencies with the network constructed so far. These improvements yield an efficient solution for this class of online optimization problems. Our approach has been implemented and tested on simulated and on real data. We present comparisons to recently proposed online and offline methods that address the problem of optimizing constraint network. Experiments illustrate that our approach converges faster to a network configuration with small errors than the previous approaches.",https://ieeexplore.ieee.org/document/4543481/,2008 IEEE International Conference on Robotics and Automation,19-23 May 2008,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IJCNN.2009.5178844,Online temporal pattern learning,IEEE,Conferences,"This paper describes a biologically motivated approach, using hierarchical temporal memory (HTM), to build a high-level self-organizing visual system for a soccer bot. Meanwhile it presents two unsupervised online learning algorithms for temporal patterns in HTMs. The algorithms were implemented in a simulated soccer bot for a real-world evaluation. After a training phase, the robot was able to recognize different static objects in the soccer field. It also learned and recognized high-level objects that are composed of simpler objects, with position invariance and was also able to learn and recognize motions in the objects, all in a completely unsupervised manner.",https://ieeexplore.ieee.org/document/5178844/,2009 International Joint Conference on Neural Networks,14-19 June 2009,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS51168.2021.9636118,Optimal scheduling and non-cooperative distributed model predictive control for multiple robotic manipulators,IEEE,Conferences,"Application of multiple robotic manipulators in a shared workspace is still restricted to repetitive tasks limiting their flexible deployment for production systems. Still, existing motion control algorithms cannot be performed online for arbitrary environments in case of multiple manipulators cooperating with each other. In this work we propose a scalable and real-time capable motion control algorithm based on non-cooperative distributed model predictive control. Furthermore, we propose an optimal scheduling algorithm, which provides optimal setpoints to each robot’s motion controller that prevents possible deadlocks beforehand. We validate our approach on a simulative setup of four robotic manipulators for multiple pick and place scenarios.",https://ieeexplore.ieee.org/document/9636118/,2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),27 Sept.-1 Oct. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SII.2012.6426933,Optimization of obstacle avoidance using reinforcement learning,IEEE,Conferences,Walking through narrow space for multi-legged robot is optimized using reinforcement learning in this paper. The walking is generated by the virtual repulsive force from the estimated obstacle position and the virtual impedance field. The resulted action depends on the parameter of the virtual impedance coefficients. The reinforcement learning is employed to find an optimal motion. The temporal walking through motion consists of each parameter optimized for a situation. Optimization of integrated walking through motion is finally achieved evaluating walking in compound encountering obstacle on simulator. The resulted motion is implemented to a real multi-legged robot and results show the effectiveness of the proposed method.,https://ieeexplore.ieee.org/document/6426933/,2012 IEEE/SICE International Symposium on System Integration (SII),16-18 Dec. 2012,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS.1997.655095,Output methods for an associative operation of programmable artificial retinas,IEEE,Conferences,"The introduction of intelligence near each photosensitive element in focal plane arrays (FPA) leads to sensory devices-called artificial retinas-which may no longer output raw images but, rather, much more concentrated forms of information. In particular, when the on-sensor image processing facilities are powerful enough to allow some structural pattern recognition, lists of pixels of interest become an output format of choice from retina to microprocessor. This implies the development of specific output techniques and operators to be integrated in the focal plane. After an in-depth presentation of the motivations in the context of programmable artificial retinas (PAR) for robot vision, two original solutions to the problem are presented, corresponding to two different trade-offs between efficiency and VLSI implementation cost. The first one is a compact hardware solution, which allows to sense pixels of interest from the sides of the 2D pixel array. The second one, a mainly software technique, exploits the mathematical concept of de Bruijn arrays for a distributed encoding of pixel addresses on the PAR.",https://ieeexplore.ieee.org/document/655095/,Proceedings of the 1997 IEEE/RSJ International Conference on Intelligent Robot and Systems. Innovative Robotics for Real-World Applications. IROS '97,11-11 Sept. 1997,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ROBOT.1988.12120,Overview of the multiple autonomous underwater vehicles (MAUV) project,IEEE,Conferences,"The US National Bureau of Standard's multiple autonomous underwater vehicles (MAUV) project involves the development of a real-time intelligent control system that performs sensing, world modeling, planning, and execution for underwater robot vehicles. The goal of the project is to have multiple vehicles exhibiting intelligent, autonomous, cooperative behavior. Initial tests have involved two identical vehicles engaged in various scenarios in Lake Winnipesaukee in New Hampshire. All software for controlling the vehicles reside on computer boards mounted onboard the vehicles.<>",https://ieeexplore.ieee.org/document/12120/,Proceedings. 1988 IEEE International Conference on Robotics and Automation,24-29 April 1988,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IOTSMS48152.2019.8939254,Person Identification using Autonomous Drone through Resource Constraint Devices,IEEE,Conferences,"Detecting a specific person from the crowd using drone along with some resource constraint device is a major concern which we are discussing in the paper. Combining the advanced algorithms and some smart hardware material, we will be finding a way to search for a missing individual in a crowd or at some location. We can also search for a person at a specific location by setting our aerial vehicle to fly autonomously and search for the required person. This will help us to cover areas which cannot be reached by humans easily. The flying robot helps to solve real-time problems and come up with some new and more advanced ways to search for the missing ones with more ease, as advanced technological methods are applied, the probability of getting accurate results increases axiomatically. The drone can fly fully autonomously and search or capture videos/photos of the required location. Location commands could be given using PC, mobile and with the help of IoT, using Raspberry Pi.",https://ieeexplore.ieee.org/document/8939254/,"2019 Sixth International Conference on Internet of Things: Systems, Management and Security (IOTSMS)",22-25 Oct. 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CNSC.2014.6906671,Pixelwise object class segmentation based on synthetic data using an optimized training strategy,IEEE,Conferences,"In this paper we present an approach for low-level body part segmentation based on RGB-D data. The RGB-D sensor is thereby placed at the ceiling and observes a shared workspace for human-robot collaboration in the industrial domain. The pixelwise information about certain body parts of the human worker is used by a cognitive system for the optimization of interaction and collaboration processes. In this context, for rational decision making and planning, the pixelwise predictions must be reliable despite the high variability of the appearance of the human worker. In our approach we treat the problem as a pixelwise classification task, where we train a random decision forest classifier on the information contained in depth frames produced by a synthetic representation of the human body and the ceiling sensor, in a virtual environment. As shown in similar approaches, the samples used for training need to cover a broad spectrum of the geometrical characteristics of the human, and possible transformations of the body in the scene. In order to reduce the number of training samples and the complexity of the classifier training, we therefore apply an elaborated and coupled strategy for randomized training data sampling and feature extraction. This allows us to reduce the training set size and training time, by decreasing the dimensionality of the sampling parameter space. In order to keep the creation of synthetic training samples and real-world ground truth data simple, we use a highly reduced virtual representation of the human body, in combination with KINECT skeleton tracking data from a calibrated multi-sensor setup. The optimized training and simplified sample creation allows us to deploy standard hardware for the realization of the presented approach, while yielding a reliable segmentation in real-time, and high performance scores in the evaluation.",https://ieeexplore.ieee.org/document/6906671/,2014 First International Conference on Networks & Soft Computing (ICNSC2014),19-20 Aug. 2014,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS.1991.174701,Planning based sensing and task executing in an autonomous machine,IEEE,Conferences,"Implementing a control system for an autonomous machine is a challenging task. Several techniques have to be applied, such as task planning, hierarchical and/or distributed control, and advanced sensing techniques. In addition, to be useful these various techniques have to be integrated into a system that has to operate more or less in real-time. The authors present a control scheme based on hierarchically organized planning-executing-monitoring-cycles which is used to solve some of the problems related to real-time control of an autonomous machine. The implementation is also presented in which the control system is applied in a pilot system based on an industrial robot.<>",https://ieeexplore.ieee.org/document/174701/,Proceedings IROS '91:IEEE/RSJ International Workshop on Intelligent Robots and Systems '91,3-5 Nov. 1991,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICTAI.2006.96,Polynomial Regression with Automated Degree: A Function Approximator for Autonomous Agents,IEEE,Conferences,"In order for an autonomous agent to behave robustly in a variety of environments, it must have the ability to learn approximations to many different functions. The function approximator used by such an agent is subject to a number of constraints that may not apply in a traditional supervised learning setting. Many different function approximators exist and are appropriate for different problems. This paper proposes a set of criteria for function approximators for autonomous agents. Additionally, for those problems on which polynomial regression is a candidate technique, the paper presents an enhancement that meets these criteria. In particular, using polynomial regression typically requires a manual choice of the polynomial's degree, trading off between function accuracy and computational and memory efficiency. Polynomial regression with automated degree (PRAD) is a novel function approximation method that uses training data to automatically identify an appropriate degree for the polynomial. PRAD is fully implemented. Empirical tests demonstrate its ability to efficiently and accurately approximate both a wide variety of synthetic functions and real-world data gathered by a mobile robot",https://ieeexplore.ieee.org/document/4031933/,2006 18th IEEE International Conference on Tools with Artificial Intelligence (ICTAI'06),13-15 Nov. 2006,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ARITH.2019.00047,Privacy-Preserving Deep Learning via Additively Homomorphic Encryption,IEEE,Conferences,"We aim at creating a society where we can resolve various social challenges by incorporating the innovations of the fourth industrial revolution (e.g. IoT, big data, AI, robot, and the sharing economy) into every industry and social life. By doing so the society of the future will be one in which new values and services are created continuously, making people's lives more conformable and sustainable. This is Society 5.0, a super-smart society. Security and privacy are key issues to be addressed to realize Society 5.0. Privacy-preserving data analytics will play an important role. In this talk we show our recent works on privacy-preserving data analytics such as privacy-preserving logistic regression and privacy-preserving deep learning. Finally, we show our ongoing research project under JST CREST “AI”. In this project we are developing privacy-preserving financial data analytics systems that can detect fraud with high security and accuracy. To validate the systems, we will perform demonstration tests with several financial institutions and solve the problems necessary for their implementation in the real world.",https://ieeexplore.ieee.org/document/8877418/,2019 IEEE 26th Symposium on Computer Arithmetic (ARITH),10-12 June 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/EURBOT.1997.633565,Q-learning of complex behaviours on a six-legged walking machine,IEEE,Conferences,"We present work on a six-legged walking machine that uses a hierarchical version of Q-learning (HQL) to learn both the elementary swing and stance movements of individual legs as well as the overall coordination scheme to perform forward movements. The architecture consists of a hierarchy of local controllers implemented in layers. The lowest layer consists of control modules performing elementary actions, like moving a leg up, down, left or right to achieve the elementary swing and stance motions for individual legs. The next level consists of controllers that learn to perform more complex tasks like forward movement by using the previously learned, lower level modules. On the third the highest layer in the architecture presented here the previously learned complex movements are themselves reused to achieve goals in the environment using external sensory input. The work is related to similar, although simulation-based, work by Lin (1993) on hierarchical reinforcement learning and Singh (1994) on compositional Q-learning. We report on the HQL architecture as well as on its implementation on the walking machine SIR ARTHUR. Results from experiments carried out on the real robot are reported to show the applicability of the HQL approach to real world robot problems.",https://ieeexplore.ieee.org/document/633565/,Proceedings Second EUROMICRO Workshop on Advanced Mobile Robots,22-24 Oct. 1997,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS40897.2019.8968551,RONet: Real-time Range-only Indoor Localization via Stacked Bidirectional LSTM with Residual Attention,IEEE,Conferences,"In this study, a three-layered bidirectional Long Short-term Memory (Bi-LSTM) with residual attention, named as RONet, is proposed to achieve localization using range measurements. Accordingly, we acquired our own datasets and tested RONet using realistic conditions. It is shown that the RONet can estimate the position of the mobile robot in real time using the Nvidia Jetson AGX Xavier based only on range measurements. We also analyzed the sequence length of LSTM as a type of hyperparameters. We found that optimal sequence length is eight for more than eight anchors and twelve for fewer anchors compared to sequences with different lengths, given that construction of the network with the optimal sequence length estimates the position precisely and accounts for uncertainties. As verified experimentally, RONet yields more precise performance and results in increased robustness against outliers compared to a conventional range-only approach based on a particle filtering and the other conventional deep-learning-based approaches. We set three cases, reduced the number of anchors, and verified that the RONet was a robust solution. We also confirmed that it is the best solution that yields the smallest Root-Mean-Square-Error (RMSE) values, equal to 4.466 cm, 3.210 cm, and 3.090 cm, in the cases where three, five, and eight anchors were deployed, respectively.",https://ieeexplore.ieee.org/document/8968551/,2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),3-8 Nov. 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ECICE52819.2021.9645719,RPA and L-System Based Synthetic Data Generator for Cost-efficient Deep Learning Model Training,IEEE,Conferences,"Deep learning (DL) models applied to computer vision have made great progress for image-based plant phenotyping in recent years, mostly for quality control process automation in the agroindustry. On the one hand, these models are able to detect objects in complex and noisy images as fast as human observations, but on the other hand, they are trained with a large amount of labeled data for parameter tuning. This turns the training process into an expensive, repetitive, and time-consuming labor. In this work, a synthetic data generator based on robotic process automation (RPA) and Lindenmayer systems (L-Systems) named RPASD is designed and implemented to train a DL model that detects artichoke seedlings in images captured by a robot. First, the growth artichoke seedling is modeled in L+C language using the LStudio software. Second, the RPASD is developed in Python to produce labeled images of grouped synthetic artichoke seedlings that alongside manually labeled images of real artichoke seedlings, taken by a robot, form the PlantiNet database. Third, a YOLOv3 model is trained with the previously built databases forming three datasets: 1) real and synthetics seedlings, 2) only synthetic seedlings, and 3) only real seedlings. The results show a 55% of Mean Intersection over the Union (mIoU) when training only with the second dataset and testing with the third one, which allows us to conclude that our proposed method could adequately boost DL model training reducing costs and time.",https://ieeexplore.ieee.org/document/9645719/,"2021 IEEE 3rd Eurasia Conference on IOT, Communication and Engineering (ECICE)",29-31 Oct. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ITSC.2019.8916915,Range-based Cooperative Localization with Nonlinear Observability Analysis,IEEE,Conferences,"Accurate localization of other cars in scenarios such as intersection navigation, intention-aware planning, and guardian systems is a critical component of safety. Multi-robot cooperative localization (CL) provides a method to estimate the joint state of a network of cars by exchanging information between communicating agents. However, there are many challenges to implementing CL algorithms on physical systems, including network delays, unmodeled dynamics, and non-constant velocities. In this work, we present a novel experimental framework for range-based cooperative localization that enables the testing of CL algorithms in realistic conditions, and we perform experiments using up to five cars. For state estimation, we develop and compare a particle filter, an Unscented Kalman Filter, and an Extended Kalman Filter that are compatible with nonlinear dynamics and the asynchronous reception of messages. We also model the relative transform between two unicycle models and perform a nonlinear observability analysis on the system, giving us insight into the measurements required to estimate the system's state. Our approach enables relative localization of multiple vehicles in the absence of any global reference frame or joint map, and we demonstrate the effectiveness of our system in real-world experiments. Our results show that the UKF is likely the best candidate to use for the CL task.",https://ieeexplore.ieee.org/document/8916915/,2019 IEEE Intelligent Transportation Systems Conference (ITSC),27-30 Oct. 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISIC.1992.225088,Reactive behavior design tools,IEEE,Conferences,"The reactive behavior of an autonomous agent can be described as collections of logical behaviors, each member of the collection controlling some aspect of the agent and working in conjunction with all the other behaviors. Such collections of reactive behaviors can be defined as combined, synchronous finite-state automata, using real-time programming languages which have strong formal components. These language tools, such as COSPAN and ESTEREL, require sophisticated users who have deep knowledge of both the syntax and semantics of the language. The authors use the simplicity of graphical finite-state automata editing to specify concurrent synchronous finite-state automata, and from those they produce COSPAN descriptions of these behaviors for analysis, and C language programs to implement the designed behaviors. The usefulness and validity of this approach was confirmed by the design, verification and implementation of several examples, including a controller demon for a robot arm.<>",https://ieeexplore.ieee.org/document/225088/,Proceedings of the 1992 IEEE International Symposium on Intelligent Control,11-13 Aug. 1992,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IJCNN.1993.714313,Real time learning algorithm for redundant manipulator movement control,IEEE,Conferences,"We propose a new learning control strategy to solve the ill-posed inverse kinematics of a redundant robot manipulator. Four distinct characteristics are observed: 1) the inverse solution is context-sensitive, which is a requisite when the manipulator starts from an arbitrary joint configuration or moves in a complex environment; 2) learning and execution are both memory-based and can be implemented in real time; 3) the property of conventional pseudoinverse control, i.e. keeping the incremental changes of joint angles minimum, is intrinsic in our scheme; and 4) control is goal-directed in that only the current end-effector position relative to the goal position is needed.",https://ieeexplore.ieee.org/document/714313/,"Proceedings of 1993 International Conference on Neural Networks (IJCNN-93-Nagoya, Japan)",25-29 Oct. 1993,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/WCICA.2000.863435,Real time path smoothing schemes in teleoperation system,IEEE,Conferences,"The Robot Teleoperation System (RTS) based on telepresence, which is aided financially by the National 863 High-Tech Development Plan, was set up by the State Key Laboratory of Intelligence Technology and Systems. This system consists of three main parts: robot control, stereo vision and hand gesture tracking. The controlled robot and the operator form a closed loop, and the operator views the robot's status and the environment through the stereo vision subsystem. RTS is developed from SAROT (an intelligent assembly robot system), which is logically divided into 5 layers: real time control, monitoring and coordination, motion planning, task scheduling and task planning. The paper proposes several active ""path smoothing"" schemes implemented in the system, which carry out the operator's hand gesture tracking in 7 DOF (position: 3, orientation: 3, and pitch: 1).",https://ieeexplore.ieee.org/document/863435/,Proceedings of the 3rd World Congress on Intelligent Control and Automation (Cat. No.00EX393),26 June-2 July 2000,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ECCTD.2005.1522965,Real time vision by FPGA implemented CNNs,IEEE,Conferences,"In order to get real time image processing for mobile robot vision, we propose to use a discrete time cellular neural network implementation by a convolutional structure on Altora FPGA using VHDL language. We obtain at least 9 times faster processing than other emulations for the same problem.",https://ieeexplore.ieee.org/document/1522965/,"Proceedings of the 2005 European Conference on Circuit Theory and Design, 2005.",2-2 Sept. 2005,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/OCEANSKOBE.2018.8559422,Real-Time Automated Evaluation of COLREGS-Constrained Interactions Between Autonomous Surface Vessels and Human Operated Vessels in Collaborative Human-Machine Partnering Missions,IEEE,Conferences,"This paper explores an extension of the real-time evaluation of COLREGS-based collision avoidance interactions between autonomous surface vessels and human-operated surface vessels. Our previous work developed the algorithms that evaluate and quantify a ship's compliance with the collision regulations, safety, and mission efficiencies with respect to the overall goal(s). Our previous work is extended in this paper by establishing a light-weight program to assign penalties to offenders of safety or protocol violations during human-machine collaborative on-water interactions. Vessels interacting in this DARPA-sponsored Aquaticus mission are grouped into teams consisting of both human and robot counterparts. These teams play a “capture the flag” like game while being required to obey the maritime collision avoidance regulations. This paper is a first step in the field toward evaluating collision avoidance rules in the context of a human or robotic vehicle cheating the COLREGS against its opponent to gain a mission advantage. The problem is representative of interactions likely seen on the open ocean using a combination of autonomous and human-operated multi-vehicle collision avoidance interactions to larger scale maritime vessel traffic interactions operating under COLREGS protocol constraints. The vessels deploy in a distributed collaborative pattern to compete against the opposing team. Upon detection of a violation, the offending vessel(s) are required to complete their penalty actions prior to being allowed to proceed in their mission goal. Human offenders are able to be linked to haptic devices that give real-time feedback using vibrations or similar queuing.",https://ieeexplore.ieee.org/document/8559422/,2018 OCEANS - MTS/IEEE Kobe Techno-Oceans (OTO),28-31 May 2018,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICIA.2006.305830,Real-Time Fusion of Multimodal Tracking Data and Generalization of Motion Patterns for Trajectory Prediction,IEEE,Conferences,"A sensor-based model of a service robot's environment is a prerequisite for interaction. Such a model should contain the positions of the robot's interaction partners. Many reasonable applications require this knowledge in realtime. It could for example be used to realize efficient path planning for delivery tasks. Additionally to the actual positions of the partners it is important for the service robot to predict their possible future positions. In this paper we propose an extensible framework that combines different sensor modalities in a general real-time tracking system. Exemplarily, a tracking system is implemented that fuses tracking algorithms in laser range scans as well as in camera images by a particle filter. Furthermore, human trajectories are predicted by deducing them from learned motion patterns. The observed trajectories are generalized to trajectory patterns by a novel method which uses self organizing maps. Those patterns are used to predict trajectories of the currently observed persons. Practical experiments show that multimodality increases the system's robustness to incorrect measurements of single sensors. It is also demonstrated that a self organizing map is suitable for learning and generalizing trajectories. Convenient predictions of future trajectories are presented which are deduced from these generalizations.",https://ieeexplore.ieee.org/document/4097763/,2006 IEEE International Conference on Information Acquisition,20-23 Aug. 2006,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/I2CACIS.2019.8825093,Real-Time Robotic Grasping and Localization Using Deep Learning-Based Object Detection Technique,IEEE,Conferences,"This work aims to increase the impact of computer vision on robotic positioning and grasping in industrial assembly lines. Real-time object detection and localization problem is addressed for robotic grasp-and-place operation using Selective Compliant Assembly Robot Arm (SCARA). The movement of SCARA robot is guided by deep learning-based object detection for grasp task and edge detection-based position measurement for place task. Deep Convolutional Neural Network (CNN) model, called KSSnet, is developed for object detection based on CNN Alexnet using transfer learning approach. SCARA training dataset with 4000 images of two object categories associated with 20 different positions is created and labeled to train KSSnet model. The position of the detected object is included in prediction result at the output classification layer. This method achieved the state-of-the-art results at 100% precision of object detection, 100% accuracy for robotic positioning and 100% successful real-time robotic grasping within 0.38 seconds as detection time. A combination of Zerocross and Canny edge detectors is implemented on a circular object to simplify the place task. For accurate position measurement, the distortion of camera lens is removed using camera calibration technique where the measured position represents the desired location to place the grasped object. The result showed that the robot successfully moved to the measured position with positioning Root Mean Square Error (0.361, 0.184) mm and 100% for successful place detection.",https://ieeexplore.ieee.org/document/8825093/,2019 IEEE International Conference on Automatic Control and Intelligent Systems (I2CACIS),29-29 June 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICARM49381.2020.9195281,Real-time Colonoscopy Image Segmentation Based on Ensemble Knowledge Distillation,IEEE,Conferences,"Colonoscopy is an important means of detecting various intestinal diseases such as bleeding, polyps, Merck diverticula, and ulcers. The sooner these diseases are detected, the better the patient's recovery. But colonoscopy is a demanding process, often leads to the high rate of misdiagnosis by experts, professional physicians and nurses and costs a lot of time. Therefore, robot-assisted colonoscopy is considered as an important method to solve this problem. In recent years, many automated deep learning models for colonoscopy have been proposed. However, these models are usually large and time-consuming, and cannot meet actual needs. Besides, due to the disconnection of data between hospitals, the strength of medical resources between different departments in different hospitals is different, so the general multi-classification model cannot fit the characteristics of such data distribution. Therefore, in this article, we ensemble multiple binary classification models (each model detects one disease) and extracted a compression model using knowledge distillation technology, which can simultaneously detect polyps, Merkel diverticula, ulcers and bleeding from colonoscopy. We tested the performance of our model on public and real data sets and found that the model can achieve acceptable results and help doctors make decisions in practice.",https://ieeexplore.ieee.org/document/9195281/,2020 5th International Conference on Advanced Robotics and Mechatronics (ICARM),18-21 Dec. 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICAR46387.2019.8981549,Real-time RGB-D semantic keyframe SLAM based on image segmentation learning from industrial CAD models,IEEE,Conferences,"This paper presents methods for performing realtime semantic SLAM aimed at autonomous navigation and control of a humanoid robot in a manufacturing scenario. A novel multi-keyframe approach is proposed that simultaneously minimizes a semantic cost based on class-level features in addition to common photometric and geometric costs. The approach is shown to robustly construct a 3D map with associated class labels relevant to robotic tasks. Alternatively to existing approaches, the segmentation of these semantic classes have been learnt using RGB-D sensor data aligned with an industrial CAD manufacturing model to obtain noisy pixel-wise labels. This dataset confronts the proposed approach in a complicated real-world setting and provides insight into the practical use case scenarios. The semantic segmentation network was fine tuned for the given use case and was trained in a semi-supervised manner using noisy labels. The developed software is real-time and integrated with ROS to obtain a complete semantic reconstruction for the control and navigation of the HRP4 robot. Experiments in-situ at the Airbus manufacturing site in Saint-Nazaire validate the proposed approach.",https://ieeexplore.ieee.org/document/8981549/,2019 19th International Conference on Advanced Robotics (ICAR),2-6 Dec. 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/WCICA.2000.863254,Real-time bilateral control of Internet-based teleoperation,IEEE,Conferences,"The growth of the Internet has been accompanied by an increase in its applications. One of the most interesting of these is teleoperation, where the Internet is used as a bridge between operators and machines. However, teleoperation over the Internet comes with several problems: delay, lost packets and disconnection. All of these limitations may cause instability in teleoperation systems, especially for those systems include haptic feedback. Most of the previous work in Internet based teleoperation rests on many limiting assumptions, for example, time delay is constant or has an upper bound, control is not in real-time. This paper presents a new real time haptic feedback system that deals with these limitations and difficulties without making any assumptions regarding the time delay. The approach is based on the event based control, which has been implemented or a mobile robot over the Internet. The haptic information include real-time feedback of force and video.",https://ieeexplore.ieee.org/document/863254/,Proceedings of the 3rd World Congress on Intelligent Control and Automation (Cat. No.00EX393),26 June-2 July 2000,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/BIOROB.2008.4762823,Real-time isometric pinch force prediction from sEMG,IEEE,Conferences,"This paper describes a real-time isometric pinch force prediction algorithm using surface electromyogram (sEMG). The activities of seven muscles related to the movements of the thumb and index finger joints, which are observable using surface electrodes, were recorded during pinch force experiments. For the successful implementation of the real-time prediction algorithm, an off-line analysis was performed using the recorded activities. From the seven muscles, four muscles were selected for monitoring using the Fisher linear discriminant paradigm in an off-line analysis, and the recordings from these four muscles provided the most effective information for mapping sEMG to the pinch force. An ANN structure was designed to perform efficient training and to avoid both under-fitting and over-fitting problems. Finally, the pinch force prediction algorithm was tested with five volunteers and the results were evaluated using two criteria: normalized root mean squared error (NRMSE) and correlation (CORR). The training time for the subjects was only 2 min 29 sec, but the prediction results were successful with NRMSE = 0.093 plusmn0.047 and CORR = 0.957 plusmn0.031. These results imply that the proposed algorithm is useful to measure the generated pinch force without force sensors. The possible applications of the proposed method include controlling bionic finger robot systems to overcome finger paralysis or amputation.",https://ieeexplore.ieee.org/document/4762823/,2008 2nd IEEE RAS & EMBS International Conference on Biomedical Robotics and Biomechatronics,19-22 Oct. 2008,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICUAS.2016.7502588,Real-time unmanned aerial vehicle 3D environment exploration in a mixed reality environment,IEEE,Conferences,"This paper presents a novel human robot interaction system that can be used for real-time 3D environment exploration with an unmanned aerial vehicle (UAV). The method creates a mixed reality environment, in which a user can interactively control a UAV and visualize the exploration data in real-time. The method uses a combination of affordable sensors, and transforms the control and viewing space from the UAV to the controller's perspective. Different hardware and software configurations are studied so that the system can be adjusted to meet different needs and environments. A prototype system is presented and test results are discussed.",https://ieeexplore.ieee.org/document/7502588/,2016 International Conference on Unmanned Aircraft Systems (ICUAS),7-10 June 2016,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICAMechS49982.2020.9310146,Realization of Robust Yo-yo Operation,IEEE,Conferences,"In this research, we aim to realize a robust yo-yo operation with a robot arm. For this purpose, we design a model predictive controller that provides input to the robot arm from predicted yo-yo states. Furthermore, we implement a controller that can be performed in real-time by using a neural network. Finally, the implemented controller realizes highly reproducible yo-yo operation by even with a usual disturbance.",https://ieeexplore.ieee.org/document/9310146/,2020 International Conference on Advanced Mechatronic Systems (ICAMechS),10-13 Dec. 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ECAI.2014.7090222,Reconfigurable robotic system based on mono-camera guidance,IEEE,Conferences,"The paper proposes an intelligent robotic system which is able to be (re)configured, at demand, for two deployment scenarios. a) The first task is to move the platform after a trajectory determined by the direction to a fixed point and avoid any obstacles occurring in the route. a) The second task is to identify and track a spherical object. The robot is equipped with a navigation system designed to maintain direction in case of interruption of video contact with the target (landmark), meaning when an obstacle interposes. It has two main subsystems: the mobile platform, which is equipped with a video camera and sensors for path correction, and the central processing system for the analysis of received information. The task control is based on extracted features from images. The communication between them is done via a wireless protocol. Algorithms for controlling the mobile platform are implemented on the embedded microcontroller and algorithms for image processing are implemented on the central system.",https://ieeexplore.ieee.org/document/7090222/,"Proceedings of the 2014 6th International Conference on Electronics, Computers and Artificial Intelligence (ECAI)",23-25 Oct. 2014,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISSRE.1993.624285,Reliability of uniprocessor and multiprocessor real-time artificial intelligence planning systems,IEEE,Conferences,"By real-time artificial intelligence (AI) planning systems, we mean those systems embedded in process-control systems that must plan and execute control strategies in response to external events within a real-time constraint. We propose a methodology for estimating the reliability of uniprocessor and multiprocessor real-time AI planning systems. We first discuss why there are intrinsic faults in AI planning programs that must be considered in the reliability modeling of real-time AI planning systems. Then, we show that for uniprocessor systems, no single planning algorithm can avoid all types of intrinsic faults. Finally, we investigate a multiprocessor architecture with parallel planning with the objective of reducing intrinsic faults of real-time AI planning systems and improving the reliability of embedded systems. A robot path-planning system in static domains is used as an example to illustrate our methodology.",https://ieeexplore.ieee.org/document/624285/,Proceedings of 1993 IEEE International Symposium on Software Reliability Engineering,3-6 Nov. 1993,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AIID51893.2021.9456477,Research on Privacy Protection Technology in Face Identity Authentication System Based on Edge Computing,IEEE,Conferences,"In today's society, the rapid development of the Internet makes People's Daily life become more intelligent and diversified. Today's society has entered a multifaceted era where everything is interconnected. Artificial intelligence technology is gradually replacing some traditional human services, such as intelligent robot customer service instead of traditional human customer service, intelligent face scanning security check in railway stations instead of traditional manual ticket checking, unmanned supermarket automatic checkout has liberated some social labor costs. All these changes are the result of the development of artificial intelligence technology in today's society. In recent years, unicorn startups focused on biometrics have sprung up all around us, such as BTU and its MEG VII (Face ++). Thanks to the development of Internet and artificial intelligence technology, in many application fields, the traditional access control and identity authentication technology based on password verification is gradually transforming to the scheme based on biometric identification verification. Secure identity authentication is very important to the application of Internet. Face recognition is the most popular technology among all biometric identification technologies. In the field of biometric identification technology, it has become the most widely used technology in the field of identity authentication because of its unique non-invasive, support for infrared and visible light, no need for user cooperation and many other advantages. In the field of education, examinee identification, pedestrian identification detection at the entrance of railway stations, face electronic payment, intelligent video surveillance system, intelligent attendance and access control system, intelligent unmanned supermarkets and customs clearance ports become the pioneer fields of face recognition applications. It can be seen that the era of “national face brushing” has arrived, and the application of face recognition technology will only be more and more widespread in the current era and in the future. However, due to the sensitivity of biometric data and the heterogeneity and openness of network environment, the privacy leakage of biometric data is difficult to avoid. At present, fog computing and edge computing have been paid more and more attention in many fields. In the case that cloud service providers are unable to provide sufficient security, edge computing shows its advantages. In this paper, mobile edge computing is introduced for the first time into the face privacy protection identity authentication system based on cloud server outsourcing computing. It can not only greatly reduce the interaction frequency between users and cloud server, improve the availability and fault tolerance of the system, but also contribute to the implementation of privacy protection scheme. A deep constitutional neural network for face feature extraction is trained using deep learning framework Cafe. Cosine similarity is used to complete face verification. A privacy protection scheme based on the secure nearest neighbor algorithm is proposed, which can not only protect the security of the face feature data at the edge computing node, but also allow the edge computing node to complete the face recognition operation against the encrypted face feature data. In addition, the encryption scheme does not require large computing resources, and the accuracy of face recognition in cipher text is exactly the same as that in explain. At present, most of the solutions either have high computational complexity or poor security performance. How to reduce the computational complexity and improve the real-time performance of the system while ensuring the high security of the private data has important research significance and value. Therefore, in the cloud server outsourcing computing environment, how to complete biometric identification on the premise of protecting the privacy of biological data has become a research hot spot.",https://ieeexplore.ieee.org/document/9456477/,2021 IEEE International Conference on Artificial Intelligence and Industrial Design (AIID),28-30 May 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCSCE.2015.7482163,Review on simultaneous localization and mapping (SLAM),IEEE,Conferences,"Simultaneous localization and mapping (SLAM) is a technique applied in artificial intelligence mobile robot for a self-exploration in numerous geographical environment. SLAM becomes fundamental research area in recent days as it promising solution in solving most of problems which related to the self-exploratory oriented artificial intelligence mobile robot field. For example, the capability to explore without any prior knowledge on environment it explores and without any human interference. The unique feature in SLAM is that the process of mapping and localization is done concurrently and recursively. Since SLAM introduction, many SLAM algorithms have been proposed to apply SLAM technique in real practice. The aim of this paper is to provide an insightful review on information background, recent development, feature, implementation and recent issue in SLAM.",https://ieeexplore.ieee.org/document/7482163/,"2015 IEEE International Conference on Control System, Computing and Engineering (ICCSCE)",27-29 Nov. 2015,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CCDC.2019.8833453,Road Garbage Cleaning Device Based on ZigBee Gateway and Image Recognition,IEEE,Conferences,"The existing road garbage automatic cleaning device has problems such as low cleaning efficiency, difficulty in management, low accuracy of information transmission and garbage identification, and cannot meet actual needs well. A road garbage cleaning device based on ZigBee gateway and image recognition is designed, which consist of power module, camera pan/tilt module, robot arm module, ZigBee gateway module, GPS module, MCU control module and vehicle model module. The road debris is identified and analyzed by the camera pan/tilt module together with the RBF neural network algorithm, and then the device can automatically go to the target location and clean up garbage by robotic arm; The networking characteristics of the ZigBee gateway module are used to collect information such as positioning and moving speed of each device node in real time. In addition, the APP software or server can view road image information and device information, analyze relevant data, and provide convenience for real-time monitoring and management of road garbage cleaning work.",https://ieeexplore.ieee.org/document/8833453/,2019 Chinese Control And Decision Conference (CCDC),3-5 June 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SII46433.2020.9026297,Robotic Grasping Using Semantic Segmentation and Primitive Geometric Model Based 3D Pose Estimation,IEEE,Conferences,"Due to the rapid development of hardware and software technologies, the machine automation has greatly improved in the past few decades. Robotic manipulators have been used in many factories for fully automated production. Compared with traditional sensor-based robot control, visual servo control has several advantages: high flexibility and precision, and robustness to calibration errors. In this paper, we present an eye-in-hand robotic arm for object recognition and grasping applications. The 2D images and depth information are acquired with an RGB-D camera for instance segmentation using convolutional neural network. The object pose estimation is achieved by ICP using primitive geometric models. Experiments carried out with different categories of objects in the real scene environment have demonstrated the feasibility of the proposed technique.",https://ieeexplore.ieee.org/document/9026297/,2020 IEEE/SICE International Symposium on System Integration (SII),12-15 Jan. 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SMC.2016.7844571,Robotic attention manager using fuzzy controller with fractal analysis,IEEE,Conferences,"This paper is focused on the application of fractal analysis in the attention management of humanoid robot. We designed a fuzzy controller to combine the face detection, movement detection and the fractal dimension signals to control the head movement of robot Nao. Also, the gaze problem is addressed by the controller. Implementation details are included in the paper, including configuration parameters, which we found optimal according to subjective analysis and possibilities of current hardware. We found the fuzzy controller to be advantageous for implementation of attention manager because of smoothing of the movement of robot when compared to the simple rule based implementation, and also because the fuzzy controller implementation of manager is more clear than a naive if-then heuristics code. We also found the fractal dimension to be useful additional signal for attention management of robot, which can be computed in near real-time on current hardware and static input images.",https://ieeexplore.ieee.org/document/7844571/,"2016 IEEE International Conference on Systems, Man, and Cybernetics (SMC)",9-12 Oct. 2016,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICNN.1993.298695,Robotic modeling and control using a fuzzy neural network,IEEE,Conferences,"A fuzzy neural network (FNN) is applied to modeling and control of a robot. Comparisons are made between the FNN and standard back propagation neural networks, as well as commercially available neural network software packages for modeling the robot. Observations on the robustness of these networks are presented. A number of experiments demonstrate that the FNN can learn faster and more accurately than the back propagation and commercial neural networks for modeling and control of a real robot.<>",https://ieeexplore.ieee.org/document/298695/,IEEE International Conference on Neural Networks,28 March-1 April 1993,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/C-CODE.2017.7918964,Robotic navigation based on logic-based planning,IEEE,Conferences,"Logic and Planning are interesting artificial intelligence problems in the context of robotic systems, i.e., robotic navigation. For such an autonomous system one of the requisites is that the goal has to be achieved without intervention of human being. We present a practical implementation of autonomous robotic navigation based on logic-based planning. We achieve this by using strength of PROLOG in order to generate plan to reach goal position from an initial. We utilize First Order Logic (FOL) that automatically asserts and retracts facts at runtime dynamically. All possible plans are computed using local search strategies (e.g., Depth and Breadth First) on state space representing a real, dynamic, and unpredictable environment. In order to navigate in the environment following optimized plan - one with fewest states, a balanced size 4-wheel differential drive robot has been carefully constructed. It can turn 90° and actuate forward by controlling linear (νt = 0.25m/s) and angular (ωt = Π/8 rad/s) velocities of two rear motorized wheels. It is also equipped with an Ultrasonic sensor to avoid collision with obstacles. The system is evaluated in an environment comprising of corridors with adjacent rooms. Graphical User Interface (GUI) is developed in .Net (C#) to map situation in Prolog and transmit plan to hardware for execution. Average time calculated for a plan to generate is 0.065 seconds. The robot moves block by block where each block in the state space represents 2m2 area. In addition to minors, our major contribution is that we offer a unified scheme for robotic navigation without calculating odometry data with the assumption the robot cannot be kidnapped nor slipped.",https://ieeexplore.ieee.org/document/7918964/,"2017 International Conference on Communication, Computing and Digital Systems (C-CODE)",8-9 March 2017,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CVPR46437.2021.00844,Robust Neural Routing Through Space Partitions for Camera Relocalization in Dynamic Indoor Environments,IEEE,Conferences,"Localizing the camera in a known indoor environment is a key building block for scene mapping, robot navigation, AR, etc. Recent advances estimate the camera pose via optimization over the 2D/3D-3D correspondences established between the coordinates in 2D/3D camera space and 3D world space. Such a mapping is estimated with either a convolution neural network or a decision tree using only the static input image sequence, which makes these approaches vulnerable to dynamic indoor environments that are quite common yet challenging in the real world. To address the aforementioned issues, in this paper, we propose a novel outlier-aware neural tree which bridges the two worlds, deep learning and decision tree approaches. It builds on three important blocks: (a) a hierarchical space partition over the indoor scene to construct the decision tree; (b) a neural routing function, implemented as a deep classification network, employed for better 3D scene understanding; and (c) an outlier rejection module used to filter out dynamic points during the hierarchical routing process. Our proposed algorithm is evaluated on the RIO-10 benchmark developed for camera relocalization in dynamic indoor environments. It achieves robust neural routing through space partitions and outperforms the state-of-the-art approaches by around 30% on camera pose accuracy, while running comparably fast for evaluation.",https://ieeexplore.ieee.org/document/9577932/,2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),20-25 June 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS.2006.282394,Saccades and Fixating Using Artificial Potential Functions,IEEE,Conferences,"This paper presents a mathematical model for saccadic motion and fixations. We relate this issue to the problem of motion planning and show that a family of artificial potential functions can be used for creating saccadic motion. The advantage of this approach is that finding the next fixation point does not require an explicit visual search - which is computationally costly and may be problematic in real-time applications. Rather, the system naturally 'slides' from the current fixation into the next. Thus real-time performance on cheap hardware can easily be achieved. Experimental results serve to provide insight into the performance of a robot APES implementing this approach",https://ieeexplore.ieee.org/document/4058392/,2006 IEEE/RSJ International Conference on Intelligent Robots and Systems,9-15 Oct. 2006,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.23919/ACC.2018.8430770,Safe Reinforcement Learning: Learning with Supervision Using a Constraint-Admissible Set,IEEE,Conferences,"Despite recent advances in Reinforcement Learning (RL), its applications in real-world engineering systems are still rare. The primary reason is that RL algorithms involve exploratory actions that can lead to system constraint violations. These violations can damage physical systems and even cause safety issues, e.g., battery overheat, robot breakdown, and car crashes, hindering RL deployment in many engineering applications. In this paper, we develop a novel safe RL framework that guarantees safety during learning by exploiting a constraint-admissible set for supervision. System knowledge and recursive feasibility techniques are exploited to construct a state-dependent constraint-admissible set. We develop a new learning scheme where the constraint-admissible set regulates the exploratory actions from the RL agent and simultaneously guides the agent to learn the system constraints with a penalty for control regulation. The proposed safe RL algorithm is demonstrated in an adaptive cruise control example where a nonlinear fuel economy cost function is optimized without violating system constraints. We demonstrate that the safe RL agent is able to learn the system constraints to gradually fade out the control supervisor.",https://ieeexplore.ieee.org/document/8430770/,2018 Annual American Control Conference (ACC),27-29 June 2018,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS51168.2021.9636440,Seeing All the Angles: Learning Multiview Manipulation Policies for Contact-Rich Tasks from Demonstrations,IEEE,Conferences,"Learned visuomotor policies have shown considerable success as an alternative to traditional, hand-crafted frameworks for robotic manipulation. Surprisingly, an extension of these methods to the multiview domain is relatively unexplored. A successful multiview policy could be deployed on a mobile manipulation platform, allowing the robot to complete a task regardless of its view of the scene. In this work, we demonstrate that a multiview policy can be found through imitation learning by collecting data from a variety of viewpoints. We illustrate the general applicability of the method by learning to complete several challenging multi-stage and contact-rich tasks, from numerous viewpoints, both in a simulated environment and on a real mobile manipulation platform. Furthermore, we analyze our policies to determine the benefits of learning from multiview data compared to learning with data collected from a fixed perspective. We show that learning from multiview data results in little, if any, penalty to performance for a fixed-view task compared to learning with an equivalent amount of fixed-view data. Finally, we examine the visual features learned by the multiview and fixed-view policies. Our results indicate that multiview policies implicitly learn to identify spatially correlated features.",https://ieeexplore.ieee.org/document/9636440/,2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),27 Sept.-1 Oct. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CSSE.2008.1256,Segmentation Methods of Fruit Image and Comparative Experiments,IEEE,Conferences,"Fruit image segmentation issue on color difference between mature fruits and backgrounds under natural illumination condition is an important and difficult content of fruit-harvesting robot vision. Some studies concerning fruit image segmentation have been presented in the last few years. However, these studies are focused on particular fruit and different from segmentation results. In this paper, four kinds of segmentation methods are presented and applied into fruit image segmentation. The tests show that these methods can segment successful several kinds of fruits image, such as apple, tomato, strawberry, persimmon and orange. Dynamic threshold segmentation method has better performance and least cost time than extended Otsu method, improved Otsu combined with genetic arithmetic and adaptive segmentation method based on LVQ network. Meanwhile, it has satisfactory effect upon fruit image under natural illumination condition. Adaptive segmentation method based on LVQ network can only be applied into balanced color instance of particular fruit, and it isnpsilat adapt to be applied into real-time occasion because of high cost time.",https://ieeexplore.ieee.org/document/4721944/,2008 International Conference on Computer Science and Software Engineering,12-14 Dec. 2008,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/TAI.1993.633961,Self-adjusting real-time search: a summary of results,IEEE,Conferences,"Real-time search algorithms need to address the deadlines imposed by applications like process control and robot navigation. Possible deadline violations should be predicted ahead of time to allow remedial actions to prevent the undesirable consequences of missing deadlines. The algorithms should also demonstrate progressively optimizing behavior. That is, they should improve the quality of the solutions as time constraints are relaxed. To successfully address these issues, a real-time search algorithm must address the central problem of choosing the proper values for its parameters, which control the time allocated to planning. The authors propose a new approach to determine the parameter values of a real-time search algorithm, in order to enable the algorithm to meet deadlines, exhibit progressively optimizing behavior, and to predict deadline violation prior to the deadline. They provide a theoretical and experimental characterization of the proposed algorithm.",https://ieeexplore.ieee.org/document/633961/,Proceedings of 1993 IEEE Conference on Tools with Al (TAI-93),8-11 Nov. 1993,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/FPA.1994.636137,Self-organizing map for reinforcement learning: obstacle-avoidance with Khepera,IEEE,Conferences,We present a self-organizing map implementation of the Q-learning algorithm. Our goal is to overcome the problems of reinforcement learning: memory requirement and generalization. We consider the map as an associative memory and we use it for obstacle avoidance with the mobile robot Khepera. Results allow real world applications to be envisaged using neural reinforcement learning.,https://ieeexplore.ieee.org/document/636137/,Proceedings of PerAc '94. From Perception to Action,7-9 Sept. 1994,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS45743.2020.9340814,SelfieDroneStick: A Natural Interface for Quadcopter Photography,IEEE,Conferences,"A physical selfie stick extends the user's reach, enabling the acquisition of personal photos that include more of the background scene. Similarly, a quadcopter can capture photos from vantage points unattainable by the user; but teleoperating a quadcopter to good viewpoints is a difficult task. This paper presents a natural interface for quadcopter photography, the SelfieDroneStick that allows the user to guide the quadcopter to the optimal vantage point based on the phone's sensors. Users specify the composition of their desired long-range selfies using their smartphone, and the quadcopter autonomously flies to a sequence of vantage points from where the desired shots can be taken. The robot controller is trained from a combination of real-world images and simulated flight data. This paper describes two key innovations required to deploy deep reinforcement learning models on a real robot: 1) an abstract state representation for transferring learning from simulation to the hardware platform, and 2) reward shaping and staging paradigms for training the controller. Both of these improvements were found to be essential in learning a robot controller from simulation that transfers successfully to the real robot.",https://ieeexplore.ieee.org/document/9340814/,2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),24 Oct.-24 Jan. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SII52469.2022.9708852,Sensory-Motor Learning for Simultaneous Control of Motion and Force: Generating Rubbing Motion against Uneven Object,IEEE,Conferences,"We propose a motion generation model for simultaneous control of motion and force using deep learning. Conventional force control methods require expensive torque sensors and complex control theory, and implementing force control for each task requires huge development costs. In this paper, we realize rubbing motions against an uneven object at low cost by using a motion generation method that takes as input the joint angles and current values of an inexpensive servo motor. We evaluated the generalization ability of the model by confirming that the robot can perform rubbing motions against unlearned uneven or tilted objects. In addition, by comparing several motion generation models, we clarified that the following two components are important for simultaneous control of motion and force. (1) The joint angles and the current values are input to different neuron layers to extract the features. A time constant, which is the speed of information transfer, is set for each layer in order to integrate and learn input information with different time characteristics. (2) In the output part of the model, a single neuron layer is used to predict the joint angle and current value simultaneously. This makes it easy to extract features and integrate learning from two inputs with different time characteristics, and the robot can generate appropriate motions based on the contact situation in real time.",https://ieeexplore.ieee.org/document/9708852/,2022 IEEE/SICE International Symposium on System Integration (SII),9-12 Jan. 2022,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA40945.2020.9197512,Sim-to-Real Transfer for Optical Tactile Sensing,IEEE,Conferences,"Deep learning and reinforcement learning methods have been shown to enable learning of flexible and complex robot controllers. However, the reliance on large amounts of training data often requires data collection to be carried out in simulation, with a number of sim-to-real transfer methods being developed in recent years. In this paper, we study these techniques for tactile sensing using the TacTip optical tactile sensor, which consists of a deformable tip with a camera observing the positions of pins inside this tip. We designed a model for soft body simulation which was implemented using the Unity physics engine, and trained a neural network to predict the locations and angles of edges when in contact with the sensor. Using domain randomisation techniques for sim-to-real transfer, we show how this framework can be used to accurately predict edges with less than 1 mm prediction error in real-world testing, without any real-world data at all.",https://ieeexplore.ieee.org/document/9197512/,2020 IEEE International Conference on Robotics and Automation (ICRA),31 May-31 Aug. 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/LARS-SBR-WRE48964.2019.00060,Sim-to-Real in Reinforcement Learning for Everyone,IEEE,Conferences,"In reinforcement learning (RL), it remains a challenge to have a robotic agent perform a task in the real world for which it was trained in simulation. In this paper, we present our work training a low-cost robotic arm in simulation to move towards a predefined target in space, represented by a red ball in an RGB image, and transferring the capability to the real arm. We exercised the entire end-to-end flow including the 3D modeling of the arm, training of a state-of-the-art RL policy in simulation with multiple actors in a distributed fashion, domain randomization in order to close the sim-to-real gap, and finally the execution of the trained model in the real robot. We also implemented a mechanism to edit the image captured from the camera before sending it to the model for inference, which allowed us to automate reward computation in the physical world. Our work highlights important challenges of training RL agents and moving them to the real world, validating important aspects shown by other works as well as detailing steps not explained by some of them (e.g. how to compute the reward in the real world). The conducted experiments show the improvements observed as the techniques were added to the final solution.",https://ieeexplore.ieee.org/document/9018558/,"2019 Latin American Robotics Symposium (LARS), 2019 Brazilian Symposium on Robotics (SBR) and 2019 Workshop on Robotics in Education (WRE)",23-25 Oct. 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/EMBC44109.2020.9176182,Simple Kinematic Feedback Enhances Autonomous Learning in Bio-Inspired Tendon-Driven Systems,IEEE,Conferences,"Error feedback is known to improve performance by correcting control signals in response to perturbations. Here we show how adding simple error feedback can also accelerate and robustify autonomous learning in a tendon-driven robot. We have implemented two versions of the General-to-Particular (G2P) autonomous learning algorithm using a tendon-driven leg with two joints and three tendons: one with and one without real-time kinematic feedback. We have performed a rigorous study on the performance of each system, for both simulation and physical implementation cases, over a wide range of tasks. As expected, feedback improved performance in simulation and hardware. However, we see these improvements even in the presence of sensory delays of up to 100 ms and when experiencing substantial contact collisions. Importantly, feedback accelerates learning and enhances G2P's continual refinement of the initial inverse map by providing the system with more relevant data to train on. This allows the system to perform well even after only 60 seconds of initial motor babbling.",https://ieeexplore.ieee.org/document/9176182/,2020 42nd Annual International Conference of the IEEE Engineering in Medicine & Biology Society (EMBC),20-24 July 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICBAIE52039.2021.9390015,Simulation of underwater vehicle control based on code generation technology,IEEE,Conferences,"Model-based design is an effective means for rapid development of embedded software, and automatic code generation is an important technology for model-based development. Combining the automatic code generation method of Matlab and STM32 with the operation and control of the autonomous underwater robot makes the design of the system more convenient. Use tools such as the Simulink library STM32 MAT/Target and STM32 CubeMX of the STM32 microcontroller to realize the automatic generation of readable and portable C code project files. At the same time, based on the design of the model, the control code of the autonomous underwater robot(AUV) is automatically generated, and the control code is added to the automatically generated C code project file. With Matlab/Simulink as the basic software platform, the motion controller of AUV is mounted on the STM32F407, and a real-time simulation system for the closed-loop control of AUV manipulation motion is constructed. The results of the semiphysical real-time simulation test show that the AUV motion controller has good heading depth control performance, realizes the manipulation and control of AUV, and verifies the practicability of the automatically generated code.",https://ieeexplore.ieee.org/document/9390015/,"2021 IEEE 2nd International Conference on Big Data, Artificial Intelligence and Internet of Things Engineering (ICBAIE)",26-28 March 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS.1990.262374,"Single leg walking with integrated perception, planning and control",IEEE,Conferences,"Describes an integrated system capable of walking over rugged terrain using a single leg suspended below a carriage that rolls along rails. To walk, the system uses a laser scanner to find a foothold, positions the leg above the foothold, contacts the terrain with the foot, and applies force enough to advance the carriage along the rails. Walking both forward and backward, the system has traversed hundreds of meters of rugged terrain including obstacles too tall to step over, trenches too deep to step in, closely spaced rocks, and sand hills. The implemented system consists of a number of task-specific processes (two for planning, two for perception, one for real-time control) and a central control process that directs the flow of communication between processes. Implementing this integrated system is a significant step toward the goal of the CMU Planetary Rover project: to prototype a autonomous six-legged robot for planetary exploration.<>",https://ieeexplore.ieee.org/document/262374/,"EEE International Workshop on Intelligent Robots and Systems, Towards a New Frontier of Applications",3-6 July 1990,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS51168.2021.9636018,Smart Pointers and Shared Memory Synchronisation for Efficient Inter-process Communication in ROS on an Autonomous Vehicle,IEEE,Conferences,"Despite the stringent requirements of a real-time system, the reliance of the Robot Operating System (ROS) on the loopback network interface imposes a considerable overhead on the transport of high bandwidth data, while the nodelet package, which is an efficient mechanism for intra-process communication, does not address the problem of efficient local inter-process communication (IPC). To remedy this, we propose a novel integration into ROS of smart pointers and synchronisation primitives stored in shared memory. These obey the same semantics and, more importantly, exhibit the same performance as their C++ standard library counterparts, making them preferable to other local IPC mechanisms. We present a series of benchmarks for our mechanism - which we call LOT (Low Overhead Transport) - and use them to assess its performance on realistic data loads based on Five’s Autonomous Vehicle (AV) system, and extend our analysis to the case where multiple ROS nodes are running in Docker containers. We find that our mechanism performs up to two orders of magnitude better than the standard IPC via local loopback. Finally, we apply industry-standard profiling techniques to explore the hotspots of code running in both user and kernel space, comparing our implementation against alternatives.",https://ieeexplore.ieee.org/document/9636018/,2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),27 Sept.-1 Oct. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICMLC.2003.1259766,Spatio-temporal representation for multi-dimensional occlusion relation,IEEE,Conferences,"Occlusion relation is the topological relation between the images of two bodies from a viewpoint. Qualitative representation of occlusion relation has been investigated in qualitative spatial reasoning. This research is important for computer vision and robot navigation. The previous models such as LOS and ROC-20 are all based on RCC (the famous topological theory). But those models couldn't support abstract objects such as point and line which are very common in real applications. To deal with this, multi-dimensional spatial occlusion relation (MSO) is put forward. The foundation of MSO is MRCC which is the multi-dimensional extension of RCC. So MSO is suitable for both real and abstract objects. The conception neighborhood and composition of MSO is given. Finally MSO is extended to spatio-temporal relation by adding time feature. MSO is an appropriate frame to express spatio-temporal knowledge.",https://ieeexplore.ieee.org/document/1259766/,Proceedings of the 2003 International Conference on Machine Learning and Cybernetics (IEEE Cat. No.03EX693),5-5 Nov. 2003,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/UR52253.2021.9494646,Splash on ROS 2: A Runtime Software Framework for Autonomous Machines,IEEE,Conferences,"ROS has been completely refactored and evolved into ROS 2 to address the ever-increasing software complexity of autonomous machines. While it has become a de facto standard software platform for autonomous machines, ROS 2 still has room for improvement. It lacks support for essential features such as real-time stream processing, mode change, sensor fusion and rate control for output shaping. Moreover, its programming model is at a lower level than what most programmers would expect.In this paper, we carefully analyze the shortcomings of ROS 2 and propose to augment it with the Splash programming framework. In doing so, we host Splash on top of the ROS 2 software stack by conducting model conversion between Splash and ROS 2. We refer to the end result as Splash on ROS 2. To show its viability, we conducted a case study with a robot arm controller performing DNN-based object detection and motion planning. The case study qualitatively confirms that Splash on ROS 2 relieves the programming burden on developers, increases the software development productivity and improves the quality of the software.",https://ieeexplore.ieee.org/document/9494646/,2021 18th International Conference on Ubiquitous Robots (UR),12-14 July 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/HPCS48598.2019.9188104,Staged deployment of interactive multi-application HPC workflows,IEEE,Conferences,"Running scientific workflows on a supercomputer can be a daunting task for a scientific domain specialist. Workflow management solutions (WMS) are a standard method for reducing the complexity of application deployment on high performance computing (HPC) infrastructure. We introduce the design for a middleware system that extends and combines the functionality from existing solutions in order to create a high-level, staged usercentric operation/deployment model. This design addresses the requirements of several use cases in the life sciences, with a focus on neuroscience. In this manuscript we focus on two use cases: 1) three coupled neuronal simulators (for three different space/time scales) with in-transit visualization and 2) a closed-loop workflow optimized by machine learning, coupling a robot with a neural network simulation. We provide a detailed overview of the application-integrated monitoring in relationship with the HPC job. We present here a novel usage model for large scale interactive multi-application workflows running on HPC systems which aims at reducing the complexity of deployment and execution, thus enabling new science.",https://ieeexplore.ieee.org/document/9188104/,2019 International Conference on High Performance Computing & Simulation (HPCS),15-19 July 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ROBOT.1996.506888,Stereo sketch: stereo vision-based target reaching behavior acquisition with occlusion detection and avoidance,IEEE,Conferences,"In this paper, we proposed a method by which a stereo vision-based mobile robot learns to reach a target by detecting and avoiding occlusions. We call the internal representation that describes the learning behavior ""stereo sketch"". First, an input scene is segmented into homogeneous regions by the enhanced ISODATA algorithm with minimum description length principle in terms of image coordinates and disparity information obtained from the fast stereo matching unit based on the coarse-to-fine control method. Then, in terms of the segmented regions including the target area and their occlusion status identified during the stereo and motion disparity estimation process, we construct a state space for the reinforcement learning method to obtain a target reaching behavior. As a result the robot can avoid obstacles without explicitly describing them. We give the computer simulation results and real robot implementation to show the validity of our method.",https://ieeexplore.ieee.org/document/506888/,Proceedings of IEEE International Conference on Robotics and Automation,22-28 April 1996,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/RTAS48715.2020.00-20,SubFlow: A Dynamic Induced-Subgraph Strategy Toward Real-Time DNN Inference and Training,IEEE,Conferences,"We introduce SubFlow-a dynamic adaptation and execution strategy for a deep neural network (DNN), which enables real-time DNN inference and training. The goal of SubFlow is to complete the execution of a DNN task within a timing constraint that may dynamically change while ensuring comparable performance to executing the full network by executing a subset of the DNN at run-time. To this end, we propose two online algorithms that enable SubFlow: 1) dynamic construction of a sub-network which constructs the best subnetwork of the DNN in terms of size and configuration, and 2) time-bound execution which executes the sub-network within a given time budget either for inference or training. We implement and open-source SubFlow by extending TensorFlow with full compatibility by adding SubFlow operations for convolutional and fully-connected layers of a DNN. We evaluate SubFlow with three popular DNN models (LeNet-5, AlexNet, and KWS), which shows that it provides flexible run-time execution and increases the utility of a DNN under dynamic timing constraints, e.g., lx-6.7x range of dynamic execution speed with average -3% of performance (inference accuracy) difference. We also implement an autonomous robot as an example system that uses SubFlow and demonstrate that its obstacle detection DNN is flexibly executed to meet a range of deadlines that varies depending on its running sped.",https://ieeexplore.ieee.org/document/9113121/,2020 IEEE Real-Time and Embedded Technology and Applications Symposium (RTAS),21-24 April 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IJCNN.2003.1223978,Systemic intelligence: methods for growing up artefacts that live,IEEE,Conferences,"The ideas of systemic intelligence provide a set of methodologies and paradigms that are, beside other advantages, suitable for constructing control systems that are capable of growing up. In particular the promising methods of systemic architecture, schedule of structural development, memory organization and rules for learning and adaptation are presented and discussed with respect to grow up an artifact. Of special interest is the concept of growth in the sense of growing up from a kind of infantile stage to a fully matured entity. To grow up an artifact from an infantile stage via a sequence of learned abilities to,a fully matured entity is still a feature of life not yet sufficiently transposed onto technical systems. To enable the capability to grow up artifacts, a set of methodologies and principles is presented in this paper. The developed methodologies are already implemented into physically existing test beds that operate, adapt (and grow up) in real time and in the real world to prove that the proposed approach is feasible under real conditions. Two realizations (robot control, audio signal processing) of a systemic architecture for an up-growing system are presented in this paper.",https://ieeexplore.ieee.org/document/1223978/,"Proceedings of the International Joint Conference on Neural Networks, 2003.",20-24 July 2003,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CIRA.1997.613854,Task decomposition and dynamic policy merging in the distributed Q-learning classifier system,IEEE,Conferences,A distributed reinforcement learning system is designed and implemented on a mobile robot for the study of complex task decomposition and dynamic policy merging in real robot learning environments. The distributed Q-learning classifier system (DBLCS) is evolved from the standard LCS proposed by Holland (1996). We address two of the limitations of the LCS through the use of Q-learning as the apportionment of credit component and a distributed learning architecture to facilitate complex task decomposition. The Q-learning update equation is derived and its advantages over the complex bucket brigade algorithm (BBA) are discussed. Holistic and monolithic shaping approaches are used to distribute reward among the learning modules of the DBLCS and allow dynamic policy merging in a variety of real robot learning experiments.,https://ieeexplore.ieee.org/document/613854/,Proceedings 1997 IEEE International Symposium on Computational Intelligence in Robotics and Automation CIRA'97. 'Towards New Computational Principles for Robotics and Automation',10-11 July 1997,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/EIT51626.2021.9491894,Teaching Vehicles to Steer Themselves with Deep Learning,IEEE,Conferences,Traditional approaches for steering a vehicle using machine vision require large amounts of robust hand-crafted software which is both time consuming and expensive. The presented method uses a deep neural network to teach cars to steer themselves without any additional software. We created a labeled dataset for the ACTor (Autonomous Campus TranspORt) electric vehicle by pairing real world images taken during a drive with the associated steering wheel angle. We trained a model end to end using modern deep learning techniques including convolutional neural networks and transfer learning to automatically detect relevant features in the input and provide a predicted output. This means that no traditional hand engineered algorithm features were required for this implementation. We currently use an pretrained inception network on the ImageNet dataset to leverage the high level features learned from ImageNet to the steering problem through transfer learning. We removed the top portion of the network and replaced it with a linear regression node to provide the output. The model is trained end to end using backpropagation. The trained model is integrated with vehicle software on ROS (Robot Operating System) to read image data and provide a corresponding steering angle in real time. The current model achieves 15.2 degree error on average. As development continues the model may replace the current lane centering software and will be used for IGVC Self-Drive competition and campus transportation.,https://ieeexplore.ieee.org/document/9491894/,2021 IEEE International Conference on Electro Information Technology (EIT),14-15 May 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICEKIM52309.2021.00100,The Creation of Multi Intelligence Music Classroom in Children's Enlightenment Stage Based on Virtual Reality Technology,IEEE,Conferences,"With the development of virtual reality technology, the real realization of virtual reality will cause great changes in human life and development. VR is the abbreviation of virtual reality, which means virtual reality in Chinese. Virtual reality is the ultimate application form of multimedia technology. It is the crystallization of the rapid development of computer hardware and software technology, sensor technology, robot technology, artificial intelligence and behavioral psychology. Because of this, the combination of VR and education classroom will become the inevitable trend of future education development. In the context of the development of traditional education, the enlightenment and influence of music education classroom on children is point like, and through the intervention and influence of VR technology, the influence of VR on music education begins to appear face like, and can maximize the mental and thinking ability of preschool children. Through the above analysis, this paper concludes the feasibility and importance of introducing VR technology into the current music education classroom, and will explore for the development of multiple intelligence education.",https://ieeexplore.ieee.org/document/9479537/,"2021 2nd International Conference on Education, Knowledge and Information Management (ICEKIM)",29-31 Jan. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ETFA.1995.496802,The REAKT project: environment and methodology for the development of real-time knowledge-based systems,IEEE,Conferences,"This paper outlines the main results of the REAKT and REAKT II ESPRIT projects (EP 5146 and 7805). The primary objective of those projects was to develop a set of tools and the associated methodology for applying knowledge-based systems in real-time domains. Two REAKT applications which we think are typical of the kind of real-time knowledge-based systems that can be developed with REAKT are presented. The first application is an online alarm management system which has been running for about one year in a large oil refinery in Italy. This application was developed within the REAKT project to act as a demonstrator for the project results. The second application is a prototype mobile robot system, currently being developed at CRIN-INRIA.",https://ieeexplore.ieee.org/document/496802/,Proceedings 1995 INRIA/IEEE Symposium on Emerging Technologies and Factory Automation. ETFA'95,10-13 Oct. 1995,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/VECIMS.2009.5068923,The hand shape recognition of Human Computer Interaction with Artificial Neural Network,IEEE,Conferences,"The hand gestures used in Human Computer Interaction (HCI) are generally posed by complicated and large amplitude actions of arm /hand. Thus usable HCI instructions are few and HCI efficiency is low. This paper presents new hand shapes and the corresponding recognition system for the HCI with robot or Coordinate Measuring Machine. Using a touch pad to precept the touching of fingers, hand shapes posed to express HCI instructions are defined by the combinations of 2 binary status, i.e. status of touching /detaching on touch pad and status of stretching /retracting over touch pad, of Index, Middle, Ring and Little fingers. Method of extracting the features in hand shape image is presented. Based on Neural Network, a decision binary tree is used in the real-time recognition of the hand shapes. A correctness ratio of about 95% is obtained when implemented by DSP processor in the recognition of 12 hand shapes.",https://ieeexplore.ieee.org/document/5068923/,"2009 IEEE International Conference on Virtual Environments, Human-Computer Interfaces and Measurements Systems",11-13 May 2009,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICSESS.2015.7339119,The research and implementation of artificial intelligence in mobile applications,IEEE,Conferences,"This paper designs and implements the bionic robot which is as an implementation of soldiers in the strategy game, and the robot has three parts: perception system, target and path system and decision-making system. The perception system is responsible for perceiving information inside the scene; the target and path system is to find the best position for attacking and the optimal path for the robot; the decision-making system determines the behavior of the robot in the next frame. This paper also introduces map information updated in real time in the game. The bionic robot system designed has a good expansibility, and soldiers of different arms using this system, the game is running well.",https://ieeexplore.ieee.org/document/7339119/,2015 6th IEEE International Conference on Software Engineering and Service Science (ICSESS),23-25 Sept. 2015,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ROMAN.2003.1251856,The role that the internal model of the others plays in cooperative behavior,IEEE,Conferences,"Internal model of the others is claimed to be essential for the prediction of the others' behavior to realize a cooperative behavior. ""Theory of mind"" is known as a leading framework of the internal modeling. The present research consists of two parts. In the former part, experimental results are shown on the cooperation between a human subject and a software model of a mobile robot. The cooperative task given to the human subject is to carry a stick from a start point to a goal point by holding the one edge of the stick, in a simulation world displayed on a computer terminal. A virtual robot holds the other edge, whose behavior is controlled by a simple deterministic rule. The human subject is asked to perform the task under the following two different conditions A and B. In condition A, the subject is not told about the existence of the robot which holds the other edge, and only the stick is displayed. In condition B, the existence of the robot is told, but still not shown on the display. The performance of the carrying task is much better in the condition A, under which the subject can separate the movements of the stick and of the robot for their prediction. This result suggests that a construction of a behavior model of the other robot is helpful to carry out the cooperative task. In the latter part, a neural network replaces the human subject to perform the same cooperative task with the robot. The network outputs a motion to move the stick, and has a simple layered structure with an additional part to learn to predict the next motion of the robot. The experiments show the improvement in the task performance through learning the prediction. This also suggests the explicit modeling of the others is effective for the cooperation.",https://ieeexplore.ieee.org/document/1251856/,"The 12th IEEE International Workshop on Robot and Human Interactive Communication, 2003. Proceedings. ROMAN 2003.",2-2 Nov. 2003,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ASAP.2018.8445099,Towards Hardware Accelerated Reinforcement Learning for Application-Specific Robotic Control,IEEE,Conferences,"Reinforcement Learning (RL) is an area of machine learning in which an agent interacts with the environment by making sequential decisions. The agent receives reward from the environment based on how good the decisions are and tries to find an optimal decision-making policy that maximises its longterm cumulative reward. This paper presents a novel approach which has showon promise in applying accelerated simulation of RL policy training to automating the control of a real robot arm for specific applications. The approach has two steps. First, design space exploration techniques are developed to enhance performance of an FPGA accelerator for RL policy training based on Trust Region Policy Optimisation (TRPO), which results in a 43% speed improvement over a previous FPGA implementation, while achieving 4.65 times speed up against deep learning libraries running on GPU and 19.29 times speed up against CPU. Second, the trained RL policy is transferred to a real robot arm. Our experiments show that the trained arm can successfully reach to and pick up predefined objects, demonstrating the feasibility of our approach.",https://ieeexplore.ieee.org/document/8445099/,"2018 IEEE 29th International Conference on Application-specific Systems, Architectures and Processors (ASAP)",10-12 July 2018,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA40945.2020.9197446,Towards Plan Transformations for Real-World Mobile Fetch and Place,IEEE,Conferences,"In this paper, we present an approach and an implemented framework for applying plan transformations to real-world mobile manipulation plans, in order to specialize them to the specific situation at hand. The framework can improve execution cost and achieve better performance by autonomously transforming robot's behavior at runtime. To demonstrate the feasibility of our approach, we apply three example transformations to the plan of a PR2 robot performing simple table setting and cleaning tasks in the real world. Based on a large amount of experiments in a fast plan projection simulator, we make conclusions on improved execution performance.",https://ieeexplore.ieee.org/document/9197446/,2020 IEEE International Conference on Robotics and Automation (ICRA),31 May-31 Aug. 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IV51971.2022.9827355,Towards Real-time Traffic Sign and Traffic Light Detection on Embedded Systems,IEEE,Conferences,"Recent work done on traffic sign and traffic light detection focus on improving detection accuracy in complex scenarios, yet many fail to deliver real-time performance, specifically with limited computational resources. In this work, we propose a simple deep learning based end-to-end detection framework, which effectively tackles challenges inherent to traffic sign and traffic light detection such as small size, large number of classes and complex road scenarios. We optimize the detection models using TensorRT and integrate with Robot Operating System to deploy on an Nvidia Jetson AGX Xavier as our embedded device. The overall system achieves a high inference speed of 63 frames per second, demonstrating the capability of our system to perform in real-time. Furthermore, we introduce CeyRo, which is the first ever large-scale traffic sign and traffic light detection dataset for the Sri Lankan context. Our dataset consists of 7984 total images with 10176 traffic sign and traffic light instances covering 70 traffic sign and 5 traffic light classes. The images have a high resolution of 1920 x 1080 and capture a wide range of challenging road scenarios with different weather and lighting conditions. Our work is publicly available at https://github.com/oshadajay/CeyRo.",https://ieeexplore.ieee.org/document/9827355/,2022 IEEE Intelligent Vehicles Symposium (IV),4-9 June 2022,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/MFI.2001.1013553,Towards a learning model for feature integration in attention control,IEEE,Conferences,"We present current efforts towards an approach for the integration of features extracted from multi-modal sensors, with which to guide the attentional behavior of robotic agents. The model can be applied in many situations and different tasks including top-down or bottom-up aspects of attention control. Basically, a pre-attention mechanism enhances attentional features that are relevant to the current task according to a weight function that can be learned. Then, an attention shift mechanism can select one between the various activated stimuli, in order for a robot to foveate on it. Also, in this approach, we consider the robot moving resources or to improve the (visual) sensory information.",https://ieeexplore.ieee.org/document/1013553/,Conference Documentation International Conference on Multisensor Fusion and Integration for Intelligent Systems. MFI 2001 (Cat. No.01TH8590),20-22 Aug. 2001,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AIM43001.2020.9158908,Towards accelerated robotic deployment by supervised learning of latent space observer and policy from simulated experiments with expert policies,IEEE,Conferences,"Up until today robotic tasks in highly variable environments remain very difficult to solve. We propose accelerated robotic deployment through task solving on low-level sensor data in simulation. A simulation allows for a lot of data, which is usually not available in a real world robotic setup due to cost and feasibility. Solving tasks in simulation is safe and a lot easier due to the huge amount of feedback from virtual sensory data. We present a novel sim2real architecture for converting simulated low level sensor data policies to high level real world policies. After solving a task we let the robot complete it a number of times in simulation using domain randomization, while doing so we save the simulated sensor data corresponding to the real robotic setup and actions taken. Given these sensor data and actions a task specific policy can be trained using our architecture. In this paper we work towards a proof of concept by simulating a simple low cost manipulator in pybullet to pick and place an object based on image observations.",https://ieeexplore.ieee.org/document/9158908/,2020 IEEE/ASME International Conference on Advanced Intelligent Mechatronics (AIM),6-9 July 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SMC42975.2020.9283277,Towards an Extended POMDP Planning Approach with Adjoint Action Model for Robotic Task,IEEE,Conferences,"In real-world environments, robotic task planning is expected to handle both partial observability and unexpected dynamics of the environment. A robust plan for the task requires the robot's observation actions to concurrently run with the task actions, to observe and adapt to environmental changes. The Partially Observable Markov Decision Process (POMDP) has been widely applied for planning under partially observable domains. For realistic robotic tasks, however, the POMDP model and planning algorithm are quite restrictive and unrealistic. One limitation is that task actions are modelled as atomic entities that only have endpoint effects, with no conditions specified at arbitrary points during task action execution. Also, the observation is obtained only after each task action execution, with no intermediate observations and decision-making during task action execution. To mitigate the limitations of POMDP planning, this paper first proposes an Adjoint Action Model (AAM) that explicitly defines the continuous interaction between robot's observation and task actions. Then we extend the POMDP task action model with intermediate invariant conditions which specifies the runtime properties of action execution. Finally, we propose the AAM-extended POMDP planning approach which handles observation action planning and task replanning for task action execution. We experimentally demonstrate that the plan from our proposed approach is more effective and robust to cope with the environment dynamics, comparing with the standard POMDP planning approach.",https://ieeexplore.ieee.org/document/9283277/,"2020 IEEE International Conference on Systems, Man, and Cybernetics (SMC)",11-14 Oct. 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/DEVLRN.2011.6037332,Towards incremental learning of task-dependent action sequences using probabilistic parsing,IEEE,Conferences,"We study an incremental process of learning where a set of generic basic actions are used to learn higher-level task-dependent action sequences. A task-dependent action sequence is learned by associating the goal given by a human demonstrator with the task-independent, general-purpose actions in the action repertoire. This process of contextualization is done using probabilistic parsing. We propose stochastic context-free grammars as the representational framework due to its robustness to noise, structural flexibility, and easiness on defining task-independent actions. We demonstrate our implementation on a real-world scenario using a humanoid robot and report implementation issues we had.",https://ieeexplore.ieee.org/document/6037332/,2011 IEEE International Conference on Development and Learning (ICDL),24-27 Aug. 2011,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/LARS/SBR/WRE.2018.00068,Traffic Signs Recognition System with Convolution Neural Networks,IEEE,Conferences,"The purpose of this paper is to develop an automatic traffic sign recognition system, making use of computation vision techniques and convolution neural networks. The work is divided in two phases, namely detection and classification, and here is presented a different approach on the detection phase. The tests were performed in a simulator and in a real controlled environment using the framework ROS (Robot Operating System) and implemented with the AmigoBot robot.",https://ieeexplore.ieee.org/document/8588574/,"2018 Latin American Robotic Symposium, 2018 Brazilian Symposium on Robotics (SBR) and 2018 Workshop on Robotics in Education (WRE)",6-10 Nov. 2018,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS.2004.1389956,Trajectory tracking control of a rotational joint using feature-based categorization learning,IEEE,Conferences,"Real world robot applications have to cope with large variations in the operating conditions due to the variability and unpredictability of the environment and its interaction with the robot. Performing an adequate control using conventional control techniques, that require the model of the plant and some knowledge about the influence of the environment, could be almost impossible. An alternative to traditional control techniques is to use an automatic learning system that uses previous experience to learn an adequate control policy. Learning by experience has been formalized in the field of reinforcement learning. But the application of reinforcement learning techniques in complex environments is only feasible when some generalization can be made in order to reduce the required amount of experience. This work presents an algorithm that performs a kind of generalization called categorization. This algorithm is able to perform efficient generalization of the observed situations, and learn accurate control policies in a short time without any previous knowledge of the plant and without the need of any kind of traditional control technique. Its performance is evaluated on the trajectory tracking control with simulated DC motors and compared with PID systems specifically tuned for the same problem.",https://ieeexplore.ieee.org/document/1389956/,2004 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) (IEEE Cat. No.04CH37566),28 Sept.-2 Oct. 2004,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICAIIC54071.2022.9722630,Two-Policy Cooperative Transfer for Alleviation of Sim-to-Real Gap,IEEE,Conferences,"The main difficulty of sim-to-real is the reality gap between the source domain and the target domain. In order to solve it, various methods where domain randomization is the mainstream have been emerged, whose essence is to make the single policy more robust. In contrast, we propose a novel transfer method, namely two-policy cooperative transfer, whose core is that one policy (task policy) is used to complete the task and another policy (gap policy) aims to assist the former to cover the gap, hence we can focus on the training of task and the overcoming of gap respectively. Based on this method, the setting of the learning objective of gap policy depends on the transfer situation of deploying task policy into real system, besides how to conduct the cooperation of the both lies in the threshold reflecting gap and the coupling of output actions of two policies. For the typical contact-rich gap in the dynamics field, we design an adaptive object pushing experiment based on UR3 robot, and verify the effectiveness of the proposed method.",https://ieeexplore.ieee.org/document/9722630/,2022 International Conference on Artificial Intelligence in Information and Communication (ICAIIC),21-24 Feb. 2022,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA.2019.8793644,Underwater Communication Using Full-Body Gestures and Optimal Variable-Length Prefix Codes,IEEE,Conferences,"In this paper we consider inter-robot communication in the context of joint activities. In particular, we focus on convoying and passive communication for radio-denied environments by using whole-body gestures to provide cues regarding future actions. We develop a communication protocol whereby information described by codewords is transmitted by a series of actions executed by a swimming robot. These action sequences are chosen to optimize robustness and transmission duration given the observability, natural activity of the robot and the frequency of different messages. Our approach uses a convolutional network to make core observations of the pose of the robot being tracked, which is sending messages. The observer robot then uses an adaptation of classical decoding methods to infer a message that is being transmitted. The system is trained and validated using simulated data, tested in the pool and is targeted for deployment in the open ocean. Our decoder achieves.94 precision and.66 recall on real footage of robot gesture execution recorded in a swimming pool.",https://ieeexplore.ieee.org/document/8793644/,2019 International Conference on Robotics and Automation (ICRA),20-24 May 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.23919/MIPRO52101.2021.9596823,Underwater ROV Software for Fish Cage Inspection,IEEE,Conferences,"This paper deals with the development of control software for a remotely operated vehicle (ROV) as part of an automated fish cage inspection system in aquaculture. The ROV navigates autonomously around the fish cages streaming video to the topside computer that runs the algorithms for vertical rope detection. The topside computer sends back the velocity references to the ROV in real-time in order to successfully complete the inspection task. These images are also used to determine the state of net biofouling using a pre-trained convolutional neural network to help determine which nets are in need of cleaning. Robot Operating System (ROS) framework is developed to enable the topside computer to access the video stream from the ROV, process the images, and send back velocity references that would result in the complete inspection of fish cages. The inspection task is planned by following the recognizable rope segments of the outer structure of the fish cage downwards by controlling the vehicle&#x0027;s yaw, heave, and depth.",https://ieeexplore.ieee.org/document/9596823/,"2021 44th International Convention on Information, Communication and Electronic Technology (MIPRO)",27 Sept.-1 Oct. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/MED48518.2020.9183337,Unsupervised Learning for Subterranean Junction Recognition Based on 2D Point Cloud,IEEE,Conferences,"This article proposes a novel unsupervised learning framework for detecting the number of tunnel junctions in subterranean environments based on acquired 2D point clouds. The implementation of the framework provides valuable information for high level mission planners to navigate an aerial platform in unknown areas or robot homing missions. The framework utilizes spectral clustering, which is capable of uncovering hidden structures from connected data points lying on non-linear manifolds. The spectral clustering algorithm computes a spectral embedding of the original 2D point cloud by utilizing the eigen decomposition of a matrix that is derived from the pairwise similarities of these points. We validate the developed framework using multiple data-sets, collected from multiple realistic simulations, as well as from real flights in underground environments, demonstrating the performance and merits of the proposed methodology.",https://ieeexplore.ieee.org/document/9183337/,2020 28th Mediterranean Conference on Control and Automation (MED),15-18 Sept. 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS.2013.6696581,Unsupervised learning of predictive parts for cross-object grasp transfer,IEEE,Conferences,"We present a principled solution to the problem of transferring grasps across objects. Our approach identifies, through autonomous exploration, the size and shape of object parts that consistently predict the applicability of a grasp across multiple objects. The robot can then use these parts to plan grasps onto novel objects. By contrast to most recent methods, we aim to solve the part-learning problem without the help of a human teacher. The robot collects training data autonomously by exploring different grasps on its own. The core principle of our approach is an intensive encoding of low-level sensorimotor uncertainty with probabilistic models, which allows the robot to generalize the noisy autonomously-generated grasps. Object shape, which is our main cue for predicting grasps, is encoded with surface densities, that model the spatial distribution of points that belong to an object's surface. Grasp parameters are modeled with grasp densities, that correspond to the spatial distribution of object-relative gripper poses that lead to a grasp. The size and shape of grasp-predicting parts are identified by sampling the cross-object correlation of local shape and grasp parameters. We approximate sampling and integrals via Monte Carlo methods to make our computer implementation tractable. We demonstrate the applicability of our method in simulation. A proof of concept on a real robot is also provided.",https://ieeexplore.ieee.org/document/6696581/,2013 IEEE/RSJ International Conference on Intelligent Robots and Systems,3-7 Nov. 2013,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICSMC.1993.385064,Use of case-based reasoning techniques for intelligent computer-aided-design systems,IEEE,Conferences,"Reuse of designs is an important research direction for the future intelligent CAD systems. The main applications of such a research are various, from mechanical systems design (spacecraft, robot, ...) to software design. This paper will present a survey of the use of case-based reasoning (CBR) techniques for intelligent CAD systems in order to reuse designs or parts of designs. First, we will briefly resume some work issued from cognitive psychology, showing the importance of analogical-reasoning for design activities and then the origins of the CBR technology in AI. Second, we will then present the main systems using case-based reasoning for design activities followed by a comparative analysis between these systems. To conclude, we will indicate the main directions in CBR for design and will propose to adopt a cognitive approach from knowledge acquisition until the development of real design support systems.<>",https://ieeexplore.ieee.org/document/385064/,Proceedings of IEEE Systems Man and Cybernetics Conference - SMC,17-20 Oct. 1993,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA48506.2021.9561936,ViNG: Learning Open-World Navigation with Visual Goals,IEEE,Conferences,"We propose a learning-based navigation system for reaching visually indicated goals and demonstrate this system on a real mobile robot platform. Learning provides an appealing alternative to conventional methods for robotic navigation: instead of reasoning about environments in terms of geometry and maps, learning can enable a robot to learn about navigational affordances, understand what types of obstacles are traversable (e.g., tall grass) or not (e.g., walls), and generalize over patterns in the environment. However, unlike conventional planning algorithms, it is harder to change the goal for a learned policy during deployment. We propose a method for learning to navigate towards a goal image of the desired destination. By combining a learned policy with a topological graph constructed out of previously observed data, our system can determine how to reach this visually indicated goal even in the presence of variable appearance and lighting. Three key insights, waypoint proposal, graph pruning and negative mining, enable our method to learn to navigate in real-world environments using only offline data, a setting where prior methods struggle. We instantiate our method on a real outdoor ground robot and show that our system, which we call ViNG, outperforms previously-proposed methods for goal-conditioned reinforcement learning, including other methods that incorporate reinforcement learning and search. We also study how ViNG generalizes to unseen environments and evaluate its ability to adapt to such an environment with growing experience. Finally, we demonstrate ViNG on a number of real-world applications, such as last-mile delivery and warehouse inspection. We encourage the reader to visit the project website for videos of our experiments and demonstrations 1.",https://ieeexplore.ieee.org/document/9561936/,2021 IEEE International Conference on Robotics and Automation (ICRA),30 May-5 June 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.23919/WAC50355.2021.9559586,Virtual Testing and Policy Deployment Framework for Autonomous Navigation of an Unmanned Ground Vehicle Using Reinforcement Learning,IEEE,Conferences,"The use of deep reinforcement learning (DRL) as a framework for training a mobile robot to perform optimal navigation in an unfamiliar environment is a suitable choice for implementing AI with real-time robotic systems. In this study, the environment and surrounding obstacles of an Ackermann-steered UGV are reconstructed into a virtual setting for training the UGV to centrally learn the optimal route (guidance actions to be taken at any given state) towards a desired goal position using Multi-Agent Virtual Exploration in Deep Q-Learning (MVEDQL) for various model configurations. The trained model policies are to be transferred to a physical vehicle and compared based on their individual effectiveness for performing autonomous waypoint navigation. Prior to incorporating the learned model with the physical UGV for testing, this paper outlines the development of a GUI application to provide an interface for remotely deploying the vehicle and a virtual reality framework reconstruction of the training environment to assist safely testing the system using the reinforcement learning model.",https://ieeexplore.ieee.org/document/9559586/,2021 World Automation Congress (WAC),1-5 Aug. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISWCS.2019.8877305,Visible Light Positioning for Location-Based Services in Industry 4.0,IEEE,Conferences,"Industry 4.0 refers to the evolution in manufacturing from computerization to fully cyberphysical systems that exploit rich sensor data, adaptive real-time safety-critical control, and machine learning. An important aspect of this vision is the sensing and subsequent association of objects in the physical world with their cyber and virtual counterparts. In this paper we propose Visible Light Positioning (VLP) as an enabler for these Industry 4.0 applications. We also explore sensing techniques, including cameras (and depth sensors), and other light-based solutions for object positioning and detection along with their respective limitations. We then demonstrate an application of positioning for real time robot control in an interactive multiparty cyber-physical-virtual deployment. Lastly, based on our experience with this cyberphysical-virtual application, we propose Ray-Surface Positioning (RSP), a novel VLP technique, as a low cost positioning system for Industry 4.0.",https://ieeexplore.ieee.org/document/8877305/,2019 16th International Symposium on Wireless Communication Systems (ISWCS),27-30 Aug. 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/I2MTC.2019.8826921,Vision-Based Deep Learning Approach for Real-Time Detection of Weeds in Organic Farming,IEEE,Conferences,"Vision-based detection and classification systems for identifying crops and weeds in captured color images have recently being extensively researched due to the advantages that they offer. The use of chemical or synthetic pesticides could drastically be reduced. One of the critical aspects of these systems is the requirement for high data volumes and the resulting lack of real time capability. This paper presents a method for detecting weeds in carrot fields in real time without segmentation and the need of a large dataset. In most vision-based measurement systems the task is divided into multiple processes like separating the objects from the background followed by the detection of the object and lastly the object classification. Our approach uses a convolution neural network to localize and classify the plants simultaneously. A precision of 89 % was achieved with a calculation rate of 18,56 FPS. A lower precision was accepted in favor of a higher calculation rate of about 56 FPS. We implemented and evaluated our system using a multi-platform robot on an organic carrot field located in Germany.",https://ieeexplore.ieee.org/document/8826921/,2019 IEEE International Instrumentation and Measurement Technology Conference (I2MTC),20-23 May 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ROBOT.1995.525277,Vision-based reinforcement learning for purposive behavior acquisition,IEEE,Conferences,"This paper presents a method of vision-based reinforcement learning by which a robot learns to shoot a ball into a goal, and discusses several issues in applying the reinforcement learning method to a real robot with vision sensor. First, a ""state-action deviation"" problem is found as a form of perceptual aliasing in constructing the state and action spaces that reflect the outputs from physical sensors and actuators, respectively. To cope with this, an action set is constructed in such a way that one action consists of a series of the same action primitive which is successively executed until the current state changes. Next, to speed up the learning time, a mechanism of learning form easy missions (or LEM) which is a similar technique to ""shaping"" in animal learning is implemented. LEM reduces the learning time from the exponential order in the size of the state space to about the linear order in the size of the state space. The results of computer simulations and real robot experiments are given.",https://ieeexplore.ieee.org/document/525277/,Proceedings of 1995 IEEE International Conference on Robotics and Automation,21-27 May 1995,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICSTCC.2019.8885611,Visual Analytics Framework for Condition Monitoring in Cyber-Physical Systems,IEEE,Conferences,"One of the biggest challenges facing the factory of the future today is to reduce the time-to-market access and increase through the improvement of competitiveness and efficiency. In order to achieve this target, data analytics in Industrial Cyber-Physical System becomes a feasible option. In this paper, a visual analytics framework for condition monitoring of the machine tool is presented with the aim to manage events and alarms at factory level. The framework is assessed in a particular use case that consists in a multi-threaded cloud-based solution for the global analysis of the behaviour of variables acquired from PLC, CNC and robot manipulator. A human-machine interface is also designed for the real-time visualization of the key performance indicators according to the user's criteria. This tool implemented is a great solution for condition monitoring and decision-making process based on data analytics from simple statistics to complex machine learning methods. The results achieved are part of the vision and implementation of the industrial test bed of “Industry and Society 5.0” platform.",https://ieeexplore.ieee.org/document/8885611/,"2019 23rd International Conference on System Theory, Control and Computing (ICSTCC)",9-11 Oct. 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CNNA.2002.1035063,Visual feedback by using a CNN chip prototype system,IEEE,Conferences,"Robot locomotion control passes through a series of sensors that, according to information from the environment, allow the robot to adapt, in real time, its locomotion scheme or trajectory. When the goal of the robot is to reach a target in a non-structured environment the best approach is visual control realized by a fast image processing system. Fast parallel image processing of the CNN-UM cP4000 chip prototype permits one to obtain good performance, even in a real time control problem. The robot controlled by the implemented CNN visual feedback has a hexapod configuration and its locomotion system is also implemented by a multi-layer CNN structure. In this paper a CNN approach for both locomotion generation and visual control of the bio-inspired robot is presented.",https://ieeexplore.ieee.org/document/1035063/,Proceedings of the 2002 7th IEEE International Workshop on Cellular Neural Networks and Their Applications,24-24 July 2002,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ROBOT.1991.131999,Visual navigation around curved obstacles,IEEE,Conferences,"An approach to path-planning around smooth obstacles that exploits visually derived geometry is proposed. A moving robot can scan the silhouette or apparent contour of an obstacle and estimate a minimum length path. This is done by seeking geodesics which can be extrapolated smoothly, around the obstacle and towards the goal. Preliminary implementation of this idea uses a real-time visual contour tracker running at 16 Hz, with a camera mounted on an Adept robot arm. The camera first dithers to generate visual motion, a safe path is estimated, and the robot steers the camera around the obstacle with a clearance of a few millimeters.<>",https://ieeexplore.ieee.org/document/131999/,Proceedings. 1991 IEEE International Conference on Robotics and Automation,9-11 April 1991,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS.1997.649086,Visually-guided obstacle avoidance in unstructured environments,IEEE,Conferences,"This paper presents an autonomous vision-based obstacle avoidance system. The system consists of three independent vision modules for obstacle detection, each of which is computationally simple and uses a different criterion for detection purposes. These criteria are based on brightness gradients, RGB (red, green, blue) color, and HSV (hue, saturation, value) color, respectively. Selection of which modules are used to command the robot proceeds exclusively from the outputs of the modules themselves. The system is implemented on a small monocular mobile robot and uses very lour resolution images. It has been tested for over 200 hours in diverse environments.",https://ieeexplore.ieee.org/document/649086/,Proceedings of the 1997 IEEE/RSJ International Conference on Intelligent Robot and Systems. Innovative Robotics for Real-World Applications. IROS '97,11-11 Sept. 1997,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/RITAPP.2019.8932854,Which LSTM Type is Better for Interaction Force Estimation?,IEEE,Conferences,"Tactile, one of the five senses classified into the main senses of human, is the first sensation developed when human beings are formed. The tactile includes various information such as pressure, temperature, and texture of objects, it also helps the person to interact with the surrounding environment. One of the tactile information, the pressure is used in various fields such as medical, beauty, mobile devices and so on. However, humans can perceive the real world with multi-modal senses such as sound, vision. In this paper, we study interaction force estimation using haptic sensor and video. Interact ion force estimation through video analysis is one of a cross-modal approach that is applicable such as a software haptic feedback method that can give haptic feedback to remote control of robot arm by predicting interaction force even in absence of haptic sensor. we compare and analyze three types of a deep neural network to predict the interaction force. In particular, the best model for the stacking structure of CNN and LSTM is selected through a detailed analysis of how the structure change of LSTM affects the video regression problem. The average error of the best suit model is MSE 0.1306, RMSE 0.2740, MAE 0.1878.",https://ieeexplore.ieee.org/document/8932854/,2019 7th International Conference on Robot Intelligence Technology and Applications (RiTA),1-3 Nov. 2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IWACI.2011.6159663,[Front and back cover],IEEE,Conferences,The following topics are dealt with: advanced computational intelligence; rough set; genetic algorithm; distributed computing; flexible job-shop scheduling; system-of-systems vulnerability analysis; V-BLAST sphere decoding; spanning tree problem; object representation model; real time path planning; ant colony optimization; multi-objective evolutionary algorithm; RBF neural network; particle swarm optimization; multi-color extraction method; adaptive NN tracking control; gravitational chaotic search algorithm; approximate optimal control tracking; multi-target tracking approach; generalize disjunctive paraconsistent data model; discriminative item mining; weighted passive nearest neighbor algorithm; structure-encoding differential evolution algorithm; real-time data stream clustering; event-driven control program automatic verification; threshold signature scheme; data model driven architecture; improved post-nonlinear independent component analysis; pruning algorithm; remote sensing image classification; fuzzy matrices; grey-box neural network; training ANFIS system; delay BAM neural network stability; tuning method; ensemble learning balancing; Kalman filtering; hybrid learning model; multi-focus image fusion; object-based image retrieval; language grounding model; decision fusion; functional network analysis; image interpolation; image deblurring method; discrete-time dynamic system stability; feature selection methods; Lasso logistic regression; vehicle scheduling; cooperative air-defense system; bifurcation analysis; close-loop time-delayed filter system; Newton iteration formula; 3D object recognition; parameter identification; intelligent displacement back-analysis method; single machine total weighted tardiness scheduling problem; dimensionality reduction method; temporal Bayesian network; online leasing problem; network supported intelligent cooperative diagnosis; Hopf bifurcation analysis; multi-mode human-machine interface; GA-fuzzy automatic generation controller; tele-operation robot system; hybrid clonal selection algorithm; modified LEACH protocol; fuzzy Lyapunov synthesis; and underwater vehicle.,https://ieeexplore.ieee.org/document/6159663/,The Fourth International Workshop on Advanced Computational Intelligence,19-21 Oct. 2011,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AICAS51828.2021.9458401,iELAS: An ELAS-Based Energy-Efficient Accelerator for Real-Time Stereo Matching on FPGA Platform,IEEE,Conferences,"Stereo matching is a critical task for robot navigation and autonomous vehicles, providing the depth estimation of surroundings. Among all stereo matching algorithms, Efficient Large-scale Stereo (ELAS) offers one of the best tradeoffs between efficiency and accuracy. However, due to the inherent iterative process and unpredictable memory access pattern, ELAS can only run at 1.5-3 fps on high-end CPUs and difficult to achieve real-time performance on low-power platforms. In this paper, we propose an energy-efficient architecture for real-time ELAS-based stereo matching on FPGA platform. Moreover, the original computational-intensive and irregular triangulation module is reformed in a regular manner with points interpolation, which is much more hardware-friendly. optimizations, including memory management, parallelism, and pipelining, are further utilized to reduce memory footprint and improve throughput. Compared with Intel i7 CPU and the state-of-the-art $\mathrm{C}\mathrm{P}\mathrm{U}+$FPGA implementation, our FPGA realization achieves up to $ 38.4\times$ and $ 3.32\times$ frame rate improvement, and up to $ 27.1\times$ and $ 1.13\times$ energy efficiency improvement, respectively.",https://ieeexplore.ieee.org/document/9458401/,2021 IEEE 3rd International Conference on Artificial Intelligence Circuits and Systems (AICAS),6-9 June 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ACCESS.2017.2787738,A Neuro-Fuzzy Visual Servoing Controller for an Articulated Manipulator,IEEE,Journals,"The challenges of selecting appropriate image features, optimizing complex nonlinear computations, and minimizing the approximation errors always exist in visual servoing. A fuzzy neural network controller is developed for a six-degrees-of-freedom robot manipulator to perform visual servoing is proposed to tackle these problems. To increase the accuracy of the image preprocesses, a synthetic image process performs feature extraction for the controller. The method combines a support vector machine contour recognition algorithm and a color-based feature recognition algorithm. For visual servoing, a control method based on the fuzzy cerebellar model articulation controller with the Takagi-Sugeno framework is proposed to directly map an image feature error vector to a desired robot end-effector velocity. This approach achieves visual servoing control without the need of computing the inverse interaction matrix. The control variables are learned and updated by the T-S fuzzy inference. This simplifies the implementation of visual servoing in real-time applications. The proposed control method is used to control an articulated manipulator with an eye-in-hand configuration. The results of simulations and experiments demonstrate that the proposed visual servoing controller has good performance, in terms of stability and convergence.",https://ieeexplore.ieee.org/document/8247175/,IEEE Access,2018,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ACCESS.2020.2973756,A New Automatic Real-Time Crop Row Recognition Based on SoC-FPGA,IEEE,Journals,"With the development of artificial intelligence technology, agricultural robot plays a significantly important role for agricultural intelligence. Crop row line detection is a critical and fundamental step for agricultural robot navigation. Although there are some crop row lines detection methods, few of them can meet the real-time requirement for agricultural robot under complex fields conditions. In view of this, a real-time crop detection system implemented on a SoC FPGA (System-on-a-Chip Field Programmable Gate Array) is first proposed in this paper, which contains crop segmentation and crop row detection, where we design parallel pipeline architecture to enhance real-time performance by using line buffer and sliding windows technologies. At the same time, the fixed point representation is used to reduce the memory resource in this system. The proposed system is evaluated and implemented on Xilinx Zynq UltraScale+ MPSoC ZCU102 SoC-FPGA. The experimental results show that the proposed system can process an image with 1920×1080 resolution only within 210 ms with the average accuracy of 89.7%, which satisfies the real-time requirements of the crop rows recognition.",https://ieeexplore.ieee.org/document/8998211/,IEEE Access,2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ACCESS.2019.2946848,A New Multi-Agent Reinforcement Learning Method Based on Evolving Dynamic Correlation Matrix,IEEE,Journals,"Multi-agent reinforcement learning approaches can be roughly classified into two categories. One is the agent-based approach which can be implemented in real distributed systems, though most approaches of this type cannot provide meaningful theoretical verifications. The other can be seen as the more formalized approach, which can provide theoretical results. However, most of current algorithms usually require unrealistic global communication, which makes them impractical for real distributed systems. In this article, we propose a dynamic correlation matrix based multi-agent reinforcement learning approach where the meta-parameters are evolved using an evolutionary algorithm. We believe that our approach is able to fill the gap between the two kinds of traditional multi-agent reinforcement learning approaches by providing both agent-level implementation and system-level convergence verification. The basic idea of this approach is that agents learn not only from local environmental feedback, i.e., their own experiences and rewards, but also from other agents' experiences. In this way, the agents' learning speed can be increased significantly. The performance of the proposed algorithm is demonstrated on a number of application scenarios, including blackjack games, urban traffic control systems and multi-robot foraging.",https://ieeexplore.ieee.org/document/8864051/,IEEE Access,2019,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/TASE.2020.2980628,A System Architecture for CAD-Based Robotic Assembly With Sensor-Based Skills,IEEE,Journals,"Specifying assembly tasks in computer-aided design (CAD) level is a promising approach to intuitively program complex robot skills. In this article, a three-layered system architecture is presented to generate sensor-based robot skills from an assembly task instance. The architecture consists of an application layer where the user instantiates assembly tasks by specifying CAD constraints between geometric primitives pairs. A process layer infers the most suitable robot skills and their appropriate parameters. This inference is made possible by reasoning on a knowledge database represented as an ontology. The ontology contains semantic models of relevant classes such as tasks, skills, and geometric primitives as well as the relations between them. A control layer executes the sensor-based skills in real time using the eTaSL programming framework. A software implementation for the three layers is presented. The application layer is implemented in FreeCAD, whereas the process layer consists of a Web ontology language (OWL) ontology, a Prolog-based reasoner, and fuzzy inference to correctly select the skill and generate its parameters. In the control layer, the instantiated eTaSL skills execute the assembly tasks by sending an optimized control command to the robot. The system is validated on two challenging assembly cases with two distinct robot types, thus demonstrating the system's capability across different scenarios. Note to Practitioners-The widespread use of computer-aided design (CAD) models for describing parts assembly has motivated the research community to create systems that automatically generate robot programs satisfying the assembly goal. While most of the existing literature focuses on generating the assembly sequence, this article deals with the aspect of translation from CAD-level assembly specification to executable robot motion, also called skills. This article systematically addresses the problem by dividing it into different layers and solving them separately. Parameters that influence the successful execution of an assembly task are identified and categorized into application- and process-related parameters. Different inference techniques are employed to address each parameter category. Experimental results show that the proposed system can successfully generate and execute robot skills for assembly scenarios of an air compressor and an electric motor.",https://ieeexplore.ieee.org/document/9061146/,IEEE Transactions on Automation Science and Engineering,July 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/TIE.2017.2764849,A Vision-Aided Approach to Perching a Bioinspired Unmanned Aerial Vehicle,IEEE,Journals,"This paper presents the implementation of a machine learning approach for replicating highly adaptive avian perching behavior. With full consideration of both the configuration of flying vehicles and perching principles, a bioinspired aerial robot comprising one flight subsystem and one perching subsystem is designed. Based on the real-time landing speed and attitude, a novel type of soft grasping mechanism for dexterous perching is proposed to provide adhesive force and absorb impact force. During the critical perching phase, the dynamics of the perching actuator change with the touchdown conditions and the type of perching target. A hybrid automation of a time-to-contact theory-based attitude controller and a robust self-localization system are utilized to regulate the desired perching maneuvers. The experimental results are provided to attest to the effectiveness of the proposed perching method.",https://ieeexplore.ieee.org/document/8074761/,IEEE Transactions on Industrial Electronics,May 2018,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/70.88099,A behavior-based arm controller,IEEE,Journals,"The author presents a working, implemented controller for an actual mobile robot arm. The goal of the system is to locate and retrieve empty soda cans in an unstructured environment using a variety of local sensors. The controller, however, is not a centralized sequential program, but rather a collection of 15 independent behaviors. Each of these behaviors contains some grain of expertise concerning the collection task and cooperates with the others to accomplish its goal. These behaviors run concurrently, in real time, on a set of eight loosely coupled on-board 8-bit microprocessors. The author describes the methodology used to decompose the collection task and discusses the types of implicit spatial representation and reasoning used by the system.<>",https://ieeexplore.ieee.org/document/88099/,IEEE Transactions on Robotics and Automation,Dec. 1989,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/TIE.2016.2538741,Adaptive Impedance Control for an Upper Limb Robotic Exoskeleton Using Biological Signals,IEEE,Journals,"This paper presents adaptive impedance control of an upper limb robotic exoskeleton using biological signals. First, we develop a reference musculoskeletal model of the human upper limb and experimentally calibrate the model to match the operator's motion behavior. Then, the proposed novel impedance algorithm transfers stiffness from human operator through the surface electromyography (sEMG) signals, being utilized to design the optimal reference impedance model. Considering the unknown deadzone effects in the robot joints and the absence of the precise knowledge of the robot's dynamics, an adaptive neural network control incorporating with a high-gain observer is developed to approximate the deadzone effect and robot's dynamics and drive the robot tracking desired trajectories without velocity measurements. In order to verify the robustness of the proposed approach, the actual implementation has been performed using a real robotic exoskeleton and a human operator.",https://ieeexplore.ieee.org/document/7426396/,IEEE Transactions on Industrial Electronics,Feb. 2017,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/LRA.2020.2965911,Aggressive Perception-Aware Navigation Using Deep Optical Flow Dynamics and PixelMPC,IEEE,Journals,"Recently, vision-based control has gained traction by leveraging the power of machine learning. In this work, we couple a model predictive control (MPC) framework to a visual pipeline. We introduce deep optical flow (DOF) dynamics, which is a combination of optical flow and robot dynamics. Using the DOF dynamics, MPC explicitly incorporates the predicted movement of relevant pixels into the planned trajectory of a robot. Our implementation of DOF is memory-efficient, data-efficient, and computationally cheap so that it can be computed in real-time for use in an MPC framework. The suggested Pixel Model Predictive Control (PixelMPC) algorithm controls the robot to accomplish a high-speed racing task while maintaining visibility of the important features (gates). This improves the reliability of vision-based estimators for localization and can eventually lead to safe autonomous flight. The proposed algorithm is tested in a photorealistic simulation with a high-speed drone racing task.",https://ieeexplore.ieee.org/document/8957291/,IEEE Robotics and Automation Letters,April 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/TIE.2020.2979561,An Intelligent Non-Integer PID Controller-Based Deep Reinforcement Learning: Implementation and Experimental Results,IEEE,Journals,"In this article, a noninteger proportional integral derivative (PID)-type controller based on the deep deterministic policy gradient algorithm is developed for the tracking problem of a mobile robot. This robot system is a typical case of nonholonomic plants and is exposed to the measurement noises and external disturbances. To accomplish the control methodology, two control mechanisms are established independently: a kinematic controller (which is designed based on the kinematic model of the vehicle), and a dynamic controller (which is realized according to the physical specifications of the vehicle dynamics). In particular, an optimal noninteger PID controller is initially designed as the primary dynamic controller for the tracking problem of a nonholonomic wheeled mobile robot. Then, a DDPG algorithm with the actor-critic framework is established for the supplementary dynamic controller, which is beneficial to the tracking stabilization by adapting to the uncertainties and disturbances. This strategy implements the supplementary based control to compensate for what the original controller is unable to handle. A prototype of the WMR was also adopted to investigate the applicability of the suggested controller from a real-time platform perspective. The outcomes in experimental environments are presented to affirm the effectiveness of the suggested control methodology.",https://ieeexplore.ieee.org/document/9042812/,IEEE Transactions on Industrial Electronics,April 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/JTEHM.2018.2822681,An IoT-Enabled Stroke Rehabilitation System Based on Smart Wearable Armband and Machine Learning,IEEE,Journals,"Surface electromyography signal plays an important role in hand function recovery training. In this paper, an IoT-enabled stroke rehabilitation system was introduced which was based on a smart wearable armband (SWA), machine learning (ML) algorithms, and a 3-D printed dexterous robot hand. User comfort is one of the key issues which should be addressed for wearable devices. The SWA was developed by integrating a low-power and tiny-sized IoT sensing device with textile electrodes, which can measure, pre-process, and wirelessly transmit bio-potential signals. By evenly distributing surface electrodes over user's forearm, drawbacks of classification accuracy poor performance can be mitigated. A new method was put forward to find the optimal feature set. ML algorithms were leveraged to analyze and discriminate features of different hand movements, and their performances were appraised by classification complexity estimating algorithms and principal components analysis. According to the verification results, all nine gestures can be successfully identified with an average accuracy up to 96.20%. In addition, a 3-D printed five-finger robot hand was implemented for hand rehabilitation training purpose. Correspondingly, user's hand movement intentions were extracted and converted into a series of commands which were used to drive motors assembled inside the dexterous robot hand. As a result, the dexterous robot hand can mimic the user's gesture in a real-time manner, which shows the proposed system can be used as a training tool to facilitate rehabilitation process for the patients after stroke.",https://ieeexplore.ieee.org/document/8356006/,IEEE Journal of Translational Engineering in Health and Medicine,2018,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/70.68083,An automatic navigation system for vision guided vehicles using a double heuristic and a finite state machine,IEEE,Journals,"A navigation system for automatic vision-guided vehicles which uses an efficient double heuristic search algorithm for path planning is presented. It is capable of avoiding unknown obstacles and recovering from unidentifiable locations. A linked list representation of the path network database makes the implementation feasible in any high-level language and renders it suitable for real-time application. Extensive simulated experiments have been conducted to verify the validity of the proposed algorithms. The combination of the techniques of robot navigation in unexplored terrain and the global map method proved to be a valid technique for automated guided vehicle (AGV) guidance. A learning mechanism is used in the AGV by updating the path network during navigation. Simulated results supported all the theoretically expected conclusions, since the robot planned its path correctly between the requested nodes and maneuvered its way around the obstacles. Overall, the results were very encouraging.<>",https://ieeexplore.ieee.org/document/68083/,IEEE Transactions on Robotics and Automation,Feb. 1991,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/TNN.2005.845217,Auditory learning: a developmental method,IEEE,Journals,"Motivated by the human autonomous development process from infancy to adulthood, we have built a robot that develops its cognitive and behavioral skills through real-time interactions with the environment. We call such a robot a developmental robot. In this paper, we present the theory and the architecture to implement a developmental robot and discuss the related techniques that address an array of challenging technical issues. As an application, experimental results on a real robot, self-organizing, autonomous, incremental learner (SAIL), are presented with emphasis on its audition perception and audition-related action generation. In particular, the SAIL robot conducts the auditory learning from unsegmented and unlabeled speech streams without any prior knowledge about the auditory signals, such as the designated language or the phoneme models. Neither available before learning starts are the actions that the robot is expected to perform. SAIL learns the auditory commands and the desired actions from physical contacts with the environment including the trainers.",https://ieeexplore.ieee.org/document/1427765/,IEEE Transactions on Neural Networks,May 2005,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ACCESS.2020.2990405,Clustering-Based Speech Emotion Recognition by Incorporating Learned Features and Deep BiLSTM,IEEE,Journals,"Emotional state recognition of a speaker is a difficult task for machine learning algorithms which plays an important role in the field of speech emotion recognition (SER). SER plays a significant role in many real-time applications such as human behavior assessment, human-robot interaction, virtual reality, and emergency centers to analyze the emotional state of speakers. Previous research in this field is mostly focused on handcrafted features and traditional convolutional neural network (CNN) models used to extract high-level features from speech spectrograms to increase the recognition accuracy and overall model cost complexity. In contrast, we introduce a novel framework for SER using a key sequence segment selection based on redial based function network (RBFN) similarity measurement in clusters. The selected sequence is converted into a spectrogram by applying the STFT algorithm and passed into the CNN model to extract the discriminative and salient features from the speech spectrogram. Furthermore, we normalize the CNN features to ensure precise recognition performance and feed them to the deep bi-directional long short-term memory (BiLSTM) to learn the temporal information for recognizing the final state of emotion. In the proposed technique, we process the key segments instead of the whole utterance to reduce the computational complexity of the overall model and normalize the CNN features before their actual processing, so that it can easily recognize the Spatio-temporal information. The proposed system is evaluated over different standard dataset including IEMOCAP, EMO-DB, and RAVDESS to improve the recognition accuracy and reduce the processing time of the model, respectively. The robustness and effectiveness of the suggested SER model is proved from the experimentations when compared to state-of-the-art SER methods with an achieve up to 72.25%, 85.57%, and 77.02% accuracy over IEMOCAP, EMO-DB, and RAVDESS dataset, respectively.",https://ieeexplore.ieee.org/document/9078789/,IEEE Access,2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/LRA.2020.3003256,Denoising IMU Gyroscopes With Deep Learning for Open-Loop Attitude Estimation,IEEE,Journals,"This article proposes a learning method for denoising gyroscopes of Inertial Measurement Units (IMUs) using ground truth data, and estimating in real time the orientation (attitude) of a robot in dead reckoning. The obtained algorithm outperforms the state-of-the-art on the (unseen) test sequences. The obtained performances are achieved, thanks to a well-chosen model, a proper loss function for orientation increments, and through the identification of key points when training with high-frequency inertial data. Our approach builds upon a neural network based on dilated convolutions, without requiring any recurrent neural network. We demonstrate how efficient our strategy is for 3D attitude estimation on the EuRoC and TUM-VI datasets. Interestingly, we observe our dead reckoning algorithm manages to beat top-ranked visual-inertial odometry systems in terms of attitude estimation although it does not use vision sensors. We believe this article offers new perspectives for visual-inertial localization and constitutes a step toward more efficient learning methods involving IMUs. Our open-source implementation is available at https://github.com/mbrossar/denoise-imu-gyro.",https://ieeexplore.ieee.org/document/9119813/,IEEE Robotics and Automation Letters,July 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/TSMCB.2004.831151,Development of a biomimetic robotic fish and its control algorithm,IEEE,Journals,"This paper is concerned with the design of a robotic fish and its motion control algorithms. A radio-controlled, four-link biomimetic robotic fish is developed using a flexible posterior body and an oscillating foil as a propeller. The swimming speed of the robotic fish is adjusted by modulating joint's oscillating frequency, and its orientation is tuned by different joint's deflections. Since the motion control of a robotic fish involves both hydrodynamics of the fluid environment and dynamics of the robot, it is very difficult to establish a precise mathematical model employing purely analytical methods. Therefore, the fish's motion control task is decomposed into two control systems. The online speed control implements a hybrid control strategy and a proportional-integral-derivative (PID) control algorithm. The orientation control system is based on a fuzzy logic controller. In our experiments, a point-to-point (PTP) control algorithm is implemented and an overhead vision system is adopted to provide real-time visual feedback. The experimental results confirm the effectiveness of the proposed algorithms.",https://ieeexplore.ieee.org/document/1315762/,"IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics)",Aug. 2004,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/TCYB.2019.2962086,Diversity-Sensitive Generative Adversarial Network for Terrain Mapping Under Limited Human Intervention,IEEE,Journals,"In a collaborative air–ground robotic system, the large-scale terrain mapping using aerial images is important for the ground robot to plan a globally optimal path. However, it is a challenging task in a novel and dynamic field without historical human supervision. To alleviate the reliance on human intervention, this article presents a novel framework that integrates active learning and generative adversarial networks (GANs) to effectively exploit small human-labeled data for terrain mapping. In order to model the diverse terrain patterns, this article designs two novel diversity-sensitive GAN models which can capture fine-grained terrain classes among aerial image patches. The proposed approaches are tested in two real-world scenarios using our collaborative air–ground robotic platform. The empirical results show that our methods can outperform their counterparts in the predictive accuracy of terrain classification, visual quality of terrain mapping, and average length of the planned ground path. In practice, the proposed terrain mapping framework is especially valuable when the budget in time or labor cost is very limited.",https://ieeexplore.ieee.org/document/8972606/,IEEE Transactions on Cybernetics,Dec. 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/JAS.2021.1003907,Domain-Invariant Similarity Activation Map Contrastive Learning for Retrieval-Based Long-Term Visual Localization,IEEE,Journals,"Visual localization is a crucial component in the application of mobile robot and autonomous driving. Image retrieval is an efficient and effective technique in image-based localization methods. Due to the drastic variability of environmental conditions, e.g., illumination changes, retrieval-based visual localization is severely affected and becomes a challenging problem. In this work, a general architecture is first formulated probabilistically to extract domain-invariant features through multi-domain image translation. Then, a novel gradient-weighted similarity activation mapping loss (Grad-SAM) is incorporated for finer localization with high accuracy. We also propose a new adaptive triplet loss to boost the contrastive learning of the embedding in a self-supervised manner. The final coarse-to-fine image retrieval pipeline is implemented as the sequential combination of models with and without Grad-SAM loss. Extensive experiments have been conducted to validate the effectiveness of the proposed approach on the CMU-Seasons dataset. The strong generalization ability of our approach is verified with the RobotCar dataset using models pre-trained on urban parts of the CMU-Seasons dataset. Our performance is on par with or even outperforms the state-of-the-art image-based localization baselines in medium or high precision, especially under challenging environments with illumination variance, vegetation, and night-time images. Moreover, real-site experiments have been conducted to validate the efficiency and effectiveness of the coarse-to-fine strategy for localization.",https://ieeexplore.ieee.org/document/9358457/,IEEE/CAA Journal of Automatica Sinica,February 2022,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ACCESS.2022.3190538,Edge Deployment Framework of GuardBot for Optimized Face Mask Recognition With Real-Time Inference Using Deep Learning,IEEE,Journals,"Deep learning based models on the edge devices have received considerable attention as a promising means to handle a variety of AI applications. However, deploying the deep learning models in the production environment with efficient inference on the edge devices is still a challenging task due to computation and memory constraints. This paper proposes a framework for the service robot named GuardBot powered by Jetson Xavier NX and presents a real-world case study of deploying the optimized face mask recognition application with real-time inference on the edge device. It assists the robot to detect whether people are wearing a mask to guard against COVID-19 and gives a polite voice reminder to wear the mask. Our framework contains dual-stage architecture based on convolutional neural networks with three main modules that employ (1) MTCNN for face detection, (2) our proposed CNN model and seven transfer learning based custom models which are Inception-v3, VGG16, denseNet121, resNet50, NASNetMobile, XceptionNet, MobileNet-v2 for face mask classification, (3) TensorRT for optimization of all the models to speedup inference on the Jetson Xavier NX. Our study carries out several analysis based on the models&#x2019; performance in terms of their frames per second, execution time and images per second. It also evaluates the accuracy, precision, recall &amp; F1-score and makes the comparison of all models before and after optimization with a main focus on high throughput and low latency. Finally, the framework is deployed on a mobile robot to perform experiments in both outdoor and multi-floor indoor environments with patrolling and non-patrolling modes. Compared to other state-of-the-art models, our proposed CNN model for face mask recognition based on the classification obtains 94.5&#x0025;, 95.9&#x0025; and 94.28&#x0025; accuracy on training, validation and testing datasets respectively which is better than MobileNet-v2, Xception and InceptionNet-v3 while it achieves highest throughput and lowest latency than all other models after optimization at different precision levels.",https://ieeexplore.ieee.org/document/9837903/,IEEE Access,2022,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/TPAMI.2019.2899570,End-to-End Active Object Tracking and Its Real-World Deployment via Reinforcement Learning,IEEE,Journals,"We study active object tracking, where a tracker takes visual observations (i.e., frame sequences) as input and produces the corresponding camera control signals as output (e.g., move forward, turn left, etc.). Conventional methods tackle tracking and camera control tasks separately, and the resulting system is difficult to tune jointly. These methods also require significant human efforts for image labeling and expensive trial-and-error system tuning in the real world. To address these issues, we propose, in this paper, an end-to-end solution via deep reinforcement learning. A ConvNet-LSTM function approximator is adopted for the direct frame-to-action prediction. We further propose an environment augmentation technique and a customized reward function, which are crucial for successful training. The tracker trained in simulators (ViZDoom and Unreal Engine) demonstrates good generalization behaviors in the case of unseen object moving paths, unseen object appearances, unseen backgrounds, and distracting objects. The system is robust and can restore tracking after occasional lost of the target being tracked. We also find that the tracking ability, obtained solely from simulators, can potentially transfer to real-world scenarios. We demonstrate successful examples of such transfer, via experiments over the VOT dataset and the deployment of a real-world robot using the proposed active tracker trained in simulation.",https://ieeexplore.ieee.org/document/8642452/,IEEE Transactions on Pattern Analysis and Machine Intelligence,1 June 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/TIM.2021.3089240,Fault Diagnosis of Harmonic Drive With Imbalanced Data Using Generative Adversarial Network,IEEE,Journals,"Harmonic drive is the core component of the industrial robot, and its fault diagnosis is crucial to the reliability and performance of the equipment. Most machine learning methods achieve good results based on the assumption of data balance. However, the scarce fault data of harmonic drive is difficult to collect, resulting in various imbalanced health status samples, which has an adverse effect on fault diagnosis. In this article, we propose a data generation method based on generative adversarial networks (GANs) to solve the problem of data imbalance and utilize the multiscale convolutional neural network (MSCNN) to realize the fault diagnosis of the harmonic drive. First, the data collected from three vibration acceleration sensors are preprocessed by fast Fourier transform (FFT) to obtain the frequency spectrum of the vibration signal. Second, multiple GANs were adopted to generate various fault spectrum data and the data selection module (DSM) is elaborately designed to filter and purify these data. Third, the filtered generated data will be combined with the real data to form a balanced dataset, and then the MSCNN is used to achieve multiclassification of the health status of the harmonic drive. Finally, the experiments have been implemented on an industrial robot vibration test bench to validate the effectiveness of our approach. The results have shown the fault multiclassification accuracy as 98.49% under imbalanced fault data conditions, which outperforms that of the other compared methods.",https://ieeexplore.ieee.org/document/9454583/,IEEE Transactions on Instrumentation and Measurement,2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/JPROC.2002.801451,First steps of robotic perception: the turning point of the 1990s,IEEE,Journals,"In this paper we analyze the early evolution of robot perception toward robot autonomy: respective impacts of the bare technology and of advanced control are put in perspective. At first, the UniBuM's vehicle, operational in the 1990s, is taken as a study example. The strong points of such systems suggest a discussion of the software technology in image processing and of the hardware technological concept of silicon retina regarding their respective contribution to real machine vision. This second part elicits limitations, which lead again to concepts and realizations in control, through the need for sensor fusion and active vision. The ultimate step to robot autonomy would be learning, and this is considered in the conclusion.",https://ieeexplore.ieee.org/document/1032796/,Proceedings of the IEEE,July 2002,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/LRA.2022.3145971,Focus on Impact: Indoor Exploration With Intrinsic Motivation,IEEE,Journals,"Exploration of indoor environments has recently experienced a significant interest, also thanks to the introduction of deep neural agents built in a hierarchical fashion and trained with Deep Reinforcement Learning (DRL) on simulated environments. Current state-of-the-art methods employ a dense extrinsic reward that requires the complete a priori knowledge of the layout of the training environment to learn an effective exploration policy. However, such information is expensive to gather in terms of time and resources. In this work, we propose to train the model with a purely intrinsic reward signal to guide exploration, which is based on the impact of the robot’s actions on its internal representation of the environment. So far, impact-based rewards have been employed for simple tasks and in procedurally generated synthetic environments with countable states. Since the number of states observable by the agent in realistic indoor environments is non-countable, we include a neural-based density model and replace the traditional count-based regularization with an estimated pseudo-count of previously visited states. The proposed exploration approach outperforms DRL-based competitors relying on intrinsic rewards and surpasses the agents trained with a dense extrinsic reward computed with the environment layouts. We also show that a robot equipped with the proposed approach seamlessly adapts to point-goal navigation and real-world deployment.",https://ieeexplore.ieee.org/document/9691914/,IEEE Robotics and Automation Letters,April 2022,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/TRO.2021.3106832,From Adaptive Locomotion to Predictive Action Selection &#x2013; Cognitive Control for a Six-Legged Walker,IEEE,Journals,"Locomotion in animals provides a model for adaptive behavior as it is able to deal with various kinds of perturbations. Work in insects suggests that this evolved flexibility results from a modular architecture, which can be characterized by a recurrent neural network allowing for various emerging attractor states. Whereas a lower control-level coordinates joint movements on a short timescale, a higher-level handles action selection on longer timescales. Implementation of such a control system on a walking hexapod robot was able to deal with various walking patterns including disturbances such as uneven terrain or loss of a leg. Here, we propose a cognitive expansion to the adaptive control system that allows dealing with novel challenging situations. This approach makes use of an internal simulation-based planner that is triggered when the model-free controller fails to recover from an unstable pose. Using a grounded internal body model, the planner then tries, in internal simulation, different solutions out of context, and thus, proposes a new plan to be executed on the real robot. We demonstrate the feasibility of this control approach for walking over terrain with uncertain footholds in three scenarios.",https://ieeexplore.ieee.org/document/9543505/,IEEE Transactions on Robotics,April 2022,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/LRA.2019.2955941,Generative Localization With Uncertainty Estimation Through Video-CT Data for Bronchoscopic Biopsy,IEEE,Journals,"Robot-assisted endobronchial intervention requires accurate localization based on both intra- and pre-operative data. Most existing methods achieve this by registering 2D videos with 3D CT models according to a defined similarity metric with local features. Instead, we formulate the bronchoscopic localization as a learning-based global localisation using deep neural networks. The proposed network consists of two generative architectures and one auxiliary learning component. The cycle generative architecture bridges the domain variance between the real bronchoscopic videos and virtual views derived from pre-operative CT data so that the proposed approach can be trained through a large number of generated virtual images but deployed through real images. The auxiliary learning architecture leverages complementary relative pose regression to constrain the search space, ensuring consistent global pose predictions. Most importantly, the uncertainty of each global pose is obtained through variational inference by sampling within the learned underlying probability distribution. Detailed validation results demonstrate the localization accuracy with reasonable uncertainty achieved and its potential clinical value. A demonstration video demo can be found on the website https://youtu.be/ci9LMY49aF8.",https://ieeexplore.ieee.org/document/8913461/,IEEE Robotics and Automation Letters,Jan. 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/TIE.2006.888791,Hardware Implementation of a Real-Time Neural Network Controller With a DSP and an FPGA for Nonlinear Systems,IEEE,Journals,"In this paper, we implement the intelligent neural network controller hardware with a field programmable gate array (FPGA)-based general purpose chip and a digital signal processing (DSP) board to solve nonlinear system control problems. The designed intelligent control hardware can perform real-time control of the backpropagation learning algorithm of a neural network. The basic proportional-integral-derivative (PID) control algorithms are implemented in an FPGA chip and a neural network controller is implemented in a DSP board. By using a high capacity of an FPGA chip, the additional hardware such as an encoder counter and a pulsewidth modulation (PWM) generator is implemented in a single FPGA chip. As a result, the controller becomes cost effective. It was tested for controlling nonlinear systems such as a robot finger and an inverted pendulum on a moving cart to show performance of the controller",https://ieeexplore.ieee.org/document/4084734/,IEEE Transactions on Industrial Electronics,Feb. 2007,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ACCESS.2022.3182803,Hierarchical Landmark Policy Optimization for Visual Indoor Navigation,IEEE,Journals,"In this paper, we study the problem of visual indoor navigation to an object that is defined by its semantic category. Recent works have shown significant achievements in the end-to-end reinforcement learning approach and modular systems. However, both approaches need a big step forward to be robust and practically applicable. To solve the problem of insufficient exploration of the scenes and make exploration more semantically meaningful, we extend standard task formulation and give the agent easily accessible landmarks in the form of the room locations and those types. The availability of landmarks allows the agent to build a hierarchical policy structure and achieve a success rate of 63&#x0025; on validation scenes in a photo-realistic Habitat simulator. In a hierarchy, a low level consists of separately trained RL skills and a high level deterministic policy, which decides which skill is needed at the moment. Also, in this paper, we show the possibility of transferring a trained policy to a real robot. After a bit of training on the reconstructed real scene, the robot shows up to 79&#x0025; SPL when solving the task of navigating to an arbitrary object.",https://ieeexplore.ieee.org/document/9795006/,IEEE Access,2022,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/JSEN.2019.2956901,Human Action Recognition Using Deep Learning Methods on Limited Sensory Data,IEEE,Journals,"In recent years, due to the widespread usage of various sensors action recognition is becoming more popular in many fields such as person surveillance, human-robot interaction etc. In this study, we aimed to develop an action recognition system by using only limited accelerometer and gyroscope data. Several deep learning methods like Convolutional Neural Network(CNN), Long-Short Term Memory (LSTM) with classical machine learning algorithms and their combinations were implemented and a performance analysis was carried out. Data balancing and data augmentation methods were applied and accuracy rates were increased noticeably. We achieved new state-of-the-art result on the UCI HAR dataset by 97.4% accuracy rate with using 3 layer LSTM model. Also, we implemented same model on collected dataset (ETEXWELD) and 99.0% accuracy rate was obtained which means a solid contribution. Moreover, the performance analysis is not only based on accuracy results, but also includes precision, recall and f1-score metrics. Additionally, a real-time application was developed by using 3 layer LSTM network for evaluating how the best model classifies activities robustly.",https://ieeexplore.ieee.org/document/8918509/,IEEE Sensors Journal,"15 March15, 2020",ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ACCESS.2020.3012557,Human Interaction Anticipation by Combining Deep Features and Transformed Optical Flow Components,IEEE,Journals,"The anticipation of ongoing human interactions is not only highly dynamic and challenging problem but extremely crucial in applications such as remote monitoring, video surveillance, human-robot interaction, anti-terrorists and anti-crime securities. In this work, we address the problem of anticipating the interactions between people monitored by single as well as multiple camera views. To this end, we propose a novel approach that integrates Deep Features with novel hand-crafted features, namely Transformed Optical Flow Components (TOFCs). In order to validate the performance of the proposed approach, we have tested the proposed approach in real outdoor environments, captured using single as well as multiple cameras, having shadow and illumination variations as well as cluttered backgrounds. The results of the proposed approach are also compared with the state-of-the-art approaches. The experimental results show that the proposed approach is promising to anticipate real human interactions.",https://ieeexplore.ieee.org/document/9151125/,IEEE Access,2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/TCAD.2021.3072340,INCAME: Interruptible CNN Accelerator for Multirobot Exploration,IEEE,Journals,"Multirobot exploration (MR-Exploration) is a primary task providing the location and map for many multirobot applications. To improve system performance, convolutional neural network (CNN) is introduced by recent researches into critical components in MR-Exploration, such as feature-point extraction (FE) and place recognition (PR). This CNN-based MR-Exploration needs to simultaneously run multiple CNN models and complex postprocessing algorithms. This significantly challenges the hardware platforms of embedded systems. Previous researches reveal that an FPGA is ideal for CNN processing on embedded platforms. Such accelerators usually process different models in sequence, while they cannot schedule multiple tasks at runtime. Furthermore, the postprocessing of CNNs is computationally intensive and becomes the bottleneck of the whole system. To handle such problems, we propose an interruptible CNN accelerator for multirobot exploration (INCAME) framework to rapidly deploy the robot applications on FPGAs. In INCAME, we propose an interrupt method based on virtual instructions to support multitasking on CNN accelerators. INCAME also includes hardware modules for accelerating the postprocessing of the CNN-based components. Organically, it integrates the postprocessing and CNN backbone by sharing memory. Experimental results reveal that INCAME enables multitask scheduling on the CNN accelerator with negligible performance degradation (0.3&#x0025;). INCAME enables embedded FPGAs to perform MR-Exploration in real time (20 fps) via the multitask support and postprocessing acceleration.",https://ieeexplore.ieee.org/document/9399672/,IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems,April 2022,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/TAMD.2011.2106781,Implicit Sensorimotor Mapping of the Peripersonal Space by Gazing and Reaching,IEEE,Journals,"Primates often perform coordinated eye and arm movements, contextually fixating and reaching towards nearby objects. This combination of looking and reaching to the same target is used by infants to establish an implicit visuomotor representation of the peripersonal space, useful for both oculomotor and arm motor control. In this work, taking inspiration from such behavior and from primate visuomotor mechanisms, a shared sensorimotor map of the environment, built on a radial basis function framework, is configured and trained by the coordinated control of eye and arm movements. Computational results confirm that the approach seems especially suitable for the problem at hand, and for its implementation on a real humanoid robot. By exploratory gazing and reaching actions, either free or goal-based, the artificial agent learns to perform direct and inverse transformations between stereo vision, oculomotor, and joint-space representations. The integrated sensorimotor map that allows to contextually represent the peripersonal space through different vision and motor parameters is never made explicit, but rather emerges thanks to the interaction of the agent with the environment.",https://ieeexplore.ieee.org/document/5703113/,IEEE Transactions on Autonomous Mental Development,March 2011,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/LRA.2022.3187843,Learning Deformable Object Manipulation From Expert Demonstrations,IEEE,Journals,"We present a novel Learning from Demonstration (LfD) method, Deformable Manipulation from Demonstrations (DMfD), to solve deformable manipulation tasks using states or images as inputs, given expert demonstrations. Our method uses demonstrations in three different ways, and balances the trade-off between exploring the environment online and using guidance from experts to explore high dimensional spaces effectively. We test DMfD on a set of representative manipulation tasks for a 1-dimensional rope and a 2-dimensional cloth from the SoftGym suite of tasks, each with state and image observations. Our method exceeds baseline performance by up to 12.9&#x0025; for state-based tasks and up to 33.44&#x0025; on image-based tasks, with comparable or better robustness to randomness. Additionally, we create two challenging environments for folding a 2D cloth using image-based observations, and set a performance benchmark for them. We deploy DMfD on a real robot with a minimal loss in normalized performance during real-world execution compared to simulation (<inline-formula><tex-math notation=""LaTeX"">$\sim 6\%$</tex-math></inline-formula>).",https://ieeexplore.ieee.org/document/9813374/,IEEE Robotics and Automation Letters,Oct. 2022,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/LRA.2020.2975706,Learning Task-Oriented Grasping From Human Activity Datasets,IEEE,Journals,"We propose to leverage a real-world, human activity RGB dataset to teach a robot Task-Oriented Grasping (TOG). We develop a model that takes as input an RGB image and outputs a hand pose and configuration as well as an object pose and a shape. We follow the insight that jointly estimating hand and object poses increases accuracy compared to estimating these quantities independently of each other. Given the trained model, we process an RGB dataset to automatically obtain the data to train a TOG model. This model takes as input an object point cloud and outputs a suitable region for task-specific grasping. Our ablation study shows that training an object pose predictor with the hand pose information (and vice versa) is better than training without this information. Furthermore, our results on a real-world dataset show the applicability and competitiveness of our method over state-of-the-art. Experiments with a robot demonstrate that our method can allow a robot to preform TOG on novel objects.",https://ieeexplore.ieee.org/document/9006947/,IEEE Robotics and Automation Letters,April 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/LRA.2022.3145947,Learning to Navigate Sidewalks in Outdoor Environments,IEEE,Journals,"Outdoor navigation on sidewalks in urban environments is the key technology behind important human assistive applications, such as last-mile delivery or neighborhood patrol. This letter aims to develop a quadruped robot that follows a route plan generated by public map services, while remaining on sidewalks and avoiding collisions with obstacles and pedestrians. We devise a two-staged learning framework, which first trains a teacher agent in an abstract world with privileged ground-truth information, and then applies Behavior Cloning to teach the skills to a student agent who only has access to realistic sensors. The main research effort of this letter focuses on overcoming challenges when deploying the student policy on a quadruped robot in the real world. We propose methodologies for designing sensing modalities, network architectures, and training procedures to enable zero-shot policy transfer to unstructured and dynamic real outdoor environments. We evaluate our learning framework on a quadrupedal robot navigating sidewalks in the city of Atlanta, USA (Fig. 1). Using the learned navigation policy and its onboard sensors, the robot is able to walk 3.2 kilometers with a limited number of human interventions.",https://ieeexplore.ieee.org/document/9691818/,IEEE Robotics and Automation Letters,April 2022,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/TVT.2019.2952926,Mechanism Design for Wireless Powered Spatial Crowdsourcing Networks,IEEE,Journals,"Wireless power transfer (WPT) is a promising technology to prolong the lifetime of the sensors and communication devices, i.e., workers, in completing crowdsourcing tasks by providing continuous and cost-effective energy supplies. In this paper, we propose a wireless powered spatial crowdsourcing framework which consists of two mutually dependent phases: task allocation phase and data crowdsourcing phase. In the task allocation phase, we propose a Stackelberg game based mechanism for the spatial crowdsourcing platform to efficiently allocate spatial tasks and wireless charging power to each worker. In the data crowdsourcing phase, the workers may have an incentive to misreport its real working location to improve its utility, which causes adverse effects to the spatial crowdsourcing platform. To address this issue, we present three strategyproof deployment mechanisms for the spatial crowdsourcing platform to place a mobile base station, e.g., vehicle or robot, which is responsible for transferring the wireless power and collecting the crowdsourced data. As the benchmark, we first apply the classical median mechanism and evaluate its worst-case performance. Then, we design a conventional strategyproof deployment mechanism to improve the expected utility of the spatial crowdsourcing platform under the condition that the workers' locations follow a known geographical distribution. For a more general case with only the historical location data available, we propose a deep learning based strategyproof deployment mechanism to maximize the spatial crowdsourcing platform's utility. Extensive experimental results based on synthetic and real-world datasets reveal the effectiveness of the proposed framework in allocating tasks and charging power to workers while avoiding the dishonest worker's manipulation.",https://ieeexplore.ieee.org/document/8895988/,IEEE Transactions on Vehicular Technology,Jan. 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/TITB.2004.840064,Mobile tele-echography: user interface design,IEEE,Journals,"Ultrasound imaging allows the evaluation of the degree of emergency of a patient. However, in some instances, a well-trained sonographer is unavailable to perform such echography. To cope with this issue, the Mobile Tele-Echography Using an Ultralight Robot (OTELO) project aims to develop a fully integrated end-to-end mobile tele-echography system using an ultralight remote-controlled robot for population groups that are not served locally by medical experts. This paper focuses on the user interface of the OTELO system, consisting of the following parts: an ultrasound video transmission system providing real-time images of the scanned area, an audio/video conference to communicate with the paramedical assistant and with the patient, and a virtual-reality environment, providing visual and haptic feedback to the expert, while capturing the expert's hand movements. These movements are reproduced by the robot at the patient site while holding the ultrasound probe against the patient skin. In addition, the user interface includes an image processing facility for enhancing the received images and the possibility to include them into a database.",https://ieeexplore.ieee.org/document/1402446/,IEEE Transactions on Information Technology in Biomedicine,March 2005,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/41.499811,Multilayered fuzzy behavior fusion for real-time reactive control of systems with multiple sensors,IEEE,Journals,"Fuzzy linguistic rules provide an intuitive and powerful means for defining control behavior. Most applications that use fuzzy control feature a single layer of fuzzy inference, mapping a function from one or two inputs to equally few outputs. Highly complex systems, with large numbers of inputs, may also benefit from the use of qualitative linguistic rules if the control task is properly partitioned. This paper presents a modular fuzzy control architecture and inference engine that can be used to control complex systems. The control function is broken down into multiple local agents, each of which samples a subset of a large sensor input space. Additional fuzzy agents are employed to fuse the recommendations of the local agents. Real-time implementation without special hardware is possible by using singleton output values during fuzzy rule evaluation. A development tool is used to translate a fuzzy programming language offline for fast execution at run time. Using this system, a multilayered fuzzy behavior fusion based reactive control system has been implemented on an autonomous mobile robot, MARGE, with great success. MARGE won first place in Event III of the 1993 Robot Competition sponsored by the American Association for Artificial Intelligence.",https://ieeexplore.ieee.org/document/499811/,IEEE Transactions on Industrial Electronics,June 1996,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/LRA.2022.3184779,Neural Scene Representation for Locomotion on Structured Terrain,IEEE,Journals,"We propose a learning-based method to reconstruct the local terrain for locomotion with a mobile robot traversing urban environments. Using a stream of depth measurements from the onboard cameras and the robot&#x2019;s trajectory, the algorithm estimates the topography in the robot&#x2019;s vicinity. The raw measurements from these cameras are noisy and only provide partial and occluded observations that in many cases do not show the terrain the robot stands on. Therefore, we propose a 3D reconstruction model that faithfully reconstructs the scene, despite the noisy measurements and large amounts of missing data coming from the blind spots of the camera arrangement. The model consists of a 4D fully convolutional network on point clouds that learns the geometric priors to complete the scene from the context and an auto-regressive feedback to leverage spatio-temporal consistency and use evidence from the past. The network can be solely trained with synthetic data, and due to extensive augmentation, it is robust in the real world, as shown in the validation on a quadrupedal robot, ANYmal, traversing challenging settings. We run the pipeline on the robot&#x2019;s onboard low-power computer using an efficient sparse tensor implementation and show that the proposed method outperforms classical map representations.",https://ieeexplore.ieee.org/document/9801620/,IEEE Robotics and Automation Letters,Oct. 2022,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ACCESS.2018.2863736,Noise-Resistant Discrete-Time Neural Dynamics for Computing Time-Dependent Lyapunov Equation,IEEE,Journals,"Z-type neural dynamics, which is a powerful calculating tool, is widely used to compute various time-dependent problems. Most Z-type neural dynamics models are usually investigated in a noise-free situation. However, noises will inevitably exist in the implementation process of a neural dynamics model. To deal with such an issue, this paper considers a new discrete-time Z-type neural dynamics model, which is analyzed and investigated to calculate the real-time-dependent Lyapunov equation in the form AT(t)X(t) + X(t)A(t) + C(t) = 0 in different types of noisy circumstances. Related theoretical analyses are provided to illustrate that, the proposed neural dynamics model is intrinsically noise-resistant and has the advantage of high precision in real-time calculation. This model is called the noise-resistant discrete-time Z-type neural dynamics (NRDTZND) model. For comparison, the conventional discrete-time Z-type neural dynamics model is also proposed and used for solving the same time-dependent problem in noisy environments. Finally, three illustrative examples, including a real-life application to the inverse kinematics motion planning of a robot arm, are performed and analyzed to prove the validity and superiority of the proposed NRDTZND model in computing the real-time-dependent Lyapunov equation under various types of noisy situations.",https://ieeexplore.ieee.org/document/8425977/,IEEE Access,2018,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/LRA.2022.3154048,Overhead Image Factors for Underwater Sonar-Based SLAM,IEEE,Journals,"Simultaneous localization and mapping (SLAM) is a critical capability for any autonomous underwater vehicle (AUV). However, robust, accurate state estimation is still a work in progress when using low-cost sensors. We propose enhancing a typical low-cost sensor package using widely available and often free prior information; overhead imagery. Given an AUV&#x2019;s sonar image and a partially overlapping, globally-referenced overhead image, we propose using a convolutional neural network (CNN) to generate a synthetic overhead image predicting the above-surface appearance of the sonar image contents. We then use this synthetic overhead image to register our observations to the provided global overhead image. Once registered, the transformation is introduced as a factor into a pose SLAM factor graph. We use a state-of-the-art simulation environment to perform validation over a series of benchmark trajectories and quantitatively show the improved accuracy of robot state estimation using the proposed approach. We also show qualitative outcomes from a real AUV field deployment. datasets, quantitatively demonstrating its accuracy, stability, and data-efficiency.",https://ieeexplore.ieee.org/document/9721066/,IEEE Robotics and Automation Letters,April 2022,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/JIOT.2019.2955035,Parallel Reinforcement Learning With Minimal Communication Overhead for IoT Environments,IEEE,Journals,"Many Internet of Things (IoT) applications require a distributed architecture for decision making either because of a lack of a centralized system, failure-prone connectivity to a centralized system or because the imposed latency to contact such a system is too high for real-time applications. Often, these IoT applications fall in the domain of reinforcement learning (RL), e.g., autonomous robot navigation in smart factories and traffic signal control in smart cities. However, RL-based applications require a long learning time. To overcome this limitation and scale with the number of agents, parallel RL (PRL) algorithms run multiple RL agents in parallel and on distributed environments. However, deploying PRL algorithms in such environments entails a communication overhead that increases the (actual) execution time. The state-of-the-art PRL algorithms are designed for reducing the learning time while assuming no (or limited) communication overhead. In this article, we present a novel partitioning algorithm that minimizes the communication overhead in PRL running on IoT environments. To the best of our knowledge, this is the first work that focuses on solving the communication overhead of distributing PRL algorithms without requiring any a priori knowledge about the structure of the problem. The proposed algorithm intelligently combines a dynamic state partitioning strategy, which exploits the agent's exploration capabilities to build partition knowledge while learning, with an efficient mapping of agents to partitions, which reduces the communication among agents. Performance evaluations show that the proposed algorithm can achieve almost no communication among PRL agents at the converged state.",https://ieeexplore.ieee.org/document/8911213/,IEEE Internet of Things Journal,Feb. 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/LRA.2022.3191205,Predicting to Improve: Integrity Measures for Assessing Visual Localization Performance,IEEE,Journals,"Visual Place Recognition (VPR) is a key component of many robot localization and mapping system processing pipelines, providing loop closure and coarse topological localization priors for pose refinement stages. When deploying these systems in the real-world, system self-characterization of when it is performing well or poorly can be more important than absolute performance. In this research, we demonstrate a new supervised learning approach to predicting localization integrity on a frame-by-frame basis along a route, using artefacts from the localization technique itself. Our method involves a lightweight post-processing step that is VPR technique-agnostic; it can be calibrated for any given place recognition technique, such that inaccurate localization points along a route can be identified and discarded. Unlike the normal parameter sweeping evaluation of a VPR system, which directly trades precision for recall, when deployed on a baseline VPR system our approach improves the precision without significantly reducing recall, resulting in improved average localization performance. Over twelve combinations of varied datasets and VPR techniques, we demonstrate our approach is able to predict localization errors with an average precision of 74&#x0025;, resulting in an improvement in mean localization accuracy.",https://ieeexplore.ieee.org/document/9830823/,IEEE Robotics and Automation Letters,Oct. 2022,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/TIP.2022.3162709,Progressive Glass Segmentation,IEEE,Journals,"Glass is very common in the real world. Influenced by the uncertainty about the glass region and the varying complex scenes behind the glass, the existence of glass poses severe challenges to many computer vision tasks, making glass segmentation as an important computer vision task. Glass does not have its own visual appearances but only transmit/reflect the appearances of its surroundings, making it fundamentally different from other common objects. To address such a challenging task, existing methods typically explore and combine useful cues from different levels of features in the deep network. As there exists a characteristic gap between level-different features, <i>i.e.</i>, deep layer features embed more high-level semantics and are better at locating the target objects while shallow layer features have larger spatial sizes and keep richer and more detailed low-level information, fusing these features naively thus would lead to a sub-optimal solution. In this paper, we approach the effective features fusion towards accurate glass segmentation in two steps. First, we attempt to bridge the characteristic gap between different levels of features by developing a Discriminability Enhancement (DE) module which enables level-specific features to be a more discriminative representation, alleviating the features incompatibility for fusion. Second, we design a Focus-and-Exploration Based Fusion (FEBF) module to richly excavate useful information in the fusion process by highlighting the common and exploring the difference between level-different features. Combining these two steps, we construct a <b>P</b>rogressive <b>G</b>lass <b>S</b>egmentation <b>Net</b>work (<b>PGSNet</b>) which uses multiple DE and FEBF modules to progressively aggregate features from high-level to low-level, implementing a coarse-to-fine glass segmentation. In addition, we build the first home-scene-oriented glass segmentation dataset for advancing household robot applications and in-depth research on this topic. Extensive experiments demonstrate that our method outperforms 26 cutting-edge models on three challenging datasets under four standard metrics. The code and dataset will be made publicly available.",https://ieeexplore.ieee.org/document/9748016/,IEEE Transactions on Image Processing,2022,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/LRA.2022.3141150,REVE-CE: Remote Embodied Visual Referring Expression in Continuous Environment,IEEE,Journals,"Ithas always been a great challenge for the robot to navigate in the visual world following natural language instructions. Recently, several tasks such as the Vision-and-Language Navigation (VLN) and Remote Embodied Visual Referring Expression in Real Indoor Environments (REVERIE) are proposed trying to solve this challenge. And the most significant difference between VLN and REVERIE tasks is that REVERIE uses a higher guidance level instruction. However, the navigation process of REVERIE is implemented in a discrete environment, which is unrealistic in real world scenarios. To make the REVERIE task more consistent with the real physical world, we develop a new task of Remote Embodied Visual Referring Expression in Continuous Environment, namely REVE-CE, in which the agent executes a much longer sequence of low-level actions given language instructions. Furthermore, we propose a multi-branch cross modal attention (MBCMA) framework to solve the proposed REVE-CE task. Extensive experiments are conducted demonstrating that the proposed framework greatly outperforms the state-of-the-art VLN baselines and a new benchmark for the proposed REVE-CE task is built.",https://ieeexplore.ieee.org/document/9674225/,IEEE Robotics and Automation Letters,April 2022,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ACCESS.2022.3152199,RFID Gazebo-Based Simulator With RSSI and Phase Signals for UHF Tags Localization and Tracking,IEEE,Journals,"Radio Frequency Identification (RFID) technology is becoming very popular in the new era of Industry 4.0, especially for warehouse management, retails, and logistics. RFID systems can be used for objects identification, localization, and tracking, facilitating everyday operators&#x2019; efforts. However, the deployment of RFID tags and reader antennas in real-world application scenarios is crucial and takes time. Indeed, deciding where to place tags and/or readers&#x2019; requires examining many conditions. If some weaknesses appear in the design, the arrangement must be reconsidered. The proposed work presents a novel open-source RFID simulator that allows modeling environments and testing the deployment of RFID tags and antennas apriori. In such a way, validating the performance of the localization or tracking algorithms in simulation, possible weaknesses that could arise may be fixed before facilities are applied on the field. Any number of tags and antennas can be placed in any position in the created scenario, and the simulator provides the phase and the RSSI signals for each tag. Every reader antenna is parametrized so that different antennas of different vendors can be reproduced. The simulator is implemented as a plugin of Gazebo, a widely used robotic framework integrated with the Robot Operating System (ROS), to reach a broad audience. In order to validate the simulator, a warehouse scenario is modeled, and a tag localization algorithm that uses the phase unwrapping technique and hyperbolae intersection method employing a reader antenna mounted on a mobile robot is used to estimate the position of the tags deployed in the scenario. The outcomes of the experiments showed realistic results.",https://ieeexplore.ieee.org/document/9715113/,IEEE Access,2022,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/LRA.2018.2798286,Real-Time 3-D Shape Instantiation From Single Fluoroscopy Projection for Fenestrated Stent Graft Deployment,IEEE,Journals,"Robot-assisted deployment of fenestrated stent grafts in fenestrated endovascular aortic repair (FEVAR) requires accurate geometrical alignment. Currently, this process is guided by two-dimensional (2-D) fluoroscopy, which is insufficiently informative and error prone. In this letter, a real-time framework is proposed to instantiate the 3-D shape of a fenestrated stent graft by utilizing only a single low-dose 2-D fluoroscopic image. First, markers were placed on the fenestrated stent graft. Second, the 3-D pose of each stent segment was instantiated by the robust perspective-n-point method. Third, the 3-D shape of the whole stent graft was instantiated via graft gap interpolation. Focal UNet was proposed to segment the markers from 2-D fluoroscopic images to achieve semiautomatic marker detection. The proposed framework was validated on five patient-specific 3-D printed aortic aneurysm phantoms and three stent grafts with new marker placements, showing an average distance error of 1-3 mm and an average angular error of 4°. Shape instantiation codes are available online.",https://ieeexplore.ieee.org/document/8269290/,IEEE Robotics and Automation Letters,April 2018,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/TDSC.2019.2903049,Real-Time Error Detection in Nonlinear Control Systems Using Machine Learning Assisted State-Space Encoding,IEEE,Journals,"Successful deployment of autonomous systems in a wide range of societal applications depends on error-free operation of the underlying signal processing and control functions. Real-time error detection in nonlinear systems has mostly relied on redundancy at the component or algorithmic level causing expensive area and power overheads. This paper describes a real-time error detection methodology for nonlinear control systems for detecting sensor and actuator degradations as well as malfunctions due to soft errors in the execution of the control algorithm on a digital processor. Our approach is based on creation of a redundant check state in such a way that its value can be computed from the current states of the system as well as from a history of prior observable state values and inputs (via machine learning algorithms). By checking for consistency between the two, errors are detected with low latency. The method is demonstrated on two test case simulations - an inverted pendulum balancing problem and a sliding mode controller driven brake-by-wire (BBW) system. In addition, hardware results from error injection experiments in an ARM core representation on an FPGA and artificial sensor degradations on a self-balancing robot prove the practical feasibility of implementation.",https://ieeexplore.ieee.org/document/8658148/,IEEE Transactions on Dependable and Secure Computing,1 March-April 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/JPHOT.2022.3147844,Real-Time Optical-Wireless Video Surveillance System for High Visual-Fidelity Underwater Monitoring,IEEE,Journals,"Real-time, dynamic, and visual monitoring of underwater scenes would become an important research topic with the development of underwater wireless optical communication (UWOC), autonomous underwater vehicle, and video fusion technologies. To this end, we developed a UWOC-based 2K real-time digital video surveillance prototype named AquaF-seer. Using a light-emitting diode and an avalanche photodetector based diffused line-of-sight UWOC system, real-time video transmission with a high resolution of 1920 × 1080 pixels is achieved over various channels, i.e., 1.5-m free space channel, pure water channel, pure water channel with 1.53-mL/s bubble-induced turbulence, pure water channel with 42.40-mL/s bubble-induced turbulence, simulated pure sea water channel, and coastal ocean water channel. Moreover, 46-m and 5-m video monitoring are implemented in free space and an outdoor diving pool. It indicates the reliability of the prototype, which is the first step to realize underwater visual monitoring in future human-robot interaction applications.",https://ieeexplore.ieee.org/document/9706307/,IEEE Photonics Journal,April 2022,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/LRA.2021.3063992,Real-Time Path Planning With Virtual Magnetic Fields,IEEE,Journals,"Humans and animals have learned or evolved to use magnetic fields for navigation. Knowing how to model and estimate these fields can be used for motion planning. However, computing the propagation of electromagnetic fields in a given environment requires solving complex differential equations with advanced numerical methods, and therefore it is not suitable for real-time decision making. In this latter, we present a real-time approximator for Maxwell's equations based on deep neural networks that predicts the distribution of a virtual magnetic field. We show how our approximator can be used to perform autonomous 2D navigation tasks, outperforming state-of-the-art navigation algorithms, ensuring completeness, and providing a near-optimal path up to 200 times per second without any post processing stage. We demonstrate the effectiveness of our method with physics-based simulations of an unmanned aerial vehicle, an autonomous car, as well as real-world experiments using a small off-road autonomous racing vehicle. Furthermore, we show how the approach can be applied to multi-robot systems, video game technology, and can be extended to 3D problems.",https://ieeexplore.ieee.org/document/9369851/,IEEE Robotics and Automation Letters,April 2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/LRA.2022.3189446,Retro-RL: Reinforcing Nominal Controller With Deep Reinforcement Learning for Tilting-Rotor Drones,IEEE,Journals,"Studies that broaden drone applications into complex tasks require a stable control framework. Recently, deep reinforcement learning (RL) algorithms have been exploited in many studies for robot control to accomplish complex tasks. Unfortunately, deep RL algorithms might not be suitable for being deployed directly into a real-world robot platform due to the difficulty in interpreting the learned policy and lack of stability guarantee, especially for a complex task such as a wall-climbing drone. This letter proposes a novel hybrid architecture that reinforces a nominal controller with a robust policy learned using a model-free deep RL algorithm. The proposed architecture employs an uncertainty-aware control mixer to preserve guaranteed stability of a nominal controller while using the extended robust performance of the learned policy. The policy is trained in a simulated environment with thousands of domain randomizations to achieve robust performance over diverse uncertainties. The performance of the proposed method was verified through real-world experiments and then compared with a conventional controller and the state-of-the-art learning-based controller trained with a vanilla deep RL algorithm.",https://ieeexplore.ieee.org/document/9822205/,IEEE Robotics and Automation Letters,Oct. 2022,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ACCESS.2021.3061634,RnR: Retrieval and Reprojection Learning Model for Camera Localization,IEEE,Journals,"Camera localization is an essential technique in many applications, such as robot navigation, mixed reality, and unmanned vehicle. We are committed to solving the problem of predicting the 6-DoF pose of cameras from a single color image in a given three-Dimensional (3D) environment. In this paper, we proposed a robust learning model for it. Basically, our proposed methodology consists of two steps: image retrieval and space reprojection. The former is in charge of simultaneous localization and mapping based on pre-captured reference images that rely on the correspondence between pixel points and scene coordinates; whereas the latter carries out camera calibration between the 2D image plane and the 3D scene. Given a two-Dimensional (2D) image, the initial localization is accomplished rapidly by matching a reference image using Siamese networks. More precise localization is achieved by camera calibration between the 2D image and the 3D scene using a fully convolutional network. The experimental results on the public dataset show that our model is more robust and expandable than the previous methods. At the end of this paper, we also apply the system to Unmanned Aerial Vehicle (UAV) localization and achieve good results.",https://ieeexplore.ieee.org/document/9361658/,IEEE Access,2021,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/PROC.1983.12684,The Stanford Cart and the CMU Rover,IEEE,Journals,"The Stanford Cart was a remotely controlled TV-equipped mobile robot. A computer program was written which drove the Cart through cluttered spaces, gaining its knowledge of the world entirely from images broadcast by an on-board TV system. The CMU Rover is a more capable, and neatly operational, robot being built to develop and extend the Stanford work and to explore new directions. The Cart used several kinds of stereopsis to locate objects around it in three dimensions and to deduce its own motion. It planned an obstacle-avoiding path to a desired destination on the basis of a model built with this information. The plan changed as the Cart perceived new obstacles on its journey. The system was reliable for short runs, but slow. The Cart moved 1 m every 10 to 15 min, in lurches. After rolling a meter it stopped, took some pictures, and thought about them for a long time. Then it planned a new path, executed a little of it, and paused again. It successfully drove the Cart through several 20-m courses (each taking about 5 h) complex enough to necessitate three or four avoiding swerves; it failed in other trials in revealing ways. The Rover system has been designed with maximum mechanical and control system flexibility to support a wide range of research in perception and control. It features an omnidirectional steering system, a dozen on-board processors for essential real-time tasks, and a large remote computer to be helped by a high-speed digitizing/data playback unit and a high-performance array processor. Distributed high-level control software similar in organization to the Hearsay II speech-understanding system and the beginnings of a vision library are being readied. By analogy with the evolution of natural intelligence, we believe that incrementally solving the control and perception problems of an autonomous mobile mechanism is one of the best ways of arriving at general artificial intelligence.",https://ieeexplore.ieee.org/document/1456952/,Proceedings of the IEEE,July 1983,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/TII.2019.2954956,Toward New Retail: A Benchmark Dataset for Smart Unmanned Vending Machines,IEEE,Journals,"Deep learning is a popular direction in computer vision and digital image processing. It is widely utilized in many fields, such as robot navigation, intelligent video surveillance, industrial inspection, and aerospace. With the extensive use of deep learning techniques, classification and object detection algorithms have been rapidly developed. In recent years, with the introduction of the concept of &#x201C;unmanned retail,&#x201D; object detection, and image classification play a central role in unmanned retail applications. However, open-source datasets of traditional classification and object detection have not yet been optimized for application scenarios of unmanned retail. Currently, classification and object detection datasets do not exist that focus on unmanned retail solely. Therefore, in order to promote unmanned retail applications by using deep learning-based classification and object detection, in this article we collected more than 30&#x2009;000 images of unmanned retail containers using a refrigerator affixed with different cameras under both static and dynamic recognition environments. These images were categorized into ten kinds of beverages. After manual labeling, images in our constructed dataset contained 155&#x2009;153 instances, each of which was annotated with a bounding box. We performed extensive experiments on this dataset using ten state-of-the-art deep learning-based models. Experimental results indicate great potential of using these deep learning-based models for real-world smart unmanned vending machines.",https://ieeexplore.ieee.org/document/8908822/,IEEE Transactions on Industrial Informatics,Dec. 2020,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/TMECH.2021.3079409,Valve Detection for Autonomous Water Pipeline Inspection Platform,IEEE,Journals,"Water distribution and transmission lines are indispensable to urban infrastructure. The water pipelines are subject to both structural and functional deterioration due to various reasons including aging, negligence, and high demand for water supply. Hence, to ensure a safe and reliable water supply, the water utilities need to perform routine pipe condition assessment. The condition assessment is usually carried out by visual inspection with the machine vision system carried by a robotic platform. The inspection platforms will capture the internal condition of the water pipelines in a video stream. However, the robotic platform frequently experiences difficulties while traversing through the valves installed along the pipeline. This inhibits and disrupts the inspection process of the water pipelines. Therefore, this article proposes a deep learning-based automatic valve detection framework to facilitate the robot&#x2019;s navigation and ensure continuous inspection without any interruptions. The valve detection model is developed by combining MobileNet-160 and Feature Pyramid Network and is named as MFPN. The developed framework also employs a generative adversarial network to solve the sparse dataset issues and improve the generality of the framework. The comparative study and ablation analyses demonstrate that the proposed framework can achieve a higher mAP value of 89.11&#x0025; in comparison with the state of the art. Hence, this light weight and efficient solution can be deployed to the robotic platform for real-time valve detection and enable autonomous navigation of the robotic platform for condition assessment of water pipelines.",https://ieeexplore.ieee.org/document/9429956/,IEEE/ASME Transactions on Mechatronics,April 2022,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/70.508435,Virtual-reality-based point-and-direct robotic inspection in manufacturing,IEEE,Journals,"This paper explores a flexible manufacturing paradigm in which robot grasping is interactively specified and skeletal images are efficiently used in combination to allow rapidly setting up surface flaw identification tasks in small-quantity/large-variety manufacturing. Two complementary technologies are combined to make implementation of inspection as rapid as possible. First, a novel material handling approach is described for robotic picking and placing of parts onto an inspection table using virtual tools. This allows an operator to point and give directives to set up robotic inspection tasks. Second, since specification may be approximate using this method, a fast and flexible means of identifying images of perfect and flawed parts is explored that avoids rotational or translational restrictions on workpiece placement. This is accomplished by using skeleton pixel counts as neural network inputs. The total system, including material handling and skeleton-based inspection, features flexibility during manufacturing set-up, and reduces the process time and memory requirements for workpiece inspection.",https://ieeexplore.ieee.org/document/508435/,IEEE Transactions on Robotics and Automation,Aug. 1996,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
