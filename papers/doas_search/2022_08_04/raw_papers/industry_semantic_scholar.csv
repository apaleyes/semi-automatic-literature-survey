paperId,url,title,abstract,venue,year,externalIds.DOI,database,query_name,query_value
0034de50716005e8bcf857e49b30b5d435a4b0bb,https://www.semanticscholar.org/paper/0034de50716005e8bcf857e49b30b5d435a4b0bb,Machine Learning Techniques for Network-based Intrusion Detection System: A Survey Paper,"The rapid growth of Internet technologies and further dependence on online services, increase the demand for keeping these networks and data secure. The protection of online information is becoming even more vital to the national security and economic stability. Recently, network security has become one of the most concerning subjects in the current research and industry fields. Intrusion Detection Systems (IDSs) are considered as the backbone for network and data protection. Throughout time, different IDS approaches have been implemented to attain maximum detection accuracy. Machine learning IDS is one of the promising IDS techniques that have been created to detect known as well as unknown attacks. This paper investigates various machine learning techniques used to deploy Network-based Intrusion Detection System (NIDS). This survey could provide a more robust understanding of the existing techniques and assists intrigued researchers to identify research opportunities and investigate more in this direction.",2021 National Computing Colleges Conference (NCCC),2021,10.1109/NCCC49330.2021.9428827,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
1698257462714eb2183e7ecb2259514a827290bf,https://www.semanticscholar.org/paper/1698257462714eb2183e7ecb2259514a827290bf,Detecting Anomalous Transactions via an IoT Based Application: A Machine Learning Approach for Horse Racing Betting †,"During the past decade, the technological advancement have allowed the gambling industry worldwide to deploy various platforms such as the web and mobile applications. Government agencies and local authorities have placed strict regulations regarding the location and amount allowed for gambling. These efforts are made to prevent gambling addictions and monitor fraudulent activities. The revenue earned from gambling provides a considerable amount of tax revenue. The inception of internet gambling have allowed professional gamblers to par take in unlawful acts. However, the lack of studies on the technical inspections and systems to prohibit unlawful internet gambling has caused incidents such as the Walkerhill Hotel incident in 2016, where fraudsters placed bets abnormally by modifying an Internet of Things (IoT)-based application called “MyCard”. This paper investigates the logic used by smartphone IoT applications to validate the location of users and then confirm continuous threats. Hence, our research analyzed transactions made on applications that operated using location authentication through IoT devices. Drawing on gambling transaction data from the Korea Racing Authority, this research used time series machine learning algorithms to identify anomalous activities and transactions. In our research, we propose a method to detect and prevent these anomalies by conducting a comparative analysis of the results of existing anomaly detection techniques and novel techniques.",Sensors,2021,10.3390/s21062039,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
f9879bbca1f2d46d63a01cfbb951195b6cdadee6,https://www.semanticscholar.org/paper/f9879bbca1f2d46d63a01cfbb951195b6cdadee6,Improving Ambulance Dispatching with Machine Learning and Simulation,,ECML/PKDD,2021,10.1007/978-3-030-86514-6_19,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
ec0e5e45759e88bb8aa795ec2047608bf8484795,https://www.semanticscholar.org/paper/ec0e5e45759e88bb8aa795ec2047608bf8484795,MLHat: Deployable Machine Learning for Security Defense,"The MLHat workshop aims to bring together academic researchers and industry practitioners to discuss the open challenges, potential solutions, and best practices to deploy machine learning at scale for security defense. The workshop will discuss related topics from both defender perspectives (white-hat) and the attacker perspectives (black-hat). We call the workshop MLHats, to serve as a place for people who are interested in using machine learning to solve practical security problems. The workshop will focus on defining new machine learning paradigms under various security application contexts and identifying exciting new future research directions. At the same time, the workshop will also have a strong industry presence to provide insights into the challenges in deploying and maintaining machine learning models and the much-needed discussion on the capabilities that the state-of-the-arts failed to provide.",KDD,2021,10.1145/3447548.3469463,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
5b320308332d871f2bf7c02ab3b9f879edd8c7fc,https://www.semanticscholar.org/paper/5b320308332d871f2bf7c02ab3b9f879edd8c7fc,Improving Ambulance Dispatching with Machine Learning and Simulation,". As an industry where performance improvements can save lives, but resources are often scarce, emergency medical services (EMS) providers continuously look for ways to deploy available resources more eﬃciently. In this paper, we report a case study executed at a Dutch EMS region to improve ambulance dispatching. We ﬁrst capture the way in which dispatch human agents currently make decisions on which ambulance to dispatch to a request. We build a decision tree based on historical data to learn human agents’ dispatch decisions. Then, insights from the ﬁtted decision tree are used to enrich the commonly assumed closest-idle dispatch policy. Subsequently, we use the captured dispatch policy as input to a discrete event simulation to investigate two enhancements to current practices and evaluate their performance relative to the current policy. Our results show that complementing the current dispatch policy with redispatching and reevaluation policies yields an improvement of the on-time performance of highly urgent ambulance requests of 0.77 percentage points. The performance gain is signiﬁcant, which is equivalent to adding additional seven weekly ambulance shifts.",,2021,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
f5792d3c5e53f406b1aee6aae4505792b23bfcc0,https://www.semanticscholar.org/paper/f5792d3c5e53f406b1aee6aae4505792b23bfcc0,Dynamic Placement of Rapidly Deployable Mobile Sensor Robots Using Machine Learning and Expected Value of Information,"
 Although the Industrial Internet of Things has increased the number of sensors permanently installed in industrial plants, there will be gaps in coverage due to broken sensors or sparse density in very large plants, such as in the petrochemical industry. Modern emergency response operations are beginning to use Small Unmanned Aerial Systems (sUAS) that have the ability to drop sensor robots to precise locations. sUAS can provide longer-term persistent monitoring that aerial drones are unable to provide. Despite the relatively low cost of these assets, the choice of which robotic sensing systems to deploy to which part of an industrial process in a complex plant environment during emergency response remains challenging.
 This paper describes a framework for optimizing the deployment of emergency sensors as a preliminary step towards realizing the responsiveness of robots in disaster circumstances. AI techniques (Long short-term memory, 1-dimensional convolutional neural network, logistic regression, and random forest) identify regions where sensors would be most valued without requiring humans to enter the potentially dangerous area. In the case study described, the cost function for optimization considers costs of false-positive and false-negative errors. Decisions on mitigation include implementing repairs or shutting down the plant. The Expected Value of Information (EVI) is used to identify the most valuable type and location of physical sensors to be deployed to increase the decision-analytic value of a sensor network. This method is applied to a case study using the Tennessee Eastman process data set of a chemical plant, and we discuss implications of our findings for operation, distribution, and decision-making of sensors in plant emergency and resilience scenarios.","Volume 13: Safety Engineering, Risk, and Reliability Analysis; Research Posters",2021,10.1115/imece2021-70759,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
6a29fae129632f40acf6cbed76645a054f6979a9,https://www.semanticscholar.org/paper/6a29fae129632f40acf6cbed76645a054f6979a9,Politics of Adversarial Machine Learning,"In addition to their security properties, adversarial machine-learning attacks and defenses have political dimensions. They enable or foreclose certain options for both the subjects of the machine learning systems and for those who deploy them, creating risks for civil liberties and human rights. In this paper, we draw on insights from science and technology studies, anthropology, and human rights literature, to inform how defenses against adversarial attacks can be used to suppress dissent and limit attempts to investigate machine learning systems. To make this concrete, we use real-world examples of how attacks such as perturbation, model inversion, or membership inference can be used for socially desirable ends. Although the predictions of this analysis may seem dire, there is hope. Efforts to address human rights concerns in the commercial spyware industry provide guidance for similar measures to ensure ML systems serve democratic, not authoritarian ends.",SSRN Electronic Journal,2020,10.2139/SSRN.3547322,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
42d6c00260ceb1f1e48ce9b24ed079f0b2a2d098,https://www.semanticscholar.org/paper/42d6c00260ceb1f1e48ce9b24ed079f0b2a2d098,Cognitive Service Virtualisation: A New Machine Learning-Based Virtualisation to Generate Numeric Values,"Continuous delivery has gained increased popularity in industry as a development approach to develop, test, and deploy enhancements to software components in short development cycles. In order for continuous delivery to be effectively adopted, the services that a component depends upon must be readily available to software engineers in order to systematically apply quality assurance techniques. However, this may not always be possible as (i) these requisite services may have limited access and (ii) defects that are introduced in a component under development may cause ripple effects in real deployment environments. Service virtualisation (SV) has been introduced as an approach to address these challenges, but existing approaches to SV still fall short of delivering the required accuracy and/or ease-of-use to virtualise services for adoption in continuous delivery. In this work, we propose a novel machine learning based approach to predict numeric fields in virtualised responses, extending existing research that has provided a way to produce values for categorical fields. The SV approach introduced here uses machine learning techniques to derive values of numeric fields that are based on a variable number of pertinent historic messages. Our empirical evaluation demonstrates that the Cognitive SV approach can produce responses with the appropriate fields and accurately predict values of numeric fields across three data sets, some of them based on stateful protocols.",Sensors,2020,10.3390/s20195664,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
4696c84f6d4a5188eafabd3aebc002abd8419262,https://www.semanticscholar.org/paper/4696c84f6d4a5188eafabd3aebc002abd8419262,A Cloud-based Analytics Architecture for the Application of Online Machine Learning Algorithms on Data Streams in Consumer-centric Internet of Things Domains,"The increasing number of smart devices in private households has lead to a large quantity of smart homes worldwide. In order to gain meaningful insights into their generated data and offer extended information and added value for consumers, data analytics architectures are essential. In addition, the development and improvement of machine learning techniques and algorithms in the past years has lead to the availability of powerful analytics tools, which have the potential to allow even more sophisticated insights at the cost of changed challenges and requierements for analytics architectures. However, architectural solutions, which offer the ability to deploy flexible, machine learning-based analytics pipelines on streaming data, are missing in research as well as in industry. In this paper, we present the motivation and a concept for machine learningbased data processing on streaming data for consumer-centric Internet of Things domains, such as smart home. This approach was evaluated in terms of its performance and may serve as a basis for further development and",IoTBDS,2020,10.5220/0009339501890196,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
64d64ea38f04e704131ecf9f1b6736bb3204cacb,https://www.semanticscholar.org/paper/64d64ea38f04e704131ecf9f1b6736bb3204cacb,Impacts of Machine Learning on Counterfeit IC Detection and Avoidance Techniques,"Globalization of integrated circuit (IC) supply chain has made counterfeiting a major source of concern in the semiconductor industry. To address this concern, extensive efforts have been put into developing effective counterfeit detection and avoidance techniques. In the recent years, machine learning (ML) algorithms have played an important role in development and evaluation of many emerging countermeasures against counterfeiting. In this paper, we aim to investigate impacts of such algorithms on the landscape of anti-counterfeiting schemes. We provide a comprehensive review of prior arts that deploy machine learning to develop or attack counterfeit detection and avoidance techniques. We also discuss future directions for application of machine learning in anti-counterfeit schemes.",2020 21st International Symposium on Quality Electronic Design (ISQED),2020,10.1109/ISQED48828.2020.9136972,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
e3ab76d3d8354e550232f4be5d5a2b7501966579,https://www.semanticscholar.org/paper/e3ab76d3d8354e550232f4be5d5a2b7501966579,Machine learning based anomaly-based intrusion detection system in a full digital substation,"The cyberattacks that occurred in recent years have raised concerns in critical infrastructures, including power system networks. Identifying ongoing attacks is essential to enable the energy industry to respond to adversaries. Many commercial products and research projects include machine learning based intrusion detection systems but there is still a need for understanding the data training requirements for those systems in order to successfully deploy them to protect power systems. This paper presents the development of an anomaly-based Intrusion Detection System (IDS) based on a machine learning methodology to create a whitelist. The system was implemented using GNU Octave. It was trained using traffic flow from real devices generated from a Virtual Site Acceptance Testing and Training (VSATT) platform where multi-vendor secondary devices were set up and communicated to each other. The system was then tested using different datasets which were also generated from the VSATT platform. Results show that the implemented IDS performed correctly under different case studies. The results also indicate that the learned traffic identifies GOOSE and MMS messages based on the normal behaviours from those protocols, but the presence of other messages might require manual inputs to be incorporated in the training dataset.",,2020,10.1049/cp.2020.0049,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
1d3539a8d94bd3ab78993d7cc584efc06ed0e460,https://www.semanticscholar.org/paper/1d3539a8d94bd3ab78993d7cc584efc06ed0e460,Synthetic Benchmarks for Scientific Research in Explainable Machine Learning,"As machine learning models grow more complex and their applications become more high-stakes, tools for explaining model predictions have become increasingly important. This has spurred a flurry of research in model explainability and has given rise to feature attribution methods such as LIME and SHAP. Despite their widespread use, evaluating and comparing different feature attribution methods remains challenging: evaluations ideally require human studies, and empirical evaluation metrics are often data-intensive or computationally prohibitive on realworld datasets. In this work, we address this issue by releasing XAI-BENCH: a suite of synthetic datasets along with a library for benchmarking feature attribution algorithms. Unlike real-world datasets, synthetic datasets allow the efficient computation of conditional expected values that are needed to evaluate groundtruth Shapley values and other metrics. The synthetic datasets we release offer a wide variety of parameters that can be configured to simulate real-world data. We demonstrate the power of our library by benchmarking popular explainability techniques across several evaluation metrics and across a variety of settings. The versatility and efficiency of our library will help researchers bring their explainability methods from development to deployment. Our code is available at https://github.com/abacusai/xai-bench.",NeurIPS Datasets and Benchmarks,2021,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
5beff11072de25e9be72bd02acc2215a254566e4,https://www.semanticscholar.org/paper/5beff11072de25e9be72bd02acc2215a254566e4,A Blockchain and Machine Learning-Based Drug Supply Chain Management and Recommendation System for Smart Pharmaceutical Industry,"From the last decade, pharmaceutical companies are facing difficulties in tracking their products during the supply chain process, allowing the counterfeiters to add their fake medicines into the market. Counterfeit drugs are analyzed as a very big challenge for the pharmaceutical industry worldwide. As indicated by the statistics, yearly business loss of around $200 billion is reported by US pharmaceutical companies due to these counterfeit drugs. These drugs may not help the patients to recover the disease but have many other dangerous side effects. According to the World Health Organization (WHO) survey report, in under-developed countries every 10th drug use by the consumers is counterfeit and has low quality. Hence, a system that can trace and track drug delivery at every phase is needed to solve the counterfeiting problem. The blockchain has the full potential to handle and track the supply chain process very efficiently. In this paper, we have proposed and implemented a novel blockchain and machine learning-based drug supply chain management and recommendation system (DSCMR). Our proposed system consists of two main modules: blockchain-based drug supply chain management and machine learning-based drug recommendation system for consumers. In the first module, the drug supply chain management system is deployed using Hyperledger fabrics which is capable of continuously monitor and track the drug delivery process in the smart pharmaceutical industry. On the other hand, the N-gram, LightGBM models are used in the machine learning module to recommend the top-rated or best medicines to the customers of the pharmaceutical industry. These models have trained on well known publicly available drug reviews dataset provided by the UCI: an open-source machine learning repository. Moreover, the machine learning module is integrated with this blockchain system with the help of the REST API. Finally, we also perform several tests to check the efficiency and usability of our proposed system.",Electronics,2020,10.3390/electronics9050852,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
3eb594bdc7057858a7bcd6243947c1944e89e2e3,https://www.semanticscholar.org/paper/3eb594bdc7057858a7bcd6243947c1944e89e2e3,Adversarial Machine Learning - Industry Perspectives,"Based on interviews with 28 organizations, we found that industry practitioners are not equipped with tactical and strategic tools to protect, detect and respond to attacks on their Machine Learning (ML) systems. We leverage the insights from the interviews and we enumerate the gaps in perspective in securing machine learning systems when viewed in the context of traditional software security development. We write this paper from the perspective of two personas: developers/ML engineers and security incident responders who are tasked with securing ML systems as they are designed, developed and deployed ML systems. The goal of this paper is to engage researchers to revise and amend the Security Development Lifecycle for industrial-grade software in the adversarial ML era.",SSRN Electronic Journal,2020,10.2139/ssrn.3532474,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
24da156dc92b1f5961f12c54c70df1455e1dc84c,https://www.semanticscholar.org/paper/24da156dc92b1f5961f12c54c70df1455e1dc84c,Identification and classification of materials using machine vision and machine learning in the context of industry 4.0,,J. Intell. Manuf.,2020,10.1007/s10845-019-01508-6,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
38be58a9f51ed2af7e9e75793c38529de8aa5267,https://www.semanticscholar.org/paper/38be58a9f51ed2af7e9e75793c38529de8aa5267,A Systematic Review on the Application of Machine Learning in Exploiting Mineralogical Data in Mining and Mineral Industry,"Machine learning is a subcategory of artificial intelligence, which aims to make computers capable of solving complex problems without being explicitly programmed. Availability of large datasets, development of effective algorithms, and access to the powerful computers have resulted in the unprecedented success of machine learning in recent years. This powerful tool has been employed in a plethora of science and engineering domains including mining and minerals industry. Considering the ever-increasing global demand for raw materials, complexities of the geological structure of ore deposits, and decreasing ore grade, high-quality and extensive mineralogical information is required. Comprehensive analyses of such invaluable information call for advanced and powerful techniques including machine learning. This paper presents a systematic review of the efforts that have been dedicated to the development of machine learning-based solutions for better utilizing mineralogical data in mining and mineral studies. To that end, we investigate the main reasons behind the superiority of machine learning in the relevant literature, machine learning algorithms that have been deployed, input data, concerned outputs, as well as the general trends in the subject area.",Minerals,2021,10.3390/min11080816,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
bfd1701f1351c7b9de3e41daae695af90142f992,https://www.semanticscholar.org/paper/bfd1701f1351c7b9de3e41daae695af90142f992,Machine-Learning-Based Microwave Sensing: A Case Study for the Food Industry,"Despite the meticulous attention of food industries to prevent hazards in packaged goods, some contaminants may still elude the controls. Indeed, standard methods, like X-rays, metal detectors and near-infrared imaging, cannot detect low-density materials. Microwave sensing is an alternative method that, combined with machine learning classifiers, can tackle these deficiencies. In this paper we present a design methodology applied to a case study in the food sector. Specifically, we offer a complete flow from microwave dataset acquisition to deployment of the classifiers on real-time hardware and we show the effectiveness of this method in terms of detection accuracy. In the case study, we apply the machine-learning based microwave sensing approach to the case of food jars flowing at high speed on a conveyor belt. First, we collected a dataset from hazelnut-cocoa spread jars which were uncontaminated or contaminated with various intrusions, including low-density plastics. Then, we performed a design space exploration to choose the best MLPs as binary classifiers, which resulted to be exceptionally accurate. Finally, we selected the two most light-weight models for implementation on both an ARM-based CPU and an FPGA SoC, to cover a wide range of possible latency requirements, from loose to strict, to detect contaminants in real-time. The proposed design flow facilitates the design of the FPGA accelerator that might be required to meet the timing requirements by using a high-level approach, which might be suited for the microwave domain experts without specific digital hardware skills.",IEEE Journal on Emerging and Selected Topics in Circuits and Systems,2021,10.1109/jetcas.2021.3097699,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
efaa246e175908ddb2faf777c2ea3984bda9a029,https://www.semanticscholar.org/paper/efaa246e175908ddb2faf777c2ea3984bda9a029,Embedded Machine Learning and Embedded Systems in the Industry.,"Abstract: Machine learning is the buzz word right now. With the machine learning algorithms one can make a computer differentiate between a human and a cow. Can detect objects, can predict different parameters and can process our native languages. But all these algorithms require a fair amount of processing power in order to be trained and fitted as a model. Thankfully, with the current improvement in technology, processing power of computers have significantly increased. But there is a limitation in power consumption and deployability of a server computer. This is where “tinyML” helps the industry out. Machine Learning has never been so easy to access before!",International Journal for Research in Applied Science and Engineering Technology,2021,10.22214/ijraset.2021.39067,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
3fc9cff6ad55986de180204b98613af42f8ac37d,https://www.semanticscholar.org/paper/3fc9cff6ad55986de180204b98613af42f8ac37d,Towards CRISP-ML(Q): A Machine Learning Process Model with Quality Assurance Methodology,"Machine learning is an established and frequently used technique in industry and academia, but a standard process model to improve success and efficiency of machine learning applications is still missing. Project organizations and machine learning practitioners face manifold challenges and risks when developing machine learning applications and have a need for guidance to meet business expectations. This paper therefore proposes a process model for the development of machine learning applications, covering six phases from defining the scope to maintaining the deployed machine learning application. Business and data understanding are executed simultaneously in the first phase, as both have considerable impact on the feasibility of the project. The next phases are comprised of data preparation, modeling, evaluation, and deployment. Special focus is applied to the last phase, as a model running in changing real-time environments requires close monitoring and maintenance to reduce the risk of performance degradation over time. With each task of the process, this work proposes quality assurance methodology that is suitable to address challenges in machine learning development that are identified in the form of risks. The methodology is drawn from practical experience and scientific literature, and has proven to be general and stable. The process model expands on CRISP-DM, a data mining process model that enjoys strong industry support, but fails to address machine learning specific tasks. The presented work proposes an industry- and application-neutral process model tailored for machine learning applications with a focus on technical tasks for quality assurance.",Mach. Learn. Knowl. Extr.,2020,10.20944/PREPRINTS202103.0135.V1,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
b1d831ae4dd4ccd6415c28052fdb24b86a7c4800,https://www.semanticscholar.org/paper/b1d831ae4dd4ccd6415c28052fdb24b86a7c4800,A Full-Stack Machine Learning Environment for Rapidly Evolving Industry Applications,"Developing, deploying, and maintaining machine learning models is a key function of many data science teams. We describe a framework built by American Family Insurance to model the risk profiles of properties. Through empirical experiments, we demonstrate that our automated, end-to-end framework provides a rapid platform for experimentation and productionalization in a business environment.",2021 IEEE 8th International Conference on Data Science and Advanced Analytics (DSAA),2021,10.1109/DSAA53316.2021.9564174,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
076dfe09f77a6b6c9951b9bc515423a82e075808,https://www.semanticscholar.org/paper/076dfe09f77a6b6c9951b9bc515423a82e075808,Architecture and pervasive platform for machine learning services in Industry 4.0,"Pervasive computing promotes the integration of smart electronic devices in our living and working spaces in order to provide new, advanced services. Recently many prototype services based on machine learning techniques have been proposed in a number of domains like smart homes, smart buildings or smart plants. However, the number of applications effectively deployed in the real world is still limited. We believe that architectural principles and integrated frameworks are still missing today to successfully and repetitively support application developers and operators. In this paper, we present a novel architecture and a pervasive platform allowing the development of machine learning based applications in smart buildings.",2021 IEEE International Conference on Pervasive Computing and Communications Workshops and other Affiliated Events (PerCom Workshops),2021,10.1109/PerComWorkshops51409.2021.9431009,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
2acb035cc2c97c18aad5d6effa7b561454615675,https://www.semanticscholar.org/paper/2acb035cc2c97c18aad5d6effa7b561454615675,SOPHIA: An Event-Based IoT and Machine Learning Architecture for Predictive Maintenance in Industry 4.0,"Predictive Maintenance (PdM) is a prominent strategy comprising all the operational techniques and actions required to ensure machine availability and to prevent a machine-down failure. One of the main challenges of PdM is to design and develop an embedded smart system to monitor and predict the health status of the machine. In this work, we use a data-driven approach based on machine learning applied to woodworking industrial machines for a major woodworking Italian corporation. Predicted failures probabilities are calculated through tree-based classification models (Gradient Boosting, Random Forest and Extreme Gradient Boosting) and calculated as the temporal evolution of event data. This is achieved by applying temporal feature engineering techniques and training an ensemble of classification algorithms to predict Remaining Useful Lifetime (RUL) of woodworking machines. The effectiveness of the proposed method is showed by testing an independent sample of additional woodworking machines without presenting machine down. The Gradient Boosting model achieved accuracy, recall, and precision of 98.9%, 99.6%, and 99.1%. Our predictive maintenance approach deployed on a Big Data framework allows screening simultaneously multiple connected machines by learning from terabytes of log data. The target prediction provides salient information which can be adopted within the maintenance management practice.",Inf.,2020,10.3390/info11040202,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
47ef1f01f549f6bb27bf196ea351a55560a554a1,https://www.semanticscholar.org/paper/47ef1f01f549f6bb27bf196ea351a55560a554a1,Machine-Learning-Guided Identification of Coordination Polymer Ligands for Crystallizing Separation of Cs/Sr.,"Separation of Cs/Sr is one of many coordination-chemistry-centered processes in the grand scheme of spent nuclear fuel reprocessing, a critical link for a sustainable nuclear energy industry. To deploy a crystallizing Cs/Sr separation technology, we planned to systematically screen and identify candidate ligands that can efficiently and selectively bind to Sr2+ and form coordination polymers. Therefore, we mined the Cambridge Structural Database for characteristic structural information and developed a machine-learning-guided methodology for ligand evaluation. The optimized machine-learning model, correlating the molecular structures of the ligands with the predicted coordinative properties, generated a ranking list of potential compounds for Cs/Sr selective crystallization. The Sr2+ sequestration capability and selectivity over Cs+ of the promising ligands identified (squaric acid and chloranilic acid) were subsequently confirmed experimentally, with commendable performances, corroborating the artificial-intelligence-guided strategy.",ACS applied materials & interfaces,2022,10.1021/acsami.2c05272,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
b29eb7e8199161edad47fffd7a487fc1f552d0a3,https://www.semanticscholar.org/paper/b29eb7e8199161edad47fffd7a487fc1f552d0a3,Design and Implementation of Advanced Machine Learning Management and Its Impact on Better Healthcare Services: A Multiple Regression Analysis Approach (MRAA),"In the current information and technology era, business enterprises are focusing in performing the process effectively by reducing the waiting time in completing the work, reduce latency and deploy the resources effectively so as to service the patient, medical practitioners, societies, and other stakeholders in an efficient manner. Hence, several organisations are using the emerging technologies so as to obtain high performance and enhance competitive edge. The advancement in machine learning, deep learning, business analytics, etc. enables the health care industry to identify the patterns based on the data collected and create a pivotal position and enhance revenues and profits in a sustainable manner. Machine learning models are considered as computational algorithms which will enable in collected the data, analyze them, and provide the necessary reports to the experts and management in order to make informed decision making. The application of advanced machine learning enables the organisation to process the image effectively, recognize the voice and enable in servicing the customers, process the available data, and identify the patterns so as to make informed decision making. The basic purpose of the study is to analyze the overall implementation of advanced machine learning approaches towards health care services for providing enhanced services, better patient engagement, and support in creating better life for them, the researchers intend to collect the closed-ended questionnaire from employees in different medical centers covering: apprehend the nature of designing and implementation of machine learning approaches in the organisation and understand the effectiveness of these tools in enhancing the sustainable growth and development of the organisation.",Computational and mathematical methods in medicine,2022,10.1155/2022/2489116,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
06e21ad9da671d47ab5c5b29cd2d2d50a36ab8df,https://www.semanticscholar.org/paper/06e21ad9da671d47ab5c5b29cd2d2d50a36ab8df,Network traffic analysis using machine learning: an unsupervised approach to understand and slice your network,,Ann. des Télécommunications,2021,10.1007/s12243-021-00889-1,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
df1c472d5ca319cc98e177c5a6910af35ceb9c55,https://www.semanticscholar.org/paper/df1c472d5ca319cc98e177c5a6910af35ceb9c55,Fog-Based Intelligent Machine Malfunction Monitoring System for Industry 4.0,"There is an exponential increase in the use of Industrial Internet of Things (IIoT) devices for controlling and monitoring the machines in an automated manufacturing industry. Different temperature sensors, pressure sensors, audio sensors, and camera devices are used as IIoT devices for pipeline monitoring and machine operation control in the industrial environment. But, monitoring and identifying the machine malfunction in an industrial environment is a challenging task. In this article, we consider machines fault diagnosis based on their operating sound using the fog computing architecture in the industrial environment. The different computing units, such as industrial controller units or micro data center are used as the fog server in the industrial environment to analyze and classify the machine sounds as normal and abnormal. The linear prediction coefficients and Mel-frequency cepstral coefficients are extracted from the machine sound to develop and deploy supervised machine learning (ML) models on the fog server to monitor and identify the malfunctioning machines based on the operating sound. The experimental results show the performance of ML models for the machines sound recorded with different signal-to-noise ratio levels for normal and abnormal operations.",IEEE Transactions on Industrial Informatics,2021,10.1109/TII.2021.3056076,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
e6212c7d11dbb5b1f43b82c5054296ca1fe9a301,https://www.semanticscholar.org/paper/e6212c7d11dbb5b1f43b82c5054296ca1fe9a301,Unsupervised machine learning framework for early machine failure detection in an industry,"Abstract This article describes a credible and prognostic analysis in this study for failure detection of a machine in the industry. An interpretable methodology and an informative functionality are portrayed, trained with the dataset and their explicatory implementation is compared and evaluated. In this paper, we will design a deployable end-to-end grading model to forecast whether or not the machine will fail. We will train state-of-the-art algorithms for gradient enhanced decision trees (GBDT) and compare their performance.",Journal of Discrete Mathematical Sciences and Cryptography,2021,10.1080/09720529.2021.1951434,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
53c2de154f13c605ff2a9374c93a6a3e0604f670,https://www.semanticscholar.org/paper/53c2de154f13c605ff2a9374c93a6a3e0604f670,Machine Learning Approach for Predictive Maintenance of the Electrical Submersible Pumps (ESPs),"Electrical submersible pumps (ESPs) are considered the second-most widely used artificial lift method in the petroleum industry. As with any pumping artificial lift method, ESPs exhibit failures. The maintenance of ESPs expends a lot of resources, and manpower and is usually triggered and accompanied by the reactive process monitoring of multivariate sensor data. This paper presents a methodology to deploy the principal component analysis and extreme gradient boosting trees (XGBoosting) in predictive maintenance in order to analyze real-time sensor data to predict failures in ESPs. The system contributes to an efficiency increase by reducing the time required to dismantle the pumping system, inspect it, and perform failure analysis. This objective is achieved by applying the principal component analysis as an unsupervised technique; then, its output is pipelined with an XGBoosting model for further prediction of the system status. In comparison to traditional approaches that have been utilized for the diagnosis of ESPs, the proposed model is able to identify deeper functional relationships and longer-term trends inferred from historical data. The novel workflow with the predictive model can provide signals 7 days before the actual failure event, with an F1-score more than 0.71 on the test set. Increasing production efficiencies through the proactive identification of failure events and the avoidance of deferment losses can be accomplished by means of the real-time alarming system presented in this work.",ACS omega,2022,10.1021/acsomega.1c05881,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
11b97b48ea1c56332b69456fda5574dbbe6c752e,https://www.semanticscholar.org/paper/11b97b48ea1c56332b69456fda5574dbbe6c752e,IoT-inspired machine learning-assisted sedentary behavior analysis in smart healthcare industry,,Journal of Ambient Intelligence and Humanized Computing,2021,10.1007/s12652-021-03371-x,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
25d599b3fb27f5119ea7e5d44ef2aa7c3e3b5bfc,https://www.semanticscholar.org/paper/25d599b3fb27f5119ea7e5d44ef2aa7c3e3b5bfc,MLGO: a Machine Learning Guided Compiler Optimizations Framework,"Leveraging machine-learning (ML) techniques for compiler optimizations has been widely studied and explored in academia. However, the adoption of ML in general-purpose, industry strength compilers has yet to happen. We propose MLGO, a framework for integrating ML techniques systematically in an industrial compiler -- LLVM. As a case study, we present the details and results of replacing the heuristics-based inlining-for-size optimization in LLVM with machine learned models. To the best of our knowledge, this work is the first full integration of ML in a complex compiler pass in a real-world setting. It is available in the main LLVM repository. We use two different ML algorithms: Policy Gradient and Evolution Strategies, to train the inlining-for-size model, and achieve up to 7\% size reduction, when compared to state of the art LLVM -Oz. The same model, trained on one corpus, generalizes well to a diversity of real-world targets, as well as to the same set of targets after months of active development. This property of the trained models is beneficial to deploy ML techniques in real-world settings.",ArXiv,2021,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
8ee8d51b5c29f6aadd0b6dcff2a475373a3a021b,https://www.semanticscholar.org/paper/8ee8d51b5c29f6aadd0b6dcff2a475373a3a021b,"The Role of AI, Machine Learning, and Big Data in Digital Twinning: A Systematic Literature Review, Challenges, and Opportunities","Digital twinning is one of the top ten technology trends in the last couple of years, due to its high applicability in the industrial sector. The integration of big data analytics and artificial intelligence/machine learning (AI-ML) techniques with digital twinning, further enriches its significance and research potential with new opportunities and unique challenges. To date, a number of scientific models have been designed and implemented related to this evolving topic. However, there is no systematic review of digital twinning, particularly focusing on the role of AI-ML and big data, to guide the academia and industry towards future developments. Therefore, this article emphasizes the role of big data and AI-ML in the creation of digital twins (DTs) or DT-based systems for various industrial applications, by highlighting the current state-of-the-art deployments. We performed a systematic review on top of multidisciplinary electronic bibliographic databases, in addition to existing patents in the field. Also, we identified development-tools that can facilitate various levels of the digital twinning. Further, we designed a big data driven and AI-enriched reference architecture that leads developers to a complete DT-enabled system. Finally, we highlighted the research potential of AI-ML for digital twinning by unveiling challenges and current opportunities.",IEEE Access,2021,10.1109/ACCESS.2021.3060863,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
5c848bf56d0339a3f671317d063865fe5fdaa114,https://www.semanticscholar.org/paper/5c848bf56d0339a3f671317d063865fe5fdaa114,Whither AutoML? Understanding the Role of Automation in Machine Learning Workflows,"Efforts to make machine learning more widely accessible have led to a rapid increase in Auto-ML tools that aim to automate the process of training and deploying machine learning. To understand how Auto-ML tools are used in practice today, we performed a qualitative study with participants ranging from novice hobbyists to industry researchers who use Auto-ML tools. We present insights into the benefits and deficiencies of existing tools, as well as the respective roles of the human and automation in ML workflows. Finally, we discuss design implications for the future of Auto-ML tool development. We argue that instead of full automation being the ultimate goal of Auto-ML, designers of these tools should focus on supporting a partnership between the user and the Auto-ML tool. This means that a range of Auto-ML tools will need to be developed to support varying user goals such as simplicity, reproducibility, and reliability.",CHI,2021,10.1145/3411764.3445306,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
3698a014e1daf5be3b8323e223c134a80ef883fd,https://www.semanticscholar.org/paper/3698a014e1daf5be3b8323e223c134a80ef883fd,Conception of a Reference Architecture for Machine Learning in the Process Industry,"The increasing global competition demands continuous optimization of products and processes from companies in the process industry. Where conventional methods of Lean Management and Six Sigma reach their limits, new opportunities and challenges arise through increasing connectivity in the Industrial Internet of Things and machine learning. The majority of industrial projects do not reach the deployment or are isolated solutions, as the structures for data integration, training, deployment and maintenance of models are not established. This paper presents the conception of a reference architecture for machine learning in the process industry to support companies in implementing their own specific structures. The focus is on the development process and an exemplary implementation in the brewing industry.",2020 IEEE International Conference on Big Data (Big Data),2020,10.1109/BigData50022.2020.9378290,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
bbc437694caa88ec640acc44aa23d657b0d3562e,https://www.semanticscholar.org/paper/bbc437694caa88ec640acc44aa23d657b0d3562e,THE PERFORMANCE OF VARIOUS SUPERVISED MACHINE LEARNING CLASSIFICATION ALGORITHMS IN SENTIMENT ANALYSIS OF ONLINE CUSTOMER FEEDBACK IN RESTAURANT SECTOR OF HOSPITALITY INDUSTRY,"In the world of internet, connecting different businesses to businesses and businesses to customers offering a multitude of choices leading to a dilemma in the minds of the customers. The customers largely depend on the available online reviews for making purchase decisions as well it helps the business to make decisions. Sentiment Analysis has a wide range of commercial applications. One of the main applications of sentiment analysis is how to extract emotions from a customer review, and how to classify the customer reviews into positive or negative reviews, & find fake reviews. Sentiment Analysis and text classification methods are applied to two different datasets of restaurant reviews namely dataset 1 and dataset 2. The results show SVM algorithm outperforms not only in text classification but also can be deployed in detecting fake reviews. More specifically, A comparison is made five supervised machine learning algorithms: Naïve Bayes (NB), Support Vector Machine (SVM), K-Nearest Neighbours (KNN-IBK), KStar (K*) and Decision Tree (DT-J48) for sentiment classification of reviews",,2020,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
5dbd4c50cfe2982cb5bec9a83f37b899f2eb6aea,https://www.semanticscholar.org/paper/5dbd4c50cfe2982cb5bec9a83f37b899f2eb6aea,Machine learning in the biopharma industry,"Modern high-throughput technologies deployed in R&D for new health products have opened the door to Machine Learning applications that allow the automation of tasks and support for data-driven risk-based decision making. Appealing opportunities of applying Machine Learning appear for the development of modern complex drugs, for biomanufacturing production lines optimization, or even for elaborating product portfolio strategies. Nevertheless, many practical challenges make it difficult to apply Machine Learning models in the biopharmaceutical field. Innovative approaches must thus be considered in many of these practical cases. This tutorial paper is an attempt to describe the landscape of Machine Learning application to the biopharmaceutical industry along three dimensions: opportunities, specificities or constraints and methods.",ESANN,2020,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
3838d9ac43739b68ac35ff16d6c72884a63906ee,https://www.semanticscholar.org/paper/3838d9ac43739b68ac35ff16d6c72884a63906ee,Traffic-profile and machine learning based regional data center design and operation for 5G network,"Data center in the fifth generation (5G) network will serve as a facilitator to move the wireless communication industry from a proprietary hardware based approach to a more software oriented environment. Techniques such as Software defined networking (SDN) and network function virtualization (NFV) would be able to deploy network functionalities such as service and packet gateways as software. These virtual functionalities however would require computational power from data centers. Therefore, these data centers need to be properly placed and carefully designed based on the volume of traffic they are meant to serve. In this work, we first divide the city of Milan, Italy into different zones using K-means clustering algorithm. We then analyse the traffic profiles of these zones in the city using a network operator's Open Big Data set. We identify the optimal placement of data centers as a facility location problem and propose the use of Weiszfeld's algorithm to solve it. Furthermore, based on our analysis of traffic profiles in different zones, we heuristically determine the ideal dimension of the data center in each zone. Additionally, to aid operation and facilitate dynamic utilization of data center resources, we use the state of the art recurrent neural network models to predict the future traffic demands according to past demand profiles of each area.",Journal of Communications and Networks,2019,10.1109/JCN.2019.000055,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
ed9b2c4d4ea93d86a6e1167c9e92cfa0a243c4bc,https://www.semanticscholar.org/paper/ed9b2c4d4ea93d86a6e1167c9e92cfa0a243c4bc,Challenges and Opportunities for Unikernels in Machine Learning Inference,"Machine Learning has become a value creator for many new and old businesses. However, efficient realworld machine learning deployments are still a challenge. Traditional Machine Learning deployments suffer from efficient resource utilization and achieving predictable latency. They cannot be treated in the same manner as other application server deployments. Unikernels are a method to specialize application deployment and performance to suit the needs of the application. Traditionally, building or porting applications to unikernels have been challenging. However, recent work has been into simplifying the development of unikernels. Real-world Unikernels as of now are only for specializing applications that run on the CPU. We survey machine learning practitioners and find out that the majority of machine learning practitioners are using the CPU for machine learning deployments, thus, creating an opportunity for unikernels to optimize the performance of these applications. We compare the architecture of two unikernels: nanos and Unikraft. We benchmarked scikit-learn, a popular machine library, inside a unikernel and found that it only offered a 1% advantage over a traditional deployment. However, our testing could not include more innovative systems like Unikraft due to their immaturity and inability to run machine learning libraries. We include a dependency analysis of three popular machine learning libraries Tensorflow Lite, PyTorch and ONNX, to help pave the way for building machine learning applications as Unikraft unikernels.","2021 9th International Conference on Reliability, Infocom Technologies and Optimization (Trends and Future Directions) (ICRITO)",2021,10.1109/icrito51393.2021.9596080,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
56e63ffea11c875f7eee257798e18cd04e453b6c,https://www.semanticscholar.org/paper/56e63ffea11c875f7eee257798e18cd04e453b6c,Synthetic Benchmarks for Scientific Research in Explainable Machine Learning,"As machine learning models grow more complex and their applications become more high-stakes, tools for explaining model predictions have become increasingly important. This has spurred a flurry of research in model explainability and has given rise to feature attribution methods such as LIME and SHAP. Despite their widespread use, evaluating and comparing different feature attribution methods remains challenging: evaluations ideally require human studies, and empirical evaluation metrics are often data-intensive or computationally prohibitive on realworld datasets. In this work, we address this issue by releasing XAI-BENCH: a suite of synthetic datasets along with a library for benchmarking feature attribution algorithms. Unlike real-world datasets, synthetic datasets allow the efficient computation of conditional expected values that are needed to evaluate groundtruth Shapley values and other metrics. The synthetic datasets we release offer a wide variety of parameters that can be configured to simulate real-world data. We demonstrate the power of our library by benchmarking popular explainability techniques across several evaluation metrics and across a variety of settings. The versatility and efficiency of our library will help researchers bring their explainability methods from development to deployment. Our code is available at https://github.com/abacusai/xai-bench.",,2021,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
b14558c0b727af0ac9086f463b6030b9072dbe16,https://www.semanticscholar.org/paper/b14558c0b727af0ac9086f463b6030b9072dbe16,Methods for Automatic Machine-Learning Workflow Analysis,,ECML/PKDD,2021,10.1007/978-3-030-86517-7_4,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
7655ecfdacf8b91a0f5a4f8074c179a20bf5a61d,https://www.semanticscholar.org/paper/7655ecfdacf8b91a0f5a4f8074c179a20bf5a61d,A Study on Edge Computing through Machine Learning for IoT Devices,"In the current computing trend, next few years from now, it is anticipated, there will be billions of devices in the world that are connected to each other in our homes, cities, vehicles and industries. Devices with resource constraints interact with users and their surroundings. Sensors and actuators have been a common part of our lives in recent years. IoT devices have restricted resources. The large amounts of data collected by such IoT devices are an attractive target for AI systems. But such resource-constrained devices do not provide the necessary resources needed to deploy machine learning models. One approach is to offload the data to external computing systems (such as cloud servers) for further data processing, which increases latency, increases communication costs, and adds to privacy problems. Additional computer devices are placed at the edge of the network, that is, near the devices in which data is generated. The problem of offload is solved effectively by allowing computations to be made close to the sources of data. This paper provides a detailed overview of the overall architecture of how machine learning models are deployed at the edge. It also discusses some light weight machine learning algorithms that have been developed that can be used on such resource constrained IoT devices.","2021 International Conference on Forensics, Analytics, Big Data, Security (FABS)",2021,10.1109/FABS52071.2021.9702557,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
a0f424da767f47f6f046d5cd75ac2971760daff2,https://www.semanticscholar.org/paper/a0f424da767f47f6f046d5cd75ac2971760daff2,ICS Cyber Attack Detection with Ensemble Machine Learning and DPI using Cyber-kit Datasets,"Digitization has pioneered to drive exceptional changes across all industries in the advancement of analytics, automation, and Artificial Intelligence (AI) and Machine Learning (ML). However, new business requirements associated with the efficiency benefits of digitalization are forcing increased connectivity between IT and OT networks, thereby increasing the attack surface and hence the cyber risk. Cyber threats are on the rise and securing industrial networks are challenging with the shortage of human resource in OT field, with more inclination to IT/OT convergence and the attackers deploy various hi-tech methods to intrude the control systems nowadays. We have developed an innovative real-time ICS cyber test kit to obtain the OT industrial network traffic data with various industrial attack vectors. In this paper, we have introduced the industrial datasets generated from ICS test kit, which incorporate the cyber-physical system of industrial operations. These datasets with a normal baseline along with different industrial hacking scenarios are analyzed for research purposes. Metadata is obtained from Deep packet inspection (DPI) of flow properties of network packets. DPI analysis provides more visibility into the contents of OT traffic based on communication protocols. The advancement in technology has led to the utilization of machine learning/artificial intelligence capability in IDS ICS SCADA. The industrial datasets are pre-processed, profiled and the abnormality is analyzed with DPI. The processed metadata is normalized for the easiness of algorithm analysis and modelled with machine learning-based latest deep learning ensemble LSTM algorithms for anomaly detection. The deep learning approach has been used nowadays for enhanced OT IDS performances.",2021 8th International Conference on Computer and Communication Engineering (ICCCE),2021,10.1109/ICCCE50029.2021.9467162,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
7dde3d8ba6115f0bce116c2f661b76d5d97ddec7,https://www.semanticscholar.org/paper/7dde3d8ba6115f0bce116c2f661b76d5d97ddec7,CLOUD BASED MODEL FOR SHARING PATIENT’S HEALTH INFORMATION ACROSS HOSPITALS & ANALYSIS USING MACHINE LEARNING,"With nationwide efforts to improve the quality, safety and efficiency of medical services, the demand for electronic health information exchange between medical professionals is growing. Medical Facility in our country has grown leaps and bounds in past 3 decades, especially with the availability of private hospitals and efforts of Government. In these past decades the cost of medical facility has also grown 100 times in this duration, every time a patient is referred to a particular hospital, he has to again go on for new test along with medical prescriptions, such actions are repeated for every hospital visit, which involves time and cost both. In this work we are propose a model to deploy Hospital Management ERP over cloud, with storage of all the records of patient including his prescriptions & Lab reports, for any time accessibility of the same. So that can reduce the cost of treatment for the patient and accessibility of the same across all hospitals. We also implement Machine Learning as a tool to analyze the health improvement of the patients, tracking their medical history and also to analyze the performance of hospitals & doctors. By accomplishing this work, we aim at providing a solution to Indian Medical Industry and Government of India, in line with HIPAA act of USA, which reduces cost of Treatment, Ease of access and regulation & monitoring of health care.",,2021,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
22540193ef0f5c78577b9856a6575c7873dfaa69,https://www.semanticscholar.org/paper/22540193ef0f5c78577b9856a6575c7873dfaa69,A Machine Learning Approach for Heart Attack Prediction,"A heart attack also known as cardiac arrest, diversify various conditions impacting the heart and became one of the chief-reason for death worldwide over the last few decades. Approximately, 31% of total deaths globally are due to CVDs. It constitutes the pinnacle of chronic processes which involve complex interactions between risk factors which can and cannot be improved. Most of the instances or cases of cardiovascular diseases can be allocated to revisable risk factors where most of the instances are considered preventable. ML became the enhancing approach for the evolution of predictive models in health care industries and was decided to test various algorithms to check what extent their prediction scores estimate or ameliorate upon the results acquired. Researchers deploy various machine learning and data mining techniques over a set of enormous data of cardiovascular patients to attain the prediction for heart attacks before their occurrence for helping healthcare industries and professionals. This research comprises various Supervised ML classifiers like, Gradient Boosting, Decision Tree, Random Forest and Logistic Regression that have been used to deploy a model for Myocardial Infarction prediction. It uses the existing datasets from the Framingham database and others from the database of the UCI Heart repository. This research intends to ideate the prediction for probabilities of occurrence of a heart attack in the patients. These classifiers have been deployed in pipeline approach of machine learning to attain the prediction using both ways i.e., without optimizations and feature transformations as well as vice-versa. The results impersonate that the Gradient Boosting classifier is achieving the highest accuracy score in such a way that prediction used by our model is of binary form in where 1 means a chance of heart attack and 0 means no chance. Some of the most influential attributes are chest pain type among which the typical angina is the most influential and asymptotic chest pain is least, cholesterol level in which the level greater than 200mg/dl are more prone, increased heart rate, thal, and age. It is concluded that premature heart attack is preventable in 80% of the total cases just by using a healthy diet along with regular exercises and not using tobacco products also the person who drinks more than 5 glasses of water daily are less likely to develop attacks. The medical checkup of Blood-pressure level, cholesterol level and heart rate on daily basis along with meditation can help you prevent the major heart attacks.",International Journal of Engineering and Advanced Technology,2021,10.35940/ijeat.f3043.0810621,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
9f4ae17db46e604337aa62ddb2c340af5a306553,https://www.semanticscholar.org/paper/9f4ae17db46e604337aa62ddb2c340af5a306553,Interpretable Automated Machine Learning in Maana(TM) Knowledge Platform,"Machine learning is becoming an essential part of developing solutions for many industrial applications, but the lack of interpretability hinders wide industry adoption to rapidly build, test, deploy and validate machine learning models, in the sense that the insight of developing machine learning solutions are not structurally encoded, justified and transferred. In this paper we describe the Maana Meta-learning Service, an interpretable and interactive automated machine learning service residing in XYZ Knowledge Platform that performs machine-guided, user assisted pipeline search and hyper-parameter tuning and generates structured knowledge about decisions for pipeline profiling and selection. The service is shipped with the Maana Knowledge Platform and is validated using benchmark dataset. Furthermore, its capability of deriving knowledge from pipeline search facilitates various inference tasks and transferring to similar data science projects.",AAMAS,2019,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
3451f2a3274dbf842787e18b666ce6838449ee51,https://www.semanticscholar.org/paper/3451f2a3274dbf842787e18b666ce6838449ee51,Hybrid Machine Learning Approach to Popularity Prediction of Newly Released Contents for Online Video Streaming Service,"In the industry of video content providers such as VOD and IPTV, predicting the popularity of video contents in advance is critical not only from a marketing perspective but also from a network optimization perspective. By predicting whether the content will be successful or not in advance, the content file, which is large, is efficiently deployed in the proper service providing server, leading to network cost optimization. Many previous studies have done view count prediction research to do this. However, the studies have been making predictions based on historical view count data from users. In this case, the contents had been published to the users and already deployed on a service server. These approaches make possible to efficiently deploy a content already published but are impossible to use for a content that is not be published. To address the problems, this research proposes a hybrid machine learning approach to the classification model for the popularity prediction of newly video contents which is not published. In this paper, we create a new variable based on the related content of the specific content and divide entire dataset by the characteristics of the contents. Next, the prediction is performed using XGBoosting and deep neural net based model according to the data characteristics of the cluster. Our model uses metadata for contents for prediction, so we use categorical embedding techniques to solve the sparsity of categorical variables and make them learn efficiently for the deep neural net model. As well, we use the FTRL-proximal algorithm to solve the problem of the view-count volatility of video content. We achieve overall better performance than the previous standalone method with a dataset from one of the top streaming service company.",ArXiv,2019,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
557e29276e7154e7ce7b0d014342cedb46626dab,https://www.semanticscholar.org/paper/557e29276e7154e7ce7b0d014342cedb46626dab,IoT and Interpretable Machine Learning Based Framework for Disease Prediction in Pearl Millet,"Decrease in crop yield and degradation in product quality due to plant diseases such as rust and blast in pearl millet is the cause of concern for farmers and the agriculture industry. The stipulation of expert advice for disease identification is also a challenge for the farmers. The traditional techniques adopted for plant disease detection require more human intervention, are unhandy for farmers, and have a high cost of deployment, operation, and maintenance. Therefore, there is a requirement for automating plant disease detection and classification. Deep learning and IoT-based solutions are proposed in the literature for plant disease detection and classification. However, there is a huge scope to develop low-cost systems by integrating these techniques for data collection, feature visualization, and disease detection. This research aims to develop the ‘Automatic and Intelligent Data Collector and Classifier’ framework by integrating IoT and deep learning. The framework automatically collects the imagery and parametric data from the pearl millet farmland at ICAR, Mysore, India. It automatically sends the collected data to the cloud server and the Raspberry Pi. The ‘Custom-Net’ model designed as a part of this research is deployed on the cloud server. It collaborates with the Raspberry Pi to precisely predict the blast and rust diseases in pearl millet. Moreover, the Grad-CAM is employed to visualize the features extracted by the ‘Custom-Net’. Furthermore, the impact of transfer learning on the ‘Custom-Net’ and state-of-the-art models viz. Inception ResNet-V2, Inception-V3, ResNet-50, VGG-16, and VGG-19 is shown in this manuscript. Based on the experimental results, and features visualization by Grad-CAM, it is observed that the ‘Custom-Net’ extracts the relevant features and the transfer learning improves the extraction of relevant features. Additionally, the ‘Custom-Net’ model reports a classification accuracy of 98.78% that is equivalent to state-of-the-art models viz. Inception ResNet-V2, Inception-V3, ResNet-50, VGG-16, and VGG-19. Although the classification of ‘Custom-Net’ is comparable to state-of-the-art models, it is effective in reducing the training time by 86.67%. It makes the model more suitable for automating disease detection. This proves that the proposed model is effective in providing a low-cost and handy tool for farmers to improve crop yield and product quality.",Sensors,2021,10.3390/s21165386,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
b403ecdbcdb1de0af4db0e7bac6da0f5752983d2,https://www.semanticscholar.org/paper/b403ecdbcdb1de0af4db0e7bac6da0f5752983d2,Fitness-Aware Containerization Service Leveraging Machine Learning,"Containerized deployment of microservices has gained immense traction across industries. To meet demand, traditional cloud providers offer container-as-a-service, where selection of the container and containerization of workloads remain developer’s responsibility. This task is arduous for a developer since the choice of containers across different cloud providers is many. Furthermore, there does not exist any mechanism using which one can compare and contrast the capabilities of containers across different providers. In this scenario, we envisage the need for a smart cloud broker that can automatically deploy a chosen IT service into the best-fit container environment mapped to performance requirements, from among the set of available underpinning brokered container hosting systems spread across multiple cloud providers. We propose a novel fitness-aware containerization-as-a-service to achieve this. We show why a best-fit container selection process is operationally complex and time consuming, and how we heuristically prune the associated decision tree in two phases so that it becomes viable to implement this as an on-demand service. We propose a new metric called fitness quotient (<inline-formula><tex-math notation=""LaTeX"">$FQ$</tex-math><alternatives><mml:math><mml:mrow><mml:mi>F</mml:mi><mml:mi>Q</mml:mi></mml:mrow></mml:math><inline-graphic xlink:href=""venkateswaran-ieq1-2898666.gif""/></alternatives></inline-formula>) to evaluate containers obtained from heterogeneous providers. We leverage machine learning techniques to inject automation into these two phases: unsupervised K-Means clustering in the first-level build-time phase to accurately classify IaaS cost and performance data, and polynomial regression during the second-level provisioning-time phase to discover relationships between SaaS performance and container strength. We also show that the utility of the framework that we propose is not limited to the container fitness use case that we analyze in this paper; rather it can be generalized to address a class of problems where overall time and cost complexity for provisioning-time decision making needs to be controlled under a given set of constraints.",IEEE Transactions on Services Computing,2019,10.1109/TSC.2019.2898666,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
f5172db961538ed9fc9cafecd0764821b6dd6714,https://www.semanticscholar.org/paper/f5172db961538ed9fc9cafecd0764821b6dd6714,Assessing Machine Learning Approaches to Address IoT Sensor Drift,"The proliferation of IoT sensors and their deployment in various industries and applications has brought about numerous analysis opportunities in this Big Data era. However, drift of those sensor measurements poses major challenges to automate data analysis and the ability to effectively train and deploy models on a continuous basis. In this paper we study and test several approaches from the literature with regard to their ability to cope with and adapt to sensor drift under realistic conditions. Most of these approaches are recent and thus are representative of the current state-of-the-art. The testing was performed on a publicly available gas sensor dataset exhibiting drift over time. The results show substantial drops in sensing performance due to sensor drift in spite of the approaches. We then discuss several issues identified with current approaches and outline directions for future research to tackle them.",ArXiv,2021,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
e77c81d6d12b71305e3d861cc3ea8832185fb994,https://www.semanticscholar.org/paper/e77c81d6d12b71305e3d861cc3ea8832185fb994,Predictive maintenance leveraging machine learning for time-series forecasting in the maritime industry,"One of the key challenges in the maritime industry refers to minimizing the time a vessel cannot be utilized, which has multiple effects. The latter is addressed through maintenance approaches that however in many cases are not efficient in terms of cost and downtime. Predictive maintenance provides optimized maintenance scheduling offering extended vessel lifespan, coupled with reduced maintenance costs. As in several industries, including the maritime domain, an increasing amount of data is made available through the deployment and exploitation of data sources, such as on board sensors that provide real-time information. These data provide the required ground for analysis and thus support for various types of data-driven decision making. In the maritime domain, sensors are deployed on vessels to monitor their engines and data analysis tools are needed to assist engineers towards reduced operational risk through predictive maintenance solutions that are put in place. In this paper, we present an approach for anomaly detection on time-series data, utilizing machine learning on the vessels sensor data, in order to predict the condition of specific parts of the vessel’s main engine and thus facilitate predictive maintenance. The novel characteristic of the proposed approach refers both to the inclusion of new innovative models to address the case of predictive maintenance in maritime and the combination of those different models, highlighting an improved result in terms of evaluation metrics.",2020 IEEE 23rd International Conference on Intelligent Transportation Systems (ITSC),2020,10.1109/ITSC45102.2020.9294450,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
d310b96f3bc0ef80ce48ab90737156ee61dd9795,https://www.semanticscholar.org/paper/d310b96f3bc0ef80ce48ab90737156ee61dd9795,Machine Learning for Creative Transfer,"The first algorithm for neural style transfer was proposed by Gatys et al (2015), since then, Style Transfer for images using Computer Vision has become increasingly efficient. Convolutional neural networks (CNN) are the main technique which make style transfer possible. Via Deep learning CNN networks deploy a process of repeated optimisation until one image adopts the style of another while retaining its own original content. The potential for style transfer in Quality Control is worth exploring, in particular, the development of visual, haptic and olfactory style transfer, to measure quality within a multi-modal, multi-dimensional problem space. Most quality control for garments is currently undertaken by human workers, with significant potential for errors to go unchecked. Such work is also tedious and fatiguing, making quality control an obvious, if challenging area for development, benefiting both industry and individual workers. Style transfer is a relatively new technique, one which has not yet been deployed within the garment industry, or within a multi-modal form.",,2019,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
b5b98051b65da6b1b3b579862b0407d48c5bef48,https://www.semanticscholar.org/paper/b5b98051b65da6b1b3b579862b0407d48c5bef48,Principles and Practice of Explainable Machine Learning,"Artificial intelligence (AI) provides many opportunities to improve private and public life. Discovering patterns and structures in large troves of data in an automated manner is a core component of data science, and currently drives applications in diverse areas such as computational biology, law and finance. However, such a highly positive impact is coupled with a significant challenge: how do we understand the decisions suggested by these systems in order that we can trust them? In this report, we focus specifically on data-driven methods—machine learning (ML) and pattern recognition models in particular—so as to survey and distill the results and observations from the literature. The purpose of this report can be especially appreciated by noting that ML models are increasingly deployed in a wide range of businesses. However, with the increasing prevalence and complexity of methods, business stakeholders in the very least have a growing number of concerns about the drawbacks of models, data-specific biases, and so on. Analogously, data science practitioners are often not aware about approaches emerging from the academic literature or may struggle to appreciate the differences between different methods, so end up using industry standards such as SHAP. Here, we have undertaken a survey to help industry practitioners (but also data scientists more broadly) understand the field of explainable machine learning better and apply the right tools. Our latter sections build a narrative around a putative data scientist, and discuss how she might go about explaining her models by asking the right questions. From an organization viewpoint, after motivating the area broadly, we discuss the main developments, including the principles that allow us to study transparent models vs. opaque models, as well as model-specific or model-agnostic post-hoc explainability approaches. We also briefly reflect on deep learning models, and conclude with a discussion about future research directions.",Frontiers in Big Data,2020,10.3389/fdata.2021.688969,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
b29ba10d797f7befe4829e40d1ecebdef2dc78da,https://www.semanticscholar.org/paper/b29ba10d797f7befe4829e40d1ecebdef2dc78da,Addressing Diverse Petroleum Industry Problems Using Machine Learning Techniques: Literary Methodology—Spotlight on Predicting Well Integrity Failures,"Artificial intelligence (AI) and machine learning (ML) are transforming industries, where low-cost, big data can utilize computing power to optimize system performance. Oil and gas (O&G) fields are getting mature, where well integrity (WI) problems become more common and field operations are now more challenging. Hence, they are good candidates for transformation due to the low cost of data storage, highlighting the oil market decline, along with dynamic risk posed during operations. This paper is presenting a comprehensive compilation of different ML applications in diverse disciplines of the petroleum industry. The pool of AI and ML with respect to different areas of applications along with publication years has been categorized. The main focus of this study is classifying well integrity failures where the authors found that the potential of AI and ML in predicting well integrity failures has not been efficiently tapped, and there is an explicit gap in the literature. First, the applications of AI, ML, and data analytics in the O&G industry are discussed thoroughly, so this paper can be a comprehensive reference for readers and future researchers. Then data preprocessing is explained. This includes data gathering, cleaning, and feature engineering. Next, the different ML models are compared and discussed. Finally, model performance evaluation and best model selection are described. This study would be a concrete foundation in the design and construction of ML programs that can be deployed for WI risk management. The developed model can be simply used for any well stock, providing quick and easy assessment instead of subjective and tedious assessment. The layout can be simply adjusted to reflect the risk profile of any well type or any field.",ACS omega,2022,10.1021/acsomega.1c05658,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
e08fffc33838d49b4d4294ebbc8ba90aced46f8c,https://www.semanticscholar.org/paper/e08fffc33838d49b4d4294ebbc8ba90aced46f8c,Blockchain and Machine Learning for Communications and Networking Systems,"Recently, with the rapid development of information and communication technologies, the infrastructures, resources, end devices, and applications in communications and networking systems are becoming much more complex and heterogeneous. In addition, the large volume of data and massive end devices may bring serious security, privacy, services provisioning, and network management challenges. In order to achieve decentralized, secure, intelligent, and efficient network operation and management, the joint consideration of blockchain and machine learning (ML) may bring significant benefits and have attracted great interests from both academia and industry. On one hand, blockchain can significantly facilitate training data and ML model sharing, decentralized intelligence, security, privacy, and trusted decision-making of ML. On the other hand, ML will have significant impacts on the development of blockchain in communications and networking systems, including energy and resource efficiency, scalability, security, privacy, and intelligent smart contracts. However, some essential open issues and challenges that remain to be addressed before the widespread deployment of the integration of blockchain and ML, including resource management, data processing, scalable operation, and security issues. In this paper, we present a survey on the existing works for blockchain and ML technologies. We identify several important aspects of integrating blockchain and ML, including overview, benefits, and applications. Then we discuss some open issues, challenges, and broader perspectives that need to be addressed to jointly consider blockchain and ML for communications and networking systems.",IEEE Communications Surveys & Tutorials,2020,10.1109/COMST.2020.2975911,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
e4ec612abcc934a3ac21ebfaa8b7b5e9bc122b82,https://www.semanticscholar.org/paper/e4ec612abcc934a3ac21ebfaa8b7b5e9bc122b82,Production Machine Learning Pipelines: Empirical Analysis and Optimization Opportunities,"Machine learning (ML) is now commonplace, powering data-driven applications in various organizations. Unlike the traditional perception of ML in research, ML production pipelines are complex, with many interlocking analytical components beyond training, whose sub-parts are often run multiple times on overlapping subsets of data. However, there is a lack of quantitative evidence regarding the lifespan, architecture, frequency, and complexity of these pipelines to understand how data management research can be used to make them more efficient, effective, robust, and reproducible. To that end, we analyze the provenance graphs of 3000 production ML pipelines at Google, comprising over 450,000 models trained, spanning a period of over four months, in an effort to understand the complexity and challenges underlying production ML. Our analysis reveals the characteristics, components, and topologies of typical industry-strength ML pipelines at various granularities. Along the way, we introduce a specialized data model for representing and reasoning about repeatedly run components in these ML pipelines, which we call model graphlets. We identify several rich opportunities for optimization, leveraging traditional data management ideas. We show how targeting even one of these opportunities, i.e., identifying and pruning wasted computation that does not translate to model deployment, can reduce wasted computation cost by 50% without compromising the model deployment cadence.",SIGMOD Conference,2021,10.1145/3448016.3457566,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
68c1ace119969bf23d7d064aa5b556b333d4326a,https://www.semanticscholar.org/paper/68c1ace119969bf23d7d064aa5b556b333d4326a,Tutorial on Software Testing & Quality Assurance for Machine Learning Applications from research bench to real world,"Rapid progress in Machine Learning (ML) has seen a swift translation to real world commercial deployment. While research and development of ML applications have progressed at an exponential pace, the required software engineering process for ML applications and the corresponding eco-system of testing and quality assurance tools which enable software reliable, trustworthy and safe and easy to deploy, have sadly lagged behind. Specifically, the challenges and gaps in quality assurance (QA) and testing of AI applications have largely remained unaddressed contributing to a poor translation rate of ML applications from research to real world. Unlike traditional software, which has a well-defined software testing methodology, ML applications have largely taken an ad-hoc approach to testing. ML researchers and practitioners either fall back to traditional software testing approaches, which are inadequate for this domain, due to its inherent probabilistic and data dependent nature, or rely largely on non-rigorous self-defined QA methodologies. These issues have driven the ML and Software Engineering research communities to develop of newer tools and techniques designed specifically for ML. These research advances need to be publicized and practiced in real world in ML development and deployment for enabling successful translation of ML from research prototypes to real world. This tutorial intends to address this need. This tutorial aims to: [1] Provide a comprehensive overview of testing of ML applications [2] Provide practical insights and share community best practices for testing ML software Besides scientific literature, we derive our insights from our conversations with industry experts in ML.",COMAD/CODS,2020,10.1145/3371158.3371233,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
d4a5a1568dade24782212e45407510f0d2b0377c,https://www.semanticscholar.org/paper/d4a5a1568dade24782212e45407510f0d2b0377c,Auto-Scaling VNFs Using Machine Learning to Improve QoS and Reduce Cost,"Virtualization of network functions (as virtual routers, virtual firewalls, etc.) enables network owners to efficiently respond to the increasing dynamicity of network services. Virtual Network Functions (VNFs) are easy to deploy, update, monitor, and manage. The number of VNF instances, similar to generic computing resources in cloud, can be easily scaled based on load. Auto-scaling (of resources without human intervention) has been investigated in academia and industry. Prior studies on auto-scaling use measured network traffic load to dynamically react to traffic changes. In this study, we propose a proactive Machine Learning (ML) based approach to perform auto-scaling of VNFs in response to dynamic traffic changes. Our proposed ML classifier learns from past VNF scaling decisions and seasonal/spatial behavior of network traffic load to generate scaling decisions ahead of time. Compared to existing approaches for ML-based auto- scaling, our study explores how the properties (e.g., start-up time) of underlying virtualization technology impacts QoS and cost savings. We consider four different virtualization technologies: Xen and KVM, based on hypervisor virtualization, and Docker and LXC, based on container virtualization. Our results show promising accuracy of the ML classifier. We also demonstrate using realistic traffic load traces and optical backbone network that our ML method improves QoS and saves significant cost for network owners as well as leasers.",2018 IEEE International Conference on Communications (ICC),2018,10.1109/ICC.2018.8422788,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
09a7eb1c08d47442e8434f6abaaf19b2c50f1063,https://www.semanticscholar.org/paper/09a7eb1c08d47442e8434f6abaaf19b2c50f1063,A Machine Learning Approach for Heart Attack Prediction,"124 Published By: Blue Eyes Intelligence Engineering and Sciences Publication © Copyright: All rights reserved. Retrieval Number: 100.1/ijeat.F30430810621 DOI:10.35940/ijeat.F3043.0810621 Journal Website: www.ijeat.org Abstract: A heart attack also known as cardiac arrest, diversify various conditions impacting the heart and became one of the chief-reason for death worldwide over the last few decades. Approximately, 31% of total deaths globally are due to CVDs. It constitutes the pinnacle of chronic processes which involve complex interactions between risk factors which can and cannot be improved. Most of the instances or cases of cardiovascular diseases can be allocated to revisable risk factors where most of the instances are considered preventable. ML became the enhancing approach for the evolution of predictive models in health care industries and was decided to test various algorithms to check what extent their prediction scores estimate or ameliorate upon the results acquired. Researchers deploy various machine learning and data mining techniques over a set of enormous data of cardiovascular patients to attain the prediction for heart attacks before their occurrence for helping healthcare industries and professionals. This research comprises various Supervised ML classifiers like, Gradient Boosting, Decision Tree, Random Forest and Logistic Regression that have been used to deploy a model for Myocardial Infarction prediction. It uses the existing datasets from the Framingham database and others from the database of the UCI Heart repository. This research intends to ideate the prediction for probabilities of occurrence of a heart attack in the patients. These classifiers have been deployed in pipeline approach of machine learning to attain the prediction using both ways i.e., without optimizations and feature transformations as well as vice-versa. The results impersonate that the Gradient Boosting classifier is achieving the highest accuracy score in such a way that prediction used by our model is of binary form in where 1 means a chance of heart attack and 0 means no chance. Some of the most influential attributes are chest pain type among which the typical angina is the most influential and asymptotic chest pain is least, cholesterol level in which the level greater than 200mg/dl are more prone, increased heart rate, thal, and age. It is concluded that premature heart attack is preventable in 80% of the total cases just by using a healthy diet along with regular exercises and not using tobacco products also the person who drinks more than 5 glasses of water daily are less likely to develop attacks.",,2020,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
05a062752331c8d4a45c82af45707bc790599154,https://www.semanticscholar.org/paper/05a062752331c8d4a45c82af45707bc790599154,Industry 4.0: Sensor Data Analysis Using Machine Learning,,,2019,10.1007/978-3-030-54595-6_3,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
c572143c39f369e3c53dbc0444b07ec3bf594b0c,https://www.semanticscholar.org/paper/c572143c39f369e3c53dbc0444b07ec3bf594b0c,Prediction of Workpiece Quality: An Application of Machine Learning in Manufacturing Industry,"A significant amount of data is generatedand could be utilized in order to improve quality, time, and cost related performance characteristics of the production process. Machine Learning (ML) is considered as a particularly effective method of data processing with the aim of generating usable knowledge from data and therefore becomes increasingly relevant in manufacturing. In this research paper, a technology framework is created that supports solution providers in the development and deployment process of ML applications. This framework is subsequently successfully employed in the development of an ML application for quality prediction in a machining process of Bosch Rexroth AG.For this purpose the 50 mostrelevant features were extracted out of time series data and used to determine the best ML operation. Extra Tree Regressor (XT) is found to achieve precise predictions with a coefficient of determination (R2) of constantly over 91% for the considered quality characteristics of a boreof hydraulic valves.",,2019,10.5121/csit.2019.91316,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
2ba6f6f08c0722aefb4d00acae7bdb19563d414d,https://www.semanticscholar.org/paper/2ba6f6f08c0722aefb4d00acae7bdb19563d414d,Machine Learning Techniques for 5G and Beyond,"Wireless communication systems play a very crucial role in modern society for entertainment, business, commercial, health and safety applications. These systems keep evolving from one generation to next generation and currently we are seeing deployment of fifth generation (5G) wireless systems around the world. Academics and industries are already discussing beyond 5G wireless systems which will be sixth generation (6G) of the evolution. One of the main and key components of 6G systems will be the use of Artificial Intelligence (AI) and Machine Learning (ML) for such wireless networks. Every component and building block of a wireless system that we currently are familiar with from our knowledge of wireless technologies up to 5G, such as physical, network and application layers, will involve one or another AI/ML techniques. This overview paper, presents an up-to-date review of future wireless system concepts such as 6G and role of ML techniques in these future wireless systems. In particular, we present a conceptual model for 6G and show the use and role of ML techniques in each layer of the model. We review some classical and contemporary ML techniques such as supervised and un-supervised learning, Reinforcement Learning (RL), Deep Learning (DL) and Federated Learning (FL) in the context of wireless communication systems. We conclude the paper with some future applications and research challenges in the area of ML and AI for 6G networks.",IEEE Access,2021,10.1109/ACCESS.2021.3051557,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
70b4101b88eeedb545ac2847d5b47dde1ff404d5,https://www.semanticscholar.org/paper/70b4101b88eeedb545ac2847d5b47dde1ff404d5,Performance Analysis of Computer Vision with Machine Learning Algorithms on Raspberry Pi 3,,,2020,10.1007/978-3-030-63128-4_17,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
5ef27251f46f8afc45e5828beaf2f4627db8146e,https://www.semanticscholar.org/paper/5ef27251f46f8afc45e5828beaf2f4627db8146e,The absorption and multiplication of uncertainty in machine‐learning‐driven finance,"Abstract Uncertainty about market developments and their implications characterize financial markets. Increasingly, machine learning is deployed as a tool to absorb this uncertainty and transform it into manageable risk. This article analyses machine‐learning‐based uncertainty absorption in financial markets by drawing on 182 interviews in the finance industry, including 45 interviews with informants who were actively applying machine‐learning techniques to investment management, trading, or risk management problems. We argue that while machine‐learning models are deployed to absorb financial uncertainty, they also introduce a new and more profound type of uncertainty, which we call critical model uncertainty. Critical model uncertainty refers to the inability to explain how and why the machine‐learning models (particularly neural networks) arrive at their predictions and decisions—their uncertainty‐absorbing accomplishments. We suggest that the dialectical relation between machine‐learning models’ uncertainty absorption and multiplication calls for further research in the field of finance and beyond.",The British journal of sociology,2021,10.1111/1468-4446.12880,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
9656acc489a092e7d4c93c86ac45ccd411214da7,https://www.semanticscholar.org/paper/9656acc489a092e7d4c93c86ac45ccd411214da7,Machine Learning based data analytics for IoT enabled Industry Automation,"The main aims of this projects to the replacement of old communication that uses wired links with new communication that is wireless communication.The main reason to move to wireless communication is to improve the mobility, reduce the deployment cost, reduce cable damage and to improve the scalability.The current industrial revolution is the 4.0 industrial revolution which combines different technologies such as Internet of Things (IOT), robotics, virtual reality and artificial intelligence. The current industrial revolution is the 4.0 industrial revolution which combines different technologies such as Internet of Things (IOT), robotics, virtual reality and artificial intelligence.The current industrial revolution is the 4.0 industrial revolution which combines different technologies such as Internet of Things (IOT), robotics, virtual reality and artificial intelligence.The second aim of this project is to connect devices to IOT so as to improve theaccessibility of the industry from anywhere in the world. These services are known as Best Effort services.","International Journal of Scientific Research in Science, Engineering and Technology",2022,10.32628/ijsrset229240,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
9ee4c1717c663c281006f31c0a2f8831c0512294,https://www.semanticscholar.org/paper/9ee4c1717c663c281006f31c0a2f8831c0512294,Counterfactual Explanations for Machine Learning: Challenges Revisited,"Copyright held by the owner/author(s). CHI’21,, May 8–9, 2021, Virtual ACM 978-1-4503-6819-3/20/04. https://doi.org/10.1145/3334480.XXXXXXX Abstract Counterfactual explanations (CFEs) are an emerging technique under the umbrella of interpretability of machine learning (ML) models. They provide “what if” feedback of the form “if an input datapoint were x′ instead of x, then an ML model’s output would be y′ instead of y.” Counterfactual explainability for ML models has yet to see widespread adoption in industry. In this short paper, we posit reasons for this slow uptake. Leveraging recent work outlining desirable properties of CFEs and our experience running the ML wing of a model monitoring startup, we identify outstanding obstacles hindering CFE deployment in industry.",ArXiv,2021,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
b362eae4b08e36a01e03ab2b7a51870ebf6d3a97,https://www.semanticscholar.org/paper/b362eae4b08e36a01e03ab2b7a51870ebf6d3a97,"Machine Learning for Cloud Data Systems: the Promise, the Progress, and the Path Forward","The goal of this tutorial is to educate the audience about the state of the art in ML for cloud data systems, both in research and in practice. The tutorial is divided in two parts: the progress, and the path forward. Part I covers the recent successes in deploying machine learning solutions for cloud data systems. We will discuss the practical considerations taken into account and the progress made at various levels. The goal is to compare and contrast the promise of ML for systems with the ground actually covered in industry. Finally, Part II discusses practical issues of machine learning in the enterprise covering the generation of explanations, model debugging, model deployment, model management, constraints on eyes-on data usage and anonymization, and a discussion of the technical debt that can accrue through machine learning and models in the enterprise. PVLDB Reference Format: Alekh Jindal and Matteo Interlandi. Machine Learning for Cloud Data Systems: the Progress so far and the Path Forward. PVLDB, 14(12):",Proc. VLDB Endow.,2021,10.14778/3476311.3476408,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
d1b01ce82b56c199e9d7399e392479e78b1a48f5,https://www.semanticscholar.org/paper/d1b01ce82b56c199e9d7399e392479e78b1a48f5,Predicting Admissions From a Paediatric Emergency Department – Protocol for Developing and Validating a Low-Dimensional Machine Learning Prediction Model,"Introduction: Patients boarding in the Emergency Department can contribute to overcrowding, leading to longer waiting times and patients leaving without being seen or completing their treatment. The early identification of potential admissions could act as an additional decision support tool to alert clinicians that a patient needs to be reviewed for admission and would also be of benefit to bed managers in advance bed planning for the patient. We aim to create a low-dimensional model predicting admissions early from the paediatric Emergency Department. Methods and Analysis: The methodology Cross Industry Standard Process for Data Mining (CRISP-DM) will be followed. The dataset will comprise of 2 years of data, ~76,000 records. Potential predictors were identified from previous research, comprising of demographics, registration details, triage assessment, hospital usage and past medical history. Fifteen models will be developed comprised of 3 machine learning algorithms (Logistic regression, naïve Bayes and gradient boosting machine) and 5 sampling methods, 4 of which are aimed at addressing class imbalance (undersampling, oversampling, and synthetic oversampling techniques). The variables of importance will then be identified from the optimal model (selected based on the highest Area under the curve) and used to develop an additional low-dimensional model for deployment. Discussion: A low-dimensional model comprised of routinely collected data, captured up to post triage assessment would benefit many hospitals without data rich platforms for the development of models with a high number of predictors. Novel to the planned study is the use of data from the Republic of Ireland and the application of sampling techniques aimed at improving model performance impacted by an imbalance between admissions and discharges in the outcome variable.",Frontiers in Big Data,2021,10.3389/fdata.2021.643558,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
0363cd91dd0eba0c49b98e49c124f104bc70615f,https://www.semanticscholar.org/paper/0363cd91dd0eba0c49b98e49c124f104bc70615f,Estimation of Bottom Hole Pressure in Electrical Submersible Pump Wells using Machine Learning Technique,"
 With the growing importance and application of Machine Learning in various complex operations in the Oil and Gas Industry, this study focuses on the implementation of data analytics for estimating and/or validating bottom-hole pressure (BHP) of Electrical Submersible Pump (ESP) wells. Depending on the placement of the ESP in the wellbore and fluid gravity of the well fluid, there can be little or no difference between BHP and Pump intake Pressure (PIP); hence these two parameters were used interchangeably. The study focuses majorly on validating PIP when there are concerns with downhole gauge readings. It also has application in estimating PIP when the gauge readings are not available, provided the relevant ESP parameters are obtainable. ESP wells generally have gauges that operate on ""Comms-on-Power"" principle i.e. downhole communication is via the power cable and loss of signal occurs when there is no good electrical integrity along the electrical path of the ESP system. For proper hydrocarbon accounting and statutory requirements, it is important to have downhole pressure readings on a continuous basis, however this cannot be guaranteed throughout the life cycle of the well. Therefore, an alternative method is essential and had to be sought.
 In this study, the Response Surface Modelling (RSM) was first used to generate a model relating the ESP parameters acquired real-time to the PIP values. The model was fine-tuned with a Supervised Machine Learning algorithm: Artificial Neural Network (ANN). The performance of the algorithms was then validated using the R-Square and Mean Square Error values. The result proves that Machine Learning can be used to estimate PIP in a well without recourse to incurring additional cost of deploying new downhole gauges for acquisition of well and reservoir data.","Day 2 Tue, August 03, 2021",2021,10.2118/207122-ms,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
9054a51a7dde6e3138156a32d015ce2e72f7cfac,https://www.semanticscholar.org/paper/9054a51a7dde6e3138156a32d015ce2e72f7cfac,Automated Estimation of Construction Equipment Emission using Inertial Sensors and Machine Learning Models,"The construction industry is one of the main producers of greenhouse gasses (GHG). Quantifying the amount of air pollutants including GHG emissions during a construction project has become an additional project objective to traditional metrics such as time, cost, and safety in many parts of the world. A major contributor to air pollution during construction is the use of heavy equipment and thus their efficient operation and management can substantially reduce the harm to the environment. Although the on-road vehicle emission prediction is a widely researched topic, construction equipment emission measurement and reduction have received very little attention. This paper describes the development and deployment of a novel framework that uses machine learning (ML) methods to predict the level of emissions from heavy construction equipment monitored via an Internet of Things (IoT) system comprised of accelerometer and gyroscope sensors. The developed framework was validated using an excavator performing real-world construction work. A portable emission measurement system (PEMS) was employed along with the inertial sensors to record data including the amount of CO, NOX, CO2, SO2, and CH4 pollutions emitted by the equipment. Different ML algorithms were developed and compared to identify the best model to predict emission levels from inertial sensors data. The results showed that Random Forest with the coefficient of determination (R) of 0.94, 0.91 and 0.94 for CO, NOX, CO2, respectively was the best algorithm among different models evaluated in this study.",ArXiv,2021,10.3390/su14052750,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
d040efeaf54aab98a5825c88cb0aa118e74eeb52,https://www.semanticscholar.org/paper/d040efeaf54aab98a5825c88cb0aa118e74eeb52,A WPT/NFC-Based Sensing Approach for Beverage Freshness Detection Using Supervised Machine Learning,"The massive deployment of wireless sensors is a fundamental piece in the growing internet of things (IoT) industry. Therefore, it is imperative to use already existing hardware to realize new sensing functions with very few or no hardware added. As wireless power transfer (WPT) and near field communication (NFC) become standard features in smart phones, this article investigates beverage freshness sensing based on the WPT/NFC technology compatible with smart phones. A circuit model for the beverage-coil interaction was developed and the performance of features from different nature (e.g., magnitude, amplitude, phase) for classification was analyzed and tested. Accuracies up to 96.7% were achieved using supervised machine learning for milk freshness classification, when 5 different types of milk were used and up to 100% when just 2% fat milk was used for classification. Additionally, the radio frequency bandwidth needed for classification was reduced to 10 MHz using singular value decomposition (SVD) and boxplot analysis without affecting the classification accuracy for two different methods of feature extraction.",IEEE Sensors Journal,2021,10.1109/JSEN.2020.3013506,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
fcbcade43320f4ad3bdc83e2fd2acd8f977af82a,https://www.semanticscholar.org/paper/fcbcade43320f4ad3bdc83e2fd2acd8f977af82a,Predicting Fraud Victimization Using Classical Machine Learning,"Protecting financial consumers from investment fraud has been a recurring problem in Canada. The purpose of this paper is to predict the demographic characteristics of investors who are likely to be victims of investment fraud. Data for this paper came from the Investment Industry Regulatory Organization of Canada’s (IIROC) database between January of 2009 and December of 2019. In total, 4575 investors were coded as victims of investment fraud. The study employed a machine-learning algorithm to predict the probability of fraud victimization. The machine learning model deployed in this paper predicted the typical demographic profile of fraud victims as investors who classify as female, have poor financial knowledge, know the advisor from the past, and are retired. Investors who are characterized as having limited financial literacy but a long-time relationship with their advisor have reduced probabilities of being victimized. However, male investors with low or moderate-level investment knowledge were more likely to be preyed upon by their investment advisors. While not statistically significant, older adults, in general, are at greater risk of being victimized. The findings from this paper can be used by Canadian self-regulatory organizations and securities commissions to inform their investors’ protection mandates.",Entropy,2021,10.3390/e23030300,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
7098d7c77e141ec72284b7211231922889327699,https://www.semanticscholar.org/paper/7098d7c77e141ec72284b7211231922889327699,A Modified Drake Equation for Assessing Adversarial Risk to Machine Learning Models,"Machine learning models present a risk of adversarial attack when deployed in production. Quantifying the contributing factors and uncertainties using empirical measures could assist the industry with assessing the risk of downloading and deploying common model types. This work proposes modifying the traditional Drake Equation’s formalism to estimate the number of potentially successful adversarial attacks on a deployed model. The Drake Equation is famously used for parameterizing uncertainties and it has been used in many research fields outside of its original intentions to estimate the number of radio-capable extra-terrestrial civilizations. While previous work has outlined methods for discovering vulnerabilities in public model architectures, the proposed equation seeks to provide a semi-quantitative benchmark for evaluating and estimating the potential risk factors for adversarial attacks.",Computer Science & Information Technology (CS & IT),2021,10.5121/CSIT.2021.111001,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
7a5e6ac3d01b7c2f374fafd40930c2c470cc5049,https://www.semanticscholar.org/paper/7a5e6ac3d01b7c2f374fafd40930c2c470cc5049,Performance Evaluation of Machine Learning Predictive Analytical Model for Determining the Job Applicants Employment Status,"Several higher institution of learning faces issue or difficulty of turning out more than 90% of their graduates who can competently satisfy and meet the requirements of the industry. However, the industry is also confronted with the difficulty of sourcing skilled tertiary institution graduates that match their needs. Failure or success of any organization depends mostly on how its workforce is recruited and retained. Therefore, the selection of an acceptable or satisfactory candidate for the job position is one of the major and vital problems of management decision-making. This work, therefore, proposes a modern, accurate and worthy machine learning classification model that can be deployed, implemented, and put to use when making predictions and assessments on job applicant's attributes from their academic performance datasets in other to meet the selection criteria for the industry. Both supervised and unsupervised machine learning classifiers were considered in this work. Naive Bayes, Logistic Regression, support vector machine (SVM). Random Forest and Decision tree performed well, but Logistic Regression outperformed others with 93% accuracy.",,2021,10.37231/MYJAS.2021.6.1.276,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
35c2e7706fd13fd62e2407455e19fee5fbd92741,https://www.semanticscholar.org/paper/35c2e7706fd13fd62e2407455e19fee5fbd92741,Training Flexible Depth Model by Multi-Task Learning for Neural Machine Translation,"The standard neural machine translation model can only decode with the same depth configuration as training. Restricted by this feature, we have to deploy models of various sizes to maintain the same translation latency, because the hardware conditions on different terminal devices (e.g., mobile phones) may vary greatly. Such individual training leads to increased model maintenance costs and slower model iterations, especially for the industry. In this work, we propose to use multi-task learning to train a flexible depth model that can adapt to different depth configurations during inference. Experimental results show that our approach can simultaneously support decoding in 24 depth configurations and is superior to the individual training and another flexible depth model training method——LayerDrop.",FINDINGS,2020,10.18653/v1/2020.findings-emnlp.385,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
50f83f6b72a6b1adb062959e5b0dbbdd068ce8f0,https://www.semanticscholar.org/paper/50f83f6b72a6b1adb062959e5b0dbbdd068ce8f0,Machine Learning in Oil & Gas Industry: A Novel Application of Clustering for Oilfield Advanced Process Control,"
 Data Analytics is an emerging area that involves using advanced statistical and machine learning algorithms to discover information & relationsips present in different types of data. The work described in this paper illustrates the application of machine learning techniques to an Oilfield Advanced Process Control (APC) project involving deployment of APC at a large onshore conventional oilfield in Saudi Aramco. APC implementation enables better control and optimization of the production from hundreds of oilwells.
 APC rollout at the large oilfield involved APC deployment on 300+ oil wells. Using conventional APC implementation methodology, the rollout would be very difficult to manage and would have taken about 3 man years which was not practical. Use of innovative data analytics techniques was essential to ensuring the timely deployment of such a large scale APC project. A machine learning algorithm used to cluster similarly behaving wells, enabled significant (80%) reduction in the engineering effort and operator involvement in developing the models for each well. This allowed the implementation to be completed one year in advance thus realizing the APC benefits earlier than planned.","Day 3 Wed, March 20, 2019",2019,10.2118/194827-MS,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
1c0272322e0d11b6871a198a5e7f015f8af77aff,https://www.semanticscholar.org/paper/1c0272322e0d11b6871a198a5e7f015f8af77aff,Classification of Digital Dental X-ray Images Using Machine Learning,"Dental diseases like dental anomalies, periapical and dental caries is increasing day by day in children and adults. Artificial intelligence and neural network with its application in medical imaging is influencing the healthcare industry. X –ray imaging is the commonly employed technique to diagnose diseases of the teeth. Segmentation and classification of differing dental anomalies using neural network is proving to be a boon to the dental field. Application of neural algorithms aids in obtaining images with better detection accuracy. Automated detection reduces the workload of a dentist with classification being accurate. A better penetration of machine learning into these processes highlights its advantages to classify dental X ray images. Different machine learning techniques are deployed to identify and classify the dental abnormalities.","2021 Seventh International conference on Bio Signals, Images, and Instrumentation (ICBSII)",2021,10.1109/ICBSII51839.2021.9445171,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
b75ebad8a221e56ce81cc8b35b0a3da39c4b51b9,https://www.semanticscholar.org/paper/b75ebad8a221e56ce81cc8b35b0a3da39c4b51b9,A Dark Art: The Machine Learning Labour Process,,"Marx, Engels, and Marxisms",2021,10.1007/978-3-030-71689-9_6,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
63f096677b797ea6b73174ee01df58d56ed41d3d,https://www.semanticscholar.org/paper/63f096677b797ea6b73174ee01df58d56ed41d3d,The Agile Deployment Using Machine Learning in Healthcare Service,,"Proceedings of the International Conference on Paradigms of Computing, Communication and Data Sciences",2020,10.1007/978-981-15-7533-4_70,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
fd787b65bd7fdf7fc28e051acdc3d9310b3173dc,https://www.semanticscholar.org/paper/fd787b65bd7fdf7fc28e051acdc3d9310b3173dc,ANALYSIS AND FORECAST OF COMMODITY PRICE USING MACHINE LEARNING AND DEPLOYMENT IN WEB APPLICATION,"Prediction of financial market accurately is certainly significant. Commodity price fluctuations affects the global economic activity. Earning in export industry rely mainly on primary commodity and the movements of commodity prices has significant impact on overall economic progress for all countries. The method of forecasting plays a vital role in predicting adverse movements in case of commodity price prediction. In today’s world, the growth in deep learning outperforms in several demonstration in fields of financial market analysis. In this paper, we present a productionized commodity price analysis and prediction using LSTM model.  The LSTM model gathers data in a sequential order periodically of the commodity value.  Datasets used in our model are collected in real time from Yahoo Finance via API to reduce local storage.  The critical task is to transform a machine learning model into user accessible real time environment. [2]  Manual conversion of a machine learning model source code into a software is a time-consuming and error-prone task.  Investors and traders can buy and sell commodities directly in the spot market.",,2021,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
48b8fc73c1609d4632d7db5e67e373a62a3cc1f6,https://www.semanticscholar.org/paper/48b8fc73c1609d4632d7db5e67e373a62a3cc1f6,Managing Virtualized Networks and Services with Machine Learning,"Virtualization is instigating a paradigm shift in the networking industry, to keep up with emerging applications quality of service requirements, massive growth in traffic volume, and to reduce capital and operational expenditures. Network virtualization coupled with function virtualization enable network providers to offer on-demand virtualized networks and services. Network slicing goes a step further by facilitating a new business model, namely Networkas-a-Service, to offer dedicated and customized network slices (i.e., partitions of physical infrastructure) to multiple tenants, while ensuring proper isolation. However, this shift introduces new challenges for network providers and calls for intelligent and automated management. Artificial Intelligence and Machine Learning are considered as enablers for the automated deployment and management of virtualized networks and services. This chapter exposes the stateof-the-art research that leverages Artificial Intelligence and Machine Learning to address complex problems in deploying and managing virtualized networks",Communication Networks and Service Management in the Era of Artificial Intelligence and Machine Learning,2021,10.1002/9781119675525.ch3,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
68b6209695745d40d225056554f7da358ac82efd,https://www.semanticscholar.org/paper/68b6209695745d40d225056554f7da358ac82efd,TO MACHINE LEARNING MODELS,"Machine learning models present a risk of adversarial attack when deployed in production. Quantifying the contributing factors and uncertainties using empirical measures could assist the industry with assessing the risk of downloading and deploying common model types. This work proposes modifying the traditional Drake Equation’s formalism to estimate the number of potentially successful adversarial attacks on a deployed model. The Drake Equation is famously used for parameterizing uncertainties and it has been used in many research fields outside of its original intentions to estimate the number of radio-capable extra-terrestrial civilizations. While previous work has outlined methods for discovering vulnerabilities in public model architectures, the proposed equation seeks to provide a semi-quantitative benchmark for evaluating and estimating the potential risk factors for adversarial attacks.",,2021,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
18093980bfec1e5e484fa1e25ca2893bc3c0b3f1,https://www.semanticscholar.org/paper/18093980bfec1e5e484fa1e25ca2893bc3c0b3f1,Concept Drift Handling in Information Systems: Preserving the Validity of Deployed Machine Learning Models,"Predictions computed by supervised machine learning models play a crucial role in a variety of innovative applications in business and industry. Typically, value is generated as soon as these models are deployed and continuously used in information systems of an organization. However, machine learning endeavors predominantly focus on conceiving applications for static situations. In this context, the management of the models’ lifecycle to preserve their effectiveness over time in dynamic environments is still in its infancy. 
 
Therefore, this thesis starts with systematically analyzing the full lifecycle of machine learning applications from an information systems (IS) perspective—and understanding and documenting all choices that have to be made throughout this cycle. On that basis, we then perform a qualitative study via practitioner interviews to map particular challenges in the deployment phase. In this context, we identify concept drift as a particularly important challenge to overcome: Concept drift refers to changes in the environment over time which affect the behavior of a machine learning model. This can have an impact on the model’s prediction quality and its overall utility. 
 
We analyze and categorize concept drift handling approaches covering both the detection of concept drift as well as the appropriate adaptation of the model. We identify two particular research gaps: the handling of concept drift for regression tasks and the handling of concept drift for tasks where additional labels for retraining a model are hard or costly to obtain. For both areas, we develop new methods and demonstrate their effectiveness in technical experiments and real-world use cases. 
 
This thesis contributes new methods to handle concept drift, for particular difficult contexts: First, this thesis should raise researchers’ and practitioners’ awareness for the topic of changing input data over time as well as for its impact on deployed machine learning. Second, the developed and tested methods can either be implemented directly or serve as inspiration to conceive appropriate drift handling strategies within information systems. Finally, we expect that advanced concept drift handling not only technically ensures a reliable prediction quality over time, but that it will also increase trust and acceptance of machine learning-based information systems—and, thus, help to boost the impact of machine learning.",,2021,10.5445/IR/1000137245,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
af700a31228f64920e66d2d71638218892f49228,https://www.semanticscholar.org/paper/af700a31228f64920e66d2d71638218892f49228,Early lessons in deploying cameras and artificial intelligence technology for fisheries catch monitoring: where machine learning meets commercial fishing,"Electronic monitoring (EM) is increasingly used to monitor catch and bycatch in wild capture fisheries. EM video data is still manually reviewed and adds to on-going management costs. Computer vision, machine learning, and artificial intelligence-based systems are seen to be the next step in automating EM data workflows. Here we show some of the obstacles we have confronted, and approaches taken as we develop a system to automatically identify and count target and bycatch species using cameras deployed to an industry vessel. A Convolutional Neural Network was trained to detect and classify target and bycatch species groups, and a visual tracking system was developed to produce counts. The multiclass detector achieved a mean Average Precision of 53.42%. Based on the detection results, the visual tracking system provided automatic fish counts for the test video data. Automatic counts were within two standard deviations of the manual counts for the target species, and most times for the bycatch species. Unlike other recent attempts, weather and lighting conditions were largely controlled by mounting cameras under cover.",Canadian Journal of Fisheries and Aquatic Sciences,2021,10.1139/cjfas-2020-0446,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
303ecb3e6750e6538e5cb7c27d676f283acb4587,https://www.semanticscholar.org/paper/303ecb3e6750e6538e5cb7c27d676f283acb4587,Non-intrusive Embedded Systems Anomaly Detection using Thermography and Machine Learning,"Quality control in electronic system manufacturing is achieved mainly through system testing. Device miniaturization and multilayer Printed Circuit Boards have increased the electronic circuit test complexity considerably and processes based on manual inspections have become outdated and inefficient. The concept of Industry 4.0 has enabled the manufacturing of customized products based on customers’ demands, which demands a high degree of flexibility in production processes, with low cost and without placing numerous test points. In this paper, we propose two automated test solutions based on machine learning and thermographic analysis. We propose deploying autoencoders and random forest in two different manners to detect firmware or hardware anomalies based on the circuit board’s temperature signature. We validate our proposal using two firmware versions running independently on the test board. We obtained an anomaly detection rate above 98%. In the random forest approach, we require all data classes for training, whereas the autoencoder only requires the reference class, which is expected in real scenarios.",Anais do 15. Congresso Brasileiro de Inteligência Computacional,2021,10.21528/cbic2021-20,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
bde1de32917e43b5153686d802a3497720e52fa7,https://www.semanticscholar.org/paper/bde1de32917e43b5153686d802a3497720e52fa7,Acquisition/Processing: Machine learning-based deblending: Dispersed source array data example,"Machine learning (ML) has proven its value in the seismic industry with successful implementations in areas of seismic interpretation such as fault and salt dome detection and velocity picking. The field of seismic processing research also is shifting toward ML applications in areas such as tomography, demultiple, and interpolation. Here, a supervised ML deblending algorithm is illustrated on a dispersed source array (DSA) data example in which both high- and low-frequency vibrators were deployed simultaneously. Training data pairs of blended and corresponding unblended data were constructed from conventional (unblended) data from another survey. From this training data, the method can automatically learn a deblending operator that is used to deblend for both the low- and the high-frequency vibrators of the DSA data. The results obtained on the DSA data are encouraging and show that the ML deblending method can offer a good performing, less user-intensive alternative to existing deblending methods.",The Leading Edge,2021,10.1190/tle40100759.1,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
346192935729c032b83df035506914a0a35dcc60,https://www.semanticscholar.org/paper/346192935729c032b83df035506914a0a35dcc60,Interpreting Machine Learning Models for Room Temperature Prediction in Non-domestic Buildings,"An ensuing challenge in Artificial Intelligence (AI) is the perceived difficulty in interpreting sophisticated machine learning models, whose ever-increasing complexity makes it hard for such models to be understood, trusted and thus accepted by human beings. The lack, if not complete absence, of interpretability for these so-called black-box models can lead to serious economic and ethical consequences, thereby hindering the development and deployment of AI in wider fields, particularly in those involving critical and regulatory applications. For example, managing building energy consumption efficiently is paramount to tackling issues of global warming and shortage of energy resources. Yet, the building services industry is a highly-regulated domain requiring transparency and decision-making processes that can be understood and trusted by humans. To this end, the design and implementation of autonomous Heating, Ventilation and Air Conditioning (HVAC) systems for the automatic but concurrently interpretable optimisation of energy efficiency and room thermal comfort is of topical interest. This work therefore presents an interpretable machine learning model aimed at predicting room temperature (RT) in non-domestic buildings, for the purpose of optimising the use of the installed HVAC system. We demonstrate experimentally that the proposed model can accurately forecast room temperatures eight hours ahead in real-time by taking into account historical RT information, as well as additional environmental and time-series features. In this paper, an enhanced feature engineering process is conducted based on the Exploratory Data Analysis (EDA) results. Furthermore, beyond the commonly used Interpretable Machine Learning (IML) techniques, we propose a Permutation Featurebased Frequency Response Analysis (PF-FRA) method for quantifying the contributions of the different predictors in the frequency domain. Based on the generated reason codes, we find that the historical RT feature is the dominant factor that has most impact on the model’s prediction.",ArXiv,2021,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
dfc09849e6e9481a6ad1441b4409cc271cadfb63,https://www.semanticscholar.org/paper/dfc09849e6e9481a6ad1441b4409cc271cadfb63,Machine Learning Explainability for External Stakeholders,"As machine learning is increasingly deployed in high-stakes contexts affecting people's livelihoods, there have been growing calls to open the black box and to make machine learning algorithms more explainable. Providing useful explanations requires careful consideration of the needs of stakeholders, including end-users, regulators, and domain experts. Despite this need, little work has been done to facilitate inter-stakeholder conversation around explainable machine learning. To help address this gap, we conducted a closed-door, day-long workshop between academics, industry experts, legal scholars, and policymakers to develop a shared language around explainability and to understand the current shortcomings of and potential solutions for deploying explainable machine learning in service of transparency goals. We also asked participants to share case studies in deploying explainable machine learning at scale. In this paper, we provide a short summary of various case studies of explainable machine learning, lessons from those studies, and discuss open challenges.",ArXiv,2020,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
4f61970a283ad91cdf72769b2311b39aecda3417,https://www.semanticscholar.org/paper/4f61970a283ad91cdf72769b2311b39aecda3417,Advances in industrial biopharmaceutical batch process monitoring: Machine‐learning methods for small data problems,"Biopharmaceutical manufacturing comprises of multiple distinct processing steps that require effective and efficient monitoring of many variables simultaneously in real‐time. The state‐of‐the‐art real‐time multivariate statistical batch process monitoring (BPM) platforms have been in use in recent years to ensure comprehensive monitoring is in place as a complementary tool for continued process verification to detect weak signals. This article addresses a longstanding, industry‐wide problem in BPM, referred to as the “Low‐N” problem, wherein a product has a limited production history. The current best industrial practice to address the Low‐N problem is to switch from a multivariate to a univariate BPM, until sufficient product history is available to build and deploy a multivariate BPM platform. Every batch run without a robust multivariate BPM platform poses risk of not detecting potential weak signals developing in the process that might have an impact on process and product performance. In this article, we propose an approach to solve the Low‐N problem by generating an arbitrarily large number of in silico batches through a combination of hardware exploitation and machine‐learning methods. To the best of authors’ knowledge, this is the first article to provide a solution to the Low‐N problem in biopharmaceutical manufacturing using machine‐learning methods. Several industrial case studies from bulk drug substance manufacturing are presented to demonstrate the efficacy of the proposed approach for BPM under various Low‐N scenarios.",Biotechnology and bioengineering,2018,10.1002/bit.26605,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
553065f0b3f297c7b3a207e39a066d892b2be30c,https://www.semanticscholar.org/paper/553065f0b3f297c7b3a207e39a066d892b2be30c,Condition Monitoring Based on Partial Discharge Diagnostics Using Machine Learning Methods: A Comprehensive State-of-the-Art Review,"This paper presents a state-of-the-art review on machine learning (ML) based intelligent diagnostics that have been applied for partial discharge (PD) detection, localization, and pattern recognition. ML techniques, particularly those developed in the last five years, are examined and classified as conventional ML or deep learning (DL). Important features of each method, such as types of input signal, sampling rate, core methodology, and accuracy, are summarized and compared in detail. Advantages and disadvantages of different ML algorithms are discussed. Moreover, technical roadblocks preventing intelligent PD diagnostics from being applied to industry are identified, such as insufficient/imbalanced dataset, data inconsistency, and difficulties in cost-effective real-time deployment. Finally, potential solutions are proposed, and future research directions are suggested.",IEEE Transactions on Dielectrics and Electrical Insulation,2020,10.1109/TDEI.2020.009070,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
664a3afe20076650aa04665d87efdf888e2c815d,https://www.semanticscholar.org/paper/664a3afe20076650aa04665d87efdf888e2c815d,"Interpretability Versus Accuracy: A Comparison of Machine Learning Models Built Using Different Algorithms, Performance Measures, and Features to Predict E. coli Levels in Agricultural Water","Since E. coli is considered a fecal indicator in surface water, government water quality standards and industry guidance often rely on E. coli monitoring to identify when there is an increased risk of pathogen contamination of water used for produce production (e.g., for irrigation). However, studies have indicated that E. coli testing can present an economic burden to growers and that time lags between sampling and obtaining results may reduce the utility of these data. Models that predict E. coli levels in agricultural water may provide a mechanism for overcoming these obstacles. Thus, this proof-of-concept study uses previously published datasets to train, test, and compare E. coli predictive models using multiple algorithms and performance measures. Since the collection of different feature data carries specific costs for growers, predictive performance was compared for models built using different feature types [geospatial, water quality, stream traits, and/or weather features]. Model performance was assessed against baseline regression models. Model performance varied considerably with root-mean-squared errors and Kendall’s Tau ranging between 0.37 and 1.03, and 0.07 and 0.55, respectively. Overall, models that included turbidity, rain, and temperature outperformed all other models regardless of the algorithm used. Turbidity and weather factors were also found to drive model accuracy even when other feature types were included in the model. These findings confirm previous conclusions that machine learning models may be useful for predicting when, where, and at what level E. coli (and associated hazards) are likely to be present in preharvest agricultural water sources. This study also identifies specific algorithm-predictor combinations that should be the foci of future efforts to develop deployable models (i.e., models that can be used to guide on-farm decision-making and risk mitigation). When deploying E. coli predictive models in the field, it is important to note that past research indicates an inconsistent relationship between E. coli levels and foodborne pathogen presence. Thus, models that predict E. coli levels in agricultural water may be useful for assessing fecal contamination status and ensuring compliance with regulations but should not be used to assess the risk that specific pathogens of concern (e.g., Salmonella, Listeria) are present.",Frontiers in Artificial Intelligence,2021,10.3389/frai.2021.628441,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
f6260978f30443388f4df5f7ab7c267a06edb750,https://www.semanticscholar.org/paper/f6260978f30443388f4df5f7ab7c267a06edb750,A Standardized Representation of Convolutional Neural Networks for Reliable Deployment of Machine Learning Models in the Manufacturing Industry,"
 The use of deep convolutional neural networks is becoming increasingly popular in the engineering and manufacturing sectors. However, managing the distribution of trained models is still a difficult task, partially due to the limitations of standardized methods for neural network representation. This paper seeks to address this issue by proposing a standardized format for convolutional neural networks, based on the Predictive Model Markup Language (PMML). A number of pre-trained ImageNet models are converted to the proposed PMML format to demonstrate the flexibility and utility of this format. These models are then fine-tuned to detect casting defects in Xray images. Finally, a scoring engine is developed to evaluate new input images against models in the proposed format. The utility of the proposed format and scoring engine is demonstrated by benchmarking the performance of the defect-detection models on a range of different computation platforms. The scoring engine and trained models are made available at https://github.com/maxkferg/python-pmml.",Volume 1: 39th Computers and Information in Engineering Conference,2019,10.1115/DETC2019-97095,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
8651d5edabb8a35f8643dee58ee6e22be759420a,https://www.semanticscholar.org/paper/8651d5edabb8a35f8643dee58ee6e22be759420a,A work load prediction strategy for power optimization on cloud based data centre using deep machine learning,,Evol. Intell.,2019,10.1007/s12065-019-00289-4,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
52670f97f7956e248972c4a940830be9598bb80a,https://www.semanticscholar.org/paper/52670f97f7956e248972c4a940830be9598bb80a,Machine-learning-based telemetry for monitoring long-haul optical transmission impairments: methodologies and challenges [Invited],"Current management of optical communication systems is conservative, manual-based, and time-consuming. To improve this situation, building an intelligent closed-loop control system is becoming an active topic of the industry. One of the key techniques to achieve such a management system is physical layer impairment telemetry, with the help of which the controller can make proper instructions. However, it is challenging to implement an accurate telemetry module due to the complex mechanisms of various impairments. To overcome that, many studies have been done. In this paper, those recent studies are reviewed, and the design of telemetry is discussed systematically. We analyze metrics for evaluating system performance and mechanisms of various impairments comprehensively, which are the theoretical foundations for designing telemetry modules. We then summarize a unified workflow for designing telemetry modules based on the review of previous works. Its effectiveness is then verified by concrete use cases of our previous studies. Finally, we discuss the challenges of deploying machine-learning-based telemetry techniques in optical communication systems.",IEEE/OSA Journal of Optical Communications and Networking,2021,10.1364/JOCN.426826,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
805233aaaf5174eaf6ea7a157738d17169682037,https://www.semanticscholar.org/paper/805233aaaf5174eaf6ea7a157738d17169682037,Machine Learning-Based Soft Sensors for Vacuum Distillation Unit,"Introduction Product quality assessment in the petroleum processing industry, such as crude distillation, can be difficult and time-consuming, e.g. due to a manual collection of liquid samples from the plant and the subsequent chemical laboratory analysis of the samples. The product quality is an important property that informs whether the products of the process are within the regulated specifications, such as ASTM Petroleum Standards. In particular, the delays caused by sample processing (collection, laboratory measurements, results analysis, reporting, etc.) can lead to detrimental economic effects. One of the strategies to deal with this problem is so-called soft sensors. Soft sensors are a collection of models that can be used to predict and forecast some infrequently measured properties (such as laboratory measurements of petroleum products) based on more frequent measurements of quantities like temperature, pressure and flow rate provided by physical sensors [1]. Soft sensors short-cut the pathway to obtain relevant information about the product quality, often providing relevant measurements as frequently as every minute. One of the applications of soft sensors is for the real-time optimization of a chemical process by a targeted adaptation of operating parameters. Models used for soft sensors can have various forms, however, among the most common are those based on artificial neural networks (ANNs) [2]. While soft sensors can deal with some of the issues in the refinery processes, their development and deployment can pose other challenges that are addressed in this paper. Firstly, it is important to enhance the quality of both sets of data (laboratory measurements and physical sensors) in a so-called data pre-processing stage (as described in Methodology section) [3]. Secondly, once the data sets are preprocessed, different models need to be tested against prediction error and the model’s interpretability. In this work, we present a framework for soft sensor development from raw data to ready-to-use models.",ArXiv,2021,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
cef916fe990163637163530c6e243104f8eff230,https://www.semanticscholar.org/paper/cef916fe990163637163530c6e243104f8eff230,Suggesting Waypoints to Autonomous Surface Vessels for Effective Emergency Response Using Machine Learning,"With the advancements in the maritime industry, which delivers almost 90 percent of the world trade, the frequency of maritime activities has drastically increased resulting a major concern in maritime safety. A significant 30 percent of maritime accidents are caused due to bad weather conditions, for instance sea storms and strong winds created due to high turbulence and waves. The deaths and casualties caused due to these accidents would have been minimized if there was a mechanism for efficient emergency response. Autonomous Surface Vessels (ASVs) have been used for several disaster mitigation and recovery operations in hurricanes, earthquakes and tsunami. ASVs are comparatively cheap and safe to be deployed on to hazardous zones in the deep sea due to their long term marine presence. A more efficient way for emergency response by the ASVs would be, the ability to predict a location where there is a possibility for an accident to take place and position itself such that it could effectively respond to the emergency. Hence the author is proposing an optimal solution using machine learning techniques to suggest waypoints to ASVs for effective emergency response on human operated surface vessels.",OCEANS 2021: San Diego – Porto,2021,10.23919/OCEANS44145.2021.9705726,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
a0c0ad35b950d758467eee6dd639c4be13104031,https://www.semanticscholar.org/paper/a0c0ad35b950d758467eee6dd639c4be13104031,AUTOMATIC LOGGING-WHILE-DRILLING DIPOLE SONIC SHEAR PROCESSING ENABLED BY PHYSICS-DRIVEN MACHINE LEARNING,"Logging-while-drilling (LWD) dipole sonic tools have been introduced to the industry as a supplement to monopole and quadrupole measurement because they can provide shear slowness anisotropy, which is essential for formation characterization and well completion applications. Due to the presence of the collar, which acts as a strong waveguide, the recorded formation signal is significantly affected at low frequencies. Consequently, an automated interpretation of LWD dipole sonic data re-mains a challenge. The traditional dispersive semblance-based method requires accurate estimates of parameters such as borehole size and/or mud slowness to avoid bias in the dispersion model used in the processing. Recently, a frequency-slowness domain inversion scheme has been developed that can invert for both the formation shear slowness and mud slowness by minimizing the guidance-mismatch cost function. However, this method uses an isotropic dispersion model and requires selecting narrow-band dispersion data in the low-frequency range with good-quality, which can limit the range of applicability of the method and also requires user input through-out the process. We have previously developed a physics-driven machine learning-based method to enhance the interpretation of wireline dipole sonic data. However, the LWD scenario introduces additional complexity. This work extends the method to support the interpretation of LWD dipole sonic. An anisotropic root-finding mode-search algorithm is first used to generate extensive synthetic formation flexural dispersion curves that can match dispersion measurements in strong anisotropic formations in high-angle and horizontal wells, with a known tool model. Special care needs to be taken to pick the formation flexural mode from several co-existing modes arising from the strong coupling between tool and formation. After quality control and verification, this comprehensive synthetic dataset is used to train a neural network model. We then develop an inversion-based algorithm, taking advantage of this efficient neural network model and combining it with a clustering algorithm, to reliably label and ex-tract the formation flexural mode, processed from either the modified Prony’s method, or a broadband dispersion analysis algorithm. The extraction around the formation flexural kick-in frequency is used for developing a quality control method. The strongest collar arrival, on the other hand, can be confidently removed due to the fundamental difference in its dispersion characteristics from the formation flexural mode. This novel method can automatically and efficiently label the formation flexural mode and simultaneously invert it for formation shear slowness together with other relevant parameters such as mud slowness without user intervention. Since this method is built upon an anisotropic model, it can be applied to the full frequency range of the data spectrum without the traditional isotropic model assumption. Additionally, the regression analysis of the inverted mud slownesses can further provide physical constraint to reduce uncertainties in the inverted shear slowness. The algorithm has been tested on field data showing good performance. It makes edge deployment possible so that LWD telemetry can be optimized to transmit the processed data to the surface in real-time, which is essential to leverage the advantages of the conveyance method.",,2021,10.30632/SPWLA-2021-0059,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
d1c468ca203f770e511d568f6e38b4306c4941ca,https://www.semanticscholar.org/paper/d1c468ca203f770e511d568f6e38b4306c4941ca,Natural Disaster Prediction by Using Image Based Deep Learning and Machine Learning,,Lecture Notes in Networks and Systems,2021,10.1007/978-3-030-84760-9_6,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
d0d453eb2a9b94ddd040b772c0af151dadeb593d,https://www.semanticscholar.org/paper/d0d453eb2a9b94ddd040b772c0af151dadeb593d,Research on Classification of Intrusion Detection in Internet of Things Network Layer Based on Machine Learning,"The emergence of the Internet of Things (IoT) is not only a global revolution in the information industry, but also brought tremendous changes to our lives. With the development of the technology and means of the IoT, information security issues have gradually emerged, and intrusion attacks have become one of the main problems of the IoT network security. The network layer of the IoT is the key connecting the platform and sensors or controllers of the IoT, and it is also the most standardized, the strongest and the most mature part of the whole physical network architecture. Its large-scale development has led to the network layer's security issues will receive more attention and face more challenges. This paper proposes an intrusion detection algorithm deployed on the network layer of the IoT, which uses the BPSO algorithm to extract features from the NSL-KDD dataset, and applies support vector machines (SVM) as the core model of the algorithm to detect and identify abnormal data, especially DoS attacks. Experimental results show that the model's detection rate of abnormal data and DoS attacks are significantly improved.",2021 IEEE International Conference on Intelligence and Safety for Robotics (ISR),2021,10.1109/ISR50024.2021.9419529,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
3fee4505bdcc8525a68f341467fa1bd9a9b11d02,https://www.semanticscholar.org/paper/3fee4505bdcc8525a68f341467fa1bd9a9b11d02,A Machine Learning-Based Strategy for Efficient Resource Management of Video Encoding on Heterogeneous MPSoCs,"The design of new streaming systems is becoming a major area of research to deploy services targeted in the Internet-of-Things (IoT) era. In this context, the new High Efficiency Video Coding (HEVC) standard provides high efficiency and scalability of quality at the cost of increased computational complexity for edge nodes, which is a new challenge for the design of IoT systems. The usage of hardware acceleration in conjunction with general-purpose cores in Multiprocessor Systems-on-Chip (MP-SoCs) is a promising solution to create heterogeneous computing systems to manage the complexity of real-time streaming for high-end IoT systems, achieving higher throughput and power efficiency when compared to conventional processors alone. Furthermore, Machine Learning (ML) provides a promising solution to efficiently use this next-generation of heterogeneous MPSoC designs that the EDA industry is developing by dynamically optimizing system performance under diverse requirements such as frame resolution, search area, operating frequency and stream allocation. In this work, we propose an ML-based approach for stream allocation and Dynamic Voltage and Frequency Scaling (DVFS) management on a heterogeneous MPSoC composed of ARM cores and FPGA fabric containing hardware accelerators for the motion estimation of HEVC encoding. Our experiments on a Zynq7000 SoC outline 20% higher throughput when compared to the state-of-the-art streaming systems for next-generation IoT devices.",2018 IEEE International Symposium on Circuits and Systems (ISCAS),2018,10.1109/ISCAS.2018.8351785,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
e106073bb94b64de0241c380097bce18d403ee3d,https://www.semanticscholar.org/paper/e106073bb94b64de0241c380097bce18d403ee3d,Attack Detection for Wireless Enterprise Network: a Machine Learning Approach,"An increasing number of enterprises are adopting wireless technology to deploy networks. However, wireless enterprise networks are more vulnerable than wired networks because of the broadcast feature. Thus, illegal attacks such as data theft and information forgery seriously threaten the property and information security of users and enterprises; these phenomena are attracting increasing attention from both academia and industry. Additionally, effectively detecting the attacks in the wireless enterprise networks is one of todays most important and challenging problems, especially in Wi-Fi networks, as attacks become increasingly covert and diverse. Fortunately, WiFi networks produce large amounts of data, providing copious big data for researchers. In this paper, using the Aegean Wi-Fi Intrusion Dataset (AWID), which is derived from the real-world Wi-Fi network, we introduce machine learning to detect network attacks. To significantly increase the training and convergence speeds, we deploy two-dimensional data cleaning and select 18 useful attributes from the original set of 154. Then, we introduce support vector machine (SVM) to detect attacks based on the cleaned dataset. The detection accuracy for flooding attacks, injection attacks, and normal data reached 89.18%, 87.34%, and 99.88% respectively. To the best of our knowledge, this is the first study to introduce a two-dimensional data cleaning method with an SVM to improve the detection accuracy for attacks. Finally, our detection results are comparable with the existing studies; however, our method operates with simpler data attributes with faster and more efficient training speed.","2018 IEEE International Conference on Signal Processing, Communications and Computing (ICSPCC)",2018,10.1109/ICSPCC.2018.8567797,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
0fdc9fc8ecff787abaaee53680b8d91e4bf9d3e9,https://www.semanticscholar.org/paper/0fdc9fc8ecff787abaaee53680b8d91e4bf9d3e9,One Explanation Does Not Fit All: The Promise of Interactive Explanations for Machine Learning Transparency,"The need for transparency of predictive systems based on Machine Learning algorithms arises as a consequence of their ever-increasing proliferation in the industry. Whenever black-box algorithmic predictions influence human affairs, the inner workings of these algorithms should be scrutinised and their decisions explained to the relevant stakeholders, including the system engineers, the system’s operators and the individuals whose case is being decided. While a variety of interpretability and explainability methods is available, none of them is a panacea that can satisfy all diverse expectations and competing objectives that might be required by the parties involved. We address this challenge in this paper by discussing the promises of Interactive Machine Learning for improved transparency of black-box systems using the example of contrastive explanations – a state-of-the-art approach to Interpretable Machine Learning. Specifically, we show how to personalise counterfactual explanations by interactively adjusting their conditional statements and extract additional explanations by asking follow-up “What if?” questions. Our experience in building, deploying and presenting this type of system allowed us to list desired properties as well as potential limitations, which can be used to guide the development of interactive explainers. While customising the medium of interaction, i.e., the user interface comprising of various communication channels, may give an Kacper Sokol Department of Computer Science, University of Bristol Bristol, United Kingdom E-mail: K.Sokol@bristol.ac.uk Peter Flach Department of Computer Science, University of Bristol Bristol, United Kingdom E-mail: Peter.Flach@bristol.ac.uk impression of personalisation, we argue that adjusting the explanation itself and its content is more important. To this end, properties such as breadth, scope, context, purpose and target of the explanation have to be considered, in addition to explicitly informing the explainee about its limitations and caveats. Furthermore, we discuss the challenges of mirroring the explainee’s mental model, which is the main building block of intelligible human-machine interactions. We also deliberate on the risks of allowing the explainee to freely manipulate the explanations and thereby extracting information about the underlying predictive model, which might be leveraged by malicious actors to steal or game the model. Finally, building an end-to-end interactive explainability system is a challenging engineering task; unless the main goal is its deployment, we recommend “Wizard of Oz” studies as a proxy for testing and evaluating standalone interactive explainability algorithms.",ArXiv,2020,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
043454093561e9d4880a095a4cfe83c7ea74dab2,https://www.semanticscholar.org/paper/043454093561e9d4880a095a4cfe83c7ea74dab2,Work-in-Progress: Making Machine Learning Real-Time Predictable,"Machine learning (ML) on edge computing devices is becoming popular in the industry as a means to make control systems more intelligent and autonomous. The new trend is to utilize embedded edge devices, as they boast higher computational power and larger memories than before, to perform ML tasks that had previously been limited to cloud-hosted deployments. In this work, we assess the real-time predictability and consider data privacy concerns by comparing traditional cloud services with edge-based ones for certain data analytics tasks. We identify the subset of ML problems appropriate for edge devices by investigating if they result in real-time predictable services for a set of widely used ML libraries. We specifically enhance the Caffe library to make it more suitable for real-time predictability. We then deploy ML models with high accuracy scores on an embedded system, exposing it to industry sensor data from the field, to demonstrates its efficacy and suitability for real-time processing.",2018 IEEE Real-Time Systems Symposium (RTSS),2018,10.1109/RTSS.2018.00029,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
accddd1e2e3c72aa1d96abb78113c49f17e39bc4,https://www.semanticscholar.org/paper/accddd1e2e3c72aa1d96abb78113c49f17e39bc4,WHITE PAPER ACHIEVE COMPLETE AUTOMATION WITH ARTIFICIAL INTELLIGENCE AND MACHINE LEARNING,"As agile models become more prominent in software development, testing teams must shift from slow manual testing to automated validation models that accelerate the time to market. Currently, automation test suites are valid only for some releases, placing greater pressure on testing teams to revamp test suites, so they can keep pace with frequent change requests. To address these challenges, artificial intelligence and machine learning (AI/ML) are emerging as viable alternatives to traditional automation test suites. This paper examines the existing challenges of traditional testing automation. It also discusses five use-cases and solutions to explain how AI/ML can resolve these challenges while providing complete and intelligent automation with little or no human intervention, enabling testing teams to become truly agile. Industry reports reveal that many enterprise initiatives aiming to completely automate quality assurance (QA) fail due to various reasons resulting in low motivation to adopt automation. It is, in fact, the challenges involved in automating QA that have prevented its evolution into a complete automation model. Despite the challenges, automation continues to be a popular initiative in today’s digital world. Testing communities agree that a majority of validation processes are repetitive. While traditional automation typically checks whether things work as they are supposed to, the advent of new technologies like artificial intelligence and machine learning (AI/ML) can support the evolution of QA into a completely automated model that requires minimal or no human intervention. Pain points of Complete Automation Introduction Let us look at the most pertinent problems that lead to low automation statistics: • Frequent requirement changes – While most applications are fluid and revised constantly, the corresponding automation test suite is not. Keeping up with changing requirements manually impedes complete automation. Moreover, maintaining automated test suites becomes increasingly complicated over time, particularly if there are frequent changes in the application under test (AUT) • Mere scripting is not automation – Testing teams must evolve beyond traditional test automation that involves frequent manual script-writing. • Inability to utilize reusable assets – It is possible to identify reusable components only after a few iterations of test release cycles. However, modularizing these in a manner that can be reused everywhere is a grueling task. • Talent scarcity – Finding software development engineers in test (SDETs) with the right mix of technical skills and QA mindset is a significant challenge QA teams today are looking for alternatives to the current slow and manual process of creating test scripts using existing methodologies. It is evident that intelligent automation (automation leveraging AI/ML) is the need of the hour. External Document © 2020 Infosys Limited External Document © 2020 Infosys Limited A. Automation test suite creation How can enterprises achieve complete automation in testing? Use cases and solutions for intelligent test automation The key to achieving complete automation lies in using AI/ML as an automation lever instead of relegating it to scripting. Optimizing manual test cases using AI/ML is a good start. Helping the application selflearn and identify test suites with reusable Use case 1: Optimizing a manual test suite Testing teams typically have a large set of manual test cases for regression testing, which are written by many people over a period of time. Consequently, this leads to overlapping cases. This increases the burden on automation experts when creating the automation test suite. Moreover, as the test case suite grows larger, it becomes difficult to find unique test cases, leading to increased execution effort and cost. Solution 1: Use a clustering approach to reduce effort and duplication A clustering approach can be used to group similar manual test cases. This helps teams easily recognize identical test cases, thereby reducing the size of the regression suite without the risk of missed coverage. During automation, only the most optimized test cases are considered, with significant effort reduction and eliminating duplicates. Use case 2: Converting manual test cases into automated test scripts Test cases are recorded or written manually in different formats based on the software test lifecycle (STLC) model, which can be either agile or waterfall. Sometimes, testers can record audio test cases instead of typing those out. They also use browserbased recorders to capture screen actions while testing. Solution 2: Use natural language processing (NLP) In the above use case, the execution steps and scenarios are clearly defined, assets can be more advanced utilities for automated test suite creation. Leveraging AI in test suite automation falls into two main categories – ‘automation test suite creation using various inputs’ after which the tester interprets the test cases, designs an automation framework and writes automation scripts. This entire process consumes an enormous amount of time and effort. With natural language processing (NLP) and pattern identification, manual test cases can be transformed into ready-to-execute automation scripts and, furthermore, reusable business process components can be easily identified. This occurs in three simple steps: • Read – Using NLP to convert text into the automation suite. • Review – Reviewing the automation suite generated. • Reuse – Using partially supervised ML techniques and pattern discovery algorithms to identify and modularize reusable components that can be plugged in anywhere, anytime and for any relevant scenario. and ‘automation test suite repository maintenance’. The following section discusses various use cases for intelligent automation solutions under these two categories which address the challenges of end-to-end automation. All these steps can be implemented in a tool-agnostic manner until the penultimate stage. Testers can review the steps and add data and verification points that are learnt by the system. Eventually, these steps and verification points are used to generate automation scripts for the tool chosen by the test engineer (Selenium, UFT or Protractor) only at the final step. In this use case, AI/ML along with a toolagnostic framework helps automatically identify tool-agnostic automation steps and reusable business process components (BPCs). The automation test suite thus created is well-structured with easy maintainability, reusability and traceability for all components. The solution slashes the planning and scripting effort when compared to traditional mechanisms. External Document © 2020 Infosys Limited External Document © 2020 Infosys Limited Use case 3: Achieving intelligent DevOps Software projects following the DevOps model usually have a continuous integration (CI) pipeline. This is often enabled with auto-deploy options, test data and API logs with their respective requests-response data or application logs from the DevOps environment. While unit tests are available by default, building integration test cases requires additional effort. Solution 3: Use AI/ML in DevOps automation By leveraging AI/ML, DevOps systems gain analytics capabilities (becoming ‘intelligent DevOps’) in addition to transforming manual cases to automated scripts. Fig 1: Achieving complete automation on a DevOps model with AI/ML AI Components Optimized Test Cases Automated Unit Tests Virtual Service / Component Test Cases Functional Test Scenarios SMART DIAGNOSTIC ANALYTICS Recovery Techniques Test Reports / Defect Logs Application Logs Test Reports / Defect Logs Request -Response Data / Test Data Develop Test Deploy",,2020,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
e25e50f7da89be55fd607eee7b06113cbc00b13a,https://www.semanticscholar.org/paper/e25e50f7da89be55fd607eee7b06113cbc00b13a,End to end CI/CD pipeline for Machine Learning,"In every industry machine learning applications are becoming popular, however, compared to traditional software applications the process fo developing, deploying, and continuously improving for machine learning applications is more complex. In industry practice continuous integration, delivery, and deployment enable organizations to release new features in their products frequently. For engineering processes of developing and designing secure pipelines to support continuous practices, how machine learning systems should be architected to gain a deep understanding in the process, and how to capture, improve and report data into different aspects of continuous integration, delivery, and deployment. Without proper pipeline for machine learning it is hard to predict, test, explain, and improve data workflow behavior. Pipelining in machine learning bringing different principles and practices to machine learning applications to work in a proper manner. In the industrial sector consequences of an irregular pipeline can cause financial, resource, and time will get wasted and some times it can indirectly influence companies' personal reputation in the market. This paper discusses the problems experience while building a machine learning pipeline and ultimately describe the framework to implement the problems in the workflow. Methodically reviewing the state of the art of continuous execution to organize approaches and tools, recognize challenges and practices. As a result, the machine learning pipeline reduces the gaps and increases the speed of experimentation in the workflow.",,2020,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
718a172417ac8cdb7a19e1f105b03024f3cdfc3b,https://www.semanticscholar.org/paper/718a172417ac8cdb7a19e1f105b03024f3cdfc3b,"Establishing fast, practical, full-chip ILT flows using machine learning","Since its introduction at Luminescent Technologies and continued development at Synopsys, Inverse Lithography Technology (ILT) has delivered industry leading quality of results (QOR) for mask synthesis designs. With the advent of powerful, widely deployed, and user-friendly machine learning (ML) training techniques, we are now able to exploit the quality of ILT masks in a ML framework which has significant runtime benefits. In this paper we will describe our MLILT flow including training data selection and preparation, network architectures, training techniques, and analysis tools. Typically, ILT usage has been limited to smaller areas owing to concerns like runtime, solution consistency, and mask shape complexity. We will exhibit how machine learning can be used to overcome these challenges, thereby providing a pathway to extend ILT solution to full chip logic design. We will demonstrate the clear superiority of ML-ILT QOR over existing mask synthesis techniques, such as rule based placements, that have similar runtime performance.",,2020,10.1117/12.2551425,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
2c58a1ac0c1ebdbac07b8def7ea98d5448a4249a,https://www.semanticscholar.org/paper/2c58a1ac0c1ebdbac07b8def7ea98d5448a4249a,Predictive and Explainable Machine Learning for Industrial Internet of Things Applications,"Predictive Analytics and Machine Learning (ML) are at the heart of some of the most popular Industry 4.0 applications such as condition-based monitoring, predictive maintenance, and quality management. To support the implementation of such use cases, various ML models have been proposed and validated in the research literature. This paper introduces a novel set of machine learning algorithms for Industry4.0 use cases, namely the QARMA algorithms, which are capable of mining of quantitative rules. QARMA models present several advantages when compared to conventional ML and Deep Learning mechanisms, including computational performance, predictive accuracy and ""explainability"". In the scope of this paper, we discuss these advantages based on practical experiences from the field deployment and validation of QARMA models in two different production lines. The deployment has been supported by a state-of-the-art Industrial Internet of Things platform, which is also presented in the paper.",2020 16th International Conference on Distributed Computing in Sensor Systems (DCOSS),2020,10.1109/DCOSS49796.2020.00043,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
19552c33b6144ba9cf02b52310cfdccdc66b14f2,https://www.semanticscholar.org/paper/19552c33b6144ba9cf02b52310cfdccdc66b14f2,Empirical observation of negligible fairness-accuracy trade-offs in machine learning for public policy,,Nat. Mach. Intell.,2020,10.1038/s42256-021-00396-x,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
ff86635c206945d3188dff4b27163e23f930ac40,https://www.semanticscholar.org/paper/ff86635c206945d3188dff4b27163e23f930ac40,Machine Learning Approach to Estimation of Internal Parameters of a Single Phase Transformer,"Transformer is the heart of power industry. So, it is necessary to track its performance every instant. Internal parameters of any transformer helps evaluate the performance and improve output waveform, raising efficiency[1]. They also state the condition of the internal windings[2]. It requires offloading of transformer to evaluate its internal parameters which is very costly since transformer needs to operate always to fully recover the investment. Many iterative analysis techniques have been proposed by many researchers in the past. Machine Learning is the current state of the art technology whose application is proposed in this paper. Various statistical methods are deployed in the training of such algorithms which comprises of Linear Regression, Stochastic Gradient Decent, Support Vector Machine and many more[3]. But, real time applications require minimum latency so Levenberg-Marquardt algorithm has been proposed for evaluation with higher accuracy.",2020 International Conference for Emerging Technology (INCET),2020,10.1109/incet49848.2020.9154161,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
22a74a3208d70bcbd7b716cbfba81b4552082f7f,https://www.semanticscholar.org/paper/22a74a3208d70bcbd7b716cbfba81b4552082f7f,Machine Learning Interpretability Application to Optimize Well Completion in Montney,"Recently machine learning has being extensively deployed for oil and gas industry for improving result and expedite process. However, the black box models do not explain their prediction which considered as a barrier to adopt machine learning. This paper is about optimizing hydraulic fracture with machine learning methods and making informative decision with interpreting machine learning model. The solution can show that it could save over million dollars per well and improve well performance significantly. Interestingly, the machine leaning explainability approach was utilized to explain and measure the reason behind of why some wells are performing better than other and vice versa.Hydraulic fracturing modeling and optimization in tight oil and unconventional reservoir requires substantial geological modeling, fracture design, post-fracture production simulation with excessive sensitivity analysis due to complexity and uncertainty in the nature of data. These types of studies are computationally and monetarily expensive. Furthermore, digital oil technology has facilitated the process of data gathering enabled operators to have access to huge amount of data. Common approaches are no longer suitable to handle this pile of data but machine learning methods could be successfully utilized for this purpose.In this paper, a variety types of advanced machine learning methods including linear regression, Random forest, Gradient Boost, XGBoost, Bagging, ExtraTrees and neural network were employed to optimize well completion in Montney formation. The objective was to create a robust predictive model capturing all the effective operational well parameters (features) capable of optimizing the first 12 months cumulative of equivalent well production.Special Individual Conditional Expectation (ICE) plots and Partial Dependency plots(PDP) were used to depict how HF completion features influence the prediction of a machine learning model. Furthermore, a novel approach was employed to explain the model prediction of an existing well by computing the contribution of each feature to the prediction.Over 1838 hydraulically fractured (HF) wells producing from 2008 till 2019 in Montney formation have been considered for this analysis. The outcome of Explanatory Data Analysis (EDA) revealed that well production performance has not been improved despite of continues enhancement of hydraulic fracture parameters such as proppant injected volume, length of stimulated horizontal wells, and number of stages per well in the course of two years. This finding raises the concern of whether operators are properly optimizing completion design. After comparing all machine learning methods, Random Forest method was chosen as the most appropriate and accurate method to proceed for further analysis. ICE and PDP plots helped to understand the impacts of different fracturing features on production for individual well in addition to define optimum operation features on Montney Formation. Furthermore, quantifying of each feature’s impact on individual well production and linking it to an economic model, we were able to demonstrate potential profit and loss for each well. The model suggests that some wells could have achieved over $1 million extra profit during the first 12-months of production.In this study, not only a reliable predictive data-driven model has been built for hydraulically-fractured wells in Montney formation, but also a comprehensive workflow of sensitivity and explainatability analysis has been introduced to obtain an optimized fit-to-purpose well completion design.",,2020,10.2118/200019-ms,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
624ddc061cc9d2d165ec1ef8ad15aa9d45146d14,https://www.semanticscholar.org/paper/624ddc061cc9d2d165ec1ef8ad15aa9d45146d14,AVAC: A Machine Learning Based Adaptive RRAM Variability-Aware Controller for Edge Devices,"Recently, the Edge Computing paradigm has gained significant popularity both in industry and academia. Researchers now increasingly target to improve performance and reduce energy consumption of such devices. Some recent efforts focus on using emerging RRAM technologies for improving energy efficiency, thanks to their no leakage property and high integration density. As the complexity and dynamism of applications supported by such devices escalate, it has become difficult to maintain ideal performance by static RRAM controllers. Machine Learning provides a promising solution for this, and hence, this work focuses on extending such controllers to allow dynamic parameter updates. In this work we propose an Adaptive RRAM Variability-Aware Controller, AVAC, which periodically updates Wait Buffer and batch sizes using on-the-fly learning models and gradient ascent. AVAC allows Edge devices to adapt to different applications and their stages, to improve computation performance and reduce energy consumption. Simulations demonstrate that the proposed model can provide up to 29% increase in performance and 19% decrease in energy, compared to static controllers, using traces of real-life healthcare applications on a Raspberry-Pi based Edge deployment.",2020 IEEE International Symposium on Circuits and Systems (ISCAS),2020,10.1109/ISCAS45731.2020.9180670,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
99bde69d899f6b275e81dfee285f7bcb3d40a012,https://www.semanticscholar.org/paper/99bde69d899f6b275e81dfee285f7bcb3d40a012,DroidAutoML: A Microservice Architecture to Automate the Evaluation of Android Machine Learning Detection Systems,,DAIS,2020,10.1007/978-3-030-50323-9_10,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
efc5d25e6ff5e772dd8d47c53c7fcb3abc4e63eb,https://www.semanticscholar.org/paper/efc5d25e6ff5e772dd8d47c53c7fcb3abc4e63eb,Applying Machine Learning to End-to-end Slice SLA Decomposition,"5G is set to revolutionize the network service industry with unprecedented use-cases in industrial automation, augmented reality, virtual reality and many other domains. Network slicing is a key enabler to realize this concept, and comes with various SLA requirements in terms of latency, throughput, and reliability. Network slicing is typically performed in an end-to-end (e2e) manner across multiple domains, for example, in mobile networks, a slice can span access, transport and core networks. Thus, if an SLA requirement is specified for e2e services, we need to ensure that the total SLA budget is appropriately proportioned to each participating domain in an adaptive manner. Such an SLA decomposition can be extremely useful for network service operators as they can plan accordingly for actual deployment. In this paper we design and implement an SLA decomposition planner for network slicing using supervised machine learning algorithms. Traditional optimization based approaches cannot deal with the dynamic nature of such services. We design machine learning models for SLA decomposition, based on random forest, gradient boosting and neural network. We then evaluate each class of algorithms in terms of accuracy, sample complexity, and model explainability. Our experiments reveal that, in terms of these three requirements, the gradient boosting and neural network algorithms for SLA decomposition out-perform random forest algorithms, given emulated data sets.",2020 6th IEEE Conference on Network Softwarization (NetSoft),2020,10.1109/netsoft48620.2020.9165317,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
282d0aebebfa08f46aa1b00f8ff405141494a156,https://www.semanticscholar.org/paper/282d0aebebfa08f46aa1b00f8ff405141494a156,Advanced machine learning eco-system to address HVM optical metrology requirements,"Machine learning (ML) techniques have been successfully deployed to resolve optical metrology challenges in semiconductor industry during recent years. With more advanced computing technology and algorithms, the ML system can be improved further to address High Volume Manufacturing (HVM) requirements. In this work, an advanced ML eco-system was implemented based on big data architecture to generate fast and user-friendly ML predictive models for metrology purposes. Application work and results completed by using this ML eco-system have revealed its capability to quickly refine solutions to predict both external reference data and to improve the throughput of conventional Optical Critical Dimension (OCD) metrology. The time-to-solution has been significantly improved and human operational time has also been greatly reduced. Results were shown for both front end and back end of line measurement applications, demonstrating good correlations and small errors in comparison with either external reference or conventional OCD results. The incremental retraining from this ML eco-system improved the correlation to external references, and multiple retrained models were analyzed to understand retraining effects and corresponding requirements. Quality Metric (QM) was also shown to have relevance in monitoring recipe performance. It has successfully demonstrated that with this advanced ML eco-system, streamlined ML models can be readily updated for high sensitivity and process development applications in HVM scenarios.",Advanced Lithography,2020,10.1117/12.2552058,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
040f55637fe6ea541a9f4018ae3960c07e459109,https://www.semanticscholar.org/paper/040f55637fe6ea541a9f4018ae3960c07e459109,QUA³CK - A Machine Learning Development Process,"Machine learning and data processing are trending topics at the moment. However, there is still alack of a standard process to support a fast, simple, and effective development of machine learningmodels for academia and industry combined. Processes such as KDD or CRISP-DM are highlyspecialized in data mining and business cases. Therefore, engineers often refer to individualapproaches to solve a machine learning problem. Especially in teaching, the lack of a standardprocess is a challenge. Students typically get a better understanding if a systematic approach tosolve problems is given to them. A challenge when formulating a machine learning developmentprocess is to provide standard actions that work on different use-cases. At the same time, it has tobe simple. Complex processes often lead to the wrong approach.The QUA³CK process was created at the Karlsruhe Institute of Technology to fill the gap inresearch and industry for a machine learning development process. However, the main focus wasto reach engineering students with an easy-to-remember, didactic way to solve machine learningproblems. This five-stage process starts with a machine learning question (Q), a problem thathas to be solved. Understanding the data (U) comes next. Then, the loop between selecting anAlgorithm (A), Adapting the features (A), and Adjusting the hyperparameters (A) is executeduntil the system is ready for Conclude and compare (C). At last, the Knowledge transfer (K) ofthe given solution can be realized as deployment in hardware or as a documentation.This paper describes the process and all individual steps in detail. Besides, we present severaluse-cases of QUA³CK in academia and research projects.",,2020,10.22323/1.372.0026,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
5b6efbb93f27bb55d28aad36445c4740f8e08a0a,https://www.semanticscholar.org/paper/5b6efbb93f27bb55d28aad36445c4740f8e08a0a,Increasing Facility Uptime Using Machine Learning and Physics-Based Hybrid Analytics in a Dynamic Digital Twin,"
 The industry remains focused on achieving efficiency gains through accessing and processing production, asset and original equipment manufacturer (OEM) data, and applying machine learning (ML) principles to arrive at improved outcomes. As a service provider, we are experiencing an increased activity level related to hybrid analytics which involves embedding high-fidelity physics-based models together with ML models to improve outcomes. Critically, rather than addressing one piece of equipment, or a subset of a facility, companies focus on economies of scale, and seek asset-wide understanding of implications to equipment condition when changing operating parameters.
 Deployment of a Dynamic Digital Twin provides production and maintenance engineers with a ‘single source of truth’ for information (i.e. P&ID, PFD, OEM information, maintenance history, etc.) integrated with high-fidelity physics-based models for subsea and topsides processes. Field sensors measuring hydrocarbon quantity, quality and other physical properties are integrated to provide real-time and historic data. Physics-based dynamic process models are first calibrated to match the field sensor data and then used to generate synthetic data for training ML models. A high-fidelity model generates virtual measurements where field sensors are not available. Access to such high-quality virtual measurements presents a paradigm shift for upstream analytics, as ML algorithms now have access to larger datasets for training. This improves quality, allowing for proactive planning and improved uptime leading to increased facility uptime by predicting equipment failure and enabling condition-based maintenance (CBM).
 In our work with major oil and gas operators, we have observed that maintenance engineers until now have struggled, because enough field sensors are not always available to support the ML algorithms, leading to less specific assumptions and lower quality results. By taking advantage of a Dynamic Digital Twin - containing the asset structure, visualization and models - hybrid analytics were applied to continuously improve predictions, thereby increasing facility uptime.
 In this paper, we present a few case studies of applying hybrid analytics with some oil and gas operators to enable virtual flow metering, prediction of unplanned equipment shutdown and prediction of optimum operating parameters for increased facility uptime. Examples presented demonstrate the integration of historic and real-time measurements with the physics-based process and multiphase flow models, and ML algorithms such as Autoencoder (AE), Long Short-term Memory (LSTM) neural networks and Reinforcement Learning.",,2020,10.4043/30723-ms,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
818e68f19f82df182f2880f0825f0de97c0f0f29,https://www.semanticscholar.org/paper/818e68f19f82df182f2880f0825f0de97c0f0f29,Delivering a machine learning course on HPC resources,"In recent years, proficiency in data science and machine learning (ML) became one of the most requested skills for jobs in both industry and academy. Machine learning algorithms typically require large sets of data to train the models and extensive usage of computing resources, both for training and inference. Especially for deep learning algorithms, training performances can be dramatically improved by exploiting Graphical Processing Units (GPUs). The needed skill set for a data scientist is therefore extremely broad, and ranges from knowledge of ML models to distributed programming on heterogeneous resources. While most of the available training resources focus on ML algorithms and tools such as TensorFlow, we designed a course for doctoral students where model training is tightly coupled with underlying technologies that can be used to dynamically provision resources. Throughout the course, students have access to a dedicated cluster of computing nodes on local premises. A set of libraries and helper functions is provided to execute a parallelized ML task by automatically deploying a Spark driver and several Spark execution nodes as Docker containers. Task scheduling is managed by an orchestration layer (Kubernetes). This solution automates the delivery of the software stack required by a typical ML workflow and enables scalability by allowing the execution of ML tasks, including training, over commodity (i.e. CPUs) or high-performance (i.e. GPUs) resources distributed over different hosts across a network. The adaptation of the same model on OCCAM, the HPC facility at the University of Turin, is currently under development.",EPJ Web of Conferences,2020,10.1051/epjconf/202024508016,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
d92e9a71e632058a778c6335e17c922769fe8de0,https://www.semanticscholar.org/paper/d92e9a71e632058a778c6335e17c922769fe8de0,One Explanation Does Not Fit All The Promise of Interactive Explanations for Machine Learning Transparency.,"The need for transparency of predictive systems based on Machine Learning algorithms arises as a consequence of their ever-increasing proliferation in the industry. Whenever black-box algorithmic predictions influence human affairs, the inner workings of these algorithms should be scrutinised and their decisions explained to the relevant stakeholders, including the system engineers, the system’s operators and the individuals whose case is being decided. While a variety of interpretability and explainability methods is available, none of them is a panacea that can satisfy all diverse expectations and competing objectives that might be required by the parties involved. We address this challenge in this paper by discussing the prom-ises of Interactive Machine Learning for improved transparency of black-box systems using the example of contrastive explanations—a state-of-the-art approach to Interpretable Machine Learning. Specifically, we show how to personalise counterfactual explanations by interactively adjusting their conditional statements and extract additional explanations by asking follow-up “What if?” questions. Our experience in building, deploying and presenting this type of system allowed us to list desired properties as well as potential limitations, which can be used to guide the development of interactive explainers. While customising the medium of interaction, i.e., the user interface comprising of various communication channels, may give an impression of personalisation, we argue that adjusting the explanation itself and its content is more important. To this end, properties such as breadth, scope, context, purpose and target of the explanation have to be considered, in addition to explicitly informing the explainee about its limitations and caveats. Furthermore, we discuss the challenges of mirroring the explainee’s mental model, which is the main building block of intelligible human–machine interactions. We also deliberate on the risks of allowing the explainee to freely manipulate the explanations and thereby extracting information about the underlying predictive model, which might be leveraged by malicious actors to steal or game the model. Finally, building an end-to-end interactive explainability system is a challenging engineering task; unless the main goal is its deployment, we recommend “Wizard of Oz” studies as a proxy for testing and evaluating standalone interactive explainability algorithms.",,2020,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
afc60c76680bbb669743078f37e97ede58d55b29,https://www.semanticscholar.org/paper/afc60c76680bbb669743078f37e97ede58d55b29,Forecasting Credit Card Attrition using Machine Learning Models,"In recent years, credit card attrition has emerged as an issue of significant concern for the banking sector. It has a significant impact on profitability, given that the cost of acquiring new customers is higher than that of retaining existing customers. In this work, a selection of supervised Machine Learning models to identify which customers want to cancel their credit cards is evaluated. The banking industry uses this technology to obtain more reliable predictions when identifying opportunities for purchase, investment, or fraud. These models can be adapted independently, by recognizing patterns and algorithms based on mathematical calculations. Four models (LightGBM, XGBoost, Random Forest and Logistic Regression) were evaluated to predict, using data about customers and products held pertaining to a bank in Colombia, the likelihood of customers canceling their credit cards. By analyzing the ROC curves using the AUC metric, it is concluded that, of the selected models, the model chosen for deployment would be LightGBM, since it was the one that performed best in the experiments conducted. Furthermore, the “Score Acierta” variable, a customer rating provided by the Colombian credit rating agency, was found to be the most discriminating in prediction models.",ICAI Workshops,2020,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
f34213fcac714e3cd2af22733c69091677d7ee49,https://www.semanticscholar.org/paper/f34213fcac714e3cd2af22733c69091677d7ee49,Feature Selection for Malicious Traffic Detection with Machine Learning,"The network technology plays an important role in the emerging industry 4.0. Industrial control systems (ICS) are related to all aspects of human life and have become the target of cyber-attackers. Attacks on ICS may not only cause economic loss, but also damage equipment and hurt staff. The biggest challenges in establishing a secure network communication system is how to effectively detect and prevent malicious network behavior. A Network Intrusion Detection System (NIDS) can be deployed as a defense mechanism for cyberattacks. However, for industrial internet-of-things (IIoT) applications with limited computing resources, designing an effective NIDS is challenging. In this paper, we propose to use machine learning as the core technology to build a compact and effective NIDS for IIoT. The proposed method is validated by using the more recent UNSW-NB 15 dataset to improve the detection capability against new types of attacks in the real world. Furthermore, we demonstrate that the method is also valid for traditional KDD-CUP-99 dataset. Experimental results show that the proposed method achieves better performance than previous methods.",2020 International Computer Symposium (ICS),2020,10.1109/ICS51289.2020.00088,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
04bf460db603a5f33177cb815fd434ab99aabaae,https://www.semanticscholar.org/paper/04bf460db603a5f33177cb815fd434ab99aabaae,How Machine Learning is Improving Production on Offshore Platforms,"
 The oil and gas industry is known for its volatile boom and bust cycles, and this has a particularly strong effect on offshore platforms, which are substantial investments that do not always succeed in generating profit. To generate value and remain competitive in the offshore sector, operators must ensure that their operations are efficient, optimized, unlikely to be interrupted by unexpected failures, and resilient to fluctuations in oil prices. Artificial intelligence, and more specifically, machine learning, applications allow operators to do all of this. This paper presents two specific case studies demonstrating how machine learning has been proven and scaled in a deployed environment, with tangible increase in offshore production leading to significant business value to the operator.",,2020,10.4043/30782-ms,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
f600db2b0eecd0a33873efdc675cb24f2cb34eef,https://www.semanticscholar.org/paper/f600db2b0eecd0a33873efdc675cb24f2cb34eef,A Machine Learning Approach for Data Quality Control of Earth Observation Data Management System,"In the big data era, innovative technologies like cloud computing, artificial intelligence, and machine learning are increasingly utilized in the large-scale data management systems of many industry sectors to make them more scalable and intelligent. Applying them to automate and optimize earth observation data management is a hot topic. To improve data quality control mechanisms, a machine learning method in combination with built-in quality rules is presented in this paper to evolve processes around data quality and enhance management of earth observation data. The rules of quality check are set up to detect the common issues, including data completeness, data latency, bad data, and data duplication, and the machine learning model is trained, tested, and deployed to address these quality issues automatically and reduce manual efforts.",IGARSS 2020 - 2020 IEEE International Geoscience and Remote Sensing Symposium,2020,10.1109/IGARSS39084.2020.9323615,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
28e28325ec5df87e899681277aa12fa1144725e3,https://www.semanticscholar.org/paper/28e28325ec5df87e899681277aa12fa1144725e3,Validate and Enable Machine Learning in Industrial AI,"Industrial Artificial Intelligence (Industrial AI) is an emerging concept which refers to the application of artificial intelligence to industry. Industrial AI promises more efficient future industrial control systems. However, manufacturers and solution partners need to understand how to implement and integrate an AI model into the existing industrial control system. A well-trained machine learning (ML) model provides many benefits and opportunities for industrial control optimization; however, an inferior Industrial AI design and integration limits the capability of ML models. To better understand how to develop and integrate trained ML models into the traditional industrial control system, test the deployed AI control system, and ultimately outperform traditional systems, manufacturers and their AI solution partners need to address a number of challenges. Six top challenges, which were real problems we ran into when deploying Industrial AI, are explored in the paper. The Petuum Optimum system is used as an example to showcase the challenges in making and testing AI models, and more importantly, how to address such challenges in an Industrial AI system.",ArXiv,2020,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
e7c8fe46d53dddf8ae02c112d8933207e22fb4d5,https://www.semanticscholar.org/paper/e7c8fe46d53dddf8ae02c112d8933207e22fb4d5,Production Operations Efficiency Gains Enablers: Machine Learning or Physics Based Models?,"
 Industry reports confirm that while the unconventional wells drilling and completion operations have achieved significant efficiency gains, which translated into steep cost reductions, the production operations costs remained largely flat. The authors provide a quick review of the above and discuss technologies aimed at addressing this situation including machine learning, AI and physics-based modeling. The latter has been the cornerstone for production optimization since the early days of oil and gas exploitation, however, it has been uneconomical for systematic use in unconventional wells and is applied to only a small fraction of the large number of active wells. This is mainly due to their complex completions, which makes modeling a tedious and resource intensive task. Data science promises faster and nimbler solutions, however, while advanced techniques such as deep learning have achieved impressive advances in many areas like face and speech recognition, they remain mostly rooted in pattern recognition, with no grasp of cause and effect, which limits the type of problems they can successfully resolve. This paper will discuss the above solutions and limitations and presents a novel physics-based approach that would be economical for widespread deployment.",,2020,10.2118/201501-ms,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
821fde6dc36d1264c765d249d4247ea66daff55f,https://www.semanticscholar.org/paper/821fde6dc36d1264c765d249d4247ea66daff55f,Edge Machine Learning for AI-Enabled IoT Devices: A Review,"In a few years, the world will be populated by billions of connected devices that will be placed in our homes, cities, vehicles, and industries. Devices with limited resources will interact with the surrounding environment and users. Many of these devices will be based on machine learning models to decode meaning and behavior behind sensors’ data, to implement accurate predictions and make decisions. The bottleneck will be the high level of connected things that could congest the network. Hence, the need to incorporate intelligence on end devices using machine learning algorithms. Deploying machine learning on such edge devices improves the network congestion by allowing computations to be performed close to the data sources. The aim of this work is to provide a review of the main techniques that guarantee the execution of machine learning models on hardware with low performances in the Internet of Things paradigm, paving the way to the Internet of Conscious Things. In this work, a detailed review on models, architecture, and requirements on solutions that implement edge machine learning on Internet of Things devices is presented, with the main goal to define the state of the art and envisioning development requirements. Furthermore, an example of edge machine learning implementation on a microcontroller will be provided, commonly regarded as the machine learning “Hello World”.",Sensors,2020,10.3390/s20092533,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
b14692c1e0658dd1d814c737171ce714b1588360,https://www.semanticscholar.org/paper/b14692c1e0658dd1d814c737171ce714b1588360,An Intelligent UAV Deployment Scheme for Load Balance in Small Cell Networks Using Machine Learning,"In wireless networks, network load can be highly unbalanced due to the mobility of user equipments (UEs). Unmanned Aerial Vehicles (UAVs) supported base station with the advantage of flexible deployment, ubiquitous wireless coverage and high speed data rate, is a promising approach to handle with the foregoing problem. However, how to achieve cost-effective UAV deployment in an autonomous and dynamic manner is a significant challenge. Facing this problem, we propose a novel UAV base station intelligent deployment scheme based on machine learning and evaluate its performance on a realworld dataset. First, we conduct data preprocessing to process, clean, and transform raw data into formatted data. Missing values are filled by Conditional Mean Imputation (CMI) method and outliers are corrected by pauta criterion. Then, we use hybrid approach which contains ARIMA model and XGBoost model. Linear predictions are carried out by ARIMA model and later nonlinear model XGBoost are applied on residue of ARIMA. Resultant prediction is obtained by adding linear and nonlinear prediction, hybrid model is estimated by Root Mean Square Error (RMSE) and R2 score. Finally, according to predicted results, UAV base stations can be deployed to cater for dynamically changing demands in the hotspot areas and achieve cost-effective deployment. Simulation results show that the propose scheme is superior to other benchmark schemes in load balancing.",2019 IEEE Wireless Communications and Networking Conference (WCNC),2019,10.1109/WCNC.2019.8885648,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
6645b8fd0c3595bea8063e89d4ca1847f2846766,https://www.semanticscholar.org/paper/6645b8fd0c3595bea8063e89d4ca1847f2846766,A Framework for Unsupervised Planning of Cellular Networks Using Statistical Machine Learning,"The wireless industry is moving towards developing smart cellular architectures that dynamically adjust the use of the network elements according to the service demand, and automating their operations in order to minimize both capital expenditure (CAPEX) and operation expenditure (OPEX). This involves developing efficient and unsupervised radio access network (RAN) planning, which has a direct impact on the system performance and CAPEX. This intelligent cellular planning aims at providing the base stations (BSs) configurations (e.g., coverage, user associations and antenna radiation pattern) that minimize the number of deployed BSs and meet the requirements in terms of coverage and capacity. The cellular planning optimization problem has been shown to be complex and non-scalable. Moreover, most of the existing cellular planning techniques result in an over or under provisioning architecture. Motivated by the above, we propose in this paper a novel and efficient unsupervised planning process. We make use of statistical machine learning (SML) to solve the problem at hand. The core idea of SML is that the planning parameters are treated as random variables. The parameters that maximize the corresponding joint probability distribution, conditioned on observations of users’ positions, are learned or inferred using Gibbs sampling theory and Bayes’ theory. To apply this theory to the planning problem, we make significant efforts to properly formulate the problem to be able to incorporate the constraints into the inference process and extract the planning parameters from the inferred model. Through several numerical examples, we compare the performance of the proposed approach to clustering-based and optimization-based existing planning approaches, and demonstrate the efficacy of our approach. We also demonstrate how our approach can leverage existing cellular infrastructures into the new design.",IEEE Transactions on Communications,2020,10.1109/TCOMM.2020.2971691,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
dbe7e45f5863863b4813f1ff6a3843a9493f3e1a,https://www.semanticscholar.org/paper/dbe7e45f5863863b4813f1ff6a3843a9493f3e1a,Anomaly Detection using Machine Learning Techniques in Wireless Sensor Networks,"The number of Wireless sensor network (WSN) deployments have been growing so exponentially over the recent years. Due to their small size and cost-effective, WSN are attracting many industries to use them in various applications. Environmental monitoring, security of buildings and precision agriculture are few example among several other fields. However, WSN faces high security threats considering most of them are deployed in unattended nature and hostile environment. In the aim of providing secure data processing in the WSN, many techniques are proposed to protect the data privacy while being transferred from the sensors to the base station. This work is focusing on attack detection which is an essential task to secure the network and the data. Anomaly detection is a key challenge in order to ensure the security and prevent malicious attacks in wireless sensor networks. Various machine learning techniques have been used by researchers these days to detect anomalies using offline learning algorithms. On the other hand online learning classifiers have not been thoroughly addressed in the literature. Our aim is to provide an intrusion detection model compatible with the characteristics of WSN. This model is built based on information gain ratio and the online Passive aggressive classifier. Firstly, the information gain ratio is used to select the relevant features of the sensor data. Secondly, the online Passive aggressive algorithm is trained to detect and classify different type of Deny of Service attacks. The experiment was conducted on a wireless sensor network-detection system (WSN-DS) dataset. The proposed model ID-GOPA results detection rate of 96% determining whether the network is in its normal mode or exposed to any type of attack. The detection accuracy is 86%, 68%, 63%, and 46% for scheduling, grayhole, flooding and blackhole attacks, respectively, in addition to 99% for normal traffic. These results shows that our model based on offline learning can be providing good anomaly detection to the WSN and replace online learning in some cases.",,2021,10.1088/1742-6596/1743/1/012021,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
024b960a1d833e495f0230c3d789571dae7ec0b1,https://www.semanticscholar.org/paper/024b960a1d833e495f0230c3d789571dae7ec0b1,Interpretability of Machine Learning Solutions in Public Healthcare: The CRISP-ML Approach,"Public healthcare has a history of cautious adoption for artificial intelligence (AI) systems. The rapid growth of data collection and linking capabilities combined with the increasing diversity of the data-driven AI techniques, including machine learning (ML), has brought both ubiquitous opportunities for data analytics projects and increased demands for the regulation and accountability of the outcomes of these projects. As a result, the area of interpretability and explainability of ML is gaining significant research momentum. While there has been some progress in the development of ML methods, the methodological side has shown limited progress. This limits the practicality of using ML in the health domain: the issues with explaining the outcomes of ML algorithms to medical practitioners and policy makers in public health has been a recognized obstacle to the broader adoption of data science approaches in this domain. This study builds on the earlier work which introduced CRISP-ML, a methodology that determines the interpretability level required by stakeholders for a successful real-world solution and then helps in achieving it. CRISP-ML was built on the strengths of CRISP-DM, addressing the gaps in handling interpretability. Its application in the Public Healthcare sector follows its successful deployment in a number of recent real-world projects across several industries and fields, including credit risk, insurance, utilities, and sport. This study elaborates on the CRISP-ML methodology on the determination, measurement, and achievement of the necessary level of interpretability of ML solutions in the Public Healthcare sector. It demonstrates how CRISP-ML addressed the problems with data diversity, the unstructured nature of data, and relatively low linkage between diverse data sets in the healthcare domain. The characteristics of the case study, used in the study, are typical for healthcare data, and CRISP-ML managed to deliver on these issues, ensuring the required level of interpretability of the ML solutions discussed in the project. The approach used ensured that interpretability requirements were met, taking into account public healthcare specifics, regulatory requirements, project stakeholders, project objectives, and data characteristics. The study concludes with the three main directions for the development of the presented cross-industry standard process.",Frontiers in Big Data,2021,10.3389/fdata.2021.660206,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
39264be15daa49511d32723d74fa9c48673ebddc,https://www.semanticscholar.org/paper/39264be15daa49511d32723d74fa9c48673ebddc,"Machine Learning for 5G/B5G Mobile and Wireless Communications: Potential, Limitations, and Future Directions","Driven by the demand to accommodate today’s growing mobile traffic, 5G is designed to be a key enabler and a leading infrastructure provider in the information and communication technology industry by supporting a variety of forthcoming services with diverse requirements. Considering the ever-increasing complexity of the network, and the emergence of novel use cases such as autonomous cars, industrial automation, virtual reality, e-health, and several intelligent applications, machine learning (ML) is expected to be essential to assist in making the 5G vision conceivable. This paper focuses on the potential solutions for 5G from an ML-perspective. First, we establish the fundamental concepts of supervised, unsupervised, and reinforcement learning, taking a look at what has been done so far in the adoption of ML in the context of mobile and wireless communication, organizing the literature in terms of the types of learning. We then discuss the promising approaches for how ML can contribute to supporting each target 5G network requirement, emphasizing its specific use cases and evaluating the impact and limitations they have on the operation of the network. Lastly, this paper investigates the potential features of Beyond 5G (B5G), providing future research directions for how ML can contribute to realizing B5G. This article is intended to stimulate discussion on the role that ML can play to overcome the limitations for a wide deployment of autonomous 5G/B5G mobile and wireless communications.",IEEE Access,2019,10.1109/ACCESS.2019.2942390,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
433ec5238fa1bdd1f2d62cd57d205de2d2f726f6,https://www.semanticscholar.org/paper/433ec5238fa1bdd1f2d62cd57d205de2d2f726f6,Machine Learning and Optimization for Resource-Constrained Platforms,"Artificial intelligence (AI) and machine learning (ML) have been growing at an incredible rate in recent years and they show no sign of stopping. Manufacturing, educational systems, transportation architecture, and genetic research are industries where artificial intelligence algorithms have been developed and found practical applications in which they can increase task efficiency and reduce cost through process optimization, pattern recognition, and automation. At NASA, one of the goals of the cognitive communications project has been to find applications for such algorithms to next-generation communication systems. The goal of this effort is to identify areas and approaches to intelligent system design and implementation which could allow NASA to support a larger space-and ground-based network while simultaneously reducing the operational costs involved with maintaining such a system This paper will evaluate the state of various approaches by searching for algorithms which are feasible to deploy directly onto future space systems with improved processing requirements. We begin by describing a set of heuristics through which algorithms may be compared, emphasizing memory and computational requirements, and heuristic bounds. We then evaluate general-purpose processing platforms onto which such algorithms may be deployed. We also evaluate how such systems may be packaged so as to offer a deterministic set of performance and decision metrics, to make the devices easier for system designers to include in present and future systems. We conclude the paper with a discussion of our findings, as well as where and how this study might continue in the future.",2019 IEEE Cognitive Communications for Aerospace Applications Workshop (CCAAW),2019,10.1109/CCAAW.2019.8904897,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
6dda5b662ef376869ce31c5095c697a6424b9b41,https://www.semanticscholar.org/paper/6dda5b662ef376869ce31c5095c697a6424b9b41,Vision-Aided Radio: User Identity Match in Radio and Video Domains Using Machine Learning,"5G is designed to be an essential enabler and a leading infrastructure provider in the communication technology industry by supporting the demand for the growing data traffic and a variety of services with distinct requirements. The use of deep learning and computer vision tools has the means to increase the environmental awareness of the network with information from visual data. Information extracted via computer vision tools such as user position, movement direction, and speed can be promptly available for the network. However, the network must have a mechanism to match the identity of a user in both visual and radio systems. This mechanism is absent in the present literature. Therefore, we propose a framework to match the information from both visual and radio domains. This is an essential step to practical applications of computer vision tools in communications. We detail the proposed framework training and deployment phases for a presented setup. We carried out practical experiments using data collected in different types of environments. The work compares the use of Deep Neural Network and Random Forest classifiers and shows that the former performed better across all experiments, achieving classification accuracy greater than 99%.",IEEE Access,2020,10.1109/ACCESS.2020.3038926,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
c94355572a20f4443889f9b93233ac2facbbef78,https://www.semanticscholar.org/paper/c94355572a20f4443889f9b93233ac2facbbef78,A Machine Learning Framework for Profitability Profiling and Dynamic Price Prediction for the New York City Taxi Trips,"The New York City Taxi & Limousine Commission’s (NYC TLC) Yellow cabs are facing increased competition from app-based car services such as Ola, Uber, Didi, Lyft and Grab which is rapidly eating away its revenue and market share. Research work: In response to this, the study proposes to do profitability profiling of the taxi trips to focus on various key aspects that generate more revenue in future, visualization to assess the departure and arrival counts of the trips in various locations based on time of the day to maintain demand and supply equilibrium and also build a dynamic price prediction model to balance both margins as well as conversion rates. Methodology/Techniques used: The NYC TLC yellow taxi trip data is analysed through a cross-industry standard process for data mining (CRISP-DM) methodology. Firstly, the taxi trips are grouped into two profitability segments according to the fare amount, trip duration and trip distance by applying K means clustering. Secondly, spatiotemporal data analysis is carried to assess the demand for taxi trips at various locations at various times of the day. Thirdly, multiple linear regression, decision tree, and random forest models are adopted for dynamic price prediction. The findings of the study are as follows, high profitable segments are characterized by airport pickup and drop trips, Count of trip arrivals to airports are more compared to departures from airports at any time of the day, and further analysis revealed that drivers making only a few numbers of airport trips can earn more revenue compared to making more number of trips in local destinations. Compared to multiple linear regression and decision tree, the random forest regression model is considered to be most reliable for dynamic pricing prediction with an accuracy of 91%. Application of research work: The practical implication of the study is the deployment of a dynamic pricing model that can increase the revenue of the NYC TLC cabs along with balancing margin and conversion rates.",International Journal of Innovative Technology and Exploring Engineering,2020,10.35940/ijitee.e2669.039520,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
26cd32ee0af552189e9075ad6a5694dabcffe3bd,https://www.semanticscholar.org/paper/26cd32ee0af552189e9075ad6a5694dabcffe3bd,Evaluation of Machine Learning approaches for resource constrained IIoT devices,"Resource-constrained devices such as sensors, industrial controllers, analyzers etc., mostly contain limited computational capacity and memory. They are largely deployed in all industries and have been generating a huge amount of data. This data is sent to the cloud servers where various Machine Learning (ML) algorithms are applied to perform the analysis or prediction as per the application. In this process, communication requires bandwidth and time. Since the data is sent into the network, the privacy of the data is not guaranteed. Cloud servers consume a huge amount of power. To reduce these cost factors, the machine learning models are compressed and optimized such that they can fit and run in small footprint devices. The Federated Learning (FL) approach at the edge device level promises to address the data privacy and bandwidth related issues. Since it is a decentralized learning method across a set of devices, the performance of the model also improves. This paper describes and evaluates the machine learning algorithms with various compression methods suitable for resource-constrained IIoT devices and federated learning approach, particularly for time series data applications. Simulation results show that FastGRNN algorithm gives the least model size compared to the traditional RNN algorithms for time series.",2021 13th International Conference on Information Technology and Electrical Engineering (ICITEE),2021,10.1109/ICITEE53064.2021.9611880,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
f8572abc5bc95c0349c6db9246c9006edf856a40,https://www.semanticscholar.org/paper/f8572abc5bc95c0349c6db9246c9006edf856a40,An Intelligence Learner Management System using Learning Analytics and Machine learning,"Learner Management System (LMS) facilitates educational institutions to offer e-learning through web-based applications. LMS provides many benefits from cost saving to flexible learning opportunities independent of any location with cloud-based deployment. Hence, LMS organizes learning data and learners detail in a central repository, helps to improve resource allocation, and facilitates access to the learning resources. These benefits drive the LMS market growth at a rapid rate and it is now deployed across the industry of any size. Despite of these significant benefits of using LMS, the traditional LMS system cannot fully support with modern learning needs in terms of learners' progression, retention rate, prediction of assessment outcomes to improve overall teaching and learning experience. Learning analytics can effectively support for a better learning experience by analyzing and correlating learner data to predicate the future needs. This work as a part of the Knowledge Transfer Project (KTP) develops an intelligence Learner Management System(iLMS) that integrates learning analytics into the learner management system. We use Machine Learning(ML) techniques for descriptive, predictive, and prescriptive analytics of learners’ data. This paper presents the key iLMS features including user interfaces, reports and learning analytics.",ICETC,2020,10.1145/3436756.3437032,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
77da4f20755f10924e9e05218407e84c850f697c,https://www.semanticscholar.org/paper/77da4f20755f10924e9e05218407e84c850f697c,Using a Machine Learning Tool to Support High-Stakes Decisions in Child Protection,"Copyright © 2021, Association for the Advancement of Artificial Intelligence. All rights reserved. ISSN 0738-4602 Spring 2021 53 Although there has been considerable achievement in the use of artificial intelligence (Ai) in the commercial sector, increasing the profits of existing businesses and disrupting industries, these techniques have not transferred easily to the social domain. indeed, there are very few cases where even the most basic machine learning tools have made any difference to the many intractable social problems facing those same countries where these Ai methods have been so enthusiastically embraced for profit. in areas such as homelessness, severe mental illness, substance-use, and child-maltreatment, many ideas have been proposed (Saxena et al. 2020) and theoretically shown to be useful, but few have been taken up, and upon evaluation, almost none of them have been found to have a positive impact. Even when some extremely limited form of Ai is proposed, the application has been so poorly executed that while intending to improve social disadvantage, they may have exacerbated existing harms. A case in point is the use of recidivism tools in criminal justice. These simple predictive analytic tools have been shown to be better than judges at deciding which prisoner should be allowed to be on parole, and if implemented could reduce the number of people in jail (Kleinberg et al. 2018). Yet, in what is now a seminal exposé by propublica, these tools were found to  Machine learning decision support tools have become popular in a range of social domains including healthcare, criminal justice, and child welfare. But the design of these tools often fails to consider the potentially complex interactions that happen between the tools and humans. This lack of humancentered design is one reason that so few tools are actually deployed, and even if they are, struggle to achieve impact. In this article we present the example of the Allegheny Family Screening Tool, a machine learning model used since 2016 to support hotline screening of child maltreatment referrals. We describe aspects of human-centered design that contributed to the successful deployment of this tool, including agency leadership and ownership, transparency by design, ethical oversight, community engagement, and social license. Finally, we identify potential next-steps to encourage greater integration of human-centered design into the development and implementation of machine learning decision support tools. Using a Machine Learning Tool to Support High-Stakes Decisions in Child Protection",AI Mag.,2021,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
221a89f5cd80a8b3d38281f4ece84260654c0388,https://www.semanticscholar.org/paper/221a89f5cd80a8b3d38281f4ece84260654c0388,Amazon SageMaker Model Monitor: A System for Real-Time Insights into Deployed Machine Learning Models,"With the increasing adoption of machine learning (ML) models and systems in high-stakes settings across different industries, guaranteeing a model’s performance after deployment has become crucial. Monitoring models in production is a critical aspect of ensuring their continued performance and reliability. We present Amazon SageMaker Model Monitor, a fully managed service that continuously monitors the quality of machine learning models hosted on Amazon SageMaker. Our system automatically detects data, concept, bias, and feature attribution drift in models in real-time and provides alerts so that model owners can take corrective actions and thereby maintain high quality models. We describe the key requirements obtained from customers, system design and architecture, and methodology for detecting different types of drift. Further, we provide quantitative evaluations followed by use cases, insights, and lessons learned from more than two years of production deployment.",ArXiv,2021,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
26633236b3ef1897e713917b635e490cbd0cae62,https://www.semanticscholar.org/paper/26633236b3ef1897e713917b635e490cbd0cae62,Computer Vision and Machine Learning Analysis of Commercial Rice Grains: A Potential Digital Approach for Consumer Perception Studies,"Rice quality assessment is essential for meeting high-quality standards and consumer demands. However, challenges remain in developing cost-effective and rapid techniques to assess commercial rice grain quality traits. This paper presents the application of computer vision (CV) and machine learning (ML) to classify commercial rice samples based on dimensionless morphometric parameters and color parameters extracted using CV algorithms from digital images obtained from a smartphone camera. The artificial neural network (ANN) model was developed using nine morpho-colorimetric parameters to classify rice samples into 15 commercial rice types. Furthermore, the ANN models were deployed and evaluated on a different imaging system to simulate their practical applications under different conditions. Results showed that the best classification accuracy was obtained using the Bayesian Regularization (BR) algorithm of the ANN with ten hidden neurons at 91.6% (MSE = <0.01) and 88.5% (MSE = 0.01) for the training and testing stages, respectively, with an overall accuracy of 90.7% (Model 2). Deployment also showed high accuracy (93.9%) in the classification of the rice samples. The adoption by the industry of rapid, reliable, and accurate methods, such as those presented here, may allow the incorporation of different morpho-colorimetric traits in rice with consumer perception studies.",Sensors,2021,10.3390/s21196354,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
6d00d7f4e89c05f87c0519790313761f8a8fc425,https://www.semanticscholar.org/paper/6d00d7f4e89c05f87c0519790313761f8a8fc425,Cooperative Robotics and Machine Learning for Smart Manufacturing: Platform Design and Trends Within the Context of Industrial Internet of Things,"Internet of Things (IoT) in industrial settings now leads to the development of a new generation of systems designed to improve the operational efficiency of the new paradigm of smart manufacturing plants. Thereby, the current article introduces in detail the definitions, concepts, standards, and other important aspects related to smart manufacturing, cooperative robotics, and machine learning techniques. The paper highlights the opportunities presented by the new paradigm and the challenges faced in effectively implementing it in the industrial context. Especially, the focus is on the challenges associated with the architectures, communications technology, and protocols that enable the integration and deployment of machine learning algorithms to improve the execution of cooperative tasks performed daily by human operators, machines, and robots. The article also provides a systematic review of state-of-the-art research efforts for the fields aforementioned. Finally, an architecture for integrating collaborative robotics and machine learning based on six layers and four hierarchies of the RAMI 4.0 (Reference Architectural Model Industry 4.0) is presented.",IEEE Access,2021,10.1109/ACCESS.2021.3094374,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
aa44dc177108f01420f0e3a3891e5184348222c8,https://www.semanticscholar.org/paper/aa44dc177108f01420f0e3a3891e5184348222c8,Hidden Technical Debts for Fair Machine Learning in Financial Services,"The recent advancements in machine learning (ML) have demonstrated the potential for providing a powerful solution to build complex prediction systems in a short time. However, in highly regulated industries, such as the financial technology (Fintech), people have raised concerns about the risk of ML systems discriminating against specific protected groups or individuals. To address these concerns, researchers have introduced various mathematical fairness metrics and bias mitigation algorithms. This paper discusses hidden technical debts and challenges of building fair ML systems in a production environment for Fintech. We explore various stages that require attention for fairness in the ML system development and deployment life cycle. To identify hidden technical debts that exist in building fair ML system for Fintech, we focus on key pipeline stages including data preparation, model development, system monitoring and integration in production. Our analysis shows that enforcing fairness for production-ready ML systems in Fintech requires specific engineering commitments at different stages of ML system life cycle. We also propose several initial starting points to mitigate these technical debts for deploying fair ML systems in production.",ArXiv,2021,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
8facb409b683ca952bf389391a995e9a51955eaf,https://www.semanticscholar.org/paper/8facb409b683ca952bf389391a995e9a51955eaf,Automated Machine Learning for Remaining Useful Life Estimation of Aircraft Engines,"Remaining useful life (RUL) of an asset or system is defined as the length from the current time and operating state to the end of the useful life. It is of paramount importance for safety-critical industries such as aviation and lies in the heart of prognostics and health management (PHM). This paper investigates the usage of automated machine learning (AutoML) for RUL estimation, based on using classical machine learning algorithms for regression. The data is pre-processed by extracting statistical features from expanding windows of the signal in order to uncover the degradation that has been accumulating from the early life of the system or after an overhaul. We evaluate our methodology on the widely-used C-MAPSS dataset and compare our approach to the state-of-the-art deep neural networks (DNNs) and classical machine learning algorithms. The experimental results show that AutoML outperforms or is comparable to traditional machine learning techniques and standard neural networks, while being outperformed by specifically designed neural networks on datasets with multiple fault mode and operating conditions. These results show that with the correct pre-processing automated machine learning is able to accurately estimate the RUL, which implies that such approaches can be industrially deployed.",2021 IEEE International Conference on Prognostics and Health Management (ICPHM),2021,10.1109/ICPHM51084.2021.9486549,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
aee2a6bd0ede04f39819c11dd0fd823356268be2,https://www.semanticscholar.org/paper/aee2a6bd0ede04f39819c11dd0fd823356268be2,Machine Learning and the Meaning of Equal Treatment,"Approaches to non-discrimination are generally informed by two principles: striving for equality of treatment, and advancing various notions of equality of outcome. We consider when and why there are trade-offs in machine learning between respecting formalistic interpretations of equal treatment and advancing equality of outcome. Exploring a hypothetical discrimination suit against Facebook, we argue that interpretations of equal treatment which require blindness to difference may constrain how machine learning can be deployed to advance equality of outcome. When machine learning models predict outcomes that are unevenly distributed across racial groups, using those models to advance racial justice will often require deliberately taking race into account. We then explore the normative stakes of this tension. We describe three pragmatic policy options underpinned by distinct interpretations and applications of equal treatment. A status quo approach insists on blindness to difference, permitting the design of machine learning models that compound existing patterns of disadvantage. An industry-led approach would specify a narrow set of domains in which institutions were permitted to use protected characteristics to actively reduce inequalities of outcome. A government-led approach would impose positive duties that require institutions to consider how best to advance equality of outcomes and permit the use of protected characteristics to achieve that goal. We argue that while machine learning offers significant possibilities for advancing racial justice and outcome-based equality, harnessing those possibilities will require a shift in the normative commitments that underpin the interpretation and application of equal treatment in non-discrimination law and the governance of machine learning.",AIES,2021,10.1145/3461702.3462556,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
b1b7a3b5801869abcb8c01cf10740031f71b0a8b,https://www.semanticscholar.org/paper/b1b7a3b5801869abcb8c01cf10740031f71b0a8b,Using Supervised Machine Learning Algorithms for Automated Lithology Prediction from Wireline Log Data,"
 This paper presents advancements in machine learning and cloud deployment that enable rapid and accurate automated lithology interpretation. A supervised machine learning technique is described that enables rapid, consistent, and accurate lithology prediction alongside quantitative uncertainty from large wireline or logging-while-drilling (LWD) datasets.
 To leverage supervised machine learning, a team of geoscientists and petrophysicists made detailed lithology interpretations of wells to generate a comprehensive training dataset. Lithology interpretations were based on applying determinist cross-plotting by utilizing and combining various raw logs. This training dataset was used to develop a model and test a machine learning pipeline. The pipeline was applied to a dataset previously unseen by the algorithm, to predict lithology. A quality checking process was performed by a petrophysicist to validate new predictions delivered by the pipeline against human interpretations.
 Confidence in the interpretations was assessed in two ways. The prior probability was calculated, a measure of confidence in the input data being recognized by the model. Posterior probability was calculated, which quantifies the likelihood that a specified depth interval comprises a given lithology.
 The supervised machine learning algorithm ensured that the wells were interpreted consistently by removing interpreter biases and inconsistencies. The scalability of cloud computing enabled a large log dataset to be interpreted rapidly; >100 wells were interpreted consistently in five minutes, yielding >70% lithological match to the human petrophysical interpretation.
 Supervised machine learning methods have strong potential for classifying lithology from log data because: 1) they can automatically define complex, non-parametric, multi-variate relationships across several input logs; and 2) they allow classifications to be quantified confidently. Furthermore, this approach captured the knowledge and nuances of an interpreter's decisions by training the algorithm using human-interpreted labels.
 In the hydrocarbon industry, the quantity of generated data is predicted to increase by >300% between 2018 and 2023 (IDC, Worldwide Global DataSphere Forecast, 2019–2023). Additionally, the industry holds vast legacy data. This supervised machine learning approach can unlock the potential of some of these datasets by providing consistent lithology interpretations rapidly, allowing resources to be used more effectively.","Day 2 Wed, November 24, 2021",2021,10.2118/208559-ms,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
dd5bb42f66d9920ba09db7e9c600854f03009509,https://www.semanticscholar.org/paper/dd5bb42f66d9920ba09db7e9c600854f03009509,Internet of Things and Machine Learning Applications for Smart Precision Agriculture,"Agriculture forms the major part of our Indian economy. In the current world, agriculture and irrigation are the essential and foremost sectors. It is a mandatory need to apply information and communication technology in our agricultural industries to aid agriculturalists and farmers to improve vice all stages of crop cultivation and post-harvest. It helps to enhance the country’s G.D.P. Agriculture needs to be assisted by modern automation to produce the maximum yield. The recent development in technology has a significant impact on agriculture. The evolutions of Machine Learning (ML) and the Internet of Things (IoT) have supported researchers to implement this automation in agriculture to support farmers. ML allows farmers to improve yield make use of effective land utilisation, the fruitfulness of the soil, level of water, mineral insufficiencies control pest, trim development and horticulture. Application of remote sensors like temperature, humidity, soil moisture, water level sensors and pH value will provide an idea to on active farming, which will show accuracy as well as practical agriculture to deal with challenges in the field. This advancement could empower agricultural management systems to handle farm data in an orchestrated manner and increase the agribusiness by formulating effective strategies. This paper highlights contribute to an overview of the modern technologies deployed to agriculture and suggests an outline of the current and potential applications, and discusses the challenges and possible solutions and implementations. Besides, it elucidates the problems, specific potential solutions, and future directions for the agriculture sector using Machine Learning and the Internet of things.",Ubiquitous Computing [Working Title],2021,10.5772/INTECHOPEN.97679,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
7eb7e238c73258d034e911f7bed7f2a9cd3c5a43,https://www.semanticscholar.org/paper/7eb7e238c73258d034e911f7bed7f2a9cd3c5a43,Automated Geosteering While Drilling Using Machine Learning. Case Studies,"
 Today's oil & gas industry faces a number of different challenges. Drilling activities are ramping up due to an increase in hydrocarbon demand combined with a reduction of easy-to-recover reserves. Horizontal drilling is growing and has become an integral part of field development. The geology is becoming more and more complex requiring drilling through dense layers targeting thin-layered reservoirs with lateral changes and anisotropy. In recent years, companies have been looking at the ways of optimizing drilling costs by increasing efficiency and process automation. This has been a driver for many companies to stay profitable and efficient in the market.
 One of the areas of interest for process automation has been a geosteering. Geosteering is the real-time adjustment well trajectory while drilling to maximize effective footage in the target zone. In this paper, innovative new approaches to automation of the geosteering process will be discussed. This approach has been successfully tested and deployed in several leading O&G companies.
 The main objective of automated geosteering is to optimize horizontal well placement while freeing up time operational geologists had spent doing routine work in order to focus on complex and more intense tasks as well as the reduction of operational errors related to human factors. This paper will provide details on several automated geosteering algorithms. They have been tested successfully on large numbers of wells. The results of automated geosteering were as close as 90% to the manual interpretations done by geologists. When the results diverged, the geologists often ""agreed"" with the interpretation proposed by the algorithm.","Day 2 Tue, October 27, 2020",2020,10.2118/202046-ms,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
91033df18d9e46014932bc4dc52961a665dee5ef,https://www.semanticscholar.org/paper/91033df18d9e46014932bc4dc52961a665dee5ef,In-Vehicle Occupancy Detection And Classification Using Machine Learning,"Occupancy detection is a difficult problem. There are several mechanisms exists for occupancy detection in vehicles, particularly in Automobiles. Now, safety has become an important and necessary aspect of the automobile industry. Airbag became a basic and important safety measure in cars. Even though airbag is a vehicle safety device, it can kill children below 12 years due to its rapid action by the exerting lot of force. This project explains about detecting the number of passengers sitting in the car and then classifying each person whether he/she is a child or an adult by processing the image taken from the camera. So that the deployment of airbags can be avoided near children. Each time car speeds from 0 Kmph to 20 Kmph, occupancy of the car is determined and each one is classified again. We are using widely used technique Haar Cascades, for detection. First, we detect faces and then classify each occupant adult or child.","2020 11th International Conference on Computing, Communication and Networking Technologies (ICCCNT)",2020,10.1109/ICCCNT49239.2020.9225661,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
dace52c2aebc2657dda059072787ad2b53b5590a,https://www.semanticscholar.org/paper/dace52c2aebc2657dda059072787ad2b53b5590a,Detection of Coordinate Based Accident-Prone Areas on Road Surface using Machine Learning Methods,"The growth in the road networks in India and other developing countries have influenced the growth in transport industry and other industries, which depends on the road network for operations. The industries such as postal services or mover services have influenced the similar growths in these industries as well. However, the dependency of these industries is high on the road surface conditions and any deviation on the road surface conditions can also influence the performance of the services provided by the mentioned services. Nonetheless, the conditions of the road surface are one of the prime factors for road safety and number of evidences are found, which are discussed in subsequent sections of this work, that the bad road surface conditions are increasing the road accidents. Several parallel research attempts are deployed in order to find out, the regions where the road surface conditions are not proper, and the traffic density is higher. Nevertheless, outcomes of these parallel works are highly criticised due to the lack of accuracy in detection of the road surface defects, detection of accurate location of the defects and detection of the traffic density data from various sources. Thus, this work proposes a novel framework for detection of the road defect and further mapping to the spatial data coordinates resulting into the detection of the accident-prone zones or accident affinities of the roads. This work deploys a self-adjusting parametric coefficient-based regression model for detection of the risk factors of the road defects and in the other hand, extracts the traffic density of the road regions and further maps the accident affinities. This work outcomes into 97.69% accurate detection of the road accident affinity and demonstrates less complexity compared with the other parallel research outcomes.",,2020,10.47277/ijceit/12(3)1,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
2690e49c0113f1ffb884c95521ba6344067f0023,https://www.semanticscholar.org/paper/2690e49c0113f1ffb884c95521ba6344067f0023,Tribuo: Machine Learning with Provenance in Java,"Machine Learning models are deployed across a wide range of industries, performing a wide range of tasks. Tracking these models and ensuring they behave appropriately is becoming increasingly difficult as the number of deployed models increases. There are also new regulatory burdens for ML systems which affect human lives, requiring a link between a model and its training data in high-risk situations. Current ML monitoring systems often provide provenance and experiment tracking as a layer on top of an ML library, allowing room for imperfect tracking and skew between the tracked object and the metadata. In this paper we introduce Tribuo, a Java ML library that integrates model training, inference, strong typesafety, runtime checking, and automatic provenance recording into a single framework. All Tribuo’s models and evaluations record the full processing pipeline for input data, along with the training algorithms, hyperparameters and data transformation steps automatically. The provenance lives inside the model object and can be persisted separately using common markup formats. Tribuo implements many popular ML algorithms for classification, regression, clustering, multi-label classification and anomaly detection, along with interfaces to XGBoost, TensorFlow and ONNX Runtime. Tribuo’s source code is available at https://github.com/oracle/tribuo under an Apache 2.0 license with documentation and tutorials available at https://tribuo.org.",ArXiv,2021,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
71adbc6367f7d10d3af87c1147fb57dd6925b101,https://www.semanticscholar.org/paper/71adbc6367f7d10d3af87c1147fb57dd6925b101,Prediction of cellulose sheet cutting using Machine Learning,"Cellulose is the main raw material for the production of paper. Companies that produce it present in their production line the cutting of the cellulose sheet. This failure is sporadic and has a high economic impact since it paralyzes the production line for several hours, incurring unproductive hours and a large deployment of human and financial resources. In this research, the use of Data Mining is proposed to define a machine learning algorithm that allows predicting the cutting of the cellulose sheet in a production line of a cellulose plant in Chile. The results show that by applying this technique it is possible to predict the cutting of the cellulose sheet well in advance to take corrective actions to avoid cutting and thus minimize the economic impact associated with the failure. 
Keywords: Data Mining, machine learning, cellulose, productivity. 
References 
[1]B. Ranaganth y G. Viswanath, «Application of artificial neural network for optimizing cutting variables in laser cutting of 304 grade stainless steel,» International Journal of Applied Engineering and Technology, vol. 1, nº 1, pp. 106-112, 2011. 
[2]M. Durica, J. Frnda y L. Svabova, «Decision tree based model of business failure prediction for Polish companies,» Oeconomia Copernicana, vol. 10, nº 3, pp. 453-469, 2019. 
[3]G. Köksal, İ. Batmaz y M. C. Testik, «A review of data mining applications for quality improvement in manufacturing industry,» Expert systems with Applications, vol. 38, nº 10, pp. 13448-13467, 2011. 
[4]H. Poblete y R. Vargas, «Relacion entre densidad y propiedades de tableros HDF producidos por un proceso seco,» Maderas. Ciencia y tecnología, vol. 8, nº 3, pp. 169-182, 2006. 
[5]B. Kovalerchuk y E. Vityaev, «Data mining for financial applications,» Data Mining and Knowledge Discovery Handbook, pp. 1203-1224, 2005. 
[6]U. Fayyad, G. Piatetsky-Shapiro, P. Smyth y R. Uthurusamy, «Advances in knowledge discovery and data mining,» American Association for Artificial Intelligence, 1996. 
[7]A. K. Pandey y A. K. Dubey, «Neuro fuzzy modeling of laser beam cutting process,» Applied Mechanics and Materials, vol. 110, pp. 4109-4117, 2012. 
[8]M. Németh y G. Michaľčonok, «Preparation and cluster analysis of data from the industrial production process for failure prediction,» Research Papers Faculty of Materials Science and Technology Slovak University of Technology, vol. 24, nº 39, pp. 111-116, 2016. 
[9]S. Ballı, «A data mining approach to the diagnosis of failure modes for two serial fastened sandwich composite plates,» Journal of Composite Materials, vol. 51, nº 20, pp. 2853-2862, 2017. 
[10]S. Dindarloo y E. Siami-Irdemoosa, «Data mining in mining engineering: results of classification and clustering of shovels failures data,» International Journal of Mining, Reclamation and Environment, vol. 31, nº 2, pp. 105-118, 2017. 
[11]E. e Oliveira, V. Miguéis, L. Guimarães y J. L. Borges, «Power Transformer Failure Prediction: Classification in Imbalanced Time Series,» U. Porto Journal of Engineering, vol. 3, nº 2, pp. 34-48, 2017. 
[12]A. Taghizadeh y N. Demirel, «Application of Machine Learning for Dragline Failure Prediction,» E3S Web of Conferences, vol. 15, p. 03002, 2017. 
[13]W. Chang, Z. Xu, M. You, S. Zhou, Y. Xiao y Y. Cheng, «A Bayesian Failure Prediction Network Based on Text Sequence Mining and Clustering,» Entropy, vol. 20, nº 12, p. 923, 2018. 
[14]K. Halteh, K. Kumar y A. Gepp, «Financial distress prediction of Islamic banks using tree-based stochastic techniques,» Managerial Finance, vol. 44, nº 6, pp. 759-773, 2018. 
[15]C.-H. Liu, C.-J. Lin, Y.-H. Hu y Z.-H. You, «Predicting the failure of dental implants using supervised learning techniques,» Applied Sciences, vol. 8, nº 5, p. 698, 2018. 
[16]B. Mohammed, I. Awan, H. Ugail y M. Younas, «Failure prediction using machine learning in a virtualised HPC system and application,» Cluster Computing, vol. 22, nº 2, pp. 471-485, 2019. 
[17]O. Sukhbaatar, T. Usagawa y L. Choimaa, «An artificial neural network based early prediction of failure-prone students in blended learning course,» International Journal of Emerging Technologies in Learning (iJET)}, vol. 14, nº 19, pp. 77-92, 2019. 
[18]Z. Wang, W. Zhao y X. Hu, «Analysis of prediction model of failure depth of mine floor based on fuzzy neural network,» Geotechnical and Geological Engineering, vol. 37, nº 1, pp. 71-76, 2019. 
[19]V. S. Gujre y R. Anand, «Machine learning algorithms for failure prediction and yield improvement during electric resistance welded tube manufacturing,» Journal of Experimental \& Theoretical Artificial Intelligence, vol. 32, nº 4, pp. 601-622, 2020. 
[20]P. du Jardin, «Forecasting corporate failure using ensemble of self-organizing neural networks,» European Journal of Operational Research, vol. 288, nº 3, pp. 869-885, 2021. 
[21]R. Brachman y T. Anand, «The process of knowledge discovery in databases,» Advances in knowledge discovery and data mining, pp. 37-57, 1996. 
[22]W. Frawley, G. Piatetsky-Shapiro y C. Matheus, «Knowledge discovery in databases: An overview,» AI magazine, vol. 13, nº 3, p. 57, 1992. 
[23]F. H. Troncoso Espinosa y J. V. Ruiz Tapia, «Predicción de fuga de clientes en una empresa de distribución de gas natural mediante el uso de minería de datos,» Universidad Ciencia y Tecnología, vol. 24, nº 106, pp. 79-87, 2020. 
[24]F. H. Troncoso, «Prediction of Recidivism in Thefts and Burglaries Using Machine Learning,» Indian Journal of Science and Technology, vol. 13, nº 6, pp. 696-711, March 2020. 
[25]M. Kantardzic, Data mining: concepts, models, methods, and algorithms, John Wiley & Sons, 2011. 
[26]F. H. Troncoso Espinosa, P. G. Fuentes Figueroa y I. R. Belmar Arriagada, «Predicción de fraudes en el consumo de agua potable mediante el uso de Minería de Datos,» Universidad Ciencia y Tecnología, vol. 24, nº 104, pp. 58-66, 2020. 
[27]C. Romero y S. Ventura, «Data mining in education,» Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery, vol. 3, nº 1, pp. 12-27, 2013. 
[28]D. Larose y C. Larose, Discovering knowledge in data: an introduction to data mining, John Wiley & Sons, 2014.",Universidad Ciencia y Tecnología,2021,10.47460/uct.v25i110.481,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
dab70ece897341ebbbc8d77415845126725a3350,https://www.semanticscholar.org/paper/dab70ece897341ebbbc8d77415845126725a3350,Machine learning product key performance indicators and alignment to model evaluation,"Machine Learning has seen amazing progress the past years with increasing commercial use from industries across the business spectrum. Businesses strive for alignment of vision and mission statement to the actual products they sell. For that reason tools like the Key Performance Indicators exist in order to monitor such progress. Nevertheless, products that embed a machine learning component are being optimized with other objective functions and are being evaluated in a vacuum with specific performance evaluation metrics that often have nothing to do with the business vision. In this position paper, we highlight this gap in different instances of the machine learning life cycle, explore and critically evaluate the current available solutions in the literature and introduce Key Performance Indicators in the machine learning development process. The paper also discusses representative machine learning KPIs in the development and deployment process.","2021 3rd International Conference on Advances in Computer Technology, Information Science and Communication (CTISC)",2021,10.1109/CTISC52352.2021.00039,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
6da3a486bf9fbacef0212e55c1abf31261531996,https://www.semanticscholar.org/paper/6da3a486bf9fbacef0212e55c1abf31261531996,"Editorial Machine Learning: The Cyber-Security, Privacy, and Public Safety Opportunities and Challenges for Emerging Applications","In recent years, as there is continuous advancement of emerging applications such as cyber-physical systems, social networks, e-commerce, and 5G systems, the collection, processing, and analysis of enterprise, government, and personal data have become greatly convenient and widespread, which makes sensitive information more vulnerable to abuses. #erefore, it is urgent to explore secure mechanisms and technologies tailored for emerging applications. Machine learning (ML) has recently gained a renewed interest as the technology powering it has become more widely available and accessible to organizations of all sizes. Applications using machine learning are being deployed in contexts and for purposes that were not even imaginable a few years ago. From a cybersecurity, privacy, and public safety angle, ML brings about both opportunities and challenges for emerging applications. On the one hand, ML can help interested parties to better protect privacy in challenging situations, improving the state-of-the-art security solutions. On the other hand, ML also presents risks of opaque decision making, biased algorithms, and safety vulnerabilities, challenging traditional notions of privacy protection. #is special issue aims to provide a forum for those from academia and industry to communicate their latest results on theoretical advances and industrial case studies that combine ML techniques, such as reinforcement learning, adversarial machine learning, and deep learning, with significant problems in cybersecurity, privacy, and public safety. Research papers can be focused on offensive and defensive applications of ML to security. Submissions can contemplate original research, serious dataset collection and benchmarking, or critical surveys. Review articles are also welcome. Potential topics include but are not limited to the following:",,2021,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
dc46f12724a82cc2fae61fbe7f965614eaa91e58,https://www.semanticscholar.org/paper/dc46f12724a82cc2fae61fbe7f965614eaa91e58,First Use of Machine Learning for Penetration Rate Optimisation on Elgin Franklin,"
 Optimising the Rate of Penetration (ROP) on Development wells contributes heavily to delivery of projects ahead of schedule and has long been a goal for drilling engineers. Selecting the best parameters to achieve this has often proved difficult due to the extensive quantities of data concerning formation types, bottom-hole assembly (BHA) design and bit specifications. Legacy drilling data can also be vast and not well characterised, making it very difficult to robustly analyse manually. Additionally, multiple stakeholders can each have their own hypotheses on how to improve drilling performance, including bit vendors, directional drilling companies, drilling engineers and offshore supervisors, creating further confusion in this field.
 Together with its team of data scientists, TotalEnergies E&P UK (TEPUK) has utilised machine learning to analyse field and equipment data and produce guidelines for optimised drilling rate. The machine learning algorithm identifies parameters which have a statistical likelihood of improving ROP performance whilst drilling. The model was developed using offset well data from TotalEnergies' Realtime Support Centre (RTSC) and bit design information. This represented the first use of Machine Learning in the 20+ years of drilling on Elgin Franklin. Adapting to this new data-based method forms part of a wider digital revolution within TEPUK and the Offshore Drilling Industry. In this case, an integrated approach from the data scientists, drilling engineers and supervisors was required to transition to a new way of working.
 The first trial of using optimised parameters was on a recent Franklin well (F13) in the Cretaceous Chalk formations. The model generated statistically optimised parameter sheets which were strictly executed on site. Within the guideline sheets were suggested ranges of Revolutions per Minute (RPM), Flowrate, Weight on Bit (WOB) and Torque, as well as recommendations for bit blades and cutters. Heatmaps were generated to show what combination of WOB and RPM would likely achieve best ROP in each sub formation. The parameter range defined was specifically narrow to reduce any time spent varying parameters. In practice the new digital approach was successfully adopted offshore and contributed to the delivery of the 12 ½"" and 8 ½"" sections in record time for the field, resulting in significant savings versus AFE.
 Following the success of the guideline implementation, steps have been taken to integrate the machine learning model with live incoming data on TotalEnergies' digital drilling online platform. Since the initial trial on Franklin, online ROP optimisation features have been deployed on the Elgin field and currently provide live parameter guidance, a forecast to section TD and data driven bit change scenario analyses whist drilling.","Day 4 Fri, September 10, 2021",2021,10.2118/205466-ms,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
d5872d25a06c37ae960f26276e132adfc7dba825,https://www.semanticscholar.org/paper/d5872d25a06c37ae960f26276e132adfc7dba825,Enhancing QoE based on Machine Learning in Cloud infrastructure,"
 The cloud computing paradigm has recently attracted many industries and academic attention. It provides network access on demand and offers applications, platforms, or access to a shared pool of hardware and software resources. For traditional deployment, the user reserves the most required resources. However, this system does not guarantee an optimal use of resources and is not profitable for users. The characteristic feature of the elasticity of the cloud Computing gives the Cloud the ability to perform an automatic up / down scale resources proportional to demand. However, classical deployment only considers the use of resources based on alarm, and does not consider the quality perceived by the end user. The aim of this paper is to set up a private IAAS Cloud infrastructure and complete it by supervision tools so we could optimize the management of the cloud elasticity based on users’ point of view or QoE. We have also used a Machine learning algorithm to predict the load charge of the physical machines of the cloud so that providers could manage efficiently their data centers.",,2021,10.21203/RS.3.RS-272172/V1,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
8e988c6f97b16da03402d1f67c348b52ef6a1edd,https://www.semanticscholar.org/paper/8e988c6f97b16da03402d1f67c348b52ef6a1edd,Fraud Identification of Credit card using Machine Learning,"Online Transaction is a popular mode of payment and most of the payments done by using credit card but credit card fraud happens continuously and it leads to huge lose. So all the banks and other financial industries supports to the progress of Fraud identification of credit card. The fraudulent transactions can takes place in different ways, identification of credit card fraud is very important to the credit card companies so that customers will not be charged for the items those are not purchased. This paper explained the concepts related to credit card and used different machine learning algorithms like logistic regression and random forest method .2,84,808 credit card transactions of dataset is collected from kaggle and these transactions are from European bank. Identification of credit card is an example of classification and in this process includes analyzing and preprocessing the data set along with the deployment of Machine learning algorithms. Index Terms Credit card fraud, logistic regression, random forest",,2021,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
c0bac27ef3093e1b9fed2bd3014febb32dce71da,https://www.semanticscholar.org/paper/c0bac27ef3093e1b9fed2bd3014febb32dce71da,A Brief History of Machine Learning in Neurosurgery.,,Acta neurochirurgica. Supplement,2021,10.1007/978-3-030-85292-4_27,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
adc27778bfc2a9dd855366130089d3a7ecdf4f23,https://www.semanticscholar.org/paper/adc27778bfc2a9dd855366130089d3a7ecdf4f23,Widening Access to Applied Machine Learning with TinyML,"Broadening access to both computational and educational resources is critical to diffusing machine-learning (ML) innovation. However, today, most ML resources and experts are siloed in a few countries and organizations. In this paper, we describe our pedagogical approach to increasing access to applied ML through a massive open online course (MOOC) on Tiny Machine Learning (TinyML). We suggest that TinyML, ML on resource-constrained embedded devices, is an attractive means to widen access because TinyML both leverages low-cost and globally accessible hardware, and encourages the development of complete, self-contained applications, from data collection to deployment. To this end, a collaboration between academia (Harvard University) and industry (Google) produced a four-part MOOC that provides application-oriented instruction on how to develop solutions using TinyML. The series is openly available on the edX MOOC platform, has no prerequisites beyond basic programming, and is designed for learners from a global variety of backgrounds. It introduces pupils to real-world applications, ML algorithms, data-set engineering, and the ethical considerations of these technologies via hands-on programming and deployment of TinyML applications in both the cloud and their own microcontrollers. To facilitate continued learning, community building, and collaboration beyond the courses, we launched a standalone website, a forum, a chat, and an optional course-project competition. We also released the course materials publicly, hoping they will inspire the next generation of ML practitioners and educators and further broaden access to cutting-edge ML technologies.",Harvard Data Science Review,2021,10.1162/99608f92.762d171a,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
7f9174deb8ca93e2606343c28dfc1d717a36d834,https://www.semanticscholar.org/paper/7f9174deb8ca93e2606343c28dfc1d717a36d834,Towards Machine Learning Enabled Security Framework for IoT-based Healthcare,"The recent developments in electronic and communication technologies have brought notable revolution in the e-healthcare industry for efficient transmission of the patient's data. One of the emergent applications of telehealth monitoring is the Internet of medical things (IoMTs). They are used to transfer and monitor medical information in patient-centred systems. Patient's data is very critical, so its secure transmission is of paramount requirement in smart healthcare applications. The current era has witnessed the large-scale usage of cryptographic and biometric systems, and machine learning (ML) approaches for authentication and anomaly detection, respectively, for securing medical systems. In IoMTs, sensor devices have limited power and battery, so to provide a balance between security and resource-efficiency is also an important aspect to consider during deploying IoMT. Therefore, this research aims to present an innovate framework to protect medical information from external threats with the consumption of less possible resources of low-powered medical devices. In this study, the ML-based biometric security framework is proposed in which features are extracted from Electrocardiogram (ECG) signals for the training phase. However, in the testing phase, the user authentication will be verified by utilizing generated unique biometric EIs from the ECG and acquired coefficients from polynomial approximation. The proposed framework has got the scientific as well as economic significance; thus, it could be used for real-time healthcare applications.",2019 13th International Conference on Sensing Technology (ICST),2019,10.1109/ICST46873.2019.9047745,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
4a68e3cf16c3a8b851e33c6850ab34e160900421,https://www.semanticscholar.org/paper/4a68e3cf16c3a8b851e33c6850ab34e160900421,Internet of Things Meets Machine Learning: A Water Usage Alert Example,"The rapid growth of the electronics industry resulted in numerous, amazing and cheap devices, while fluent documentation and user-friendly programming environments are available for them. Modern educational systems worldwide have exploited this dynamic by including in their didactic curricula innovative practices that are usually called STEM actions. Added to this, enriching educational methods with real-world problem solving techniques increases students’ interest and prepares them for their future role in the society. Apparently, such challenging problems are not missing, with the depletion of natural resources to be one of the most intense ones. In this context, promising modern technological flavors like Internet of Things (IoT) and Machine Learning (ML) can join their potential to form educationally fruitful and also practically important activities targeted at increasing the social awareness for the water misuse problem, like the ones proposed herein. These activities also encourage the deployment of low-cost appliances that, only with minor modifications, can respond to a wide variety real problems in either urban or rural environments.",2022 IEEE Global Engineering Education Conference (EDUCON),2022,10.1109/EDUCON52537.2022.9766555,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
3d2c439e0aef92f0e4f1d4ec080cec05cc784951,https://www.semanticscholar.org/paper/3d2c439e0aef92f0e4f1d4ec080cec05cc784951,How to Manage Tiny Machine Learning at Scale: An Industrial Perspective,"Tiny machine learning (TinyML) has gained widespread popularity where machine learning (ML) is democratized on ubiquitous microcontrollers, processing sensor data everywhere in real-time. To manage TinyML in the industry, where mass deployment happens, we consider the hardware and software constraints, ranging from available onboard sensors and memory size to ML-model architectures and runtime platforms. However, Internet of Things (IoT) devices are typically tailored to specific tasks and are subject to heterogeneity and limited resources. Moreover, TinyML models have been developed with different structures and are often distributed without a clear understanding of their working principles, leading to a fragmented ecosystem. Considering these challenges, we propose a framework using Semantic Web technologies to enable the joint management of TinyML models and IoT devices at scale, from modeling information to discovering possible combinations and benchmarking, and eventually facilitate TinyML component exchange and reuse. We present an ontology (semantic schema) for neural network models aligned with the World Wide Web Consortium (W3C) Thing Description, which semantically describes IoT devices. Furthermore, a Knowledge Graph of 23 publicly available ML models and six IoT devices were used to demonstrate our concept in three case studies, and we shared the code and examples to enhance reproducibility: Github",ArXiv,2022,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
1f181e73a4201e37f7b74e15359f7cf84491d640,https://www.semanticscholar.org/paper/1f181e73a4201e37f7b74e15359f7cf84491d640,Supervised Machine Learning System Based Segmentation and Classification of Strokes Using Deep Learning Techniques,"Machine learning has gained importance in recent years, helped by lower computer power and also more economical data collection, enabling the safe storage, processing, and analysis of an ever-increasing volume of information. Enhanced procedures were created and deployed in huge datasets for examining hidden information and connections within non-human data sources. These analytics allow corporations to make better decisions and improve key aspects of their applicability. Machine learning has increased in popularity as just an outcome of the balanced utilization of learning techniques. The investigation takes into account various machinery and equipment, as well as supplementary cancer cell treatments for spreading medical resources and several other innovative methods to use these reserves during the period of machine learning to consider making human life a part of this process and explore the better conditions transmitted both by medicine and the computer industry. That the quality of a captured image should be increased directly, and then properly obtained information would be used to identify the image's strokes. This technique provides artifact removal, skull extraction, and system elements to improve those images. This stroke's limits and size are calculated using the feature extraction procedure. The H2O machine learning approach is used to classify strokes, combining texture-based and analytical information.",2022 IEEE International Conference on Distributed Computing and Electrical Circuits and Electronics (ICDCECE),2022,10.1109/icdcece53908.2022.9792818,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
7724522f88daa3d208ac1a7270125e6f870b6920,https://www.semanticscholar.org/paper/7724522f88daa3d208ac1a7270125e6f870b6920,Successful Development and Deployment of a Global ROP Optimization Machine Learning Model,"
 Drilling rate of penetration (ROP) is a major contributor to drilling costs. ROP is influenced by many different controllable and uncontrollable factors that are difficult to distinguish with the naked eye. Thus, machine learning (ML) models such as neural networks (NN) have gained momentum in the drilling industry. Existing models were either field-based or tool-based, which impacted the accuracy outside of the trained field. This work aims to develop one generally applicable global ROP model, reducing the effort needed to re-develop models for every application.
 A drilling dataset was gathered from exploration and development wells in both onshore and offshore operations from a variety of fields and regions. The wells were curated to have different water depths, down hole drive such as Rotary Steerable System (RSS), PDM, Standard Rotary, bit types (Mill Tooth, TCI, PDC) and inclinations (vertical or deviated). A deep neural network was used for modelling the relationship between ROP and inputs taken from real-time surface data, such as Torque, Weight-on-Bit (WOB), rotary speed (RPM), flow and pressure measurements. The performance of the ROP model was analyzed using historical data via summary statistics such as Mean Absolute Percentage Error, as well as graphical results such as residuals distributions, cumulative distribution functions of errors, and plots of ROP vs depth for independent holdout testing wells not included in the model fitting process. Analysis was done both in aggregate, and for each specific well.
 The ROP model was demonstrated to generalize effectively in all cases, with only minor increases in error metrics for the holdout test wells, where the Mean Absolute Percentage Error averaged across wells was ~20%, compared to 17.5% averaged across training wells. Furthermore, residuals distributions were centered close to zero, indicating low systematic error. This work proves the case for a ""global"" ROP prediction model applicable ""out-of-the-box"" to a broad set of drilling operations.
 A global ROP model has the potential to eliminate learning curves, reducing time and costs associated with having to develop a new model for every field. Furthermore, a model that effectively captures the relationships between parameters controllable by drillers and ROP can be used for automatically identifying drilling parameters that improve ROP. Preliminary field-testing of the ROP optimization system yielded positive results, with many examples of increased ROP realized after following drilling parameter recommendations provided by the software.","Day 2 Wed, March 23, 2022",2022,10.4043/31680-ms,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
3e3c4cc54548e6387b3fd2250705c2d6460762c4,https://www.semanticscholar.org/paper/3e3c4cc54548e6387b3fd2250705c2d6460762c4,Steel Defect Classification Using Machine Learning,"Ensuring the quality of industrial production for the steel industry is very crucial. For complete defect detection, it is important to know the exact location and class of defect, due to which it becomes difficult to apply this method and attain accuracy in both location and classification. Methods used for the detection of steel defects include the YOLO detection network, acoustic emission method, end-to-end steel surface defect classification, and detection of defects by magneto-optical imaging and neural networks. But as data is too large, deployment and training of these systems become expensive and time-consuming, therefore the algorithms used for the detection of defects should have good generalization. With the growth in computer vision and deep learning automation it is possible to classify images with maximum accuracy. We used machine learning algorithms KNN and transfer learning using VGG-16 for this task to help in quality improvement, quick detection, and classification. KNN used for the classification of defects provided fairly improved results with a significant gain in the accuracy. Detection of defects done by transfer learning via VGG-16 provided promising results. The model trained using VGG-16 achieved high accuracy of 97.54%. These techniques provide an optimal solution for both the classification and detection of defects.",2022 16th International Conference on Ubiquitous Information Management and Communication (IMCOM),2022,10.1109/IMCOM53663.2022.9721728,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
d8e02d876457c93787e5bb4a10032b62a5f2eb3d,https://www.semanticscholar.org/paper/d8e02d876457c93787e5bb4a10032b62a5f2eb3d,Extreme Cold-Bending: Geometric Considerations and Shape Prediction with Machine Learning,"Cold-bent glass is seeing increasing adoption in construction projects with non-planar geometries. This paper presents work undergone for a set of four high-rise towers, featuring 11,136 unique cold-bent panels, hundreds of which are pushed beyond 250mm. The panels are all unique, non-rectangular, and in some cases, slightly curved. The challenging geometry complicates the prediction of the final panel shape, which is an essential step for producing fabrication drawings of a panel’s flat shape prior to bending. While Machine Learning is still a nascent technology in the AEC industry, prediction is a class of problems for which many Machine Learning techniques are ideal, especially when dealing with a large quantity of data, or in this case, panels. The paper discusses the geometric characteristics of highly bent glass, a methodology for the shape prediction of the panels, and the use of Machine Learning in its implementation. The methodology was deployed for over 3,500 pieces of installed architectural glass, and was shown to reduce geometric deviations as much as 75%, down to sub-millimetre tolerances.",Challenging Glass Conference Proceedings,2022,10.47982/cgc.8.460,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
04b3acad7324420d77af71ce6e4131b27f0d7c12,https://www.semanticscholar.org/paper/04b3acad7324420d77af71ce6e4131b27f0d7c12,A Systematic Overview of the Machine Learning Methods for Mobile Malware Detection,"With the deployment of the 5G cellular system, the upsurge of diverse mobile applications and devices has increased the potential challenges and threats posed to users. Industry and academia have attempted to address cyber security challenges by implementing automated malware detection and machine learning algorithms. This study expands on previous research on machine learning-based mobile malware detection. We critically evaluate 154 selected articles and highlight their strengths and weaknesses as well as potential improvements. We explore the mobile malware detection techniques used in recent studies based on attack intentions, such as server, network, client software, client hardware, and user. In contrast to other SLR studies, our study classified the means of attack as supervised and unsupervised learning. Therefore, this article aims at providing researchers with in-depth knowledge in the field and identifying potential future research and a framework for a thorough evaluation. Furthermore, we review and summarize security challenges related to cybersecurity that can lead to more effective and practical research.",Security and Communication Networks,2022,10.1155/2022/8621083,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
de45f5d246deac33a5ffe1ef0936e5cc24601d39,https://www.semanticscholar.org/paper/de45f5d246deac33a5ffe1ef0936e5cc24601d39,Off-targetP ML: an open source machine learning framework for off-target panel safety assessment of small molecules,,Journal of Cheminformatics,2022,10.1186/s13321-022-00603-w,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
144de7c3896bbb4ea9173e257e3a449894328862,https://www.semanticscholar.org/paper/144de7c3896bbb4ea9173e257e3a449894328862,Design and Development of an Efficient Network Intrusion Detection System using Ensemble Machine Learning Techniques for Wifi Environments,"—Intrusion Detection Systems(IDS) are vital for com- puter networks as they protect against attacks that lead to privacy breaches and data leaks. Over the years, researchers have formulated IDS using machine learning (ML) and/or deep learning(DL) to detect network anomalies and identify attacks. Network Intrusion Detection Systems (NIDS) within corporate networks is a form of security that detects and generates an alarm for any cyberattacks. In both academia and industry, the concept of deploying a NIDS has been studied and adopted. The majority of NIDS research, on the other hand, has focused on detecting threats that emerge from outside of a wired connection. In addition, the NIDSs recognize Wi-Fi and wired networks alike. The Wi-Fi network’s accessible connectivity distinguishes this from the wired network. A wired connection is highly resistant to many insider threats that could occur on a Wi-Fi router. A conventional view to developing NIDSs may miss malicious activities. This paper aims to design a multi-level NIDS for Wi- Fi predominant networks to identify both organizational WiFi networks malicious activity and standard network malicious activity. Wi-Fi devices are common on campuses and businesses, and they are incorporated into the fixed wired network at the gateway. Wi-Fi networks are the primary target for this implementation; however, they are also designed to function in wired environments. For the Multi-Level NIDS, the proposed model used an ensemble learning method that pools the strengths of multiple weak learners into a single strong learner.",International Journal of Advanced Computer Science and Applications,2022,10.14569/ijacsa.2022.0130499,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
70dea8bedbdafd831b18befc7c63764557e62eff,https://www.semanticscholar.org/paper/70dea8bedbdafd831b18befc7c63764557e62eff,Benchmarking and feasibility aspects of machine learning in space systems,"Compute in space, e.g., in miniaturized satellites, requires dealing with special physical and boundary constraints, including the limited energy budget. These constraints impose strict operational conditions on the on-board data processing system and its capability in dealing with sophisticated workloads suchlike Machine Learning (ML). In the meantime, the breakthroughs in ML based on Deep Neural Networks (DNNs) in the last decade promise innovative solutions to expand the functional capabilities of on-board data processing and to drive the space industry forward. Therefore, due to the aforementioned special requirements, performance- and power-efficient, and novel solutions and architectures for deploying ML via, e.g., FPGA-enabled SoC, particularly Commercial-Off-The-Shelf (COTS) solutions, are gaining significant interest in the space industry. Therefore it is essential to conduct extensive benchmarking and feasibility and efficiency analyses in different aspects: such analyses would require the investigation of options for programming and deployment as well as the investigation of various real-world models and datasets. To this end, a research and development activity is funded by the European Space Agency (ESA) General Support Technology Programme and is led by Airbus Defence and Space GmbH with the goal of developing an ML Application Benchmark (MLAB) that covers benchmarking aspects mentioned above. In this invited paper, we provide an overview of the MLAB project and discuss development and progress in various directions, including framework analyses, model, and dataset investigation. We elaborate on a benchmarking methodology developed in the context of this project to enable the analysis of various hardware platforms and options. In the end, focus on a particular use case of aircraft detection as a real-world example and provide an analysis of various performance and accuracy indicators including, accuracy, throughput, latency, and power consumption.",CF,2022,10.1145/3528416.3530986,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
f7c13ae43697e205759f124c5e656daaa02ccd75,https://www.semanticscholar.org/paper/f7c13ae43697e205759f124c5e656daaa02ccd75,Simulation for predictive maintenance using weighted training algorithms in machine learning,"In the production, the efficient employment of machines is realized as a source of industry competition and strategic planning. In the manufacturing industries, data silos are harvested, which is needful to be monitored and deployed as an operational tool, which will associate with a right decision-making for minimizing maintenance cost. However, it is complex to prioritize and decide between several results. This article utilizes a synthetic data from a factory, mines the data to filter for an insight and performs machine learning (ML) tool in artiﬁcial intelligence (AI) to strategize a decision support and schedule a plan for maintenance. Data includes machinery, category, machinery, usage statistics, acquisition, owner’s unit, location, classiﬁcation, and downtime. An open-source ML software tool is used to replace the short of maintenance planning and schedule. Upon data mining three promising training algorithms for the insightful data are employed as a result their accuracy figures are obtained. Then the accuracy as weighted factors to forecast the priority in maintenance schedule is proposed. The analysis helps monitor the anticipation of new machines in order to minimize mean time between failures (MTBF), promote the continuous manufacturing and achieve production’s safety.",International Journal of Electrical and Computer Engineering (IJECE),2022,10.11591/ijece.v12i3.pp2839-2846,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
579117d3a7f055a5bd82cc4b71cbe6ed022bb597,https://www.semanticscholar.org/paper/579117d3a7f055a5bd82cc4b71cbe6ed022bb597,SeLoC-ML: Semantic Low-Code Engineering for Machine Learning Applications in Industrial IoT,". Internet of Things (IoT) is transforming the industry by bridging the gap between Information Technology (IT) and Operational Technology (OT). Machines are being integrated with connected sensors and managed by intelligent analytics applications, accelerating digital transformation and business operations. Bringing Machine Learning (ML) to industrial devices is an advancement aiming to promote the convergence of IT and OT. However, developing an ML application in Industrial IoT (IIoT) presents various challenges, including hardware heterogeneity, non-standardized representations of ML models, device and ML model compatibility issues, and slow application development. Successful deployment in this area requires a deep understanding of hardware, algorithms, software tools, and applications. Therefore, this paper presents a framework called Se mantic Lo w- C ode Engineering for ML Applications (SeLoC-ML), built on a low-code platform to support the rapid development of ML applications in IIoT by leveraging Semantic Web technologies. SeLoC-ML enables non-experts to easily model, discover, reuse, and matchmake ML models and devices at scale. The project code can be automatically generated for deployment on hardware based on the matching results. Developers can beneﬁt from semantic application templates, called recipes , to fast prototype end-user applications. The evaluations conﬁrm an engineering eﬀort reduction by a factor of at least three compared to traditional approaches on an industrial ML classiﬁcation case study, showing the eﬃciency and usefulness of SeLoC-ML. We share the code and welcome any contributions 4 .",ArXiv,2022,10.48550/arXiv.2207.08818,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
59f575d0c1f8e0f0dbc7f38e2307dbc536159808,https://www.semanticscholar.org/paper/59f575d0c1f8e0f0dbc7f38e2307dbc536159808,A Scalable and Reconfigurable Machine Learning and Deployment Platform for Power Plant,"With the rapid development of the Internet of Things(IoT) and industrial automation systems, the data accumulated by the industry is exponentially increasing. With the dynamics of the system, this platform should support different, possibly diverse types of models with different resource requirements. The life-long data collection and analysis for the complex coal-fired Power Plant requires a Machine Learning (ML) and deployment platform that is scalable and reconfigurable. This paper proposes a scalable and reconfigurable ML platform for a power plant based on docker technologies that support online model deployment, execution, and scheduling. In order to support the model retraining, a mechanism is proposed to manage the execution of the model training and the seamless transitions between models, without interrupting the online operation of model serving. This platform has been deployed in a power plant with two coal-fired units for about five months. Results of the field test prove that this system is flexible, model reconfigurable, and can achieve smooth model switches with minimal overhead.",IOP Conference Series: Earth and Environmental Science,2022,10.1088/1755-1315/1044/1/012004,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
ab5faef896b5b1dcb9adf2549fe13ee29b23c7de,https://www.semanticscholar.org/paper/ab5faef896b5b1dcb9adf2549fe13ee29b23c7de,Knowledge Based Recommender System for Academia Using Machine Learning: A Case Study on Higher Education Landscape of Pakistan,"Allocation of courses and research students based on faculty’s subject specialization and area of interest has always remained a challenging task for university administration due to the presence of academics’ cross-domain interests, stale faculty resumes at university portals and changing the skill set demands from the industry. Collaborative filtering and content-based recommender systems have already been in use by the industry for recommending things, such as movies, news, restaurants, and shopping items to the users, and however, no one has utilized these off-the-shelf models for enhancing the student experience and improving the quality of higher education in academia. This paper presents a case study showcasing the use of probabilistic topic models for generating recommendations to users in academia through appropriate course allocation and supervisor assignment. The proposed system coined as ScholarLite harnesses the power of machine learning to extract research themes from faculty members’ past publications, mines research interests from their resumes, and combines it with their educational background to generate recommendations for course teaching, research supervision, and industry–academia collaboration. We have shown the recommendation results on real-world data gathered from the higher education commission of the country and demonstrated that the proposed techniques are scalable across various programs offered by the universities and could be deployed in a small budget by universities for automating course and supervisor allocation procedures. The experiments confirm our performance expectation by showing good relevance and objectivity in results, thus making this decision management system more appealing for large-scale deployment and use by academia.",IEEE Access,2019,10.1109/ACCESS.2019.2912012,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
fe5b1addb4a74179fdf86346cdc63c33c14e4c3f,https://www.semanticscholar.org/paper/fe5b1addb4a74179fdf86346cdc63c33c14e4c3f,An Automated Machine Learning Based Decision Support System to Predict Hotel Booking Cancellations,"Booking cancellations negatively contribute to the production of accurate forecasts, which comprise a critical tool in the hospitality industry. Research has shown that with today’s computational power and advanced machine learning algorithms it is possible to build models to predict bookings cancellation likelihood. However, the effectiveness of these models has never been evaluated in a real environment. To fill this gap and investigate how these models can be implemented in a decision support system and its impact on demand-management decisions, a prototype was built and deployed in two hotels. The prototype, based on an automated machine learning system designed to learn continuously, lead to two important research contributions. First, the development of a training method and weighting mechanism designed to capture changes in cancellations patterns over time and learn from previous days’ predictions hits and errors. Second, the creation of a new measure – Minimum Frequency – to measure the precision of predictions over time. From a business standpoint, the prototype demonstrated its effectiveness, with results exceeding 84% in accuracy, 82% in precision, and 88% in Area Under the Curve (AUC). The system allowed hotels to predict their net demand and thus making better decisions about which bookings to accept and reject, what prices to make, and how many rooms to oversell. The systematic prediction of bookings with high probability of being canceled allowed hotels to reduce cancellations by 37 percentage points by acting to avoid their cancellation.",Data Sci. J.,2019,10.5334/DSJ-2019-032,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
2abca5533ead85215c11be31283a731a6c1fea68,https://www.semanticscholar.org/paper/2abca5533ead85215c11be31283a731a6c1fea68,"Machine Learning for 5 G / B 5 G Mobile and Wireless Communications : Potential , Limitations , and Future Directions","Driven by the demand to accommodate today’s growing mobile traffic, 5G is designed to be a key enabler and a leading infrastructure provider in the information and communication technology industry by supporting a variety of forthcoming services with diverse requirements. Considering the everincreasing complexity of the network, and the emergence of novel use cases such as autonomous cars, industrial automation, virtual reality, e-health, and several intelligent applications, machine learning (ML) is expected to be essential to assist in making the 5G vision conceivable. This paper focuses on the potential solutions for 5G from an ML-perspective. First, we establish the fundamental concepts of supervised, unsupervised, and reinforcement learning, taking a look at what has been done so far in the adoption of ML in the context of mobile and wireless communication, organizing the literature in terms of the types of learning. We then discuss the promising approaches for how ML can contribute to supporting each target 5G network requirement, emphasizing its specific use cases and evaluating the impact and limitations they have on the operation of the network. Lastly, this paper investigates the potential features of Beyond 5G (B5G), providing future research directions for how ML can contribute to realizing B5G. This article is intended to stimulate discussion on the role that ML can play to overcome the limitations for a wide deployment of autonomous 5G/B5G mobile and wireless communications. INDEX TERMS Machine learning, 5G mobile communication, B5G, Wireless communication, Mobile communication, Artificial intelligence.",,2019,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
b872b12fd0200641765b90203dddc7491430c0ac,https://www.semanticscholar.org/paper/b872b12fd0200641765b90203dddc7491430c0ac,A scalable and efficient solution to R deployment in industry with application to machine learning,"R is a popular programming language for data science and advanced analytics. Although its numerous advantages, mainly the large number of plug-and-play libraries it offers, R may not be as massively adopted in companies as it could, probably because it lacks the tooling traditionally needed in companies (client-server split, scalability, etc.). In this paper, we describe a straight-forward multi-layer approach to R deployment in industrial environments. To this end, we rely on Rserve framework, a client-server emulator for R, to which we add a load balancing layer to optimize CPU usage. Through an application to machine learning, we demonstrate how the proposed architecture significantly reduces response times, which opens new perspectives of application in emergent domains, like mobile apps and cloud services.","2017 8th International Conference on Information, Intelligence, Systems & Applications (IISA)",2017,10.1109/IISA.2017.8316398,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
d14ac2acf1b18e815385c631216eb4ee3a4fc842,https://www.semanticscholar.org/paper/d14ac2acf1b18e815385c631216eb4ee3a4fc842,"Artificial intelligence, machine learning and the evolution of healthcare","vol. 7, No. 3, MaRch 2018 223 First proposed by Professor John Mccarthy at Dartmouth college in the summer of 1956,1 artificial Intelligence (aI) – human intelligence exhibited by machines – has occupied the lexicon of successive generations of computer scientists, science fiction fans, and medical researchers. The aim of countless careers has been to build intelligent machines that can interpret the world as humans do, understand language, and learn from realworld examples. In the early part of this century, two events coincided that transformed the field of aI. The advent of widely available Graphic Processing Units (GPUs) meant that parallel processing was faster, cheaper, and more powerful. at the same time, the era of ‘Big Data’ – images, text, bioinformatics, medical records, and financial transactions, among others – was moving firmly into the mainstream, along with almost limitless data storage. These factors led to a dramatic resurgence in interest in aI in both academic circles and industries outside traditional computer science. once again, aI occupies the zeitgeist, and is poised to transform medicine at a basic science, clinical, healthcare management, and financial level. Terminology surrounding these technologies continues to evolve and can be a source of confusion for non-computer scientists. aI is broadly classified as: general aI, machines that replicate human thought, emotion, and reason (and remain, for now, in the realm of science fiction); and narrow aI, technologies that can perform specific tasks as well as, or better than, humans. Machine learning (Ml) is the study of computer algorithms that can learn complex relationships or patterns from empirical data and make accurate decisions.2 Rather than coding specific sets of instructions to accomplish a task, the machine is ‘trained’ using large amounts of data and algorithms that confer it the ability to learn how to perform the task. Unlike normal algorithms, it is the data that ‘tells’ the machine what the ‘good answer’ is, and learning occurs without explicit programming. Ml problems can be classified as supervised learning or unsupervised learning.3 In a supervised machine learning algorithm, such as face recognition, the machine is shown several examples of ‘face’ or ‘non-face’ and the algorithm learns to predict whether an unseen image is a face or not. In unsupervised learning, the images shown to the machine are not labelled as ‘face’ or ‘non-face’. artificial Neural Networks (aNN)4 are one group of algorithms used for machine learning. While aNNs have existed for over 60 years, they fell out of favour during the 1990s and 2000s. In the last half-decade, aNNs have had a resurgence under a new name: deep artificial networks (or ‘Deep learning’). aNNs are uniquely poised to take full advantage of the computational boost offered by GPUs, allowing them to crunch through data sets of enormous sizes. These range from computer vision tasks, such as image classification, object detection, face recognition, and optical character recognition (ocR), to natural language processing and even gameplaying problems (from mastering simple atari games to the recent alphaGo victory against human grandmasters).5 aNNs work by constructing layers upon layers of simple processing units (often referred to as ‘neurons’), interconnected via many differentially weighted connections. aNNs are ‘trained’ by using backpropagation algorithms, essentially telling the machine how to alter the internal parameters that are used to compute the representation in each layer from the representation in the previous Artificial intelligence, machine learning and the evolution of healthcare",Bone & joint research,2018,10.1302/2046-3758.73.BJR-2017-0147.R1,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
f72b55f4fe808b11c6c6c8fecc863cbc0bdda7b2,https://www.semanticscholar.org/paper/f72b55f4fe808b11c6c6c8fecc863cbc0bdda7b2,"Artificial Intelligence, Machine Learning, and Bias In Finance: Toward Responsible Innovation","Over the last decade, a growing number of digital startups launched bids to lure business from the financial services industry. Financial technology (“fintech”) firms deploying ever more complex and opaque algorithms assess the creditworthiness of consumers. Armed with vast quantities of data and complex algorithms to interpret the data, these firms are reigniting debates about how best to regulate financial institutions and technology firms engaged in consumer banking activities. <br><br>With a few quick taps on a smart phone, consumers can access a growing universe of apps that offer discounted interest rates on consumer loans. For proponents, the launch of fintech firms marks a new frontier in the ever-expanding utopian vision of the “technological sublime” or faith-like devotion to the potential for technology to transform us into a more equitable and just society. Consumer advocates are justifiably skeptical. While legally prohibited today, well-documented discriminatory, exclusionary, and predatory credit market practices persist. <br><br>This Essay describes fintech firms’ integration of learning algorithms and their anticipated economic and social welfare benefits — enhanced efficiency, accuracy, and accessibility. We then examine the emerging regulatory landscape. Over the last decade, federal banking regulators signaled and adopted policies that preempted state regulatory authority over fintech firms. A recent announcement by the Office of the Comptroller of the Currency (OCC) revealed the agency’s intention to allow fintech firms to apply for special purpose charters that would permit them to operate, in many respects, as national banks (“Fintech Charter Decision”). <br><br>The OCC’s Fintech Charter Decision creates gaps in the supervision of fintech firms and encourages market participants to engage in regulatory arbitrage. We argue that federal special purpose charters set the stage for regulatory arbitrage and may enable fintech firms to minimize their exposure to state antidiscrimination and consumer protection regulations. Reducing regulatory oversight of these important legal and ethical norms in a dynamic and evolving market defined by a technology that may import unconscious biases and disadvantage lower-income individuals and families raises red flags. We conclude with brief reflections regarding the necessity for courts and regulators to balance the promised benefits of fintech firms’ neo-banking initiatives with the historic and special gatekeeping role of banking platforms. Unilateral deregulatory action by state or federal regulators may undermine efforts to ensure effective oversight of fintech firms that seek to extend access to safe and affordable banking services.",,2019,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
0b08e80cf515a8e6805d6fb55fd6ab304cfe1f58,https://www.semanticscholar.org/paper/0b08e80cf515a8e6805d6fb55fd6ab304cfe1f58,Overview of Signal Processing and Machine Learning for Smart Grid Condition Monitoring,"Nowadays, the main grid is facing several challenges related to the integration of renewable energy resources, deployment of grid-level energy storage devices, deployment of new usages such as the electric vehicle, massive usage of power electronic devices at different electric grid stages and the inter-connection with microgrids and prosumers. To deal with these challenges, the concept of a smart, fault-tolerant, and self-healing power grid has emerged in the last few decades to move towards a more resilient and efficient global electrical network. The smart grid concept implies a bi-directional flow of power and information between all key energy players and requires smart information technologies, smart sensors, and low-latency communication devices. Moreover, with the increasing constraints, the power grid is subjected to several disturbances, which can evolve to a fault and, in some rare circumstances, to catastrophic failure. These disturbances include wiring issues, grounding, switching transients, load variations, and harmonics generation. These aspects justify the need for real-time condition monitoring of the power grid and its subsystems and the implementation of predictive maintenance tools. Hence, researchers in industry and academia are developing and implementing power systems monitoring approaches allowing pervasive and effective communication, fault diagnosis, disturbance classification and root cause identification. Specifically, a focus is placed on power quality monitoring using advanced signal processing and machine learning approaches for disturbances characterization. Even though this review paper is not exhaustive, it can be considered as a valuable guide for researchers and engineers who are interested in signal processing approaches and machine learning techniques for power system monitoring and grid-disturbance classification purposes.",Electronics,2021,10.3390/electronics10212725,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
ce9d9a33c93e9a7cac2a1e77be2dce2984542361,https://www.semanticscholar.org/paper/ce9d9a33c93e9a7cac2a1e77be2dce2984542361,Prediction of Throughput of EXT WLANs through Machine Learning,"WLANs are expected to be one of the foundations of next-generation wireless communication systems. They're well-known for their unique capacity to deliver high data speeds in key places (hotspots).WLANs, with IEEE 802.11 as the industry's most widely accepted standard, are a cost-effective alternative for wireless internet connection that can cover most of today's communication needs in residential and business contexts. However, the scarcity of frequency spectrum in the ISM radio bands, growing throughput demands from new bandwidth-demanding applications, and the heterogeneity of current wireless network architecture all contribute to tremendous complexity [1]. In dense WLAN deployments, such concerns become more important, resulting in several partially overlapping scenarios and coexistence issues.",2022 International Conference for Advancement in Technology (ICONAT),2022,10.1109/ICONAT53423.2022.9726087,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
47e045aec019d02f03cab06980c9d09176c6b0f5,https://www.semanticscholar.org/paper/47e045aec019d02f03cab06980c9d09176c6b0f5,Internet of Nanothings (IoNT) and Machine Learning (ML) – Innovations in drug discovery and Healthcare system,"The IoNT offers a medium to connect various nanodevices with the help of high speed networks. Using this technology nanodevices can be deployed along with other advanced technologies such as cloud computing, big data and ML. Many tedious tasks can be taken over by linked inanimate objects and better availability of information using IoNT-ML. This technology has shown great promise in improving efficiencies across numerous pharmaceutical and healthcare industries with high quality and vast datasets. It has the potential to foster innovation while simultaneously improving productivity and delivering better outcomes across the value chain. IoNT-ML can significantly improve the value proportion of pharmaceutical companies by driving innovation and the creation of new business models. This technology can be implemented in almost every aspect of the pharmaceutical industry, right from the drug discovery and development to manufacturing and marketing. This review discusses the principle of ML and its various applications in healthcare sector.",International Journal of Frontiers in Science and Technology Research,2022,10.53294/ijfstr.2022.2.1.0021,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
e332d2091e697a87bd19eb3d2bb779e6204b0bef,https://www.semanticscholar.org/paper/e332d2091e697a87bd19eb3d2bb779e6204b0bef,SON Function Coordination in Campus Networks Using Machine Learning,"With the advent of 5G, network lifecycle operations such as service initial deployment, configuration changes, upgrades, optimization, and self-healing to name a few, should be fully automated processes to reduce capital expenditure (CAPEX) and operational expenditure (OPEX), and also to allow new players such as industry owners, to come into the scene as nontraditional network operators. To this end, self-organized networks functions (SF) have been proposed as a first attempt to provide self-adaptation capabilities to mobile networks on different fronts and to reduce the error-prone human intervention. Nevertheless, deploying multiple optimization functions in a network brings demanding challenges in terms of conflicting objectives in coordination. Automatically coordinating all those functions is paramount for industry owners in campus networks (CN) since they often do not have a deep expertise to carry out network optimization in an agile manner. Typically, each SF aim at individual goals modifying coupled network parameters, generally in dissonant directions with respect to other SF, jeopardizing the global stability of the system. This work presents an explicit formulation of the joint optimization problem when load balancing optimization (LBO) and coverage and capacity optimization (CCO) are instantiated in a CN.",2022 IEEE Wireless Communications and Networking Conference (WCNC),2022,10.1109/wcnc51071.2022.9771586,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
e542a6f8eebe357e858123c7cfcb3298edbf4d41,https://www.semanticscholar.org/paper/e542a6f8eebe357e858123c7cfcb3298edbf4d41,Machine Learning-Based Overkill Reduction through Inter-Test Correlation,"As quality expectations of integrated circuits (ICs) continue to rise, contemporary semiconductor manufacturing and test solutions experience increased pressure to prevent any defective parts from being shipped, even if this comes at the cost of sacrificing yield. Known as “overkill”, this lost yield is essentially the result of overly conservative decisions made to compensate for imperfect silicon, imperfect test, as well as uncertainties related to the application wherein a fabricated IC will be eventually deployed. Such decisions are often driven by auxiliary production characterization or quality control tests and processes, which are not directly related to the specifications of a product but, rather, mainly reflect the test environment. Nevertheless, based on these tests and in an effort to err on the side of caution, industry often scraps a small yet not insignificant percentage of perfectly good devices. To address this problem and judiciously recover a portion of the yield that is left on the table without increasing risk, we introduce a machine-learning based solution which exploits the correlation between specification tests and auxiliary tests in order to independently assess confidence in the validity and significance of the latter, for which limits are empirically defined. Effectiveness of our method is evaluated using an industrial dataset provided by Texas Instruments.",2022 IEEE 40th VLSI Test Symposium (VTS),2022,10.1109/VTS52500.2021.9794170,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
4a58b12509d9b5d03e75859c24c52cf9add0acb2,https://www.semanticscholar.org/paper/4a58b12509d9b5d03e75859c24c52cf9add0acb2,Predicting Time-to-Failure of Plasma Etching Equipment using Machine Learning,"Predicting unscheduled breakdowns of plasma etching equipment can reduce maintenance costs and production losses in the semiconductor industry. However, plasma etching is a complex procedure and it is hard to capture all relevant equipment properties and behaviors in a single physical model. Machine learning offers an alternative for predicting upcoming machine failures based on relevant data points. In this paper, we describe three different machine learning tasks that can be used for that purpose: (i) predicting Time-To-Failure (TTF), (ii) predicting health state, and (iii) predicting TTF intervals of an equipment. Our results show that trained machine learning models can outperform benchmarks resembling human judgments in all three tasks. This suggest that machine learning offers a viable alternative to currently deployed plasma etching equipment maintenance strategies and decision making processes.",2019 IEEE International Conference on Prognostics and Health Management (ICPHM),2019,10.1109/ICPHM.2019.8819404,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
40fd0f9f2d0024c3a74b96dd6446959413c010ba,https://www.semanticscholar.org/paper/40fd0f9f2d0024c3a74b96dd6446959413c010ba,Application of Machine Learning (ML) for Enhancing the Transient Performance of Thermal Energy Storage (TES) Platforms Using Radial Basis Function (RBF),"Thermal energy storage (TES) can be utilized as supplemental platforms for improving operational reliability and systemic efficiency in variety of industries, such as for reducing water usage in power production (food-energy-water/ FEW nexus), chemical and agro-process industries and for improving sustainability (e.g., desalination), etc. Phase change materials (PCMs) can be used in Original Research Article Shettigar et al.; JERR, 20(4): 70-84, 2021; Article no.JERR.65675 71 TES due to their high latent heat storage capacity during phase transformation. Inorganic PCMs typically have the highest latent heat capacity and are attractive for their ability to store the larger quantities of thermal energy in small form factors while conferring respectable power ratings (however, they suffer from compromised reliability issues, that often arise from the need for subcooling). Subcooling (also known as supercooling) is a phenomenon where the temperature needs to be reduced substantially below the melting point to initiate solidification. A technique for obviating subcooling issues is to allow a small portion of the PCM to remain un-melted. This allows the PCM to initiate nucleation from the un-melted portion of PCM (this is termed as the “cold finger” technique). Thus, reliability is enhanced at the expense of substantial reduction in storage capacity. A fundamental challenge for using this technique is the inability to reliably predict and control the amount of melt fraction in the total volume of the PCM (such that a target amount of the PCM remains solidified or un-melted at the end of each melt-cycle during repeated melting and solidification of the total mass of PCM). However, using Machine Learning (ML) techniques, this deficiency can be addressed by reliably predicting and thus controlling the amount of melt fraction in the total volume of the PCM with a higher accuracy than conventional techniques (such as using multi-physics-based models or numerical solvers). Conventional techniques for predicting transient characteristics in real time control schemes typically leverage multi-physics-based models that are often effective only for a narrow range of operating conditions with concomitant disadvantages: they are highly sensitive to small variations in the measurement uncertainties and are therefore susceptible to large levels of error in the real time predictions (and are unreliable for implementation in diverse range of operating conditions). In this pioneering study, nearest neighbor search processes (such as radial basis functions) were utilized along with machine learning (ML) algorithm using a training data set to predict the PCM melt fraction and to demonstrate the feasibility (and efficacy) of this approach. This technique is simple to implement and is device independent as well as robust (i.e., it can be deployed successfully even under conditions where the sensors malfunction, such as thermocouples that are off-calibration). This technique was demonstrated successfully for predicting the melt fraction of a PCM with high accuracy and robustness. With this method, the melt fraction of a PCM can be accurately determined, which allows the maximum thermal capacity of a PCM to be utilized while mitigating reliability issues (such as subcooling) and enhancing the thermodynamic efficiencies of the TES platforms. Melting experiments were performed using a digital camera (for video recording) and a graduated cylinder containing PCM for monitoring the transient values of the melt fraction based on the height of the liquid phase of the PCM in the cylinder. An array of 3 thermocouples was mounted at specific heights within the body of the PCM to monitor the temperature transients at these specific location during the propagation of the melt front within the PCM. In the final stages of the melting process, the predictions from the ML algorithm was found to be more accurate (90~95% accuracy) than that of the conventional techniques based on physics-based solvers (~60% accuracy). The accuracy of the ML algorithm was low at smaller melt fractions (~30%) and improved substantially at higher melt fractions (~95%). Furthermore, the accomplishments of this study display the feasibility of a RBF ML method which can be implemented for the accurate prediction and control of a real world stochastic system which can exhibit nonlinear and chaotic dynamics which change over time.",,2021,10.9734/JERR/2021/V20I417296,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
649024e1a5fa08d209e1b8a4882bcd9e5876061f,https://www.semanticscholar.org/paper/649024e1a5fa08d209e1b8a4882bcd9e5876061f,Editorial: Special Section on Services Computing Management for Artificial Intelligence and Machine Learning,"F IFTEEN years ago, few would have imagined that employees could work entirely remotely or that an entire business infrastructure could exist on the Internet. With the adoption of services computing, a service that allows companies to access processing and data storage through the Internet, these business models are becoming a reality. Services computing requires a multidisciplinary lens that integrates science and technology to bridge the gap between business services and information technology (IT) services [item 1) in the Appendix]. Services computing management involves 1) ensuring services computing strategy which is allied with how the organization manages IT and how IT is aligned with organizational strategy, 2) designing, building, sourcing, and deploying resilient computing solutions, trusted, efficient, and address quality of service (QoS) expectations, and 3) overseeing all matters related to business and IT services operations and resources both across business domains and within domains such as retail, finance, healthcare, logistics, and others [item 2) in the Appendix]. The goal of services computing is to enable IT services and computing technology to perform business services more efficiently and effectively [item 3) in the Appendix]. The pervasive nature of services computing management is exhibited in almost all industry settings [item 4) in the Appendix]. In everyday life, new business service innovations will give rise to an emergent dataand information-focused economy that will only pick up steam as both consumer and business utilization of Internet of Things are advanced. Concomitantly, we are moving toward an era of artificially intelligent (AI) (e.g., cognitive computing) services, which are deployed in multiscale, complex distributed architectures. Cognitive computing is the use of computerized models to simulate the human thought process in complex situations where the answers may be ambiguous and uncertain. Computers are increasingly capable of doing things that humans could once do exclusively. Today smart machines are becoming like humans by recognizing voices, processing natural language, learning, and interacting and learning with the physical world through their vision, smell, touch, and other senses, mobility, and motor control. In some cases, they do a much faster and better job than humans at recognizing patterns, performing rule-based analysis on a very large amount of data, and solving both structured and unstructured problems [item 5) in the Appendix].",IEEE Trans. Engineering Management,2021,10.1109/tem.2020.3024363,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
3266b92b981057a5ef152ccbdbdc0481045c03be,https://www.semanticscholar.org/paper/3266b92b981057a5ef152ccbdbdc0481045c03be,"Realistically Integrating Machine Learning Into Clinical Practice: A Road Map of Opportunities, Challenges, and a Potential Future.","Recently, interest in machine learning has rapidly grown within the health care community. This popularity has been due in part to the advances in perception tasks such as speech processing and image analysis, which has led to the successful implementation of machine learning across a variety of industries: from the ancient game of Go to Netflix movie suggestions.1 The abundance of electronic health record data coupled with the low costs of computation power and data storage makes machine learning and medicine well matched. Although health care has been traditionally slow to adopt new technologies, the time is approaching when machine learning delivers on promises to improve the current and future practice of perioperative medicine. Deployment and operationalization of a machinelearning model require synthesizing knowledge in data processing and model development with a knowledge of medicine and clinical workflows. In this article, we hope to elucidate what machine learning is and why it will transform clinical care, discuss what it takes to implement machine learning in clinical care, address current limitations and drawbacks, and ultimately examine what the future of machine learning in health care may hold. A PRIMER ON MACHINE LEARNING Machine learning is a subset of artificial intelligence in which a computer iteratively learns from data without explicit rule-based programming. Machine-learning models find patterns within data and apply these patterns to new data to make predictions. Because of the computer’s ability to rapidly process large amounts of electronic data, these models are advantageous when using large datasets. Where humans can become overwhelmed with increasingly large and complex data inputs, machine-learning models often thrive. The recent rapid explosion in computational processing power and increasing availability of large data enable capabilities beyond traditional rule-based modeling, including improved analytical capacities and accessibility to unique, potentially hidden insights. As these methods become more widely adopted in the medical community, all parties will benefit. Improvements include increased care efficiency and more accurate hospital reimbursements and increase care quality, such as improved disease classification, predicting complications, and ultimately better outcomes, Machine-learning models have been created to predict an increasing number of clinical outcomes, such as diagnoses and mortality, with applications including C. difficile infection in the inpatient hospital setting,2 identifying molecular markers for cancer treatments,3 and postoperative surgical outcomes.4 Examples of machine learning include a cardiologist using an automated interpretation of an ECG and a radiologist using an automated detection of a lung nodule in a chest x-ray. In both of these examples, a machine-learning model approximates a trained physician’s diagnosis with high accuracy. To date, models are successfully constructed to answer a single clinical question by using appropriately labeled representative data. The tools to develop these models (using languages such as R and Python) are largely free of charge and openly Realistically Integrating Machine Learning Into Clinical Practice: A Road Map of Opportunities, Challenges, and a Potential Future",Anesthesia and analgesia,2020,10.1213/ANE.0000000000004575,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
201fb209698fa02fdc717a9fc4431193ef37d4ec,https://www.semanticscholar.org/paper/201fb209698fa02fdc717a9fc4431193ef37d4ec,Supervised Machine Learning Techniques for Efficient Network Intrusion Detection,"Cloud computing is gaining significant traction and virtualized data centers are becoming popular as a cost-effective infrastructure in telecommunication industry. Infrastructure as a Service (IaaS), Platform as a Service (PaaS) and Software as a Service (SaaS) are being widely deployed and utilized by end users, including many private as well as public organizations. Despite its wide-spread acceptance, security is still the biggest threat in cloud computing environments. Users of cloud services are under constant fear of data loss, security breaches, information theft and availability issues. Recently, learning-based methods for security applications are gaining popularity in the literature with the advents in machine learning (ML) techniques. In this work, we explore applicability of two well-known machine learning approaches, which are, Artificial Neural Networks (ANN) and Support Vector Machines (SVM), to detect intrusions or anomalous behavior in the cloud environment. We have developed ML models using ANN and SVM techniques and have compared their performances. We have used UNSW-NB-15 dataset to train and test the models. In addition, we have performed feature engineering and parameter tuning to find out optimal set of features with maximum accuracy to reduce the training time and complexity of the ML models. We observe that with proper features set, SVM and ANN techniques have been able to achieve anomaly detection accuracy of 91% and 92% respectively, which is higher compared against that of the one achieved in the literature, with reduced number of features needed to train the models.",2019 28th International Conference on Computer Communication and Networks (ICCCN),2019,10.1109/ICCCN.2019.8847179,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
abe44d51eaa69e752a06e7d2113c59af6f13ab4e,https://www.semanticscholar.org/paper/abe44d51eaa69e752a06e7d2113c59af6f13ab4e,MACHINE LEARNING APPROACH FOR SENSING HARMFUL GAS DETECTION,"Industrial Air pollution that acts on our everyday activities and general well being. This thing constitutes a warning via the system and therefore the general well being on the earth. The terrible need to be compelled to detect the quality of ambient air is incredibly obtrusive, due to the rise in factory-made pursuit over the latest years. Folks have to be compelled to grasp the extent on which their pursuit has an effect on quality of ambient air. This paper came up with the Associate in Industrial Air pollution observance approach. This approach is being expanded to exploit the Arduino microcontroller. This Industrial pollution observance approach is sketched to analyze quality of ambient air in period. The clean air is estimated by the designed approach to be precise. The consequence is shown on the created apparatus exhibit port along with accessed cloud on any sensible portable accessory. This paper offers the improvement of pollution tracking systems with deployment of intelligent sensors. The proposed approach finds application in industry and additionally in monitoring of causes like a fan, motors.",,2021,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
958b6ab9bdc23304c6f0245609b480f431b909f6,https://www.semanticscholar.org/paper/958b6ab9bdc23304c6f0245609b480f431b909f6,Risk Metrics to Measure Safety Performance of the National Airspace System: Implementation Using Machine Learning,"Metrics are the key tools to monitor the safety performance of complex systems like air traffic control. Developing risk metrics is a complex process, which involves prediction of risk and identification of different potential outcomes of undesired safety events. For instance, a metric that helps monitor the risk on the surface in an airport environment needs to detect and identify events such as runway excursions, runway incursions, and taxiway incidents and assign appropriate numerical indices proportional to the outcome of each event. An accident with a fatality, injury and aircraft damage will have a corresponding weight for each outcome and incident that involves no outcome can be assigned a severity weight based on its probability of becoming an accident. Models that support such metrics needs to be able to process data from diverse sources. In this paper, we discuss key aspects of two comprehensive metrics that the office of Safety in the Air Traffic Organization has deployed recently: 1) How to employ an automation to detect all relevant events from different data sources and 2) how to assign severity weights for different types of events (accidents and incidents).1. Detection of Relevant Events: one key element of supporting a comprehensive metrics is an automated detection of relevant events by categories. Unlike in aviation, the use of AI and ML have permeated in most industries. Aviation is a safety-critical domain in which there is very little tolerance for failures. The stringent requirements of aviation have contributed to the slow adoption of AI and ML in aviation, in general. However, many aviation sectors are increasingly adopting AI systems, ranging from automating simple, yet tedious and repetitive tasks to a more sophisticated application of a complex autonomous air traffic control system to de-conflict traffic. In this paper, we outline how machine learning models were employed to identify relevant accidents and incidents to support two metrics, a surface safety metric and an airborne safety metric.2. Severity Weighting Scheme: for a metric to be a comprehensive measure and indicate the overall performance of the system, it needs to account for various types of accidents and incidents (precursors) that occur in the system. This paper shows how a weighting scheme we developed to measure the outcome of accidents, such as injuries to people and damage to property, as well as probabilistically determine the severity of incidents with no outcome. The aggregation of all weights along with the frequency of occurrence in a given period represent the overall safety performance of the system.",2021 IEEE/AIAA 40th Digital Avionics Systems Conference (DASC),2021,10.1109/dasc52595.2021.9594373,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
45c2576390cb05603ce9d796105d94567575beab,https://www.semanticscholar.org/paper/45c2576390cb05603ce9d796105d94567575beab,Secure Cloud Auditability for Virtual Machines by Adaptive Characterization Using Machine Learning Methods,"With the Present days increasing demand for the higher performance with the application developers have started considering cloud computing and cloud-based data centres as one of the prime options for hosting the application. Number of parallel research outcomes have for making a data centre secure, the data centre infrastructure must go through the auditing process. During the auditing process, auditors can access VMs, applications and data deployed on the virtual machines. The downside of the data in the VMs can be highly sensitive and during the process of audits, it is highly complex to permits based on the requests and can increase the total time taken to complete the tasks. Henceforth, the demand for the selective and adaptive auditing is the need of the current research. However, these outcomes are criticised for higher time complexity and less accuracy. Thus, this work proposes a predictive method for analysing the characteristics of the VM applications and the characteristics from the auditors and finally granting the access to the virtual machine by building a predictive regression model. The proposed algorithm demonstrates 50% of less time complexity to the other parallel research for making the cloud-based application development industry a safer and faster place.",Operational Research in Engineering Sciences: Theory and Applications,2021,10.31181/oresta20402059t,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
15de9b5958345f9e9327206e6fd14af8b95906d9,https://www.semanticscholar.org/paper/15de9b5958345f9e9327206e6fd14af8b95906d9,IoT based Smart Agriculture using Machine Learning,"Agriculture balances both food requirement for mankind and supplies indispensable raw materials for many industries, and it is the most significant and fundamental occupation in India. The advancement in inventive farming techniques is gradually enhancing the crop yield making it more profitable and reduce irrigation wastages. The proposed model is a smart irrigation system which predicts the water requirement for a crop, using machine learning algorithm. Moisture, temperature and humidity are the three most essential parameters to determine the quantity of water required in any agriculture field. This system comprises of temperature, humidity and moisture sensor, deployed in an agricultural field, sends data through a microprocessor, developing an IoT device with cloud. Decision tree algorithm, an efficient machine learning algorithm is applied on the data sensed from the field in to predict results efficiently. The results obtained through decision tree algorithm is sent through a mail alert to the farmers, which helps in decision making regarding water supply in advance.",2020 Second International Conference on Inventive Research in Computing Applications (ICIRCA),2020,10.1109/ICIRCA48905.2020.9183373,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
d4626f9d6d4401a12d8d98ab5abd4bb61f5a1286,https://www.semanticscholar.org/paper/d4626f9d6d4401a12d8d98ab5abd4bb61f5a1286,Computer Vision Machine Learning and Future-Oriented Ethics,"Computer Vision Machine Learning (CVML) in the application of facial recognition is currently being researched, developed, and deployed across the world. It is of interest to governments, technology companies, and consumers. However, fundamental issues remain related to human rights, error rates, and bias. These issues have the potential to create societal backlash towards the technology which could limit its benefits as well as harm people in the process. To develop facial recognition technology that will be beneficial to society in and beyond the next decade, society must put ethics at the forefront. Drawing on AI4People’s adaption of bioethics for AI, Luciano Floridi’s distributed morality framework, Kate Crawford’s definition of harms of representation, and Microsoft’s leadership in facial recognition ethics within the industry, this paper explores stakeholder responsibility within CVML to create the best integration of CVML for society. The paper attempts to connect ethics with praxis in making decisions related to",,2019,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
a24566a1decf21cd7b0cdc3156b3425153544086,https://www.semanticscholar.org/paper/a24566a1decf21cd7b0cdc3156b3425153544086,Machine Learning and Artificial Intelligence as a Complement to Condition Monitoring in a Predictive Maintenance Setting,"
 In recent years, the oil and gas industry has gained greater operational efficiencies and productivity by deploying advanced technologies, such as smart sensors, data analytics, artificial intelligence and machine learning — all linked via Internet of Things connectivity. This transformation is profound, but just starting. Leading offshore E&P operators envision using such applications to help drive their production costs to as low as $7 per barrel or less. A large North Sea operator among them successfully deployed a low-manned platform in the Ivar Aasen field in December 2016, operating it via redundant control rooms — one on the platform, the other onshore 1,000 kilometers away in Trondheim, Norway. In January 2019, the offshore control room operators handed over the platform's control to the onshore operators, and it is now managed exclusively from the onshore one. One particular application — remote condition monitoring of equipment — supports a proactive, more predictive condition-based maintenance program, which is helping to ensure equipment availability, maximize utilization, and find ways to improve performance. This paper will explain the use case in greater detail, including insights into how artificial intelligence and machine learning are incorporated into this operational model. Also described will be the application of a closed-loop lifecycle platform management model, using the concepts of digital twins from pre-FEED and FEED phases through construction, commissioning, and an expected lifecycle spanning 20 years of operations. It is derived from an update to a paper presented at the 2018 SPE Offshore Technology Conference (OTC) that introduced the use case in its 2017-18 operating model, but that was before the debut of the platform's exclusive monitoring of its operations by its onshore control room.","Day 1 Tue, April 09, 2019",2019,10.2118/194590-MS,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
d3098f6352b7bcb1f9bcc8220b41798d43dbfeef,https://www.semanticscholar.org/paper/d3098f6352b7bcb1f9bcc8220b41798d43dbfeef,Service-Oriented Pervasive Platform Supporting Machine Learning Applications in Smart Buildings,,ICSOC Workshops,2019,10.1007/978-3-030-45989-5_1,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
65d77c4e885d0476fe46db4e22df383d14904eee,https://www.semanticscholar.org/paper/65d77c4e885d0476fe46db4e22df383d14904eee,Event Recognition on Time Series Frac Data using Machine Learning – Part II,"
 Hydraulic fracturing pumping data is recorded in the field at one-second intervals. Engineers spend hours identifying events such as Instantaneous Shut-in Pressure (ISIP) in the time-series data that is generated. The ISIP flag is placed at the end of the stage pumping time, immediately after shut-in and before the pressure starts to drop. This is estimated by placing a straight line on the early pressure decline and locating the point in time where the pressure rate is zero. Manual selection of this flag is time-consuming, prone to error, and inconsistent due to differing interpretation methods across the industry. The purpose of this study is to demonstrate an automated process to identify accurate and consistent ISIP events in a high-frequency time-series data set using machine learning algorithms.
 This study is based on the analysis of metered high-frequency fracturing treatment data from wells landed in different formations across North America coupled with supervised machine learning algorithms. The pumping data includes treating pressure and slurry rate for 870 stages from the Wolfcamp, Bone Spring, Granite Wash, Barnett, Meramec, Niobrara, Codell, Bakken, Three Forks, Haynesville, Bossier, Caney, and Marcellus formations, for a total of over 7 million rows of data per channel. Eighty percent of the data is used to train the model, seven percent is used for validation, and the remaining thirteen percent forms the test set used for the final evaluation. To allow the algorithm to run leaner, the dataset was pre-processed using smoothing techniques, and the rate of change of the main data channels were added. The selected algorithm, an artificial neural network (ANN), was trained to recognize and isolate the necessary data from the treating plot that will be used to predict the ISIP. Once the data is isolated, a filter is used to extract the portion of the data to be used. A second machine learning algorithm, linear regression, is then applied to the portion of extracted data to predict the ISIP value when the slurry rate is equal to zero.
 Classification techniques were used to generate an accurate suggestion of the reduced dataset needed to recognize the ISIP event in a high-frequency treating plot. The neural network achieved a classification accuracy (on the training and validation sets) of approximately 98 percent when isolating the target region. The subsequent ISIP predictions from the linear regression on the test set had an average accuracy of +/- 50 psi when compared to the manually picked values. Considering that the typical range for ISIP values is between 2,500 psi and 9,000 psi, 50 psi represents a 0.5% to 2% error. A limitation of this method is that it requires periodic re-training with new field data to improve the prediction robustness and to maintain high accuracy.
 Automatically labeling relevant regions of high-frequency hydraulic fracturing treatment plots using classification techniques can lead to simple and effective procedures for identifying events of interest. Accurate flag selection makes processing large volumes of fracture treatment data viable and significantly reduces the time spent reviewing field data for quality control. The method will also allow rapid reprocessing of historical data. The benefits of using simple (and accurate) models include ease of deployment, ease of debugging, and extremely fast prediction and re-training (updating the model).","Day 2 Fri, November 08, 2019",2019,10.2118/197093-ms,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
9dac06fb416dfbfca7ffaa70da3a2f8b19aed3bd,https://www.semanticscholar.org/paper/9dac06fb416dfbfca7ffaa70da3a2f8b19aed3bd,Exploring Machine Learning Models to Predict Harmonized System Code,,EMCIS,2019,10.1007/978-3-030-44322-1_22,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
d20a249f7fa8587e172ad4f82c954b3a0ca65938,https://www.semanticscholar.org/paper/d20a249f7fa8587e172ad4f82c954b3a0ca65938,Thales XAI Platform: Adaptable Explanation of Machine Learning Systems - A Knowledge Graphs Perspective,"Explanation in Machine Learning systems has been identified to be the main asset to have for large scale deployment of Artificial Intelligence (AI) in critical systems. Explanations could be example-, features-, semantics-based or even counterfactual to potentially action on an AI system; they could be represented in many different ways e.g., textual, graphical, or visual. All representations serve different means, purpose and operators. We built the first-of-its-kind XAI (eXplainable AI) platform for critical systems i.e., Thales XAI Platform which aims at serving explanations through various forms. This paper emphasizes on the semantics-based explanations for Machine Learning systems. 1 Explainable AI in Critical Systems Motivation: The current hype of Artificial Intelligence (AI) mostly refers to the success of Machine Learning (ML) and its sub-domain of deep learning. However industries operating with critical systems are either highly regulated, or require high level of certification and robustness. Therefore, such industry constraints do limit the adoption of non deterministic and ML systems. Answers to the question of explainability will be intrinsically connected to the adoption of AI in industry at scale. Indeed explanation, which could be used for debugging intelligent systems or deciding to follow a recommendation in real-time, will increase acceptance and (business) user trust. Explainable AI (XAI) is now referring to the core backup for industry to apply AI in products at scale, particularly for industries operating with critical systems. Focus: Thales XAI Platform is designed to provide explanation for a ML task (classification, regression, object detection, segmentation). Although Thales XAI Platform does provide different levels of explanation e.g., example-based, featuresbased, counterfactual using textual and visual representations, we emphasis only on the semantics-based explanation through knowledge graphs. ? Copyright c © 2019 for this paper by its authors. Use permitted under Creative Commons License Attribution 4.0 International (CC BY 4.0). Critical Applications: From adapting a plane trajectory, stopping a train, refitting a boat to reconfiguring a satellite, all are examples of critical situations where explanation is a must-to-have to follow an AI system decision. 2 Why Knowledge Graphs for Explainable AI? State-of-the-Art Limitations: Most approaches limits explanation of ML systems to features involved in the data and model, or at best to examples, prototypes or counterfactuals. Explanation should go beyond correlation (features importance) and numerical similarity (local explanation). Opportunity: By expanding and linking initial (training, validation and test) data with entities in knowledge graphs, (i) context is encoded, (ii) connections and relations are exposed, and (iii) inference and causation are natively supported. Knowledge graphs are used for encoding better representation of data, structuring a ML model in a more interpretable way, and adopt a semantic similarity for local (instance-based) and global (model-based) explanation. 3 Thales XAI Platform: A Knowledge Graph Perspective (Semantic) Perspective: The platform is combining ML and reasoning functionalities to expose a human-like rational as explanation when (i) recognizing an object (in a raw image) of any class in a knowledge graph, (ii) predicting a link in a knowledge graph. Thales XAI Platform is using state-of-the-art Semantic Web tools for enriching input, output (class) data with DBpedia (4, 233, 000 resources) and domain-specific knowledge graphs, usually enterprise knowledge graphs. This is a crucial step for contextualizing training, validation, test data. Explainable ML Classifications: Starting from raw images, as unstructured data, but with class labels augmented with a domain knowledge graph, Thales XAI Platform relies on existing neural network architectures to build the most appropriate models. All confidence scores of output classes on any input image are updated based on the semantic description of the output classes. For instance, an input classified as a car will have a higher overall confidence score in case some properties of car in the knowledge graph are retrieved e.g., having wheels, being on a road. In addition the platform is embedding naturally explanation i.e., properties of the objects retrieved in both the raw data and knowledge graph. Explainable Relational Learning: Starting from relational data, structured as graph, and augmented with a domain knowledge graph, Thales XAI Platform relies on existing knowledge graph embeddings frameworks to build the most appropriate models. Explanation of any link prediction is retrieved by identifying representative hotspots in the knowledge graph i.e., connected parts of the graphs that negatively impact prediction accuracy when removed.",SEMWEB,2019,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
ae70d4843199ece95f07c8fc75fd0abf2a913a5c,https://www.semanticscholar.org/paper/ae70d4843199ece95f07c8fc75fd0abf2a913a5c,A Framework to Monitor Machine Learning Systems Using Concept Drift Detection,,BIS,2019,10.1007/978-3-030-20485-3_17,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
efda16f0b76a13c6bd5f41e1e8733a7b2b35a8d3,https://www.semanticscholar.org/paper/efda16f0b76a13c6bd5f41e1e8733a7b2b35a8d3,Jumping in at the deep end: how to experiment with machine learning in post-production software,"Recent years has seen an explosion in Machine Learning (ML) research. The challenge is now to transfer these new algorithms into the hands of artists and TD's in visual effects and animation studios, so that they can start experimenting with ML within their existing pipelines. This paper presents some of the current challenges to experimentation and deployment of ML frameworks in the post-production industry. It introduces our open-source ""ML-Server"" client / server system as an answer to enabling rapid prototyping, experimentation and development of ML models in post-production software. Data, code and examples for the system can be found on the GitHub repository page: https://github.com/TheFoundryVisionmongers/nuke-ML-server",DigiPro,2019,10.1145/3329715.3338880,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
d4fbbd76f98d6685b67678f2739db26b97720391,https://www.semanticscholar.org/paper/d4fbbd76f98d6685b67678f2739db26b97720391,"Machines as the New Oompa-Loompas: Trade Secrecy, the Cloud, Machine Learning, and Automation","In previous work, I wrote about how trade secrecy drives the plot of Roald Dahl’s novel Charlie and the Chocolate Factory, explaining how the Oompa-Loompas are the ideal solution to Willy Wonka’s competitive problems. Since publishing that piece, I have been struck by the proliferating Oompa-Loompas in contemporary life: computing machines filled with software and fed on data. These computers, software, and data might not look like Oompa-Loompas, but they function as Wonka’s tribe does: holding their secrets tightly and internally for the businesses for which these machines are deployed. 
 
Computing machines were not always such effective secret-keeping Oompa Loompas. As this Article describes, at least three recent shifts in the computing industry — cloud computing, the increasing primacy of data and machine learning, and automation — have turned these machines into the new Oompa-Loompas. While new technologies enabled this shift, trade secret law has played an important role here as well. Like other intellectual property rights, trade secret law has a body of built-in limitations to ensure that the incentives offered by the law’s protection do not become so great that they harm follow-on innovation — new innovation that builds on existing innovation — and competition. This Article argues that, in light of the technological shifts in computing, the incentives that trade secret law currently provide to develop these contemporary Oompa-Loompas are excessive in relation to their worrisome effects on follow-on innovation and competition by others. These technological shifts allow businesses to circumvent trade secret law’s central limitations, thereby overfortifying trade secrecy protection. The Article then addresses how trade secret law might be changed — by removing or diminishing its protection — to restore balance for the good of both competition and innovation.",,2019,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
41b21ca8f029ef01a6ef191c5963327f266464de,https://www.semanticscholar.org/paper/41b21ca8f029ef01a6ef191c5963327f266464de,Mitigation of Distributed Denial of Service ( DDoS ) Attacks over Software Defined Networks ( SDN ) using Machine Learning and Deep Learning Techniques,"Software Defined Networking (SDN) is an emerging networking paradigm which enables network control to be confined to a logically centralized controller. This enables global visibility of network and easier network management. The capability to program network through high level programming languages makes SDN a suitable network model to be extensively deployed in live environments. Still SDN is subject to several network attacks, among which DDoS Distributed Denial of Service attack is the most prominent one. Controller which is the brain of SDN can be paralyzed by a high scale DDoS attack. Security of SDN is in immature state and considerable research is done in this area by both industry and academia. This paper focuses on the SDN DDoS mitigation techniques using Machine Learning (ML) and Deep Learning (DL) techniques. Network traffic features for determining DDoS are also surveyed in this work.",,2019,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
5e4fb30721f9188200ef5b13ea2538664a5acf27,https://www.semanticscholar.org/paper/5e4fb30721f9188200ef5b13ea2538664a5acf27,A Machine Learning Based Write Policy for SSD Cache in Cloud Block Storage,"Nowadays, SSD cache plays an important role in cloud storage systems. The associated write policy, which enforces an admission control policy regarding filling data into the cache, has a significant impact on the performance of the cache system and the amount of write traffic to SSD caches. Based on our analysis on a typical cloud block storage system, approximately 47.09% writes are write-only, i.e., writes to the blocks which have not been read during a certain time window. Naively writing the write-only data to the SSD cache unnecessarily introduces a large number of harmful writes to the SSD cache without any contribution to cache performance. On the other hand, it is a challenging task to identify and filter out those write-only data in a real-time manner, especially in a cloud environment running changing and diverse workloads.In this paper, to alleviate the above cache problem, we propose an ML-WP, Machine Learning Based Write Policy, which reduces write traffic to SSDs by avoiding writing write-only data. The main challenge in this approach is to identify write-only data in a real-time manner. To realize ML-WP and achieve accurate write-only data identification, we use machine learning methods to classify data into two groups (i.e., write-only and normal data). Based on this classification, the write-only data is directly written to backend storage without being cached. Experimental results show that, compared with the industry widely deployed write-back policy, ML-WP decreases write traffic to SSD cache by 41.52%, while improving the hit ratio by 2.61% and reducing the average read latency by 37.52%.","2020 Design, Automation & Test in Europe Conference & Exhibition (DATE)",2020,10.23919/DATE48585.2020.9116539,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
cdcbcca54c19d6577ecd067b1b6cc80ce2d3a5fd,https://www.semanticscholar.org/paper/cdcbcca54c19d6577ecd067b1b6cc80ce2d3a5fd,Machine learning: technologies and potential application at mining companies,"Implementation of machine learning systems is currently one of the most sought-after spheres of human activities at the interface of information technologies, mathematical analysis and statistics. Machine learning technologies are penetrating into our life through applied software created with the help of artificial intelligence algorithms. It is obvious that machine learning technologies will be developing fast and becoming part of the human information space both in our everyday life and in professional activities. However, building of machine learning systems requires great labour contribution of specialists in the sphere of artificial intelligence and the subject area where this technology is to be applied. The article considers technologies and potential application of machine learning at mining companies. The article describes basic methods of machine learning: unsupervised learning, action learning, semi-supervised machine learning. The criteria are singled out to assess machine learning: operation speed; assessment time; implemented model accuracy; ease of integration; flexible deployment within the subject area; ease of practical application; result visualization. The article describes practical application of machine learning technologies and considers the dispatch system at a mining enterprise (as exemplified by the dispatch system of the mining and transportation complex “Quarry” used to increase efficiency of operating management of enterprise performance; to increase reliability and agility of mining and transportation complex performance records and monitoring. There is also a list of equipment performance data that can be stored in the database and used as a basis for processing by machine learning algorithms and obtaining new knowledge. Application of machine learning technologies in the mining industry is a promising and necessary condition for increasing mining efficiency and ensuring environmental security. Selection of the optimal process flow sheet of mining operations, selection of the optimal complex of stripping and mining equipment, optimal planning of mining operations and mining equipment performance control are some of the tasks where machine learning technologies can be used. However, despite prospectivity of machine learning technologies, this trend still remains understudied and requires further research.",,2020,10.1051/e3sconf/202016603007,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
e5f8e9cb3480536cd50fdc4664780138a654a33f,https://www.semanticscholar.org/paper/e5f8e9cb3480536cd50fdc4664780138a654a33f,"The Dark Side of Machine Learning Algorithms: How and Why They Can Leverage Bias, and What Can Be Done to Pursue Algorithmic Fairness","Machine learning and access to big data are revolutionizing the way many industries operate, providing analytics and automation to many aspects of real-world practical tasks that were previously thought to be necessarily manual. With the pervasiveness of artificial intelligence and machine learning over the past decade, and their epidemic spread in a variety of applications, algorithmic fairness has become a prominent open research problem. For instance, machine learning is used in courts to assess the probability that a defendant recommits a crime; in the medical domain to assist with diagnosis or predict predisposition to certain diseases; in social welfare systems; and autonomous vehicles. The decision making processes in these real-world applications have a direct effect on people's lives, and can cause harm to society if the machine learning algorithms deployed are not designed with considerations to fairness. The ability to collect and analyze large datasets for problems in many domains brings forward the danger of implicit data bias, which could be harmful. Data, especially big data, is often heterogeneous, generated by different subgroups with their owncharacteristics and behaviors. Furthermore, data collection strategies vary vastly across domains, and labelling of examples is performed by human annotators, thus causing the labelling process to amplify inherent biases the annotators might harbor. A model learned on biased data may not only lead to unfair and inaccurate predictions, but also significantly disadvantage certain subgroups, and lead to unfairness in downstream learning tasks. There aremultiple ways in which discriminatory bias can seep into data: for example, in medical domains, there are many instances in whichthe data used are skewed toward certain populations-which canhave dangerous consequences for the underrepresented communities [1]. Another example are large-scale datasets widely used in machine learning tasks, like ImageNet and Open Images: [2] shows that these datasets suffer from representation bias, and advocates for the need to incorporate geo-diversity and inclusion. Yet another example are the popular face recognition and generation datasets like CelebA and Flickr-Faces-HQ, where the ethnic and racial breakdown of example faces shows significant representation bias, evident in downstream tasks like face reconstruction from an obfuscated image [8]. In order to be able to fight discriminatory use of machine learning algorithms that leverage such biases, one needs to first define the notion of algorithmic fairness. Broadly, fairness is the absence of any prejudice or favoritism towards an individual or a group based on their intrinsic or acquired traits in the context of decision making [3]. Fairness definitions fall under three broad types: individual fairness (whereby similar predictions are given to similar individuals [4, 5]), group fairness (whereby different groups are treated equally [4, 5]), and subgroup fairness (whereby a group fairness constraint is being selected, and the task is to determine whether the constraint holds over a large collection of subgroups [6, 7]). In this talk, I will discuss a formal definition of these fairness constraints, examine the ways in which machine learning algorithms can amplify representation bias, and discuss how bias in both the example set and label set of popular datasets has been misused in a discriminatory manner. I will touch upon the issues of ethics and accountability, and present open research directions for tackling algorithmic fairness at the representation level.",KDD,2020,10.1145/3394486.3411068,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
cdcbcca54c19d6577ecd067b1b6cc80ce2d3a5fd,https://www.semanticscholar.org/paper/cdcbcca54c19d6577ecd067b1b6cc80ce2d3a5fd,Machine learning: technologies and potential application at mining companies,"Implementation of machine learning systems is currently one of the most sought-after spheres of human activities at the interface of information technologies, mathematical analysis and statistics. Machine learning technologies are penetrating into our life through applied software created with the help of artificial intelligence algorithms. It is obvious that machine learning technologies will be developing fast and becoming part of the human information space both in our everyday life and in professional activities. However, building of machine learning systems requires great labour contribution of specialists in the sphere of artificial intelligence and the subject area where this technology is to be applied. The article considers technologies and potential application of machine learning at mining companies. The article describes basic methods of machine learning: unsupervised learning, action learning, semi-supervised machine learning. The criteria are singled out to assess machine learning: operation speed; assessment time; implemented model accuracy; ease of integration; flexible deployment within the subject area; ease of practical application; result visualization. The article describes practical application of machine learning technologies and considers the dispatch system at a mining enterprise (as exemplified by the dispatch system of the mining and transportation complex “Quarry” used to increase efficiency of operating management of enterprise performance; to increase reliability and agility of mining and transportation complex performance records and monitoring. There is also a list of equipment performance data that can be stored in the database and used as a basis for processing by machine learning algorithms and obtaining new knowledge. Application of machine learning technologies in the mining industry is a promising and necessary condition for increasing mining efficiency and ensuring environmental security. Selection of the optimal process flow sheet of mining operations, selection of the optimal complex of stripping and mining equipment, optimal planning of mining operations and mining equipment performance control are some of the tasks where machine learning technologies can be used. However, despite prospectivity of machine learning technologies, this trend still remains understudied and requires further research.",,2020,10.1051/e3sconf/202016603007,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
e402547a3cfa9ec455055def2c2add46634a130e,https://www.semanticscholar.org/paper/e402547a3cfa9ec455055def2c2add46634a130e,Symmetry-Adapted Machine Learning for Information Security,"Nowadays, data security is becoming an emerging and challenging issue due to the growth in web-connected devices and significant data generation from information and communication technology (ICT) platforms. Many existing types of research from industries and academic fields have presented their methodologies for supporting defense against security threats. However, these existing approaches have failed to deal with security challenges in next-generation ICT systems due to the changing behaviors of security threats and zero-day attacks, including advanced persistent threat (APT), ransomware, and supply chain attacks. The symmetry-adapted machine-learning approach can support an effective way to deal with the dynamic nature of security attacks by the extraction and analysis of data to identify hidden patterns of data. It offers the identification of unknown and new attack patterns by extracting hidden data patterns in next-generation ICT systems. Therefore, we accepted twelve articles for this Special Issue that explore the deployment of symmetry-adapted machine learning for information security in various application areas. These areas include malware classification, intrusion detection systems, image watermarking, color image watermarking, battlefield target aggregation behavior recognition models, Internet Protocol (IP) cameras, Internet of Things (IoT) security, service function chains, indoor positioning systems, and cryptoanalysis.",Symmetry,2020,10.3390/sym12061044,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
5e4fb30721f9188200ef5b13ea2538664a5acf27,https://www.semanticscholar.org/paper/5e4fb30721f9188200ef5b13ea2538664a5acf27,A Machine Learning Based Write Policy for SSD Cache in Cloud Block Storage,"Nowadays, SSD cache plays an important role in cloud storage systems. The associated write policy, which enforces an admission control policy regarding filling data into the cache, has a significant impact on the performance of the cache system and the amount of write traffic to SSD caches. Based on our analysis on a typical cloud block storage system, approximately 47.09% writes are write-only, i.e., writes to the blocks which have not been read during a certain time window. Naively writing the write-only data to the SSD cache unnecessarily introduces a large number of harmful writes to the SSD cache without any contribution to cache performance. On the other hand, it is a challenging task to identify and filter out those write-only data in a real-time manner, especially in a cloud environment running changing and diverse workloads.In this paper, to alleviate the above cache problem, we propose an ML-WP, Machine Learning Based Write Policy, which reduces write traffic to SSDs by avoiding writing write-only data. The main challenge in this approach is to identify write-only data in a real-time manner. To realize ML-WP and achieve accurate write-only data identification, we use machine learning methods to classify data into two groups (i.e., write-only and normal data). Based on this classification, the write-only data is directly written to backend storage without being cached. Experimental results show that, compared with the industry widely deployed write-back policy, ML-WP decreases write traffic to SSD cache by 41.52%, while improving the hit ratio by 2.61% and reducing the average read latency by 37.52%.","2020 Design, Automation & Test in Europe Conference & Exhibition (DATE)",2020,10.23919/DATE48585.2020.9116539,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
b08740bf7a7a45591b89434c987f268f5ba06211,https://www.semanticscholar.org/paper/b08740bf7a7a45591b89434c987f268f5ba06211,Exposing Hardware Building Blocks to Machine Learning Frameworks,"There are a plethora of applications that demand high throughput and low latency algorithms leveraging machine learning methods. This need for real time processing can be seen in industries ranging from developing neural network based pre-distortors for enhanced mobile broadband to designing FPGA-based triggers in major scientific efforts by CERN for particle physics. In this thesis, we explore how niche domains can benefit vastly if we look at neurons as a unique boolean function of the form $f:B^{I} \rightarrow B^{O}$, where $B = \{0,1\}$. We focus on how to design topologies that complement such a view of neurons, how to automate such a strategy of neural network design, and inference of such networks on Xilinx FPGAs. Major hardware borne constraints arise when designing topologies that view neurons as unique boolean functions. Fundamentally, realizing such topologies on hardware asserts a strict limit on the 'fan-in' bits of a neuron due to the doubling of permutations possible with every increment in input bit-length. We address this limit by exploring different methods of implementing sparsity and explore activation quantization. Further, we develop a library that supports training a neural network with custom sparsity and quantization. This library also supports conversion of trained Sparse Quantized networks from PyTorch to VERILOG code which is then synthesized using Vivado, all of which is part of the LogicNet tool-flow. To aid faster prototyping, we also support calculation of the worst-case hardware cost of any given topology. We hope that our insights into the behavior of extremely sparse quantized neural networks are of use to the research community and by extension allow people to use the LogicNet design flow to deploy highly efficient neural networks.",ArXiv,2020,10.13140/RG.2.2.31661.23527,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
456a390e1053429f040fb025c1bad4a8cf85cfd2,https://www.semanticscholar.org/paper/456a390e1053429f040fb025c1bad4a8cf85cfd2,Interactive machine learning for user-innovation toolkits: an action design research approach,"Machine learning offers great potential to developers and end users in the creative industries. 
However, to better support creative software developers' needs and empower them as machine 
learning users and innovators, the usability of and developer experience with machine learning 
tools must be considered and better understood. This thesis asks the following research questions: 
How can we apply a user-centred approach to the design of developer tools for rapid prototyping 
with Interactive Machine Learning? In what ways can we design better developer tools to accelerate 
and broaden innovation with machine learning? 
 
This thesis presents a three-year longitudinal action research study that I undertook within a 
multi-institutional consortium leading the EU H2020 -funded Innovation Action RAPID-MIX. The 
scope of the research presented here was the application of a user-centred approach to the design 
and evaluation of developer tools for rapid prototyping and product development with machine 
learning. This thesis presents my work in collaboration with other members of RAPID-MIX, 
including design and deployment of a user-centred methodology for the project, interventions for 
gathering requirements with RAPID-MIX consortium stakeholders and end users, and prototyping, 
development and evaluation of a software development toolkit for interactive machine learning. 
 
This thesis contributes with new understanding about the consequences and implications of a 
user-centred approach to the design and evaluation of developer tools for rapid prototyping of 
interactive machine learning systems. This includes 1) new understanding about the goals, needs, 
expectations, and challenges facing creative machine-learning non-expert developers and 2) an 
evaluation of the usability and design trade-offs of a toolkit for rapid prototyping with interactive 
machine learning. This thesis also contributes with 3) a methods framework of User-Centred 
Design Actions for harmonising User-Centred Design with Action Research and supporting the 
collaboration between action researchers and practitioners working in rapid innovation actions, 
and 4) recommendations for applying Action Research and User-Centred Design in similar contexts 
and scale.",,2020,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
9d97c3412dead8970fe9d09347c6b2a11d2ea7bb,https://www.semanticscholar.org/paper/9d97c3412dead8970fe9d09347c6b2a11d2ea7bb,Prediction of Remaining Useful Lifetime of Membrane Using Machine Learning,"We present a novel analytical procedure estimating the remaining useful life (RUL) of complex systems or facilities based on degradation data obtained over time; we consider the maintenance characteristics of units that are incompletely repaired. We develop an extended prognostic model
 that accurately predicts the RUL; we use machine-learning featuring smoothing, logging, variable transformation and clustering to this end. The performance of a general model was more predictable than that of an extended model. A linear regression (LR) method was superior in terms of root
 mean square error prediction and an artificial neural network (ANN) was superior in terms of prognostics and health management (PHM) scoring. The procedure is both practical and efficient, and can be deployed in various industries, yielding low-cost prognostics even in low-expertise domains.
 The procedure can be applied to high-risk industries, aiding management decision-making in terms of the establishment of optimal, preventative maintenance policies.",,2020,10.1166/sam.2020.3788,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
946e25b009baa067d99f6c2397a5fed72de260db,https://www.semanticscholar.org/paper/946e25b009baa067d99f6c2397a5fed72de260db,Machine Learning-Based System for the Availability and Reliability Assessment and Management of Critical Infrastructures (CASO),"A critical infrastructure is a complex interconnected system of systems providing basic and essential services to support the operation of particle accelerators but also industries and households for which they must guarantee high reliability of critical functions. 
Model-based approaches are usually adopted to provide an early identiﬁcation of failures and to reveal hidden dependencies among subsystems. System models are complex and require constant updating to be reactive to system changes and real operating conditions, wear and aging. The interconnections between the different systems and the functional dependencies between their components are in many cases modified at both physical and functional levels while their degraded performances impact the overall system availability and reliability. 
A novel approach is proposed which combines model-based and Big Data analytics by machine learning techniques to extract descriptive and predictive models directly from data. The objective is to foresee and react in time to failures to reduce downtimes as well as to optimize maintenance and operation costs. 
The Computer-Aided System for critical infrastructure Operation (CASO) is designed to significantly and efficiently enhance the quality, safety, reliability and availability of critical infrastructures. 
We report on the design of CASO, its implementation and on the preliminary results inferred on historical and live stream data recorded from CERN’s technical infrastructure. Proposal for the full deployment and expected long-term capabilities will also be discussed.",,2020,10.22323/1.372.0041,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
26bbe82467cee9cc8ec1332df4825a32eebdca41,https://www.semanticscholar.org/paper/26bbe82467cee9cc8ec1332df4825a32eebdca41,Energy Meter Data Analysis Using Machine Learning Techniques,"With the advancement of technology, existence of energy meters are not merely to measure energy units. The proliferation of energy meter deployments had led to significant interest in analyzing the energy usage by the machines. Energy meter data is often difficult to analyzeowing to the aggregation of many disparate and complex loads. At utility scales, analysis is further complicated by the vast quantity of data and hence industries turn towards applying machine learning techniques for monitoring and measuring loads of the machines. The energy meter data analysis aims at analyzing the behavior of the machine and providing insights on usage of the energy. This will help the industries to identify the faults in the machine and to rectify it.Two use cases with two different motor specifications is considered for evaluation and the efficiency is proved by considering accuracy, precision, F-measure and recall as metrics.",International Journal on Recent and Innovation Trends in Computing and Communication,2020,10.17762/ijritcc.v8i6.5409,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
077d0e6af0fbd1f38212e4d43768e8039756bcac,https://www.semanticscholar.org/paper/077d0e6af0fbd1f38212e4d43768e8039756bcac,Light chain consensus reinforcement machine learning: An effective blockchain model for Internet of Things using for its advancement and challenges,"Recently, blockchain intersected the Internet of Things (IoT) has come up with an integrated opportunity for different applications such as industries, medical diagnosis, and the education sector. Several conflicts have risen during the intersection, where the purpose of addressing the enormous resource utilization of blockchain, efficiency, and security issues of massive IoT has not been tackled in the present scenario. Presently, Ruff‐chain, blockchain consortium basis, mobile cloud blockchain (MCBC), probed IoT, and proof of work deployed to overcome the drawback of blockchain intersected IoT demands high resource utilization and power consumption. To address this issue, a light chain consensus reinforcement machine learning (LCC‐RML) method has been developed to optimize the blockchain effectively intersected IoT system and it assists in providing a learning methodology from the aspects of resource utilization, data security decentralization, scalability, and latency. In LCC‐RML, without affecting the decentralization system, security, and latency, scalability has been improved with the underlying blockchain approach. Here, a lighter model is designed especially for the blockchain intersected IoT platform, which contains optimized learning procedures, reduced block size, lightweight consensus data structure, and related effective block interval to streamline the data processing. The experimental analysis has been evaluated in the learning framework to improve the performance of the blockchain intersected IoT system with a computational speed of 84.89% and resource utilization reduction of 85.88%. Further in the power consumption has been reduced up to 57.55% with the computation cost of 29.55% with the scalability ratio of 86.88%.",Computational Intelligence,2020,10.1111/coin.12395,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
327f73d6cf239c568afc85796642f54b52941158,https://www.semanticscholar.org/paper/327f73d6cf239c568afc85796642f54b52941158,Dynamic filtering of malicious records using machine learning integrated databases,"Machine Learning, Deep Learning and Predictive Analytics are the key domains of research in assorted domains of implementations including engineering, finance, economics, real time imaging and many others. The researchers are working on different tools and technologies including open source and own developed frameworks so that the higher degree of accuracy can be achieved. The research reports from Market Research News US predicted that the global market size of machine learning based implementations will exceed 20 billion dollars in year 2024. Most of the government and social services are nowadays in process to be deployed with the advanced technologies of machine learning and deep learning so that the minimum error factor can be there. The key players in the industry include; Google, Facebook, IBM Watson, Baidu, Apple, Microsoft, Wipro, Amazon, Intel, Nuance and many others which are working on the advanced algorithms and implementation perspectives of machine learning.",,2019,10.21533/pen.v7i4.898,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
49caa9a3fbb8fcbbb97c19a84b60259cf273ea96,https://www.semanticscholar.org/paper/49caa9a3fbb8fcbbb97c19a84b60259cf273ea96,Embedded Memories for Silicon-In-Package: Optimization of Memory Subsystem from IoT to Machine Learning,"Traditional memory subsystem consisting of SRAM, DRAM and SSD/HDD has served the needs of electronics industry for decades. With the rapid increase in Graphics, IoT and Machine Learning applications, several new memories have been innovated to optimize the memory hierarchy. Optane memory is deployed to close the performance/area gap between DRAM and SSD. Similarly, embedded DRAM inserted in products as L4 Cache to close the gap between SRAM and DRAM. Lately, MRAM and ReRAM are also brought to reality to target wide range of applications, covering embedded Non-Volatile Memory and Flash buffer at platform level. This talk will go through these innovate memories and how one needs to optimize the memory subsystem for the best application. There's no memory that fits all, but design and architecture opportunities exist for targeted applications.",VLSI-DAT,2019,10.1109/VLSI-DAT.2019.8741637,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
9f5ea59ce4041cda55fefe1f27b346d884ba84a0,https://www.semanticscholar.org/paper/9f5ea59ce4041cda55fefe1f27b346d884ba84a0,Virtual Tribes: Analyzing Attitudes Toward the LGBT Movement by Applying Machine Learning on Twitter Data,,,2019,10.1007/978-3-030-48993-9_12,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
55b7fbbdb99381a08852a76db190aac12ee7d90f,https://www.semanticscholar.org/paper/55b7fbbdb99381a08852a76db190aac12ee7d90f,Identifying and Mitigating Bias in Machine Learning Applications,"This study addresses the existence of bias in machine learning applications and examines techniques for identifying and mitigating bias using scholarly literature published between 2012 and 2019. The intended audience is machine learning engineers, system analysts, and data analysts of any industry. This study is significant because there may be considerable ethical implications caused by machine learning bias; identifying and mitigating these biases is key to the development and deployment of effective machine learning algorithms.",,2019,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
6a0584f428cd1c6061df020daade681331ba2873,https://www.semanticscholar.org/paper/6a0584f428cd1c6061df020daade681331ba2873,Embedded Memories for Silicon-ln-Package: Optimization of Memory Subsystem from loT to Machine Learning,"Traditional memory subsystem consisting of SRAM, DRAM and SSD/HDD has served the needs of electronics industry for decades. With the rapid increase in Graphics, loT and Machine Learning applications, several new memories have been innovated to optimize the memory hierarchy. Optane memory is deployed to close the performance/area gap between DRAM and SSD. Similarly, embedded DRAM inserted in products as L4 Cache to close the gap between SRAM and DRAM. Lately, MRAM and ReRAM are also brought to reality to target wide range ofapplications, covering embedded Non-Volatile Memory and Flash buffer at platform level. This talk will go through these innovate memories and how one needs to optimize the memory subsystem for the best application. There's no memory that fits all, but design and architecture opportunities exist for targeted applications.","2019 International Symposium on VLSI Technology, Systems and Application (VLSI-TSA)",2019,10.1109/VLSI-TSA.2019.8804633,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
4804351ddb9f1b28817c6dced7c00b644da48f1a,https://www.semanticscholar.org/paper/4804351ddb9f1b28817c6dced7c00b644da48f1a,Performance Analysis of Data Parallelism Technique in Machine Learning for Human Activity Recognition Using LSTM,"Human activity recognition (HAR), driven by large deep learning models, has received a lot of attention in recent years due to its high applicability in diverse application domains, manipulate time-series data to speculate on activities. Meanwhile, the cloud term ""as-a-service"" has essentially revolutionized the information technology industry market over the last ten years. These two trends somehow are incorporating to inspire a new model for the assistive living application: HAR as a service in the cloud. However, with frequently updates deep learning frameworks in open source communities as well as various new hardware features release, which make a significant software management challenge for deep learning model developers. To address this problem, container techniques are widely employed to facilitate the deep learning software development cycle. In addition, models and the available datasets are being larger and more complicated, and so, an expanding amount of computing resources is desired so that these models are trained in a feasible amount of time. This requires an emerging distributed training approach, called data parallelism, to achieve low resource utilization and faster execution in training time. Therefore, in this paper, we apply the data parallelism to build an assistive living HAR application using LSTM model, deploying in containers within a Kubernetes cluster to enable the real-time recognition as well as prediction of changes in human activity patterns. We then systematically measure the influence of this technique on the performance of the HAR application. Firstly, we evaluate our system performance with regard to CPU and GPU when deployed in containers and host environment, then analyze the outcomes to verify the difference in terms of the model learning performance. Through the experiments, we figure out that data parallelism strategy is efficient for improving model learning performance. In addition, this technique helps to increase the scaling efficiency in our system.",2019 IEEE International Conference on Cloud Computing Technology and Science (CloudCom),2019,10.1109/CloudCom.2019.00066,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
83338fac01dcee281ad4f41d05559a71a5d582bf,https://www.semanticscholar.org/paper/83338fac01dcee281ad4f41d05559a71a5d582bf,2019-01242-Temporary scientific engineer / Software development and machine learning for network security,"Scientific Context: In last years, Internet-of-Things became a reality with numerous protocols, platforms and devices [8] being developed and used to support the growing deployment of smart* services: smart-home, transport, -health, -city... and even the rather usual rigid systems with industry 4.0. Providing new services have required first the development of new functionalities with as underlining goals to have more powerand computeefficient devices which can embed various sensors. Obviously, IoT also supposes a full infrastructure to guarantee the efficiency of communications and processing of information. The embedded devices are thus completed by access points, routers, servers, etc. At the higher levels services are developed and provided to the users. This ecosystem is very rich and cannot be controlled by a unique entity, e.g. services are o en developed by third parties, manufacturer of embed devices are different to those providing connectivity... As a result, such a complex system is naturally a source of potential threats and real cases recently demonstrates that IoT can be affected by naïve weaknesses [1,6]. At Inria, we even demonstrated how simple and cheap can it be take over the control of a Z-Wave home installation in a silent manner [2]. Therefore, security is paramount of importance. In last decade, many IoT architectures have been proposed, such as the reference model IoT-A [3], including security modules. However, as highlighted before, security cannot be guaranteed without failure or by-design and this is all the more true with evolving ecosystems such as IoT, with now the emerging trend of using fog-based architecture rather than well-established cloud models. To enhance security, one option is to redesign an IoT architecture with stronger security but this will face the same problems as before, since some security issues can appear afterwards. Maintaining the architecture with new security elements would be therefore required but a remaining problems is the numerous number protocols or platforms that already exist. Nowadays, the only viable solution is so to provide new security mechanisms that could be composed on demand and deployed in any IoT deployment by the operators, the integrators or the vendors rather than developing protocolor architecture-centric security solutions. [1] Manos Antonakakis et. al , Understanding the Mirai Botnet, USENIX Security, 2017 [2] L. Rouch et. Al, A Universal Controller to Take Over a Z-Wave Network, Black Hat Europe, 2017 [3] Alessandro Bassi, Martin Bauer, Martin Fiedler, Thorsten Kramp, Rob van Kranenburg, Sebastian Lange, Stefan Meissner (eds), “Enabling Things to Talk”, Designing IoT solutions with the IoT Architectural Reference Model, Springer, 2013 [4] J. François et. al, PTF: Passive Temporal Fingerprinting, IFIP/IEEE International Symposium on Integrated Network Management (IM), 2011 [5] BF Van Dongen et. al, The prom framework: A new era in process mining tool support, ICATPN 2005 [6] C. Kolias, G. Kambourakis, A. Stavrou and J. Voas, ""DDoS in the IoT: Mirai and Other Botnets,"" in Computer, vol. 50, no. 7, pp. 80-84, 2017. [7] Markus Miettinen, Samuel Marchal, Ibbad Hafeez, N. Asokan, Ahmad-Reza Sadeghi, Sasu Tarkoma: IoT SENTINEL: Automated Device-Type Identification for Security Enforcement in IoT. ICDCS 2017: [8] A. Al-Fuqaha, M. Guizani, M. Mohammadi, M. Aledhari and M. Ayyash, ""Internet of Things: A Survey on Enabling Technologies, Protocols, and Applications,"" in IEEE Communications Surveys & Tutorials , vol. 17, no. 4, pp. 2347-2376, Fourthquarter 2015.",,2019,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
a178a0bdee7549d87402b6c6128c569109128458,https://www.semanticscholar.org/paper/a178a0bdee7549d87402b6c6128c569109128458,Challenges in Deploying Machine Learning: a Survey of Case Studies,"In recent years, machine learning has transitioned from a field of academic research interest to a field capable of solving real-world business problems. However, the deployment of machine learning models in production systems can present a number of issues and concerns. This survey reviews published reports of deploying machine learning solutions in a variety of use cases, industries and applications and extracts practical considerations corresponding to stages of the machine learning deployment workflow. By mapping found challenges to the steps of the machine learning deployment workflow we show that practitioners face issues at each stage of the deployment process. The goal of this paper is to lay out a research agenda to explore approaches addressing these challenges.",ACM Computing Surveys,2020,10.1145/3533378,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
a4f5af1c85bae92907e357a3d1f61a06395f8f59,https://www.semanticscholar.org/paper/a4f5af1c85bae92907e357a3d1f61a06395f8f59,Ransomware Auto-Detection in IoT Devices using Machine Learning,"The term Internet of Things (often abbreviated IoT) was coined by industry researchers but has emerged into mainstream public view only more recently. The IoT is a massive group of devices containing sensors or actuators connected over wired or wireless networks. IoT has been rapidly growing over the past decade and, during the growth, security has been identified as one of the weakest areas in IoT. There are over six billion estimated devices currently connected to the Internet and an estimate of over 25 billion connected by 2020. IoT and its applications propagate to majority of life’s infrastructure ranging from health and food production to smart cities and urban management. While efficiency and prevalence of IoT are increasing, security issues remain a necessary concern for industries. Internet connected devices, including those deployed in an IoT architecture, are increasingly targeted by cybercriminals due to their pervasiveness and the ability to use the compromised devices to further attack the underlying architecture. In the case of ransomware, for example, devices that can store a reasonably amount of data are likely to be targeted. Thus, ensuring the security of IoT nodes against threats such as malware is a topic of ongoing interest. While malware detection and mitigation research are now new, ransomware detection and mitigation remain challenging. Ransomware is a relatively new malware type that attempts to encrypt a compromised device’s data using a strong encryption algorithm. The victim will then have to pay the ransom (usually using bitcoins) to obtain the password or decryption key. Consequences include temporary or permanent loss of sensitive information, disruption of regular operations, direct/indirect financial losses. In this paper, we present a machine learning based approach to detect ransomware of IoT devices. Specifically, our proposed approach outperforms K-Nearest Neighbors, Neural Networks, Support Vector Machine and Random Forest, in terms of accuracy rate, recall rate and precision rate.",,2019,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
5066be99ecd0c40a84fcb4bdebff67fbaff59746,https://www.semanticscholar.org/paper/5066be99ecd0c40a84fcb4bdebff67fbaff59746,Improving Deepwater Facility Uptime Using Machine Learning Approach,"
 Deepwater oil and gas facilities typically encounter on an average up to 5% annual production losses due to unplanned downtime, conservatively estimated at billions of dollars impact for the industry. The existing toolkit and systems in place are not always adequate to identify and predict abnormal events that could lead towards unplanned facility shutdown. The interaction amongst process sub-systems and disturbances that propagate across these sub-systems with changing operating conditions are hard to predict without a fit-for-purpose model (or a digital twin). The focus of current work is on deepwater facility having several oil export pipeline pumps in parallel and several gas compressors in series. The alarm database showed records of several unplanned shutdown events around these critical equipements that resulted in undesirable outcomes such as production deferment, complete facility shutdown, loss of sales volumes and increased operational costs. In this work, an intelligent prognostic solution is proposed using machine learning (ML) framework for automatic prediction of impending facility downtime, and identification of key causative process variables. A systematic workflow was developed to identify, cleanse and process real time data for both model training and prediction. Several ML methods were evaluated; anomaly detection based on Principal Component Analysis (PCA) and Autoencoder (AE) algorithms were found performing better for the type of data available for the deepwater facility. The ML framework also supported analysis of underlying downtime causes to propose suitable mitigation steps. Knowledge based on physical understanding of the process was used to select each sub-system boundary and sensor list on which ML model was trained. These models were then cross-validated to test the accuracy of trained models. Finally, the alarm database was used to confirm the accuracy of the machine leaning models and identify root-causes for unplanned shutdowns. If the operating condition changes over time, the anomaly detection based ML models were setup to adapt to changing conditions by automatic model updates, resulting in significant reduction in false alarms. The adaptive ML models, when applied to one of the sub-system (with 30 different sensor data), predicted 24 unplanned events in 6 months of period, while when applied to another sub-system (with 40 sensor data), predicted only 6 unplanned downtime events. Several predictions were found as early as 30 mins to 2 hours, providing adequate early warning to take proactive actions. Case studies shown in the paper present diagnostic charts and identified early indicators were found in agreement with pre-alarms generated by existing alarm system, thus validating the ML solution. Current toolkit available to identify anomalous process behavior is limited to exception based surveillance with fixed min-max limits on each sensor data. Therefore, proposed adaptive ML solution has shown potential to revolutionize the topside process surveillance. This paper also describes how the ML framework can be scaled for a sustainable solution that provides prediction every minute, keeps the model evergreen utilizing cloud-based model deployment platform to train, predict and trigger automatic model updates and also span multiple process systems and facilities. Finally, we present directions for future work, where the current model can keep predicting various events and over time when sufficient events are collected, more advanced machine learning methods based on supervised ML can be developed and deployed.","Day 2 Tue, October 01, 2019",2019,10.2118/195875-ms,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
9ff8b9bc8afcddab1123101152c2d7adfd19ae24,https://www.semanticscholar.org/paper/9ff8b9bc8afcddab1123101152c2d7adfd19ae24,Using Machine Learning to Design Precision Digital Engagement,"MEMOTEXT has developed the following 6step Digital Health Engagement Methodology for Data Mining (DHEM-DM) based on the systematic process and experience of designing, developing, and deploying personalized digital health interventions across chronic disease and patient-specific domains. This structure approach draws on aspects from the Cross Industry Standard Process Data Mining (CRISPDM), the Analytics Solutions Unified Method Data Mining (ASUM-DM) and the Team Data Science Process (TDSP) methodologies but is specifically tailored for a data-driven approach in designing digital health interventions to help patients meet their health goals and produce sustained behaviour change.",,2019,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
2b9fd83059cfe5d41e2386734bac6f1f1d1546c6,https://www.semanticscholar.org/paper/2b9fd83059cfe5d41e2386734bac6f1f1d1546c6,Predicting Arrival Times of Container Vessels : A Machine Learning Application,"A Dutch Logistic Service Provider (LSP) currently applies a reactive attitude towards arrival time information that is solely based on the carrier’s sailing schedule. However, this sailing schedule historically appears to be unreliable: 20% of the orders that the LSP executed last 2.5 years, did not arrive on time. Since LSPs remain dependent on carriers from the container shipping industry, a platform capable of delivering and processing accurate information is essential for increasing efficiency, visibility and customer service. Not being able to exactly know when an order will arrive, negatively affects the businesses of both the LSP and the customer in terms of decreased efficiency and increased costs. We therefore propose a more proactive attitude towards arrival times by means of a predictive model based on historical order data. We applied the Random Forest technique to this end. The model is able to predict the deviation in the arrival time that is provided by the carrier in their sailing schedule in advance of actual shipment. Deployment of the actual prediction algorithm is expected to lead to improved business processes in terms of increased efficiency and decreased costs for both the LSP and the customer.",,2019,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
90b0a4f5f5bc0170b587017f03414836a99a25e7,https://www.semanticscholar.org/paper/90b0a4f5f5bc0170b587017f03414836a99a25e7,"Machine learning for network automation: overview, architecture, and applications [Invited Tutorial]","Networks are complex interacting systems involving cloud operations, core and metro transport, and mobile connectivity all the way to video streaming and similar user applications.With localized and highly engineered operational tools, it is typical of these networks to take days to weeks for any changes, upgrades, or service deployments to take effect. Machine learning, a sub-domain of artificial intelligence, is highly suitable for complex system representation. In this tutorial paper, we review several machine learning concepts tailored to the optical networking industry and discuss algorithm choices, data and model management strategies, and integration into existing network control and management tools. We then describe four networking case studies in detail, covering predictive maintenance, virtual network topology management, capacity optimization, and optical spectral analysis.",IEEE/OSA Journal of Optical Communications and Networking,2018,10.1364/JOCN.10.00D126,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
7086831c5c1dca6248c2aa4495c1d10a552721a5,https://www.semanticscholar.org/paper/7086831c5c1dca6248c2aa4495c1d10a552721a5,Automatic Classification of Rotating Machinery Defects Using Machine Learning (ML) Algorithms,,KES-HCIS,2020,10.1007/978-981-15-5784-2_16,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
436277489568e28c0748704939ed35ee36cda755,https://www.semanticscholar.org/paper/436277489568e28c0748704939ed35ee36cda755,EDITORIAL MACHINE LEARNING AND ITS APPLICATIONS,"recently, machine learning has attracted close attention of researchers and has also been applied successfully in real-life problems, for example, the areas of administration, commerce, and industry. These successful applications of machine learning in the real-world problems have caused increased interest in learning techniques, dictating further effort in informing people from other disciplines about the art in machine learning and its uses. The objective of this Special Issue of Neural Network World on Machine Learning and Its Applications is to encourage the researchers who could provide their significant recent developments on machine learning, and machine learning for realworld applications. By publishing this Special Issue, we hope to make a modest contribution to the effort of introducing the most significant recent developments on the topics of Machine Learning and Its Applications. Among all the submissions received for the special issue, we finally selected 5 articles. The paper titled “Bus arrival time prediction using support vector machine with genetic algorithm” (by Yang, et al., 2016) proposed a prediction model of bus arrival time based on support vector machine with genetic algorithm (GA-SVM). To increase the speed and optimality of the parameter selection, GA was used to search the optimal combination of the various parameters in the SVM. The experimental results showed that the prediction model the authors proposed were superior to the traditional SVM model and the Artificial Neural Network (ANN) model. In the paper titled “Multi-step hybrid prediction model of baltic supermax index based on support vector machine” (by Guan, et al., 2016) a hybrid multistep prediction model to predict the Baltic index was proposed. In the hybrid model, the direct prediction and iterative prediction were combined. The iterative model was used for a rough prediction and the direct model was used for adjustment. Compared with history mean prediction model, ARMA model and simple iterative prediction model, the hybrid multistep prediction model based on SVM had high accuracy, and was feasible in the BSI index prediction. The paper titled “Possibilistic LVQ neural networkan application to childhood autism grading” (by Kanimozhiselvi and Pratap, 2016) was concerned with a PoLVQ based assessment support system for the diagnostic confirmation in grading",,2016,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
4a215520c5c57ba29e8a55a71e1691c0521a5ab5,https://www.semanticscholar.org/paper/4a215520c5c57ba29e8a55a71e1691c0521a5ab5,Designing and deploying insurance recommender systems using machine learning,"Recommender systems have become extremely important to various types of industries where customer interaction and feedback is paramount to the success of the business. For companies that face changes that arise with ever‐growing markets, providing product recommendations to new and existing customers is a challenge. Our goal is to give our customers personalized recommendations based on what other similar people with similar portfolios have, in order to make sure they are adequately covered for their needs. Our system uses customer characteristics in addition to customer portfolio data. Since the number of possible recommendable products is relatively small, compared to other recommender domains, and missing data is relatively frequent, we chose to use Bayesian Networks for modeling our systems. We also present a deep‐learning‐based approach to provide recommendations to prospects (potential customers) where only external marketing data is available at the time of prediction.",Wiley Interdiscip. Rev. Data Min. Knowl. Discov.,2020,10.1002/widm.1363,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
591b386c013d1f52cd6285445945e6b35460ecd7,https://www.semanticscholar.org/paper/591b386c013d1f52cd6285445945e6b35460ecd7,Using machine learning techniques to predict the cost of repairing hard failures in underground fiber optics networks,,Journal of Big Data,2020,10.1186/s40537-020-00343-4,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
309f1fa5345e8ab46addd5b3b87343d3a06c7430,https://www.semanticscholar.org/paper/309f1fa5345e8ab46addd5b3b87343d3a06c7430,Implementation of machine learning algorithm in embedded devices,"This paper describes the usage of neural networks in microcontrollers for deployment in embedded devices. The issue is focused on the design of a suitable neural network, its optimization and deployment in a 32-bit microcontroller with regards to the limiting factors of the chosen microcontroller. The introductory part of the article is a description of the used technology and hardware on which the solution will be implemented. Accelerometer motion recognition was chosen as a practical application. The proposed solution recognizes 6 basic movements, respectively movement in three axes. Tensorflow and Keras frameworks were used to design and implement a neural network. The created neural network model was after optimization implemented in the firmware of the STM32L4x microcontroller. The proposed solution implements automatic motion detection and its subsequent classification. The proposed principle can be applied to a group of sensors connected to the available interfaces of the microcontroller. Application with an accelerometer can be used to detect specific vibrations, application with MEMS microphones can be used to detect specific sound patterns that indicate a possible fault condition of the monitored device in industry.",2020 19th International Conference on Mechatronics - Mechatronika (ME),2020,10.1109/ME49197.2020.9286705,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
635455d251604d184a30d09139eb4351f1f6a1ea,https://www.semanticscholar.org/paper/635455d251604d184a30d09139eb4351f1f6a1ea,A Novel Scheme for Access Control Policy Generating and Evaluating in IoT based on Machine Learning,"The ever-increasing demand for data exchanges has boosted the development of the Internet of Things (IoT). IoT has brought a new revolution to a variety of industries by integrating smart devices as well as information and communication technologies into traditional systems. However, due to the dynamic and heterogeneous structure of IoT, unauthorized access and data leakage may be much easier. Attribute-based access control (ABAC) is suitable for complex and changeable access control environments due to its flexibility and universality in capturing authorizations in terms of the attributes of users and resources. However, the dynamic nature of IoT bring new challenges to access control. On the one hand, new services and applications continue to be deployed, administrators need to formulate new policies for those services and applications. Therefore, manual development of ABAC polices in IoT environment is time consuming and expensive. On the other hand, because the access environment is constantly changing, access control policies may become unsuitable for current environment. Manual identification of these low-quality rules is often after they cause severe consequences. To address the above two problems, we propose a scheme for generating and evaluating polices in IoT based on machine learning. This scheme referred to as PGEML, contains two module, policy generalization (PG) and policy evaluation (PE). In the PG module, we define a novel measure, resource similarity, and integrate it into policy mining so that policies could generalize among related resources. In the PE module, we introduce a quantitative method to assess rules and prune rules of low-quality. We conduct our experiments on a real-world enterprise access logs from Amazon. The experimental results has qualitatively and quantitatively showed the effectiveness of our proposed scheme.","2020 International Conferences on Internet of Things (iThings) and IEEE Green Computing and Communications (GreenCom) and IEEE Cyber, Physical and Social Computing (CPSCom) and IEEE Smart Data (SmartData) and IEEE Congress on Cybermatics (Cybermatics)",2020,10.1109/iThings-GreenCom-CPSCom-SmartData-Cybermatics50389.2020.00079,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
1080ba3f963eb940427e72d405bb822b3a543b73,https://www.semanticscholar.org/paper/1080ba3f963eb940427e72d405bb822b3a543b73,Challenges in the Deployment and Operation of Machine Learning in Practice,"Machine learning has recently emerged as a powerful technique to increase operational efficiency or to develop new value propositions. However, the translation of a prediction algorithm into an operationally usable machine learning model is a time-consuming and in various ways challenging task. In this work, we target to systematically elicit the challenges in deployment and operation to enable broader practical dissemination of machine learning applications. To this end, we first identify relevant challenges with a structured literature analysis. Subsequently, we conduct an interview study with machine learning practitioners across various industries, perform a qualitative content analysis, and identify challenges organized along three distinct categories as well as six overarching clusters. Eventually, results from both literature and interviews are evaluated with a comparative analysis. Key issues identified include auto- mated strategies for data drift detection and handling, standardization of machine learning infrastructure, and appropriate communication and expectation management.",ECIS,2019,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
774c9e4a613959fd07a604d07db88f37bb779c16,https://www.semanticscholar.org/paper/774c9e4a613959fd07a604d07db88f37bb779c16,Alternative data and sentiment analysis: Prospecting non-standard data in machine learning-driven finance,"Social media commentary, satellite imagery and GPS data are a part of ‘alternative data’, that is, data that originate outside of the standard repertoire of market data but are considered useful for predicting stock prices, detecting different risk exposures and discovering new price movement indicators. With the availability of sophisticated machine-learning analytics tools, alternative data are gaining traction within the investment management and algorithmic trading industries. Drawing on interviews with people working in investment management and algorithmic trading firms utilizing alternative data, as well as firms providing and sourcing such data, we emphasize social media-based sentiment analytics as one manifestation of how alternative data are deployed for stock price prediction purposes. This demonstrates both how sentiment analytics are developed and subsequently utilized by investment management firms. We argue that ‘alternative data’ are an open-ended placeholder for every data source potentially relevant for investment management purposes and harnessing these disparate data sources requires certain standardization efforts by different market participants. Besides showing how market participants understand and use alternative data, we demonstrate that alternative data often undergo processes of (a) prospecting (i.e. rendering such data amenable to processing with the aid of analytics tools) and (b) assetization (i.e. the transformation of data into tradable assets). We further contend that the widespread embracement of alternative data in investment management and trading encourages a financialization process at the data level which raises new governance issues.",Big Data & Society,2022,10.1177/20539517211070701,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
bf189fd7af63b3a6eecb221526f89f1c6c5ccd0d,https://www.semanticscholar.org/paper/bf189fd7af63b3a6eecb221526f89f1c6c5ccd0d,Machine-learning-based top-view safety monitoring of ground workforce on complex industrial sites,,Neural Comput. Appl.,2021,10.1007/s00521-021-06489-3,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
5bc7ad4c5d2537a6892b3bcb397d4a9512949f45,https://www.semanticscholar.org/paper/5bc7ad4c5d2537a6892b3bcb397d4a9512949f45,Drone Forensics and Machine Learning: Sustaining the Investigation Process,"Drones have been increasingly adopted to address several critical challenges faced by humanity to provide support and convenience . The technological advances in the broader domains of artificial intelligence and the Internet of Things (IoT) as well as the affordability of off-the-shelf devices, have facilitated modern-day drone use. Drones are readily available for deployment in hard to access locations for delivery of critical medical supplies, for surveillance, for weather data collection and for home delivery of purchased goods. Whilst drones are increasingly beneficial to civilians, they have also been used to carry out crimes. We present a survey of artificial intelligence techniques that exist in the literature in the context of processing drone data to reveal criminal activity. Our contribution also comprises the proposal of a novel model to adopt the concepts of machine learning for classification of drone data as part of a digital forensic investigation. Our main conclusions include that properly trained machine-learning models hold promise to enable an accurate assessment of drone data obtained from drones confiscated from a crime scene. Our research work opens the door for academics and industry practitioners to adopt machine learning to enable the use of drone data in forensic investigations.",Sustainability,2022,10.3390/su14084861,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
8538877d7ebfb30c6a91822340f1c26b3286d57c,https://www.semanticscholar.org/paper/8538877d7ebfb30c6a91822340f1c26b3286d57c,A Social Media Sentiment Analysis: Machine Intelligence Model For Worldwide Covid-19 Vaccination Using Twitter Data,"
 Understanding human emotions is one of the crucial aspects when we are to take action. Our emotions dictate our apparent behaviors. In simple words, what we feel inside can predict things about what we would do. This creates a huge opportunity for government and businesses industry to understand and predict people's behaviors. There has been some really great research done on this with high accuracy. Recently, Covid-19 vaccination process is a challenging task going on all over the world and it is necessary to explore people’s reaction over this for more effective vaccination process spread. In this paper, wetried to understand an event (Covid-19 vaccination) with a relatively simple model with decent accuracy compared to other sophisticated models. We use simple machine learning models to train and deploy it over the network. We have used KNIME Analytical Platform to design and implement our model as it provides end-to-end analytics. We have managed to get 88.67% accuracy and Cohen’s kappa 0.789 with SVM model by tuning some parameters. The model is deployed on Twitter data. This paper shows our efforts trying to make a simple model to analyze an event (Covid-19 vaccination) and understand people's emotions towards the event. The methodology involves identifying important topics (terms) and finding out the sentiment (positive, negative, neutral). This paper tries to find a low-cost solution to analyze an event and provide data-driven insights from it without involving sophisticated algorithms.",,2021,10.21203/rs.3.rs-734770/v1,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
9fae71ae9d497c5544549439d9bc6ee07c440ca2,https://www.semanticscholar.org/paper/9fae71ae9d497c5544549439d9bc6ee07c440ca2,Minimising pipeline leaks and maximising operational life by application of machine learning at Cooper Basin,"The development of technologies in the last few decades has enabled operators to collect significantly more data than previously possible. Despite availability, making data-driven decisions on asset health, and developing efficient asset management strategies, is not common. This is mainly due to challenges with compilation, and alignment of all the data into a comprehensive picture of pipeline integrity, as it consumes significant resources deploying conventional methods. A critical advantage of modern data storage, analysis and visualisation techniques is the relative ease of performing statistical assessments of integrity data. Analysis of correlated data can be equally challenging as algorithms used can be overly simplistic and inaccurate. Machine learning algorithms parse, analyse and learn from data, enabling the operators to make an educated decision. This has been extensively deployed in other industries such as finance, healthcare and supply chain management but has never been fully developed and enhanced in pipeline integrity industry until very recently. This paper provides an overview of the development in machine learning tools in pipeline integrity, allowing enhancement of asset performance, through the application of machine learning and automation, to predict integrity threats, and prevent leaks and failures. It provides a case study where a tool was developed, and this technique was successfully implemented across a significant number of upstream pipelines in the Cooper Basin, enabling the Santos integrity engineering team to make the most effective decisions on asset condition and to develop a data-driven asset management plan.",The APPEA Journal,2022,10.1071/aj21060,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
8a3282528f15b2f088a6a00d5d38442d4e496aa0,https://www.semanticscholar.org/paper/8a3282528f15b2f088a6a00d5d38442d4e496aa0,Concurrent 18. Presentation for: Minimising pipeline leaks and maximising operational life by application of machine learning at Cooper Basin,"Presented on Wednesday 18 May: Session 18 The development of technologies in the last few decades has enabled operators to collect significantly more data than previously possible. Despite availability, making data-driven decisions on asset health, and developing efficient asset management strategies, is not common. This is mainly due to challenges with compilation, and alignment of all the data into a comprehensive picture of pipeline integrity, as it consumes significant resources deploying conventional methods. A critical advantage of modern data storage, analysis and visualisation techniques is the relative ease of performing statistical assessments of integrity data. Analysis of correlated data can be equally challenging as algorithms used can be overly simplistic and inaccurate. Machine learning algorithms parse, analyse and learn from data, enabling the operators to make an educated decision. This has been extensively deployed in other industries such as finance, healthcare and supply chain management but has never been fully developed and enhanced in pipeline integrity industry until very recently. This paper provides an overview of the development in machine learning tools in pipeline integrity, allowing enhancement of asset performance, through the application of machine learning and automation, to predict integrity threats, and prevent leaks and failures. It provides a case study where a tool was developed, and this technique was successfully implemented across a significant number of upstream pipelines in the Cooper Basin, enabling the Santos integrity engineering team to make the most effective decisions on asset condition and to develop a data-driven asset management plan. To access the presentation click the link on the right. To read the full paper click here",The APPEA Journal,2022,10.1071/aj21363,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
f8a3fcd9f603785ccfafc73e04af667dac62ec44,https://www.semanticscholar.org/paper/f8a3fcd9f603785ccfafc73e04af667dac62ec44,Machine Learning Based Prediction of Output PV Power in India and Malaysia with the Use of Statistical Regression,"Climate change and pollution are serious issues that are driving people to adopt renewable energy instead of fossil fuels. Most renewable energy technologies rely on atmospheric conditions to generate power. Solar energy is a renewable energy source that causes the least environmental damage. Solar energy can be converted to electricity, which necessitates the use of a PV system. This study presents a design, which analyses the output power performance of PV, using machine learning technique in India and Malaysia; using this, we would get the predicted amount of solar power using different weather conditions for both India and Malaysia. This study is divided into two sections, such as the data collection section and the implementation system. Dataset was collected from a weather NASA website, which took various weather parameters, based on which the model will be evaluated. The proposed research work is developed using ANN and is an amalgamation of statistical regression and neural networks, which help the model to get high accuracy by helping the model learn more complex relationships between parameters, which is able to evaluate the output power performance of photovoltaic cells with different environmental condition parameters in India and Malaysia. The ANN models are found to successfully predict PV output power with root mean square error (RMSE) of 1.5565, which was used as a measure of our model’s accuracy. This ANN model also outperforms other models available in the literature. This will have a noteworthy contribution in scaling the PV deployment in countries such as India and Malaysia and will increase the share of PV power in their national power production, as it would give the industry and the two countries an idea as to how the predicted output PV power would vary based on weather conditions, such as temperature.",Mathematical Problems in Engineering,2022,10.1155/2022/5680635,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
f87b8b9348a713a7e37babed332986b738fc9e6b,https://www.semanticscholar.org/paper/f87b8b9348a713a7e37babed332986b738fc9e6b,Auto Modelling for Machine Learning: A Comparison Implementation between RapidMiner and Python,"Recently, business intelligence is creating many changes and challenges to the business models of many industries globally. While a bigger impact has been reported on business intelligence models, there has been very little effort that investigates the deployment of business intelligence models based on auto modelling approaches of machine learning. Design and implement a machine learning business intelligence model involved a series of hassle tasks and was mostly time-consuming for an inexpert data scientist. Therefore, this paper presents different approaches to auto modelling machine learning provided by RapidMiner and Python machine learning software tools. To compare the results of modelling from the different approaches, the Airbnb hospitality dataset has been used as a case study for predicting the hospitality prices. The results show that Random Forest Regressors have been very promising to produce a high percentage of accuracy score with all the auto modelling machine learning Keywords— Machine Learning, Auto Modelling, Price Prediction, TPOT Python, RapidMinerja",International Journal of Emerging Technology and Advanced Engineering,2022,10.46338/ijetae0522_03,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
cf2e5d7c1395cb7a5b62655f3ee4f68eb9417268,https://www.semanticscholar.org/paper/cf2e5d7c1395cb7a5b62655f3ee4f68eb9417268,Challenges Faced by Industries and Their Potential Solutions in Deploying Machine Learning Applications,"Across all sectors, organizations attempt to make efficiency savings and performance improvements by incorporating machine learning (ML) into commercial application services. However, in comparison to traditional software applications, design, deployment, and maintenance of ML applications is more complicated. In particular, ML introduces new challenges of data availability, concept drift, scalability, and technical debt. In this paper, we introduce some of the practical challenges that arise when deploying ML applications, and describe potential solutions. Our analysis is based on experience designing and deploying a commercial spend classification service.",2022 IEEE 12th Annual Computing and Communication Workshop and Conference (CCWC),2022,10.1109/CCWC54503.2022.9720900,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
c47e086bdb8588e396f386b82ab19fbf01487fe1,https://www.semanticscholar.org/paper/c47e086bdb8588e396f386b82ab19fbf01487fe1,Machine Learning-Based Intelligent Wireless Communication System for Solving Real-World Security Issues,"The intelligent wireless system focuses on integrating with the advanced technologies like machine learning and related approaches in order to enhance the performance, productivity, and output. The implementation of machine learning approaches is mainly applied in order to enhance the efficient communication system, enable creation of variable node locations, support collection of data and information, analyze the pattern, and forecast so as to provide better services to the end users. The efficiency of using these technologies tend to lower the cost and support in deploying the resources effectively. The wireless network system tends to enhance the bandwidth, and the application of novel machine learning approaches supports detection of unrelated data and information and enables analysis of latency at each part of the communication channel. The study involves critically analyzing the key determinants of machine learning approaches in supporting enhanced intelligent network communication in the industries. The researchers are aimed at gathering both primary data and secondary data for the study. The respondents are chosen in the industry so that they can provide better inputs and insights related to the area of research. The key determinants considered for the study are machine learning-influenced management of hotspots, identification of critical congestion points, spectrum availability, and management. The analysis is made using SPSS data analysis package based on which it is noted that all the factors make major influences towards the intelligent communication, and hence machine learning supports critically in enhancing the user experience effectively.",Security and Communication Networks,2022,10.1155/2022/7978822,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
cd2e8ad7816b8d27f2c0bb53dc1bda44ff240500,https://www.semanticscholar.org/paper/cd2e8ad7816b8d27f2c0bb53dc1bda44ff240500,Machine learning techniques for robotic and autonomous inspection of mechanical systems and civil infrastructure,,Auton. Intell. Syst.,2022,10.1007/s43684-022-00025-3,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
b4654c2846d2cc8338bd45d7bbeb4fb99f8f1d9c,https://www.semanticscholar.org/paper/b4654c2846d2cc8338bd45d7bbeb4fb99f8f1d9c,AGRICULTURE PRODUCTION PREDICTION USING MACHINE LEARNING ALGORITHMS,": In general, agriculture is the backbone of India and also plays an important role in Indian economy. Most of Indians have agriculture as their occupation. Farmers usually have the mind-set of planting the same crop, using more fertilizers and following the public choice. But now-a-days, food production and prediction is getting depleted due to unnatural climatic changes, which will adversely affect the economy of farmers by getting a poor yield and also help the farmers to remain less familiar in getting high yield of crops. Machine learning is one such advanced technique deployed to predict crop yield in agriculture. By looking at the past few years, there have been significant developments in how machine learning can be used in various industries and research. The surveyed research papers have given a rough idea about using ML with only one attribute. It gives us an idea for the finest predicted crop which will be cultivate in the field weather conditions [1]. These predictions can be done by a machine learning algorithm called Random Forest. Various machine learning techniques we are using such as KNN, DT, RF, BAGGING and GRADIENT BOOSTING [7]. However, the selection of the appropriate algorithm from the pool of available algorithms imposes challenge to the researchers with respect to the chosen crop. This system will be useful to justify which crop can be grown in a particular region [8]. a that incorporates satellite-derived seasonal forecasting data physical models and other to a pre-season prediction of soybean/maize yield---with no need of This provides significantly useful results by the exempting the need for high-resolution remote-sensing allowing farmers to prepare for adverse climate influence on the crop cycle. In our studies, we forecast the soybean and maize for and USA, which corresponded to of the world's 2016. Results show the error metrics for soybean and maize yield forecasts are comparable to similar systems that only provide yield forecast information in the first to months of the crop cycle.[18][19][20][21].",,2022,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
793657833492032254880ff9691774a02eb8a173,https://www.semanticscholar.org/paper/793657833492032254880ff9691774a02eb8a173,Prediction of Factors Influencing the Starting Salary of College Graduates Based on Machine Learning,"It is an important deployment of the Party Central Committee and the State Council to fully promote the employment of college graduates with higher quality, and salary is an important indicator of quality measurement. This paper takes the cross-sectional data of the employment of graduates from a financial and economic university in 2020 as the sample; whether the actual starting salary is a high salary as the dependent variable; and human capital, social capital, labor market as the explanatory variables and uses R to establish a logistic regression model to analyze the determinants of the high salary of graduates. Five machine learning methods, SVM, naive Bayes, CART, random forest, and XGBoost, are used to predict whether graduates can get a high starting salary, compare the advantages and disadvantages of various methods horizontally, optimize the parameters at the same time, and further enhance the performance of the model. Based on the employment data of graduate students in a university of finance and economics in 2020, this paper makes an empirical study. The study shows that academic qualifications, professional disciplines, employment regions, employment industries, the nature of employment units, gender, and whether they have served as student cadres have a significant impact on whether graduates can get “high salaries.” The main factors affecting the starting salary of graduates are the accumulation of human capital and social capital, but the segmentation of labor market is also the main reason affecting the starting salary of graduates. The prediction results of several models show that the integrated models have better performance than single models, and the XGBoost model is the best, which can help predict whether graduates get high salary.",Wireless Communications and Mobile Computing,2022,10.1155/2022/7845545,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
d2777dc615d7cf850066d2ffa4555ed1b7dd8d86,https://www.semanticscholar.org/paper/d2777dc615d7cf850066d2ffa4555ed1b7dd8d86,Machine learning models for effective prediction of firefighting systems and reducing environmental damage,"Artificial intelligence is now widely used in various industries, including fire safety. When designing and deploying firefighting systems in enterprises and organizations, a large number of different factors are taken into account. This article considers the possibility of applying machine learning models to ensure a high level of protection of the environment and industrial facilities from fires with a properly selected fire suppression system based on the analysis of the data provided.",IOP Conference Series: Earth and Environmental Science,2022,10.1088/1755-1315/981/3/032060,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
b211e16e7e6c70a003c89f4bc2a9f3a434bb70cd,https://www.semanticscholar.org/paper/b211e16e7e6c70a003c89f4bc2a9f3a434bb70cd,"Open-Source Clinical Machine Learning Models: Critical Appraisal of Feasibility, Advantages, and Challenges","Machine learning applications promise to augment clinical capabilities and at least 64 models have already been approved by the US Food and Drug Administration. These tools are developed, shared, and used in an environment in which regulations and market forces remain immature. An important consideration when evaluating this environment is the introduction of open-source solutions in which innovations are freely shared; such solutions have long been a facet of digital culture. We discuss the feasibility and implications of open-source machine learning in a health care infrastructure built upon proprietary information. The decreased cost of development as compared to drugs and devices, a longstanding culture of open-source products in other industries, and the beginnings of machine learning–friendly regulatory pathways together allow for the development and deployment of open-source machine learning models. Such tools have distinct advantages including enhanced product integrity, customizability, and lower cost, leading to increased access. However, significant questions regarding engineering concerns about implementation infrastructure and model safety, a lack of incentives from intellectual property protection, and nebulous liability rules significantly complicate the ability to develop such open-source models. Ultimately, the reconciliation of open-source machine learning and the proprietary information–driven health care environment requires that policymakers, regulators, and health care organizations actively craft a conducive market in which innovative developers will continue to both work and collaborate.",JMIR formative research,2022,10.2196/33970,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
f4cf81cc95826ad3dbca25ab73b76b51ddf26f80,https://www.semanticscholar.org/paper/f4cf81cc95826ad3dbca25ab73b76b51ddf26f80,A Comprehensive Study on Machine Learning Algorithms for Intrusion Detection System,"Rapid advancements in areas of communication and the internet ensued a significant boost in data and capacity of the network. As a consequence, a plethora of novel threats are being built, creating it harder for network security to efficiently identify breaches. Furthermore, intruders also initiate multiple attacks inside this network must be taken into account. An intrusion Detection System (IDS) is a method that checks network activity for availability, consistency, and secrecy to defend the network from potential invasions. Although the greatest efforts of professionals, IDSs continue to struggle with enhancing detecting accurateness whilst lowering false alarm rates and detecting newer intrusions. IDS systems based on ML (Machine Learning), as well as DL (Deep Learning), have recently been deployed as feasible procedures for swiftly identifying network intrusions. The taxonomy presented in this article is based on well-known ML as well as DL approaches for building network-based IDS (NIDS) systems, as well as it first defines the concept of IDS. In this comprehensive review of existing NIDS-based research, the merits and limitations of the suggested solutions are thoroughly explored. This evaluation may act as a standard for researchers and industry in terms of the advancement and development of Network Intrusion Detection Systems in the future.",2022 10th International Conference on Emerging Trends in Engineering and Technology - Signal and Information Processing (ICETET-SIP-22),2022,10.1109/ICETET-SIP-2254415.2022.9791586,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
9ea1afcd95045073a41501548cf54470222569dc,https://www.semanticscholar.org/paper/9ea1afcd95045073a41501548cf54470222569dc,Crop recommendation and yield prediction using machine learning algorithms,"Agriculture is the foundation of many countries' economies, particularly in India and Tamil Nadu. The young generation who are new to farming may confront the challenge of not understanding what to sow and what to reap benefit from. This is a problem that has to be addressed, and it is one that we are addressing. Predicting the proper crop and production will aid in making better decisions, reducing losses and managing the risk of price fluctuations. The existing system is not deployed, unlike ours, which is done by applying classification and regression algorithms to calculate crop type recommendations and yield predictions. Agricultural industries must use machine learning algorithms to anticipate the crop from a given dataset. The supervised machine learning technique is used to analyse a dataset in order to capture information from multiple sources, such as variable identification, uni-variate analysis, bi-variate and multi-variate analysis, missing value treatments, and so on. A comparison of machine learning algorithms was conducted in order to identify which algorithm was more accurate in predicting the best harvest. The results show that the proposed machine learning algorithm technique has the best accuracy when comparing entropy calculation, precision, Recall, F1 Score, Sensitivity, Specificity, and Entropy.",World Journal of Advanced Research and Reviews,2022,10.30574/wjarr.2022.14.3.0581,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
ede53cd87532acbc8b947e2f311d0510ccef7f04,https://www.semanticscholar.org/paper/ede53cd87532acbc8b947e2f311d0510ccef7f04,Automated machine learning: AI-driven decision making in business analytics,"The realization that AI-driven decision-making is indispensable in today’s fast-paced and ultra-competitive marketplace has raised interest in industrial machine learning (ML) applications significantly. The current demand for analytics experts vastly exceeds the supply. One solution to this problem is to increase the user-friendliness of ML frameworks to make them more accessible for the non-expert. Automated machine learning (AutoML) is an attempt to solve the problem of expertise by providing fully automated off-the-shelf solutions for model choice and hyperparameter tuning. This paper analyzed the potential of AutoML for applications within business analytics, which could help to increase the adoption rate of ML across all industries. The H2O AutoML framework was benchmarked against a manually tuned stacked ML model on three real-world datasets to test its performance, robustness, and reliability. The manually tuned ML model could reach a performance advantage in all three case studies used in the experiment. Nevertheless, the H2O AutoML package proved to be quite potent. It is fast, easy to use, and delivers reliable results, which come close to a professionally tuned ML model. The H2O AutoML framework in its current capacity is a valuable tool to support fast prototyping with the potential to shorten development and deployment cycles. It can also bridge the existing gap between supply and demand for ML experts and is a big step towards fully automated decisions in business analytics.",SSRN Electronic Journal,2022,10.48550/arXiv.2205.10538,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
49f737096f821a2c8404d2d6349c7e1360179dc9,https://www.semanticscholar.org/paper/49f737096f821a2c8404d2d6349c7e1360179dc9,SSD Drive Failure Prediction on Alibaba Data Center Using Machine Learning,"Flash-based Solid-State Drives (SSDs) have become a critical storage tier in data centers and enterprise storage systems. Cloud companies are very interested in predicting drive failures. Drive failure prediction enables managing drive replacement and backup data beforehand and helps planning drive purchase strategies. Solidigm and Alibaba collaborate to collect and analyze Self-Monitoring, Analysis, and Reporting Technology (SMART) data and predict SSD failures 30 days ahead of time using machine learning techniques. In this paper, we use group k-fold cross-validation to select the best parameters for machine learning models and avoid overfitting. After obtaining the prediction score of each sample from the model, a post-processing with neural network is applied on those prediction scores to get the drive-level prediction. A modified ensemble learning method is designed and implemented by majority voting on different models of Light GBM and Random Forest to further improve prediction results. This paper is the first work in both academia and the storage industry to design a drive failure prediction system for deploying in data centers by optimizing models with the highest Precision instead of the highest F1-score to minimize false positive rate. We advance to get drive failure prediction with 100% Precision and 21% Recall, enabling us to avoid the high cost of false positives.",2022 IEEE International Memory Workshop (IMW),2022,10.1109/IMW52921.2022.9779284,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
c08f80ac3d645219834edaaa3b467d24d3c1a91e,https://www.semanticscholar.org/paper/c08f80ac3d645219834edaaa3b467d24d3c1a91e,Prospects and Challenges of Using Machine Learning for Academic Forecasting,"The study examines the prospects and challenges of machine learning (ML) applications in academic forecasting. Predicting academic activities through machine learning algorithms presents an enhanced means to accurately forecast academic events, including the academic performances and the learning style of students. The use of machine learning algorithms such as K-nearest neighbor (KNN), random forest, bagging, artificial neural network (ANN), and Bayesian neural network (BNN) has potentials that are currently being applied in the education sector to predict future events. Many gaps in the traditional forecasting techniques have greatly been bridged by the use of artificial intelligence-based machine learning algorithms thereby aiding timely decision-making by education stakeholders. ML algorithms are deployed by educational institutions to predict students' learning behaviours and academic achievements, thereby giving them the opportunity to detect at-risk students early and then develop strategies to help them overcome their weaknesses. However, despite the benefits associated with the ML approach, there exist some limitations that could affect its correctness or deployment in forecasting academic events, e.g., proneness to errors, data acquisition, and time-consuming issues. Nonetheless, we suggest that machine learning remains one of the promising forecasting technologies with the power to enhance effective academic forecasting that would assist the education industry in planning and making better decisions to enrich the quality of education.",Computational intelligence and neuroscience,2022,10.1155/2022/5624475,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
3848a772ce1f276d7b4ec51b35205ff4afc6d7ed,https://www.semanticscholar.org/paper/3848a772ce1f276d7b4ec51b35205ff4afc6d7ed,A Machine Learning Centered Approach for Uncovering Excavators’ Last Known Location Using Bluetooth and Underground WSN,"Machine learning and data analytics are two of the most popular subdisciplines of modern computer science which have a variety of scopes in most of the industries ranging from hospitals to hotels, manufacturing to pharmaceuticals, mining to banking, etc. Additionally, mining and hospitals are two of the most critical industries where applications when deployed security, accuracy, and cost effectiveness are the major concerns, due to the huge involvement of man and machines. In this paper, the problem of finding out the location of man and machines has been focused on in case of an accident during the mining process. The primary scope of the research is to guarantee that the projected position is near to the real place so that the trained model’s performance can be tested. The solution has been implemented by first proposing the MLAELD (Machine Learning Architecture for Excavators’ Location Detection), in which Bluetooth Low Energy (BLE) beacons have been used for tracking the live locations of excavators preceded by collecting the data of the signal strength mapping from multiple beacons at each specific point in a closed area. Second, machine learning techniques are proposed to develop and train multioutput regression models using linear regression, K-nearest neighbor regression, decision tree regression, and random forest regression. These techniques can predict the live locations of the required persons and machines with a high level of precision from the last beacon strengths received.",Wireless Communications and Mobile Computing,2022,10.1155/2022/9160031,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
182bf523f4fcfa2d2377037bd38eb96ad25e61f3,https://www.semanticscholar.org/paper/182bf523f4fcfa2d2377037bd38eb96ad25e61f3,Computer Vision and Machine Learning for Viticulture Technology,"This paper gives two contributions to the state-of-the-art for viticulture technology research. First, we present a comprehensive review of computer vision, image processing, and machine learning techniques in viticulture. We summarize the latest developments in vision systems and techniques with examples from various representative studies, including, harvest yield estimation, vineyard management and monitoring, grape disease detection, quality evaluation, and grape phenology. We focus on how computer vision and machine learning techniques can be integrated into current vineyard management and vinification processes to achieve industry relevant outcomes. The second component of the paper presents the new GrapeCS-ML database which consists of images of grape varieties at different stages of development together with the corresponding ground truth data (e.g., pH and Brix) obtained from chemical analysis. One of the objectives of this database is to motivate computer vision and machine learning researchers to develop practical solutions for deployment in smart vineyards. We illustrate the usefulness of the database for a color-based berry detection application for white and red cultivars and give baseline comparisons using various machine learning approaches and color spaces. This paper concludes by highlighting future challenges that need to be addressed prior to successful implementation of this technology in the viticulture industry.",IEEE Access,2018,10.1109/ACCESS.2018.2875862,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
4309d32e74383f2cecced7f8e349058c57d08f92,https://www.semanticscholar.org/paper/4309d32e74383f2cecced7f8e349058c57d08f92,On-The-Go Hyperspectral Imaging Under Field Conditions and Machine Learning for the Classification of Grapevine Varieties,"Grapevine varietal classification is an important plant phenotyping issue for grape growing and wine industry. This task has been achieved from destructive techniques like classic ampelography and DNA analysis under laboratory conditions. This work displays a new approach for the classification of a high number of grapevine (Vitis vinifera L.) varieties under field conditions using on-the-go hyperspectral imaging and different machine learning algorithms. On-the-go imaging was performed under natural illumination using a hyperspectral camera mounted on an all-terrain vehicle at 5 km/h. Spectra were acquired over two different leaf phenological stages on the canopy of 30 different varieties on a commercial vineyard located in La Rioja, Spain. A total of 1,200 spectral samples were generated. Support vector machines (SVM) and artificial neural networks (multilayer perceptrons, MLP) were used for the development of a large number of models, testing different algorithm parameters and spectral pre-processing techniques. Both classifiers yielded notable performance values and were able to train models with recall F1 scores and area under the receiver operating characteristic curve marks up to 0.99 for 5-fold cross validation. Statistical analyses supported that the best SVM kernel was linear and the best activation function for MLP was the hyperbolic tangent function. The prediction performance for individual varieties of MLP ranged from 0.94 to 0.99, displaying low levels of variability. In the case of SVM, slightly higher differences were obtained, ranging from 0.83 to 0.97 for individual varieties. These results support the possibility of deploying an on-the-go hyperspectral imaging system in the field capable of successfully classifying leaves from different grapevine varieties. This technology could thus be considered as a new useful non-destructive tool for plant phenotyping under field conditions.",Front. Plant Sci.,2018,10.3389/fpls.2018.01102,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
d02ec7d794aa9b4fbae1614f4b8e835a1ced291e,https://www.semanticscholar.org/paper/d02ec7d794aa9b4fbae1614f4b8e835a1ced291e,"SDN/NFV, Machine Learning, and Big Data Driven Network Slicing for 5G","5G networks are expected to be able to satisfy a variety of vertical services for mobile users, business demands, and automotive industry. Network slicing is a promising technology for 5G to provide a network as a service (NaaS) for a wide range of services that run on different virtual networks deployed on a shared network infrastructure. Moreover, the SON (self-organizing network) in 5G is expected as a significant evolution to guarantee for full intelligence, automatic, and faster management and optimization. To deal with those requirements, recently, software-defined networking (SDN), network functions virtualization (NFV), big data, and machine learning have been proposed as emerging technologies and the necessary tools for 5G, especially, for network slicing. This study aims to integrate various machine learning (ML) algorithms, big data, SDN, and NFV to build a comprehensive architecture and an experimental framework for the future SONs and network slicing. Finally, based on this framework, we successfully implemented an early state traffic classification and network slicing for mobile broadband traffic applications implemented at Broadband Mobile Lab (BML), National Chiao Tung University (NCTU).",2018 IEEE 5G World Forum (5GWF),2018,10.1109/5GWF.2018.8516953,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
f10cc958252a7c18444ee2634f84b48623c5725a,https://www.semanticscholar.org/paper/f10cc958252a7c18444ee2634f84b48623c5725a,"Machine learning 2018 and Big Data 2018- Digital transformation and the convergence of new emerging digital technologies - Samir El Masri - Digitalization. Cloud, UAE","Digital transformation is a journey that stems from strong beliefs in the digital economy by senior management supported by a digital transformation strategy. The strategy is much more difficult to deploy than develop and it may only be achieved when the transformation is led by CEOs reinforced by mature capabilities. Unfortunately, most digital transformation initiatives have failed in the past and many more will fail in the future. Advanced change is the method of using advanced innovations to make modern — or adjust existing — trade forms, culture, and client encounters to meet changing business and showcase prerequisites. Additionally, advanced changes have reshaped how companies approach client benefits. Making call centers and in-store benefit work areas run more proficiently with advanced innovation is of course awesome. But genuine change comes after you see at all accessible advances and consider how adjusting your trade to them can donate clients distant better;a much better;a higher;a stronger;an improved"">a higher involvement. Social media wasn’t designed to require the put of call centers, but it’s ended up an extra channel (and opportunity) to offer way better client benefit. Adjusting your benefit offerings to grasp social media is another great case of a computerized change.The ancient show was to hold up for clients to come to discover you, whether in individual or by calling an 800 number. But the rise of social media has changed benefit much like it’s changed promoting, promoting, and indeed deals and client benefit. Dynamic companies grasp social media as a chance to expand their benefits offerings by assembly clients on their stages of choice. 
 
Machine learning includes computers finding how they can perform errands without being expressly modified to do so. For straightforward errands relegated to computers, it is conceivable to program calculations telling the machine how to execute all steps required to fathom the issue at hand; on the computer's portion, no learning is required. For more progressed assignments, it can be challenging for a human to physically make the required calculations. In hone, it can turn out to be more successful to assist the machine develop its claim calculation, instead of have human software engineers indicate each required step. The discipline of machine learning employs different approaches to assist computers learn to achieve errands where no completely palatable calculation is accessible. In cases where tremendous numbers of potential answers exist, one approach is to name a few of the right answers as substantial. This will at that point be utilized as preparing information for the computer to progress the algorithm(s) it employments to decide rectify answers. 
This reimagining of commerce within the advanced age is computerized transformation.A key component of computerized change is understanding the potential of your innovation. Once more, that doesn’t cruel inquiring “How much speedier can we do things the same way?” It implies inquiring “What is our innovation truly able of, and how can we adjust our trade and forms to create the foremost of our technology investments?” Before Netflix, individuals chose motion pictures to lease by attending to stores and combing through racks of tapes and circles in look of something that looked great. Presently, libraries of computerized substance are served up on individual gadgets, total with proposals and audits based on client inclinations.It rises above conventional parts like deals, showcasing, and client benefit. Instep, computerized change starts and closes with how you think almost, and lock in with, clients. As we move from paper to spreadsheets to keen applications for overseeing our trade, we have the chance to reimagine how we do commerce — how we lock in our clients — with computerized innovation on our side. For little businesses fair getting begun, there’s no ought to set up your trade forms and change them afterward. You'll be able to future-proof your association from the word go. Building a 21st-century commerce on stickies and manually written records fair isn’t feasible. Considering, arranging, and building carefully sets you up to be spry, adaptable, and prepared to develop. Not so long prior, businesses kept records on paper. Whether transcribed in records or written into reports, commerce information was analog. On the off chance that you needed to assemble or share data, you managed with physical reports — papers and folios, xeroxes, and faxes. Then computers went standard, and most businesses begun changing over all of those ink-on-paper records to advanced computer records. Typically called digitisation: the method of changing over data from analog to computerized.The method of using digitized data to form set up ways of working less difficult and more proficient is called digitalisation. Note the word established in that definition: Digitalisation isn’t approximately changing how you are doing trade, or making unused sorts of businesses. It’s around keeping on keeping on, but quicker and way better presently that your information is right away open and not caught in a record cabinet someplace in a dusty archive. Think of client benefit, whether in retail, field ops, or a call center. Digitalization changed benefit until the end of time by making client records effortlessly and quickly retrievable through the computer. The essential technique of client benefit didn’t alter, but the method of handling an inquiry, looking up the important information, and advertising a determination got to be much more productive when looking paper records was supplanted by entering many keystrokes on a computer screen or versatile gadget. Digital change is changing the way trade gets done and, in a few cases, making completely unused classes of businesses. With the computerized change, companies are taking a step back and returning to everything they do, from inside frameworks to clients intuitive both online and in individual. They’re inquiring huge questions like “Can we alter our forms in a way that will empower way better decision-making, game-changing efficiencies, or distant better;a much better; a higher;a stronger;an improved"">an improved client encounter with more personalization?” Now we’re immovably dug in within the computerized age, and businesses of all sorts are making intelligent, viable, and troublesome ways of leveraging innovation. Netflix could be an incredible case. It begun out as a mail arrange benefit and disturbed the brick-and-mortar video rental commerce. At that point, computerized advancements made wide-scale gushing video conceivable. Nowadays, Netflix takes on conventional broadcast and cable tv systems and generation studios all at once by advertising a developing library of on-demand 
 
These failures have been mainly due to organizations undertaking digital change instead of digital transformation in addition to the lack of capabilities and non-readiness of the company to manage this transformation. New digital emerging technologies remain the backbone and the enabler of any digital transformation activities. The digitization of operations, workforce, marketing, and new digital business models will be realized by the convergence of all new emerging digital technologies through new products/services, price, customer experience, and platform values. In this talk, data science, machine learning, analytics, big data, IOT and their interrelationships will be demonstrated. Examples of how digital initiatives could help the industry by improving efficiency, avoiding trips, reducing unplanned downtime and transforming from time-based to condition-based maintenance will also be illustrated.",,2018,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
6ea25b3405e8cd27a0f0a5a8e0f2dcbc053d6280,https://www.semanticscholar.org/paper/6ea25b3405e8cd27a0f0a5a8e0f2dcbc053d6280,Quantitative Finance Research Newsletter Oxford-Man Institute OMIReNew Recovering Missing Firm Characteristics with Attention-based Machine Learning,"model reconstructs firm characteristics with high accuracy and comfortably outperforms competing approaches. Revisiting the vast literature on risk factors in financial research reveals the of missing observations The work develops an approach for solving time-consistent risk-sensitive stochastic optimisation problems using model-free reinforcement learning. It assumes that agents assess the risk of a sequence of random variables using dynamic convex risk measures. They employ a time-consistent dynamic programming principle to determine the value of a particular policy, and develop policy gradient update rules that aid in obtaining optimal policies. They further develop an actor-critic style algorithm using neural networks to optimise over policies. Finally, the work demonstrates the performance and flexibility of their approach by applying it to three optimisation problems: statistical arbitrage trading strategies, obstacle avoidance robot control, and financial hedging. We present a simple and effective methodology for the generation of lexicons (word lists) that may be used in natural language scoring applications. In particular, in the finance industry, word lists have become ubiquitous for sentiment scoring. These have been derived from dictionaries such as the Harvard Inquirer and require manual curation. Here, we present an automated approach to the curation of lexicons, which makes automatic preparation of any initial word list immediate, which can then be further curated. We show that our automated word lists deliver comparable performance to traditional lexicons on machine learning classification tasks. This new approach will enable finance academics and practitioners to create and deploy new word lists in addition to the few traditional ones in a facile manner. The work studies the long-term impact of climate change on economic activity across countries, using a stochastic growth model where productivity is affected by deviations of temperature and precipitation from their long-term moving average historical norms. Using a panel data set of 174 countries the 1960 2014, they find that the per-capita real output divergence to The demonstrates the effectiveness with numerical experiments which highlight both removal and the fidelity of the calibrated simulator. on the estimation of the equity joint estimation of the The work investigates the impact of order flow imbalance (OFI) on price movements in equity markets in a multiasset setting. First, authors show that taking into account multiple levels of the order book when defining order book imbalance leads to higher explanatory power for the contemporaneous price impact of OFI. Using a principal component analysis of OFI across order book levels, they define a notion of integrated OFI which shows superior explanatory power for market impact both in-sample and out-of-sample. Second, they examine the notion of cross-impact and show that, once the information from multiple levels is included in OFI, multi-asset models with cross-impact do not provide additional explanatory power for contemporaneous impact compared to a sparse model without cross-impact terms. However, they find evidence that cross-impact terms provide additional information for intraday forecasting of future returns",,2022,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
c54d8b94e3290ae5e02ffcfdc407e6d519ceec2f,https://www.semanticscholar.org/paper/c54d8b94e3290ae5e02ffcfdc407e6d519ceec2f,Medical Healthcare System with Hybrid Block based Predictive models for Quality preserving in Medical Images using Machine Learning Techniques,"Cloud technology is a business strategy that aims to provide the necessary material to customers depending on their needs. Individuals and cloud businesses alike have embraced the cloud storage service, which has become the most widely used service. The industries outsource their data to cloud storage space to relieve themselves of a load of dealing with redundant data contents. This must be protected to prevent the theft of personal belongings, and privacy must be improved as well. Different research projects have been suggested to ensure the safe management of the information included within the data content. The security of current research projects, on the other hand, still needs improvement. As a result, this method has been suggested to address the security concerns associated with cloud computing. The primary goal of this study effort is to offer a safe environment for cloud users while also increasing the profit of cloud resource providers by managing and securely delivering data contents to the cloud users. The bulk of sectors, including business, finance, the military, and the healthcare industry, do not store data in cloud-based storage systems. This technique is used to attract these kinds of customers. Increasing public acceptance Medical researchers are drawn to cloud computing because it allows them to store their study material in a centralized location and distribute and access it in a more flexible manner. They were collected from numerous individuals who were being evaluated for medical care at the time. Scalable and Enhanced Key Aggregate Cryptosystem is a protected data protection method that provides highly effective security in the health care industry. This approach handles disagreements in the outflow of sensitive information and guarantees the data security deployment of a Cloud-based Intelligent Health Monitoring system for the parties involved in the dispute. Using the suggested method, the encrypted data format of medical and health-care prescriptions is recorded as it passes through the hands of patients and healthcare institutions. To increase the level of security, the double encryption method is used. During the encryption process, the Ciphertext ID is referred to as a class. The keyholder is a master secret key that aids in the retrieval of the secret keys of different kinds of monsters and creatures. The extracted key is transmitted and kept as a single aggregate for the benefit of the patient or client to facilitate decryption. Between the use of a key aggregation cryptosystem and double encryption method, the Cloud-based Intelligent Health Monitoring systems may establish a secure link with Healthcare Organizations and patients. As a result, when compared to prior methods, the results demonstrate that the study methodology achieves high levels of security in terms of confidentiality and integrity, as well as great scalability.",2022 International Conference on Advanced Computing Technologies and Applications (ICACTA),2022,10.1109/ICACTA54488.2022.9753355,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
b2b8efc1ef1cd6248119e3503c43a352ec9dc973,https://www.semanticscholar.org/paper/b2b8efc1ef1cd6248119e3503c43a352ec9dc973,Real-Time Event-Driven Learning in Highly Volatile Systems: A Case for Embedded Machine Learning for SCADA Systems,"Extracting key system parameters and their impact on state transition is a necessity for knowledge and data engineering. In Decision Support Systems, the quest for yet more efficient and faster methods of sensitivity analysis (SA) and feature extraction in complex and volatile systems persists. A new improved event tracking methodology, the fastTracker, for real-time SA in large scale complex systems is proposed in this paper. The main feature of fastTracker is its high-frequency analytics using meager computational cost. It is suitable for data processing and prioritization in embedded systems, Internet of Things (IoT), distributed computing (e.g. Edge computing) applications. The presented algorithm’s underpinning rationale is event driven; its objective is to correctly and succinctly quantify the sensitivity of observable changes in the system (output) with respect to the input variables. To demonstrate the performance of the proposed fastTracker methodology, fastTracker was deployed in the Supervisory control and data acquisition (SCADA) system from real cement industry. fastTracker has been verified by system experts in real industrial application. Its performance was compared with other real-time event-based SA techniques. The comparison revealed savings of 98.8% in processing time per sensitivity index and 20% in memory usage when compared with EventTracker, its closest rival. The proposed methodology is more accurate and 80.9% faster than an entropy-based method. Its application is recommended for reinforced learning and/or formulating system key performance indicators from raw data.",IEEE Access,2022,10.1109/access.2022.3173376,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
1d81480f1ea5263974bfa900a841860be3d8184b,https://www.semanticscholar.org/paper/1d81480f1ea5263974bfa900a841860be3d8184b,Big Data Statistics Analysis of Computer Intelligence and Machine Learning Technology,"In the recent technological world, Computer technology and internet technology plays a vital role. These technologies are deployed in many industries. In this research, we are going to study the correlation between the complexities of financial reporting and the value of economic statistics in the context of Computer intelligence and information technology. Financial reporting is referred to as an accounting practice that involves processes like documentation & communication of financial activities. The performance over time of the companies is derived from the financial report. Financial reports provide vital information about the company’s financial status and it also helps the company in decision making. Balance sheets, income statements, statements on shareholder equity, cash flow statements, are some of the types of financial reports. It facilitates tracking the cash flow, analyzing shareholder equity, evaluating the assets and liabilities, checking the profitability etc. Economic statistics are referred to us as the branch of applied statistics that deals with the collection and processing of economic data. It also compiles and analyzes the economic data. Financial reports check the money flow for businesses. Economic Statistics checks the larger trends that decide the money & resources flow. Even though, both help in making financial decisions there is some complexity between the two correlated fields. With the help of computer intelligence and internet technology, the complexity is reduced considerably.",2022 6th International Conference on Intelligent Computing and Control Systems (ICICCS),2022,10.1109/ICICCS53718.2022.9788198,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
6d6147d29202c86f23ee8ce1b00659fa5720380f,https://www.semanticscholar.org/paper/6d6147d29202c86f23ee8ce1b00659fa5720380f,"Intrusion Detection in Internet of Things Systems: A Review on Design Approaches Leveraging Multi-Access Edge Computing, Machine Learning, and Datasets","The explosive growth of the Internet of Things (IoT) applications has imposed a dramatic increase of network data and placed a high computation complexity across various connected devices. The IoT devices capture valuable information, which allows the industries or individual users to make critical live dependent decisions. Most of these IoT devices have resource constraints such as low CPU, limited memory, and low energy storage. Hence, these devices are vulnerable to cyber-attacks due to the lack of capacity to run existing general-purpose security software. It creates an inherent risk in IoT networks. The multi-access edge computing (MEC) platform has emerged to mitigate these constraints by relocating complex computing tasks from the IoT devices to the edge. Most of the existing related works are focusing on finding the optimized security solutions to protect the IoT devices. We believe distributed solutions leveraging MEC should draw more attention. This paper presents a comprehensive review of state-of-the-art network intrusion detection systems (NIDS) and security practices for IoT networks. We have analyzed the approaches based on MEC platforms and utilizing machine learning (ML) techniques. The paper also performs a comparative analysis on the public available datasets, evaluation metrics, and deployment strategies employed in the NIDS design. Finally, we propose an NIDS framework for IoT networks leveraging MEC.",Sensors,2022,10.3390/s22103744,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
c8187a590c73e685d9ed2c23060bdc5549080540,https://www.semanticscholar.org/paper/c8187a590c73e685d9ed2c23060bdc5549080540,Machine Learning Enabled Techniques for Protecting Wireless Sensor Networks by Estimating Attack Prevalence and Device Deployment Strategy for 5G Networks,"A number of disadvantages of traditional networks may be attributed to the close relationship that exists between the control plane and the data plane inside proprietary hardware designs, as described above. The problem of security is one of the most difficult to deal with. There are a plethora of network hazards and attacks that might be encountered these days. DDoS attacks are one of the most popular and disruptive attacks on the internet today, and they affect a wide range of organisations. Despite a large number of traditional mitigation solutions now available, the frequency, volume, and intensity of distributed denial-of-service (DDoS) attacks continue to rise. According to the findings of this paper, a new network paradigm is necessary to satisfy the requirements of today’s complex security concerns. It was necessary to develop a software-defined network (SDN) in order to meet the real-time needs of the massive network that was expanding at an exponential rate. Many advantages of SDN exist, including simplicity of administration, scalability, and agility, but one of the most critical is security, which is one of the most important considerations when implementing SDN. SDS may be seen as a paradigm in which the implementation of new security regulations in the computer environment is performed via the use of protected software, which is described further below. The goal is to provide a flexible and extensible architecture for DDoS detection and prevention that is both flexible and extendable; the suggested clustering approach, which is based on the Open Day Light (ODL) Controller, is employed to carry out the experimental findings. In this section, we emphasise DDoS penetration techniques from a range of tools, and we evaluate the vulnerability against various tactics. It is necessary to use a Mininet emulation tool to construct a detection and prevention system against distributed denial of service (DDoS) attacks in order to achieve success. There is a range of other simulation tools that are utilised in conjunction with this research in order to bring it to a conclusion. Integration of industry standards such as SNORT and Flow has been accomplished in a variety of situations and parameter settings. During the creation of a framework capable of detecting and mitigating DDoS attacks at an early stage in both the control and application levels, the implementation of this framework has been shown to be crucial in the development of a framework.",Wireless Communications and Mobile Computing,2022,10.1155/2022/5713092,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
28943e4f8380fc989b2fd6067c9b366f4cdd48cc,https://www.semanticscholar.org/paper/28943e4f8380fc989b2fd6067c9b366f4cdd48cc,Key Aggregation Cryptosystem and Double Encryption Method for Cloud-Based Intelligent Machine Learning Techniques-Based Health Monitoring Systems,"Cloud technology is a business strategy that aims to provide the necessary material to customers depending on their needs. Individuals and cloud businesses alike have embraced the cloud storage service, which has become the most widely used service. The industries outsource their data to cloud storage space to relieve themselves of the load of dealing with redundant data contents. This must be protected to prevent the theft of personal belongings, and privacy must be improved as well. Different research projects have been suggested to ensure the safe management of the information included within the data content. The security of current research projects, on the contrary, still needs improvement. As a result, this method has been suggested to address the security concerns associated with cloud computing. The primary goal of this study effort is to offer a safe environment for cloud users while also increasing the profit of cloud resource providers by managing and securely delivering data contents to the cloud users. The bulk of sectors, including business, finance, military, and healthcare industry, do not store data in cloud-based storage systems. This technique is used to attract these kinds of customers. Increasing public acceptance, medical researchers are drawn to cloud computing because it allows them to store their study material in a centralized location and distribute and access it in a more flexible manner. They were collected from numerous individuals who were being evaluated for medical care at the time. Scalable and enhanced key aggregate cryptosystem is a protected data protection method that provides highly effective security in the healthcare industry. When parties interested in a dispute disagree on the outflow of sensitive information, this technique manages the disputes and ensures the data security deployment of a cloud-based intelligent health monitoring system for the parties involved. The encrypted data structure of medical and healthcare prescriptions is recorded as they move through the hands of patients and healthcare facilities, according to the technique recommended. The double encryption approach is used in order to raise the overall degree of security. An encryption class is created by referring to the Ciphertext ID during the encryption procedure. The keyholder is a master secret key that facilitates in the recovery of the secret keys of various monsters and creatures by acting as a conduit between them. It is transferred and stored as a single aggregate for the benefit of the patient or customer in order to make decryption more convenient and efficient. A safe connection between cloud-based intelligent health monitoring systems and healthcare organizations and their patients may be established via the use of a key aggregation cryptosystem and a double encryption approach, according to the researchers. Because of this, when compared to earlier techniques, the findings reveal that the research methodology provides high levels of security in terms of confidentiality and integrity, in addition to excellent scalability.",Computational intelligence and neuroscience,2022,10.1155/2022/3767912,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
d64c3b866d605f131611666191899d9aabf5cf59,https://www.semanticscholar.org/paper/d64c3b866d605f131611666191899d9aabf5cf59,"ISTHMUS: Secure, Scalable, Real-time and Robust Machine Learning Platform for Healthcare","In recent times, machine learning (ML) and artificial intelligence (AI) based systems have evolved and scaled across different industries such as finance, retail, insurance, energy utilities, etc. Among other things, they have been used to predict patterns of customer behavior, to generate pricing models, and to predict the return on investments. But the successes in deploying machine learning models at scale in those industries have not translated into the healthcare setting. There are multiple reasons why integrating ML models into healthcare has not been widely successful, but from a technical perspective, general-purpose commercial machine learning platforms are not a good fit for healthcare due to complexities in handling data quality issues, mandates to demonstrate clinical relevance, and a lack of ability to monitor performance in a highly regulated environment with stringent security and privacy needs. In this paper, we describe Isthmus, a turnkey, cloud-based platform which addresses the challenges above and reduces time to market for operationalizing ML/AI in healthcare. Towards the end, we describe three case studies which shed light on Isthmus capabilities. These include (1) supporting an end-to-end lifecycle of a model which predicts trauma survivability at hospital trauma centers, (2) bringing in and harmonizing data from disparate sources to create a community data platform for inferring population as well as patient level insights for Social Determinants of Health (SDoH), and (3) ingesting live-streaming data from various IoT sensors to build models, which can leverage real-time and longitudinal information to make advanced time-sensitive predictions.",ArXiv,2019,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
6d9cd2d863d33b57b9bf435605a4e149d13e6d39,https://www.semanticscholar.org/paper/6d9cd2d863d33b57b9bf435605a4e149d13e6d39,ML4IoT: A Framework to Orchestrate Machine Learning Workflows on Internet of Things Data,"Internet of Things (IoT) applications generate vast amounts of real-time data. Temporal analysis of these data series to discover behavioural patterns may lead to qualified knowledge affecting a broad range of industries. Hence, the use of machine learning (ML) algorithms over IoT data has the potential to improve safety, economy, and performance in critical processes. However, creating ML workflows at scale is a challenging task that depends upon both production and specialized skills. Such tasks require investigation, understanding, selection, and implementation of specific ML workflows, which often lead to bottlenecks, production issues, and code management complexity and even then may not have a final desirable outcome. This paper proposes the Machine Learning Framework for IoT data (ML4IoT), which is designed to orchestrate ML workflows, particularly on large volumes of data series. The ML4IoT framework enables the implementation of several types of ML models, each one with a different workflow. These models can be easily configured and used through a simple pipeline. ML4IoT has been designed to use container-based components to enable training and deployment of various ML models in parallel. The results obtained suggest that the proposed framework can manage real-world IoT heterogeneous data by providing elasticity, robustness, and performance.",IEEE Access,2019,10.1109/ACCESS.2019.2948160,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
c64aed1549ac94a5110ca9fb9f0da784355c5a4b,https://www.semanticscholar.org/paper/c64aed1549ac94a5110ca9fb9f0da784355c5a4b,Democratisation of Usable Machine Learning in Computer Vision,"Many industries are now investing heavily in data science and automation to replace manual tasks and/or to help with decision making, especially in the realm of leveraging computer vision to automate many monitoring, inspection, and surveillance tasks. This has resulted in the emergence of the 'data scientist' who is conversant in statistical thinking, machine learning (ML), computer vision, and computer programming. However, as ML becomes more accessible to the general public and more aspects of ML become automated, applications leveraging computer vision are increasingly being created by non-experts with less opportunity for regulatory oversight. This points to the overall need for more educated responsibility for these lay-users of usable ML tools in order to mitigate potentially unethical ramifications. In this paper, we undertake a SWOT analysis to study the strengths, weaknesses, opportunities, and threats of building usable ML tools for mass adoption for important areas leveraging ML such as computer vision. The paper proposes a set of data science literacy criteria for educating and supporting lay-users in the responsible development and deployment of ML applications.",CVPR 2019,2019,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
674fa65c3dac8622c29c670c3ce3bee1339f7b76,https://www.semanticscholar.org/paper/674fa65c3dac8622c29c670c3ce3bee1339f7b76,Adversarial Machine Learning in Malware Detection: Arms Race between Evasion Attack and Defense,"Since malware has caused serious damages and evolving threats to computer and Internet users, its detection is of great interest to both anti-malware industry and researchers. In recent years, machine learning-based systems have been successfully deployed in malware detection, in which different kinds of classifiers are built based on the training samples using different feature representations. Unfortunately, as classifiers become more widely deployed, the incentive for defeating them increases. In this paper, we explore the adversarial machine learning in malware detection. In particular, on the basis of a learning-based classifier with the input of Windows Application Programming Interface (API) calls extracted from the Portable Executable (PE) files, we present an effective evasion attack model (named EvnAttack) by considering different contributions of the features to the classification problem. To be resilient against the evasion attack, we further propose a secure-learning paradigm for malware detection (named SecDefender), which not only adopts classifier retraining technique but also introduces the security regularization term which considers the evasion cost of feature manipulations by attackers to enhance the system security. Comprehensive experimental results on the real sample collections from Comodo Cloud Security Center demonstrate the effectiveness of our proposed methods.",2017 European Intelligence and Security Informatics Conference (EISIC),2017,10.1109/EISIC.2017.21,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
c3e4f2a0cad00eb0bff399821490206913d23b58,https://www.semanticscholar.org/paper/c3e4f2a0cad00eb0bff399821490206913d23b58,Deploying Machine Learning for a Sustainable Future,"To meet the environmental challenges of a warming planet and an increasingly complex, high tech economy, government must become smarter about how it makes policies and deploys its limited resources. It specifically needs to build a robust capacity to analyze large volumes of environmental and economic data by using machine-learning algorithms to improve regulatory oversight, monitoring, and decision-making. Three challenges can be expected to drive the need for algorithmic environmental governance: more problems, less funding, and growing public demands. This paper explains why algorithmic governance will prove pivotal in meeting these challenges, but it also presents four likely obstacles that environmental agencies will need to surmount if they are to take full advantage of big data and predictive analytics. First, agencies must invest in upgrading their information technology infrastructure to take advantage of computational advances. Relatively modest technology investments, if made wisely, could support the use of algorithmic tools that could yield substantial savings in other administrative costs. Second, agencies will need to confront emerging concerns about privacy, fairness, and transparency associated with its reliance on Big Data and algorithmic analyses. Third, government agencies will need to strengthen their human capital so that they have the personnel who understand how to use machine learning responsibly. Finally, to work well, algorithms will need clearly defined objectives. Environmental officials will need to continue to engage with elected officials, members of the public, environmental groups, and industry representatives to forge clarity and consistency over how various risk and regulatory objectives should be specified in machine learning tools. Overall, with thoughtful planning, adequate resources, and responsible management, governments should be able to overcome the obstacles that stand in the way of the use of artificial intelligence to improve environmental sustainability. If policy makers and the public will recognize the need for smarter governance, they can then start to tackle obstacles that stand in its way and better position society for a more sustainable future.",A Better Planet,2020,10.2307/j.ctvqc6gcq.26,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
134dab26c17b6d63551bbf5bac6274733e8cab31,https://www.semanticscholar.org/paper/134dab26c17b6d63551bbf5bac6274733e8cab31,Brokered Agreements in Multi-Party Machine Learning,"Rapid machine learning (ML) adoption across a range of industries has prompted numerous concerns. These range from privacy (how is my data being used?) to fairness (is this model's result representative?) and provenance (who is using my data and how can I restrict this usage?).
 Now that ML is widely used, we believe it is time to rethink security, privacy, and incentives in the ML pipeline by re-considering control. We consider distributed multi-party ML proposals and identify their shortcomings. We then propose brokered learning, which distinguishes the curator (who determines the training set-up) from that of the broker coordinator (who runs the training process). We consider the implications of this setup and present evaluation results from implementing and deploying TorMentor, an example of a brokered learning system that implements the first distributed ML training system with anonymity guarantees.",APSys '19,2019,10.1145/3343737.3343744,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
8f4957c04ec36563ae31924208d2a0390936a55f,https://www.semanticscholar.org/paper/8f4957c04ec36563ae31924208d2a0390936a55f,Application of Deep Learning in the Deployment of an Industrial SCARA Machine for Real-Time Object Detection,"In the spirit of innovation, the development of an intelligent robot system incorporating the basic principles of Industry 4.0 was one of the objectives of this study. With this aim, an experimental application of an industrial robot unit in its own isolated environment was carried out using neural networks. In this paper, we describe one possible application of deep learning in an Industry 4.0 environment for robotic units. The image datasets required for learning were generated using data synthesis. There are significant benefits to the incorporation of this technology, as old machines can be smartened and made more efficient without additional costs. As an area of application, we present the preparation of a robot unit which at the time it was originally produced and commissioned was not capable of using machine learning technology for object-detection purposes. The results for different scenarios are presented and an overview of similar research topics on neural networks is provided. A method for synthetizing datasets of any size is described in detail. Specifically, the working domain of a given robot unit, a possible solution to compatibility issues and the learning of neural networks from 3D CAD models with rendered images will be discussed.",Robotics,2022,10.3390/robotics11040069,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
c2daf8097171ba157d42d72a7c96b5681eca4d85,https://www.semanticscholar.org/paper/c2daf8097171ba157d42d72a7c96b5681eca4d85,Ensemble Machine Learning Systems for the Estimation of Steel Quality Control,"Recent advances in the steel industry have encountered challenges in soliciting decision making solutions for quality control of products based on data mining techniques. In this paper, we present a steel quality control prediction system encompassing with real-world data as well as comprehensive data analysis results. The core process is cautiously designed as a regression problem, which is then best handled by grouping various learning algorithms with their massive resource of historical production datasets. The characteristics of the currently most popular learning models used in regression problem analysis are as well investigated and compared. The performance indicates our steel quality control prediction system based on ensemble machine learning model can offer promising result whilst delivering high usability for local manufacturers to address the production problem by aid of development of machine learning techniques. Furthermore, real-world deployment of this system is demonstrated and discussed. Finally, future directions and the performance expectation are pointed out.",2018 IEEE International Conference on Big Data (Big Data),2018,10.1109/BigData.2018.8622583,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
00b8e8c4fe2fe855f1b2d1f0e7b46b91ce775438,https://www.semanticscholar.org/paper/00b8e8c4fe2fe855f1b2d1f0e7b46b91ce775438,A Feature-Aware Online Learning Approach for Support Vector Machine Classification,"Online machine learning algorithm has attracted increasing attention especially in the era of Industry 4.0. The reason is that traditional batch learning algorithm cannot deal with the streaming data produced by sensorized machines and make real-time decisions. In this paper, we propose a Feature-aware online learning approach of Support Vector Machine (FSVM) for classification problem. Usually, online learning algorithm has limited access to streaming data due to resource and computation constraints. In FSVM, we introduced a feature vector selection method to reduce the size of training dataset without losing key information and maintain an acceptable classification accuracy. Here, such small set of selected feature vectors is able to represent the original dataset. What is more, we can detect feature drifting by checking whether or not a new input data can be represented by the current feature vectors. We evaluate the performance of FSVM based on several realworld datasets. The results show that even train the SVM model with around 10% data, an acceptable misclassification rate can be reached.","2018 15th International Conference on Control, Automation, Robotics and Vision (ICARCV)",2018,10.1109/ICARCV.2018.8581175,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
8d985c7cfde1b760e685c4b09604593f2ca714be,https://www.semanticscholar.org/paper/8d985c7cfde1b760e685c4b09604593f2ca714be,Machine Learning Approach for Agriculture IoT using SVM&ANN,"The rapid growth of Internet of Things (IoT) devices in cities, homes, buildings, industries, health care, automotive and also in agricultural farms have paved the way for deployment of wide range of sensors in them. In return IoT turns out to be the major contributor of new data in any of these fields. A data driven farm management techniques will in turn help in increasing the agricultural yield by planning the input cost, reducing loss and efficient use of resources. IoT on top of increasing the volume of data it also give rise to big data with varied characteristics based on time and locality. To increase the agricultural yield by smart farm management astute analysis and processing of the data generated becomes imperative. With high performance computing at machine learning has created new opportunities for data intensive science. Machine learning will help the farm management system to achieve its goal by exploiting the data that is continuously made available with the help of Agricultural IoT (AIoT) platform and helps the farmer with insights, decisive action and support.",,2019,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
70ffc876b23d2e15794fc28564f9e920bf64db5f,https://www.semanticscholar.org/paper/70ffc876b23d2e15794fc28564f9e920bf64db5f,Orchestration of machine learning workflows on Internet of Things data,"Applications empowered by machine learning (ML) and the Internet of Things (IoT) are changing the way people live and impacting a broad range of industries. However, creating and automating ML workflows at scale using real-world IoT data often leads to complex systems integration and production issues. Examples of challenges faced during the development of these ML applications include glue code, hidden dependencies, and data pipeline jungles. This research proposes the Machine Learning Framework for IoT data (ML4IoT), which is designed to orchestrate ML workflows to perform training and enable inference by ML models on IoT data. In the proposed framework, containerized microservices are used to automate the execution of tasks specified in ML workflows, which are defined through REST APIs. To address the problem of integrating big data tools and machine learning into a unified platform, the proposed framework enables the definition and execution of end-to-end ML workflows on large volumes of IoT data. In addition, to address the challenges of running multiple ML workflows in parallel, the ML4IoT has been designed to use container-based components that provide a convenient mechanism to enable the training and deployment of numerous ML models in parallel. Finally, to address the common production issues faced during the development of ML applications, the proposed framework used microservices architecture to bring flexibility, reusability, and extensibility to the framework. Through the experiments, we demonstrated the feasibility of the (ML4IoT), which managed to train and deploy predictive ML models in two types of IoT data. The obtained results suggested that the proposed framework can manage real-world IoT data, by providing elasticity to execute 32 ML workflows in parallel, which were used to train 128 ML models simultaneously. Also, results demonstrated that in the ML4IoT, the performance of rendering online predictions is not affected when 64 ML models are deployed concurrently to infer new information using online IoT data.",,2019,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
74c937dfb5806cc5b83cf9d4fb71c6dd30a1a55d,https://www.semanticscholar.org/paper/74c937dfb5806cc5b83cf9d4fb71c6dd30a1a55d,Machine learning for human performance capture from multi-viewpoint video.,"Performance capture is used extensively within the creative industries to efficiently produce high quality, realistic character animation in movies and video games. Existing commercial systems for performance capture are limited to working within constrained environments, requiring wearable visual markers or suits, and frequently specialised imaging devices (e.g. infra-red cameras) both of which limit deployment scenarios (e.g. indoor capture). This thesis explores novel methods to relax these constraints, applying machine learning techniques to estimate human pose using regular video cameras and without the requirement of visible markers on the performer. This unlocks the potential for co-production of principal footage and performance capture data, leading to production efficiencies. For example, using an array of static witness cameras deployed on-set, performance capture data for a video games character accompanying a major movie franchise might be captured at the same time the movie is shot. The need to call the actor for a second day of shooting in a specialised motion capture (mo-cap) facility is avoided, saving time and money, since performance capture was possible without corrupting the principal movie footage with markers or constraining set design. Furthermore, if such performance capture data is available in real-time, the director may immediately pre-visualize the look and feel of the final character animation enabling tighter capture iteration and improved creative direction. This further enhances the potential for production efficiencies. 
 
The core technical contributions of this thesis are novel software algorithms that leverage machine learning to fuse of data from multiple sensors – synchronised video cameras, and in some cases, inertial measurement units (IMUs) – in order to robustly estimate human body pose over time, doing so at real-time or near real-time rates. 
 
Firstly, a hardware-accelerated capture solution is developed for acquiring coarse volumetric occupancy data from multiple viewpoint video footage, in the form of a probabilistic visual hull (PVH). Using CUDA-based GPU acceleration the PVH may be estimated in real-time, and subsequently used to train machine learning algorithms to infer human skeletal pose from PVH data. 
 
Initially a variety of machine learning approaches for skeletal joint pose estimation are explored, contrasting classical and deep inference methods. By quantizing volumetric data into a two-dimensional (2D) spherical histogram representation it is shown that convolutional neural networks (CNN) architectures used traditionally for object recognition may be re-purposed for skeletal joint estimation given suitable a training methodology and data augmentation strategy. 
 
The generalization of such architectures to a fully volumetric (3D) CNN is explored, achieving state of the art performance at human pose estimation using an volumetric auto-encoder (hour-glass) architecture that emulates networks traditionally used for de-noising and super-resolution (up-scaling) of 2D data. A framework is developed that is capable of simultaneously estimating human pose from volumetric data, whilst also up-scaling that volumetric data to enable fine-grain estimation of surface detail given a deeply learned prior from previous performance. The method is shown to generalise well even when that prior is learned across different subjects, performing different movements even in different studio camera configurations. 
 
Performance can be further improved using a learned temporal model of data, and through the fusion of complementary sensor modalities – video and IMUs – to enhance the accuracy of human pose estimation inferred from a volumetric CNN. Although IMUs have been applied in the performance capture domain for many years, they are prone to drift limiting their use to short capture sequences. The novel fusion of IMU with video data enables improved global localization and so reduced error over time whilst simultaneously mitigating the issues of limb inter-occlusion that can frustrate video-only approaches.",,2019,10.15126/THESIS.00850064,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
26937db6917952a9c13bfe152eddd7defc08e695,https://www.semanticscholar.org/paper/26937db6917952a9c13bfe152eddd7defc08e695,Guest Editorial: Special Issue on Machine Learning Implementations,,J. Signal Process. Syst.,2019,10.1007/s11265-018-1432-1,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
ae37ae4428d6aacef054f7f530abd9ee2e0034d2,https://www.semanticscholar.org/paper/ae37ae4428d6aacef054f7f530abd9ee2e0034d2,100Gb/s/λIM-DD PON using 20G-class optical devices by machine learning based equalization,"We propose the novel machine learning based equalization algorithm for IM-DD PON and extend the capacity of PON from 50Gbps/λ to 100Gbps/λ. 100Gb/s PAM8/PAM16 IM-DD transmission is achieved over 25km SSMF using 20G-class optics. Introduction Nowadays, there are many applications and market-demanding factors driving the need for higher-speed access network, such as the rapid growth of the high-definition video streaming services, the burst of smart devices of Internet of Things (IoT) and the development of wireless backhaul of 5G 1. Currently standard groups like IEEE 802.3ca and ITU-T SG15 are working on their 50Gbps/λ passive optical network (PON) industry standard, aiming at deployment in the next few years. Besides, several feasible 50Gbps/λ solutions have been proposed during the past several years. Due to the nature requirement of low cost in PON, optics with limited bandwidth, advanced modulation formats and advanced equalization algorithms are widely chosen as the main research topic of 50Gbps/λ PON. Algorithms like feed-forward equalization, maximum likelihood sequence estimation, volterra nonlinear equalization and machine learning based equalization have been investigated to overcome the limitation of channel impairment 2-4. To further increase the loss budget, some optical functions have also been introduced such as dispersion shifted fiber (DSF) and semiconductor optical amplifier (SOA) 5-7. Based on our proposed machine learning based equalization technique, we have also realized 50Gbps/λ PON with 29-dB loss budget8. While the research and standardization of 50Gbps/λ are steadily progressing, we decide to push the limits and pay more attentions to the next step 100Gbps/λ research. In the intensity modulation and direct detection (IM-DD) community, there is a solid foundation of 100Gbps transmission, and we have also made some attempts based on our proposed machine learning based equalization technique 9. In this paper, we introduce the novel machine learning based equalization for highspeed PON applications and show the experimental demonstration of 50Gb/s IM-DD PON based on 10G-class optics and 100Gb/s IM-DD PON based on 20G-class optics. 56Gb/s IM-DD PON based on Machine Learning Fig. 1: Network Structure The proposed machine learning based equalization algorithm is based on convolutional neural network (CNN), of which the structure is shown in Fig. 1. It consists of 5 layers, with 2 convolutional layers followed by 3 linear layers interleaved by rectified linear unit (ReLU) nonlinear activation layers. The input signal is a T-spaced time domain window of 101. After the processing of the network, the output signal is decreased to M units, which denotes the M-th symbol of M-PAM signal. The training process is completed with the help of mini-batch gradient descent algorithm with batch size of 128. After each batch of forward processing, the cross entropy loss signal is calculated and backpropagated to the first layer of the network, and then the network parameters are adjusted with this gradient. When training, the neurons are randomly removed to increase capability of generalization, which is called “Dropout”. The dropout probability of 0.1 is used. The dataset contains ten independent PRBS15 sequences, with 60% for training, 20% for cross validation and 20% for test. The experimental setup is shown in Fig. 2(a). At transmitter side, the 56Gbps PAM4 signal is generated by the delay-attenuate-add scheme of two 28Gbps PRBS15 NRZ signal generated by Keysight N4960A pulse pattern generator (PPG). 10G-class O-band DML and 10G-class APD with 25km SSMF transmission are tested. The output power of DML is set to 10 dBm to improve the link loss budget. Fig. 2: (a) Experimental setup and DSP flow chart and (b) channel frequency response for 56Gb/s IM/DD PON After the optical channel, the signal is digitally sampled by the DSO, then the offline digital signal processing is performed to evaluate the system performance. As shown in Fig. 2(b), the total 3-dB channel bandwidth is 6 GHz. The eye diagrams of electrical back-to-back, optical back-to-back and 25-km transmission can also be found in Fig. 2. Fig. 3: BER performance comparison for (a) optical back-toback and (b) 25km transmission Figure 3 shows the BER performance of different equalization algorithms under the condition of the experimental setup. Different configuration of FFE, volterra nonlinear equalizer are tested. With the help of our proposed machine learning based equalizer, receiver sensitivity of -19.2dBm is achieved for optical back-to-back and 25km SSMF transmission. Considering 10-dBm output of the DML, total loss budget of 29.2dB is achieved. 100Gb/s IM-DD PON based on Machine Learning We further evaluated the performance of our proposed machine learning based equalization for 100Gb/s IM-DD PON applications. The network structure is shown in Fig. 4. Fig. 4: Network Structure and Configuration Comparing to the previous network setup, one more linear layer is added to the network to increase the total network learning capability. What’s more, the output layer is adjusted to 8 or 16 based on whether PAM8 or PAM16 is transmitted. The network is trained using minibatch gradient descent with a batch size of 512, which take the advantage of the parallel computing capability of GPU and largely accelerate the training process. In addition, dropout with p=0.1 is also introduced to deal with the over-fitting problem. The dataset is composed of 10 independent PRBS15 receiving symbols, divided in a ratio of 6:2:2 for training, cross validation and testing. The experimental setup can be found in Fig. 5. The 33GBd PAM8 and 25GBd PAM16 signal are generated by a Keysight 8196A arbitrary waveform generator. Then the signal is Fig. 5: (a) Experimental Setup (b) Eye Diagram (c) Frequency Response for 100Gb/s IM-DD PON. loaded onto light by two different DML in C-band and O-band. After 25km SSMF transmission, a 20G PIN is used to perform the photoelectric conversion. Then the signal is digitized and recorded by the DSO, after which offline DSP is performed. Fig. 6: BER performance comparison of different modulation formats and optics for (a) optical back-to-back and (b) 25kmSSMF transmission Figure 6 shows the BER performance comparison of different modulation formats and optics for optical back-to-back and 25km SSMF transmission. All the transmission configuration achieves BER under 7% hard-decision forwarderror-correction (HD-FEC) limit of 3.8e-3. The BER performance is similar for both optical back-to-back and 25km SSMF transmission. The achieved receiver sensitivity range from -6.5 dBm to -5.5 dBm. What’s more, due to the higher bandwidth, the performance of O-band link outperforms C-band. As for the comparison between optical back-to-back and 25km SSMF transmission, dispersion in C-band does not seem to introduce significant penalty to the BER performance, which might be explained by the strong linear equalization capability of CNN and implies that longer transmission distance can also be supported. What’s more, even with higher baud rate, the BER performance of 33 GBd PAM8 is better than 25 GBd PAM16, which might be accounted for the higher susceptibility of PAM16 to device nonlinearity and noise. Thus, 33G PAM8 might be a feasible solution for 100Gb/s PON. In this experiment, the loss budget is only around 16 dB considering 10-dBm output power of DML. The loss budget can be improved by using APD and SOA-based pre-amplifier or advanced CNN algorithms with optimized structure, which are under our investigations. In addition, compared with CNN, recurrent neural network (RNN) is more suitable for sequence signals. We will further investigate RNN-based equalization algorithms and its-variants such as reservoir computing for improved performance and reduced complexity. Conclusions We propose the novel machine learning based equalization technique based on CNN network and test its performance under the condition of 56Gb/s PAM4 and 100Gb/s PAM8/PAM16 transmission. The algorithm shows its strength in each of the test configurations.",,2018,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
f38bf1651f0633d19be5cdc778d0862b2f72f566,https://www.semanticscholar.org/paper/f38bf1651f0633d19be5cdc778d0862b2f72f566,Developing Machine Learning Products Better and Faster at Startups,"There are many uncertainties in developing machine-learning (ML)-based products. Due to the gap between research and development, the overall progress becomes slow, and experiences many failures and learnings only to see an initial idea not working or generating no significant revenue. To minimize these drastic effects, there are continuing studies to make a balanced handshake between research and development to shorten the time span of ML-based products from idea generation to deployment. This paper demonstrates a three-phase ML product development workflow at OneClass. The workflow is a combination of multiple best-practices in the innovation-based startup industry. The first phase of the workflow considers the pivotal idea generation for products that involves data reliability assessment, idea prioritization, expectation setting, and building trust among users. The second phase concentrates on several state-of-the-art strategies for planning and future re-use of several product components. Finally, the actual research and development phase describes the fail-fast method practiced by OneClass to learn quickly from failures and act accordingly. The workflow is followed by the company to develop many sophisticated ML-based products successfully within a very short period of time.",IEEE Engineering Management Review,2018,10.1109/EMR.2018.2870669,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
e6013556f4046c73618e17439c6b07ad7bbc2277,https://www.semanticscholar.org/paper/e6013556f4046c73618e17439c6b07ad7bbc2277,Machine Learning IP Protection,"Machine learning, specifically deep learning is becoming a key technology component in application domains such as identity management, finance, automotive, and healthcare, to name a few. Proprietary machine learning models - Machine Learning IP - are developed and deployed at the network edge, end devices and in the cloud, to maximize user experience. With the proliferation of applications embedding Machine Learning IPs, machine learning models and hyper-parameters become attractive to attackers, and require protection. Major players in the semiconductor industry provide mechanisms on device to protect the IP at rest and during execution from being copied, altered, reverse engineered, and abused by attackers. In this work we explore system security architecture mechanisms and their applications to Machine Learning IP protection.",2018 IEEE/ACM International Conference on Computer-Aided Design (ICCAD),2018,10.1145/3240765.3270589,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
b0bf64ccbd651e8c7bc141d8aabaecff562e93a1,https://www.semanticscholar.org/paper/b0bf64ccbd651e8c7bc141d8aabaecff562e93a1,"AI, Machine Learning & Deep Learning Risk Management & Controls: Beyond Deep Learning and Generative Adversarial Networks: Model Risk Management in AI, Machine Learning & Deep Learning","The current paper proposes how model risk management in operationalizing machine learning for algorithm deployment can be applied in national C4I and Cyber projects such as Project Maven. It builds upon recent leadership of global Management and Leadership industry executives for AI and Machine Learning Executive Education for MIT Sloan School of Management and the MIT Computer Science and Artificial Intelligence Laboratory (CSAIL) and invited presentations at Princeton University. After building understanding about why model risk management is most crucial to robust AI, Machine Learning, Deep Learning, and, Neural Networks deployment, it introduces a Knowledge Management Framework for Model Risk Management to advance beyond ‘AI Automation’ to ‘AI Augmentation.’",,2018,10.2139/SSRN.3193693,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
a9e3fae6da524ae16fce0a30cc77a25010d1bdea,https://www.semanticscholar.org/paper/a9e3fae6da524ae16fce0a30cc77a25010d1bdea,IIoT Edge Analytics: Deploying Machine Learning at the Wellhead to Identify Rod Pump Failure,"
 Oil and Gas operators now have the possibility to collect and leverage significant amounts of data directly at the extremities of their production networks. Data combined with Industrial Internet of Things (IIoT) architecture is an opportunity to improve maintenance of assets, increase their up-time, reduce safety risks and optimize operational costs. However, to turn data into meaningful insights, Oil and Gas industry needs to fully take benefit of Machine Learning (ML) models which are able to consume real-time data and provide insights in isolated locations with scarce connectivity. These ML models need to be precise, robust and compatible with Edge computing capabilities.
 This paper presents an analytics solution for rod pumps, capable of automated Dynagraph Card recognition at the wellhead leveraging an ensemble of ML models deployed at the Edge. The proposed solution does not require Internet connectivity to generate alarms and addresses confidentiality requirements of Oil and Gas industry. An overview of the employed ML models as well as the computing and communication infrastructure is given. We believe the given outline is insightful for the petroleum industry on its road to digitization and optimization of Artificial Lift systems.","Day 2 Thu, November 29, 2018",2018,10.2118/192513-MS,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
c222ac679511c2bdf4958420b116cfe5706c4730,https://www.semanticscholar.org/paper/c222ac679511c2bdf4958420b116cfe5706c4730,Machine Learning for Assessing Real-Time Safety Conditions of Scaffolds,"Researchers have taken advantage of technological advancements to automate construction processes; as a result, significant progress has been made in designing and planning temporary structures. Despite this effort, relatively little attention has been placed on automating the monitoring of safety issues of scaffolding structures, which are one of the major elements used in the construction industry. A need has emerged for a reliable means to assess the safety conditions of scaffoldings. This paper proposes a method of integrating strain-gage sensing with a machine-learning algorithm (support vector machine) to assess the real-time safety conditions of scaffolds. Based on actual strain data of scaffolding members, which were collected using wireless sensors for various loading cases on the scaffolding structure, a support vector machine was applied to differentiate the scaffolding conditions into 'safe', 'overturning', 'uneven settlement', or 'overloading' conditions. Such an automated differentiation of the condition of a scaffold could help to determine whether or not the scaffolding is safe to use without deploying safety inspectors throughout the site. The proposed method was experimentally validated to be successful in estimating the safety condition of a scaffold with an average accuracy of 97.66% for the cases that were tested. The proposed methodology could serve as a real-time monitoring system to determine the status of scaffolding structures. Its application is expected to significantly improve reliability in assessing the safety conditions of scaffolding structures, compared to conventional safety inspections, and to resolve the related safety issues.",Proceedings of the 35th International Symposium on Automation and Robotics in Construction (ISARC),2018,10.22260/ISARC2018/0008,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
7f4a01bfa9d2370f2f7b8c939b0e8e78d8c34f5d,https://www.semanticscholar.org/paper/7f4a01bfa9d2370f2f7b8c939b0e8e78d8c34f5d,Evaluation of Apache Spot's machine learning capabilities in an SDN/NFV enabled environment,"Software Defined Networking (SDN) and Network Function Virtualisation (NFV) are transforming modern networks towards a service-oriented architecture. At the same time, the cybersecurity industry is rapidly adopting Machine Learning (ML) algorithms to improve detection and mitigation of complex attacks. Traditional intrusion detection systems perform signature-based detection, based on well-known malicious traffic patterns that signify potential attacks. The main drawback of this method is that attack patterns need to be known in advance and signatures must be preconfigured. Hence, typical systems fail to detect a zero-day attack or an attack with unknown signature. This work considers the use of machine learning for advanced anomaly detection, and specifically deploys the Apache Spot ML framework on an SDN/NFV-enabled testbed running cybersecurity services as Virtual Network Functions (VNFs). VNFs are used to capture traffic for ingestion by the ML algorithm and apply mitigation measures in case of a detected anomaly. Apache Spot utilises Latent Dirichlet Allocation to identify anomalous traffic patterns in Netflow, DNS and proxy data. The overall performance of Apache Spot is evaluated by deploying Denial of Service (Slowloris, BoNeSi) and a Data Exfiltration attack (iodine).",ARES,2018,10.1145/3230833.3233278,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
7a740e3b96cd95bb0d03561bf05e9b32b922c9ff,https://www.semanticscholar.org/paper/7a740e3b96cd95bb0d03561bf05e9b32b922c9ff,Using Machine Learning Technique for Telecom Service Providers in Malaysia to Prioritize Broadband Investment in Urban and Rural Areas,"The companies that provide telecommunication services (Telco) in Malaysia commonly use the return on investment (ROI) model to strategize their network investment plans and to deploy their broadband services in their intended markets. The numbers of subscribers and average revenue per user (ARPU) are two dominant contributions to a good ROI. The rural areas are lacking both dominant factors and thus very often fall outside the radar of the Telco’s investment plans. The government agencies, therefore, shoulder the responsibility to provide broadband services in rural areas through the implementation of national broadband initiatives and regulated policies and funding for universal service provision. In this paper, we will outline a machine-learning technique which the Telco can use to plan for broadband investments in urban areas and beyond. The proposed technique predicts the socioeconomic potential of a geographical area in correspondence to its local characteristics. This technique is an empirical model that produces a correlation coefficient to quantify the statistical relationships between two or more values of local characteristics and socioeconomic potential. The model can help Telcos to prioritize their investments in urban and rural areas with higher potential for socioeconomic growth. By using this technique as a policy tool, Telcos will be able to prioritize areas where broadband infrastructure can be implemented using a government-industry partnership approach. Both public and private parties can share the initial cost and collect future revenues appropriately as the socioeconomic correlation coefficient improves. The proposed technique functions to formulate an empirical model using a curve-fitting software and to generate sufficient data using Genetic Algorithm to train a Support Vector Machine.",,2017,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
a6de0afdf98e47e02a0aee055a5aa9bc1c23be2a,https://www.semanticscholar.org/paper/a6de0afdf98e47e02a0aee055a5aa9bc1c23be2a,Identification and Classification of Poultry Eggs: A Case Study Utilizing Computer Vision and Machine Learning,"We developed a method to identify, count, and classify chickens and eggs inside nesting boxes of a chicken coop. Utilizing an Internet of Things (IoT) AWS DeepLens Camera for data capture and inferences, we trained and deployed a custom Single Shot multibox Detector (SSD) model for object detection and classification. This allows us to monitor a complex environment with multiple chickens and eggs moving and appearing simultaneously within the video frames. The models can label video frames with classifications for eight breeds of chickens and/or four colors of eggs, with 98% accuracy on chickens or eggs alone and 82.5% accuracy while detecting both types of objects. With the ability to directly infer and store classifications on the camera, this setup can work in a low/no internet bandwidth setting in addition to an internet connected environment. Having these classifications benefits farmers by providing the necessary base data required for accurately measuring the individual egg production of every chicken in the flock. Additionally, this data supports comparative analysis between individual flocks and industry benchmarks.",,2019,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
31822a047b95739f17cc569cded8b488aeb653ea,https://www.semanticscholar.org/paper/31822a047b95739f17cc569cded8b488aeb653ea,Using behavioral analytics and machine learning to improve churn management,"New trends are shaping the telecommunications, media and technology (TMT) industries. Consumers are demanding to be connected anytime to hundreds of thousands of applications that are one click away. In addition, loyalty levels are decreasing and customers do not hesitate to switch providers if they do not receive value for their money. Because of this, churn management is a key driver of profits. However, few companies excel at churn management and most underestimate its impact. The thesis is focused on describing a technological solution targeted to improve churn management capabilities within companies that belong to the TMT sector and explore the opportunities and hurdles of selling this kind of solution in a B2B context. The hypothesis is that a world class churn management solution can effectively deploy statistical models to score customers by their likelihood to churn and execute targeted treatments for each segment through the operator service channels. The study will focus on how behavioral analytics and machine learning can increase customer's life time value and boost margins in TMT companies. Throughout the research, I will describe the best practices within the industry to establish a state of the art churn management solution. Thesis Supervisor: Duncan Simester Title: NTU Professor of Marketing",,2017,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
4157ed3db4c656854e69931cb6089b64b08784b9,https://www.semanticscholar.org/paper/4157ed3db4c656854e69931cb6089b64b08784b9,DaDianNao: A Machine-Learning Supercomputer,"Many companies are deploying services, either for consumers or industry, which are largely based on machine-learning algorithms for sophisticated processing of large amounts of data. The state-of-the-art and most popular such machine-learning algorithms are Convolutional and Deep Neural Networks (CNNs and DNNs), which are known to be both computationally and memory intensive. A number of neural network accelerators have been recently proposed which can offer high computational capacity/area ratio, but which remain hampered by memory accesses. However, unlike the memory wall faced by processors on general-purpose workloads, the CNNs and DNNs memory footprint, while large, is not beyond the capability of the on chip storage of a multi-chip system. This property, combined with the CNN/DNN algorithmic characteristics, can lead to high internal bandwidth and low external communications, which can in turn enable high-degree parallelism at a reasonable area cost. In this article, we introduce a custom multi-chip machine-learning architecture along those lines. We show that, on a subset of the largest known neural network layers, it is possible to achieve a speedup of 450.65x over a GPU, and reduce the energy by 150.31x on average for a 64-chip system. We implement the node down to the place and route at 28nm, containing a combination of custom storage and computational units, with industry-grade interconnects.",2014 47th Annual IEEE/ACM International Symposium on Microarchitecture,2014,10.1109/MICRO.2014.58,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
98d01fc70ff577acc2c8da839a17dc0c47f047a2,https://www.semanticscholar.org/paper/98d01fc70ff577acc2c8da839a17dc0c47f047a2,Experiences and lessons in developing industry-strength machine learning and data mining software,"Traditionally academic machine learning and data mining researchers focus on proposing new algorithms. The task of implementing these methods is often left to companies that are developing software packages. However, the gap between the two sides has caused some problems. First, the practical deployment of new algorithms still involves some challenging issues that need to be studied by researchers. Second, without further investigation after publishing their papers, researchers have neither the opportunity to work with real problems nor see how their methods are used. We discuss the experiences in developing two machine learning packages LIBSVM and LIBLINEAR, that are widely used in both academia and industry. We demonstrate that the interaction with users leads us to identify some important research problems. For example, the decision to study and then support multi-class SVM was essential in the early stage of developing LIBSVM. The birth of LIBLINEAR was driven by the need to classify large-scale documents in Internet companies. For fast training of large-scale problems, we had to create new algorithms other than those used in LIBSVM for kernel SVM. We present some practical use of LIBLINEAR for Internet applications. Finally, we give lessons learned and future perspectives for developing industry-strength machine learning and data mining software.",KDD,2012,10.1145/2339530.2339714,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
199a5314a931db42ae4c4a52fcf191e407374784,https://www.semanticscholar.org/paper/199a5314a931db42ae4c4a52fcf191e407374784,Relevant framework for social applications of internet of things by means of machine learning techniques,"Los desarrollos recientes de las tecnologias inalambricas y el despliegue extendido de dispositivos distribuidos espacialmente con capacidades de identificacion, deteccion y actuacion integradas crearon la Internet de las cosas (IoT). Este paradigma prometedor se ha desarrollado de forma explosiva en los ultimos anos. Se cree que es la proxima tecnologia revolucionaria al llevar el Internet tradicional al ambito fisico. Miles de millones de cosas inteligentes interconectadas estan transmitiendo una gran cantidad de datos a cada momento y promocionando el mundo en la era del ”big data”. El valor potencial inimaginable se puede extraer de estos datos respaldados por tecnologias avanzadas como aprendizaje automatico y la computacion en la nube. Con la ayuda de herramientas avanzadas de mineria de datos, IoT puede aportar grandes beneficios para varios dominios de la sociedad, incluida la atencion de salud. La industria de la salud ha cambiado drasticamente debido a la revolucion de la tecnologia de la informacion que comenzo en el siglo pasado. Las tecnologias nuevas, como la telemedicina, el hospital digital y la atencion sanitaria electronica, se han aplicado ampliamente durante las ultimas decadas y ahora el rapido desarrollo de IoT y el aprendizaje automatico promueven la atencion de salud de digital a inteligente. Como un aspecto importante de IoT, la tecnologia portatil tambien ha mostrado un aumento rapido en la ultima decada. Se han introducido en el mercado diferentes tipos de dispositivos portatiles que contienen varios sensores integrados con precios asequibles. Estos dispositivos portatiles generan grandes cantidades de datos relacionados con la salud durante diferentes actividades diarias. Estos datos de bajo costo, respaldados por tecnicas de computacion movil y aprendizaje automatico, hacen posible el desarrollo de sistemas de soporte de decisiones inteligentes (SDSS) que pueden ser beneficiosos para el monitoreo de actividades a largo plazo, el diagnostico remoto de enfermedades y la promocion de alertas medicas de emergencia. Como una de las herramientas mas importantes para realizar la Inteligencia Artificial (IA), el aprendizaje automatico ha crecido de manera explosiva en las ultimas decadas con el desarrollo de Internet. Varias de las tecnicas de aprendizaje automatico se han utilizado ampliamente para implementar diferentes tareas de mineria de datos, entre las cuales, el aprendizaje profundo ha mostrado un rendimiento sobresaliente en los ultimos anos debido a la disponibilidad de “big data”. Nuestra investigacion tiene como objetivo abordar las aplicaciones de la tecnologia IoT respal dada por tecnicas avanzadas de aprendizaje automatico en diferentes areas sociales, especialmente en la atencion de salud. Se construyo un marco de aplicacion general, que incluye la recopilacion y transferencia de datos, el almacenamiento y el analisis de datos, y La entrega de resultados de analisis a los usuarios. Con el fin de verificar la viabilidad del marco de aplicacion propuesto, se desarrollo un sistema practico de recoleccion de datos de movimiento humano basado en tecnologia portatil. Incluye tres modulos: un reloj inteligente, un telefono inteligente y una NoSQL base de datos remota. El sistema se aplico en un hospital para recopilar datos de actividad diaria y temblor de pacientes con Temblor Esencial (ET). Se adoptaron tecnicas avanzadas de aprendizaje automatico, incluido el aprendizaje profundo, para realizar tareas de Reconocimiento de Actividad Humana (HAR) y evaluacion de ET. A traves del procesamiento adecuado y la transformacion de los datos, los modelos propuestos podrian reconocer una serie de actividades diarias humanas y clasificar los niveles de temblor con una alta precision. Estos modelos podrian permitir a los neurologos monitorear de forma remota y continua las actividades diarias de los pacientes con ET y la correspondiente situacion de temblor. El resultado de la evaluacion podria ayudarles a mejorar los planes de tratamiento. Este caso demostro la viabilidad del marco de aplicacion IoT presentado y otras aplicaciones similares podrian desarrollarse en otros escenarios. Como una de las futuras direcciones de investigacion, al final del estudio se propuso un sistema de intercambio de datos personales basado en la tecnologia blockchain. El objetivo es proteger la privacidad y la seguridad durante el proceso de intercambio de datos. ----------ABSTRACT---------- The recent adaptation of wireless technologies and the widespread deployment of spatially distributed devices with embedded identification, sensing and actuation capabilities created the Internet of Things (IoT). This promising paradigm has been developing explosively in recent years. It is believed to be the next revolutionary technology by bringing the traditional sense of Internet into the physical realm. Billions of inter-connected smart things are streaming out a huge amount of data every moment and promote the world into “big data” era. Unimaginable potential value can be mined from these data supported by advanced data storage and analysis technologies, such as machine learning and cloud computing. With the help of advanced data mining tools, IoT can bring great benefits for various domains of the society including healthcare. The healthcare industry has been dramatically changed because of the information technology revolution that started in the last century. New technologies such as telemedicine, digital hospital and e-health have been widely applied during the past decades and now the rapidly development of IoT and machine learning is promoting healthcare from digital into intelligent. As an important aspect of IoT, wearable technology has also shown a surge in the past decade. Different types of wearable devices containing various embedded sensors have been introduced into the market with affordable prices. Large amounts of health-related data are generated by these wearable devices related to different daily activities. These low-cost data, supported by mobile computing and machine learning techniques, make it possible to develop Smart Decision Support Systems (SDSS) which can be beneficial to the long-term activity monitoring, remote disease diagnosis, emergency medical alerts promoting and so on. As one of the most important tools to realize Artificial Intelligence (AI), machine learning has been growing explosively in the past decades with the development of Internet. Various of machine learning techniques have been widely used to implement different data mining tasks, among which, deep learning has shown outstanding performance in recent years due to the availability of big data. Our research aims to address the applications of IoT technology supported by advanced machine learning techniques in different social areas, especially in healthcare. A general application framework was constructed, which includes data collecting and transferring, data storage and analysis, and analysis result sharing. In order to verify the feasibility of proposed application framework, a practical human movement data collecting system was developed based on wearable technology. I t includes three modules: a smart watch, a smartphone and a remote NoSQL database. The system was applied in a hospital to collect daily activity and tremor data from patients with Essential Tremor (ET). Advanced machine learning techniques, including deep learning, were adopted to realize Human Activity Recognition (HAR) and ET evaluation tasks. Through proper data preprocessing and data transformation, based on the collected acceleration data, the proposed models could recognize a series of human daily activities and classify tremor levels with high accuracy. These models could enable neurologists remotely and continuously monitor ET patients’ daily activities and the corresponding tremor situation. The evaluation result could help them to improve the treatment plans. This case proved the feasibility of the presented IoT application framework and similar applications could be developed in other scenarios. As one of the future research directions, a personal health data sharing system based on blockchain technology was proposed in the end of the study. The aim is to protect the privacy and security during the data sharing process.",,2019,10.20868/upm.thesis.55328,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
38fb00e49a34a15f692add91e889a51a3b994904,https://www.semanticscholar.org/paper/38fb00e49a34a15f692add91e889a51a3b994904,Predicting Hotel Bookings Cancellation with a Machine Learning Classification Model,"Booking cancellations have significant impact on demand-management decisions in the hospitality industry. To mitigate the effect of cancellations, hotels implement rigid cancellation policies and overbooking tactics, which in turn can have a negative impact on revenue and on the hotel reputation. To reduce this impact, a machine learning based system prototype was developed. It makes use of the hotel’s Property Management Systems data and trains a classification model every day to predict which bookings are “likely to cancel” and with that calculate net demand. This prototype, deployed in a production environment in two hotels, by enforcing A/B testing, also enables the measurement of the impact of actions taken to act upon bookings predicted as “likely to cancel”. Results indicate good prototype performance and provide important indications for research progress whilst evidencing that bookings contacted by hotels cancel less than bookings not contacted.",2017 16th IEEE International Conference on Machine Learning and Applications (ICMLA),2017,10.1109/ICMLA.2017.00-11,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
f257974e9fac9ad922611d99030b8f469e6c7224,https://www.semanticscholar.org/paper/f257974e9fac9ad922611d99030b8f469e6c7224,Mesh Learning: A Cloud and Edge–based Computing Network Providing Data–Driven Solutions to the Oil and Gas Industry,"
 Over the past decade, the application of machine learning has become one of the most studied topics in the oil and gas industry. The recent success of some small-scale pilot projects has resulted in more attention being paid to deploying machine learning models at scale for operations. While cloud-based deployment (data backhauled for analysis in the cloud) and edge-based deployment (data being analyzed locally at a remote site without persistent Internet connection) are the two major approaches, each strategy has specific advantages and drawbacks. In this paper, we proposed ""mesh learning"", a new approach that optimally balances cloud computing and edge computing for deploying machine learning solutions in an industrial internet of things (IIoT) environment. We present how the system enables some new strategies to manage machine learning model lifecycles and how it helps with relevant use cases for the oil and gas industry.",,2020,10.4043/30365-ms,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
30ec74fdf06e16706d46a55a65b27f4cc238d8ae,https://www.semanticscholar.org/paper/30ec74fdf06e16706d46a55a65b27f4cc238d8ae,Dynamic detection of mobile malware using real-life data and machine learning,"Mobile malwares are malicious programs that target mobile devices, which are an increasing problem. This is reflected by the rise of detected mobile malware samples per year. 
Additionally, the number of active smartphone users is expected to grow, stressing the importance of research on the detection of mobile malware. 
Detection methods for mobile malware exists, although methods are still limited and incomprehensive. 
In this paper, we propose detection methods that use device information such as the CPU usage, battery usage, and memory usage for the detection of 10 subtypes of Mobile Trojans. The focus of this paper is the Android Operating System (OS) as it is dominating the mobile device industry with an 80 per cent market share. 
This research uses a dataset containing device and malware data of 47 users for an entire year (2016) to create multiple mobile malware detection methods. By using real-life data this research provides a realistic assessment of its detection methods. Additionally, using this dataset we examine which features, i.e. aspects, of a device, are most important in detecting (subtypes of) Mobile Trojans. The performance of the following machine learning classifiers are assessed: Random Forest, K-Nearest neighbour, Naive Bayes, Multilayer perceptron, and AdaBoost. All classifiers are assessed using a 4-fold cross-validation with holdout method. Additionally, the hyperparameters of all classifiers are tuned with the use of a GridSearch. Furthermore, we assess performances of classifiers when one model is trained for all subtypes of Mobile Trojans, and when separate models are trained for each subtype of Mobile Trojans. 
Our results show that the Random Forest classifier is most suited for the detection of Mobile Trojans. The Random Forest classifier achieves an f1 score of 0,73 with an False Positve Rate (FPR) of 0.009 and False Negative Rate (FNR) of 0.380 when one model is created to detect all 10 subtypes of Mobile Trojans. Furthermore, our research shows that the Random Forest, K-nearest neighbour classifier, and AdaBoost classifiers achieve, on average, an f1 score > 0.72, an FPR of <0.02 and an FNR <0.33, when models are created separately for each subtype of Mobile Trojans. 
Moreover, we examine the usability of the different detection methods. By assessing multiple metrics such as the model size and training times, we analyse whether the methods can be deployed locally on devices. Lastly, we examine the cost and benefits, for businesses, associated with deploying self-made detection methods.",,2018,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
e1ee319f2a107cd9df6387879c1b7d3b27df47b8,https://www.semanticscholar.org/paper/e1ee319f2a107cd9df6387879c1b7d3b27df47b8,Machine Learning and Sepsis: On the Road to Revolution.,"1946 www.ccmjournal.org November 2017 • Volume 45 • Number 11 With each passing day, the drumbeat of the advancing Big Data and machine learning revolution grows louder (1). Already having conquered previously intractable tasks like interpreting complex images or voices, driving autonomous vehicles, and winning abstract strategy games, computationally driven advances are producing disruptions across a wide swath of industries. These advances have arisen from a confluence of three major trends: rapid expansions in the availability of large-scale data, exponential increases in the accessibility of computational power, and critical progress in machine learning methods. Naturally, many see healthcare as the next major industry ripe for innovation. And, as the National Academy of Medicine has highlighted in numerous landmark reports, advances in the use of data and information technology will be critically important for addressing systemic failures in U.S. healthcare quality, safety, and value (2). In particular, recent successes in clinical image recognition and ongoing advances in leveraging largescale omics data highlight the great promise of machine learning applications in the biomedical and healthcare enterprise (3, 4). In this issue of Critical Care Medicine, Churpek et al (5) further expand on the promise of machine learning approaches in sepsis: the single most common, costly, and deadly cause of U.S. hospitalization. Extending their prior work in developing and validating the electronic Cardiac Arrest Risk Triage (eCART) score among hospitalized ward patients—the score incorporates granular vital signs, laboratory values, and patient demographics extracted from electronic medical records data (6)—they demonstrate that eCART scores uniformly provided the highest discrimination among inpatients with varying degrees of “suspected infection” for predicting a composite adverse outcome of ICU transfer or death within 48 hours. As is critical for innovation in this domain, their work capitalized on a large and highly granular dataset including greater than 53,000 infected patients as well as access to the computational resources needed to develop and deploy a random forest-based machine learning approach to score development and determination. Yet, even while the drumbeat of the machine learning revolution portends great promise, it is also accompanied by potential pitfalls (7, 8). In this case, the discrimination of the eCART score was uniformly better than all other risk scores (median C-statistic, 0.73); however, it was only modestly better than the seven-element, vital sign–based National Early Warning Score (NEWS) (median C-statistic, 0.71). Thus, although the eCART was statistically superior to the NEWS, it remains unclear what the clinical impact of this advantage would be particularly since the eCART score also demonstrated modestly lower specificity in some scenarios. Additionally, the comparisons of the eCART versus other scores were repeated in a sample drawn from the same setting in which the score was derived. Whether eCART shows better performance in external validation cohorts remains unclear. Beyond the statistical development of early warning scores, another major challenge remains identifying the appropriate interventions that should follow a high-risk alert. In a randomized trial deploying a real-time early warning score with excellent discrimination, Kollef et al (9) found that the transmitted alerts in the intervention arm neither decreased mortality nor increased the rate of ICU transfers. The alerts did result in a shorter overall hospital length of stay. Given the impressive performance characteristics of the eCART score, we look forward to a comprehensive evaluation of its impact on clinical outcomes and, in particular, a detailed description of the efferent intervention arm. This will greatly improve our understanding of which elements of implementation are most likely to drive improved outcomes. Finally, Churpek et al (5) found that eCART performance remained consistent even as the inclusion cohort criteria varied from minimal evidence of suspected infection (i.e., any culture ordered) to considerably stronger evidence (i.e., blood culture orders and IV antibiotics for at least 4 of 7 living d). Their findings confirm that the predictors conferring prognostic risk (i.e., Copyright © 2017 by the Society of Critical Care Medicine and Wolters Kluwer Health, Inc. All Rights Reserved. DOI: 10.1097/CCM.0000000000002673 *See also p. 1805.",Critical care medicine,2017,10.1097/CCM.0000000000002673,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
15c8125c49001952d25af8d7bcb74ac992affa92,https://www.semanticscholar.org/paper/15c8125c49001952d25af8d7bcb74ac992affa92,Predictive Model for Multiclass Classification of E-Commerce Data: An Azure Machine Learning Approach,"Electronics Commerce (E-Com) is one among the various business methodologies that addresses the growing requirement of business organizations, and customers. The Ecommerce industry is one of the world's leading and growing industries with market worth around $22.1 trillion globally. Through E-Com, companies are developing the competence in business domain. The business giants like Amazon, Flipkart, etc. utilizing Machine Learning (ML) potential to make matchless competitiveness in the market through data analytics and business intelligence. ML has empowered businesses by analyze the data collected through various sources including social media reviews. Data scientists are in huge demand in E-Commerce market researches because predictive data analytics based on ML can enhance sale prospects and discover the reasons of customer churn, by analyzing customer’s reviews and click-through actions, preferences and past purchase history, in real-time. Massive increase in the volume, variety and velocity of data generated through various businesses or E-Commerce platforms pose a huge computational and storage challenge for data analysis and intelligence tasks. Addressing the computational and storage needs for business intelligence tasks, cloud computing paradigm is evolved. The data and computation can be distributed to any Cloud computing environment with minimal effort nowadays. Also, Cloud computing paradigm turned out to be valuable alternatives to speed-up machine learning platforms. The work, first discusses the ‘E-Commerce advantages’, ‘Importance of Machine Learning in E-Commerce Domain’, ‘Cloud Computing and Need of Cloud platforms for Machine Learning tasks’. Also, the background for ‘E-Commerce Product Data Classification Task’ ‘is established. Introduction to multiclass classification and the literature survey for various classification tasks is presented. Finally, a Predictive Model for E-commerce Data Classification Task is proposed and deployed over Microsoft Azure Cloud. The proposed model predicts the Product Class from a large product dataset released by a well-known e-commerce company for a competition. The proposed model is build using ‘Neural Network’ (Multiclass) and R-Script module for better convergence. The obtained results are compared with benchmark model “Multiclass Logistic Regression” and evaluation is done on basis of prediction accuracy. Proposed work also demonstrates the use of one of the foremost cloud environments for machine learning. The results attained by the proposed model are promising and the paper also mentioned the future research work in the field.",,2017,10.5120/IJCA2017914447,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
5994ea254d8e60e9b240cf0c3824b3e5f40df85a,https://www.semanticscholar.org/paper/5994ea254d8e60e9b240cf0c3824b3e5f40df85a,Machine Learning for Indoor Localization Using Mobile Phone-Based Sensors,"In this paper we investigate the problem of localizing a mobile device based on readings from its embedded sensors utilizing machine learning methodologies. We consider a realworld environment, collect a large dataset of 3110 datapoints, and examine the performance of a substantial number of machine learning algorithms in localizing a mobile device. We have found algorithms that give a mean error as accurate as 0.76 meters, outperforming other indoor localization systems reported in the literature. We also propose a hybrid instance-based approach that results in a speed increase by a factor of ten with no loss of accuracy in a live deployment over standard instance-based methods, allowing for fast and accurate localization. Further, we determine how smaller datasets collected with less density affect accuracy of localization, important for use in real-world environments. Finally, we demonstrate that these approaches are appropriate for real-world deployment by evaluating their performance in an online, in-motion experiment.",ArXiv,2015,10.1109/CCNC.2016.7444919,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
fbbee5da12c0e0f15813254b9ffca18e339d3546,https://www.semanticscholar.org/paper/fbbee5da12c0e0f15813254b9ffca18e339d3546,10 - Machine learning for future intelligent air quality networks,"During the last few years, machine learning emerged as a very effective tool for data analysis and sematic value extraction from the large amount of data generated from deployed chemical multisensors devices. Many works have now highlighted the potential impact on multisensor device calibration, drift counteraction, data assimilation, optimal deployment of these classes of algorithms. Unlike 5 years ago, the huge amount of available data make possible to confirm this potential on realworld long-term deployments. This work analyze the literature produced by EuNetAir partners extracting the lessons cooperatively learnt about their impact and propose a novel architecture for future intelligent air quality networks based on the machine learning emerging paradigm.",,2016,10.5162/6EuNetAir2016/10,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
5a1a8eb1048a1eec7f3770578dad7babe5e3a2ad,https://www.semanticscholar.org/paper/5a1a8eb1048a1eec7f3770578dad7babe5e3a2ad,Proactive Maintenance Model Using Reinforcement Learning Algorithm in Rubber Industry,"This paper presents an investigation into the enhancement of availability of a curing machine deployed in the rubber industry, located in Tamilnadu in India. Machine maintenance is a major task in the rubber industry, due to the demand for product. Critical component identification in curing machines is necessary to prevent rapid failure followed by subsequent repairs that extend curing machine downtime. A reward in the Reinforcement Learning Algorithm (RLA) prevents frequent downtime by improving the availability of the curing machine at time when unscheduled long-term maintenance would interfere with operation, due to the occurrence of unspecified failure to a critical component. Over time, depreciation and degradation of components in a machine are unavoidable, as is shown in the present investigation through intelligent assessment of the lifespan of components. So far, no effective methodology has been implemented in a real-time maintenance environment. RLAs seem to be a more effective application when it is based on intelligent assessment, which encompasses the failure and repair rate used to calculate availability in an automated environment. Training of RLAs is performed to evaluate overall equipment efficiency (OEE) in terms of availability. The availability of a curing machine in the form of state probability is modeled in first-order differential-difference equations. RLAs maximize the rate of availability of the machine. Preventive maintenance (PM) rate for four modules of 16 curing machines is expressed in a transition diagram, using transition rate. Transition rate indicates the degree of PM and unplanned maintenance rates that defines the total availability of the four modules. OEE is expressed in terms of the availability of curing machines, which is related to performance and quality. The results obtained by RLA are promising regarding short-term and long-term efficiencies of OEE, which are 95.19% and 83.37%, respectively.",Processes,2022,10.3390/pr10020371,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
d61e36f1c572fabecee46340bc348a57db8e6777,https://www.semanticscholar.org/paper/d61e36f1c572fabecee46340bc348a57db8e6777,Discussion on the Flexible Manufacturing and Operation Strategy Model for a Machine Tool Component Company in Taiwan,"Recently, many manufacturing industries have been facing challenges such as rising material costs, small-volume and large-variety products, shortened production cycles, increased labor costs and longer after-sales service times, which is a very tough challenge for most small and medium-sized component manufacturing suppliers. In addition to the current hot topics in the manufacturing industry - Smart Manufacturing (Industry 4.0) and lean production management, if small and medium-sized enterprises are not able to adjust the pace of manufacturing timely and find a suitable production model, they will soon be overwhelmed by the torrent of the era of speed and accuracy. In the face of the dramatic changes in the industry structure, the company can deploy the global expansion of overseas customers in advance, and adjust to apply and implement a flexible manufacturing model system through the introduction of the Industrial Internet of Things and flexible manufacturing production management. In order to meet the market needs, the manufacturing industry is gradually oriented towards customized production and the rapid development of new products. To meet such stringent requirements, flexible manufacturing becomes one of the necessary ways for enterprises to consider their development models. Therefore, the efficiency and reliability of work can be improved through the Industrial Internet of Things that facilitates machine-to-machine communication, cloud-based big data and learning and imitations of smart robots. This study is an in-depth study of a company that is currently in the process of digital transformation, collecting relevant information and reviewing the analysis to find a suitable smart manufacturing solution for the company and to explore the impact of the COVID-19 pandemic on the strategic development of the company. The findings can provide a significant reference for homotypic companies in the development of their business strategies.",International Business Research,2021,10.5539/ibr.v14n10p1,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
b61289f39883e01d20f0051a931e4a3655666887,https://www.semanticscholar.org/paper/b61289f39883e01d20f0051a931e4a3655666887,WrapperFL: A Model Agnostic Plug-in for Industrial Federated Learning,"Federated learning, as a privacy-preserving collaborative machine learning paradigm, has been gaining more and more attention in the industry. With the huge rise in demand, there have been many federated learning platforms that allow federated participants to set up and build a federated model from scratch. However, exiting platforms are highly intrusive, complicated, and hard to integrate with built machine learning models. For many realworld businesses that already have mature serving models, existing federated learning platforms have high entry barriers and development costs. This paper presents a simple yet practical federated learning plug-in inspired by ensemble learning, dubbed WrapperFL, allowing participants to build/join a federated system with existing models at minimal costs. The WrapperFL works in a plug-and-play way by simply attaching to the input and output interfaces of an existing model, without the need of re-development, significantly reducing the overhead of manpower and resources. We verify our proposed method on diverse tasks under heterogeneous data distributions and heterogeneous models. The experimental results demonstrate that WrapperFL can be successfully applied to a wide range of applications under practical settings and improves the local model with federated learning at a low cost.",ArXiv,2022,10.48550/arXiv.2206.10407,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
dfbf5f757de62732595b67f73dee5989a00a5c25,https://www.semanticscholar.org/paper/dfbf5f757de62732595b67f73dee5989a00a5c25,How to deploy AI software to self driving cars,"The automotive industry is embracing new challenges to deliver self-driving cars, and this in turn requires increasingly complex hardware and software. Software developers are leveraging artificial intelligence, and in particular machine learning, to deliver the capabilities required for an autonomous vehicle to operate. This has driven automotive systems to become increasingly heterogeneous offering multi-core processors and custom co-processors capable of performing the intense algorithms required for artificial intelligence and machine learning. These new processors can be used to vastly speed up common operations used in AI (Artificial Intelligence) and machine learning algorithms. The R-Car V3H system-on-chip (SoC) from the Renesas AutonomyâĎć platform for ADAS (Advanced Driver Assistance Systems) and automated driving supports Level 3 and above (as defined by SAE's automation level definitions). It follows the heterogeneous IP concept of the Renesas Autonomy platformâĎć, giving the developer the choice of high performance computer vision at low power consumption, as well as flexibility to implement the latest algorithms such as those used in machine learning. By examining the architecture of the R-Car hardware we can understand how this differs from HPC and desktop heterogeneous systems, and how this can be mapped to the SYCL and OpenCL programming models. When both power consumption and performance are important, as is the case in the automotive industry, the focus for implementing OpenCL and SYCL on these hardware platforms must be a balanced approach. The memory capacity and layout must be used in the most optimum way to build a pipeline that provides the best throughput. The R-Car hardware provides DMA and on-chip memory where these are used to facilitate efficient data transfer on the device. The memory hierarchy layers can be seen on how it is efficiently mapped to OpenCL paradigm. The R-Car hardware also offers many fixed function IP blocks, each performing a specific function like convolution for deep neural networks, optical flow and more, beyond the programmable processor. The flexibility of OpenCL enables the development of built in kernels so that developers can take advantage of these architecture designs. The OpenCL model enables extensive usage of the heterogenous hardware, including fully programmable IP, efficient data transfer using the DMA to the on-chip device memory via OpenCL extension, and fixed function IP block, such as CNN for enabling high throughput convolution operations, via OpenCL builtin kernels, device triggered DMA using partial subgroups. We examine the memory mapping to bring in efficiency and the software pipelining & parallelism. These hardware architectures include AI accelerator processors specifically designed to be used in the next generation of vehicles. In particular, the processors are designed to tackle complex algorithms whilst limiting the overall consumption of power. Benchmarks will be presented to show how portable code can also deliver performance for developers using this hardware. As well as enabling developers to choose OpenCL or SYCL, we will talk about how these standards enable additional high-level frameworks that can be used to target this hardware. These include libraries for deep neural networks and linear algebra operations.",IWOCL,2019,10.1145/3318170.3318184,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
67fb80473d022e8d702990c55dd453984eb13a9d,https://www.semanticscholar.org/paper/67fb80473d022e8d702990c55dd453984eb13a9d,"Cognitive-Neuromorphic Computing for Anticipatory Risk Analytics in Intelligence, Surveillance & Reconnaissance (ISR): Model Risk Management in Artificial Intelligence & Machine Learning (Presentation Slides)","Drawing upon insights shared in the MIT: AI & Machine Learning: Management and Leadership learning community of practice, the current Intelligence, Surveillance, Reconnaissance (ISR) presentation advances the focus on ""collective intelligence of people and computers"" in the context of Cognitive Computing for Anticipatory Risk Analytics in Intelligence, Surveillance, & Reconnaissance (ISR). It defines as well as distinguishes multi-level Cognitive Computing process engineering frameworks of Artificial Intelligence (AI) & Machine Learning as applied in KM practices of US and worldwide firms, governments, and ISR agencies from Cognitive-Neuromorphic Chips. A recent IEEE Spectrum report also published as ""The Neuromorphic Chip's Make-or-Break Moment"" observes that ""Neuromorphic Chips Are Destined for Deep Learning—or Obscurity"" given that the Neuromorphic Chip researchers ""have hitched their wagon to deep learning's star."" Drawing upon insights on Model Risk Management and Anticipatory Risk Analytics focus of top Wall Street investment banks and hedge funds beyond the Global Financial Crisis currently guiding national and global Cyber Risk Insurance industry practices, we demonstrate how Model Risk Management (MRM) and Anticipatory Risk Analytics are even more critical in the global and national domains of Intelligence, Surveillance, Reconnaissance (ISR). The first operational use of AI and Deep Learning AI technologies in the Defense Intelligence Enterprise led by Project Maven and Algorithmic Warfare Cross-Functional Team is used as a case study for illustrating how Anticipatory Risk Analytics & MRM assume even greater significance at the intersections of Space and Cyberspace wherein Offensive and Defensive Cybersecurity strategies, such as discussed in the recent 2015 and 2016 presentations at the Princeton Quant Trading Conference, are deployed.",,2018,10.2139/ssrn.3111837,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
a35154c628c9d20ccbb1c45807d9fb5ef21cb91f,https://www.semanticscholar.org/paper/a35154c628c9d20ccbb1c45807d9fb5ef21cb91f,Adaptive Scheme for Caching YouTube Content in a Cellular Network: Machine Learning Approach,"Content caching at base stations is a promising solution to address the large demands for mobile data services over cellular networks. Content caching is a challenging problem as it requires predicting the future popularity of the content and the operating characteristics of the cellular networks. In this paper, we focus on constructing an algorithm that improves the users’ quality of experience (QoE) and reduces network traffic. The algorithm accounts for users’ behavior and properties of the cellular network (e.g. cache size, bandwidth, and load). The constructed content and network aware adaptive caching scheme uses an extreme-learning machine neural network to estimate the popularity of content, and mixed-integer linear programming to compute where to place the content and select the physical cache sizes in the network. The proposed caching scheme simultaneously performs efficient cache deployment and content caching. Additionally, a simultaneous perturbation stochastic approximation method is developed to reduce the number of neurons in the extreme-learning machine method while ensuring a sufficient predictive performance is maintained. Using real-world data from YouTube and a NS-3 simulator, we demonstrate how the caching scheme improves the QoE of users and network performance compared with industry standard caching schemes.",IEEE Access,2017,10.1109/ACCESS.2017.2678990,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
c6bde5c3b6b5bd9cc4c9de2be3a3c4ed48fe4e86,https://www.semanticscholar.org/paper/c6bde5c3b6b5bd9cc4c9de2be3a3c4ed48fe4e86,"Tambe , Developing the Science and Applications of Security Games : Machine Learning , Uncertainty and Preference Elicitation in Game Theory for Security","Having successfully founded the research area of security games, which has led to real-world applications in scheduling the deployment of limited resources (patrols, checkpoints, inspections, etc.), we now provide fundamental advances by incorporating machine learning to enhance realworld security applications, new models of opportunistic security games, robust methods for handling uncertainty, and novel techniques for preference elicitation techniques.",,2015,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
eb01e82c9184e6bfcf9474c955ce6eba2a605d3c,https://www.semanticscholar.org/paper/eb01e82c9184e6bfcf9474c955ce6eba2a605d3c,Stochastic Optimization and Machine Learning Modeling for Wireless Networking,"In the last years, the telecommunications industry has seen an increasing interest in the development of advanced solutions that enable communicating nodes to exchange large amounts of data. Indeed, well-known applications such as VoIP, audio streaming, video on demand, real-time surveillance systems, safety vehicular requirements, and remote computing have increased the demand for the efficient generation, utilization, management and communication of larger and larger data quantities. New transmission technologies have been developed to permit more efficient and faster data exchanges, including multiple input multiple output architectures or software defined networking: as an example, the next generation of mobile communication, known as 5G, is expected to provide data rates of tens of megabits per second for tens of thousands of users and only 1 ms latency. In order to achieve such demanding performance, these systems need to effectively model the considerable level of uncertainty related to fading transmission channels, interference, or the presence of noise in the data. 
In this thesis, we will present how different approaches can be adopted to model these kinds of scenarios, focusing on wireless networking applications. In particular, the first part of this work will show how stochastic optimization models can be exploited to design energy management policies for wireless sensor networks. Traditionally, transmission policies are designed to reduce the total amount of energy drawn from the batteries of the devices; here, we consider energy harvesting wireless sensor networks, in which each device is able to scavenge energy from the environment and charge its battery with it. In this case, the goal of the optimal transmission policies is to efficiently manage the energy harvested from the environment, avoiding both energy outage (i.e., no residual energy in a battery) and energy overflow (i.e., the impossibility to store scavenged energy when the battery is already full). 
In the second part of this work, we will explore the adoption of machine learning techniques to tackle a number of common wireless networking problems. These algorithms are able to learn from and make predictions on data, avoiding the need to follow limited static program instructions: models are built from sample inputs, thus allowing for data-driven predictions and decisions. In particular, we will first design an on-the-fly prediction algorithm for the expected time of arrival related to WiFi transmissions. This predictor only exploits those network parameters available at each receiving node and does not require additional knowledge from the transmitter, hence it can be deployed without modifying existing standard transmission protocols. Secondly, we will investigate the usage of particular neural network instances known as autoencoders for the compression of biosignals, such as electrocardiography and photo plethysmographic sequences. A lightweight lossy compressor will be designed, able to be deployed in wearable battery-equipped devices with limited computational power. Thirdly, we will propose a predictor for the long-term channel gain in a wireless network. Differently from other works in the literature, such predictor will only exploit past channel samples, without resorting to additional information such as GPS data. An accurate estimation of this gain would enable to, e.g., efficiently allocate resources and foretell future handover procedures. Finally, although not strictly related to wireless networking scenarios, we will show how deep learning techniques can be applied to the field of autonomous driving. This final section will deal with state-of-the-art machine learning solutions, proving how these techniques are able to considerably overcome the performance given by traditional approaches.",,2017,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
4132e988046539efd8620d426e413cc80789a017,https://www.semanticscholar.org/paper/4132e988046539efd8620d426e413cc80789a017,The Third Army Research Office ( ARO ) Workshop on Adversarial Machine Learning Talk Abstracts and Bios Data,"The proliferation of machine learning (ML) and artificial intelligence (AI) systems for military and security applications creates substantial challenges for designing and deploying such mechanisms that would learn, adapt, reason and act with Dinky, Dirty, Dynamic, Deceptive, Distributed (D5) data. While Dinky and Dirty challenges have been extensively explored in ML theory, the Dynamic challenge has been a persistent problem in ML applications (when the statistical distribution of training data differs from that of test data). The most recent Deceptive challenge is a malicious distribution shift between training and test data that amplifies the effects of the Dynamic challenge to the complete breakdown of the ML algorithms. Using the MNIST dataset as a simple calibration example, we explore the following two questions: (1) What geometric and statistical characteristics of data distribution can be exploited by an adversary with a given magnitude of the attack? (2) What counter-measures can be used to protect the constructed decision rule (at the cost of somewhat decreased performance) against malicious distribution shift within a given magnitude of the attack? While not offering a complete solution to the problem, we collect and interpret obtained observations in a way that provides practical guidance for making more adversary-resistant choices in the design of ML algorithms. Bio: Rauf Izmailov is a Senior Research Scientist at Perspecta Labs and an established researcher in mathematical and computer models for networking and control systems, machine learning, optimization, and statistical data analysis. He has more than 20 years of industry experience (including AT&T Bell Labs and NEC Labs America) in research and technical leadership of R&D teams. With Dr. Vapnik, he co-invented the new machine learning paradigm, Learning Using Privileged Information (LUPI). He was the PI on the DARPA PPAML (“Probabilistic Programming for Advanced Machine Learning”) program and is currently the PI on the DARPA D3M (“Data-Driven Discovery of Models”) program and the analytics task leader on the DARPA LADS (“Leveraging the Analog Domain for Security”) program. He is also a co-PI on the AFOSR program “Science of Information, Computation, Learning and Fusion”. Adversarial Unsupervised Learning Abstract: Nowadays more and more data are gathered for detecting and preventing cyber attacks. In cyber security applications, data analytics techniques have to deal with active adversaries that try to deceive the data analytics models and avoid being detected. The existence of such adversarial behavior motivates the development of robust and resilient adversarial learning techniques for various tasks. Most of the existing work focused on adversarial classification techniques, which assumed the existence of a large amount of labeled data instances. However, in practice, labeling the data instances often requires costly and time-consuming human expertise and becomes a significant bottleneck. Nowadays more and more data are gathered for detecting and preventing cyber attacks. In cyber security applications, data analytics techniques have to deal with active adversaries that try to deceive the data analytics models and avoid being detected. The existence of such adversarial behavior motivates the development of robust and resilient adversarial learning techniques for various tasks. Most of the existing work focused on adversarial classification techniques, which assumed the existence of a large amount of labeled data instances. However, in practice, labeling the data instances often requires costly and time-consuming human expertise and becomes a significant bottleneck. Meanwhile, a large number of unlabeled instances can also be used to understand the adversaries' behavior. To address the above mentioned challenges, we develop a novel grid based adversarial clustering algorithm. Our adversarial clustering algorithm is able to identify the normal and abnormal regions, and to draw defensive walls around the centers of the normal objects utilizing game theoretic ideas. Our algorithm also identifies the overlapping areas within large mixed clusters, and outliers which may be potential anomalies. Bio: Bowei Xi received her Ph.D. in statistics from the Department of Statistics at the University of Michigan, Ann Arbor in 2004. She is an associate professor in the Department of Statistics at Purdue University. She was a visiting faculty in the Department of Statistics at Stanford University in summer 2007, and a visiting faculty at Statistical and Applied Mathematical Sciences Institute (SAMSI) from September 2012 to May 2013. Her research focuses on multidisciplinary work involving big datasets with complex structure from very different application areas including cyber security, Internet traffic, metabolomics, machine learning, and data mining. She has a US patent on an automatic system configuration tool and has filed another patent application for identification of blood-based metabolite biomarkers of pancreatic cancer. Limitations of the Lipschitz Constant as a Defense Against Adversarial Examples Abstract: Several recent papers have discussed utilizing Lipschitz constants to limit the susceptibility of neural networks to adversarial examples. We analyze recently proposed methods for computing the Lipschitz constant. We show that the Lipschitz constant may indeed enable adversarially robust neural networks. However, the methods currently employed for computing it suffer from theoretical and practical limitations. We argue that addressing this shortcoming is a promising direction for future research into certified adversarial defenses. Several recent papers have discussed utilizing Lipschitz constants to limit the susceptibility of neural networks to adversarial examples. We analyze recently proposed methods for computing the Lipschitz constant. We show that the Lipschitz constant may indeed enable adversarially robust neural networks. However, the methods currently employed for computing it suffer from theoretical and practical limitations. We argue that addressing this shortcoming is a promising direction for future research into certified adversarial defenses. Bio: Todd Huster is a research scientist at Perspecta Labs. He has extensive experience solving challenging problems in the fields of machine learning, remote sensing, evaluation methodologies, and symbolic reasoning. He holds an M.S. in Computer Science from Wright State University. Certified Defenses Against Adversarial Examples Abstract: While neural networks have achieved high accuracy on standard image classification benchmarks, their accuracy drops to nearly zero in the presence of small adversarial perturbations to test inputs. Defenses based on regularization and adversarial training have been proposed, but often followed by new, stronger attacks that defeat these defenses. Can we somehow end this arms race? In this talk, I will present some methods based on convex relaxations (with a focus on semidefinite programming) that output a certificate that for a given network and test input, no attack can force the error to exceed a certain value. I will then discuss how these certification procedures can be incorporated into neural network training to obtain provably While neural networks have achieved high accuracy on standard image classification benchmarks, their accuracy drops to nearly zero in the presence of small adversarial perturbations to test inputs. Defenses based on regularization and adversarial training have been proposed, but often followed by new, stronger attacks that defeat these defenses. Can we somehow end this arms race? In this talk, I will present some methods based on convex relaxations (with a focus on semidefinite programming) that output a certificate that for a given network and test input, no attack can force the error to exceed a certain value. I will then discuss how these certification procedures can be incorporated into neural network training to obtain provably robust networks. Finally, I will present some empirical results on the performance of attacks and different certificates on networks trained using different objectives. This is joint work with Jacob Steinhardt and Percy Liang. Bio: Aditi Raghunathan is a third year PhD student at Stanford University working with Percy Liang. She is a recipient of the Google PhD Fellowship in Machine Learning and the Open Philanthrophy Project AI Fellowship. She is primarily interested in making machine learning systems provably robust to adversarial perturbations. She is also interested in ensuring fairness in the outcomes of ML systems. She spent the summer of 2018 at Google Brain working with Ian Goodfellow and Alex Kurakin. Previously, she was an undergraduate at IIT Madras. Is Robust ML Really Robust? Abstract: Machine learning (ML) techniques are increasingly common in security applications, such as malware and intrusion detection. However, ML models are often susceptible to evasion attacks, in which an adversary makes changes to the input (such as malware) in order to avoid being detected. A conventional approach to evaluate ML robustness to such attacks, as well as to design robust ML, is by considering simplified feature-space models of attacks, where the attacker changes ML features directly to effect evasion, while minimizing or constraining the magnitude of this change. We investigate the effectiveness of this approach to designing robust ML in the face of attacks that can be realized in actual malware (realizable attacks). We demonstrate that in the context of structure-based PDF malware detection, such techniques appear to have limited effectiveness. On the other hand, they are quite effective with contentbased detectors. In either case, we show that augmenting the feature space models with conserved features (those that cannot be unilaterally modified without compromising malicious functionality",,2018,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
16651515d0fd738c6c1079d2b9ff079cf947adcc,https://www.semanticscholar.org/paper/16651515d0fd738c6c1079d2b9ff079cf947adcc,Cloud-based machine learning for the detection of anonymous web proxies,"The emergence and growth of cloud computing has made a serious impact on the IT industry in recent years with large companies starting to offer powerful, reliable and cost-efficient platforms for businesses to build and reshape their business models. Showing no sign of slowing down, cloud computing capabilities now include machine learning, with facilities for both designing and deploying models. With this capability of machine learning using cloud computing comes the increasing need to be able to classify whether an incoming connection is from a legitimate originating IP address or if it is being sent through an intermediary like a web proxy. Taking inspiration from Intrusion Detection Systems that make use of machine learning capabilities to improve anomaly detection accuracy, this paper proposes that cloud based machine learning can be used in order to detect and classify web proxy usage by capturing packet data and feeding it into a cloud based machine learning web service.",2016 27th Irish Signals and Systems Conference (ISSC),2016,10.1109/ISSC.2016.7528443,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
dd61223b8913d6af89e0976b945c54c2648b6456,https://www.semanticscholar.org/paper/dd61223b8913d6af89e0976b945c54c2648b6456,Data analytics and machine learning for continued semiconductor scaling,"Although there has been a rapid and greatly publicized growth of data analytics and machine learning methodologies across many applications, and in virtually every industry, these developments seem to have almost completely been missed in the semiconductor integrated circuit (IC) space. With the 14nm process technology node currently in production, and both 10 and 7nm nodes at different stages of development, the IC ‘ecosystem’ is being restructured and consolidated across its four traditional components (i.e., fabless design companies, electronic design automation and intellectual property suppliers, process and metrology tools suppliers, and silicon foundries). There are, however, intrinsic technology factors (e.g., the continual deceleration of geometric scaling and the delayed introduction of key patterning technologies) that are primary sources of disruption to this restructuring. There are also critical hidden gaps and bottlenecks in the design-to-manufacturing data information pipeline. The deployment of carefully selected data analytics techniques (with/without machine learning algorithms) therefore represents a strategic opportunity to enable a 2 year/node (‘more-Moore’) cycle at 10nm and below in the semiconductor industry. In this work,1 we present a survey of the state-of-the-art and ongoing developments in data analytics and machine learning. We also offer a perspective on the functional interactions and data information flows for IC design-to-manufacturing, and we discuss the risks and opportunities that arise from the introduction of big-data analytics and machine-learning technologies. Although the terms ‘data analytics’ (or ‘big data’) and ‘machine learning’ comprise a vast constellation2 of mathematical methodologies, computational platforms, and system-level Figure 1. Illustrating the generation of a topological network map for semiconductor integrated circuit (IC) research and development. Left: The physical domain (i.e., a layout) is broken down into abstract element components that are then represented as a set of multidimensional points. Right: The high-dimensional data cloud thus represents the physical domain space in topological graph space. (Courtesy of Motivo Data Analytics.)",,2016,10.1117/2.1201609.006702,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
9bd5667f9cabda4bd23d588811c25e500c9bed27,https://www.semanticscholar.org/paper/9bd5667f9cabda4bd23d588811c25e500c9bed27,An end-to-end process model for supervised machine learning classification: from problem to deployment in information systems,"Extracting meaningful knowledge from (big) data represents a key success factor in many industries today. Supervised machine learning (SML) has emerged as a popular technique to learn patterns in complex data sets and to identify hidden correlations. When this insight is turned into action, business value is created. However, common data mining processes are generally not tailored to SML. In addition, they fall short of providing an end-to-end view that not only supports building a ”one off” model, but also covers its operational deployment within an information system. In this research-in-progress work we apply a Design Science Research (DSR) approach to develop a SML process model artifact that comprises model initiation, error estimation and deployment. In a first cycle, we evaluate the artifact in an illustrative scenario to demonstrate suitability. The results encourage us to further refine the approach and to prepare evaluations in concrete use cases. Thus, we move towards contributing a general process model that supports the systematic design of machine learning solutions to turn insights into continuous action.",,2017,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
f120af01dbcf080fde66b6d9bad96eb0ffbead16,https://www.semanticscholar.org/paper/f120af01dbcf080fde66b6d9bad96eb0ffbead16,Towards Semantic Integration of Machine Vision Systems to Aid Manufacturing Event Understanding,"A manufacturing paradigm shift from conventional control pyramids to decentralized, service-oriented, and cyber-physical systems (CPSs) is taking place in today’s 4th industrial revolution. Generally accepted roles and implementation recipes of cyber systems are expected to be standardized in the future of manufacturing industry. The authors intend to develop a novel CPS-enabled control architecture that accommodates: (1) intelligent information systems involving domain knowledge, empirical model, and simulation; (2) fast and secured industrial communication networks; (3) cognitive automation by rapid signal analytics and machine learning (ML) based feature extraction; (4) interoperability between machine and human. Semantic integration of process indicators is fundamental to the success of such implementation. This work proposes an automated semantic integration of data-intensive process signals that is deployable to industrial signal-based control loops. The proposed system rapidly infers manufacturing events from image-based data feeds, and hence triggers process control signals. Two image inference approaches are implemented: cloud-based ML model query and edge-end object shape detection. Depending on use cases and task requirements, these two approaches can be designated with different event detection tasks to provide a comprehensive system self-awareness. Coupled with conventional industrial sensor signals, machine vision system can rapidly understand manufacturing scenes, and feed extracted semantic information to a manufacturing ontology developed by either expert or ML-enabled cyber systems. Moreover, extracted signals are interpreted by Programmable Logical Controllers (PLCs) and field devices for cognitive automation towards fully autonomous industrial systems.",Sensors,2021,10.3390/s21134276,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
362edc0f31ca39ad58d473a3872715a04fe516ed,https://www.semanticscholar.org/paper/362edc0f31ca39ad58d473a3872715a04fe516ed,Fair Policy Learning,"Ensuring machine learning algorithms deployed in the real world do not result in unexpected unfairness or social implications is becoming increasingly important. However, there exists a clear gap in literature for a measure of fairness that can detect discrimination against multiple sensitive attributes while also handling continuous or discrete outcomes. In this thesis, we propose a fairness measure, Fair-COCCO, based on the conditional cross-covariance operator on reproducing kernel Hilbert Spaces. This novel method generalise to the majority of existing fairness notions and naturally extends to settings with continuous outcomes and multidimensional sensitive attributes. Additionally, we demonstrate how the proposed measure can be readily implemented in stochastic gradient optimisation for fair policy learning in supervised learning settings. Empirical evaluations of Fair-COCCO on synthetic and realworld experiments reveal favourable comparisons to state-of-the-art techniques in balancing predictive power and fairness. We also see much potential in applying machine learning to analyse fairness in observed behaviour, especially in complex and high-dimensional real-world environments. To that end, we propose the first known definition of fairness for sequences of decisions and showcase how Fair-COCCO can be applied to quantify fairness in these problems. Building off these definitions, we turn to learning fair policies in real-world conditions, where learning is constrained to be performed offline. We propose Fair-PoLe, a novel inverse reinforcement learning that operates completely offline and is computationally efficient and functionally expressive when compared to existing methods. We illustrate the potential for Fair-PoLe to learn policies that balance imitation of expert policies with fair outcomes on the challenging problem of sepsis treatment.",,2021,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
53bc41f281e8e80ffd2290fb3cb5be123d53d7a9,https://www.semanticscholar.org/paper/53bc41f281e8e80ffd2290fb3cb5be123d53d7a9,Next-Generation Self-Organizing Networks through a Machine Learning Approach,"At the present time, for most of the population, mobile phones have become the instruments through which to interact with the surrounding world. From its original service of voice transmission, cellular communications have evolved to provide a variety of services that could be hardly imagined just four decades ago. From that starting point, cellular networks were first enhanced to support data transmission, which opened the door to services like videocalls or web surfing. Later on, successive improvements were made so as to reach resource- and energy-efficient networks, while enhancing the users' perceived quality of experience (QoE) by means of an increasingly higher performance. In order to consequently reduce the management costs in such scenario, some further improvements needed to be made. This led to the concept of self-organizing networks (SONs). That is, the automation of the management tasks of a cellular network to reduce the operational and capital expenditure (OPEX and CAPEX, respectively). 
SON tasks are divided in three categories: self-configuration, self-optimization and self-healing. Self-configuration aims at automating the actions required when a network is to be deployed, like the initial configuration parameter setting. Self-optimization tasks pursue maximizing the efficiency of the networks in a time-varying environment, which takes shape as a variety of mechanisms addressing mobility, accessibility and integrity issues. Finally, the targets of self-healing are identifying and repairing possible failures that may arise while the network is operated.
Thus, one of the main tasks of self-healing is determining the cause of a failure, which is called root cause analysis (RCA). Tools for RCA are automatic systems which, in the shape of classification systems, aim at determining a class (or network state) regarding a set of features (or key performance indicators, KPIs). Although different mechanisms have been proposed until now as tools for RCA, there is a long way to develop accurate systems that can deal with the large amount of performance information that is normally collected in a cellular network.
Together with self-healing, self-optimization appears as the SON function group that attracts the most attention from industry and academia. This is mainly due to the optimization opportunities that novel functionalities from the upcoming networks bring. In particular, the management of multi-link connections, and within these, multi-connectivity (MC), occupies an eminent place in the next generation of mobile communications: the Fifth Generation New Radio (5G NR). However, given its novelty, multi-link communications currently lack of efficient management mechanisms, which will be one of the research hot topics in the coming years. 
The objective of this thesis is the improvement of SON functions through the development and use of machine learning (ML) tools for the network management. In particular, its target is twofold. On the hand, self-healing is addressed through the proposal of a novel tool for RCA, which takes the shape of a combination of multiple RCA baseline systems to develop an enhanced ensemble-based system. In order to further enhance the RCA accuracy while lowering both the CAPEX and OPEX, ML techniques for dimensionality reduction are proposed and assessed in combination with RCA tools. On the other hand, multi-link functionalities within self-optimization are studied, and techniques for automatic link management are proposed. In the field of enhanced mobile broadband (eMBB) communications, a component carrier manager implementing network operators' policies is proposed, whereas in the field of low-latency vehicular communications, a mechanism for multi-path traffic steering is proposed. 
Many of the methods proposed in this thesis have been assessed using data from live cellular networks, which has allowed them to demonstrate both their validity in realistic environments and their ability to be deployed in current and forthcoming cellular networks.",,2018,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
53da9065f338c26b69990f6a3e39c3f56682cac3,https://www.semanticscholar.org/paper/53da9065f338c26b69990f6a3e39c3f56682cac3,Collaborative Learning Model for Cyberattack Detection Systems in IoT Industry 4.0,"Although the development of IoT Industry 4.0 has brought breakthrough achievements in many sectors, e.g., manufacturing, healthcare, and agriculture, it also raises many security issues to human beings due to a huge of emerging cybersecurity threats recently. In this paper, we propose a novel collaborative learning-based intrusion detection system which can be efficiently implemented in IoT Industry 4.0. In the system under consideration, we develop smart “filters” which can be deployed at the IoT gateways to promptly detect and prevent cyberattacks. In particular, each filter uses the collected data in its network to train its cyberattack detection model based on the deep learning algorithm. After that, the trained model will be shared with other IoT gateways to improve the accuracy in detecting intrusions in the whole system. In this way, not only the detection accuracy is improved, but our proposed system also can significantly reduce the information disclosure as well as network traffic in exchanging data among the IoT gateways. Through thorough simulations on real datasets, we show that the performance obtained by our proposed method can outperform those of the conventional machine learning methods.",2020 IEEE Wireless Communications and Networking Conference (WCNC),2020,10.1109/WCNC45663.2020.9120761,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
72188532ea7acd4e323ed64c1dbe1e53dbcc1068,https://www.semanticscholar.org/paper/72188532ea7acd4e323ed64c1dbe1e53dbcc1068,On-Device Deep Learning Inference for System-on-Chip (SoC) Architectures,"As machine learning becomes ubiquitous, the need to deploy models on real-time, embedded systems will become increasingly critical. This is especially true for deep learning solutions, whose large models pose interesting challenges for target architectures at the “edge” that are resource-constrained. The realization of machine learning, and deep learning, is being driven by the availability of specialized hardware, such as system-on-chip solutions, which provide some alleviation of constraints. Equally important, however, are the operating systems that run on this hardware, and specifically the ability to leverage commercial real-time operating systems which, unlike general purpose operating systems such as Linux, can provide the low-latency, deterministic execution required for embedded, and potentially safety-critical, applications at the edge. Despite this, studies considering the integration of real-time operating systems, specialized hardware, and machine learning/deep learning algorithms remain limited. In particular, better mechanisms for real-time scheduling in the context of machine learning applications will prove to be critical as these technologies move to the edge. In order to address some of these challenges, we present a resource management framework designed to provide a dynamic on-device approach to the allocation and scheduling of limited resources in a real-time processing environment. These types of mechanisms are necessary to support the deterministic behavior required by the control components contained in the edge nodes. To validate the effectiveness of our approach, we applied rigorous schedulability analysis to a large set of randomly generated simulated task sets and then verified the most time critical applications, such as the control tasks which maintained low-latency deterministic behavior even during off-nominal conditions. The practicality of our scheduling framework was demonstrated by integrating it into a commercial real-time operating system (VxWorks) then running a typical deep learning image processing application to perform simple object detection. The results indicate that our proposed resource management framework can be leveraged to facilitate integration of machine learning algorithms with real-time operating systems and embedded platforms, including widely-used, industry-standard real-time operating systems.",Electronics,2021,10.3390/ELECTRONICS10060689,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
6247c0acba0dd88204a0d8d23b89a293f9d58e20,https://www.semanticscholar.org/paper/6247c0acba0dd88204a0d8d23b89a293f9d58e20,Tutorial on Machine Learning for Spectrum Sharing in Wireless Networks,"As spectrum utilization efficiency is the major bottleneck in current wireless networking, many stakeholders discuss that spectrum should be shared rather than being exclusively allocated. Shared spectrum access raises many challenges which, if not properly addressed, degrades the performance level of the co-existing networks. Coexistence scenarios may involve two or more networks: same or different types; operated by the same or different operators. The complex interactions among the coexisting networks can be addressed by the machine learning tools which by their nature embrace uncertainty and can model the complex interactions. In this tutorial, we start with the basics of coexistence of wireless networks in the unlicensed bands. Then, we focus on WiFi and LTE-U coexistence. After providing a brief overview of machine learning topics such as supervised learning, unsupervised learning, reinforcement learning, we overview five particular examples which exploit learning schemes to enable efficient spectrum sharing entailing a generic cognitive radio setting as well as LTE and WiFi coexistence scenarios. We conclude with a list of challenges and research directions. I. SPECTRUM SHARING IN WIRELESS NETWORKS Spectrum sharing is the situation where at least two users or technologies are authorized to use the same portion of the radio spectrum on a non-exclusive manner1. We overview the current state of spectrum sharing and provide a taxonomy of spectrum sharing scenarios. We can list the main challenges in providing peaceful coexistence as follows: (i) scarcity of the resources, (ii) heterogeneity of the coexisting networks, (iii) power asymmetry, and (iv) lack of coordination, communication, and cooperation among the coexisting networks. II. COEXISTENCE IN THE UNLICENSED BANDS: THE CASE OF WIFI AND LTE-U The success of IEEE 802.11 networks in the unlicensed bands, i.e., 2.4 GHz and 5 GHz, has proved the efficiency and feasibility of using spectrum in a license-exempt manner. Currently, even the cellular providers consider expanding their network’s capacity with unlicensed spectrum to cope with the increasing wireless traffic demand. More particularly, Qualcomm’s 2013 proposal [1] of aggregating 5GHz bands with the licensed carriers of an LTE network has paved the way for LTE unlicensed networks. However, operation in the unlicensed bands has to address the coexistence challenges. For example, WiFi networks at 2.4 GHz bands, e.g., 802.11b/g/n, have to find the best channel 1S. Forge, R. Horvitz, and C. Blackman. Perspectives on the value of shared spectrum access. Final Report for the European Commission, 2012. among three non-overlapping channels for operation in a very-dense WLAN deployment. Additionally, 2.4 GHz band accommodates also non-WiFi technologies such as Bluetooth, ZigBee, or microwave ovens which all create interference on WLANs. As for 5GHz which has many more non-overlapping channels compared to 2.4 GHz, the more severe challenge is to coexist with technologies other than 802.11n/ac/ax networks, namely unlicensed LTE networks and radars. The main coexistence mechanism of WiFi is listen-beforetalk (LBT) which is also known as carrier sense multiple access with collision avoidance (CSMA/CA). A station with a traffic to transmit has to first check whether the medium is free or not. To decide on the state of the medium, two approaches exist: carrier sense and energy detection. In carrier sensing, a WiFi node decodes the preamble of a WiFi frame that is received above some energy detection level. The node extracts the information from the PLCP header which carries information about the occupancy duration of the medium by that ongoing flow. This mechanism is also referred to as channel reservation. With this information, a WiFi node knows when to re-start sensing the medium for a transmission opportunity. Energy detection (ED) is a simpler approach in which a candidate transmitter decides that the air is free if the signal level is below a predefined ED threshold. This approach is used for detecting inter-technology signals, where the received signal is not decodable, i.e., it belongs to other technologies (or corrupted WiFi signals). Despite its simplicity, ED requires more effort on a potential transmitter as it must constantly sense the energy level in the air to detect a transmission opportunity. As LTE follows a scheduled medium access on the licensedspectrum, there is no notion or necessity of politeness or LBT in more technical terms. However, it is vital for LTE unlicensed to implement such mechanisms for coexistence with WiFi and other unlicensed LTE networks at 5 GHz bands. Currently, frequency-domain sharing is a first step only. In other words, an LTE small cell first checks the channel activities and selects a clear channel, if any. For time sharing, there are two approaches taken by two variants of LTE unlicensed: duty cycling by LTE-U and LBT by License-Assisted-Access (LAA). LTE-U which is an industry-led effort lets small cells apply duty cycling where during the OFF periods WiFi can access the medium. As this approach does not mandate LBT before turning small cell transmissions on, it may degrade WiFi performance drastically. LAA requires LBT similar to WiFi’s CSMA/CA. LAA speciCROWNCOM2017 TUTORIAL 2 fication is led by 3GPP and aims to develop a global solution in contrast to LTE-U which is only compliant to countries like US, Korea, China where LBT is not mandatory. We overview basics of these two variants and list the major issues in their peaceful coexistence with WiFi networks. III. BACKGROUND ON MACHINE LEARNING We provide a sparse overview of learning approaches: supervised, unsupervised, and reinforcement learning. IV. THE ROLE OF MACHINE LEARNING IN SPECTRUM SHARING AND COEXISTENCE In this part, we examine the literature using ML approaches to solve the coexistence issues as our case studies. Is the channel idle or busy?This question is at the heart of coexistence of networks in a multi-channel environment, as the first step of coexistence is to choose a channel that is clear. For cognitive radio networks, it is mandatory to detect the channel state to avoid violating the rules of secondary spectrum access. Casting this question into a binary classification problem, authors [2] introduce several (un)supervised learning algorithms to correctly identify the state of a channel. While supervised approaches require the real channel state information from the Primary Users, unsupervised learning such as K-means does not require any input from the PUs which is a desirable property of classification scheme in a practical setting. Which unlicensed channel to select for each LAA SBS for inter-operator coexistence?As we expect multiple LAA operators deploy their small cells independently, there is surely the question of how to select an unlicensed channel to aggregate, particularly in case there are more cells than the number of available channels. One way of channel selection is to let every LAA BS learn from its own observations via trial-and-error, Q-learning [3]. Which unlicensed carrier to aggregate and how long to use this carrier?Q-learning framework can also be applied to an LAA setting where an LAA BS needs to select an unlicensed carrier and the transmission duration on the selected carrier [4]. Can WiFi exploit ML for defending itself against LTEU interference?Different than the literature which develops coexistence solutions to be deployed at the LTE base stations for WiFi/LTE setting, [5] proposes to also equip the WiFi APs with a tool that estimates the ON-duration of an existing LTE-U network in the neighborhood. Moreover, the developed solution can estimate the remaining airtime for the WiFi AP based on the LTE’s predicted ON duration. Key idea of WiPLUS is to detect the times where LTE-U has an ongoing transmission using the data passively collected from the MAC FSM of the NIC. However, although LTE-U signal may not be detected above the ED level, it may still have a severe impact on WiFi. Thus, PHY-layer analysis solely on signal level is short of detecting the moderate interference regime. WiPLUS overcomes this challenge by combining data from MAC FSM states and ARQ missing acknowledgments. Sampled data from a testbed has a lot of noise due to imperfections of the measuring devices and the complex interactions among the coexisting systems as well as PHY and MAC layers. WiPLUS applies K-means clustering to detect outliers on the estimated LTE-U on-durations. After filtering the data points based on the signal’s frequency harmonics, WiPLUS calculates the LTEU on-time as the average of the data points, each of which corresponds to an estimate of LTE-U on-time. Can we estimate WiFi link performance by learning from real-world link capacity measurements?In a multiAP setting, an AP can select the operation channel based on the expected capacity of the existing links. The traditional way is to take the SNIR-based capacity estimate into account, i.e., Shannon’s capacity formula. However, this capacity model may sometimes fail to represent the complex interactions between PHY and MAC layers, e.g., partially-overlapping channels in case of channel bonding in new 802.11ac/ax standards. The idea of [6] is to use supervised learning as a tool to model the complex interactions among many factors such as power and PHY rate of a neighboring WiFi link implicitly rather than modelling it explicitly. V. OPEN RESEARCH DIRECTIONS Machine learning-based solutions can embrace the complexity and uncertainty prevalent in the complex scenarios, especially hybrid horizontal spectrum sharing, by learning from the observations. However, a wireless network poses peculiar challenges such as the energy limitations, real-time operation, and sometimes fast changes in the operation environment that render learning less effective. We overview such challenges and conclude with some open questions in this ",,2017,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
3e00b5b5adafb952b49006038eeb62979faab8a2,https://www.semanticscholar.org/paper/3e00b5b5adafb952b49006038eeb62979faab8a2,Sensor retrofit for a coffee machine as condition monitoring and predictive maintenance use case,". The concept of Industry 4.0 provides promising approaches to reduce downtime and increase overall equipment efficiency in manufacturing processes through interconnected devices in the industrial internet of things (IIoT). As the procurement of new IIoT-ready machines is costly, the retrofit of old machines can be an idea worth exploring. In this paper, we designed a simple experiment setup using affordable sensors and a coffee machine (due to the absence of machinery) to measure grinding vibrations and to predict the last coffee before grinder no-load. Microsoft Azure Machine Learning Studio was used to deploy machine learning techniques in order train prediction models. While prediction accuracy in this experiment was non-satisfactory, our results nonetheless indicate that retrofit is indeed a proper approach to make an older machine park smart, provided that sensors (especially their sample rate) are suitable for the application.",Wirtschaftsinformatik,2019,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10265d1c6b2274f3f18e7c7ecf04b461d1406fcc,https://www.semanticscholar.org/paper/10265d1c6b2274f3f18e7c7ecf04b461d1406fcc,Machine learing for mobile edge computing,"In recent years, mobile edge computing has attracted a considerable amount of attention from both academia and industry through its many advantages (such as low latency, computation efficiency and privacy) caused by its local model of providing storage and computation resources. In addition, machine learning has become the dominant approach in applications such as industry, healthcare, smart home, and transportation. All of these applications heavily rely on technologies that can be deployed at the network edge. Therefore, it is essential to combine machine learning with mobile edge computing to further promote the proliferation of intelligent edges. In general, machine learning relies on powerful computation and storage resources for superior performance, while mobile edge computing typically provides limited computation resources locally. To this end, the implementations of machine learning algorithms should be revisited for mobile edge computing.",China Communications,2021,10.23919/JCC.2021.9631177,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
662ffffbbd207e428190291ac6068ef9740278d0,https://www.semanticscholar.org/paper/662ffffbbd207e428190291ac6068ef9740278d0,A random forest based machine learning approach for mild steel defect diagnosis,"Industries today need to stay ahead in competition by servicing and satisfying customer's needs. Quality of the produced products to match as per customer demands is the key goal for a product manufacturing company. A product produced with variation in characteristics, than the anticipated are called as defect. In the mild steel coil manufacturing plants, large amount of data is generated with the help of many sensors deployed to measure different parameters which can be used for defect diagnosis of the coils produced. In case of mild steel coil, deviation of the final cooling temperature of the coil from desired temperature produces defective coils. The paper presents machine learning approach and the methodology for cooling temperature deviation defect diagnosis that consists of four phases namely data structuring, association identification, statistical derivation and classification. We also provide comparative results obtained with various data mining algorithms like decision trees, neural networks, SVM, ensemble techniques (boosting and random forest) in terms of performance parameters and prove that random forest outperforms rest of the techniques by achieving an accuracy of 95%.",2016 IEEE International Conference on Computational Intelligence and Computing Research (ICCIC),2016,10.1109/ICCIC.2016.7919549,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
e01d16f2587203b7efd2fa636c9c79cf1116e50d,https://www.semanticscholar.org/paper/e01d16f2587203b7efd2fa636c9c79cf1116e50d,Pessimistic Reward Models for Off-Policy Learning in Recommendation,"Methods for bandit learning from user interactions often require a model of the reward a certain context-action pair will yield – for example, the probability of a click on a recommendation. This common machine learning task is highly non-trivial, as the data-generating process for contexts and actions is often skewed by the recommender system itself. Indeed, when the deployed recommendation policy at data collection time does not pick its actions uniformly-at-random, this leads to a selection bias that can impede effective reward modelling. This in turn makes off-policy learning – the typical setup in industry – particularly challenging. In this work, we propose and validate a general pessimistic reward modelling approach for off-policy learning in recommendation. Bayesian uncertainty estimates allow us to express scepticism about our own reward model, which can in turn be used to generate a conservative decision rule. We show how it alleviates a well-known decision making phenomenon known as the Optimiser’s Curse, and draw parallels with existing work on pessimistic policy learning. Leveraging the available closed-form expressions for both the posterior mean and variance when a ridge regressor models the reward, we show how to apply pessimism effectively and efficiently to an off-policy recommendation use-case. Empirical observations in a wide range of environments show that being conservative in decision-making leads to a significant and robust increase in recommendation performance. The merits of our approach are most outspoken in realistic settings with limited logging randomisation, limited training samples, and larger action spaces.",RecSys,2021,10.1145/3460231.3474247,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
6aa8f8ae07a57a2025d6adc27a0bc53f6a7ee385,https://www.semanticscholar.org/paper/6aa8f8ae07a57a2025d6adc27a0bc53f6a7ee385,Machine learning for targeted display advertising: transfer learning in action,,Machine Learning,2013,10.1007/s10994-013-5375-2,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
fa98e060df37e412f1a949b4012b5b0ded587209,https://www.semanticscholar.org/paper/fa98e060df37e412f1a949b4012b5b0ded587209,An overview on the evolution and adoption of deep learning applications used in the industry,"With continuous improvements in performance of microprocessors over the years, they now possess capabilities of supercomputers of earlier decade. Further with the continuous increase in the packaging density on the silicon and General Purpose Graphics Processing Unit (GPGPU) enhancements, has led to utilization the deep learning (DL) techniques, which had lost steam during the last decade. A GPGPU is a parallel programming setup using a combination of GPUs and CPUs that can manipulate large matrices. Interestingly, GPUs were created for faster graphic processing, but found its way into relevant scientific computing. DL is a subset of the artificial intelligence (AI) domain and falls specifically under the set of machine learning (ML) techniques which are based on learning data representations rather than task‐specific algorithms. It has been observed that the accuracy and the pragmatism of deploying DL at massive level was restricted by technological issues of executing DL based AI models, with extremely large training sessions running into weeks. DL applications can solve problems of very large order and areas like computer vision/image processing is one of the early successes and becoming quite a sensation in many areas such as natural language processing (NLP) with state of the art real‐time translation capabilities, automatic game playing, optical character recognition especially handwritten text, and so on. This overview traverses the evolution and successful adoption in the various industry verticals.",WIREs Data Mining Knowl. Discov.,2018,10.1002/widm.1257,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
8003537c51f56ee60298718ee5fa60eda4c64947,https://www.semanticscholar.org/paper/8003537c51f56ee60298718ee5fa60eda4c64947,Machine Learning For Cloud Computing Intrution Detection,"The term “cloud computing” has emerged as a major ICT trend and has been acknowledged by respected industry survey organizations as a key technology and market development theme for the industry and ICT users in 2010. In Cloud Computing intrusion detection research, one popular strategy for finding attacks is monitoring a cloud’s activity for anomalies: deviations from profiles of normality previously learned from benign traffic typically identified using tools borrowed from the machine learning community. However, despite extensive academic research one finds a striking gap in terms of actual deployments of such systems: compared with other intrusion detection approaches, machine learning is rarely employed in operational “real world” settings. We examine the differences between the cloud computing intrusion detection problem and other areas where machine learning regularly finds much more success. Our main claim is that the task of finding attacks is fundamentally different from these other applications, making it significantly harder for the intrusion detection community to employ machine learning effectively. We support this claim by identifying challenges particular to cloud computing intrusion detection, and provide a set of guidelines meant to strengthen future research on anomaly detection.",,2013,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
d5318a4ebb4160743e3af568b310ebc5aaebc09e,https://www.semanticscholar.org/paper/d5318a4ebb4160743e3af568b310ebc5aaebc09e,"Federated learning for 6G communications: Challenges, methods, and future directions","As the 5G communication networks are being widely deployed worldwide, both industry and academia have started to move beyond 5G and explore 6G communications. It is generally believed that 6G will be established on ubiquitous Artificial Intelligence (AI) to achieve data-driven Machine Learning (ML) solutions in heterogeneous and massive-scale networks. However, traditional ML techniques require centralized data collection and processing by a central server, which is becoming a bottleneck of large-scale implementation in daily life due to significantly increasing privacy concerns. Federated learning, as an emerging distributed AI approach with privacy preservation nature, is particularly attractive for various wireless applications, especially being treated as one of the vital solutions to achieve ubiquitous AI in 6G. In this article, we first introduce the integration of 6G and federated learning and provide potential federated learning applications for 6G. We then describe key technical challenges, the corresponding federated learning methods, and open problems for future research on federated learning in the context of 6G communications.",China Communications,2020,10.23919/JCC.2020.09.009,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
06268e90084dbbbc11b6de20bd6261c7cf0b3071,https://www.semanticscholar.org/paper/06268e90084dbbbc11b6de20bd6261c7cf0b3071,Multi-Class Taxonomy of Well Integrity Anomalies Applying Inductive Learning Algorithms: Analytical Approach for Artificial-Lift Wells,"
 Well integrity has become a crucial field with increased focus and being published intensively in industry researches. It is important to maintain the integrity of the individual well to ensure that wells operate as expected for their designated life (or higher) with all risks kept as low as reasonably practicable, or as specified. Machine learning (ML) and artificial intelligence (AI) models are used intensively in oil and gas industry nowadays. ML concept is based on powerful algorithms and robust database. Developing an efficient classification model for well integrity (WI) anomalies is now feasible because of having enormous number of well failures and well barrier integrity tests, and analyses in the database.
 Circa 9000 dataset points were collected from WI tests performed for 800 wells in Gulf of Suez, Egypt for almost 10 years. Moreover, those data have been quality-controlled and quality-assured by experienced engineers. The data contain different forms of WI failures. The contributing parameter set includes a total of 23 barrier elements.
 Data were structured and fed into 11 different ML algorithms to build an automated systematic tool for calculating imposed risk category of any well. Comparison analysis for the deployed models was performed to infer the best predictive model that can be relied on. 11 models include both supervised and ensemble learning algorithms such as random forest, support vector machine (SVM), decision tree and scalable boosting techniques. Out of 11 models, the results showed that extreme gradient boosting (XGB), categorical boosting (CatBoost), and decision tree are the most reliable algorithms. Moreover, novel evaluation metrics for confusion matrix of each model have been introduced to overcome the problem of existing metrics which don't consider domain knowledge during model evaluation.
 The innovated model will help to utilize company resources efficiently and dedicate personnel efforts to wells with the high-risk. As a result, progressive improvements on business, safety, environment, and performance of the business. This paper would be a milestone in the design and creation of the Well Integrity Database Management Program through the combination of integrity and ML.","Day 3 Thu, September 23, 2021",2021,10.2118/206129-ms,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
460a171901341431d90eac9d229ab57ee3c419fb,https://www.semanticscholar.org/paper/460a171901341431d90eac9d229ab57ee3c419fb,ActiveAnno: General-Purpose Document-Level Annotation Tool with Active Learning Integration,"ActiveAnno is an annotation tool focused on document-level annotation tasks developed both for industry and research settings. It is designed to be a general-purpose tool with a wide variety of use cases. It features a modern and responsive web UI for creating annotation projects, conducting annotations, adjudicating disagreements, and analyzing annotation results. ActiveAnno embeds a highly configurable and interactive user interface. The tool also integrates a RESTful API that enables integration into other software systems, including an API for machine learning integration. ActiveAnno is built with extensible design and easy deployment in mind, all to enable users to perform annotation tasks with high efficiency and high-quality annotation results.",NAACL,2021,10.18653/v1/2021.naacl-demos.12,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
d53a39fc59bd0af3d93a324b77ad7cdac6b7f35c,https://www.semanticscholar.org/paper/d53a39fc59bd0af3d93a324b77ad7cdac6b7f35c,Improving the quality control of seismic data through active learning,"In image denoising problems, the increasing density of available images makes an exhaustive visual inspection impossible and therefore automated methods based on machine-learning must be deployed for this purpose. This is particulary the case in seismic signal processing. Engineers/geophysicists have to deal with millions of seismic time series. Finding the sub-surface properties useful for the oil industry may take up to a year and is very costly in terms of computing/human resources. In particular, the data must go through different steps of noise attenuation. Each denoise step is then ideally followed by a quality control (QC) stage performed by means of human expertise. To learn a quality control classifier in a supervised manner, labeled training data must be available, but collecting the labels from human experts is extremely time-consuming. We therefore propose a novel active learning methodology to sequentially select the most relevant data, which are then given back to a human expert for labeling. Beyond the application in geophysics, the technique we promote in this paper, based on estimates of the local error and its uncertainty, is generic. Its performance is supported by strong empirical evidence, as illustrated by the numerical experiments presented in this article, where it is compared to alternative active learning strategies both on synthetic and real seismic datasets.",SSRN Electronic Journal,2022,10.2139/ssrn.3996066,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
21e9d6a9ae87305e1bb2ea6056b600e69d1ca550,https://www.semanticscholar.org/paper/21e9d6a9ae87305e1bb2ea6056b600e69d1ca550,Comparing Industry Frameworks with Deeply Quantized Neural Networks on Microcontrollers,"Differently to the common belief, the industry quest for ultra-low-power neural networks is just at the beginning. Examples are the efforts carried by open communities such as TinyML and TinyMLPerf currently focusing on deep learning frameworks for Machine Learning (ML) and associated applications targeting micro-controllers (MCUs). However little attention has been put on deep learning frameworks and applications to enable ultra-low precision ML. These are enabling technologies to target uW hardware implementations. This work aims to compare two Deep Learning frameworks with support to deep quantization, QKeras and Larq, that abstract Tensorflow and Keras frameworks. Currently, Tensorflow is one of the most used deep learning tools by the research and industry communities aimed at deploying ML on the field. Two applications are presented with associated deeply quantized neural networks: Human Activity Recognition (HAR) exploiting a Hybrid Binary Neural Network (HBN) and Anomaly Detection for Industry 4.0 based on a Hybrid Binary AutoEncoder (HBAE). The pros and cons of the frameworks will be discussed during their usage on those applications. Results show an accuracy of up to 98.6% for the HBN and a PSNR up to 111.2 for the HBAE. The inference time has also been measured on different ARM-based architecture.",2021 IEEE International Conference on Consumer Electronics (ICCE),2021,10.1109/ICCE50685.2021.9427638,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
ae210d38d487d0a963bf2c6f2341612118339b3b,https://www.semanticscholar.org/paper/ae210d38d487d0a963bf2c6f2341612118339b3b,Deployment and Management of Time Series Forecasts in Ocean Industry,"Machine learning has not achieved the same degree of success in environmental applications as in other industries. Challenges around data sparsity, quality, and consistency have limited the impact of deep neural network approaches and restricted the focus to research applications. An alternative approach – that is more amenable to the characteristics of data coming from disparate IoT devices deployed at different times and locations in the ocean – is to develop many lightweight models that can be readily scaled up or down based on the number of devices available at any time. This paper presents a serverless framework that naturally marries a single IoT sensor device with a forecasting model. Aspects related to data ingestion, data processing, model training and deployment are described. The framework is applied to a fish farm site in Atlantic Canada.",2021 IEEE International Conference on Big Data (Big Data),2021,10.1109/BigData52589.2021.9671877,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
d9a8e6356dd70c1bd2673b82ecf73873e29707b6,https://www.semanticscholar.org/paper/d9a8e6356dd70c1bd2673b82ecf73873e29707b6,Deep learning in the automotive industry: Applications and tools,"Deep Learning refers to a set of machine learning techniques that utilize neural networks with many hidden layers for tasks, such as image classification, speech recognition, language understanding. Deep learning has been proven to be very effective in these domains and is pervasively used by many Internet services. In this paper, we describe different automotive uses cases for deep learning in particular in the domain of computer vision. We surveys the current state-of-the-art in libraries, tools and infrastructures (e. g. GPUs and clouds) for implementing, training and deploying deep neural networks. We particularly focus on convolutional neural networks and computer vision use cases, such as the visual inspection process in manufacturing plants and the analysis of social media data. To train neural networks, curated and labeled datasets are essential. In particular, both the availability and scope of such datasets is typically very limited. A main contribution of this paper is the creation of an automotive dataset, that allows us to learn and automatically recognize different vehicle properties. We describe an end-to-end deep learning application utilizing a mobile app for data collection and process support, and an Amazon-based cloud backend for storage and training. For training we evaluate the use of cloud and on-premises infrastructures (including multiple GPUs) in conjunction with different neural network architectures and frameworks. We assess both the training times as well as the accuracy of the classifier. Finally, we demonstrate the effectiveness of the trained classifier in a real world setting during manufacturing process.",2016 IEEE International Conference on Big Data (Big Data),2016,10.1109/BigData.2016.7841045,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
62463c4a4a3014c889632932c06ec53435f47697,https://www.semanticscholar.org/paper/62463c4a4a3014c889632932c06ec53435f47697,Cyber risk at the edge: current and future trends on cyber risk analytics and artificial intelligence in the industrial internet of things and industry 4.0 supply chains,,Cybersecur.,2019,10.1186/s42400-020-00052-8.,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
4ed3ac2d1d6fc0bc02f739ffd79cf924e30b4019,https://www.semanticscholar.org/paper/4ed3ac2d1d6fc0bc02f739ffd79cf924e30b4019,Active Learning for ML Enhanced Database Systems,"Recent research has shown promising results by using machine learning (ML) techniques to improve the performance of database systems, e.g., in query optimization or index recommendation. However, in many production deployments, the ML models' performance degrades significantly when the test data diverges from the data used to train these models. In this paper, we address this performance degradation by using B-instances to collect additional data during deployment. We propose an active data collection platform, ADCP, that employs active learning (AL) to gather relevant data cost-effectively. We develop a novel AL technique, Holistic Active Learner (HAL), that robustly combines multiple noisy signals for data gathering in the context of database applications. HAL applies to various ML tasks, budget sizes, cost types, and budgeting interfaces for database applications. We evaluate ADCP on both industry-standard benchmarks and real customer workloads. Our evaluation shows that, compared with other baselines, our technique improves ML models' prediction performance by up to 2x with the same cost budget. In particular, on production workloads, our technique reduces the prediction error of ML models by 75% using about 100 additionally collected queries.",SIGMOD Conference,2020,10.1145/3318464.3389768,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
b1f1abbcd2e4c478f259b3a8e9ff0b0f26d0759b,https://www.semanticscholar.org/paper/b1f1abbcd2e4c478f259b3a8e9ff0b0f26d0759b,An Agnostic Data-Driven Approach to Predict Stoppages of Industrial Packing Machine in Near,"As data awareness in manufacturing companies increases with the deployment of sensors and Internet of Things (IoT) devices, data-driven maintenance and prediction have become quite popular in the Industry 4.0 paradigm. Machine Learning (ML) has been recognised as a promising, efficient and reliable tool for fault detection use cases, as it allows to export important knowledge from monitored assets. Scientists deal with issues such as the small amount of data that indicate potential problems, or the imbalance which exists between the standard process data and the data inadequacy of the systems to make a high precision forecast. Currently, in this context, even large industries are not able to effectively predict abnormal behaviors in their tools, processes and equipment, when adopting strategies to anticipate crucial events. In this paper, we propose a methodology to enable prediction of a packing machine’s stoppages in manufacturing process of a large industry, by using forecasting techniques based on univariate time series data. There are more than 100 reasons that cause the machine to stop, in a quite big production line length. However, we use a single signal, concerning the machines operational status to make our prediction, without considering other fault or warning signals, hence its characterization as ""agnostic"". A workflow is presented for cleaning and preprocessing the data, and for training and evaluating a predictive model. Two predictive models, namely ARIMA and Prophet, are applied and evaluated on real data from an advanced machining process used for packing. Training and evaluation tests indicate that the results of the applied methods perform well on a daily basis. Our work can be further extended and act as reference for future research activities that could lead to more robust and accurate prediction frameworks.",2020 16th International Conference on Distributed Computing in Sensor Systems (DCOSS),2020,10.1109/DCOSS49796.2020.00046,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
09405e2a3bb4d874f4780f7464ba68bde8d2da4a,https://www.semanticscholar.org/paper/09405e2a3bb4d874f4780f7464ba68bde8d2da4a,Conceptual Thinking in Statistics and Data Science Education: Interactive Formative Assessment with Meaning Equivalence Reusable Learning Objects (MERLO),"Computer age statistics, machine learning, data science and in general, data analytics, are having an ubiquitous impact on industry, business and services. Deploying a data transformation strategy requires a workforce which is up to the job in terms of knowledge, experience and capabilities. The application of analytics needs to address organizational needs, invoke proper methods, build on adequate infrastructures and ensure availability of the right skills to the right people. such upskilling requires a focus on conceptual understanding affecting both the pedagogical approach and the complementary learning assessment tools, This paper is about the application of advanced educational concepts to the teaching and evaluation of statistical and data science related concepts. Two educational elements will be included in the discussion: i) the use of simulations to facilitate problem based experiential learning and ii) an emphasis on information quality, as the overall objective of statistics and data science activity. 
 
We begin with an introduction to conceptual thinking and meaning equivalence and the application of Meaning Equivalence Reusable Learning Objects (MERLO) in the classroom. We then describe how MERLO and concept science can be applied in the domain of Statistics and Data Science. Section 3 is about the use of simulations in statistical education. In section 4 we discuss practical aspects of an education program focused on conceptual understanding. Section 5 is on the conceptual mapping of information quality as a MERLO infrastructure. The paper concludes with a discussion.",SSRN Electronic Journal,2021,10.2139/ssrn.3862006,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
29299e92232ea21e46c2a66e45cad0ecfde09d90,https://www.semanticscholar.org/paper/29299e92232ea21e46c2a66e45cad0ecfde09d90,Industrializing Intelligence: A Political Economic History of the AI Industry,,"Marx, Engels, and Marxisms",2021,10.1007/978-3-030-71689-9_4,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
487a2f0fda790ff61c69331a3d69c8d1f3a80488,https://www.semanticscholar.org/paper/487a2f0fda790ff61c69331a3d69c8d1f3a80488,Recent Scope for AI in the Food Production Industry Leading to the Fourth Industrial Revolution,"In today's situation, Artificial intelligence and computer vision collectively join together to analyze the big data obtained from predicted models. The role of AI in the agri-based food industry helps the stakeholders to access and monitor the supply chain. The phenomenon of applying AI and computer vision in the food industry would improve the entire operations. This research paper tries to provide an assisting model for farmers in food-processing and agriculture through the state-of-the-art method. Several concepts related to sustainability in food processing have been estimated through machine learning, and the deep learning model as a worldwide concept. The demand for the usage of AI and computer vision in the Ag-TECH industry has increased which impacts sustainable food production to feed the future. Certain implications have been suggested for real-time monitoring of the farming process, politics behind sustainable food production, and investment which is the main game-player in the present situation. The 4th Industrial Revolution [IR-4.0] was ushered in by the deployment of computer vision and AI in the food business, with computer vision robotics playing a crucial role in ensuring sustainable food production.",Webology,2021,10.14704/web/v18i2/web18375,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
25ee6b6cffcc3ff756bc53c30c3400d44d0fff1a,https://www.semanticscholar.org/paper/25ee6b6cffcc3ff756bc53c30c3400d44d0fff1a,Industry First AI-Powered Fully Automated Safety Observation System Deployed to Global Offshore Fleet,"
 As the oil and gas industry is facing tumultuous challenges, adoption of cutting-edge digital technologies has been accelerated to deliver safer, more efficient operations with less impact on the environment.
 While advanced AI and other digital technologies have been rapidly evolving in many fields in the industry, the HSE sector is playing catch-up. With the increasing complexity of risks and safety management processes, the effective application of data-driven technologies has become significantly harder, particularly for international organizations with varying levels of digital readiness across diverse global operations. Leaders are more cautious to implement solutions that are not fit-for purpose, due to concerns over inconsistencies in rolling out the program across international markets and the impact this may have on ongoing operations.
 This paper describes how the effective application of Artificial intelligence (AI) and Machine Learning (ML) technologies have been used to engineer a solution that fully digitizes and automates the end-to-end offshore behavior-based safety program across a global offshore fleet; optimizing a critical safety process used by many leading oil & gas organization to drive positive workplace safety culture. The complex safety program has been transformed into clear, efficient and automated workflow, with real-time analytics and live transparent dashboards which detail critical safety indicators in real time, aiding decision-making and improving operational performance.
 The novel behavior-based safety digital solution, referred to as 3C observation tool within Noble drilling, has been built to be fully aligned with the organization's safety management system requirements and procedures, using modern and agile tools and applications for fully scalability and easy deployment. It has been critical in sharpening the offshore safety observation program across global operations, resulting in a boost of the workforce engagement by 30%, and subsequently increasing safety awareness skill set attainment; improving overall offshore safety culture, all while reducing operating costs by up to 70% and cutting carbon footprint through the elimination of 15,000 manhours and half a million paper cards each year, when compared to previously used methods and workflows","Day 4 Fri, September 10, 2021",2021,10.2118/205465-ms,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
15e13c9567fad5e8db9ecceedb963ed512cba90f,https://www.semanticscholar.org/paper/15e13c9567fad5e8db9ecceedb963ed512cba90f,Baidu Apollo Auto-Calibration System - An Industry-Level Data-Driven and Learning based Vehicle Longitude Dynamic Calibrating Algorithm,"For any autonomous driving vehicle, control module determines its road performance and safety, i.e. its precision and stability should stay within a carefully-designed range. Nonetheless, control algorithms require vehicle dynamics (such as longitudinal dynamics) as inputs, which, unfortunately, are obscure to calibrate in real time. As a result, to achieve reasonable performance, most, if not all, research-oriented autonomous vehicles do manual calibrations in a one-by-one fashion. Since manual calibration is not sustainable once entering into mass production stage for industrial purposes, we here introduce a machine-learning based auto-calibration system for autonomous driving vehicles. In this paper, we will show how we build a data-driven longitudinal calibration procedure using machine learning techniques. We first generated offline calibration tables from human driving data. The offline table serves as an initial guess for later uses and it only needs twenty-minutes data collection and process. We then used an online-learning algorithm to appropriately update the initial table (the offline table) based on real-time performance analysis. This longitudinal auto-calibration system has been deployed to more than one hundred Baidu Apollo self-driving vehicles (including hybrid family vehicles and electronic delivery-only vehicles) since April 2018. By August 27, 2018, it had been tested for more than two thousands hours, ten thousands kilometers (6,213 miles) and yet proven to be effective.",ArXiv,2018,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
183fc8eba11c4a30f902aa34d2d8ad108bd3bb14,https://www.semanticscholar.org/paper/183fc8eba11c4a30f902aa34d2d8ad108bd3bb14,Accelerated AI Inference at CNN-Based Machine Vision in ASICs: A Design Approach,"Machine Vision (MV) has become an increasingly popular technology in Industry 4.0 for advanced manufacturing processes. Specially, assembly lines require fast MV of the machine objects, assembly-functions, and operations to improve accuracy and quality. In order to achieve this, MV deployed customized Application Specific Integrated Circuits (ASICs). These ASICs designed to run Convolution Neural Network (CNNs) and perform AI inference. The processing needs high performance computing (HPC) of a large number of tensors with least energy spending in the ASICs. In the paper, we propose a design approach for efficient tensor processing in tensor cores for accelerated AI inference at CNN based MV in ASICs. The design approach deploys the small DNN (Deep Learning Neural Network) model SqueezeNext along with application of quantization, fast arithmetic reduction, tensor cores aware tuning, pruning, and fusion for the efficient AI inference in ASICs. Successful implementation of proposed design can provide a competitive advantage to industries in terms of quality and cost of product along with time saving in manufacturing process.",ECS Transactions,2022,10.1149/10701.5165ecst,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
e857721185318f846d64adcf5e067b6dc6c5893e,https://www.semanticscholar.org/paper/e857721185318f846d64adcf5e067b6dc6c5893e,CellDefectNet: A Machine-designed Attention Condenser Network for Electroluminescence-based Photovoltaic Cell Defect Inspection,"—Photovoltaic cells are electronic devices that convert light energy to electricity, forming the backbone of solar energy harvesting systems. An essential step in the manufacturing process for photovoltaic cells is visual quality inspection using electroluminescence imaging to identify defects such as cracks, ﬁnger interruptions, and broken cells. A big challenge faced by industry in photovoltaic cell visual inspection is the fact that it is currently done manually by human inspectors, which is extremely time consuming, laborious, and prone to human error. While deep learning approaches holds great potential to automating this inspection, the hardware resource-constrained manufac- turing scenario makes it challenging for deploying complex deep neural network architectures. In this work, we introduce CellDefectNet, a highly efﬁcient attention condenser network designed via machine-driven design exploration speciﬁcally for electroluminesence-based photovoltaic cell defect detection on the edge. We demonstrate the efﬁcacy of CellDefectNet on a benchmark dataset comprising of a diversity of photovoltaic cells captured using electroluminescence imagery, achieving an accuracy of ∼ 86.3% while possessing just 410K parameters ( ∼ 13 × lower than EfﬁcientNet-B0, respectively) and ∼ 115M FLOPs ( ∼ 12 × lower than EfﬁcientNet-B0) and ∼ 13 × faster on an ARM Cortex A-72 embedded processor when compared to EfﬁcientNet-B0.",ArXiv,2022,10.48550/arXiv.2204.11766,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
8125830fd1aca4ceb193715acc2284aef5839de2,https://www.semanticscholar.org/paper/8125830fd1aca4ceb193715acc2284aef5839de2,A Markerless Deep Learning-based 6 Degrees of Freedom Pose Estimation for Mobile Robots using RGB Data,"Augmented Reality has been subject to various integration efforts within industries due to its ability to enhance human machine interaction and understanding. Neural networks have achieved remarkable results in areas of computer vision, which bear great potential to assist and facilitate an enhanced Augmented Reality experience. However, most neural networks are computationally intensive and demand huge processing power, thus are not suitable for deployment on Augmented Reality devices. In this work, we propose a method to deploy state of the art neural networks for real time 3D object localization on augmented reality devices. As a result, we provide a more automated method of calibrating the AR devices with mobile robotic systems. To accelerate the calibration process and enhance user experience, we focus on fast 2D detection approaches which are extracting the 3D pose of the object fast and accurately by using only 2D input. The results are implemented into an Augmented Reality application for intuitive robot control and sensor data visualization. For the 6D annotation of 2D images, we developed an annotation tool, which is, to our knowledge, the first open source tool to be available. We achieve feasible results which are generally applicable to any AR device, thus making this work promising for further research in combining high demanding neural networks with Internet of Things devices.",2020 17th International Conference on Ubiquitous Robots (UR),2020,10.1109/UR49135.2020.9144789,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
f2758f140a8611472de612e8722a4ac5e46d37ea,https://www.semanticscholar.org/paper/f2758f140a8611472de612e8722a4ac5e46d37ea,Multi-Agent Team Learning in Virtualized Open Radio Access Networks (O-RAN),"Starting from the concept of the Cloud Radio Access Network (C-RAN), continuing with the virtual Radio Access Network (vRAN) and most recently with the Open RAN (O-RAN) initiative, Radio Access Network (RAN) architectures have significantly evolved in the past decade. In the last few years, the wireless industry has witnessed a strong trend towards disaggregated, virtualized and open RANs, with numerous tests and deployments worldwide. One unique aspect that motivates this paper is the availability of new opportunities that arise from using machine learning, more specifically multi-agent team learning (MATL), to optimize the RAN in a closed-loop where the complexity of disaggregation and virtualization makes well-known Self-Organized Networking (SON) solutions inadequate. In our view, Multi-Agent Systems (MASs) with MATL can play an essential role in the orchestration of O-RAN controllers, i.e., near-real-time and non-real-time RAN Intelligent Controllers (RIC). In this article, we first provide an overview of the landscape in RAN disaggregation, virtualization and O-RAN, then we present the state-of-the-art research in multi-agent systems and team learning as well as their application to O-RAN. We present a case study for team learning where agents are two distinct xApps: power allocation and radio resource allocation. We demonstrate how team learning can enhance network performance when team learning is used instead of individual learning agents. Finally, we identify challenges and open issues to provide a roadmap for researchers in the area of MATL based O-RAN optimization.",Sensors,2022,10.3390/s22145375,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
e8cfafb96834af64983b5641543950c6e9d336ec,https://www.semanticscholar.org/paper/e8cfafb96834af64983b5641543950c6e9d336ec,Automatic Anomaly Detection on In-Production Manufacturing Machines Using Statistical Learning Methods,"Anomaly detection is becoming increasingly important to enhance reliability and resiliency in the Industry 4.0 framework. In this work, we investigate different methods for anomaly detection on in-production manufacturing machines taking into account their variability, both in operation and in wear conditions. We demonstrate how the nature of the available data, featuring any anomaly or not, is of importance for the algorithmic choice, discussing both statistical machine learning methods and control charts. We finally develop methods for automatic anomaly detection, which obtain a recall close to one on our data. Our developed methods are designed not to rely on a continuous recalibration and hand-tuning by the machine user, thereby allowing their deployment in an in-production environment robustly and efficiently.",Sensors,2020,10.3390/s20082344,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
952ec800bb69063afb0abba7c1f446d21a9475d1,https://www.semanticscholar.org/paper/952ec800bb69063afb0abba7c1f446d21a9475d1,Modular approach to data preprocessing in ALOHA and application to a smart industry use case,"Applications in the smart industry domain, such as interaction with collaborative robots using vocal commands or machine vision systems often requires the deployment of deep learning algorithms on heterogeneous low power computing platforms. The availability of software tools and frameworks to automatize different design steps can support the effective implementation of DL algorithms on embedded systems, reducing related effort and costs. One very important aspect for the acceptance of the framework, is its extensibility, i.e. the capability to accommodate different datasets and define customized preprocessing, without requiring advanced skills. The paper addresses a modular approach, integrated into the ALOHA 1 tool flow, to support the data preprocessing and transformation pipeline. This is realized through customizable plugins and allows the easy extension of the tool flow to encompass new use cases. To demonstrate the effectiveness of the approach, we present some experimental results related to a keyword spotting use case and we outline possible extensions to different use cases. Keywords— Deep Learning, flows and tools, computer-aided design, edge computing.",ArXiv,2021,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
9651beaab778de960239e82bbb51396c50e77b17,https://www.semanticscholar.org/paper/9651beaab778de960239e82bbb51396c50e77b17,Machine monitoring system: a decade in review,,,2020,10.1007/s00170-020-05620-3,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
cb44a59f95d08dc729f8152c71aa999cde46fe2e,https://www.semanticscholar.org/paper/cb44a59f95d08dc729f8152c71aa999cde46fe2e,Neural Auction: End-to-End Learning of Auction Mechanisms for E-Commerce Advertising,"In e-commerce advertising, it is crucial to jointly consider various performance metrics, e.g., user experience, advertiser utility, and platform revenue. Traditional auction mechanisms, such as GSP and VCG auctions, can be suboptimal due to their fixed allocation rules to optimize a single performance metric (e.g., revenue or social welfare). Recently, data-driven auctions, learned directly from auction outcomes to optimize multiple performance metrics, have attracted increasing research interests. However, the procedure of auction mechanisms involves various discrete calculation operations, making it challenging to be compatible with continuous optimization pipelines in machine learning. In this paper, we design Deep Neural Auctions (DNAs) to enable end-to-end auction learning by proposing a differentiable model to relax the discrete sorting operation, a key component in auctions. We optimize the performance metrics by developing deep models to efficiently extract contexts from auctions, providing rich features for auction design. We further integrate the game theoretical conditions within the model design, to guarantee the stability of the auctions. DNAs have been successfully deployed in the e-commerce advertising system at Taobao. Experimental evaluation results on both large-scale data set as well as online A/B test demonstrated that DNAs significantly outperformed other mechanisms widely adopted in industry.",KDD,2021,10.1145/3447548.3467103,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
22b525b91c0f0847cb53f583fca5a7125075c881,https://www.semanticscholar.org/paper/22b525b91c0f0847cb53f583fca5a7125075c881,"Dispersed Federated Learning: Vision, Taxonomy, and Future Directions","The ongoing deployments of the Internet of Things (IoT)-based smart applications are spurring the adoption of machine learning as a key technology enabler. To overcome the privacy and overhead challenges of centralized machine learning, there has been significant recent interest in the concept of federated learning. Federated learning offers on-device machine learning without the need to transfer end-device data to a third party location. However, federated learning has robustness concerns because it might stop working due to a failure of the aggregation server (e.g., due to a malicious attack or physical defect). Furthermore, federated learning over IoT networks requires a significant amount of communication resources for training. To cope with these issues, we propose a novel framework of dispersed federated learning (DFL) that is based on true decentralization. We opine that DFL will serve as a practical implementation of federated learning for various IoT-based smart applications such as smart industries and intelligent transportation systems. First, the fundamentals of the DFL are presented. Second, a taxonomy is devised with a qualitative analysis of various DFL schemes. Third, a DFL framework for IoT networks is proposed with a matching theory-based solution. Finally, an outlook on future research directions is presented.",IEEE Wireless Communications,2020,10.1109/mwc.011.2100003,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
b58634ff95ff769e54e35fedc0b4bbcfbdcba1b8,https://www.semanticscholar.org/paper/b58634ff95ff769e54e35fedc0b4bbcfbdcba1b8,A Decision Model for Federated Learning Architecture Pattern Selection,". Federated learning is growing fast in both academia and industry to resolve data hungriness and privacy issues in machine learning. A federated learning system being widely distributed with diﬀerent components and stakeholders requires software system design thinking. For instance, multiple patterns and tactics have been summarised by researchers that cover various aspects, from client management, training conﬁguration, model deployment, etc. However, the multitude of patterns leaves the designers confused about when and which pattern to adopt or adapt. Therefore, in this paper, we present a set of decision models to assist designers and architects who have limited knowledge in federated learning, in selecting architectural patterns for federated learning architecture design. Each decision model maps functional and non-functional requirements of federated learning systems to a set of patterns. we also clarify the trade-oﬀs that may be implicit in the patterns. We evaluated the decision model through a set of interviews with practitioners to assess the correctness and usefulness in guiding the architecture design process through various design decision options.",ArXiv,2022,10.48550/arXiv.2204.13291,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
92de8faa2c1a2b32d5d9f97cac83fbb77664f66e,https://www.semanticscholar.org/paper/92de8faa2c1a2b32d5d9f97cac83fbb77664f66e,A Decision Model for Federated Learning Architecture Pattern Selection,". Federated learning is growing fast in both academia and industry to resolve data hungriness and privacy issues in machine learning. A federated learning system being widely distributed with diﬀerent components and stakeholders requires software system design thinking. For instance, multiple patterns and tactics have been summarised by researchers that cover various aspects, from client management, training conﬁguration, model deployment, etc. However, the multitude of patterns leaves the designers confused about when and which pattern to adopt or adapt. Therefore, in this paper, we present a set of decision models to assist designers and architects who have limited knowledge in federated learning, in selecting architectural patterns for federated learning architecture design. Each decision model maps functional and non-functional requirements of federated learning systems to a set of patterns. we also clarify the trade-oﬀs that may be implicit in the patterns. We evaluated the decision model through a set of interviews with practitioners to assess the correctness and usefulness in guiding the architecture design process through various design decision options.",,2022,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
d7ddab13f66472e768986e31115fefc957195edf,https://www.semanticscholar.org/paper/d7ddab13f66472e768986e31115fefc957195edf,A Federated Learning-Based Crop Yield Prediction for Agricultural Production Risk Management,"A spiralling global population and changing dietary needs have scaled up the demand for food and raw materials supplied to the industry. The agricultural production is struggling to keep up the level of required crop yields due to numerous risks affecting the yield. The past two decades have witnessed the increased intensity of agricultural production risks due to challenges posed by climate changes. There is a dire need to address it with proper insights into the data attributes impacting the crop yield. Currently, many of the machine learning and deep learning methods focus on training the model using the data collected and stored in a centralized data repository. However, many attributes related to weather data, soil data and crop management data are scattered and siloed to particular organization servers or smart farming devices. In this study, we are proposing a federated learning method for training yield prediction models on a horizontally distributed dataset located on different client devices. In particular, federated averaging algorithm is used to train the deep residual network based regression models such as ResNet-16 and ResNet-28 for soybean yield prediction in a decentralized setting and compare its performance with deep learning and machine learning methods. The results from experimented learning models show that federated averaging using ResNet-16 regression model with Adam optimizer yielded optimal results compared to centralized learning models and can be easily deployed for yield prediction in a federated setting.",2022 IEEE Delhi Section Conference (DELCON),2022,10.1109/DELCON54057.2022.9752836,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
192bac40d1b56d8d3fe1ff5f6d16b56f324da19a,https://www.semanticscholar.org/paper/192bac40d1b56d8d3fe1ff5f6d16b56f324da19a,Wireless Power Transfer Sensing Approach for Milk Adulteration Detection Using Supervised Learning,"With the increasing demand for wireless sensors due to the growing Internet of Things (IoT) industry, it becomes desirable to use existing technologies to realize new sensing functions. As wireless power transfer (WPT) becomes a standard feature in smartphones, this paper studies the non-invasive classification of liquid solutions with different concentrations, based on the WPT technology already deployed in mobile devices. Average accuracies of up to 97.6% were achieved utilizing supervised machine learning for the classification of milk adulterated with different water volumes. For these experiments, milk concentrations of 100%, 80%, 60%, and 40% were used for classification. Additionally, singular value decomposition and boxplot analysis were used to reduce the radio frequency bandwidth needed for classification to 9.45 MHz, leading to a drastic reduction in hardware complexity and computational cost.",2022 IEEE Radio and Wireless Symposium (RWS),2022,10.1109/rws53089.2022.9719981,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
5a5c88eda3141cb068346c9c321b2ced0957f959,https://www.semanticscholar.org/paper/5a5c88eda3141cb068346c9c321b2ced0957f959,Scene setter presentation: Why does innovation and technology matter to the industry’s future?,"Technology is an enabler, not a metaphysical abstraction that will resolve the energy transition. Many of the technologies our industry is working on could help us reduce potential environmental impacts, advance decarbonisation, and improve efficiency. Some cost-saving examples of innovation include centralising and automating work planning and monitoring in well operations, oil and gas production, and refining through to using Robotic Process Automation to automate manual processes. Revenues can be enhanced by using machine learning and advanced analytics to optimise entire systems from the reservoir through to refineries, enabling intervention ahead of predicted equipment failure, optimising supply and demand functions through trading and shipping. Huge investment is also being placed in carbon reduction technology like satellite imaging and drones. We are finally seeing some momentum in CCUS and hydrogen. Digital technologies enable everything we do, particularly because they can be deployed relatively quickly and low cost. But we need to consider a few things when we consider technology. First, we cannot afford to wait for game-changing technologies to solve climate change. Second, the Paris goals can be met with technologies that exist today, albeit not at the price point we would ideally like. The challenge is pace of deployment, supported by customer demand and regulation. A technology-inclusive approach to policymaking can ensure they are able to use the broadest possible range of emission mitigation solutions possible. By picking winners early in the race and focusing public support on a small number of technologies, we are concentrating risk and increasing chances of failure. To view the video, click on the link to the right.",The APPEA Journal,2022,10.1071/aj21456,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
ac1eeed21f7b7a1a3294141be81591a44f8c3dcb,https://www.semanticscholar.org/paper/ac1eeed21f7b7a1a3294141be81591a44f8c3dcb,Empirically Evaluating Meta Learning of Robot Explainability with Humans,"—As physically-embodied robots and digital assistants are deployed in the real world, these agents must be able to communicate their decision-making criteria to build trust, improve human-robot teaming, and enable collaboration. While the ﬁeld of explainable machine learning has made great strides in building a set of mechanisms to enable such communication, these advancements often assume that one approach is ideally suited to one problem (e.g., decision trees are best for explaining how to triage patients in an emergency room), failing to recognize that individual users may have different past experiences or preferences. In this work, we present the design of a user study to evaluate a novel approach to personalization of robot explainability through meta-learning with humans. Our study will be the ﬁrst to evaluate meta learning with humans in the loop and with multiple approaches to robot explainability. Our results will help to pave the way for academic and industry deployments of explainable machine learning to diverse user populations.",,2022,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
1def29b65d295562bcc4e6e21d02a962531a5a8b,https://www.semanticscholar.org/paper/1def29b65d295562bcc4e6e21d02a962531a5a8b,Case-study Led Investigation of Explainable AI (XAI) to Support Deployment of Prognostics in the industry,"Civil nuclear generation plant must maximise it’s operational uptime in order to maintain it’s viability. With aging plant and heavily regulated operating constraints, monitoring is commonplace, but identifying health indicators to pre-empt disruptive faults is challenging owing to the volumes of data involved. Machine learning (ML) models are increasingly deployed in prognostics and health management (PHM) systems in various industrial applications, however, many of these are black box models that provide good performance but little or no insight into how predictions are reached. In nuclear generation, there is significant regulatory oversight and therefore a necessity to explain decisions based on outputs from predictive models. These explanations can then enable stakeholders to trust these outputs, satisfy regulatory bodies and subsequently make more effective operational decisions. How ML model outputs convey explanations to stakeholders is important, so these explanations must be in human (and technical domain related) understandable terms. Consequently, stakeholders can rapidly interpret, then trust predictions better, and will be able to act on them more effectively. The main contributions of this paper are: 1. introduce XAI into the PHM of industrial assets and provide a novel set of algorithms that translate the explanations produced by SHAP to text-based human-interpretable explanations; and 2. consider the context of these explanations as intended for application to prognostics of critical assets in industrial applications. The use of XAI will not only help in understanding how these ML models work, but also describe the most important features contributing to predicted degradation of the nuclear generation asset.",PHM Society European Conference,2022,10.36001/phme.2022.v7i1.3336,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
6291255977e6ee13a13bb7d024629561e9b99067,https://www.semanticscholar.org/paper/6291255977e6ee13a13bb7d024629561e9b99067,Data Poisoning Attacks to Deep Learning Based Recommender Systems,"Recommender systems play a crucial role in helping users to find their interested information in various web services such as Amazon, YouTube, and Google News. Various recommender systems, ranging from neighborhood-based, associationrule-based, matrix-factorization-based, to deep learning based, have been developed and deployed in industry. Among them, deep learning based recommender systems become increasingly popular due to their superior performance. In this work, we conduct the first systematic study on data poisoning attacks to deep learning based recommender systems. An attacker’s goal is to manipulate a recommender system such that the attacker-chosen target items are recommended to many users. To achieve this goal, our attack injects fake users with carefully crafted ratings to a recommender system. Specifically, we formulate our attack as an optimization problem, such that the injected ratings would maximize the number of normal users to whom the target items are recommended. However, it is challenging to solve the optimization problem because it is a non-convex integer programming problem. To address the challenge, we develop multiple techniques to approximately solve the optimization problem. Our experimental results on three realworld datasets, including small and large datasets, show that our attack is effective and outperforms existing attacks. Moreover, we attempt to detect fake users via statistical analysis of the rating patterns of normal and fake users. Our results show that our attack is still effective and outperforms existing attacks even if such a detector is deployed.",NDSS,2021,10.14722/ndss.2021.24525,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
e8e335a90f8f0b9870b28c147360f2ad612e1039,https://www.semanticscholar.org/paper/e8e335a90f8f0b9870b28c147360f2ad612e1039,Pitching machine intelligence against fi nancial crime,"Introduction Anti-money laundering (AML) transaction monitoring (TM) to identify potentially suspicious transactions is a challenging undertaking for banks and other fi nancial institutions. It is an important tool to stop the funding of terrorist activity, money laundering, as well as other crimes such as human traffi cking and drug distribution. Yet existing AML TM systems and processes have proven to be operationally ineffi cient and often ineffective. Machine intelligence solutions (which includes cognitive computing, artificial intelligence, machine learning, and deep learning (collectively MI)) can enhance existing technologies and processes to detect possible financial crime better. The precise application of these technologies can deliver significant efficiencies without the need to engage in large-scale software upgrades. As a result, leading financial institutions, with industry partners, have started to deploy MI against the complex challenges presented by AML and TM in particular.",,2018,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
db1e90d20e4eda202d08a6107e23898a681f6866,https://www.semanticscholar.org/paper/db1e90d20e4eda202d08a6107e23898a681f6866,Joint Fusion Learning of Multiple Time Series Prediction,"Accurate traffic density estimations is essential for numerous purposes like the developing successful transit policies or to forecast future traffic conditions for navigation. Current developments in the machine learning and computer systems bring the transportation industry numerous possibilities to improve their operations using data analyses on traffic flow sensor data . However, even state-of-art algorithms for time series forecasting perform well on some transportation problems, they still fail to solve some critical tasks. In particular, existing traffic flow forecasting methods that are not utilising causality relations between different data sources are still unsatisfying for many real-world applications . In this report, we have focused on a new method named joint fusion learning that uses underlying causality in time series. We test our method in a very detailed synthetic environment that we specially developed to imitate realworld traffic flow dataset. In the end, we use our joint-fusion learning on a historical traffic flow dataset for Thessaloniki, Greece which is published by Hellenic Institute of Transport (HIT) . We obtained better results on the short-term forecasts compared the widely-used benchmarks models that uses single time series to forecast the future.",,2019,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
e545b981c54b55e01bf52e0c82d8eb589841302d,https://www.semanticscholar.org/paper/e545b981c54b55e01bf52e0c82d8eb589841302d,Systematic trading : calibration advances through machine learning,"Systematic trading in finance uses computer models to define trade goals, risk controls and rules that can execute trade orders in a methodical way. This thesis investigates how performance in systematic trading can be crucially enhanced by both i) persistently reducing the bid-offer spread quoted by the trader through optimized and realistically backtested strategies and ii) improving the out-of-sample robustness of the strategy selected through the injection of theory into the typically data-driven calibration processes. While doing so it brings to the foreground sound scientific reasons that, for the first time to my knowledge, technically underpin popular academic observations about the recent nature of the financial markets. The thesis conducts consecutive experiments across strategies within the three important building blocks of systematic trading: a) execution, b) quoting and c) risk-reward allowing me to progressively generate more complex and accurate backtested scenarios as recently demanded in the literature (Cahan et al. (2010)). The three experiments conducted are: 1. Execution: an execution model based on support vector machines. The first experiment is deployed to improve the realism of the other two. It analyses a popular model of execution: the volume weighted average price (VWAP). The VWAP algorithm targets to split the size of an order along the trading session according to the expected intraday volume's profile since the activity in the markets typically resembles convex seasonality – with more activity around the open and the closing auctions than along the rest of the day. In doing so, the main challenge is to provide the model with a reasonable expected profile. After proving in my data sample that two simple static approaches to the profile overcome the PCA-ARMA from Bialkowski et al. (2008) (a popular two-fold model composed by a dynamic component around an unsupervised learning structure) a further combination of both through an index based on supervised learning is proposed. The Sample Sensitivity Index hence successfully allows estimating the expected volume's profile more accurately by selecting those ranges of time where the model shall be less sensitive to past data through the identification of patterns via support vector machines. Only once the intraday execution risk has been defined can the quoting policy of a mid-frequency (in general, up to a week) hedging strategy be accurately analysed. 2. Quoting: a quoting model built upon particle swarm optimization. The second experiment analyses for the first time to my knowledge how to achieve the disruptive 50% bid-offer spread discount observed in Menkveld (2013) without increasing the risk profile of a trading agent. The experiment depends crucially on a series of variables of which market impact and slippage are typically the most difficult to estimate. By adapting the market impact model in Almgren et al. (2005) to the VWAP developed in the previous experiment and by estimating its slippage through its errors' distribution a framework within which the bid-offer spread can be assessed is generated. First, a full-replication spread, (that set out following the strict definition of a product in order to hedge it completely) is calculated and fixed as a benchmark. Then, by allowing benefiting from a lower market impact at the cost of assuming deviation risk (tracking error and tail risk) a non-full-replication spread is calibrated through particle swarm optimization (PSO) as in Diez et al. (2012) and compared with the benchmark. Finally, it is shown that the latter can reach a discount of a 50% with respect to the benchmark if a certain number of trades is granted. This typically occurs on the most liquid securities. This result not only underpins Menkveld's observations but also points out that there is room for further reductions. When seeking additional performance, once the quoting policy has been defined, a further layer with a calibrated risk-reward policy shall be deployed. 3. Risk-Reward: a calibration model defined within a Q-learning framework. The third experiment analyses how the calibration process of a risk-reward policy can be enhanced to achieve a more robust out-of-sample performance – a cornerstone in quantitative trading. It successfully gives a response to the literature that recently focusses on the detrimental role of overfitting (Bailey et al. (2013a)). The experiment was motivated by the assumption that the techniques underpinned by financial theory shall show a better behaviour (a lower deviation between in-sample and out-of-sample performance) than the classical data-driven only processes. As such, both approaches are compared within a framework of active trading upon a novel indicator. The indicator, called the Expectations' Shift, is rooted on the expectations of the markets' evolution embedded in the dynamics of the prices. The crucial challenge of the experiment is the injection of theory within the calibration process. This is achieved through the usage of reinforcement learning (RL). RL is an area of ML inspired by behaviourist psychology concerned with how software agents take decisions in an specific environment incentivised by a policy of rewards. By analysing the Q-learning matrix that collects the set of state/actions learnt by the agent within the environment, defined by each combination of parameters considered within the calibration universe, the rationale that an autonomous agent would have learnt in terms of risk management can be generated. Finally, by then selecting the combination of parameters whose attached rationale is closest to that of the portfolio manager a data-driven solution that converges to the theory-driven solution can be found and this is shown to successfully outperform out-of-sample the classical approaches followed in Finance. The thesis contributes to science by addressing what techniques could underpin recent academic findings about the nature of the trading industry for which a scientific explanation was not yet given: • A novel agent-based approach that allows for a robust out-of-sampkle performance by crucially providing the trader with a way to inject financial insights into the generally data-driven only calibration processes. It this way benefits from surpassing the generic model limitations present in the literature (Bailey et al. (2013b), Schorfheid and Wolpin (2012), Van Belle and Kerr (2012) or Weiss and Kulikowski (1991)) by finding a point where theory-driven patterns (the trader's priors tend to enhance out-of-sample robustness) merge with data-driven ones (those that allow to exploit latent information). • The provision of a technique that, to the best of my knowledge, explains for the first time how to reduce the bid-offer spread quoted by a traditional trader without modifying her risk appetite. A reduction not previously addressed in the literature in spite of the fact that the increasing regulation against the assumption of risk by market makers (e.g. Dodd–Frank Wall Street Reform and Consumer Protection Act) does yet coincide with the aggressive discounts observed by Menkveld (2013). As a result, this thesis could further contribute to science by serving as a framework to conduct future analyses in the context of systematic trading. • The completion of a mid-frequency trading experiment with high frequency execution information. It is shown how the latter can have a significant effect on the former not only through the erosion of its performance but, more subtly, by changing its entire strategic design (both, optimal composition and parameterization). This tends to be highly disregarded by the financial literature. More importantly, the methodologies disclosed herein have been crucial to underpin the setup of a new unit in the industry, BBVA's Global Strategies & Data Science. This disruptive, global and cross-asset team gives an enhanced role to science by successfully becoming the main responsible for the risk management of the Bank's strategies both in electronic trading and electronic commerce. Other contributions include: the provision of a novel risk measure (flowVaR); the proposal of a novel trading indicator (Expectations’ Shift); and the definition of a novel index that allows to improve the estimation of the intraday volume’s profile (Sample Sensitivity Index).",,2015,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
82c94eb724e69cbfe660b06a491b72af5d4990d4,https://www.semanticscholar.org/paper/82c94eb724e69cbfe660b06a491b72af5d4990d4,Early Prediction of Quality Issues in Automotive Modern Industry,"Many industries today are struggling with early the identification of quality issues, given the shortening of product design cycles and the desire to decrease production costs, coupled with the customer requirement for high uptime. The vehicle industry is no exception, as breakdowns often lead to on-road stops and delays in delivery missions. In this paper we consider quality issues to be an unexpected increase in failure rates of a particular component; those are particularly problematic for the original equipment manufacturers (OEMs) since they lead to unplanned costs and can significantly affect brand value. We propose a new approach towards the early detection of quality issues using machine learning (ML) to forecast the failures of a given component across the large population of units. In this study, we combine the usage information of vehicles with the records of their failures. The former is continuously collected, as the usage statistics are transmitted over telematics connections. The latter is based on invoice and warranty information collected in the workshops. We compare two different ML approaches: the first is an auto-regression model of the failure ratios for vehicles based on past information, while the second is the aggregation of individual vehicle failure predictions based on their individual usage. We present experimental evaluations on the real data captured from heavy-duty trucks demonstrating how these two formulations have complementary strengths and weaknesses; in particular, they can outperform each other given different volumes of the data. The classification approach surpasses the regressor model whenever enough data is available, i.e., once the vehicles are in-service for a longer time. On the other hand, the regression shows better predictive performance with a smaller amount of data, i.e., for vehicles that have been deployed recently.",Inf.,2020,10.3390/info11070354,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
faa6c3cb293ab8c130ab9065e63153a632678d31,https://www.semanticscholar.org/paper/faa6c3cb293ab8c130ab9065e63153a632678d31,Identifying Attack Surfaces in the Evolving Space Industry Using Reference Architectures,"The space environment is currently undergoing a substantial change and many new entrants to the market are deploying devices, satellites and systems in space; this evolution has been termed as NewSpace. The change is complicated by technological developments such as deploying machine learning based autonomous space systems and the Internet of Space Things (IoST). In the IoST, space systems will rely on satellite-to-x communication and interactions with wider aspects of the ground segment to a greater degree than existing systems. Such developments will inevitably lead to a change in the cyber security threat landscape of space systems. Inevitably, there will be a greater number of attack vectors for adversaries to exploit, and previously infeasible threats can be realised, and thus require mitigation. In this paper, we present a reference architecture (RA) that can be used to abstractly model in situ applications of this new space landscape. The RA specifies high-level system components and their interactions. By instantiating the RA for two scenarios we demonstrate how to analyse the attack surface using attack trees.",2020 IEEE Aerospace Conference,2020,10.1109/AERO47225.2020.9172785,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
a6c12c9a12f3dacad738d8298358166b15f6e74f,https://www.semanticscholar.org/paper/a6c12c9a12f3dacad738d8298358166b15f6e74f,Estimation of Andrographolides and Gradation of Andrographis paniculata Leaves Using Near Infrared Spectroscopy Together With Support Vector Machine,"Andrographis paniculata (Burm. F) Nees, has been widely used for upper respiratory tract and several other diseases and general immunity for a historically long time in countries like India, China, Thailand, Japan, and Malaysia. The vegetative productivity and quality with respect to pharmaceutical properties of Andrographis paniculata varies considerably across production, ecologies, and genotypes. Thus, a field deployable instrument, which can quickly assess the quality of the plant material with minimal processing, would be of great use to the medicinal plant industry by reducing waste, and quality grading and assurance. In this paper, the potential of near infrared reflectance spectroscopy (NIR) was to estimate the major group active molecules, the andrographolides in Andrographis paniculata, from dried leaf samples and leaf methanol extracts and grade the plant samples from different sources. The calibration model was developed first on the NIR spectra obtained from the methanol extracts of the samples as a proof of concept and then the raw ground samples were estimated for gradation. To grade the samples into three classes: good, medium and poor, a model based on a machine learning algorithm - support vector machine (SVM) on NIR spectra was built. The tenfold classification results of the model had an accuracy of 83% using standard normal variate (SNV) preprocessing.",Frontiers in Pharmacology,2021,10.3389/fphar.2021.629833,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
892ec27495b7e78a3ff34a26170f4bf07117628a,https://www.semanticscholar.org/paper/892ec27495b7e78a3ff34a26170f4bf07117628a,A Plastic Contamination Image Dataset for Deep Learning Model Development and Training,"The removal of plastic contamination in cotton lint is an issue of top priority for the U.S. cotton industry. One of the main sources of plastic contamination appearing in marketable cotton bales is plastic used to wrap cotton modules on cotton harvesters. To help mitigate plastic contamination at the gin, automatic inspection systems are needed to detect and control removal systems. Due to significant cost constraints in the U.S. cotton ginning industry, the use of low-cost color cameras for detection of plastic contamination has been successfully adopted. However, some plastics of similar color to background are difficult to detect when utilizing traditional machine learning algorithms. Hence, current detection/removal system designs are not able to remove all plastics and there is still a need for better detection methods. Recent advances in deep learning convolutional neural networks (CNNs) show promise for enabling the use of low-cost color cameras for detection of objects of interest when placed against a background of similar color. They do this by mimicking the human visual detection system, focusing on differences in texture rather than color as the primary detection paradigm. The key to leveraging the CNNs is the development of extensive image datasets required for training. One of the impediments to this methodology is the need for large image datasets where each image must be annotated with bounding boxes that surround each object of interest. As this requirement is labor-intensive, there is significant value in these image datasets. This report details the included image dataset as well as the system design used to collect the images. For acquisition of the image dataset, a prototype detection system was developed and deployed into a commercial cotton gin where images were collected for the duration of the 2018–2019 ginning season. A discussion of the observational impact that the system had on reduction of plastic contamination at the commercial gin, utilizing traditional color-based machine learning algorithms, is also included.",AgriEngineering,2020,10.3390/agriengineering2020021,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
84e88786fdaa33adc4bf239339373094b26ce38d,https://www.semanticscholar.org/paper/84e88786fdaa33adc4bf239339373094b26ce38d,A deep learning approach for detecting drill bit failures from a small sound dataset,,Scientific reports,2022,10.1038/s41598-022-13237-7,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
61d9307b4d0147d6c0f706c2ff93d29ef268a244,https://www.semanticscholar.org/paper/61d9307b4d0147d6c0f706c2ff93d29ef268a244,AIBench: An Industry Standard AI Benchmark Suite from Internet Services,"The booming successes of machine learning in different domains boost industry-scale deployments of innovative AI algorithms, systems, and architectures, and thus the importance of benchmarking grows. However, the confidential nature of the workloads, the paramount importance of the representativeness and diversity of benchmarks, and the prohibitive cost of training a state-of-the-art model mutually aggravate the AI benchmarking challenges. In this paper, we present a balanced AI benchmarking methodology for meeting the subtly different requirements of different stages in developing a new system/architecture and ranking/purchasing commercial off-the-shelf ones. Performing an exhaustive survey on the most important AI domain-Internet services with seventeen industry partners, we identify and include seventeen representative AI tasks to guarantee the representativeness and diversity of the benchmarks. Meanwhile, for reducing the benchmarking cost, we select a benchmark subset to a minimum-three tasks-according to the criteria: diversity of model complexity, computational cost, and convergence rate, repeatability, and having widely-accepted metrics or not. We contribute by far the most comprehensive AI benchmark suite-AIBench. The evaluations show AIBench outperforms MLPerf in terms of the diversity and representativeness of model complexity, computational cost, convergent rate, computation and memory access patterns, and hotspot functions. With respect to the AIBench full benchmarks, its subset shortens the benchmarking cost by 41%, while maintaining the primary workload characteristics. The specifications, source code, and performance numbers are publicly available from the web site this http URL.",ArXiv,2020,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
79c0f009d5b75c874392df69fc3c391d6a76c106,https://www.semanticscholar.org/paper/79c0f009d5b75c874392df69fc3c391d6a76c106,SafetyNet: Safe planning for real-world self-driving vehicles using machine-learned policies,"In this paper we present the first safe system for full control of self-driving vehicles trained from human demonstrations and deployed in challenging, real-world, urban environments. Current industry-standard solutions use rulebased systems for planning. Although they perform reasonably well in common scenarios, the engineering complexity renders this approach incompatible with human-level performance. On the other hand, the performance of machine-learned (ML) planning solutions can be improved by simply adding more exemplar data. However, ML methods cannot offer safety guarantees and sometimes behave unpredictably. To combat this, our approach uses a simple yet effective rule-based fallback layer that performs sanity checks on an ML planner’s decisions (e.g. avoiding collision, assuring physical feasibility). This allows us to leverage ML to handle complex situations while still assuring the safety, reducing ML planner-only collisions by 95%. We train our ML planner on 300 hours of expert driving demonstrations using imitation learning and deploy it along with the fallback layer in downtown San Francisco, where it takes complete control of a real vehicle and navigates a wide variety of challenging urban driving scenarios.",ICRA,2021,10.1109/icra46639.2022.9811576,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
5095b7019ab1adea2d30abc8a55d380241fd5368,https://www.semanticscholar.org/paper/5095b7019ab1adea2d30abc8a55d380241fd5368,Scheduling Policies for Federated Learning in Wireless Networks: An Overview,"Due to the increasing need for massive data analysis and machine learning model training at the network edge, as well as the rising concerns about data privacy, a new distrib⁃ uted training framework called federated learning (FL) has emerged and attracted much at⁃ tention from both academia and industry. In FL, participating devices iteratively update the local models based on their own data and contribute to the global training by uploading mod⁃ el updates until the training converges. Therefore, the computation capabilities of mobile de⁃ vices can be utilized and the data privacy can be preserved. However, deploying FL in re⁃ source-constrained wireless networks encounters several challenges, including the limited energy of mobile devices, weak onboard computing capability, and scarce wireless band⁃ width. To address these challenges, recent solutions have been proposed to maximize the convergence rate or minimize the energy consumption under heterogeneous constraints. In this overview, we first introduce the backgrounds and fundamentals of FL. Then, the key challenges in deploying FL in wireless networks are discussed, and several existing solu⁃ tions are reviewed. Finally, we highlight the open issues and future research directions in FL scheduling.",,2020,10.12142/ZTECOM.202002003,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
8e4e5d7369fa701bd9d852694c77d7ca9a31f1df,https://www.semanticscholar.org/paper/8e4e5d7369fa701bd9d852694c77d7ca9a31f1df,Intelligent Robot for Worker Safety Surveillance: Deep Learning Perception and Visual Navigation,"The fatal injury rate for the construction industry is higher than the average for all industries. Recently, researchers have shown an increased interest in occupational safety in the construction industry. However, all the current methods using conventional machine learning with stationary cameras suffer from some severe limitations, perceptual aliasing (e.g., different places/objects can appear identical), occlusion (e.g., place/object appearance changes between visits), seasonal / illumination changes, significant viewpoint changes, etc. This paper proposes a perception module using end-to-end deep-learning and visual SLAM (Simultaneous Localization and Mapping) for an effective and efficient object recognition and navigation using a differential-drive mobile robot. Various deep-learning frameworks and visual navigation strategies with evaluation metrics are implemented and validated for the selection of the best model. The deep-learning model's predictions are evaluated via the metrics (model speed, accuracy, complexity, precision, recall, P-R curve, F1 score). The YOLOv3 shows the best trade-off among all algorithms, 57.9% mean average precision (mAP), in real-world settings, and can process 45 frames per second (FPS) on NVIDIA Jetson TX2 which makes it suitable for real-time detection, as well as a right candidate for deploying the neural network on a mobile robot. The evaluation metrics used for the comparison of laser SLAM are Root Mean Square Error (RMSE). The Google Cartographer SLAM shows the lowest RMSE and acceptable processing time. The experimental results demonstrate that the perception module can meet the requirements of head protection criteria in Occupational Safety and Health Administration (OSHA) standards for construction. To be more precise, this module can effectively detect construction worker's non-hardhat-use in different construction site conditions and can facilitate improved safety inspection and supervision.",2020 International Conference on Advanced Robotics and Intelligent Systems (ARIS),2020,10.1109/ARIS50834.2020.9205772,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
c230fe3f32c39d777b8a92e535b09eb42f96e092,https://www.semanticscholar.org/paper/c230fe3f32c39d777b8a92e535b09eb42f96e092,Learning to Estimate the Travel Time,"Vehicle travel time estimation or estimated time of arrival (ETA) is one of the most important location-based services (LBS). It is becoming increasingly important and has been widely used as a basic service in navigation systems and intelligent transportation systems. This paper presents a novel machine learning solution to predict the vehicle travel time based on floating-car data. First, we formulate ETA as a pure spatial-temporal regression problem based on a large set of effective features. Second, we adapt different existing machine learning models to solve the regression problem. Furthermore, we propose a Wide-Deep-Recurrent (WDR) learning model to accurately predict the travel time along a given route at a given departure time. We then jointly train wide linear models, deep neural networks and recurrent neural networks together to take full advantages of all three models. We evaluate our solution offline with millions of historical vehicle travel data. We also deploy the proposed solution on Didi Chuxing's platform, which services billions of ETA requests and benefits millions of customers per day. Our extensive evaluations show that our proposed deep learning algorithm significantly outperforms the state-of-the-art learning algorithms, as well as the solutions provided by leading industry LBS providers.",KDD,2018,10.1145/3219819.3219900,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
0966363ea46368f297999b026c827f0f5ea7fcc9,https://www.semanticscholar.org/paper/0966363ea46368f297999b026c827f0f5ea7fcc9,Efficient Machine-Type Communication Using Multi-Metric Context-Awareness for Cars Used as Mobile Sensors in Upcoming 5G Networks,"Upcoming 5G-based communication networks will be confronted with huge increases in the amount of transmitted sensor data related to massive deployments of static and mobile Internet of Things (IoT) systems. Cars acting as mobile sensors will become important data sources for cloud-based applications like predictive maintenance and dynamic traffic forecast. Due to the limitation of available communication resources, it is expected that the grows in Machine-Type Communication (MTC) will cause severe interference with Human-to-human (H2H) communication. Consequently, more efficient transmission methods are highly required. In this paper, we present a probabilistic scheme for efficient transmission of vehicular sensor data which leverages favorable channel conditions and avoids transmissions when they are expected to be highly resource-consuming. Multiple variants of the proposed scheme are evaluated in comprehensive realworld experiments. Through machine learning based combination of multiple context metrics, the proposed scheme is able to achieve up to 164% higher average data rate values for sensor applications with soft deadline requirements compared to regular periodic transmission.",2018 IEEE 87th Vehicular Technology Conference (VTC Spring),2018,10.1109/VTCSpring.2018.8417753,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
a096820f01913266adf8928ef837bde6a5f5d1b3,https://www.semanticscholar.org/paper/a096820f01913266adf8928ef837bde6a5f5d1b3,A Deep Learning Based Approach on Categorization of Tea Leaf,"Tea grading is a very prominent factor of the tea industry. The standard, fragrance and sweetness of tea mostly relies on this grading system. This research is a step to introduce machine learning with the tea industry, where image classification and recognition is deployed to digitize the grading system by eradicating human intervention in it. Three models are used in this system in which two were pre-trained. They are Faster RCNN (Inception-v2), and VGG16. The other one is manually trained, that is Sequential model or CNN. After a successful session of compulsory augmentation and scaling, we gathered 3000 raw images which were used to train and test the model spontaneously. Our productivity has rendered us tremendous satisfaction by supplying astonishing accuracy. So, it will be not wrong saying that this research has amalgamated machine learning technology with the grading system of tea very productively which can escort a great revolution to the tea industry.",ICCA,2020,10.1145/3377049.3377122,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
fabc67311339044d9f55e32d274dc44db610f1f2,https://www.semanticscholar.org/paper/fabc67311339044d9f55e32d274dc44db610f1f2,Real Time Customer Churn Scoring Model for the Telecommunications Industry,"There are two types of customers in the telecommunication industry; the pre-paid and the contract customers. In South Africa it is the pre-paid customers that keep telcos constantly worried because such customers do not have anything binding them to the company, they can leave and join a competitor at any time. To retain such customers, telcos need to customise suitable solutions especially for those customers that are agitating and can churn at any time. This needs customer churn prediction models that would take advantage of big data analytics and provide the telco industry with a real time solution. The purpose of this study was to develop a real time customer churn prediction model. The study used the CRISP-DM methodology and the three machine learning algorithms for implementation. Watson Studio software was used for the model prototype deployment. The study used the confusion matrix to unpack a number of performance measures. The results showed that all the models had some degree of misclassification, however the misclassification rate of the Logistic Regression was very minimal (2.2%) as differentiated from the Random Forest and the Decision Tree, which had misclassification rates of 20.8% and 21.7% respectively. The results further showed that both Random Forest and the Decision Tree had good accuracy rates of 78.3% and 79.2% respectively, although they were still not better than that of the Logistic Regression. Despite the two having good accuracy rates, they had the highest rates of misclassification of class events. The conclusion we drew from this was that, accuracy is not a dependable measure for determining model performance.",2020 2nd International Multidisciplinary Information Technology and Engineering Conference (IMITEC),2020,10.1109/IMITEC50163.2020.9334129,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
5a7906ff069220733777bc578825a44ead61c741,https://www.semanticscholar.org/paper/5a7906ff069220733777bc578825a44ead61c741,AIBENCH: AN INDUSTRY STANDARD AI BENCHMARK SUITE,"The booming successes of machine learning in different domains boost industry-scale deployments of innovative AI algorithms, systems, and architectures, and thus the importance of benchmarking grows. However, the confidential nature of the workloads, the paramount importance of the representativeness and diversity of benchmarks, and the prohibitive cost of training a state-of-the-art model mutually aggravate the AI benchmarking challenges. In this paper, we present a balanced AI benchmarking methodology for meeting the subtly different requirements of different stages in developing a new system/architecture and ranking/purchasing ∗Jianfeng Zhan is the corresponding author.",,2020,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
960f04224214035e6bfda0e0c13627bd1df644b8,https://www.semanticscholar.org/paper/960f04224214035e6bfda0e0c13627bd1df644b8,Scalable IoT Solution using Cloud Services – An Automobile Industry Use Case,"The role of IoT and related internet-based applications in otherwise mechanical devices to monitor, manage and enhance the performance of the same is quite widespread now. Almost all public cloud service providers provide scalable, fully managed and elastic IoT related services. The data flows from these services are essentially streaming and can be consumed for further use in various predictive, descriptive and visualization modules. The cloud platforms enable ingestion, transformation and usage of the data by providing streaming, machine learning and sharable visualization services. This ecosystem greatly reduces the time to create IoT based minimum viable product creation which in turn enhances the business value realization cycle. The effect of cycle time reduction to design, architect and develop IoT solutions leads to a rapid improvement of business lead time and makes it easier for businesses to gain from the data insights and plan the next course of action. In this paper, one such enterprise graded use case is explored, in which the Azure IoT platform in terms of the offerings and associated ecosystem of Azure Stream Analytics and Azure Machine learning services are explained. This paper covers design, architecture, development and deployment of the solution prepared and how the same is monitored once in production. Security is a very important aspect of the same and here the security architecture is being explored. A conclusion is presented with the scope of future enhancements using auto ML services in serverless platforms to enable real-time automated decision making augmented with human expertise and intelligence.","2020 Fourth International Conference on I-SMAC (IoT in Social, Mobile, Analytics and Cloud) (I-SMAC)",2020,10.1109/I-SMAC49090.2020.9243544,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
cd61df330af337aa6f8a53725a33a3a3ec3ff055,https://www.semanticscholar.org/paper/cd61df330af337aa6f8a53725a33a3a3ec3ff055,System to Integrate Fairness Transparently: An Industry Approach,"There have been significant research efforts to address the issue of unintentional bias in Machine Learning (ML). Many well-known companies have dealt with the fallout after the deployment of their products due to this issue. In an industrial context, enterprises have large-scale ML solutions for a broad class of use cases deployed for different swaths of customers. Trading off the cost of detecting and mitigating bias across this landscape over the lifetime of each use case against the risk of impact to the brand image is a key consideration. We propose a framework for industrial uses that addresses their methodological and mechanization needs. Our approach benefits from prior experience handling security and privacy concerns as well as past internal ML projects. Through significant reuse of bias handling ability at every stage in the ML development lifecycle to guide users we can lower overall costs of reducing bias.",ArXiv,2020,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
2f7a54d43f03004b3c67b9d2b8921026a5aa1a3b,https://www.semanticscholar.org/paper/2f7a54d43f03004b3c67b9d2b8921026a5aa1a3b,2020 ITU Kaleidoscope: Industry-Driven Digital Transformation (ITU K),"The proceedings contain 25 papers The topics discussed include: digital transformation via 5G: deployment plans;an accelerated digital transformation, courtesy of the recent pandemic;STCCS: segmented time controlled count-min sketch;beyond 5G/6G telecommunications ensuring continuity in business, research and education Akihiro Nakao, special advisor to the president and vice dean of the interfaculty initiative;a case study for modeling machine tool systems using standard representations;5G healthcare applications in COVID-19 prevention and control;lightweight and instant access technologies and protocols to boost digital transformations;automation of computational resource control of cyber-physical systems with machine learning;a technique for extracting the intention of messengers in social media;and towards a digital process platform for future construction sites",,2020,10.23919/ituk50268.2020,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
1e8b8323dcdfb000399dc6a71e8fd86846b141f6,https://www.semanticscholar.org/paper/1e8b8323dcdfb000399dc6a71e8fd86846b141f6,Annotating Documents using Active Learning Methods for a Maintenance Analysis Application,"The aircraft cargo industry still maintains vast amounts of the maintenance history of aircraft components in electronic (i.e. scanned) but unsearchable images. For a given supplier, there can be hundreds of thousands of image documents only some of which contain useful information. Using supervised machine learning techniques has been shown to be effective in recognising these documents for further information extraction. A well known deficiency of supervised learning approaches is that annotating sufficient documents to create an effective model requires valuable human effort. This paper first shows how to obtain a representative sample from a supplier's corpus. Given this sample of unlabelled documents an active learning approach is used to select which documents to annotate first using a normalised certainty measure derived from a soft classifier's prediction distribution. Finally the accuracy of various selection approaches using this certainty measure are compared along each iteration of the active learning cycle. The experiments show that a greedy selection method using the uncertainty measure can significantly reduce the number of annotations required for a certain accuracy. The results provide valuable information for users and more generally illustrate an effective deployment of a machine learning application.",AIPR,2020,10.1145/3430199.3430214,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
b780a8438ccc552698670040acad3c0856de669f,https://www.semanticscholar.org/paper/b780a8438ccc552698670040acad3c0856de669f,Deep learning application in diverse fields with plant weed detection as a case study,"Machine learning applications have gained popularity over the years as more advanced algorithms like the deep learning (DL) algorithm are being employed in signal identification, classification and detection of cracks or faults in structures. The DL algorithm has broader applications compared to other machine learning systems and it is a creative algorithm capable of processing data, creating pattern, interpreting information due to its high level of accuracy in pattern recognition under stochastic conditions. This research gives an exposition of DL in diverse areas of operations with a focus on plant weed detection which is inspired by the need to treat a specific class of weed with a particular herbicide. A Convolutional Neural Network (CNN) model was trained through transfer learning on a pre-trained ResNet50 model and the performance was evaluated using a random forest (RF) classifier, the trained model was deployed on a raspberry pi for prediction of the test data. Training accuracies of 99% and 93% were obtained for the CNN and RF classifier respectively. Some recommendations have been proffered to improve inference time such as the use of better embedded systems such as the Nvidia Jetson TX2, synchronizing DL hardware accelerators with appropriate optimization techniques. A prospect of this work would be to incorporate an embedded system, deployed with DL algorithms, on an unmanned aerial vehicle or ground vehicle. Overall, it is revealed from this study that DL is highly efficient in every sector and can improve the accuracy on automatic detection of systems in especially in this era of Industry 4.0.",icARTi,2021,10.1145/3487923.3487926,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
fe77e603004d7ba298fe140d5279e167e1c30555,https://www.semanticscholar.org/paper/fe77e603004d7ba298fe140d5279e167e1c30555,System-level Design Methods for Deep Learning on Heterogeneous Architectures,"Edge Artificial Intelligence is the newmegatrend, as privacy concerns and networks bandwidth/latency bottlenecks prevent cloud offloading of sensor analytics functions in many application domains, from autonomous driving to advanced prosthetic. The nextwave of “Extreme EdgeAI"" pushes signal processing and machine learning aggressively into sensors and actuators, opening major research and business development opportunities. In this talk, I will focus on recent efforts in developing anAI-centric Extreme Edge heterogeneous computing platform based on open source, parallel ultra-low power (PULP) customized ISA-RISC-V processors coupled with domain-specific accelerators, and I will look ahead to the next step: namely three-dimensional integration of sensors, mixed-signal front-ends and AI-processing engines. Specialization in Hardware Architectures for Deep Learning [Keynote Talk] Michaela Blott Xilinx Research, Ireland Abstract Neural Networks are playing a key role in enabling machine vision and speech recognition; however, their computational complexity and memory demands are challenging, which limits their deployment in particular within energy-constrained, embedded environments. To address these challenges, a broad spectrum of increasingly customized and heterogeneous hardware architectures has emerged. During this talk, we will discuss various forms of specializations that have been leveraged by the industry with their impact on potential applications, flexibility, performance and efficiency. Furthermore, we will discuss how the specialization in hardware architectures can be automated through end-to-end tool flows.Neural Networks are playing a key role in enabling machine vision and speech recognition; however, their computational complexity and memory demands are challenging, which limits their deployment in particular within energy-constrained, embedded environments. To address these challenges, a broad spectrum of increasingly customized and heterogeneous hardware architectures has emerged. During this talk, we will discuss various forms of specializations that have been leveraged by the industry with their impact on potential applications, flexibility, performance and efficiency. Furthermore, we will discuss how the specialization in hardware architectures can be automated through end-to-end tool flows.",,2021,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
01c2c083906aa9deedbf213c548750d4405e39cf,https://www.semanticscholar.org/paper/01c2c083906aa9deedbf213c548750d4405e39cf,ActiVis: Visual Exploration of Industry-Scale Deep Neural Network Models,"While deep learning models have achieved state-of-the-art accuracies for many prediction tasks, understanding these models remains a challenge. Despite the recent interest in developing visual tools to help users interpret deep learning models, the complexity and wide variety of models deployed in industry, and the large-scale datasets that they used, pose unique design challenges that are inadequately addressed by existing work. Through participatory design sessions with over 15 researchers and engineers at Facebook, we have developed, deployed, and iteratively improved ActiVis, an interactive visualization system for interpreting large-scale deep learning models and results. By tightly integrating multiple coordinated views, such as a computation graph overview of the model architecture, and a neuron activation view for pattern discovery and comparison, users can explore complex deep neural network models at both the instance-and subset-level. ActiVis has been deployed on Facebook's machine learning platform. We present case studies with Facebook researchers and engineers, and usage scenarios of how ActiVis may work with different models.",IEEE Transactions on Visualization and Computer Graphics,2017,10.1109/TVCG.2017.2744718,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
2c407d596a2758722f70978dc57d58fd1ab3d01e,https://www.semanticscholar.org/paper/2c407d596a2758722f70978dc57d58fd1ab3d01e,Personalized Benchmarking with the Ludwig Benchmarking Toolkit,"The rapid proliferation of machine learning models across domains and deploy-ment settings has given rise to various communities (e.g. industry practitioners) which seek to benchmark models across tasks and objectives of personal value. Unfortunately, these users cannot use standard benchmark results to perform such value-driven comparisons as traditional benchmarks evaluate models on a single objective (e.g. average accuracy) and fail to facilitate a standardized training framework that controls for confounding variables (e.g. computational budget), making fair comparisons difﬁcult. To address these challenges, we introduce the open-source Ludwig Benchmarking Toolkit (LBT), a personalized benchmarking toolkit for running end-to-end benchmark studies (from hyperparameter optimization to evaluation) across an easily extensible set of tasks, deep learning models, datasets and evaluation metrics. LBT provides a conﬁgurable interface for controlling training and customizing evaluation, a standardized training framework for eliminating confounding variables, and support for multi-objective evaluation. We demonstrate how LBT can be used to create personalized benchmark studies with a large-scale comparative analysis for text classiﬁcation across 7 models and 9 datasets. We explore the trade-offs between inference latency and performance, relationships between dataset attributes and performance, and the effects of pretraining on convergence and robustness, showing how LBT can be used to satisfy various benchmarking objectives. between inference factors.",NeurIPS Datasets and Benchmarks,2021,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
e2b46a76c71c45f85c410841267575ca0832073d,https://www.semanticscholar.org/paper/e2b46a76c71c45f85c410841267575ca0832073d,When Information Freshness Meets Service Latency in Federated Learning: A Task-Aware Incentive Scheme for Smart Industries,"For several industrial applications, a sole data owner may lack sufficient training samples to train effective machine learning based models. As such, we propose a federated learning (FL) based approach to promote privacy-preserving collaborative machine learning for applications in smart industries. In our system model, a model owner initiates an FL task involving a group of workers, i.e., data owners, to perform model training on their locally stored data before transmitting the model updates for aggregation. There exists a tradeoff between service latency, i.e., the time taken for the training request to be completed, and age of information (AoI), i.e., the time elapsed between data aggregation from the deployed industrial Internet of Things devices to completion of the FL-based training. On one hand, if the data are collected only upon the model owner's request, the AoI is low. On the other hand, the service latency incurred is more significant. Furthermore, given that different training tasks may have varying AoI requirements, we propose a contract-theoretic task-aware incentive scheme that can be calibrated based on the weighted preferences of the model owner toward AoI and service latency. The performance evaluation validates the incentive compatibility of our contract amid information asymmetry, and shows the flexibility of our proposed scheme toward satisfying varying preferences of AoI and service latency.",IEEE Transactions on Industrial Informatics,2020,10.1109/tii.2020.3046028,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
dbc50540a546d60c5981c049ac6a3de90546e626,https://www.semanticscholar.org/paper/dbc50540a546d60c5981c049ac6a3de90546e626,Multipath TCP Meets Transfer Learning: A Novel Edge-Based Learning for Industrial IoT,"We consider a fifth-generation (5G)-empowered future Industrial IoT (IIoT) networking problem where IIoT machines are capable of communicating and sharing their data networking knowledge gained (and experiences) with other neighboring devices/tools. For such an IIoT setting, deep-learning (DL)-based communication protocols are known to be highly efficient but having a computationally complex training procedure in terms of both time/space and volume of data sets. One solution for such training is to be completed offline for each equipment and machines of IIoT before deployment. A better approach would be to replicate the model from the expert existing machine and implant it into new machines. Such training for the transfer of knowledge can be done by manufacturers using high computational power, even for large-scale DL models. After sufficient training and the desired level of accuracy, the trained machines can be deployed in the smart factory equipment to perform life-long collaborative learning. We design a novel distributed transfer learning (TL) framework to maximize multipath communication networking performance for Industry 4.0 environment. To conduct seamless sharing of knowledge gain by the multipath TCP (MPTCP) agents and tackle retraining issues of DL-based approaches, we investigate TL for MPTCP from the IIoT networking perspective. With relevant insights from transfer and collaborative learning, we develop a distributed TL-MPTCP framework to accelerate the learning efficiency and enhance the performance of newly deployed machines. Our approach is validated with numerical and emulated NS-3 experiments in comparison with the state-of-the-art schemes.",IEEE Internet of Things Journal,2021,10.1109/JIOT.2021.3056466,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
923c4104c48a83407d2a20444a332e4a5ed1451a,https://www.semanticscholar.org/paper/923c4104c48a83407d2a20444a332e4a5ed1451a,Overcoming limited battery data challenges: A coupled neural network approach,"The electric vehicle (EV) industry has seen extraordinary growth in the last few years. This is primarily due to an ever increasing awareness of the detrimental environmental effects of fossil fuel powered vehicles and availability of inexpensive lithium‐ion batteries (LIBs). In order to safely deploy these LIBs in electric vehicles, certain battery states need to be constantly monitored to ensure safe and healthy operation. The use of machine learning to estimate battery states such as state‐of‐charge and state‐of‐health have become an extremely active area of research. However, limited availability of open‐source diverse datasets has stifled the growth of this field, and is a problem largely ignored in the literature. In this work, we propose a novel method of time‐series battery data augmentation using deep neural networks. We introduce and analyze the method of using two neural networks working together to alternatively produce synthetic charging and discharging battery profiles. One model produces battery charging profiles, and another produces battery discharging profiles. The proposed approach is evaluated using few public battery datasets to illustrate its effectiveness, and our results show the efficacy of this approach to solve the challenges of limited battery data. We also test this approach on dynamic electric vehicle drive cycles as well.",International Journal of Energy Research,2021,10.1002/er.7081,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
97e7a468dea46c4233ff353d3021da15a979494d,https://www.semanticscholar.org/paper/97e7a468dea46c4233ff353d3021da15a979494d,PCC Vivace: Online-Learning Congestion Control,"TCP’s congestion control architecture suffers from notoriously bad performance. Consequently, recent years have witnessed a surge of interest in both academia and industry in novel approaches to congestion control. We show, however, that past approaches fall short of attaining ideal performance. We leverage ideas from the rich literature on online (convex) optimization in machine learning to design Vivace, a novel rate-control protocol, designed within the recently proposed PCC framework. Our theoretical and experimental analyses establish that Vivace significantly outperforms traditional TCP variants, the previous realization of the PCC framework, and BBR in terms of performance (throughput, latency, loss), convergence speed, alleviating bufferbloat, reactivity to changing network conditions, and friendliness towards legacy TCP in a range of scenarios. Vivace requires only sender-side changes and is thus readily deployable.",NSDI,2018,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
ccbe83578d44cb90a79aee05588e4a3323d4403d,https://www.semanticscholar.org/paper/ccbe83578d44cb90a79aee05588e4a3323d4403d,Scalable Fully Pipelined Hardware Architecture for In-Network Aggregated AllReduce Communication,"The Ring-AllReduce framework is currently the most popular solution to deploy industry-level distributed machine learning tasks. However, only about half of the maximum bandwidth can be achieved in the optimal condition. In recent years, several in-network aggregation frameworks have been proposed to overcome the drawback, but limited hardware information have been disclosed. In this paper, we propose a scalable fully-pipelined architecture that handles tasks like forwarding, aggregation and retransmission with no bandwidth loss. The architecture is implemented on a Xilinx Ultrascale FPGA that connects to 8 working servers with 10 Gb/s network adapters, and it is able to scale to more complicated scenarios involving more workers. Compared with Ring-AllReduce, using AllReduce-Switch improves the efficient bandwidth of AllReduce communication with a ratio of <inline-formula> <tex-math notation=""LaTeX"">$1.75\times $ </tex-math></inline-formula>. In image training tasks, the proposed hardware architecture helps to achieve up to <inline-formula> <tex-math notation=""LaTeX"">$1.67\times $ </tex-math></inline-formula> speedup to the training process. For computing-intensive models, the speedup from communication may be partially hidden by computing. In particular, for ResNet-50, AllReduce-Switch improves the training process with MPI and NCCL by <inline-formula> <tex-math notation=""LaTeX"">$1.30\times $ </tex-math></inline-formula> and <inline-formula> <tex-math notation=""LaTeX"">$1.04\times $ </tex-math></inline-formula> respectively.",IEEE Transactions on Circuits and Systems I: Regular Papers,2021,10.1109/tcsi.2021.3098841,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
5b90758d0921226ac2e2c44ec218d32dd20d51df,https://www.semanticscholar.org/paper/5b90758d0921226ac2e2c44ec218d32dd20d51df,Privacy-preserving Data Mining in Industry,"Preserving privacy of users is a key requirement of web-scale data mining applications and systems such as web search, recommender systems, crowdsourced platforms, and analytics applications, and has witnessed a renewed focus in light of recent data breaches and new regulations such as GDPR. In this tutorial, we will first present an overview of privacy breaches over the last two decades and the lessons learned, key regulations and laws, and evolution of privacy techniques leading to differential privacy definition / techniques. Then, we will focus on the application of privacy-preserving data mining techniques in practice, by presenting case studies such as Apple's differential privacy deployment for iOS / macOS, Google's RAPPOR, LinkedIn Salary, and Microsoft's differential privacy deployment for collecting Windows telemetry. We will conclude with open problems and challenges for the data mining / machine learning community, based on our experiences in industry.",WSDM,2019,10.1145/3289600.3291384,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
55f3e3cc9f4af08c9d17cddf622a22134e216683,https://www.semanticscholar.org/paper/55f3e3cc9f4af08c9d17cddf622a22134e216683,The Design of Intelligent production line for Clothing Industry,"To promote the development of the textile industrial and upgrade the existing production technology. This article explored the intelligent application scenarios by analyzing the production process and technology of clothing production process. The hardware deployment and the management architecture of the production line were designed by combining the machine vision, deep learning and robot technology. And the performance of the intelligent algorithms under the production framework were also verified in this article.",2020 IEEE 6th International Conference on Computer and Communications (ICCC),2020,10.1109/ICCC51575.2020.9344935,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
81615b07cabd5d2cf15a807a054e56fc322cd72c,https://www.semanticscholar.org/paper/81615b07cabd5d2cf15a807a054e56fc322cd72c,Investigation of Data Compression Methods for Intelligent Machine Condition Monitoring,"Condition monitoring (CM) deliveries significant benefits to the industry by minimising breakdown losses and enhancing the safety and high-performance operation of machinery. However, the use of data acquisition systems with multiple sensors and high sampling rates leads to massive data and causes considerably high cost for purchasing and deploying hardware for data transmission, storage and processing. Hence, data compression is crucial and important to reduce the data size and speed up the calculation for the development of intelligent machine CM systems. Although data compression has received high attention in many fields, few researchers have focused on their research in the field of machine CM. Therefore, this PhD research concentrates on investigating novel and high-performance data compression algorithms according to the characteristics of one-dimensional (1D) and two-dimensional (2D) signals to solve the bottleneck of the massive data transmission, and hence improve the performance of remote and real-time machine CM systems. 
 
 
The research is carried out according to a compound experimental and analytic route based on a wireless senor network. To demonstrate the effectiveness of data compression based techniques for CM, the prototype of an intelligent wireless sensing system is developed using cost-effective micro-electromechanical systems (MEMS) accelerometers and the Bluetooth low energy (BLE) communication module. Moreover, various waveform parameters with low cost computing in time and frequency domains are investigated and identified that RMS is the most effective parameter to give good indication for the leakage in a piping system, showing that data compression via statistics is effective and thus indicates that the performance of data compression for CM highly depends on applications. 
 
 
Subsequently, high-performance but high-complexity methods are proposed base on dimension reduction, sparse representation, feature extraction and advanced compressive sensing (CS) for fault diagnosis of rotating machinery with 1D or 2D signals, which have the potentials to be implemented on MEMS modules in a wireless sensor network (WSN) in future. Firstly, a compression scheme based on dimension reduction is proposed to extract the periodic characteristics of the 1D vibration signal. Recurrence plot (RP) of vibration phase space trajectory and its quantification indicators, as well as principal component analysis (PCA), are combined to realize feature extraction, compression and fault classification for a tapered roller bearing system. 
 
 
Secondly, a two-step compression method is performed on 1D vibration signals based on frequency shift, adaptive sparse representation and CS is explored to overcome the problem of the large quantity of data storage for ball bearing fault diagnosis. Simultaneously, this compression method has the capability to reconstruct envelope signals with noise elimination.Then, for 2D thermal images captured from a two-stage reciprocating compressor, the dense scale-invariant feature transform (SIFT) features indicating edge information are extracted and represented as a sparse matrix by sparse coding. The compressed features are used for the classification of six different types of faults with the support vector machine (SVM). 
 
 
Finally, the advanced CS technique is exploited on pre-processing the 2D thermal images of gearboxes to realise intelligent fault classification with high accuracy of more than 99.81% by a typical deep learning algorithm, namely convolutional neural network (CNN). The CNN calculation speed is dramatically accelerated with compressed images. All these proposed approaches are evaluated by simulations and experiments, which verifies that they can reliably detect the fault types or classify different fault types with very high accuracy. Besides, the proposed data compression based intelligent CM approaches provide theoretical bases for maintenance-free CM systems because data compression can save the transmission bandwidth and power consumption for remote and real-time machine CM systems.",,2020,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
005b91e2378093803c7342f256268ee0643b9c3d,https://www.semanticscholar.org/paper/005b91e2378093803c7342f256268ee0643b9c3d,Sample-Efficient I-Projections for Robot Learning,"Robots had a great impact on the manufacturing industry ever since the early seventies when companies such as KUKA and ABB started deploying their first industrial robots. These robots merely performed very specific tasks in specific ways within well-defined environments. Still, they proved to be very useful as they could exceed human performance at these tasks. However, in order to enable robots to enter our daily life, they need to become more versatile and need to operate in much less structured environments. 
This thesis is partly devoted to stretching these limitations by means of learning, namely imitation learning (IL) and inverse reinforcement learning (IRL). 
 
Reinforcement learning (RL) is a powerful approach to enable robots to solve a task in an unknown environment. The practitioner describes a desired behavior by specifying a reward function and the robot autonomously interacts with the environment in order to find a control policy that generates high accumulated reward. However, RL is not suitable for teaching new tasks by non-experts because specifying appropriate reward functions can be difficult. Demonstrating the desired behavior is often easier for non-experts. Imitation learning can be used in order to enable the robot to reproduce the demonstrations. However, without explicitly inferring and modeling the intentions of the demonstrations, it can become difficult to solve the task for unseen situations. Inverse reinforcement learning (IRL) therefore aims to infer a reward function from the demonstrations, such that optimizing this reward function yields the desired behavior even for different situations. 
 
This thesis introduces a unifying approach to solve the inverse reinforcement learning problem in the same way as the reinforcement learning problem. This is achieved by framing both problems as information projection problems, i.e., we strive to minimize the relative entropy between a probabilistic model of the robot behavior and a given desired distribution. Furthermore, a trust region on the robot behavior is used to stabilize the optimization. 
For inverse reinforcement learning, the desired distribution is implicitly given by the expert demonstrations. The resulting optimization can be efficiently solved using state-of-the-art reinforcement learning methods. 
For reinforcement learning, the log-likelihood of the desired distribution is given by the reward function. The resulting optimization problem corresponds to a standard reinforcement learning formulation, except for an additional objective of maximizing the entropy of the robot behavior. This entropy objective adds little overhead to the optimization, but can lead to better exploration and more diversified policies. 
 
Trust-region I-projections are not only useful for training robots, but can also be applied to other machine learning problems. I-projections are typically used for variational inference, in order to approximate an intractable distribution by a simpler model. However, the resulting optimization problems are usually optimized based on stochastic gradient descent which often suffers from high variance in the gradient estimates. As trust-region I-projections where shown to be effective for reinforcement learning and inverse reinforcement learning, this thesis also explores their use for variational inference. More specifically, trust-region I-projections are investigated for the problem of approximating an intractable distribution by a Gaussian mixture model (GMM) with an adaptive number of components. GMMs are highly desirable for variational inference because they can yield arbitrary accurate approximations while inference from GMMs is still relatively cheap. In order to make learning the GMM feasible, we derive a lower bound that enables us to decompose the objective function. The optimization can then be performed by iteratively updating individual components using a technique from reinforcement learning. The resulting method is capable of learning approximations of significantly higher quality than existing variational inference methods. 
 
Due to the similarity of the underlying optimization problems, the insights gained from our variational inference method are also useful for IL and IRL. Namely, a similar lower bound can be applied also for the I-projection formulation of imitation learning. However, whereas for variational inference the lower bound serves to decompose the objective function, for imitation learning it allows us to provide a reward signal to the robot that does not depend on its behavior. Compared to reward functions that are relative to the current behavior of the robot---which are typical for popular adversarial methods---behavior-independent reward functions have the advantages that we can show convergence even for greedy optimization. Furthermore, behavior-independent reward functions solve the inverse reinforcement learning problem, thereby closing the gap between imitation learning and IRL. However, algorithms derived from our non-adversarial formulation are actually very similar to existing AIL methods, and we can even show that adversarial inverse reinforcement learning (AIRL) is indeed an instance of our formulation. AIRL was derived from an adversarial formulation, and we point out several problems of that derivation. In contrast, we show that AIRL can be straightforwardly derived from out non-adversarial formulation. Furthermore, we demonstrate that the non-adversarial formulation can be also used to derive novel algorithms by presenting a non-adversarial method for offline imitation learning.",,2021,10.12921/TUPRINTS-00014271,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
3a449c424bb396cb796f8577a4b042b76c1bfd99,https://www.semanticscholar.org/paper/3a449c424bb396cb796f8577a4b042b76c1bfd99,Automated defect detection in digital radiography of aerospace welds using deep learning,,Welding in the World,2022,10.1007/s40194-022-01257-w,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
e5db631b9237de584eadab60dd6529470438ad5d,https://www.semanticscholar.org/paper/e5db631b9237de584eadab60dd6529470438ad5d,Realization of an Intrusion Detection use-case in ONAP with Acumos,"With Software-Defined Networking and Machine Learning/Artificial Intelligence (ML/AI) reaching new paradigms in their corresponding fields, both academia and industry have exhibited interests in discovering unique aspects of intelligent and autonomous communication networks. Transforming such intentions and interests to reality involves software development and deployment, which has its own story of significant evolution. There has been a notable shift in the strategies and approaches to software development. Today, the divergence of tools and technologies as per demand is so substantial that adapting a software application from one environment to another could involve tedious redesign and redevelopment. This implies enormous effort in migrating existing applications and research works to a modern industrial setup. Additionally, the struggles with sustainability maintenance of such applications could be painful. Concerning ML/AI, the capabilities to train, deploy, retrain, and re-deploy AI models as quickly as possible will be crucial for AI-driven network systems. An end-to-end workflow using unified open-source frameworks is the need of the hour to facilitate the integration of ML/AI models into the modern software-driven virtualized communication networks. Hence, in our paper, we present such a prototype by demonstrating the journey of a sample SVM classifier from being a python script to be deployed as a micro-service using ONAP and Acumos. While illustrating various features of Acumos and ONAP, this paper intends to make readers familiar with an end-to-end workflow taking advantage of the integration of both open-source platforms.",2021 International Conference on Computer Communications and Networks (ICCCN),2021,10.1109/ICCCN52240.2021.9522281,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
69fb4c8b9bd2ea88c0ba24979c715e9bb580d363,https://www.semanticscholar.org/paper/69fb4c8b9bd2ea88c0ba24979c715e9bb580d363,Data-Driven Condition Monitoring of Mining Mobile Machinery in Non-Stationary Operations Using Wireless Accelerometer Sensor Modules,"This paper presents the development of an easy-to-deploy and smart monitoring IoT system that utilizes vibration measurement devices to assess real-time condition of bulldozers, power shovels and backhoes, in non-stationary operations in the mining industry. According to operating experience data and the type of mining machine, total loss failure rates per machine fleet can reach up to 30%. Vibration analysis techniques are commonly used for condition monitoring and early detection of unforeseen failures to generate predictive maintenance plans for heavy machinery. However, this maintenance strategy is intensively used only for stationary machines and/or mobile machinery in stationary operations. Today, there is a lack of proper solutions to detect and prevent critical failures for non-stationary machinery. This paper shows a cost-effective solution proposal for implementing a vibration sensor network with wireless communication and machine learning data-driven capabilities for condition monitoring of non-stationary heavy machinery in mining operations. During the machine operation, 3-axis accelerations were measured using two sensors deployed across the machine. The machine accelerations (amplitudes and frequencies) are measured in two different frequency spectrums to improve each sensing location’s time resolution. Multiple machine learning algorithms use this machine data to assess conditions according to manufacturer recommendations and operational benchmarks Proposed data-driven machine learning models classify the machine condition in states according to the ISO 2372 standards for vibration severity: Good, Acceptable, Unsatisfactory, or Unacceptable. After performing field tests with bulldozers and backhoes from different manufacturers, the machine learning algorithms are able to classify machine health status with an accuracy between 85% - 95%. Moreover, the system allows early detection of “Unacceptable” states between 120 to 170 hours prior to critical failure. These results demonstrate that the proposed system will collect relevant data to generate predictive maintenance plans and avoid unplanned downtimes.",IEEE Access,2021,10.1109/ACCESS.2021.3051583,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
bbec7b61e82d59fb923c3ebb798241e9b38f9863,https://www.semanticscholar.org/paper/bbec7b61e82d59fb923c3ebb798241e9b38f9863,Survey of ML Model Serving Solutions and criteria for selection thereof,"Machine learning operations is an emerging field in software engineering. It aims to streamline the machine learning development process from the model’s concept to its application. In this thesis, we focus on machine learning model serving approaches for online inference use cases. First, we provide a brief background for machine learning operations and delineate the situations we consider in this thesis. Then, we propose four criteria used to describe and evaluate the deployment approaches. Next, we identify five different schemes to deploy machine learning models and assess them with regard to the defined criteria. We provide the architectures of the solutions and the use case where they fit along with example implementations. Further, we propose a framework to guide selecting the approach that best fits the use case. Lastly, we provide two industry case studies describing the architecture, implementation, and reasoning behind those decisions.",,2021,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
7da8a032f2104b92e94616e314630d6bf71930a9,https://www.semanticscholar.org/paper/7da8a032f2104b92e94616e314630d6bf71930a9,ADVANTAGE OF A GREEN AND SMART PORT OF THE FUTURE,"The digitalization is undeniably arriving to the port industry. However, modern digital technologies had not pervaded before in the sector because of, mainly, ports’ complexity and heterogeneity as wide ecosystems. When it comes to applying innovative digital tools in maritime ports, a series of additional common barriers are usually faced: (i) unavailability of clear reference in open-source based technologies, (ii) closed-environments and high pricing rates of private providers, (iii) restricting regulations inside and outside the ports preventing port authorities to deploy useful products and (iv) high heterogeneity of objectives, data or perspectives to carry out focused accountable actions. The PIXEL project has helped ports of all sizes to overcome those barriers during the last three years. Throughout a variety of domains of action – including clean energy, environmental performance, smart intermodal transport or machine learning-based maritime data analytics, PIXEL has addressed those hindrances, driving four European ports towards the Port of The Future stand. The different open applications developed form an ecosystem that may be adopted by external ports aiming at improving their digitalisation levels and their operational and environmental performance. For the 2021 edition, we are presenting the suite of tools deployed in the ports in the context of the action, their success stories and best practices, and how they can be leveraged by worldwide maritime transport entities in the future. It is our objective to provide a comprehensive review of their functioning, technical traits and particularities and how they are planned to be exploited by the Consortium. The PIXEL team truly believes that this will mean a milestone in the operational research field for the ports sector. Finally, we aim at offering a perspective on the usage of modern technologies in maritime ports based on the experience and lessons learned in the conduction of PIXEL project and the interaction with other initiatives in the period 2018–2021.",Urban and Maritime Transport XXVII,2021,10.2495/ut210171,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
737a990572562440f564dd001b33b8136a23f719,https://www.semanticscholar.org/paper/737a990572562440f564dd001b33b8136a23f719,Parameterisation of lane-change scenarios from real-world data,"Recent Autonomous Vehicles (AV) technology includes machine learning and probabilistic techniques that add significant complexity to the traditional verification and validation methods. The research community and industry have widely accepted scenario-based testing in the last few years. As it is focused directly on the relevant crucial road situations, it can reduce the effort required in testing. Encoding real-world traffic participants’ behaviour is essential to efficiently assess the System Under Test (SUT) in scenario-based testing. So, it is necessary to capture the scenario parameters from the realworld data that can model scenarios realistically in simulation. The primary emphasis of the paper is to identify the list of meaningful parameters that adequately model real-world lanechange scenarios. With these parameters, it is possible to build a parameter space capable of generating a range of challenging scenarios for AV testing efficiently. We validate our approach using Root Mean Square Error(RMSE) to compare the scenarios generated using the proposed parameters against the realworld trajectory data. In addition to that, we demonstrate that adding a slight disturbance to a few scenario parameters can generate different scenarios and utilise Responsibility-Sensitive Safety (RSS) metric to measure the scenarios’ risk.",ArXiv,2022,10.48550/arXiv.2206.09744,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
17f1cf7431e1867442cba9854e86f5c907b27e1c,https://www.semanticscholar.org/paper/17f1cf7431e1867442cba9854e86f5c907b27e1c,A Support Vector Machine-based Prediction Model for Electrochemical Machining Process,"Manufacturing of quality products is one of the core measures to address competitiveness in industries. Hence, it is always necessary to accomplish quality prediction at early stages of a manufacturing process to attain high quality products at the minimum possible cost. To achieve this goal, the past researchers have developed and investigated the applications of different intelligent techniques for their effective deployment at various stages of manufacturing processes. In this paper, support vector machine (SVM), a supervised learning system based on a novel artificial intelligence paradigm, is employed for prediction of three responses, like material removal rate, surface roughness and radial overcut during an electrochemical machining (ECM) operation. Gaussian radial basis kernel function is adopted in this algorithm to provide higher prediction accuracy. Regression analyses are also carried out to validate the effectiveness of these prediction models. The SVM-based results show good agreement between the experimental and predicted response values as compared to linear and quadratic models. Among the four ECM process parameters, i.e. applied voltage, tool feed rate, electrolyte concentration and percentage of reinforcement of B4C particles in the metal matrix, tool feed rate is identified having the maximum influence on the considered responses.",Karbala International Journal of Modern Science,2020,10.33640/2405-609x.1508,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
dab813204988d9284d59b1004f190b88f6feb35f,https://www.semanticscholar.org/paper/dab813204988d9284d59b1004f190b88f6feb35f,Managing the Dynamics of New Technologies in the Global Supply Chain,"Technology has been rapidly changing the business environment over the past several decades, and these changes are occurring at an accelerating rate. How to effectively integrate, deploy, and manage technologies in a cost-efficient manner across an organization's supply chain is a key concern of business practitioners. Due to the significant resources of personnel and money required to implement these technologies, as well as the potential for disruptions to ongoing business operations, it is imperative for technology implementers to reduce their risks. In this article, we discuss the supply chain impact of the following technologies: radio frequency identification, the Internet of Things, Industry 4.0, artificial intelligence and machine learning, and blockchains. We identify the implementation challenges and expected benefits of these technologies and the managerial implications of merging the technologies to create a long-term strategic competitive advantage within a multiorganizational supply chain setting.",IEEE Engineering Management Review,2020,10.1109/EMR.2020.2968889,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
114aa720872462b0ca1b97bfdec0ebd56c36fd0a,https://www.semanticscholar.org/paper/114aa720872462b0ca1b97bfdec0ebd56c36fd0a,Towards Understanding and Mitigating Social Biases in Language Models,"Warning: this paper contains model outputs that may be offensive or upsetting. As machine learning methods are deployed in realworld settings such as healthcare, legal systems, and social science, it is crucial to recognize how they shape social biases and stereotypes in these sensitive decision-making processes. Among such real-world deployments are large-scale pretrained language models (LMs) that can be potentially dangerous in manifesting undesirable representational biases harmful biases resulting from stereotyping that propagate negative generalizations involving gender, race, religion, and other social constructs. As a step towards improving the fairness of LMs, we carefully define several sources of representational biases before proposing new benchmarks and metrics to measure them. With these tools, we propose steps towards mitigating social biases during text generation. Our empirical results and human evaluation demonstrate effectiveness in mitigating bias while retaining crucial contextual information for highfidelity text generation, thereby pushing forward the performance-fairness Pareto frontier.",ICML,2021,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
8d5b9403278c1b0939488285a0dddefe274213d8,https://www.semanticscholar.org/paper/8d5b9403278c1b0939488285a0dddefe274213d8,Improving automated visual fault inspection for semiconductor manufacturing using a hybrid multistage system of deep neural networks,,J. Intell. Manuf.,2022,10.1007/s10845-021-01906-9,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
f569e6285f0a61ba5a66590d3f415737d0d5b791,https://www.semanticscholar.org/paper/f569e6285f0a61ba5a66590d3f415737d0d5b791,Surveillance of Parimutuel Wagering Integrity Using Expert Systems and Machine Learning,"Parimutuel wagering is a significant source of revenue for many state governments. MonitorPlus is a surveillance system for parimutuel operators and regulators. Using industry expertise and best practices, MonitorPlus examines each and every wager and account transaction for evidence of fraud, crime, and money laundering. Alerts are generated in real-time. In forensic discovery mode, MonitorPlus is designed to collaborate with skilled analysts to discover more complex suspicious wagering patterns. MonitorPlus utilizes machine learning, so its risk profiles are current: its knowledge base improves with time. Each alert is accompanied by an automatically generated, rule-based explanation. This is critically important if an event rises to the level where legal action is required. Our development and deployment strategy is based on a new paradigm of a secure surveillance utility, where real-time alerts and dataintensive forensics support multiple regulatory jurisdictions. We believe this surveillance paradigm can be applied to other application domains such as lotteries, casinos, online gaming, and financial services.",IAAI,2010,10.1609/aaai.v24i2.18805,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
52be9068834217408fa5ada0ab32eb6c3619bebc,https://www.semanticscholar.org/paper/52be9068834217408fa5ada0ab32eb6c3619bebc,ADABench - Towards an Industry Standard Benchmark for Advanced Analytics,,TPCTC,2019,10.1007/978-3-030-55024-0_4,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
6f9a8003fbe29f7586f94d1a9d0d675760a1e9a3,https://www.semanticscholar.org/paper/6f9a8003fbe29f7586f94d1a9d0d675760a1e9a3,Low-cost passive wireless water leak sensor for the automotive industry,"The evolution of wireless technologies has shifted the expectations on how sensors anddata collection is performed. By reducing the complexity to deploy and connect sensors, anincrease in the variety and quantity of data collected is leading to the concept of ubiquitoussensing.However, there are measurements that still rely on manual work. In particular, the auto-motive industry still performs manually the rain isolation quality tests. These tests requirenowadays for an employee to manually place a handheld moisture sensor device in con-tact with the area to analyse, taking long time, low scalability and prone to human errors.Additionally, workers are exposed to these humid conditions which is hazardous for theirhealth.In order to tackle this problem, low-cost wireless identification tags are proposed in thiswork to be used instead. These tags, known as Radio Frequency Identification (RFID),are bundled in pairs, one waterproofed and one exposed to work as moisture detectionsensor. By interacting with them with a RFID telecommunications system, low level datais collected to perform predictions about their status. This way, it is possible to measuresignal differences when subjected to different quantities of water exposure, both in staticand in-movement scenarios, which lead to predictive methods of water leaks.In this work, different methodologies and setups will be evaluated on their performanceas predictors of wet RFID tags. The project involves researching the state of the art onthis topic, empirical experimentation of the different proposed methods, development of anautomated software platform and potential contribution to the academia with the resultsfound, by means of paper publication in conferences and/or journals. In particular, twodifferent experimental setups were tested, a single antenna setup and dual antenna setup.These setups were implemented in a controlled scenario, where 48 different experimen-tal iterations were performed. Later, the setup was implemented on a in-vehicle scenarioto emulate the real industrial operating scenario. In this scenario up to 70 different ex-perimental iterations where performed. When analysing the data samples, four differentmethods are proposed and implemented. Finally, the project also includes the evaluationthrough machine learning algorithms of the best method which achieved higher than80%accuracy test on predicting water leakages in real life scenario such as automotive “raintests”, through the developed sensors and system.With this research, the viability of this methodology for wireless detection of leaks hasbeen shown. Next steps are to explore different Tags manufacturers or complex scenariosinside production lines.",,2020,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
29811399204f74db0951c04b5b018177e0e7b5a4,https://www.semanticscholar.org/paper/29811399204f74db0951c04b5b018177e0e7b5a4,Redefining Banking and Financial Industry through the application of Computational Intelligence,"Computational Intelligence applications in banking and finance have been developed and deployed in the recent past and have been offering business solutions in both front end and back end processes in order to create efficiency and exceptional customer experience. In recent times, AI and machine learning are perceived to be the most valuable enabler to achieve competitive advantage by enhancing the decision making capabilities and transforming the banking industry. This paper will highlight the applications of AI and evaluate its utility in different functional areas of financial industry focusing primarily on automation of banking operations and customer engagement. It concludes with an analysis of how banking and financial organizations frame their environment and effectively use computational intelligence to improve their business.",2019 Advances in Science and Engineering Technology International Conferences (ASET),2019,10.1109/ICASET.2019.8714305,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
614945fc2f6f251f1cf69fa50d7faf0033c03484,https://www.semanticscholar.org/paper/614945fc2f6f251f1cf69fa50d7faf0033c03484,TensorFlow in Deep learning,"Deep learning has revolutionized the technology industry. Modern machine translation, search engines, and computer assistants are all powered by deep learning. TensorFlow is used to do all its complex work very simple. TensorFlow is an open source software library for high performance numerical computation. Its flexible architecture allows easy deployment of computation across a variety of platforms (CPUs, GPUs, TPUs), and from desktops to clusters of servers to mobile and edge devices. Originally developed by researchers and engineers from the Google Brain team within Google’s AI organization, it comes with strong support for machine learning and deep learning and the flexible numerical computation core is used across many other scientific domains. This trend will only continue as deep learning expands its reach into robotics, pharmaceuticals, energy, and all other fields of contemporary technology. It is rapidly becoming essential for the modern software professional to develop a working knowledge of the principles of deep learning.",,2019.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
7aa4e49e05e45d994a3628efd2ac57587adf46de,https://www.semanticscholar.org/paper/7aa4e49e05e45d994a3628efd2ac57587adf46de,Visible Light Positioning for Location-Based Services in Industry 4.0,"Industry 4.0 refers to the evolution in manufacturing from computerization to fully cyberphysical systems that exploit rich sensor data, adaptive real-time safety-critical control, and machine learning. An important aspect of this vision is the sensing and subsequent association of objects in the physical world with their cyber and virtual counterparts. In this paper we propose Visible Light Positioning (VLP) as an enabler for these Industry 4.0 applications. We also explore sensing techniques, including cameras (and depth sensors), and other light-based solutions for object positioning and detection along with their respective limitations. We then demonstrate an application of positioning for real time robot control in an interactive multiparty cyber-physical-virtual deployment. Lastly, based on our experience with this cyberphysical-virtual application, we propose Ray-Surface Positioning (RSP), a novel VLP technique, as a low cost positioning system for Industry 4.0.",2019 16th International Symposium on Wireless Communication Systems (ISWCS),2019.0,10.1109/ISWCS.2019.8877305,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
2a75b6768778e0609dd91dc663b7a6a4b879edec,https://www.semanticscholar.org/paper/2a75b6768778e0609dd91dc663b7a6a4b879edec,Transforming Underwriting in the Life Insurance Industry,"Life insurance provides trillions of dollars of financial security for hundreds of millions of individuals and families worldwide. Life insurance companies must accurately assess individual-level mortality risk to simultaneously maintain financial strength and price their products competitively. The traditional underwriting process used to assess this risk is based on manually examining an applicant’s health, behavioral, and financial profile. The existence of large historical data sets provides an unprecedented opportunity for artificial intelligence and machine learning to transform underwriting in the life insurance industry. We present an overview of how a rich application data set and survival modeling were combined to develop a life score that has been deployed in an algorithmic underwriting system at MassMutual, an American mutual life insurance company serving millions of clients. Through a novel evaluation framework, we show that the life score outperforms traditional underwriting by 6% on the basis of claims. We describe how engagement with actuaries, medical doctors, underwriters, and reinsurers was paramount to building an algorithmic underwriting system with a predictive model at its core. Finally, we provide details of the deployed system and highlight its value, which includes saving millions of dollars in operational efficiency while driving the decisions behind tens of billions of dollars of benefits.",AAAI,2019.0,10.1609/aaai.v33i01.33019373,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
791f43487e6c7cf588fdf191977233e87f5e00bb,https://www.semanticscholar.org/paper/791f43487e6c7cf588fdf191977233e87f5e00bb,Introduction to special issue on machine learning for adaptivity in spoken dialogue systems,"In the 1960s, AI researchers were predicting that machines capable of spoken dialogue behavior, somewhat like the preceding example, would be possible within a few decades. Fifty years later, we are still confronted with very difficult problems in AI, but we have powerful new tools with which to address the spoken dialogue problem. The research landscape in spoken dialogue systems has undergone significant changes over the past decade. This transformation has been the result of new momentum and fresh insights coming from the investigation of data-driven, statistical machine learning methods in three core areas of dialogue system research: spoken language understanding, dialogue management, and natural language generation. These methods hold the promise of mathematically precise approaches to system design, optimization, and evaluation, based on data collected from real user interactions with dialogue systems. The articles collected together in this special issue represent important themes in these research areas and illustrate current research directions in the field. As such, we hope that they will form a valuable resource for the international community of dialogue system researchers, both in industry and academia. Speech and language processing techniques have now achieved such a level of maturity that voice-enabled user interfaces are widely deployed and have created a billion dollar industry. However, the design and development of these interfaces is far from a simple and standardized process. Indeed, it is not enough to simply plug together speech recognition and synthesis systems: recognized speech must be understood in the context of the application, the overall interaction must be appropriately managed, and spoken",TSLP,2011.0,10.1145/1966407.1966408,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
bcb5d981e356ce5bf8c148c583f87fc3984c897a,https://www.semanticscholar.org/paper/bcb5d981e356ce5bf8c148c583f87fc3984c897a,ThunderML: A Toolkit for Enabling AI/ML Models on Cloud for Industry 4.0,,ICWS,2019.0,10.1007/978-3-030-23499-7_11,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
4f2c76d791dd2e1248188bd69d1ef18ea7fe9a50,https://www.semanticscholar.org/paper/4f2c76d791dd2e1248188bd69d1ef18ea7fe9a50,Deep-Learning and HPC to Boost Biomedical Applications for Health (DeepHealth),"This document introduces the DeepHealth project: ""Deep-Learning and HPC to Boost Biomedical Applications for Health"". This project is funded by the European Commission under the H2020 framework program and aims to reduce the gap between the availability of mature enough AI-solutions and their deployment in real scenarios. Several existing software platforms provided by industrial partners will integrate state-of-the-art machine-learning algorithms and will be used for giving support to doctors in diagnosis, increasing their capabilities and efficiency. The DeepHealth consortium is composed by 21 partners from 9 European countries including hospitals, universities, large industry and SMEs.",2019 IEEE 32nd International Symposium on Computer-Based Medical Systems (CBMS),2019.0,10.1109/CBMS.2019.00040,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
0265553e0763d712fc86b52930d9c40b8b9d9ee0,https://www.semanticscholar.org/paper/0265553e0763d712fc86b52930d9c40b8b9d9ee0,Missing the Forest for the Trees: End-to-End AI Application Performance in Edge Data Centers,"Artificial intelligence and machine learning are experiencing widespread adoption in the industry, academia, and even public consciousness. This has been driven by the rapid advances in the applications and accuracy of AI through increasingly complex algorithms and models; this, in turn, has spurred research into developing specialized hardware AI accelerators. The rapid pace of the advances makes it easy to miss the forest for the trees: they are often developed and evaluated in a vacuum without considering the full application environment in which they must eventually operate. In this paper, we deploy and characterize Face Recognition, an AI-centric edge video analytics application built using open source and widely adopted infrastructure and ML tools. We evaluate its holistic, end-to-end behavior in a production-size edge data center and reveal the ""AI tax"" for all the processing that is involved. Even though the application is built around state-of-the-art AI and ML algorithms, it relies heavily on pre-and post-processing code which must be executed on a general-purpose CPU. As AI-centric applications start to reap the acceleration promised by so many accelerators, we find they impose stresses on the underlying software infrastructure and the data center's capabilities: storage and network bandwidth become major bottlenecks with increasing AI acceleration. By not having to serve a wide variety of applications, we show that a purpose-built edge data center can be designed to accommodate the stresses of accelerated AI at 15% lower TCO than one derived from homogeneous servers and infrastructure. We also discuss how our conclusions generalize beyond Face Recognition as many AI-centric applications at the edge rely upon the same underlying software and hardware infrastructure.",2020 IEEE International Symposium on High Performance Computer Architecture (HPCA),2020.0,10.1109/HPCA47549.2020.00049,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
df686eccf406cd725dc47e9f6334518163dc1cf6,https://www.semanticscholar.org/paper/df686eccf406cd725dc47e9f6334518163dc1cf6,0 Machine Learning for Adaptivity in Spoken Dialogue Systems,"In the 1960s, AI researchers were predicting that machines capable of spoken dialogue behaviour, somewhat like the example above, would be possible within a few decades. Fifty years later, we are still confronted with very difficult problems in AI, but we have powerful new tools with which to address the spoken dialogue problem. The research landscape in spoken dialogue systems has undergone significant changes over the past decade. This transformation has been the result of new momentum and fresh insights coming from the investigation of data-driven, statistical machine learning methods in three core areas of dialogue system research: Spoken Language Understanding, Dialogue Management, and Natural Language Generation. These methods hold the promise of mathematically precise approaches to system design, optimisation, and evaluation, based on data collected from real user interactions with dialogue systems. The papers collected together in this special issue represent important themes in these research areas, and illustrate current research directions in the field. As such, we hope that they will form a valuable resource for the international community of dialogue system researchers, both in industry and academia. Speech and language processing techniques have now achieved such a level of maturity that voice-enabled user interfaces are widely deployed, and have created a billion dollar industry. However, the design and development of these interfaces is far from being a simple and standardised process. Indeed, it is not enough to simply plug together speech recognition and synthesis systems: recognised speech must be understood in the context of the application, the overall interaction must be appropriately managed, and spoken language must be suitably generated to improve overall system efficiency",,2011.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
ee9e36c0abe8fd1c48aa73d1f83bcbb22fe3670b,https://www.semanticscholar.org/paper/ee9e36c0abe8fd1c48aa73d1f83bcbb22fe3670b,Hyper-Tune: Towards Efficient Hyper-parameter Tuning at Scale,"The ever-growing demand and complexity of machine learning are putting pressure on hyper-parameter tuning systems: while the evaluation cost of models continues to increase, the scalability of state-of-the-arts starts to become a crucial bottleneck. In this paper, inspired by our experience when deploying hyper-parameter tuning in a real-world application in production and the limitations of existing systems, we propose Hyper-Tune, an efficient and robust distributed hyper-parameter tuning framework. Compared with existing systems, Hyper-Tune highlights multiple system optimizations, including (1) automatic resource allocation, (2) asynchronous scheduling, and (3) multi-fidelity optimizer. We conduct extensive evaluations on benchmark datasets and a large-scale realworld dataset in production. Empirically, with the aid of these optimizations, Hyper-Tune outperforms competitive hyper-parameter tuning systems on a wide range of scenarios, including XGBoost, CNN, RNN, and some architectural hyper-parameters for neural networks. Compared with the state-of-the-art BOHB and A-BOHB, Hyper-Tune achieves up to 11.2× and 5.1× speedups, respectively. PVLDB Reference Format: Yang Li, Yu Shen, Huaijun Jiang, Wentao Zhang, Jixiang Li, Ji Liu, Ce Zhang, and Bin Cui. Hyper-Tune: Towards Efficient Hyper-parameter Tuning at Scale. PVLDB, 14(1): XXX-XXX, 2020. doi:XX.XX/XXX.XX PVLDB Availability Tag: The source code of this research paper has been made publicly available at https://github.com/PKU-DAIR/HyperTune.",Proc. VLDB Endow.,2022.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
058b47ee71aedf48b852f10baf415c368cbd5cc4,https://www.semanticscholar.org/paper/058b47ee71aedf48b852f10baf415c368cbd5cc4,AI-driven platform enterprise maturity: from human led to machine governed,"PurposeTo be more effective, artificial intelligence (AI) requires a broad overall view of the design and transformation of enterprise architecture and capabilities. Maturity models (MMs) are the recognized tools to identify strengths and weaknesses of certain domains of an organization. They consist of multiple, archetypal levels of maturity of a certain domain and can be used for organizational assessment and development. In the case of AI, quite a few numbers of MMs have been proposed. Generally, the links between AI technology, AI usage and organizational performance stay unclear. To address these gaps, this paper aims to introduce the complete details of the AI maturity model (AIMM) for AI-driven platform companies. The associated AI-Driven Platform Enterprise Maturity framework proposed here can help to achieve most of the AI-driven platform companies' objectives.Design/methodology/approachQualitative research is performed in two stages. In the first stage, a review of the existing literature is performed to identify the types, barriers, drivers, challenges and opportunities of MMs in AI, Advanced Analytics and Big Data domains. In the second stage, a research framework is proposed to align company value chain with AI technologies and levels of the platform enterprise maturity.FindingsThe paper proposes a new five level AI-Driven Platform Enterprise Maturity framework by constructing a formal organizational value chain taxonomy model that explains a vast group of MM phenomena related with the AI-Driven Platform Enterprises. In addition, this study proposes a clear and precise description and structuring of the information in the multidimensional Platform, AI, Advanced Analytics and Big Data domains. The AI-Driven Platform Enterprise Maturity framework assists in identification, creation, assessment and disclosure research of AI-driven platform business organizations.Research limitations/implicationsThis research is focused on the basic dimensions of AI value chain. The full reference model of AI consists of much more concepts. In the last few years, AI has achieved a notable drive that, if connected appropriately, may deliver the best of expectations over many application sectors across the field. For this to occur shortly in machine learning, especially in deep neural networks, the entire community stands in front of the barrier of explainability. Paradigms underlying this problem fall within the so-called eXplainable AI (XAI) field, which is widely acknowledged as a crucial feature for the practical deployment of AI models in industry. Our prospects lead toward the concept of a methodology for the large-scale implementation of AI methods in platform organizations with fairness, model explainability and accountability at its core.Practical implicationsAI-driven platform enterprise maturity framework can be used for better communicate to clients the value of AI capabilities through the lens of changing human-machine interactions and in the context of legal, ethical and societal norms.Social implicationsThe authors discuss AI in the enterprise platform stack including talent platform, human capital management and recruiting.Originality/valueThe AI value chain and AI-Driven Platform Enterprise Maturity framework are original and represent an effective tools for assessing AI-driven platform enterprises.",Kybernetes,2021.0,10.1108/K-06-2020-0384,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
64b474666d9c5b98b424bb7dc51204fb4ca4c421,https://www.semanticscholar.org/paper/64b474666d9c5b98b424bb7dc51204fb4ca4c421,"Learning to Grasp for Robotics Applications in Uncertain
Environments","We are currently witnessing a revolution in the use and deployment of robotics systems. On top of an increased pace of automation in traditional industries like automotive and electronics manufacturing, robots are now used in a wide range of applications, from health care to agriculture. New records of robot sales have been set every year for the past several years. This revolution is fueled by incredible advances in several fields including machine learning, machine vision, human-robot interaction, and computing systems and architectures.",,2020.0,10.11159/cdsr20.04,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
72c3753ef9385179d68e81377a08e090b6e5183f,https://www.semanticscholar.org/paper/72c3753ef9385179d68e81377a08e090b6e5183f,Remodeling Hospitality Industry through Artificial Intelligence,"Remodeling the hospitality industry through artificial intelligence (AI) – that uses big data analytics and complex machine learning – is a concept that will help the industry to leapfrog to the next level. The notion put forward in this paper is to develop a framework to utilize machine learning analyses the multi-channel user data for efficient decision making to enrich the customer experience and to provide the maximum revenue to the vendor. We propose strategies to infer customer behaviors by capturing otherwise salient information – e.g. through the various digital footprints. Feeding such analytics to a suitably trained collection of machine learning algorithms called the “Digital Operations Manager” helps to automate complex decision making, removing human error and bias. The proposed AI system, if appropriately deployed within a hospitality industry environment, is thought to bring out a significant gain in the user choice and experience as well as efficiency in resource management and revenue optimization.","2019 13th International Conference on Software, Knowledge, Information Management and Applications (SKIMA)",2019.0,10.1109/SKIMA47702.2019.8982488,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
27a92f069aa172d734690cc8517bbe84f96c6b08,https://www.semanticscholar.org/paper/27a92f069aa172d734690cc8517bbe84f96c6b08,Performance Evaluation of Distributed Deep Learning Frameworks in Cloud Environment,"2016 has become the year of the Artificial Intelligence explosion. AI technologies are getting more and more matured that most world well-known tech giants are making large investment to increase the capabilities in AI. Machine learning is the science of getting computers to act without being explicitly programmed, and deep learning is a subset of machine learning that uses deep neural network to train a machine to learn features directly from data. Deep learning realizes many machine learning applications which expand the field of AI. At the present time, deep learning frameworks have been widely deployed on servers for deep learning applications in both academia and industry. In training deep neural networks, there are many standard processes or algorithms, but the performance of different frameworks might be different. In this paper we evaluate the running performance of two state-of-the-art distributed deep learning frameworks that are running training calculation in parallel over multi GPU and multi nodes in our cloud environment. We evaluate the training performance of the frameworks with ResNet-50 convolutional neural network, and we analyze what factors that result in the performance among both distributed frameworks as well. Through the experimental analysis, we identify the overheads which could be further optimized. The main contribution is that the evaluation results provide further optimization directions in both performance tuning and algorithmic design. Keywords—Artificial Intelligence, machine learning, deep learning, convolutional neural networks",,2019.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
f02f6e666aef6b0675cc4a189f9962b716c17487,https://www.semanticscholar.org/paper/f02f6e666aef6b0675cc4a189f9962b716c17487,FARE: Enabling Fine-grained Attack Categorization under Low-quality Labeled Data,"Supervised machine learning classifiers have been widely used for attack detection, but their training requires abundant high-quality labels. Unfortunately, high-quality labels are difficult to obtain in practice due to the high cost of data labeling and the constant evolution of attackers. Without such labels, it is challenging to train and deploy targeted countermeasures. In this paper, we propose FARE, a clustering method to enable fine-grained attack categorization under low-quality labels. We focus on two common issues in data labels: 1) missing labels for certain attack classes or families; and 2) only having coarsegrained labels available for different attack types. The core idea of FARE is to take full advantage of the limited labels while using the underlying data distribution to consolidate the lowquality labels. We design an ensemble model to fuse the results of multiple unsupervised learning algorithms with the given labels to mitigate the negative impact of missing classes and coarsegrained labels. We then train an input transformation network to map the input data into a low-dimensional latent space for fine-grained clustering. Using two security datasets (Android malware and network intrusion traces), we show that FARE significantly outperforms the state-of-the-art (semi-)supervised learning methods in clustering quality/correctness. Further, we perform an initial deployment of FARE by working with a large e-commerce service to detect fraudulent accounts. With realworld A/B tests and manual investigation, we demonstrate the effectiveness of FARE to catch previously-unseen frauds.",NDSS,2021.0,10.14722/NDSS.2021.24403,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
dba3478cb46874a97e301deb0f20f7692c1f1ae9,https://www.semanticscholar.org/paper/dba3478cb46874a97e301deb0f20f7692c1f1ae9,PenDer: Incorporating Shape Constraints via Penalized Derivatives,"When deploying machine learning models in the real-world, system designers may wish that models exhibit certain shape behavior, i.e., model outputs follow a particular shape with respect to input features. Trends such as monotonicity, convexity, diminishing or accelerating returns are some of the desired shapes. Presence of these shapes makes the model more interpretable for the system designers, and adequately fair for the customers. We notice that many such common shapes are related to derivatives, and propose a new approach, PenDer (Penalizing Derivatives), which incorporates these shape constraints by penalizing the derivatives. We further present an Augmented Lagrangian Method (ALM) to learn the joint unconstrained objective function. Experiments on three realworld datasets illustrate that even though both PenDer and state-of-the-art Lattice models achieve similar conformance to shape, PenDer captures better sensitivity of prediction with respect to intended features. We also demonstrate that PenDer achieves better test performance than Lattice while enforcing more desirable shape behavior.",AAAI,2021.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
d3ac65b10af091759863b8e2c488036bf52a2ce6,https://www.semanticscholar.org/paper/d3ac65b10af091759863b8e2c488036bf52a2ce6,DAG Card is the new Model Card,"With the progressive commoditization of modeling capabilities, data-centric AI recognizes that what happens before and after training becomes crucial for realworld deployments. Following the intuition behind Model Cards, we propose DAG Cards as a form of documentation encompassing the tenets of a data-centric point of view. We argue that Machine Learning pipelines (rather than models) are the most appropriate level of documentation for many practical use cases, and we share with the community an open implementation to generate cards from code.",ArXiv,2021.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
3c01ba4b376577517823039c9f1a6511d69e1c2d,https://www.semanticscholar.org/paper/3c01ba4b376577517823039c9f1a6511d69e1c2d,Intelligent Embedded Load Detection at the Edge on Industry 4.0 Powertrains Applications,"In the context of Industry 4.0, there has been great focus in developing intelligent sensors. Deploying them, condition monitoring and predictive maintenance have become feasible solutions to minimize operating and maintenance costs while also increasing safety. A critical aspect is the applied load to the supervised machinery system. Vibration data can be used to determine the current condition, but this needs signal processing specially developed and adapted to the monitored machine part for feature extraction. Artificial intelligence (AI) can, on one hand, simplify the development of such special purpose processing and on another hand be used to monitor and classify machine conditions by learning features directly from data. By bringing the AI computation as close as possible to the sensor (Edge-AI), data bandwidth can be minimized, system scalability and responsiveness can be improved and real-time requirements can be fulfilled. This work describes how Edge-AI on a STM32-bit microcontroller can be implemented. Our experimental setup demonstrates how AI can be effectively used to detect and classify the load on a powertrain. In order to do this, we use a MEMS capacity accelerometer to sense vibrations of the system. Also, this work demonstrates how Deep Neural Networks (DNN) for signal classification are build and trained by using an open-source deep learning framework and how the code library for the microcontroller is automatically generated afterwards by using STM32Cube. AI toolchain. We compare the classification accuracy of a memory compressed DNN against an uncompressed DNN.",2019 IEEE 5th International forum on Research and Technology for Society and Industry (RTSI),2019.0,10.1109/RTSI.2019.8895598,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
2516652a00f5620ff27a1a7c99a99a7926466ca2,https://www.semanticscholar.org/paper/2516652a00f5620ff27a1a7c99a99a7926466ca2,MLaaS : A Framework for Exposing Machine Learning as a Service on Cloud Platforms,"​ ​​Machine​ ​Learning​ ​has​ ​been​ ​seeping​ ​into​ ​every​ ​sphere​ ​of​ ​our​ ​lives​ ​from​ ​autonomous driving,​ ​movie​ ​recommendations​ ​and​ ​online​ ​shopping​ ​to​ ​targeted​ ​advertising​ ​campaigns, detecting​ ​anomalous​ ​masses​ ​in​ ​the​ ​brain​ ​and​ ​stock​ ​market​ ​analysis.​ ​With​ ​the​ ​impact​ ​machine learning​ ​has​ ​had​ ​on​ ​our​ ​lives,​ ​there​ ​have​ ​been​ ​advocates​ ​for​ ​democratizing​ ​the​ ​science​ ​behind​ ​it to​ ​make​ ​it​ ​more​ ​accessible​ ​to​ ​people​ ​who​ ​might​ ​need​ ​it.​ ​In​ ​our​ ​project,​ ​we​ ​aim​ ​to​ ​make​ ​a contribution​ ​to​ ​this​ ​drive​ ​by​ ​exposing​ ​the​ ​power​ ​of​ ​machine​ ​learning​ ​as​ ​a​ ​service​ ​on​ ​a​ ​cloud based​ ​distributed​ ​platform.​ ​We​ ​have​ ​striven​ ​to​ ​abstract​ ​the​ ​intricacies​ ​of​ ​training​ ​and​ ​predicting on​ ​data​ ​as​ ​much​ ​as​ ​possible​ ​to​ ​allow​ ​the​ ​end​ ​user​ ​to​ ​focus​ ​on​ ​the​ ​application​ ​of​ ​the​ ​​machine learning​​ ​rather​ ​than​ ​the​ ​science​ ​behind​ ​it.​ ​We​ ​have​ ​designed​ ​a​ ​distributed​ ​framework​ ​capable​ ​of both​ ​automatic​ ​scale-up​ ​and​ ​scale-out​ ​to​ ​ensure​ ​that​ ​the​ ​user​ ​gets​ ​the​ ​performance​ ​they​ ​seek​ ​from the​ ​service​ ​without​ ​any​ ​hiccups. 1.​​ ​​ ​​ ​​ ​​ ​​Introduction: Machine Learning[1] has boomed in the last few years from being used by just a handful of computer engineers exploring whether computers could learn to play games and mimic the human brain, to a broad discipline that has produced fundamental statistical and computational theories of learning trends and patterns in any and all forms of data. In recent years, machine learning has been incorporated into almost all applications, from driving cars to monitoring your heart-rate in order to detect anomalies. But even now, ​machine learning is still inaccessible outside a niche community pushing forward it development. However, that trend is changing and in recent years, we have seen the community working towards making machine learning accessible​ ​to​ ​all​ ​[2]. Our project aims to give anybody access to the learning capabilities of machine learning without the baggage of having to understand the inner workings of its various algorithms. Additionally, the power of machine learning is amplified by the benefits of cloud computing. Users can offload the computational and management heavy-lifting to the cloud wherein the performance guarantee is ensured by the inherent automatic scale-out and scale-up design principles of the same. Our framework is primarily targeted towards IoT developers in need of real-time training and prediction on raw data, which can be accomplished through our API endpoints. However, we have designed our framework to also allow for a more hands-on and guided setting up of a machine​ ​learning​ ​pipeline​ ​through​ ​an​ ​intuitive​ ​user​ ​interface. 2.​​ ​​ ​​ ​​ ​​ ​​Related​ ​Work: Machine Learning as a Service: Baldominos et al.[3] also proposed a platform built on top of Hadoop. Its implementation was capable of handling up to 30 requests at one time while maintaining a response time of less than one second. Our implementation, on the other hand, is not built on top of any existing distributed computing framework. OpenCPU is another open-source platform, launched in 2014, that creates a Web API for R, a popular statistical analysis software environment. However, because it is practically a middleware for accessing R functions, it does not take into account many non-functional requirements like scalability and performance. In the industry, Google, Microsoft, and Amazon have been releasing their own proprietary platforms. Google released its Prediction API2 [4] in 2014 but it largely allows for prediction capabilities on Google’s own pretrained machine learning model. Also in 2014, Microsoft launched Azure Machine Learning .and inn 2015, Amazon released AWS Machine Learning. Their popularity proves that the demand exists but unfortunately, the designs and implementation specifications​ ​of​ ​these​ ​products​ ​are​ ​not​ ​publicly​ ​available. PredictionIO, OpenCPU, and Baldominos’ platforms are built on top of a specific analytical tools and suffer from its restrictions. This means less flexibility for adding new machine learning algorithms, for data storage, and for deployment. Although Hadoop and R are open-source projects, it is not a trivial challenge to adapt them to a new approach. The same happens with the industry players and their proprietary solutions when external developers cannot have access to the​ ​code​ ​to​ ​add​ ​new​ ​algorithms. Machine Learning Through An Interface: We have striven to abstract the complexities involved with setting up a machine learning pipeline as much as possible. This allows users to focus on the results rather than the intermediate steps of reaching their goals. The closest to our work is Keras [5] which offers an easy to use programming interface over more involved backend libraries like Tensorflow [6] or Theano [7]. Our implementation on the other hand is library agnostic and in our attempt to provide fast and reliable results, we have chosen whichever library​ ​or​ ​tool​ ​is​ ​best​ ​suited​ ​to​ ​the​ ​task​ ​at​ ​hand. Moreover, different models can perform better or worse, depending on the used algorithms, parameters and data set. There is no such a thing as the best learning algorithm. For any algorithm, there are data sets that perform very accurately and others that perform very poorly. For the same data set, different algorithms can perform differently because of their own nature. MLaaS helps the user to run multiple algorithms and compare their performances, so the most suitable​ ​algorithm​ ​can​ ​be​ ​chosen. 3.​​ ​​ ​​ ​​Implementation​ ​Details: Our approach for this project is to build an open source cloud framework powered by a highly custom server-client architecture that handles user interaction with management of machine learning​ ​tools. The architecture of our proposed model is described above. The architecture is separated into two primary zones the Client and the Cloud. The MLaaS framework ties components in both these zones together and utilizes the developed mechanisms to alleviate a seamless interaction between the user and cloud framework. The user interacts with a client application which allows several interactions to access the functionalities and features of the MLaaS cloud framework. The user can create a machine learning model using the front-end client application which simultaneously initializes a shared placeholder that has all the information related to the model to be trained including, but not restricted to, the model name, type, parameters and the dataset to be trained. The different​ ​components​ ​and​ ​its​ ​role​ ​in​ ​the​ ​architecture​ ​is​ ​described​ ​below: 1. Client: a. The client application allows the user to manage multiple machine learning models and offers interfaces for these models to be exposed through the cloud as a personalized​ ​RESTful​ ​service. b. The client application exposes an interface to the user for the management of machine learning model. It is responsible for assisting the user with the following operations: 1. Creation​ ​of​ ​a​ ​Machine​ ​Learning​ ​Model 2. Reading​ ​Properties​ ​of​ ​an​ ​existing​ ​Machine​ ​Learning​ ​Model 3. Updating​ ​the​ ​Properties​ ​of​ ​an​ ​existing​ ​Machine​ ​Learning​ ​Model 4. Deleting​ ​an​ ​existing​ ​Machine​ ​Learning​ ​Model 5. Triggering the training of an existing Machine Learning Model on the Distributed​ ​Framework. 6. Displaying the URL for the user to access the Machine Learning model for real-time​ ​use. c. The​ ​following​ ​screenshots​ ​highlight​ ​the​ ​main​ ​features​ ​of​ ​the​ ​Client​ ​Application",,,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
29d12a495ae810de9e3bc6e53af31934a164b821,https://www.semanticscholar.org/paper/29d12a495ae810de9e3bc6e53af31934a164b821,Accelerating Scientific Applications With SambaNova Reconfigurable Dataflow Architecture,"Artificial intelligence (AI)-driven science is an integral component in several science domains such as materials, biology, high energy physics, and smart energy. Science workflows can span one or more computational, observational, and experimental systems. The AI for Science report put forth by a wide community of stakeholders from national laboratories, academia, and industry collectively stress the need for a tighter integration of AI infrastructure ecosystem with experimental and leadership computing facilities. The AI component of science applications, which generally deploy deep learning (DL) models, are unique and exhibit different characteristics from traditional industrial workloads. They implement complex models and typically incorporate hundreds of millions of model parameters. Data from simulations are usually sparse, multimodal, multidimensional, and exhibit temporal and spatial correlations.Moreover, AI-driven science applications benefit from flexible coupling of simulations with DL training or inference. Such complexity of the AI for science workloads with increasingly large DL models is typically limited by traditional computing architectures. The adoption of novel AI architectures and systems aimed to accelerate machine learning models is critical to reduce the time-to-discovery for science. The Argonne Leadership Computing Facility (ALCF), a US Department of Energy Office of Science user facility, provides supercomputing resources to power scientific breakthroughs. Applications with significant DL components are increasingly being run on existing supercomputers at the facility. Scientists at ALCF are exploring novel AI-hardware systems, such as SambaNova, in an attempt to address the challenges in scaling the performance of AI models.",Computing in Science & Engineering,2021.0,10.1109/MCSE.2021.3057203,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
0ce7eb261e452f47706b0c2dea4295b1dd9a8e28,https://www.semanticscholar.org/paper/0ce7eb261e452f47706b0c2dea4295b1dd9a8e28,A Framework for Implementing Prediction Algorithm over Cloud Data as a Procedure for Cloud Data Mining,"The cloud has become an important phrase in data storage for many reasons. Cloud services and applications are widespread in many industries including healthcare due to easy access. The limitless quantity of data available on the clouds has triggered the interest of many researchers in the recent past. It has forced us to deploy machine learning for analyzing the data to get insights as well as model building. In this paper, we have built a service on Heroku Cloud which is a cloud platform as a service (PaaS) and has 15 thousand records with 25 features. The data belongs to healthcare and is related to post-surgery complications. The boost prediction algorithm was applied for analysis and implementation was done in python. The results helped us to determine and tune some of the hyperparameters which have correlations with complications and the reported accuracy of training and testing was found to be 91% and 88% respectively.",Journal of Informatics Electrical and Electronics Engineering (JIEEE),2021.0,10.54060/jieee/002.02.021,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
e8b5b6c135e91ec71f7aa0ff7195f6cc229bbc8c,https://www.semanticscholar.org/paper/e8b5b6c135e91ec71f7aa0ff7195f6cc229bbc8c,Edge Inference for UWB Ranging Error Correction Using Autoencoders,"Indoor localization knows many applications, such as industry 4.0, warehouses, healthcare, drones, etc., where high accuracy becomes more critical than ever. Recent advances in ultra-wideband localization systems allow high accuracies for multiple active users in line-of-sight environments, while they still introduce errors above 300 mm in non-line-of-sight environments due to multi-path effects. Current work tries to improve the localization accuracy of ultra-wideband through offline error correction approaches using popular machine learning techniques. However, these techniques are still limited to simple environments with few multi-path effects and focus on offline correction. With the upcoming demand for high accuracy and low latency indoor localization systems, there is a need to deploy (online) efficient error correction techniques with fast response times in dynamic and complex environments. To address this, we propose (i) a novel semi-supervised autoencoder-based machine learning approach for improving ranging accuracy of ultra-wideband localization beyond the limitations of current improvements while aiming for performance improvements and a small memory footprint and (ii) an edge inference architecture for online UWB ranging error correction. As such, this paper allows the design of accurate localization systems by using machine learning for low-cost edge devices. Compared to a deep neural network (as state-of-the-art, with a baseline error of 75 mm) the proposed autoencoder achieves a 29% higher accuracy. The proposed approach leverages robust and accurate ultra-wideband localization, which reduces the errors from 214 mm without correction to 58 mm with correction. Validation of edge inference using the proposed autoencoder on a NVIDIA Jetson Nano demonstrates significant uplink bandwidth savings and allows up to 20 rapidly ranging anchors per edge GPU.",IEEE Access,2020.0,10.1109/access.2020.3012822,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
6a1ef8c870ccb24eed8c11c761f88ed99c118094,https://www.semanticscholar.org/paper/6a1ef8c870ccb24eed8c11c761f88ed99c118094,Oracle AutoML,"Machine learning (ML) is at the forefront of the rising popularity of data-driven software applications. The resulting rapid proliferation of ML technology, explosive data growth, and shortage of data science expertise have caused the industry to face increasingly challenging demands to keep up with fast-paced develop-and-deploy model lifecycles. Recent academic and industrial research efforts have started to address this problem through automated machine learning (AutoML) pipelines and have focused on model performance as the first-order design objective. We present Oracle AutoML, a novel iteration-free AutoML pipeline designed to not only provide accurate models, but also in a shorter runtime. We are able to achieve these objectives by eliminating the need to continuously iterate over various pipeline configurations. In our feed-forward approach, each pipeline stage makes decisions based on metalearned proxy models that can predict candidate pipeline configuration performances before building the full final model. Our approach, which builds and tunes only the best candidate pipeline, achieves better scores at a fraction of the time compared to state-of-the-art open source AutoML tools, such as H2O and Auto-sklearn. This makes Oracle AutoML a prime candidate for addressing current industry challenges.",Proc. VLDB Endow.,2020.0,10.14778/3415478.3415542,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
c2bdb33d4a402f0d530c5847fe0fe9c81de6a99b,https://www.semanticscholar.org/paper/c2bdb33d4a402f0d530c5847fe0fe9c81de6a99b,Using Small Business Banking Data for Explainable Credit Risk Scoring,Machine learning applied to financial transaction records can predict how likely a small business is to repay a loan. For this purpose we compared a traditional scorecard credit risk model against various machine learning models and found that XGBoost with monotonic constraints outperformed scorecard model by 7% in K-S statistic. To deploy such a machine learning model in production for loan application risk scoring it must comply with lending industry regulations that require lenders to provide understandable and specific reasons for credit decisions. Thus we also developed a loan decision explanation technique based on the ideas of WoE and SHAP. Our research was carried out using a historical dataset of tens of thousands of loans and millions of associated financial transactions. The credit risk scoring model based on XGBoost with monotonic constraints and SHAP explanations described in this paper have been deployed by QuickBooks Capital to assess incoming loan applications since July 2019.,AAAI,2020.0,10.1609/aaai.v34i08.7055,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
3352eb2b731571ca9ba85e671a214b2e2467d8d9,https://www.semanticscholar.org/paper/3352eb2b731571ca9ba85e671a214b2e2467d8d9,Online and Scalable Model Selection with Multi-Armed Bandits,"Many online applications running on live traffic are powered by machine learning models, for which training, validation, and hyperparameter tuning are conducted on historical data. However, it is common for models demonstrating strong performance in offline analysis to yield poorer performance when deployed online. This problem is a consequence of the difficulty of training on historical data in non-stationary environments. Moreover, the machine learning metrics used for model selection may not sufficiently correlate with real-world business metrics used to determine the success of the applications being tested. These problems are particularly prominent in the Real-Time Bidding (RTB) domain, in which ML models power bidding strategies, and a change in models will likely affect performance of the advertising campaigns. In this work, we present Automatic Model Selector (AMS), a system for scalable online selection of RTB bidding strategies based on realworld performance metrics. AMS employs Multi-Armed Bandits (MAB) to near-simultaneously run and evaluate multiple models against live traffic, allocating the most traffic to the bestperforming models while decreasing traffic to those with poorer online performance, thereby minimizing the impact of inferior models on overall campaign performance. The reliance on offline data is avoided, instead making model selections on a case-by-case basis according to actionable business goals. AMS allows new models to be safely introduced into live campaigns as soon as they are developed, minimizing the risk to overall performance. In livetraffic tests on multiple ad campaigns, the AMS system proved highly effective at improving ad campaign performance.",ArXiv,2021.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
ae6f3e289b6d4c62d81f6355e3a92f1b19efee63,https://www.semanticscholar.org/paper/ae6f3e289b6d4c62d81f6355e3a92f1b19efee63,Improving Custom Vision cognitive services model,"Computer vision is a branch of artificial intelligence that enables computers and systems to extract valuable information from digital pictures, movies, and other visual inputs to solve realworld visual problems. Machine learning is a branch of artificial intelligence. Deep lea rning is a subfield of machine learning. Cognitive Services are a set of machine learning ap proaches based on data mining. Cognitive Machine Learning is a sort of artificial intelligen ce designed to solve problems (AI). Computer vision technology powered by Deep Learning (DL) delivers real-world value to a variety of industries. Deep learning is used to evaluate photos in the Custom Vision service. Custom Vision employs a variety of deep learning models in the system, each of which is tuned for a certain goal, such as accuracy, training and inference speed, memory cost, and so on. Advanced training will use the Auto Machine learning system in the backend, which delivers superior training performance. Quick training only trains with a few default configurations, which costs less for training. By embedding cloud vision capabilities as part of Azure Cognitive Services, you can extract rich information from photos and video, analyze images and video in real time, and create solutions that more people can use. the research Label content with objects and concepts, extract language, generate image descriptions, censor content, and analyze people's movement in physical environments using visual data processing. The primary goal of this work is to present and demonstrate how to use Custom Vision cloud services, we’ll go through what these services are, when they should be used, and how they can help business in different domain.",Journal of the ACS Advances in Computer Science,2021.0,10.21608/asc.2021.240134,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
0f5dc98f4ad051b7e49aa31cfc8fae5c2b532a1e,https://www.semanticscholar.org/paper/0f5dc98f4ad051b7e49aa31cfc8fae5c2b532a1e,Using system context information to complement weakly labeled data,"Real-world datasets collected with sensor networks often contain incomplete and uncertain labels as well as artefacts arising from the system environment. Complete and reliable labeling is often infeasible for large-scale and long-term sensor network deployments due to the labor and time overhead, limited availability of experts and missing ground truth. In addition, if the machine learning method used for analysis is sensitive to certain features of a deployment, labeling and learning needs to be repeated for every new deployment. To address these challenges, we propose to make use of system context information formalized in an information graph and embed it in the learning process via contrastive learning. Based on realworld data we show that this approach leads to an increased accuracy in case of weakly labeled data and leads to an increased robustness and transferability of the classifier to new sensor locations.",ArXiv,2021.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
34c59ec266e2f1798918d765c2a2519a2b8d03d2,https://www.semanticscholar.org/paper/34c59ec266e2f1798918d765c2a2519a2b8d03d2,ReLink: Complete-Link Industrial Record Linkage Over Hybrid Feature Spaces,"Record Linkage (ReL) is the task of identifying records from a pair of databases referring to the same realworld entity. This has many applications in organisations of all sizes where related data often exist in silos leading to inefficiency in data engineering and analytics applications as well as ineffectiveness of business applications (e.g., unable to personalise marketing campaigns).State-of-the-art (SOTA) machine learning and deep learning based ReL techniques use adaptive similarity measures and learn their relative contributions based on labeled data. However, we report here that they do not work with similar efficacy on industrial data owing to its fundamental differences such as magnitude of schema heterogeneity, need for leveraging structure of the data, lack of training data etc. Through our proposed system ‘ReLink’, we carefully mitigate these challenges and demonstrate that it not only significantly outperforms SOTA baselines on industrial datasets but also on majority of research benchmarks. ReLink introduces the notion of complete-linkage over attributes as well as uses hybrid feature spaces on lexical and semantic similarity measures using pre-trained models such as BERT. Going beyond empirical demonstration, we provide insights and prescriptive guidance on choice of ReL techniques in industrial settings from our observations and lessons learnt from the experience of transferring and deploying for real use-cases in a large financial services organization.",2021 IEEE 37th International Conference on Data Engineering (ICDE),2021.0,10.1109/ICDE51399.2021.00293,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
275f66ef562a00d01e9eaa10b0363688b7c5e36f,https://www.semanticscholar.org/paper/275f66ef562a00d01e9eaa10b0363688b7c5e36f,Xilinx Vitis Unified Software Platform,"FPGAs provide significant advantages in throughput, latency, and energy efficiency for implementing low-latency, compute-intensive applications when compared to general-purpose CPUs and GPUs. Over the last decade, FPGAs have evolved into highly configurable SoCs with on-chip CPUs, domain-specific programmable accelerators, and flexible connectivity options. Recently, Xilinx introduced a new heterogeneous compute architecture, the Adaptive Compute Acceleration Platform (ACAP), with significantly more flexibility and performance to address an evolving set of new applications such as machine learning. This advancement on the device side is accompanied by similar advances on higher-level programming approaches to make FPGAs and ACAPs significantly easy to use for a wide range of applications. Xilinx Vitis Unified Software Platform is a comprehensive development environment to build and seamlessly deploy accelerated applications on Xilinx platforms including Alveo cards, FPGA-instances in the cloud, and embedded platforms. It addresses the three major industry trends: the need for heterogenous computing, applications that span cloud to edge to end-point, and AI proliferation. Vitis supports application programming using C, C++ and OpenCL, and it enables the development of large-scale data processing and machine learning applications using familiar, higher-level frameworks such as TensorFlow and SPARK. To facilitate communication between the host application and accelerators, Xilinx Runtime library (XRT) provides APIs for accelerator life-cycle management, accelerator execution management, memory allocation, and data communication between the host application and accelerators. In addition, a rich set of performance-optimized, open-source libraries significantly ease the application development. Vitis AI, an integral part of Vitis, enables AI inference acceleration on Xilinx platforms. It supports industry's leading deep learning frameworks like Tensorflow and Caffe, and offers a comprehensive suite of tools and APIs to prune, quantize, optimize, and compile pre-trained models to achieve the highest AI inference performance on Xilinx platforms. This talk provides an overview of Vitis and Vitis AI development environments.",FPGA,2020.0,10.1145/3373087.3375887,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
dd3255f459fa849935b870e95e0fc5e436830c17,https://www.semanticscholar.org/paper/dd3255f459fa849935b870e95e0fc5e436830c17,Intrusion Detection Systems for Industrial Internet of Things: A Survey,"Industrial Internet of Things (IIoT) applies Internet of Things (IoT) technology in industrial systems, to optimize business processes efficiency, service quality, and reliability. However, with a large of isolated IoT networks deployed in various industries, many vulnerabilities have been exposed to security incidents and posed threats to IIoT security. An intrusion detection system (IDS) is a security monitoring mechanism that promotes cyber security solutions for information systems. The system’s role is to detect abnormal activities of intruders and enable preventive measures to avoid risks. However, applying a traditional IDS-based solution to IIoT is challenging due to its particular characteristics such as resource-constrained, data privacy, and heterogeneity. Researchers are using the new emerging technologies such as Fog/Edge computing, Machine Learning (ML), Deep Learning (DL) to deploy an effective and adaptive IDS for various IIoT operating environments. This study focus is on the development of IDS in particular industrial environments. To this end, we provide a systemic review that addresses IDS deployment strategies, detection approaches, and methodologies and data sources used for evaluation. We also present some suggestions and challenges to be considered when designing IDS-based security for Industrial IoT as future research.",2021 International Conference on Theoretical and Applicative Aspects of Computer Science (ICTAACS),2021.0,10.1109/ICTAACS53298.2021.9715177,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
3862698833e74d527b66a57901004092b26b11d1,https://www.semanticscholar.org/paper/3862698833e74d527b66a57901004092b26b11d1,"More Than ""If Time Allows"": The Role of Ethics in AI Education","Even as public pressure mounts for technology companies to consider societal impacts of products, industries and governments in the AI race are demanding technical talent. To meet this demand, universities clamor to add technical artificial intelligence (AI) and machine learning (ML) courses into computing curriculum-but how are societal and ethical considerations part of this landscape? We explore two pathways for ethics content in AI education: (1) standalone AI ethics courses, and (2) integrating ethics into technical AI courses. For both pathways, we ask: What is being taught? As we train computer scientists who will build and deploy AI tools, how are we training them to consider the consequences of their work? In this exploratory work, we qualitatively analyzed 31 standalone AI ethics classes from 22 U.S. universities and 20 AI/ML technical courses from 12 U.S. universities to understand which ethics-related topics instructors include in courses. We identify and categorize topics in AI ethics education, share notable practices, and note omissions. Our analysis will help AI educators identify what topics should be taught and create scaffolding for developing future AI ethics education.",AIES,2020.0,10.1145/3375627.3375868,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
2d97e166ad61497f461943c54ca02947398c025a,https://www.semanticscholar.org/paper/2d97e166ad61497f461943c54ca02947398c025a,AI Tax: The Hidden Cost of AI Data Center Applications,"Artificial intelligence and machine learning are experiencing widespread adoption in industry and academia. This has been driven by rapid advances in the applications and accuracy of AI through increasingly complex algorithms and models; this, in turn, has spurred research into specialized hardware AI accelerators. Given the rapid pace of advances, it is easy to forget that they are often developed and evaluated in a vacuum without considering the full application environment. This paper emphasizes the need for a holistic, end-to-end analysis of AI workloads and reveals the ""AI tax."" We deploy and characterize Face Recognition in an edge data center. The application is an AI-centric edge video analytics application built using popular open source infrastructure and ML tools. Despite using state-of-the-art AI and ML algorithms, the application relies heavily on pre-and post-processing code. As AI-centric applications benefit from the acceleration promised by accelerators, we find they impose stresses on the hardware and software infrastructure: storage and network bandwidth become major bottlenecks with increasing AI acceleration. By specializing for AI applications, we show that a purpose-built edge data center can be designed for the stresses of accelerated AI at 15% lower TCO than one derived from homogeneous servers and infrastructure.",,2020.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
d412e1c4e3f35f2d6006f1c661b24fc6370262bb,https://www.semanticscholar.org/paper/d412e1c4e3f35f2d6006f1c661b24fc6370262bb,Identification and Classification of Poultry Eggs: A Case Study Identification and Classification of Poultry Eggs: A Case Study Utilizing Computer Vision and Machine Learning Utilizing Computer Vision and Machine Learning,". We developed a method to identify, count, and classify chickens and eggs inside nesting boxes of a chicken coop. Utilizing an Internet of Things (IoT) AWS DeepLens Camera for data capture and inferences, we trained and deployed a custom Single Shot multibox Detector (SSD) model for object detection and classiﬁcation. This allows us to monitor a complex environment with multiple chickens and eggs moving and ap-pearing simultaneously within the video frames. The models can label video frames with classiﬁcations for eight breeds of chickens and/or four colors of eggs, with 98% accuracy on chickens or eggs alone and 82.5% accuracy while detecting both types of objects. With the ability to directly infer and store classiﬁcations on the camera, this setup can work in a low/no internet bandwidth setting in addition to an internet connected environment. Having these classiﬁcations beneﬁts farmers by providing the necessary base data required for accurately measuring the individual egg production of every chicken in the ﬂock. Additionally, this data supports comparative analysis between individual ﬂocks and industry benchmarks.",,,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
7d6e9e6e39c70b947296cb4c17aacf4452fc5b29,https://www.semanticscholar.org/paper/7d6e9e6e39c70b947296cb4c17aacf4452fc5b29,Exploring the Use of Synthetic Gradients for Distributed Deep Learning across Cloud and Edge Resources,"With the explosive growth of data, largely contributed by the rapidly and widely deployed smart devices on the edge, we need to rethink the training paradigm for learning on such realworld data. The conventional cloud-only approach can hardly keep up with the computational demand from these deep learning tasks; and the traditional back propagation based training method also makes it difficult to scale out the training. Fortunately, the continuous advancement in System on Chip (SoC) hardware is transforming edge devices into capable computing platforms, and can potentially be exploited to address these challenges. These observations have motivated this paper’s study on the use of synthetic gradients for distributed training cross cloud and edge devices. We employ synthetic gradients into various neural network models to comprehensively evaluate its feasibility in terms of accuracy and convergence speed. We distribute the training of the various layers of a model using synthetic gradients, and evaluate its effectiveness on the edge by using resource-limited containers to emulate edge devices. The evaluation result shows that the synthetic gradient approach can achieve comparable accuracy compared to the conventional back propagation, for an eight-layer model with both fully-connected and convolutional layers. For a more complex model (VGG16), the training suffers from some accuracy degradation (up to 15%). But it achieves 11% improvement in training speed when the layers of a model are decoupled and trained on separate resource-limited containers, compared to the training of the whole model using the conventional method on the physical machine.",HotEdge,2019.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
5319040656bc9671fc46012f6f7d1937a1e93e1a,https://www.semanticscholar.org/paper/5319040656bc9671fc46012f6f7d1937a1e93e1a,MLPerf Tiny Benchmark,"Advancements in ultra-low-power tiny machine learning (TinyML) systems promise to unlock an entirely new class of smart applications. However, continued progress is limited by the lack of a widely accepted and easily reproducible benchmark for these systems. To meet this need, we present MLPerf Tiny, the ﬁrst industry-standard benchmark suite for ultra-low-power tiny machine learning systems. The benchmark suite is the collaborative effort of more than 50 organizations from industry and academia and reﬂects the needs of the community. MLPerf Tiny measures the accuracy, latency, and energy of machine learning inference to properly evaluate the tradeoffs between systems. Additionally, MLPerf Tiny implements a modular design that enables benchmark submitters to show the beneﬁts of their product, regardless of where it falls on the ML deployment stack, in a fair and reproducible manner. The suite features four benchmarks: keyword spotting, visual wake words, image classiﬁcation, and anomaly detection.",NeurIPS Datasets and Benchmarks,2021.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
8b357b17f707935a706461c1db0d39e8b6801070,https://www.semanticscholar.org/paper/8b357b17f707935a706461c1db0d39e8b6801070,Learning Enhanced Diagnosis of Logic Circuit Failures,"As semiconductor manufacturing progresses to smaller process nodes, it is becoming increasingly difficult to climb the yield learning curve rapidly. The rate of yield learning dictates the growth and success of the semiconductor industry, and must be accelerated to fulfill competitive time-to-market, time-to-money and time-to-volume requirements. Software-based diagnosis plays a crucial role in yield learning. Diagnosis comprehends the test response of a failing circuit to determine the location, and sometimes, in addition, characterize the nature of a defect affecting the failing circuit. Besides identifying likely failure mechanisms and increasing the quality of chip testing, the feedback provided by diagnosis is used to select chips for physical failure analysis (PFA). PFA aims to visually examine a chip to characterize a defect, prevent similar defects in the future and, consequently, improve the design and manufacturing of a chip.However, PFA is often destructive, time- and cost-intensive, and not always successful. Diagnosis, on the other hand, is non-invasive and time- and cost-effective; moreover, it assists PFA and guides yield learning. The advantages of diagnosis, coupled with the diminishing performance of PFA with advancing technology, make it an encouraging facilitator for rapid yield learning.Therefore, the objective of diagnosis must be logic-level defect characterization to minimize (and ideally eliminate) the need for PFA, and accelerate yield learning. Logic characterization of a defect includes the derivation of its physical location and precise logic behavior. In this dissertation, a comprehensive diagnosis methodology is developed to actualize the aforementioned objective.The developed methodology comprises of three methods. LearnX/MD-LearnX is a physically-aware method that employs (a) the X-fault model to avert the elimination of a correct defect candidate and (b) machine learning to build a candidate-ranking model that learns the hidden correlations between the tester response and the defect candidates to pinpoint the correct candidate. PADLOC, which stands for Physically-Aware Defect LOcalization and Characterization, improves the physical location of a back-end defect (i.e., a defect that affects one or more interconnects and resides outside a standard cell) returned by LearnX/MD-LearnX by partitioning the defective net into physical subnets and identifying the subnets that influence defect excitation. In addition, PADLOC deduces the precise impact of a defect on the circuit functionality by examining its surrounding circuitry. NOIDA, which stands for NOise-resistant Intra-cell Diagnosis Approach, pinpoints the location of a defect within a failing standard cell implicated by LearnX/MD-LearnX. In contrast to prior work that typically constructs/employs a fault dictionary, NOIDA ascertains the location as well as the behavior of a front-end defect (i.e., a defect that resides inside a standard cell) by monitoring the logical activity of its intra-cell neighborhood. Additionally, NOIDA is resistant to circuit-level noise that may originate from potentially inaccurate transistor-level simulation. Results from numerous experiments reveal that our diagnosis methodology outperforms state-of-the-art commercial diagnosis. LearnX/MD-LearnX reports fewer defect candidates than commercial diagnosis for 69.4% silicon fail logs without losing accuracy. PADLOC implicates a smaller physical area for a defect for 47.2% silicon fail logs and attains at most 44X improvement. NOIDA reports an ideal diagnosis for 38.0% more front-end defects, when compared to leading-edge commercial diagnosis. In the presence of noise, NOIDA achieves an ideal diagnosis 7.6X more often.In summary, this dissertation endeavors to characterize a defect residing in a logic chip in terms of its precise physical location and logic behavior, which, consequently, most likely, enables rapid yield learning. The deployment of machine learning to pinpoint the correct candidate in LearnX/MD-LearnX, and the investigation of the neighborhood of a defect to determine its exact physical location and logic behavior in PADLOC and NOIDA are the novel components of this dissertation, and the reasons for its superiority over the state-of-the-art.",,2020.0,10.1184/R1/11962164.V1,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
b0982a1895f2a792c8b0c2dd691533012ef3f1b5,https://www.semanticscholar.org/paper/b0982a1895f2a792c8b0c2dd691533012ef3f1b5,Interactive Learning of Temporal Features for Control,"The ongoing industry revolution is demanding more flexible products, including robots in household environments or medium scale factories. Such robots should be able to adapt to new conditions and environments, and to be programmed with ease. As an example, let us suppose that there are robot manipulators working in an industrial production line that need to perform a new task. If these robots were hard coded, it could take days to adapt them to the new settings, which would stop the production of the factory. Easily programmable robots by non-expert humans would speed up this process considerably. In this regard, we present a framework in which robots are capable to quickly learn new control policies and state representations, by using occasional corrective human feedback. To achieve this, we focus on interactively learning these policies from non-expert humans that act as teachers. We present a Neural Network (NN) architecture, along with an Interactive Imitation Learning (IIL) method, which efficiently learns spatiotemporal features and policies from raw high dimensional observations (raw pixels from an image), for tasks in environments not fully temporally observable. We denominate IIL as a branch of Imitation Learning (IL) where human teachers provide different kinds of feedback to the robots, like new demonstrations triggered by robot queries [1], corrections [2], preferences [3], reinforcements [4], etc. Most IL methods work under the assumption of learning from perfect demonstrations; therefore, they fail when teachers only have partial insights in the task execution. Non-expert teachers could be considered all the users who are neither Machine Learning (ML)/control experts, nor skilled to fully show the desired behavior of the policy. Interactive approaches like COACH [5], and some Interactive Reinforcement Learning (IRL) approaches [4], [6], are intended for non-expert teachers, but are not completely deployable for end-users. Sequential decision-making learning methods (IL, IIL, IRL, etc.) rely on good state representations, which make the shaping of the policy landscape simple, and provide good generalization properties. However, this requirement brings the need of experts on feature engineering to pre-process the states properly, before running the learning algorithms.",,2020.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
e43cb4a1eff92b74306235256e3bd26f96c9b9c1,https://www.semanticscholar.org/paper/e43cb4a1eff92b74306235256e3bd26f96c9b9c1,Towards an AEC-AI Industry Optimization Algorithmic Knowledge Mapping: An Adaptive Methodology for Macroscopic Conceptual Analysis,"The Architecture, Engineering, and Construction (AEC) Industry is one of the most important productive sectors, hence also produce a high impact on the economic balances, societal stability, and global challenges in climate change. Regarding its adoption of technologies, applications and processes is also recognized by its status-quo, its slow innovation pace, and the conservative approaches. However, a new technological era - Industry 4.0 fueled by AI- is driving productive sectors in a highly pressurized global technological competition and sociopolitical landscape. In this paper, we develop an adaptive approach to mining text content in the literature research corpus related to the AEC and AI (AEC-AI) industries, in particular on its relation to technological processes and applications. We present a first stage approach to an adaptive assessment of AI algorithms, to form an integrative AI platform in the AEC industry, the AEC-AI industry 4.0. At this stage, a macroscopic adaptive method is deployed to characterize “Optimization,” a key term in AEC-AI industry, using a mixed methodology incorporating machine learning and classical evaluation process. Our results show that effective use of metadata, constrained search queries, and domain knowledge allows getting a macroscopic assessment of the target concept. This allows the extraction of a high-level mapping and conceptual structure characterization of the literature corpus. The results are comparable, at this level, to classical methodologies for the literature review. In addition, our method is designed for an adaptive assessment to incorporate further stages.",IEEE Access,2021.0,10.1109/ACCESS.2021.3102215,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
61efff04bf68fce3384b7daa0379338297552310,https://www.semanticscholar.org/paper/61efff04bf68fce3384b7daa0379338297552310,Closed Loop Paging Optimization for Efficient Mobility Management,"The 4G/5G networks deploy conventional Tracking Area Update and multi-step paging procedures for mobility management. The paging procedure consumes significant amount of licensed spectrum resources. The signaling overhead is going to worsen further with the increasing use of small cells and higher user mobility speed. To address this challenge, the telecommunication industry is embracing closed loop approaches to predict user mobility patterns. Though the mobility pattern prediction is a known problem, most of the existing solutions apply it for enhancing the handover management and use academic dataset. Furthermore, limited research has been done on idle-state users. In this paper, we propose a Closed Loop Paging (CLOP) optimization solution using semi-supervised learning model to reduce paging overhead. We harness the real network dataset to analyze the location trail of users in the network to predict a subset of Base Stations for paging to locate idle-state users. Our empirical results demonstrate that Linear Support Vector Machine (L-SVM) classification method excels when compared to other supervised learning models. The L-SVM Classifier saves nearly 43% of the paging overhead with a marginal increase in connection establishment delay by around 7.3%.",2021 IEEE 18th Annual Consumer Communications & Networking Conference (CCNC),2021.0,10.1109/CCNC49032.2021.9369589,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
ed95e2f54f8738c9d80cd8eac62ad5479bf43553,https://www.semanticscholar.org/paper/ed95e2f54f8738c9d80cd8eac62ad5479bf43553,"Ph.D. in Information Technology: Thesis Defenses March 11th, 2021 online by Teams – at 14.00 Alberto Maria METELLI – XXXIII Cycle Exploiting Environment Configurability in Reinforcement Learning","In the last decades, Reinforcement Learning (RL) has emerged as an effective approach to address complex control tasks. The formalism typically employed to model the sequential interaction between the artificial agent and the environment is the Markov Decision Process (MDP). In an MDP, the agent perceives the state of the environment and performs actions. As a consequence, the environment transitions to a new state and generates a reward signal. The goal of the agent consists of learning a policy, i.e., a prescription of actions, that maximizes the long-term reward. In the traditional setting, the environment is assumed to be a fixed entity that cannot be altered externally. However, there exist several real-world scenarios in which the environment can be modified to a limited extent and, therefore, it might be beneficial to act on some of its features. We call this activity environment configuration, that can be carried out by the agent itself or by an external entity, such as a configurator. Although environment configuration arises quite often in real applications, this topic is very little explored in the literature. In this dissertation, we aim at formalizing and studying the diverse aspects of environment configuration. The contributions are theoretical, algorithmic, and experimental and can be broadly subdivided into three parts. The first part of the dissertation introduces the novel formalism of Configurable Markov Decision Processes (Conf-MDPs) to model the configuration opportunities offered by the environment. At an intuitive level, there exists a tight connection between environment, policy, and learning process. We explore the different nuances of environment configuration, based on whether the configuration is fully auxiliary to the agent’s learning process (cooperative setting) or guided by a configurator having an objective that possibly conflicts with the agent’s one (non-cooperative setting). In the second part, we focus on the cooperative Conf-MDP setting and we investigate the learning problem consisting of finding an agent policy and an environment configuration that jointly optimize the long-term reward. We provide algorithms for solving finite and continuous Conf-MDPs and experimental evaluations are conducted on both synthetic and realistic domains. The third part addresses two specific applications of the Conf-MDP framework: policy space identification and control frequency adaptation. In the former, we employ environment configurability to improve the identification of the agent’s perception and actuation capabilities. In the latter, instead, we analyze how a specific configurable environmental parameter, the control frequency, can affect the performance of the batch RL algorithms Alessandro NUARA – XXXIII Cycle Machine Learning Algorithms for the Optimization of Internet Advertising Campaigns Supervisor: Prof. Nicola Gatti Abstract: Online advertising revenue grew to 124:6 billion dollars in 2019, showing a 15% increase over the previous year. The opportunities provided by this market attracted wide attention of the scientific community as well as the industry that requires automatic tools to manage the main processes involved in this market. For this purpose, Artificial Intelligence can play a crucial role in providing techniques to support both publishers and advertisers in their tasks. In this thesis, we design and analyze Machine Learning algorithms for the optimization of Internet advertising campaigns from the advertiser’s point-of-view. An advertising campaign is composed of a set of sub-campaigns that Online advertising revenue grew to 124:6 billion dollars in 2019, showing a 15% increase over the previous year. The opportunities provided by this market attracted wide attention of the scientific community as well as the industry that requires automatic tools to manage the main processes involved in this market. For this purpose, Artificial Intelligence can play a crucial role in providing techniques to support both publishers and advertisers in their tasks. In this thesis, we design and analyze Machine Learning algorithms for the optimization of Internet advertising campaigns from the advertiser’s point-of-view. An advertising campaign is composed of a set of sub-campaigns that differ for the ad (e.g., including text or images), target (e.g., keywords, age, interests), or channel (e.g., search, social, display). In pay-per-click advertising, to get an ad impressed, the advertisers take part in an auction carried out by the publisher, in which they set a bid and a daily budget for each sub-campaign. The bid represents the maximum amount of money the advertisers are willing to pay for a click, whereas the daily budget is the maximum spend in a day for a sub-campaign. The advertisers’ goal is to create a set of sub-campaigns and, for each of them, set the bid/daily budget values that maximize the revenue under budget or return-on-investment constraints. This optimization problem is particularly challenging, as it includes many intricate subproblems. In this work, we study four different settings, and we propose algorithms specifically crafted for each of them. For all the problems, we provide a theoretical analysis and an empirical evaluation of our approaches in both synthetic and real-world scenarios showing their superiority if compared with baselines and with the human campaign management. Matteo PAPINI – XXXIII Cycle Safe Policy Optimization Supervisor: Prof. Marcello Restelli Abstract: Policy Optimization is a family of reinforcement learning algorithms that is particularly suited to realworld control tasks due to its ability of managing high-dimensional decision variables and noisy signals. This also makes Policy Optimization one of the most pressing targets of safety concerns. Outside of simulation, the trial-and-error behavior typical of learning agents can have concrete, potentially catastrophic consequences. The design of reliable adaptive agents for real-world settings requires, first of all, a better theoretical understanding of the learning algorithms used to train them. In this dissertation, we highlight the potential and limitations of existing policy optimization techniques, with a special focus on policy gradient algorithms. We study theoretical properties of policy gradients that are relevant to safety. We establish novel guarantees of monotonic performance improvement and convergence. We also study the trade-offs that safety requirements inevitably engage with sample complexity and exploration. Besides improving the theoretical Policy Optimization is a family of reinforcement learning algorithms that is particularly suited to realworld control tasks due to its ability of managing high-dimensional decision variables and noisy signals. This also makes Policy Optimization one of the most pressing targets of safety concerns. Outside of simulation, the trial-and-error behavior typical of learning agents can have concrete, potentially catastrophic consequences. The design of reliable adaptive agents for real-world settings requires, first of all, a better theoretical understanding of the learning algorithms used to train them. In this dissertation, we highlight the potential and limitations of existing policy optimization techniques, with a special focus on policy gradient algorithms. We study theoretical properties of policy gradients that are relevant to safety. We establish novel guarantees of monotonic performance improvement and convergence. We also study the trade-offs that safety requirements inevitably engage with sample complexity and exploration. Besides improving the theoretical understanding of policy gradient methods, we design new algorithms with more desirable properties, and evaluate them on simulated continuous control tasks. Andrea TIRINZONI – XXXIII Cycle Exploiting Structure for Transfer in Reinforcement Learning Supervisor: Prof. Marcello RESTELLI",,2021.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
ab6f7512737b24d9f5981ecf017aadd7b5b8558a,https://www.semanticscholar.org/paper/ab6f7512737b24d9f5981ecf017aadd7b5b8558a,When Homomorphic Encryption Marries Secret Sharing: Secure Large-Scale Sparse Logistic Regression and Applications in Risk Control,"Logistic Regression (LR) is the most widely used machine learning model in industry for its efficiency, robustness, and interpretability. Due to the problem of data isolation and the requirement of high model performance, many applications in industry call for building a secure and efficient LR model for multiple parties. Most existing work uses either Homomorphic Encryption (HE) or Secret Sharing (SS) to build secure LR. HE based methods can deal with high-dimensional sparse features, but they incur potential security risks. SS based methods have provable security, but they have efficiency issue under high-dimensional sparse features. In this paper, we first present CAESAR, which combines HE and SS to build secure large-scale sparse logistic regression model and achieves both efficiency and security. We then present the distributed implementation of CAESAR for scalability requirement. We have deployed CAESAR in a risk control task and conducted comprehensive experiments. Our experimental results show that CAESAR improves the state-of-the-art model by around 130 times.",KDD,2020.0,10.1145/3447548.3467210,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
7058de5bf7126b310509b2f147c5c2940fcb689b,https://www.semanticscholar.org/paper/7058de5bf7126b310509b2f147c5c2940fcb689b,Challenges for the Application of Machine Learning,"By most accounts, the applied branch of machine learning has been a clear success. Induction techniques have aided the development of elded systems in science and industry, on a range of tasks, including me-searchers in the area can feel genuinely proud that their algorithms have proven so robust and developers deserve major credit for identifying promising applications and seeing them through to completion. The basic development story should by now be quite The developer works with a domain expert or user to understand some problem, and then reformulates the problem into one that can be addressed by well-established methods for supervised learning. He then determines a set of likely features to describe the training cases, and devises an approach to collecting and preparing those data. Once these are available, he runs some induction method over the data. The developer (and possibly the expert) then evaluate the resulting knowledge base along dimensions of interest, such as accuracy, understandability, and consistency with existing domain knowledge. If the result seems acceptable, they attempt to deploy the learned knowledge in the eld. 1 Applied work in machine learning diiers from academic learning research in its acknowledgement of this development process. Most research papers on learning continue to emphasize reenements in the induction technique and ignore the steps that must occur 1 Of course, this process is not linear but iterative. Problems at any step can lead the developer to backtrack to an earlier stage and revisit decisions made there. before and after its invocation. In contrast, applied efforts recognize the importance of problem formulation, representation engineering, data collection and preparation , inspection of the learned knowledge, and the elding process itself. Within the applications community , there is an emerging consensus that these steps play a role at least as important as the induction stage itself. Indeed, there is even a common belief that, once they are handled, the particular induction method one uses has little eeect on the outcome. 2 Automating the Overall Process Clearly, the applied induction community could continue along these lines and be quite successful. The discipline could use its established approach to develop more elded applications and train a cadre of problem and representation engineers to become expert at the overall process. Over time, this new generation of developers would come to replace the knowledge engineers currently charged with creating knowledge-based systems. But this is a limited vision, and …",,1997.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
d9e5bc45202111ebf83501465aeb77cfee2cb690,https://www.semanticscholar.org/paper/d9e5bc45202111ebf83501465aeb77cfee2cb690,IoT for the Power Industry: Recent Advances and Future Directions with Pavatar,"The development of Internet-of-Things (IoT) technologies in recent years brings us unprecedented opportunities for innovations in the power industry. This demo abstract introduces our research and practice with Pavatar - IoT for the power industry. Pavatar includes a series of system deployments in the core sections of Global Energy Internet (GEI), for the purposes of automatic surveillance and remote diagnosis of ultra-high-voltage converter stations (UHVCSs). Pavatar incorporates technologies like lower-power or battery-free sensing, cross-technology communication, edge computing, machine learning, and enhances the user experience with 3D virtual reality. The deployed system significantly reduces the manpower cost and enhances the operational efficiency of the UHVCS.",SenSys,2018.0,10.1145/3274783.3275179,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
9a3fa28855b6b3df9652938ccb13bfaacc346192,https://www.semanticscholar.org/paper/9a3fa28855b6b3df9652938ccb13bfaacc346192,Auto-Split: A General Framework of Collaborative Edge-Cloud AI,"In many industry scale applications, large and resource consuming machine learning models reside in powerful cloud servers. At the same time, large amounts of input data are collected at the edge of cloud. The inference results are also communicated to users or passed to downstream tasks at the edge. The edge often consists of a large number of low-power devices. It is a big challenge to design industry products to support sophisticated deep model deployment and conduct model inference in an efficient manner so that the model accuracy remains high and the end-to-end latency is kept low. This paper describes the techniques and engineering practice behind Auto-Split, an edge-cloud collaborative prototype of Huawei Cloud. This patented technology is already validated on selected applications, is on its way for broader systematic edge-cloud application integration, and is being made available for public use as an automated pipeline service for end-to-end cloud-edge collaborative intelligence deployment. To the best of our knowledge, there is no existing industry product that provides the capability of Deep Neural Network (DNN) splitting.",KDD,2021.0,10.1145/3447548.3467078,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
4c7ac647f06a8128d478809ec1d06c8189b6d05c,https://www.semanticscholar.org/paper/4c7ac647f06a8128d478809ec1d06c8189b6d05c,Digital Transformation In Indian Insurance Industry,"This paper titled “Digital Transformation in Indian Insurance Industry” is an attempt to give deep insights to all the readers on digital transformation in insurance space. Technological innovations are extensive and all encompassing. Disruptions are not industry specific and insurance industry is no exception to this. Recently regulator published a draft regulation on sandbox concept, which permits carriers to innovate their offering to end user. This is led by fintech and insure tech companies and carriers have structured digital boards to take this revolution forward.Major findings of this paper are usage of block chain technology and data security in insurance industry. With companies constituting digital boards, pandemic has only acted like a tailwind for the digital push wherein entire sales process is migrated to digital way of selling. This move has a multiplier effect on customer reach, cost efficiency and service precision. Customers who are keen to have the best in terms of technological innovation will be delighted with the advancement in digital transformation.Also with big data and analytics, we are coming back to risk based pricing, which is proportionate to the risk borne by the customer. This is still evolving in life insurance as the deployment of wearables is at a nascent stage.Newer technologies like AI and machine learning are facilitating companies register higher growth both on cross and upsell opportunities. This will indisputably have an immense and long term positive impact on the bottom line of most insurance companies thus enhancing profitability.Researcher concludes that digital innovation will surely have a great and positive impact on profitability of insurance companies.",,2021.0,10.17762/TURCOMAT.V12I4.509,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
a9faaf32c0e4743ea97744a54322bc3620239ac6,https://www.semanticscholar.org/paper/a9faaf32c0e4743ea97744a54322bc3620239ac6,Internet of Things in Space: A Review of Opportunities and Challenges from Satellite-Aided Computing to Digitally-Enhanced Space Living,"Recent scientific and technological advancements driven by the Internet of Things (IoT), Machine Learning (ML) and Artificial Intelligence (AI), distributed computing and data communication technologies have opened up a vast range of opportunities in many scientific fields—spanning from fast, reliable and efficient data communication to large-scale cloud/edge computing and intelligent big data analytics. Technological innovations and developments in these areas have also enabled many opportunities in the space industry. The successful Mars landing of NASA’s Perseverance rover on 18 February 2021 represents another giant leap for humankind in space exploration. Emerging research and developments of connectivity and computing technologies in IoT for space/non-terrestrial environments is expected to yield significant benefits in the near future. This survey paper presents a broad overview of the area and provides a look-ahead of the opportunities made possible by IoT and space-based technologies. We first survey the current developments of IoT and space industry, and identify key challenges and opportunities in these areas. We then review the state-of-the-art and discuss future opportunities for IoT developments, deployment and integration to support future endeavors in space exploration.",Sensors,2021.0,10.3390/s21238117,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
8994fd8dc3a4e18e93013b8aa8a79c732bf81572,https://www.semanticscholar.org/paper/8994fd8dc3a4e18e93013b8aa8a79c732bf81572,Graph-less Neural Networks: Teaching Old MLPs New Tricks via Distillation,"Graph Neural Networks (GNNs) are popular for graph machine learning and have shown great results on wide node classification tasks. Yet, they are less popular for practical deployments in the industry owing to their scalability challenges incurred by data dependency. Namely, GNN inference depends on neighbor nodes multiple hops away from the target, and fetching them burdens latency-constrained applications. Existing inference acceleration methods like pruning and quantization can speed up GNNs by reducing Multiplication-and-ACcumulation (MAC) operations, but the improvements are limited given the data dependency is not resolved. Conversely, multi-layer perceptrons (MLPs) have no graph dependency and infer much faster than GNNs, even though they are less accurate than GNNs for node classification in general. Motivated by these complementary strengths and weaknesses, we bring GNNs and MLPs together via knowledge distillation (KD). Our work shows that the performance of MLPs can be improved by large margins with GNN KD. We call the distilled MLPs Graph-less Neural Networks (GLNNs) as they have no inference graph dependency. We show that GLNNs with competitive accuracy infer faster than GNNs by 146×-273× and faster than other acceleration methods by 14×-27×. Under a production setting involving both transductive and inductive predictions across 7 datasets, GLNN accuracies improve over stand-alone MLPs by 12.36% on average and match GNNs on 6/7 datasets. Comprehensive analysis shows when and why GLNNs can achieve competitive accuracies to GNNs and suggests GLNN as a handy choice for latency-constrained applications.",ArXiv,2021.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
a9618def7d29a4caa561c3e6ade372dd37cfcc3e,https://www.semanticscholar.org/paper/a9618def7d29a4caa561c3e6ade372dd37cfcc3e,TIP4.0: Industrial Internet of Things Platform for Predictive Maintenance,"Industry 4.0, allied with the growth and democratization of Artificial Intelligence (AI) and the advent of IoT, is paving the way for the complete digitization and automation of industrial processes. Maintenance is one of these processes, where the introduction of a predictive approach, as opposed to the traditional techniques, is expected to considerably improve the industry maintenance strategies with gains such as reduced downtime, improved equipment effectiveness, lower maintenance costs, increased return on assets, risk mitigation, and, ultimately, profitable growth. With predictive maintenance, dedicated sensors monitor the critical points of assets. The sensor data then feed into machine learning algorithms that can infer the asset health status and inform operators and decision-makers. With this in mind, in this paper, we present TIP4.0, a platform for predictive maintenance based on a modular software solution for edge computing gateways. TIP4.0 is built around Yocto, which makes it readily available and compliant with Commercial Off-the-Shelf (COTS) or proprietary hardware. TIP4.0 was conceived with an industry mindset with communication interfaces that allow it to serve sensor networks in the shop floor and modular software architecture that allows it to be easily adjusted to new deployment scenarios. To showcase its potential, the TIP4.0 platform was validated over COTS hardware, and we considered a public data-set for the simulation of predictive maintenance scenarios. We used a Convolution Neural Network (CNN) architecture, which provided competitive performance over the state-of-the-art approaches, while being approximately four-times and two-times faster than the uncompressed model inference on the Central Processing Unit (CPU) and Graphical Processing Unit, respectively. These results highlight the capabilities of distributed large-scale edge computing over industrial scenarios.",Sensors,2021.0,10.3390/s21144676,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
f41a32130b4d9db7e57fbca691184b894ec36ccb,https://www.semanticscholar.org/paper/f41a32130b4d9db7e57fbca691184b894ec36ccb,TensorDiffEq: Scalable Multi-GPU Forward and Inverse Solvers for Physics Informed Neural Networks,"Physics-Informed Neural Networks promise to revolutionize science and engineering practice, by introducing domain-aware deep machine learning models into scientific computation. Several software suites have emerged to make the implementation and usage of these architectures available to the research and industry communities. Here we introduce TensorDiffEq, built on Tensorflow 2.x, which presents an intuitive Keras-like interface for problem domain definition, model definition, and solution of forward and inverse problems using physics-aware deep learning methods. TensorDiffEq takes full advantage of Tensorflow 2.x infrastructure for deployment on multiple GPUs, allowing the implementation of large high-dimensional and complex models. Simultaneously, TensorDiffEq supports the Keras API for custom neural network architecture definitions. In the case of smaller or simpler models, the package allows for rapid deployment on smaller-scale CPU platforms with negligible changes to the implementation scripts. We demonstrate the basic usage and capabilities of TensorDiffEq in solving forward, inverse, and data assimilation problems of varying sizes and levels of complexity. The source code is available at https://github.com/tensordiffeq.",ArXiv,2021.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
b9ececa2ac247156290e1668c1c87ac11f76deaf,https://www.semanticscholar.org/paper/b9ececa2ac247156290e1668c1c87ac11f76deaf,The case for digital twins in metal additive manufacturing,"The digital twin (DT) is a relatively new concept that is finding increased acceptance in industry. A DT is generally considered as comprising a physical entity, its virtual replica, and two-way digital data communications in-between. Its primary purpose is to leverage the process intelligence captured within digital models—or usually their faster-solving surrogates—towards generating increased value from the physical entities. The surrogate models are created using machine learning based on data obtained from the field, experiments and digital models, which may be physics-based or statistics-based. Anomaly detection and correction, and diagnostic closed-loop process control are examples of how a process DT can be deployed. In the manufacturing industry, its use can achieve improvements in product quality and process productivity. Metal additive manufacturing (AM) stands to gain tremendously from the use of DTs. This is because the AM process is inherently chaotic, resulting in poor repeatability. However, a DT acting in a supervisory role can inject certainty into the process by actively keeping it within bounds through real-time control commands. Closed-loop feedforward control is achieved by observing the process through sensors that monitor critical parameters and, if there are any deviations from their respective optimal ranges, suitable corrective actions are triggered. The type of corrective action (e.g. a change in laser power or a modification to the scanning speed) and its magnitude are determined by interrogating the surrogate models. Because of their artificial intelligence (AI)-endowed predictive capabilities, which allow them to foresee a future state of the physical twin (e.g. the AM process), DTs proactively take context-sensitive preventative steps, whereas traditional closed-loop feedback control is usually reactive. Apart from assisting a build process in real-time, a DT can help with planning the build of a part by pinpointing the optimum processing window relevant to the desired outcome. Again, the surrogate models are consulted to obtain the required information. In this article, we explain how the application of DTs to the metal AM process can significantly widen its application space by making the process more repeatable (through quality assurance) and cheaper (by getting builds right the first time).",Journal of Physics: Materials,2021.0,10.1088/2515-7639/ac09fb,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
4b3aaaaa92f1bfb23e690b4132e2a36a80f82905,https://www.semanticscholar.org/paper/4b3aaaaa92f1bfb23e690b4132e2a36a80f82905,Cloud computing for big data analytics in the Process Control Industry,"The aim of this article is to present an example of a novel cloud computing infrastructure for big data analytics in the Process Control Industry. Latest innovations in the field of Process Analyzer Techniques (PAT), big data and wireless technologies have created a new environment in which almost all stages of the industrial process can be recorded and utilized, not only for safety, but also for real time optimization. Based on analysis of historical sensor data, machine learning based optimization models can be developed and deployed in real time closed control loops. However, still the local implementation of those systems requires a huge investment in hardware and software, as a direct result of the big data nature of sensors data being recorded continuously. The current technological advancements in cloud computing for big data processing, open new opportunities for the industry, while acting as an enabler for a significant reduction in costs, making the technology available to plants of all sizes. The main contribution of this article stems from the presentation for a fist time ever of a pilot cloud based architecture for the application of a data driven modeling and optimal control configuration for the field of Process Control. As it will be presented, these developments have been carried in close relationship with the process industry and pave a way for a generalized application of the cloud based approaches, towards the future of Industry 4.0.",2017 25th Mediterranean Conference on Control and Automation (MED),2017.0,10.1109/MED.2017.7984310,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
be88daa1e8e95877797468246f8ac16c07670a6d,https://www.semanticscholar.org/paper/be88daa1e8e95877797468246f8ac16c07670a6d,A Novel Radar Signal Recognition Method Based On Deep Learning Doc Download,"Micro-Doppler Characteristics of Radar Targets is a monograph on radar target’s micro-Doppler effect theory and micro-Doppler feature extraction techniques. The micro-Doppler effect is presented from two aspects, including micro-Doppler effect analysis and micro-Doppler feature extraction, with micro-Doppler effects induced by different micro-motional targets in different radar systems analyzed and several methods of micro-Doppler feature extraction and three-dimensional micro-motion feature reconstruction presented. The main contents of this book include micro-Doppler effect in narrowband radar, micro-Doppler effect in wideband radar, micro-Doppler effect in bistatic radar, microDoppler feature analysis and extraction, and three-dimensional micro-motion feature reconstruction, etc. This book can be used as a reference for scientific and technical personnel engaged in radar signal processing and automatic target recognition, etc. It is especially suitable for beginners who are interested in research on micro-Doppler effect in radar. Presents new views on micro-Doppler effects, analyzing and discussing micro-Doppler effect in wideband radar rather than focusing on narrowband Provides several new methods for micro-Doppler feature extraction which are very helpful and practical for readers Includes practical cases that align with main MATLAB codes in each chapter, with detailed program annotations Wavelet analysis and its applications have been one of the fastest-growing research areas in the past several years. Wavelet theory has been employed in numerous fields and applications, such as signal and image processing, communication systems, biomedical imaging, radar, and air acoustics. Active media technology is concerned with the development of autonomous computational or physical entities capable of perceiving, reasoning, adapting, learning, cooperating, and delegating in a dynamic environment.This book captures the essence of the state of the art in wavelet analysis and its applications and active media technology. At the Congress, invited talks were delivered by distinguished researchers, namely Prof John Daugman of Cambridge University, UK; Prof Bruno Torresani of INRIA, France; Prof Victor Wickerhauser of Washington University, USA, Prof Ning Zhong of the Maebashi Institute of Technology, Japan; Prof John Yen of Pennsylvania State University, USA; and Prof Sankar K Pal of the Indian Statistical Institute, India. This volume is an initiative undertaken by the IEEE Computational Intelligence Society’s Task Force on Security, Surveillance and Defense to consolidate and disseminate the role of CI techniques in the design, development and deployment of security and defense solutions. Applications range from the detection of buried explosive hazards in a battlefield to the control of unmanned underwater vehicles, the delivery of superior video analytics for protecting critical infrastructures or the development of stronger intrusion detection systems and the design of military surveillance networks. Defense scientists, industry experts, academicians and practitioners alike will all benefit from the wide spectrum of successful applications compiled in this volume. Senior undergraduate or graduate students may also discover uncharted territory for their own research endeavors. By studying applications in radar, telecommunications and digital image restoration, this monograph discusses signal processing techniques based on bispectral methods. Improved robustness against different forms of noise as well as preservation of phase information render this method a valuable alternative to common power-spectrum analysis used in radar object recognition, digital wireless communications, and jitter removal in images. The first book to present a systematic and coherent picture of MIMO radars Due to its potential to improve target detection and discrimination capability, Multiple-Input and Multiple-Output (MIMO) radar has generated significant attention and widespread interest in academia, industry, government labs, and funding agencies. This important new work fills the need for a comprehensive treatment of this emerging field. Edited and authored by leading researchers in the field of MIMO radar research, this book introduces recent developments in the area of MIMO radar to stimulate new concepts, theories, and applications of the topic, and to foster further cross-fertilization of ideas with MIMO communications. Topical coverage includes: Adaptive MIMO radar Beampattern analysis and optimization for MIMO radar MIMO radar for target detection, parameter estimation, tracking,association, and recognition MIMO radar prototypes and measurements Space-time codes for MIMO radar Statistical MIMO radar Waveform design for MIMO radar Written in an easy-to-follow tutorial style, MIMO Radar Signal Processing serves as an excellent course book for graduate students and a valuable reference for researchers in academia and industry. This 1179-page book assembles the complete contributions to the International Conference on Intelligent Computing, ICIC 2006: one volume of Lecture Notes in Computer Science (LNCS); one of Lecture Notes in Artificial Intelligence (LNAI); one of Lecture Notes in Bioinformatics (LNBI); and two volumes of Lecture Notes in Control and Information Sciences (LNCIS). Include are 149 revised full papers, and a Special Session on Computing for Searching Strategies to Control Dynamic Processes. Novel deep learning approaches are achieving state-of-the-art accuracy in the area of radar target recognition, enabling applications beyond the scope of human-level performance. This book provides an introduction to the unique aspects of machine learning for radar signal processing that any scientist or engineer seeking to apply these technologies ought to be aware of. Collects the revised and updated versions of lectures presented at an advanced course on [title] held at the Accademia dei Lincei, Rome, 1988, as well as some additional chapters. The 13 chapters address basic concepts on detection, estimation, and optimum filtering; models of clutter; CFAR techniques in clutter; pulse compression and equivalent technologies; pulse doppler radar; MTI, MTD, and adaptive clutter cancellation; rejection of active interference; architecture and implementation of radar signal processors; identification of radar targets; phased arrays; bistatic radars; space-based radar; and evolution and future trends of radar. Primarily for radar engineers and researchers, as well as advanced students. Distributed by INSPEC. Annotation copyright by Book News, Inc., Portland, OR This definitive volume covers state-of-the-art over-the-horizon radar systems, with emphasis on the practical application of advanced signal processing techniques. In recent years rough set theory has attracted the attention of many researchers and practitioners all over the world, who have contributed essentially to its development and applications. Weareobservingagrowingresearchinterestinthefoundationsofroughsets, including the various logical, mathematical and philosophical aspects of rough sets. Some relationships have already been established between rough sets and other approaches, and also with a wide range of hybrid systems. As a result, rough sets are linked with decision system modeling and analysis of complex systems, fuzzy sets, neural networks, evolutionary computing, data mining and knowledge discovery, pattern recognition, machine learning, and approximate reasoning. In particular, rough sets are used in probabilistic reasoning, granular computing (including information granule calculi based on rough mereology), intelligent control, intelligent agent modeling, identi?cation of autonomous stems, and process speci?cation. Methods based on rough set theory alone or in combination with other proacheshavebeendiscoveredwith awide rangeofapplicationsinsuchareasas: acoustics, bioinformatics, business and ?nance, chemistry, computer engineering (e.g., data compression, digital image processing, digital signal processing, pallel and distributed computer systems, sensor fusion, fractal engineering), desion analysis and systems, economics, electrical engineering (e.g., control, signal analysis, power systems), environmental studies, informatics, medicine, molelar biology, musicology, neurology, robotics, social science, software engineering, spatial visualization, Web engineering, and Web mining. These proceedings present technical papers selected from the 2012 International Conference on Intelligent Systems and Knowledge Engineering (ISKE 2012), held on December 15-17 in Beijing. The aim of this conference is to bring together experts from different fields of expertise to discuss the state-of-the-art in Intelligent Systems and Knowledge Engineering, and to present new findings and perspectives on future developments. The proceedings introduce current scientific and technical advances in the fields of artificial intelligence, machine",,2021.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
b02ecc7f4ed9874069a978a511c73c69efebc160,https://www.semanticscholar.org/paper/b02ecc7f4ed9874069a978a511c73c69efebc160,On the Importance of Text Classification Pipeline Components for Practical Applications: A Case Study,"The worlds of academia and industry have different priorities for machine learning models. In the academic world, the model’s performance is often the main focus, whereas finding the balance between the model’s performance, resource requirements, and the ease of its deployment is often deemed more important in the production environment of the industry. In this paper we consider a real world text classification problem, compare the specifics of different parts of natural language processing pipelines and investigate their contribution to the final model’s performance. We also take into consideration the practical aspects of the model’s use and deployment, such as the size of the model and preprocessing time. Our case-study shows that in this particular scenario the performance of simpler models can be on par with the more complex ones. We find this result valuable, as simpler and smaller models are normally also easier to deploy in practice, e.g. in a serverless environment. To showcase the practical usefulness of our final model, we deploy it to AWS Lambda and show that its execution time in this environment scales linearly with the input text’s length.",,2020.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
14c11fbd97fe44780f2ea6c6def082109d74a1e8,https://www.semanticscholar.org/paper/14c11fbd97fe44780f2ea6c6def082109d74a1e8,A Systemic Method of Nesting Multiple Classifiers Using Ensemble Techniques for Telecom Churn Prediction,,,2020.0,10.1007/978-981-15-5258-8_2,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
bf7c1613adea1d2575b36cdf145a8a06206afd17,https://www.semanticscholar.org/paper/bf7c1613adea1d2575b36cdf145a8a06206afd17,Artificial Intelligence and Technology in Teaching Negotiation,"Artificial intelligence (AI), machine learning (ML), affective computing, and big-data techniques are improving the ways that humans negotiate and learn to negotiate. These technologies, long deployed in industry and academic research, are now being adopted for educational use. We describe several systems that help human negotiators evaluate and learn from role-play simulations as well as applications that help human instructors teach negotiators at the individual, team, and organizational levels. AI can enable the personalization of negotiation instruction, taking into consideration factors such as culture and bias. These tools will enable improvements not only in the teaching of negotiation, but also in teaching humans how to program and collaborate with technology-based negotiation systems, including avatars and computer-controlled negotiation agents. These advances will provide theoretical and practical insights, require serious consideration of ethical issues, and revolutionize the way we practice and teach negotiation.",Negotiation Journal,2021.0,10.1111/NEJO.12351,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
23838783bfb72a364f672084f2ccd178d13f630a,https://www.semanticscholar.org/paper/23838783bfb72a364f672084f2ccd178d13f630a,Assessing Wireless Sensing Potential With Large Intelligent Surfaces,"Sensing capability is one of the most highlighted new feature of future 6G wireless networks. This paper addresses the sensing potential of Large Intelligent Surfaces (LIS) in an exemplary Industry 4.0 scenario. Besides the attention received by LIS in terms of communication aspects, it can offer a high-resolution rendering of the propagation environment. This is because, in an indoor setting, it can be placed in proximity to the sensed phenomena, while the high resolution is offered by densely spaced tiny antennas deployed over a large area. By treating an LIS as a radio image of the environment relying on the received signal power, we develop techniques to sense the environment, by leveraging the tools of image processing and machine learning. Once a radio image is obtained, a Denoising Autoencoder (DAE) network can be used for constructing a super-resolution image leading to sensing advantages not available in traditional sensing systems. Also, we derive a statistical test based on the Generalized Likelihood Ratio (GLRT) as a benchmark for the machine learning solution. We test these methods for a scenario where we need to detect whether an industrial robot deviates from a predefined route. The results show that the LIS-based sensing offers high precision and has a high application potential in indoor industrial environments.",IEEE Open Journal of the Communications Society,2020.0,10.1109/OJCOMS.2021.3073467,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
2ed326c13a9b0140ddc202bd3c3b809c4731a744,https://www.semanticscholar.org/paper/2ed326c13a9b0140ddc202bd3c3b809c4731a744,Machine Learning Toolbox and PCA Visualization for Data-Driven PHM,"Increased awareness of big data has led to active development of machine learning algorithms for big data analytics. The advent of rapidly emerging data analytics technologies has also brought about considerable changes to the diagnostics and prognostics for smart manufacturing industries. As the importance of managing massive factory data also has grown, many engineers are putting in efforts to implement machine learning algorithms for a PHM (Prognostics and Health Management) purpose in accordance with the type of machinery of interest. In this thesis, research on assisting the quick deployment of supervised and unsupervised learning classification algorithms with data visualization is conducted by building up the GUI software with an emphasis on PHM. It can plot raw data, select hand-crafted features based on an expert knowledge, followed by a dimension reduction step if necessary. The various machine learning algorithms will provide the classification decision boundaries to enable us to diagnose current machine health conditions. Therefore, it can suggest to engineers a guideline to determine suitable features and date-driven PHM algorithms. Principal Component Analysis (PCA) is a widely used dimension reduction algorithm without losing too much information for high-dimensional data analysis. It transforms the high-dimensional data into a meaningful representation of reduced dimensional data. In a machine health monitoring system, a result of dimension reduction via PCA is often utilized. Although eigenvectors and eigenvalues of PCA are important information, it is too difficult for users to interpret where the principal components are coming from. In order to assist the user in better understanding and interpreting PCA, data visualization can be used. We have developed a system that visualizes the eigenvectors and eigenvalues of PCA using JavaScript library, D3 and demonstrated that how the key information and insights of PCA results can be intuitively visualized.",,2008.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
987d822694ecbe3f6bd461cb8d2721cb00dc326b,https://www.semanticscholar.org/paper/987d822694ecbe3f6bd461cb8d2721cb00dc326b,SIGIR 2021 E-Commerce Workshop Data Challenge,"The massive growth of e-commerce which is estimated to turn into a 4.5 trillion industry in 2021 [37] has fueled significant investments in areas such as Information Retrieval, NLP and recommendation systems. One of the principal goals of such investments has been to improve personalization in the customer journey. Optimal customer journeys in e-commerce are ideally supported by systems that transform the experience of each customer based on customer behavior and (a minimal set of) business rules [43]. Although compelling progress has been made, interesting challenges remain in personalization. In particular, customer intentions can be different depending on the occasion and even change within the same session, depending on where users are in their customer journey. In this context, the feedback loop determined by behavioural signals such as searches, clicks, add-to-cart, cart-abandonment spans from hours to a few seconds [38] and machine learning models need to adapt as fast as possible to the continuously changing nature of the customer journey. The need for efficient procedures for personalization is even clearer if we consider the e-commerce landscapemore broadly. Outside of giant digital retailers, the constraints of the problem are stricter, due to smaller user bases and the realization that most users are not frequently returning customers: in dozens of deployments across several verticals, we found that it is not uncommon",ArXiv,2021.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
445cf9e3b58333c56e3dfd1339cea79fc2cf04a7,https://www.semanticscholar.org/paper/445cf9e3b58333c56e3dfd1339cea79fc2cf04a7,Pilot-Edge: Distributed Resource Management Along the Edge-to-Cloud Continuum,"Many science and industry IoT applications necessitate data processing across the edge-to-cloud continuum to meet performance, security, cost, and privacy requirements. However, diverse abstractions and infrastructures for managing resources and tasks across the edge-to-cloud scenario are required. We propose Pilot-Edge as a common abstraction for resource management across the edge-to-cloud continuum. Pilot-Edge is based on the pilot abstraction, which decouples resource and workload management, and provides a Function-as-a-Service (FaaS) interface for application-level tasks. The abstraction allows applications to encapsulate common functions in high-level tasks that can then be configured and deployed across the continuum. We characterize Pilot-Edge on geographically distributed infrastructures using machine learning workloads (e. g., k-means and auto-encoders). Our experiments demonstrate how Pilot-Edge manages distributed resources and allows applications to evaluate task placement based on multiple factors (e. g., model complexities, throughput, and latency).",2021 IEEE International Parallel and Distributed Processing Symposium Workshops (IPDPSW),2021.0,10.1109/IPDPSW52791.2021.00130,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
fb6bed51490c87c6aa7a60000b6031aac570ee70,https://www.semanticscholar.org/paper/fb6bed51490c87c6aa7a60000b6031aac570ee70,Recurrence-Aware Long-Term Cognitive Network for Explainable Pattern Classification,"Machine-learning solutions for pattern classification problems are nowadays widely deployed in society and industry. However, the lack of transparency and accountability of most accurate models often hinders their safe use. Thus, there is a clear need for developing explainable artificial intelligence mechanisms. There exist model-agnostic methods that summarize feature contributions, but their interpretability is limited to predictions made by black-box models. An open challenge is to develop models that have intrinsic interpretability and produce their own explanations, even for classes of models that are traditionally considered black boxes like (recurrent) neural networks. In this article, we propose a long-term cognitive network (LTCN) for interpretable pattern classification of structured data. Our method brings its own mechanism for providing explanations by quantifying the relevance of each feature in the decision process. For supporting the interpretability without affecting the performance, the model incorporates more flexibility through a quasi-nonlinear reasoning rule that allows controlling nonlinearity. Besides, we propose a recurrence-aware decision model that evades the issues posed by the unique fixed point while introducing a deterministic learning algorithm to compute the tunable parameters. The simulations show that our interpretable model obtains competitive results when compared to state-of-the-art white and black-box models.",IEEE transactions on cybernetics,2021.0,10.1109/TCYB.2022.3165104,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
d3eb93449328ee7b9913f0f9a54bc46f12c312a7,https://www.semanticscholar.org/paper/d3eb93449328ee7b9913f0f9a54bc46f12c312a7,Data Twin-Driven Cyber-Physical Factory for Smart Manufacturing,"Because of the complex production processes and technology-intensive operations that take place in the aerospace and defense industry, introducing Industry 4.0 into the manufacturing processes of aircraft composite materials is inevitable. Digital Twin and Cyber-Physical Systems in Industry 4.0 are key techniques to develop digital manufacturing. Since it is very difficult to create high-fidelity virtual models, the development of digital manufacturing for aircraft manufacturers is challenging. In this study, we provide a view from a data simulation perspective and adopt machine learning approaches to simplify the high-fidelity virtual models in Digital Twin. The novel concept is called Data Twin, and the deployable service to support the simulation is known as the Data Twin Service (DTS). Relying on the DTS, we also propose a microservice software architecture, Cyber-Physical Factory (CPF), to simulate the shop floor environment. Additionally, there are two war rooms in the CPF that can be used to establish a collaborative platform: one is the Physical War Room, used to integrate real data, and the other is the Cyber War Room for handling simulation data and the results of the CPF.",Sensors,2022.0,10.3390/s22082821,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
98b90daeb4dbbb8c581c734b11297e902a096b06,https://www.semanticscholar.org/paper/98b90daeb4dbbb8c581c734b11297e902a096b06,Switching Scheme: A Novel Approach for Handling Incremental Concept Drift in Real-World Data Sets,"Machine learning models nowadays play a crucial role for many applications in business and industry. However, models only start adding value as soon as they are deployed into production. One challenge of deployed models is the effect of changing data over time, which is often described with the term concept drift. Due to their nature, concept drifts can severely affect the prediction performance of a machine learning system. In this work, we analyze the effects of concept drift in the context of a real-world data set. For efficient concept drift handling, we introduce the switching scheme which combines the two principles of retraining and updating of a machine learning model. Furthermore, we systematically analyze existing regular adaptation as well as triggered adaptation strategies. The switching scheme is instantiated on New York City taxi data, which is heavily influenced by changing demand patterns over time. We can show that the switching scheme outperforms all other baselines and delivers promising prediction results.",HICSS,2020.0,10.24251/HICSS.2021.120,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
5cc6155331c67de0238a633341fe66b6de8ae793,https://www.semanticscholar.org/paper/5cc6155331c67de0238a633341fe66b6de8ae793,Functional Anomaly Detection: a Benchmark Study,"The increasing automation in many areas of the Industry expressly demands to design efficient machine-learning solutions for the detection of abnormal events. With the ubiquitous deployment of sensors monitoring nearly continuously the health of complex infrastructures, anomaly detection can now rely on measurements sampled at a very high frequency, providing a very rich representation of the phenomenon under surveillance. In order to exploit fully the information thus collected, the observations cannot be treated as multivariate data anymore and a functional analysis approach is required. It is the purpose of this paper to investigate the performance of recent techniques for anomaly detection in the functional setup on real datasets. After an overview of the state-of-theart and a visual-descriptive study, a variety of anomaly detection methods are compared. While taxonomies of abnormalities (e.g. shape, location) in the functional setup are documented in the literature, assigning a specific type to the identified anomalies appears to be a challenging task. Thus, strengths and weaknesses of the existing approaches are benchmarked in view of these highlighted types in a simulation study. Anomaly detection methods are next evaluated on two datasets, related to the monitoring of helicopters in flight and to the spectrometry of construction materials namely. The benchmark analysis is concluded by a recommendation guidance for practitioners.",ArXiv,2022.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
bffaeca2b5f380dde2c294d1c993549f5ac1e70e,https://www.semanticscholar.org/paper/bffaeca2b5f380dde2c294d1c993549f5ac1e70e,Towards Semantic Management of On-Device Applications in Industrial IoT,"The Internet of Things (IoT) is revolutionizing the industry. Powered by pervasive embedded devices, the Industrial IoT (IIoT) provides a unique solution for retrieving and analyzing data near the source in real-time. Many emerging techniques, such as Tiny Machine Learning (TinyML) and Complex Event Processing (CEP), are actively being developed to support decision making at the edge, shifting the paradigm from centralized processing to distributed computing. However, distributed computing presents management challenges, as IoT devices are diverse and constrained, and their number is growing exponentially. The situation is even more challenging when various on-device applications (so-called artifacts) are deployed across decentralized IoT networks. Questions to be addressed include how to discover an appropriate function, whether that function can be executed on a certain device, and how to orchestrate a cross-platform service. To tackle these challenges, we propose an approach for the scalable management of on-device applications among distributed IoT devices. By leveraging the W3C Web of Things (WoT), the capabilities of each IoT device, or more precisely, its interaction patterns, can be semantically expressed in a Thing Description (TD). In addition, we introduce semantic modeling of on-device applications to supplement an TD with additional information regarding applications on the device. Specifically, we demonstrate two examples of semantic modeling: neural networks (NN) and CEP rules. The ontologies are evaluated by answering a set of competency questions. By hosting the enriched semantic knowledge of the entire IoT system in a Knowledge Graph (KG), we can discover and interoperate edge devices and artifacts across the decentralized network. This can reduce fragmentation and increase the reusability of IoT components. We demonstrate the feasibility of our concept on an industrial workstation consisting of a conveyor belt and several IoT devices. Finally, the requirements for constructing an IoT semantic management system are discussed.",ACM Transactions on Internet Technology,2022.0,10.1145/3510820,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
d5a6f1d51703432d39fa089d89c25bda3611f56c,https://www.semanticscholar.org/paper/d5a6f1d51703432d39fa089d89c25bda3611f56c,E-Pilots: A System to Predict Hard Landing During the Approach Phase of Commercial Flights,"More than half of all commercial aircraft operation accidents could have been prevented by executing a go-around. Making timely decision to execute a go-around manoeuvre can potentially reduce overall aviation industry accident rate. In this paper, we describe a cockpit-deployable machine learning system to support flight crew go-around decision-making based on the prediction of a hard landing event. This work presents a hybrid approach for hard landing prediction that uses features modelling temporal dependencies of aircraft variables as inputs to a neural network. Based on a large dataset of 58177 commercial flights, the results show that our approach has 85% of average sensitivity with 74% of average specificity at the go-around point. It follows that our approach is a cockpit-deployable recommendation system that outperforms existing approaches.",IEEE Access,2022.0,10.1109/access.2021.3138167,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
8b102784f9b0eb62a713c077bedd44d97e48c1a2,https://www.semanticscholar.org/paper/8b102784f9b0eb62a713c077bedd44d97e48c1a2,TinyIREE: An ML Execution Environment for Embedded Systems from Compilation to Deployment,"—Machine learning model deployment for training and execution has been an important topic for industry and academic research in the last decade. Much of the attention has been focused on developing speciﬁc toolchains to support acceleration hardware. In this paper, we present IREE, a uniﬁed compiler and runtime stack with the explicit goal to scale down machine learning programs to the smallest footprints for mobile and edge devices, while maintaining the ability to scale up to larger deployment targets. IREE adopts a compiler-based approach and optimizes for heterogeneous hardware accelerators through the use of the MLIR compiler infrastructure which provides the means to quickly design and implement multi-level compiler intermediate representations (IR). More speciﬁcally, this paper is focused on TinyIREE, which is a set of deployment options in IREE that accommodate the limited memory and computation resources in embedded systems and bare-metal platforms, while also demonstrating IREE’s intuitive workﬂow that generates workloads for different ISA extensions and ABIs through LLVM. M ACHINE LEARNING has attracted a lot of attention in the last decade, both in industry and academic research. As new, deeper machine learning (ML) model architectures emerge and bigger datasets become available, the demands on the hardware are also increasing. This has led to the development of new processor architectures that accelerate ML model training and execution (inference). However, to fully take advantage of such acceleration hardware, the development",IEEE Micro,2022.0,10.1109/mm.2022.3178068,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
e91b348b0262a743e0b243d68c54abe886ceae56,https://www.semanticscholar.org/paper/e91b348b0262a743e0b243d68c54abe886ceae56,Artificial Intelligence to Manage Food Waste,"Changes in consumer demands have led to a transformation in the food and beverage industry, with demand for fast, affordable, and readily available food options. Currently, the industry is witnessing a surge in innovative startups and tech company collaborations. Deploying Artificial Intelligence (AI) and Machine Learning (ML) solutions aid in managing wastes, scale-up operations, and stay relevant in a dynamic market environment of the food industry.",,2022.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
16c5e8a02a1a2cb50748ecdad0ccaebe57e4d4ea,https://www.semanticscholar.org/paper/16c5e8a02a1a2cb50748ecdad0ccaebe57e4d4ea,Balancing Robustness and Responsiveness in a Real-time Indoor Location System using Bluetooth Low Energy Technology and Deep Learning to Facilitate Clinical Applications,"An indoor, real-time location system (RTLS) can benefit both hospitals and patients by improving clinical efficiency through data-driven optimization of procedures. Bluetooth-based RTLS systems are cost-effective but lack accuracy and robustness because Bluetooth signal strength is subject to fluctuation. We developed a machine learning-based solution using a Long Short-Term Memory (LSTM) network followed by a Multilayer Perceptron classifier and a posterior constraint algorithm to improve RTLS performance. Training and validation datasets showed that most machine learning models perform well in classifying individual location zones, although LSTM was most reliable. However, when faced with data indicating cross-zone trajectories, all models showed erratic zone switching. Thus, we implemented a history-based posterior constraint algorithm to reduce the variability in exchange for a slight decrease in responsiveness. This network increases robustness at the expense of latency. When latency is less of a concern, we computed the latency-corrected accuracy which is 100% for our testing data, significantly improved from LSTM without constraint which is 96.2%. The balance between robustness and responsiveness can be considered and adjusted on a case-by-case basis, according to the specific needs of downstream clinical applications. This system was deployed and validated in an academic medical center. Industry best practices enabled system scaling without substantial compromises to performance or cost.",ArXiv,2019.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
c0ae3786f6fe26cefef392639108a8bbe898b5a1,https://www.semanticscholar.org/paper/c0ae3786f6fe26cefef392639108a8bbe898b5a1,PyTorch 1.x Reinforcement Learning Cookbook,"Implement reinforcement learning techniques and algorithms with the help of real-world examples and recipes
Key Features
Use PyTorch 1.x to design and build self-learning artificial intelligence (AI) models

Implement RL algorithms to solve control and optimization challenges faced by data scientists today

Apply modern RL libraries to simulate a controlled environment for your projects
Book Description
Reinforcement learning (RL) is a branch of machine learning that has gained popularity in recent times. It allows you to train AI models that learn from their own actions and optimize their behavior. PyTorch has also emerged as the preferred tool for training RL models because of its efficiency and ease of use.

With this book, you'll explore the important RL concepts and the implementation of algorithms in PyTorch 1.x. The recipes in the book, along with real-world examples, will help you master various RL techniques, such as dynamic programming, Monte Carlo simulations, temporal difference, and Q-learning. You'll also gain insights into industry-specific applications of these techniques. Later chapters will guide you through solving problems such as the multi-armed bandit problem and the cartpole problem using the multi-armed bandit algorithm and function approximation. You'll also learn how to use Deep Q-Networks to complete Atari games, along with how to effectively implement policy gradients. Finally, you'll discover how RL techniques are applied to Blackjack, Gridworld environments, internet advertising, and the Flappy Bird game.

By the end of this book, you'll have developed the skills you need to implement popular RL algorithms and use RL techniques to solve real-world problems.
What you will learn
Use Q-learning and the state–action–reward–state–action (SARSA) algorithm to solve various Gridworld problems

Develop a multi-armed bandit algorithm to optimize display advertising

Scale up learning and control processes using Deep Q-Networks

Simulate Markov Decision Processes, OpenAI Gym environments, and other common control problems

Select and build RL models, evaluate their performance, and optimize and deploy them

Use policy gradient methods to solve continuous RL problems
Who this book is for
Machine learning engineers, data scientists and AI researchers looking for quick solutions to different reinforcement learning problems will find this book useful. Although prior knowledge of machine learning concepts is required, experience with PyTorch will be useful but not necessary.",,2019.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
a242bfc8cddc52784dc68ed46a14adbbe9e3abd0,https://www.semanticscholar.org/paper/a242bfc8cddc52784dc68ed46a14adbbe9e3abd0,Operationalizing Heterogeneous Data-Driven Process Models for Various Industrial Sectors through Microservice-Oriented Cloud-Based Architecture,"Industrial performance optimization increasingly makes the use of various analytical data-driven models. In this context, modern machine learning capabilities to predict future production quality outcomes, model predictive control to better account for complex multivariable environments of process industry, Bayesian Networks enabling improved decision support systems for diagnostics and fault detection are some of the main examples to be named. The key challenge is to integrate these highly heterogeneous models in a holistic system, which would also be suitable for applications from the most different industries. Core elements of the underlying solution architecture constitute highly decoupled model microservices, ensuring the creation of largely customizable model runtime environments. Deployment of isolated user-space instances, called containers, further extends the overall possibilities to integrate heterogeneous models. Strong requirements on high availability, scalability, and security are satisfied through the application of cloud-based services. Tieto successfully applied the outlined approach during the participation in FUture DIrections for Process industry Optimization (FUDIPO), a project funded by the European Commission under the H2020 program, SPIRE-02-2016.",,2021.0,10.5772/INTECHOPEN.92896,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
cef345d252503559472ba128aaf95ba5e23476ec,https://www.semanticscholar.org/paper/cef345d252503559472ba128aaf95ba5e23476ec,Artificial Intelligence in the Hospitality Sector,"Hospitality is one of the most important sectors of the economy and offers employment to thousands of people. The recent advances in technology has seen that quite a few of the players in this industry have successfully deployed artificial intelligence, machine learning, and robotics. This chapter delves into the details of such deployment in the various processes in this sector and discusses the short-term, medium-term, and long-term impact of these technologies on all the major stakeholders of this industry. The author also looks at the cost benefit analysis of this technologies and concludes that most players sooner, rather than later would be forced by competition to strongly adopt them. The chapter also briefly discusses the changing roles of human employees in this scenario.",,2021.0,10.4018/978-1-7998-3919-4.CH013,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
932ab11e3c564f73a85a668be4c0367823108e66,https://www.semanticscholar.org/paper/932ab11e3c564f73a85a668be4c0367823108e66,End-to-End Management System Framework for Smart Public Buildings,"This paper presents a project aiming to design a complete framework to measure energy (electricity and gas) and water consumptions in a local Parish Council building and an adjacent Sports Hall located in the central part of Portugal. The goal is an end-to-end solution, from data acquisition to data analysis. Besides acquiring and storing the data, the aim is to make this information available and valuable to enhance better decisions in building management actions, to enable detection of situations of anomalous consumption and also to promote building users’ awareness. To pursue this goal, PLCnext technology solutions from Phoenix Contact are adopted. The system is based on a new generation industrial controller that communicates with energy and water meters distributed throughout the building using a standard Information Technology (IT) network. The solution explores Industry 4.0 concept, such as Cloud Data Management, Cybersecurity, and Machine Learning. With historic consumption records available, Machine Learning strategies are being used to predict load profiles in a short-term horizon and also planned to classify untypical consumption behaviors (for monitor and alarm purposes). This project is being deployed in partnership between Polytechnic of Leiria, EduNet International Education Network and involving the local Parish Council, owner of the monitored buildings.",2021 IEEE Green Energy and Smart Systems Conference (IGESSC),2021.0,10.1109/IGESSC53124.2021.9618689,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
273ac46d05ceb24d2bb3ff8ca42aec9c115463e9,https://www.semanticscholar.org/paper/273ac46d05ceb24d2bb3ff8ca42aec9c115463e9,The Devil is in the GAN: Defending Deep Generative Models Against Backdoor Attacks,"Deep Generative Models (DGMs) allow users to synthesize data from complex, high-dimensional manifolds. Industry applications of DGMs include data augmentation to boost performance of (semi-)supervised machine learning, or to mitigate fairness or privacy concerns. Large-scale DGMs are notoriously hard to train, requiring expert skills, large amounts of data and extensive computational resources. Thus, it can be expected that many enterprises will resort to sourcing pre-trained DGMs from potentially unverified third parties, e.g. open source model repositories. As we show in this paper, such a deployment scenario poses a new attack surface, which allows adversaries to potentially undermine the integrity of entire machine learning development pipelines in a victim organization. Specifically, we describe novel training-time attacks resulting in corrupted DGMs that synthesize regular data under normal operations and designated target outputs for inputs sampled from a trigger distribution. Depending on the control that the adversary has over the random number generation, this imposes various degrees of risk that harmful data may enter the machine learning development pipelines, potentially causing material or reputational damage to the victim organization. Our attacks are based on adversarial loss functions that combine the dual objectives of attack stealth and fidelity. We show its effectiveness for a variety of DGM architectures (Generative Adversarial Networks (GANs), Variational Autoencoders (VAEs)) and data domains (images, audio). Our experiments show that even for large-scale industry-grade DGMs our attack can be mounted with only modest computational efforts. We also investigate the effectiveness of different defensive approaches (based on static/dynamic model and output inspections) and prescribe a practical defense strategy that paves the way for safe usage of",ArXiv,2021.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
3e13cd262aeaefe9b538b6453cc5eafa8a616c1f,https://www.semanticscholar.org/paper/3e13cd262aeaefe9b538b6453cc5eafa8a616c1f,Intelligent Traffic Monitoring Systems for Vehicle Classification: A Survey,"A traffic monitoring system is an integral part of Intelligent Transportation Systems (ITS). It is one of the critical transportation infrastructures that transportation agencies invest a huge amount of money to collect and analyze the traffic data to better utilize the roadway systems, improve the safety of transportation, and establish future transportation plans. With recent advances in MEMS, machine learning, and wireless communication technologies, numerous innovative traffic monitoring systems have been developed. In this article, we present a review of state-of-the-art traffic monitoring systems focusing on the major functionality–vehicle classification. We organize various vehicle classification systems, examine research issues and technical challenges, and discuss hardware/software design, deployment experience, and system performance of vehicle classification systems. Finally, we discuss a number of critical open problems and future research directions in an aim to provide valuable resources to academia, industry, and government agencies for selecting appropriate technologies for their traffic monitoring applications.",IEEE Access,2019.0,10.1109/ACCESS.2020.2987634,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
21fabe038a529358a4d0628c6d94d213f01bcd80,https://www.semanticscholar.org/paper/21fabe038a529358a4d0628c6d94d213f01bcd80,FlowGuard: An Intelligent Edge Defense Mechanism Against IoT DDoS Attacks,"Internet-of-Things (IoT) devices are getting more and more popular in recent years and IoT networks play an important role in the industry as well as people’s activities. On the one hand, they bring convenience to every aspect of our daily life; on the other hand, they are vulnerable to various attacks that in turn cancels out their benefits to a certain degree. In this article, we target the defense techniques against IoT Distributed Denial-of-Service (DDoS) attacks and propose an edge-centric IoT defense scheme termed FlowGuard for the detection, identification, classification, and mitigation of IoT DDoS attacks. We present a new DDoS attack detection algorithm based on traffic variations and design two machine learning models for DDoS identification and classification. To demonstrate the effectiveness of the two machine learning models, we generate a large data set by DDoS simulators BoNeSi and SlowHTTPTest, and combine it with the CICDDoS2019 data set, to test the identification and classification accuracy as well as the model efficiency. Our results indicate that the identification accuracy of the proposed long short-term memory is as high as 98.9%, which significantly outperforms the other four well-known learning models mentioned in the most related work. The classification accuracy of the proposed convolutional neural network is up to 99.9%. Besides, our models satisfactorily meet the delay requirements of IoT when deployed in edge servers with computational powers higher than a personal computer.",IEEE Internet of Things Journal,2020.0,10.1109/JIOT.2020.2993782,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
1efe9c3efd044e0d3d5ee7e7cab970273d2b6cfa,https://www.semanticscholar.org/paper/1efe9c3efd044e0d3d5ee7e7cab970273d2b6cfa,One Explanation Does Not Fit All,,KI - Künstliche Intelligenz,2020.0,10.1007/s13218-020-00637-y,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
fc2c062436fdabe85b7eb2d59b282f3561b5ebce,https://www.semanticscholar.org/paper/fc2c062436fdabe85b7eb2d59b282f3561b5ebce,GenSyth: a new way to understand deep learning,"Deep learning is a branch in machine learning which focuses on learning hierarchical feature representations of data using neural networks, with each successive layer representing information at a progressively more abstract level than the previous one. The progress of research in deep learning has greatly accelerated in the past decade, with tremendous advances leading to state-of-the-art performance in a wide range of tasks well beyond conventional machine learning methods. For example, deep neural networks have excelled at visual perception tasks, leading to an increase in top-5 accuracy from 74.2% in 2011 (before deep learning) to 97.6% in 2019, which is beyond human performance. Similar demonstrations of state-of-the-art performance beyond past machine learning methods have also been achieved for audio perception [1], natural language processing [2], game playing [3], scientific discovery [4], and content generation [5]. Given all the recent success, there has now been tremendous interest and focus on not only the academic research community, but also significant investment by industry in the wide-spread adoption of deep learning for solving complex real-world problems such as autonomous driving, smart cities, manufacturing, and finance. Despite the advances in accuracy and performance gained via deep learning, one of the biggest challenges with widespread ‘operational’ adoption is the sheer complexity of these high-performant deep neural networks created by the research community. As much of the focus had been on modelling accuracy and performance, many of the created deep neural networks have highly complex architectures that are intractable from both computational and memory perspectives in real-world operational scenarios. This complexity issue is particularly challenging for edge and mobile scenarios, where on-device processing is highly desirable (and often necessary) for privacy and latency/bandwidth reasons, and the embedded chips have greatly restricted computational, memory, and energy resources. As such, the ability to design and create deep neural networks that are not only high-performing but also operate well on low-power embedded devices has become a crucial path in breaking the barrier towards real-world, operational use and deployment of deep learning within industrial scenarios.",Electronics Letters,2019.0,10.1049/el.2019.2376,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
0caa61c0ffa59fd74e11368b77e267bc27bf5564,https://www.semanticscholar.org/paper/0caa61c0ffa59fd74e11368b77e267bc27bf5564,Engineering AI Systems: A Research Agenda,"Artificial intelligence (AI) and machine learning (ML) are increasingly broadly adopted in industry, However, based on well over a dozen case studies, we have learned that deploying industry-strength, production quality ML models in systems proves to be challenging. Companies experience challenges related to data quality, design methods and processes, performance of models as well as deployment and compliance. We learned that a new, structured engineering approach is required to construct and evolve systems that contain ML/DL components. In this paper, we provide a conceptualization of the typical evolution patterns that companies experience when employing ML as well as an overview of the key problems experienced by the companies that we have studied. The main contribution of the paper is a research agenda for AI engineering that provides an overview of the key engineering challenges surrounding ML solutions and an overview of open items that need to be addressed by the research community at large.",ArXiv,2020.0,10.4018/978-1-7998-5101-1.CH001,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
d4fb626ed46ce33e727ab402c7bff165700e9b4a,https://www.semanticscholar.org/paper/d4fb626ed46ce33e727ab402c7bff165700e9b4a,Visible light positioning for location-based services in Industry 4.0,"Industry 4.0 refers to the evolution in manufacturing from computerization to fully cyberphysical systems that exploit rich sensor data, adaptive real-time safety-critical control, and machine learning. An important aspect of this vision is the sensing and subsequent association of objects in the physical world with their cyber and virtual counterparts. In this paper we propose Visible Light Positioning (VLP) as an enabler for these Industry 4.0 applications. We also explore sensing techniques, including cameras (and depth sensors), and other light-based solutions for object positioning and detection along with their respective limitations. We then demonstrate an application of positioning for real time robot control in an interactive multiparty cyber-physical-virtual deployment. Lastly, based on our experience with this cyber-physical-virtual application, we propose Ray-Surface Positioning (RSP), a novel VLP technique, as a low cost positioning system for Industry 4.0.",,2018.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
4ab0f4aae3420c48886b36f926c357c161a199e2,https://www.semanticscholar.org/paper/4ab0f4aae3420c48886b36f926c357c161a199e2,Chapter 3 Graph-Based Decision Making in Industry,"Decision-making in industry can be focused on different types of problems. Classification and prediction of decision problems can be solved with the use of a decision tree, which is a graph-based method of machine learning. In the presented approach, attribute-value system and quality function deployment (QFD) were used for decision problem analysis and training dataset preparation. A decision tree was applied for generating decision rules.",,2018.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
42a7c10ae3e385c69fbb242987eb28ff329b6a20,https://www.semanticscholar.org/paper/42a7c10ae3e385c69fbb242987eb28ff329b6a20,CARL : Cloud Assisted Reinforcement Learning,"Reinforcement Learning (RL) is becoming increasingly important for next generation applications (e.g., self-driving cars [5]). While its benefits in dealing with problems requiring dynamic control under stochastic settings has been shown, there exists a lot of ground to cover before RL can become the defacto tool for the industry. It has been a center of criticism from leading machine learning experts due to its hunger for data while computational requirements of RL algorithms stifle their chances of deployment. With the cloud now offering scalable and inexpensive compute and storage power ([1]), there is hope for applications dependent on RL to see deployment success. Additionally, there has been a wave of new computer architecture, custom tailored to the requirements of machine learning applications (e.e., TPUs [2]). Other efforts include creating cloud based systems that can provide useful abstractions to programmers interested in using machine learning for their applications (e.g., Tensorflow [4], Keras [3]). However, offloading computation and storage to the cloud does not come for free: the fedility of the network in between the RL agent and the backend system running at the cloud becomes the key. If the network is poor, then the performance of an RL agent may degrade. This is especially true for thin RL agents designed to rely on the cloud and expend most of their efforts in collecting and reporting sensory data. In this project, we aim to look at the performance of specific RL algorithms with computationally intensive code-paths offloaded to the cloud. In particular, our focus will be on learning and planning methods, with the planning phase offloaded to the cloud. By simulating varying level of network fidelity, we aim to understand:",,2018.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
87bfa66d04837ed144ff024ba4dba2d8ca3eeede,https://www.semanticscholar.org/paper/87bfa66d04837ed144ff024ba4dba2d8ca3eeede,iUmami-SCM: A Novel Sequence-Based Predictor for Prediction and Analysis of Umami Peptides Using a Scoring Card Method with Propensity Scores of Dipeptides,"Umami or the taste of monosodium glutamate represents one of the major attractive taste modalities in humans. Therefore, knowledge about biophysical and biochemical properties of the umami taste is important for both scientific research and the food industry. Experimental approaches for predicting umami peptides are labor intensive, time consuming, and expensive. To date, computational models for the prediction and analysis of umami peptides as a function of sequence information have not been developed yet. In this study, we have proposed the first sequence-based predictor named iUmami-SCM using primary sequence information for the identification and characterization of umami peptides. iUmami-SCM utilized a newly developed scoring card method (SCM) in conjunction with the propensity scores of amino acids and dipeptide. Our predictor demonstrated excellent prediction performance ability for predicting umami peptides as well as outperforming other commonly used machine learning classifiers. Particularly, iUmami-SCM afforded the highest accuracy and Matthews correlation coefficient of 0.865 and 0.679, respectively, on an independent data set. Furthermore, the analysis of SCM-derived propensity scores was performed so as to provide a more in-depth understanding and knowledge of biophysical and biochemical properties of umami intensities of peptides. To develop a convenient bioinformatics tool, the best model is deployed as a web server that is made publicly available at http://camt.pythonanywhere.com/iUmami-SCM. The iUmami-SCM, as presented herein, serves as a powerful computational technique for large-scale umami peptide identification as well as facilitating the interpretation of umami peptides.",J. Chem. Inf. Model.,2020.0,10.1021/acs.jcim.0c00707,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
f4ba147e57beee5e57cc4548feb130cbe16c9192,https://www.semanticscholar.org/paper/f4ba147e57beee5e57cc4548feb130cbe16c9192,Network Intrusion Detection for IoT Security Based on Learning Techniques,"Pervasive growth of Internet of Things (IoT) is visible across the globe. The 2016 Dyn cyberattack exposed the critical fault-lines among smart networks. Security of IoT has become a critical concern. The danger exposed by infested Internet-connected Things not only affects the security of IoT but also threatens the complete Internet eco-system which can possibly exploit the vulnerable Things (smart devices) deployed as botnets. Mirai malware compromised the video surveillance devices and paralyzed Internet via distributed denial of service attacks. In the recent past, security attack vectors have evolved bothways, in terms of complexity and diversity. Hence, to identify and prevent or detect novel attacks, it is important to analyze techniques in IoT context. This survey classifies the IoT security threats and challenges for IoT networks by evaluating existing defense techniques. Our main focus is on network intrusion detection systems (NIDSs); hence, this paper reviews existing NIDS implementation tools and datasets as well as free and open-source network sniffing software. Then, it surveys, analyzes, and compares state-of-the-art NIDS proposals in the IoT context in terms of architecture, detection methodologies, validation strategies, treated threats, and algorithm deployments. The review deals with both traditional and machine learning (ML) NIDS techniques and discusses future directions. In this survey, our focus is on IoT NIDS deployed via ML since learning algorithms have a good success rate in security and privacy. The survey provides a comprehensive review of NIDSs deploying different aspects of learning techniques for IoT, unlike other top surveys targeting the traditional systems. We believe that, this paper will be useful for academia and industry research, first, to identify IoT threats and challenges, second, to implement their own NIDS and finally to propose new smart techniques in IoT context considering IoT limitations. Moreover, the survey will enable security individuals differentiate IoT NIDS from traditional ones.",IEEE Communications Surveys & Tutorials,2019.0,10.1109/COMST.2019.2896380,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
742e715f0197914aa695caf3fa87ed558b769897,https://www.semanticscholar.org/paper/742e715f0197914aa695caf3fa87ed558b769897,Literature Review on Big Data Analytics Methods,"Companies and industries are faced with a huge amount of raw data, which have information and knowledge in their hidden layer. Also, the format, size, variety, and velocity of generated data bring complexity for industries to apply them in an efficient and effective way. So, complexity in data analysis and interpretation incline organizations to deploy advanced tools and techniques to overcome the difficulties of managing raw data. Big data analytics is the advanced method that has the capability for managing data. It deploys machine learning techniques and deep learning methods to benefit from gathered data. In this research, the methods of both ML and DL have been discussed, and an ML/DL deployment model for IOT data has been proposed.",Social Media and Machine Learning,2019.0,10.5772/INTECHOPEN.86843,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
9eec5595fe73e0858d862061506650052ee5e507,https://www.semanticscholar.org/paper/9eec5595fe73e0858d862061506650052ee5e507,General Co-Chairs,"Track 1: 5G Technologies Track 2: 5G Application and Services Track 3: 5G & IoT Track 4: 5G Security and Privacy Track 5: 5G Trials, Experimental Results and Deployment Scenarios Track 6: 5G Hardware and Test / Measurements Track 7: 5G Special Verticals Track 8: 5G Special Topicals WORKSHOP PAPERS SS1: New IP and Future Internet data Planes (NIPFIP) WS1: Workshop on 5G Security: Current Trends, Challenges and New Enablers WS2: Workshop/Industry Panel: 5G Validation Trials Across Multiple Vertical Industries WS3: Blockchain based Secure Trust Environment Model for 5G (B-STEM-5G2020) WS4: 5G from Theory to Practice (5GToP) Workshop WS5: 3rd Workshop on Blockchain for 5G/6G, IoT, and CPS WS6: Workshop Evolution of Connected Vehicles: The Role of AI/Machine Learning, Emerging Standards, and 5G and Beyond WS7: Workshop on Satellite and Non-Terrestrial Networks for 5G WS8: Workshop on Optical Wireless Communication for 5G and Beyond WS9: Workshop on Large Scale Experimentation PROPOSALS Tutorials Industry Forums & Panel Sessions 5G Vertical Areas 5G Topical Areas Industry Demonstrations Entrepreneurship and Innovations Forum (EIF)",2021 International Conference on Multimedia Analysis and Pattern Recognition (MAPR),2021.0,10.1109/mapr53640.2021.9585265,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
e8db164ea96a61abcd282c65b4131908473a15c4,https://www.semanticscholar.org/paper/e8db164ea96a61abcd282c65b4131908473a15c4,Turning a Breakthrough Technology into a Scalable Process: Sealed Wellbore Pressure Monitoring,"
 A breakthrough patent-pending pressure diagnostic technique using offset sealed wellbores as monitoring sources was introduced at the 2020 Hydraulic Fracturing Technology Conference. This technique quantifies various hydraulic fracture parameters using only a surface gauge mounted on the sealed wellbore(s). The initial concept, operational processes, and analysis techniques were developed and deployed by Devon Energy. By scaling and automating the process, Sealed Wellbore Pressure Monitoring (SWPM) is now available to the industry as a repeatable workflow that greatly reduces analysis time and improves visualizations to aid data interpretations.
 The authors successfully automated the SWPM analysis procedure using a cloud-based software platform designed to ingest, process, and analyze high-frequency hydraulic fracturing data. The minimum data for the analysis consists of the standard frac treatment data combined with the high-resolution pressure gauge data for each sealed wellbore. The team developed machine learning algorithms to identify the key events required by a sealed wellbore pressure analysis: the start, end, and magnitude of each pressure response detected in the sealed wellbore(s) while actively fracturing offset wells. The result is a rapid, repeatable SWPM analysis that minimizes individual interpretation biases.
 The primary deliverables from SWPM analyses are the Volumes to First Response (VFR) on a per stage basis. In many projects, multiple pressure responses within a single stage have been observed, which provides valuable insight into fracture network complexity and cluster/stage efficiency. Various methods are used to visualize and statistically analyze the data.
 A scalable process facilitates creating a statistical database for comparing completion designs that can be segmented by play, formation, or other geological variations. Completion designs can then be optimized based upon the observed well responses. With enough observations and based on certain spacings, probabilities of when to expect fracture interactions could be assigned for different plays.",,2021.0,10.2118/204193-MS,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
ddd213ecbc9ba61dd1907f493c050d8d5593dbc7,https://www.semanticscholar.org/paper/ddd213ecbc9ba61dd1907f493c050d8d5593dbc7,Contemporary NLP Modeling in Six Comprehensive Programming Assignments,"We present a series of programming assignments, adaptable to a range of experience levels from advanced undergraduate to PhD, to teach students design and implementation of modern NLP systems. These assignments build from the ground up and emphasize full-stack understanding of machine learning models: initially, students implement inference and gradient computation by hand, then use PyTorch to build nearly state-of-the-art neural networks using current best practices. Topics are chosen to cover a wide range of modeling and inference techniques that one might encounter, ranging from linear models suitable for industry applications to state-of-the-art deep learning models used in NLP research. The assignments are customizable, with constrained options to guide less experienced students or open-ended options giving advanced students freedom to explore. All of them can be deployed in a fully autogradable fashion, and have collectively been tested on over 300 students across several semesters.",TEACHINGNLP,2021.0,10.18653/V1/2021.TEACHINGNLP-1.17,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
0b0647b68b332057635fd1cd9b9387a0d7e6fd89,https://www.semanticscholar.org/paper/0b0647b68b332057635fd1cd9b9387a0d7e6fd89,Real-Time Lime Quality Control through Process Automation,"In the steel industry Tata steel, India, most of the lime produced in the lime plant is used in the steel-making process at LD shops. The quality of steel produced at LD shops depends on the quality of lime used. Moreover, the lime also helps in the crucial dephosphorization process during steel-making. The calcined lime produced in the lime plant goes to the laboratory for testing its final quality (CaO%), which is very difficult to control. To predict, control and enhance the quality of lime during lime making process, five machine-learning-based models such as multivariate linear regression, support vector machine, decision tree, random forest and extreme gradient boosting have been developed using different algorithms. Python has been used as a tool to integrate the algorithms in the models. Each model has been trained on the past 14 months’ data of process parameters, collected from level 1 sensor devices, to predict the future quality of lime. To boost the model’s prediction performance, hyper-parameter tuning has been performed using grid-search algorithm. A comparative study has been done among all the models to select a final model with the least root mean square error in predicting and control future lime quality. After the comparison, results show that the model incorporating support vector machine algorithm has least value of root mean square error of 1.23 in predicting future lime quality. In addition to this, a self-learning approach has also been incorporated into support vector machine model to enhance its performance further in realtime. The result shows that the performance has been boosted from 85% strike-rate in +/-2 error range to 90% of strike-rate in +/-1 error range in real-time. Further, the above predictive model has been extended to build a control model which gives prescriptions as output to control the future quality of lime. For this purpose, a golden batch of good data has been fetched which has shown the best quality of lime (≥ 94% of CaO%). A good range of process parameters has been extracted in the form of upper control limit and lower control limit to tune the set-points and to give the prescriptions to the user. The integration of these two models (Predictive model and control model) helps in controlling the quality of lime 12 hours before its final production of lime in lime plant. Results show that both models (Predictive model and control model) have 90% of strike-rate within +/-1 of error in real-time. Finally, a human machine interface has been developed to facilitate the user to take action based on control model’s output. Eventually this work is deployed as a lime making process automation to predict and control the lime quality.",,2021.0,10.35940/IJESE.B2502.057221,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
9c0a10217286b64a2f5a64724d636f78d712adc4,https://www.semanticscholar.org/paper/9c0a10217286b64a2f5a64724d636f78d712adc4,Predictive Asset Analytics: The Future of Maintenance,"
 Major Overhauls (MOH) of major Rotating Equipment is an essential activity to ensure equipment and overall plant's productivity and reliability requirements are met. This submission summarizes Maintenance cost reduction and MOH extension benefits on an integrally geared centrifugal Instrument Air (IA) compressor through a first of its kind Predictive Maintenance (PdM) solution project in ADNOC.
 Appropriate planning for Major Overhauls (MOH) in accordance with OEM, company standards and international best practices are crucial steps. Digitalization continues to transform the industry, with enhancements to maintenance practices a fundamental aspect. Centralized Predictive Analytics & Diagnostics (CPAD) project is a first of its kind in ADNOC as it ventures into on one of the largest predictive maintenance projects in the oil & gas industry. CPAD enables Predictive Maintenance (PdM) through Advanced Pattern Recognition (APR) and Machine Learning (ML) technologies to effectively monitor & assess equipment performance and overall healthiness. Equipment performance is continuously assessed through the developed asset management predictive analytics tool. Through this tool, models associated with the equipment were evaluated to detect performance deviation from historical normal operating behavior. Any deviation from the historical norm would be flagged to indicate condition degradation and/or performance drop. Moreover, the software is configured to alert for subtle changes in the system behavior that are often an early warning sign of failure. This allows for early troubleshooting, planning and appropriate intervention by maintenance teams.
 Using the predictive analytics software solution, an MOH interval extension was implemented for an integrally geared centrifugal IA compressor installed at an ADNOC Gas Processing site. The compressor was due for MOH at its traditional fixed maintenance interval of 40,000 running hours in Nov 2019. Through this approach, the actual performance and condition of the compressor was assessed. Its process and equipment parameters (i.e. casing vibrations, bearing vibrations, bearing temperatures and lube oil supply temperature/pressure, etc.) were reviewed, which did not flag any abnormality. The compressor's performance had not deviated from the historical norm; indicating that the equipment was in a healthy condition and had no signs of performance degradation. With this insight, a 15 months extension of the MOH was achieved. Furthermore, a 30% maintenance cost reduction throughout the compressor's life cycle is projected while ensuring equipment's reliability and integrity are upheld. A total of 7 days maintenance down time including work force and materials planning for the MOH activities was deferred. The equipment remained in operation until its rescheduled date for MOH.
 Through the deployment of predictive analytics solutions, informed decisions can be made by maintenance professionals to challenge traditional maintenance practices, increase Mean Time Between Overhauls (MBTO), realize the full potential of a plant's process & utilities machinery and optimize operational costs of plant assets.","Day 3 Wed, November 17, 2021",2021.0,10.2118/207616-ms,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
44ddce4528251f6357503de05531992efe081c31,https://www.semanticscholar.org/paper/44ddce4528251f6357503de05531992efe081c31,Digital Monitoring and Optimization of Breeding Processes: An Aquaculture Case Study,,Proceedings of the Fifteenth International Conference on Management Science and Engineering Management,2021.0,10.1007/978-3-030-79203-9_40,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
47bf258d736fd91cc2e4a9f8a27f309d00db03f5,https://www.semanticscholar.org/paper/47bf258d736fd91cc2e4a9f8a27f309d00db03f5,Task and Model Agnostic Adversarial Attack on Graph Neural Networks,"Graph neural networks (GNNs) have witnessed significant adoption in the industry owing to impressive performance on various predictive tasks. Performance alone, however, is not enough. Any widely deployed machine learning algorithm must be robust to adversarial attacks. In this work, we investigate this aspect for GNNs, identify vulnerabilities, and link them to graph properties that may potentially lead to the development of more secure and robust GNNs. Specifically, we formulate the problem of task and model agnostic evasion attacks where adversaries modify the test graph to affect the performance of any unknown downstream task. The proposed algorithm, GRAND (Graph Attack via Neighborhood Distortion) shows that distortion of node neighborhoods is effective in drastically compromising prediction performance. Although neighborhood distortion is an NP-hard problem, GRAND designs an effective heuristic through a novel combination of Graph Isomorphism Network with deep Q-learning. Extensive experiments on real datasets show that, on average, GRAND is up to 50% more effective than state of the art techniques, while being more than 100 times faster.",ArXiv,2021.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
b5b931c6c334464c7a720c3fa35bbb0e9640daea,https://www.semanticscholar.org/paper/b5b931c6c334464c7a720c3fa35bbb0e9640daea,A Systematic Approach for Evaluating Artificial Intelligence Models in Industrial Settings,"Artificial Intelligence (AI) is one of the hottest topics in our society, especially when it comes to solving data-analysis problems. Industry are conducting their digital shifts, and AI is becoming a cornerstone technology for making decisions out of the huge amount of (sensors-based) data available in the production floor. However, such technology may be disappointing when deployed in real conditions. Despite good theoretical performances and high accuracy when trained and tested in isolation, a Machine-Learning (M-L) model may provide degraded performances in real conditions. One reason may be fragility in treating properly unexpected or perturbed data. The objective of the paper is therefore to study the robustness of seven M-L and Deep-Learning (D-L) algorithms, when classifying univariate time-series under perturbations. A systematic approach is proposed for artificially injecting perturbations in the data and for evaluating the robustness of the models. This approach focuses on two perturbations that are likely to happen during data collection. Our experimental study, conducted on twenty sensors’ datasets from the public University of California Riverside (UCR) repository, shows a great disparity of the models’ robustness under data quality degradation. Those results are used to analyse whether the impact of such robustness can be predictable—thanks to decision trees—which would prevent us from testing all perturbations scenarios. Our study shows that building such a predictor is not straightforward and suggests that such a systematic approach needs to be used for evaluating AI models’ robustness.",Sensors,2021.0,10.3390/s21186195,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
f7fe5aee32641a18e1bb906d818af6e4a4bdcfb6,https://www.semanticscholar.org/paper/f7fe5aee32641a18e1bb906d818af6e4a4bdcfb6,Toward Scalable Artificial Intelligence in Finance,"Innovation in Artificial Intelligence (AI) continues to produce a wealth of techniques, mostly coming from the inductive form of AI also known as Machine Learning (ML). The majority of ML algorithms is industry-neutral and business process agnostic. ML innovation is propelled by publicly available research, which gets harvested into Open Source for wide distribution through software and Cloud vendors.Ongoing AI technology work creates an immense source of assets for data-driven modeling, delivered as software libraries. However, the application of these assets for data monetization in finance does not happen with nearly comparable success or speed. The latter challenge is commonly known as the ""scalability problem of AI"". As new techniques continue to grow vigorously, the investment from large finance institutions to cost-effectively produce applications for a variety of lines-of-business (LoBs) and business processes will increase. The availability of ML capabilities on Public Cloud is a way for enterprises to increase productivity by benefiting from the best AI assets available from providers and startups. But data is constrained in terms of location, access and use in most finance competences by either laws or internal Governance, Risk and Compliance (GRC) rules. Legal limitations include, and go beyond, Privacy Acts, impacting non-retail processes where AI techniques must be explained in layperson language to decision-makers and regulators before field deployment. The latter is not yet achieved satisfactorily. Lastly, a large percentage of AI projects fail, in part due to unsuitable ML modeling for analytics and forecasting problems in finance. The variety and complexity of human behavior present in most finance processes calls for understanding AI at a level of cognitive depth that has no precedent in other industries. It is imperative that AI be approached so that finance competence and functional specificity are embedded a-priori into ML techniques and not as use-case afterthoughts. For acceleration of AI assessments, it is critical that ML techniques available in software implement models readily aligned to finance problems.This paper presents an approach to building an Architecture for Artificial Intelligence (AI) in Finance by focusing on analytics and forecasting for business-to-business operations. This AI Architecture hinges on three axes and their interplay: Design Dimensions, Modeling Building-Blocks and Work-Practice. The goal is to support finance practitioners navigate the plethora of AI options more effectively and accelerate data monetization. While ML techniques in data analytics and forecasting apply to many scenarios, this paper focuses on selected competences in Banking, Financial Markets and Chief Finance Officer (CFO) operations.The architecture and method introduced in this paper is a first step toward a service practice. We harvest from our work carried out in banks, asset management firms and CFO lines-of-business as well as R&D experiences in new finance technologies for over one decade. As with any other architecture and deployment methodology, this work requires further harvesting, more information technology tools and sharing experiences across practitioners. It is hoped that finance organizations could adopt these new capabilities in their own Centers of Excellence or other internal organizations leading data-driven transformation and monetization across the firm.",2021 IEEE International Conference on Services Computing (SCC),2021.0,10.1109/scc53864.2021.00067,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
137873d22d49760b9469c63dad4211f3083653b8,https://www.semanticscholar.org/paper/137873d22d49760b9469c63dad4211f3083653b8,"Real Time Cloud-Based Automation for Formation Evaluation Optimization, Risk Mitigation and Decarbonization","
 Recent developments in artificial intelligence (AI) have enabled upstream exploration and production companies to make better, faster and accurate decisions at any stage of well construction, while reducing operational expenditure and risk, increasing logistic efficiencies. The achieved optimization through digitization at the wellsite will significantly reduce the carbon emissions per well drilled when fully embraced by the industry. In addition, an industry pushed to drill in more challenging environments, they must embrace safer and more practical methods.
 An increase in prediction techniques, to generate synthetic formation evaluation wellbore logs, has unlocked the ability to implement a combination of predictive and prescriptive analytics with petrophysical and geochemical workflows in real time. The foundation of the real time automation is based on advanced machine learning (ML) techniques that are deployed via cloud connectivity.
 Three levels of logging precision are defined in the automated workflow based on the data inputs and machine learning models. The first level is the forecasting ahead of the bit that implements advanced machine learning using historical data, aiding proactive operational decisions. The second level has improved precision by incorporating real time drilling measurements and providing a credible contingency to for wellbore logging program. The last level incorporates petrophysical workflows and geochemical measurements to achieve the highest precision for logging prediction in the industry. Supervised and unsupervised machine learning models are presented to demonstrate the path for automation.
 Precision above 95% in the real time automated workflows was achieved with a combination of physics and advanced machine learning models. The automation of the workflow has assisted with optimization of logging programs utilizing technology with costly lost in hole charges and high rate of tool failures in offshore operations.
 The optimization has reduced the requirement for logistics associated with logging and eliminated the need for radioactive sources and lithium batteries.
 Highest precision in logging prediction has been achieved through an automated workflow for real time operations. In addition, the workflow can also be deployed with robotics technology to automate sample collection, leading to increased efficiencies.","Day 1 Tue, September 07, 2021",2021.0,10.2118/205402-ms,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
15fe17152151c755c95a2eac9783ea02e44e594e,https://www.semanticscholar.org/paper/15fe17152151c755c95a2eac9783ea02e44e594e,Flight Delay Prediction: Data Analysis and Model Development,"Flight delays in air transportation are a major concern that has adverse effects on the economy, the passengers, and the aviation industry. This matter critically requires an accurate estimation for future flight delays that can be implemented to improve airport operations and customer satisfaction. Having said that, a massive volume of data and an extreme number of parameters have restricted the way to build an accurate model. Many existing flight delay prediction methods are based on small samples and/or are complex to interpret with little or no opportunity for machine learning deployment. This paper develops a prediction model by analysing the data of domestic flights within the United States of America (USA). The proposed model gains insight into factors causing flight delays, cancellations and the relationship between departure and arrival delay using exploratory data analysis. In addition, Random Forest (RF) algorithm is used to train and test the big dataset to help the model development. A web application has also been developed to implement the model and the testing results are presented with the limitation discussed.",2021 26th International Conference on Automation and Computing (ICAC),2021.0,10.23919/icac50006.2021.9594260,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
ace327beebe2a790d07d060faa5270f9ae9668cb,https://www.semanticscholar.org/paper/ace327beebe2a790d07d060faa5270f9ae9668cb,A collaborative approach to support interoperability and awareness of Internet of Everything (IoE) enablers,"Internet of Everything (IoE) is a promising paradigm that integrates the Internet of Things (IoT), Industrial Internet, Internet of People, and many Internet-based paradigms to transform the industry, society, and people lives. It provides seamless integration of intelligent devices – with sensing, identification, processing, communication, networking capabilities, Big Data (machine learning, analytics, and distributed computing), and human sensors (collaboration, intelligent cognition, and social networks). IoE brings excellent opportunities to improve changes in society with collaborative intelligent systems. However, these new technologies also bring significant challenges and do not address major existing issues, including interoperability, reliability, and intelligence management. Awareness of these issues is required before IoE applications can be widely deployed. This work introduces an observatory for cataloging IoE applications. Registering these applications will support researchers, developers, and users to exchange more information, design improved IoE applications, facilitating the composition of different enablers (human and non-humans). The main contributions of this work include the proposal of a technology platform, the IoE Database (IoEDB), that enables the distributed and collaborative cataloging of IoE initiatives and provides the evolution of a ‘live’ IoE knowledge-based taxonomy to support interoperability and awareness of IoE enablers.",2021 IEEE 2nd International Conference on Human-Machine Systems (ICHMS),2021.0,10.1109/ICHMS53169.2021.9582657,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
7f8faa2145674752541f9d6d3d2414ddca9c9936,https://www.semanticscholar.org/paper/7f8faa2145674752541f9d6d3d2414ddca9c9936,How Artificial Intelligence can Guide Marcellus Development,"
 The application of Artificial Intelligence for planning has received increased attention in the energy industry in the past few years, particularly for the increased production efficiency requirements and environmental standards. The objective of this paper is to show the successful integration of production, completion, subsurface and spatial data using machine-learning algorithms to predict production performance for future development wells.
 The internal Marcellus Business Unit (MBU) well database, populated with data of 500+ historical wells, has been used in this study. Production data, treated as timeseries, has been processed using functional Principal Component Analysis (PCA) to allow removal of outliers and mode detection. Utilizing this data, a suite of machine-learning algorithms has been applied to reconstruct gas production from available and target well data. Uncertainty quantification has been provided for production curves to identify the quality of prediction. During the study, the sensitivity analysis on input variables has been performed iteratively to screen and rank the most important variables for prediction. The workflow, Unconventional Reservoir Assistant (URA), has been implemented in a proprietary cloud-based platform providing the necessary means for data upload, integration, pre-processing, and finally model training and deployment. This allows the user to focus on the evaluation of model output quality, data filter and workspace generation for continuous model testing and improvement. The full well dataset, split into trained and tested data, has been used for prediction as a preliminary guide to where the most prolific areas of development are located. Results were ranked based on production expected by pad and based on normalized performance. The information was then used to compare with type curves and original development order. In parallel, economic evaluation of break-even was performed to rank all future pads. Consequently, integration of the model prediction and breakeven ranking were used to generate the final development order for the MBU.
 The URA tool has shown preliminary success in predicting production performance for the pilot development area. Multiple case studies have been run achieving blind test results with high accuracy for historical prediction. Results show some dependency of predictor variable ranking on the field development area, providing insight on how subsurface may affect well dynamic behavior. This paper describes how the integration of URA can complement the development workflow for unconventional reservoirs and be used to predict performance based on complex data integration.
 The methodology used is superior to standard machine learning models providing only production indicators, as it gives the user the possibility to evaluate economics and completion design sensitivity for future well activities. The methodology can be further extended as a proxy model for well schedule optimization in planning or for better insight into well refrac selection.","Day 2 Wed, November 03, 2021",2021.0,10.2118/201786-ms,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
c1e231a7694f0c51cbf15924aed10fad578358e6,https://www.semanticscholar.org/paper/c1e231a7694f0c51cbf15924aed10fad578358e6,Novel applications of soft computing techniques for industrial and environmental enterprises,"This special issue compiles recent applications of soft-computing techniques for managing enterprises within industrial and environmental sectors. It is aimed at both researchers and practitioners from academia and industry who are engaged in deploying soft-computing solutions for real-life enterprise problems. Six papers are included in this special issue, covering a wide variety of case studies, ranging from conversational systems to vehicle routing problems. Additionally, a variety of soft-computing techniques have been applied, such as deep learning, random forest, particle swarm optimization, clustering, and metaheuristics among others. Thanks to the wide-range panoramic view presented by these complementary works, readers can get a feel for developing up-to-date soft-computing systems to solve present problems in enterprise contexts. In the first contribution by Cárdenas-Montes, recent machine learning models are applied for finding anomalies in Ozone-urban images generated from an Air Quality Monitoring Network. To do so, deep learning with maps-based data coming from an air-quality sensor network is applied. Differentiating from previous work, time series are not individually handled, but they are used for generating maps. This is carried out thanks to advances in computer vision to process the data. Besides, the manuscript contributes to the outlier detection problem, investigating the most appropriate map-generation technique for different scientific targets. The author's proposal is compared to the direct use of DBSCAN on the intensity of the pixels, as well as other approaches for learning latent variables, such as autoencoders based on Convolutional Neural Networks (undercomplete and denoising). The proposed approach achieves high performance when labelling maps with anomalies at a local and a global scale, including the outliers identified by the direct use of DBSCAN on the intensity of the pixels. By using the author's approach, anomalous measurements in a few monitoring stations are also detected. López-Sánchez et al. propose a new method to solve a bi-objective variant of the Vehicle Routing Problem, taking into account its industrial and environmental implications. More precisely, the transportation of products by industrial enterprises to their customers is addressed, in which has been named the Periodic Vehicle Routing Problem with Service Choice (PVRP-SC) problem. It is defined as the problem of finding a set of routes for each vehicle and each day over a planning horizon in order to minimize total travel cost minus service benefit while satisfying operational constraints. The authors approach PVRP-SC from a novel perspective; instead of considering the problem as a single-objective optimization problem, they have solved the problem as a bi-objective optimization one, that minimizes the total emissions produced by all vehicles and maximizes the service quality measured as the number of times that a customer is visited by a vehicle in order to be served. To do that, a multi-objective approach based on Multi-Start Local Searches is applied. According to the experimental setup that has been carried out, the proposed algorithm outperforms other standard alternatives. The next contribution by Burkart et al. aims to examine how participants, supported by a Decision Support System, revise their initial prediction by four different treatments in a between-subject design study. This research work examines how prediction revision (adjusting an initial prediction on the basis of new information) is affected by the provided explanations. The four treatments (interpretable regression model, Random Forest, black box with a local explanation, and black box with a global explanation) differ in the degree of explainability in order to understand the system decisions. Participants in each treatment were told basic facts about their corresponding model and how each is used for producing a response when presented with a set of predictor values. Amazon Mechanical Turk with Sophie Labs was used as a platform for designing online experiments, in which 80 participants were recruited and randomly assigned to the experiments. The obtained results show that all participants improved their predictions after receiving advice whether it was a complete black box or a black box with an explanation. Authors conclude that naturally interpretable models were not incorporated more heavily in the decision process than black box models and explanations from them. Bayram et al. propose an anomaly detection system based on acoustic signals, especially required to quickly detect and interfere with the abnormal events during the industrial processes. More precisely, the authors propose a real time Acoustic Anomaly Detection System employing sequence-to-sequence Autoencoder models. The proposed processing pipeline uses the audio features extracted from the streaming audio signal captured by a single-channel microphone. The reconstruction error generated by the Autoencoder models is calculated to measure the degree of abnormality of the sound event. The performance of Convolutional Long Short-Term Memory Autoencoder is evaluated and compared with sequential Convolutional Autoencoder using sounds captured from various industrial manufacturing processes. The experimental setup comprises different Signal-to-Noise Ratio (SNR) conditions of sound events such as an explosion, fire, and glass breaking. Four acoustic datasets have been analyzed, containing sounds associated to the following industrial activities: painting, cutting, welding, and robotic arm operation. In the validation that was carried out with the real time system, it is shown that the Long Short-Term Memory version of the Autoencoder outperforms the other version of such model according to the AUC (Area Under the ROC Curve) metric. Received: 5 October 2020 Accepted: 7 October 2020",Expert Syst. J. Knowl. Eng.,2020.0,10.1111/exsy.12654,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
25862e229bea4f601e7b8ea747f1fd1217cce08c,https://www.semanticscholar.org/paper/25862e229bea4f601e7b8ea747f1fd1217cce08c,Failure Prediction for Large-scale Water Pipe Networks Using GNN and Temporal Failure Series,"Pipe failure prediction in the water industry aims to prioritize the pipes that are at high risk of failure for proactive maintenance. However, existing statistical or machine learning models that rely on historical failures and asset attributes can hardly leverage the structure information of pipe networks. In this work, we develop a failure prediction framework for pipe networks by jointly considering the pipes' features, the network structure, the geographical neighboring effect, and the temporal failure series. We apply a multi-hop Graph Neural Network (GNN) to failure prediction. We propose a method of constructing a geographical graph structure depending on not only the physical connections but also geographical distances between pipes. To differentiate the pipes with diverse properties, we employ an attention mechanism in the neighborhood aggregation process of each GNN layer. Also, residual connections and layer-wise aggregation are used to avoid the over-smoothing issue in deep GNNs. The historical failures exhibit a strong temporal pattern. Inspired by point process, we develop a module to learn the pipes' evolutionary effect and the time-decayed excitement of historical failures on the current state of the pipe. The proposed framework is evaluated on two real-world large-scale pipe networks. It outperforms the existing statistical, machine learning, and state-of-the-art GNN baselines. Our framework provides the water utility with core data-driven support for proactive maintenance including regular pipe inspection, pipe renewal planning, and sensor system deployment. It can be extended to other infrastructure networks in the future.",CIKM,2021.0,10.1145/3459637.3481918,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
eff83f213e3355e5d8ed0dc50285c3c487dfb2c3,https://www.semanticscholar.org/paper/eff83f213e3355e5d8ed0dc50285c3c487dfb2c3,Automated weed detection system in smart farming for developing sustainable agriculture,,International Journal of Environmental Science and Technology,2021.0,10.1007/s13762-021-03606-6,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
27507038d7b2f709c7fe5af83eee6460f3d70d6f,https://www.semanticscholar.org/paper/27507038d7b2f709c7fe5af83eee6460f3d70d6f,A Framework of Best Practices for Delivering Successful Artificial Intelligence Projects. A Case Study Demonstration,"
 A practical framework that outlines the critical steps of a successful process that uses data, machine learning (Ml), and artificial intelligence (AI) is presented in this study. A practical case study is included to demonstrate the process. The use of artificial intelligent and machine learning has not only enhanced but also sped up problem-solving approaches in many domains, including the oil and gas industry. Moreover, these technologies are revolutionizing all key aspects of engineering including; framing approaches, techniques, and outcomes. The proposed framework includes key components to ensure integrity, quality, and accuracy of data and governance centered on principles such as responsibility, equitability, and reliability. As a result, the industry documentation shows that technology coupled with process advances can improve productivity by 20%.
 A clear work-break-down structure (WBS) to create value using an engineering framework has measurable outcomes. The AI and ML technologies enable the use of large amounts of information, combining static & dynamic data, observations, historical events, and behaviors. The Job Task Analysis (JTA) model is a proven framework to manage processes, people, and platforms. JTA is a modern data-focused approach that prioritizes in order: problem framing, analytics framing, data, methodology, model building, deployment, and lifecycle management. The case study exemplifies how the JTA model optimizes an oilfield production plant, similar to a manufacturing facility. A data-driven approach was employed to analyze and evaluate the production fluid impact during facility-planned or un-planned system disruptions. The workflows include data analytics tools such as ML&AI for pattern recognition and clustering for prompt event mitigation and optimization.
 The paper demonstrates how an integrated framework leads to significant business value. The study integrates surface and subsurface information to characterize and understand the production impact due to planned and unplanned plant events. The findings led to designing a relief system to divert the back pressure during plant shutdown. The study led to cost avoidance of a new plant, saving millions of dollars, environment impact, and safety considerations, in addition to unnecessary operating costs and maintenance. Moreover, tens of millions of dollars value per year by avoiding production loss of plant upsets or shutdown was created. The study cost nothing to perform, about two months of not focused time by a team of five engineers and data scientists. The work provided critical steps in ""creating a trusting"" model and ""explainability’. The methodology was implemented using existing available data and tools; it was the process and engineering knowledge that led to the successful outcome. Having a systematic WBS has become vital in data analytics projects that use AI and ML technologies. An effective governance system creates 25% productivity improvement and 70% capital improvement. Poor requirements can consume 40%+ of development budget. The process, models, and tools should be used on engineering projects where data and physics are present.
 The proposed framework demonstrates the business impact and value creation generated by integrating models, data, AI, and ML technologies for modeling and optimization. It reflects the collective knowledge and perspectives of diverse professionals from IBM, Lockheed Martin, and Chevron, who joined forces to document a standard framework for achieving success in data analytics/AI projects.","Day 2 Wed, September 22, 2021",2021.0,10.2118/206014-ms,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
e8db164ea96a61abcd282c65b4131908473a15c4,https://www.semanticscholar.org/paper/e8db164ea96a61abcd282c65b4131908473a15c4,Turning a Breakthrough Technology into a Scalable Process: Sealed Wellbore Pressure Monitoring,"
 A breakthrough patent-pending pressure diagnostic technique using offset sealed wellbores as monitoring sources was introduced at the 2020 Hydraulic Fracturing Technology Conference. This technique quantifies various hydraulic fracture parameters using only a surface gauge mounted on the sealed wellbore(s). The initial concept, operational processes, and analysis techniques were developed and deployed by Devon Energy. By scaling and automating the process, Sealed Wellbore Pressure Monitoring (SWPM) is now available to the industry as a repeatable workflow that greatly reduces analysis time and improves visualizations to aid data interpretations.
 The authors successfully automated the SWPM analysis procedure using a cloud-based software platform designed to ingest, process, and analyze high-frequency hydraulic fracturing data. The minimum data for the analysis consists of the standard frac treatment data combined with the high-resolution pressure gauge data for each sealed wellbore. The team developed machine learning algorithms to identify the key events required by a sealed wellbore pressure analysis: the start, end, and magnitude of each pressure response detected in the sealed wellbore(s) while actively fracturing offset wells. The result is a rapid, repeatable SWPM analysis that minimizes individual interpretation biases.
 The primary deliverables from SWPM analyses are the Volumes to First Response (VFR) on a per stage basis. In many projects, multiple pressure responses within a single stage have been observed, which provides valuable insight into fracture network complexity and cluster/stage efficiency. Various methods are used to visualize and statistically analyze the data.
 A scalable process facilitates creating a statistical database for comparing completion designs that can be segmented by play, formation, or other geological variations. Completion designs can then be optimized based upon the observed well responses. With enough observations and based on certain spacings, probabilities of when to expect fracture interactions could be assigned for different plays.",,2021,10.2118/204193-MS,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
7f8faa2145674752541f9d6d3d2414ddca9c9936,https://www.semanticscholar.org/paper/7f8faa2145674752541f9d6d3d2414ddca9c9936,How Artificial Intelligence can Guide Marcellus Development,"
 The application of Artificial Intelligence for planning has received increased attention in the energy industry in the past few years, particularly for the increased production efficiency requirements and environmental standards. The objective of this paper is to show the successful integration of production, completion, subsurface and spatial data using machine-learning algorithms to predict production performance for future development wells.
 The internal Marcellus Business Unit (MBU) well database, populated with data of 500+ historical wells, has been used in this study. Production data, treated as timeseries, has been processed using functional Principal Component Analysis (PCA) to allow removal of outliers and mode detection. Utilizing this data, a suite of machine-learning algorithms has been applied to reconstruct gas production from available and target well data. Uncertainty quantification has been provided for production curves to identify the quality of prediction. During the study, the sensitivity analysis on input variables has been performed iteratively to screen and rank the most important variables for prediction. The workflow, Unconventional Reservoir Assistant (URA), has been implemented in a proprietary cloud-based platform providing the necessary means for data upload, integration, pre-processing, and finally model training and deployment. This allows the user to focus on the evaluation of model output quality, data filter and workspace generation for continuous model testing and improvement. The full well dataset, split into trained and tested data, has been used for prediction as a preliminary guide to where the most prolific areas of development are located. Results were ranked based on production expected by pad and based on normalized performance. The information was then used to compare with type curves and original development order. In parallel, economic evaluation of break-even was performed to rank all future pads. Consequently, integration of the model prediction and breakeven ranking were used to generate the final development order for the MBU.
 The URA tool has shown preliminary success in predicting production performance for the pilot development area. Multiple case studies have been run achieving blind test results with high accuracy for historical prediction. Results show some dependency of predictor variable ranking on the field development area, providing insight on how subsurface may affect well dynamic behavior. This paper describes how the integration of URA can complement the development workflow for unconventional reservoirs and be used to predict performance based on complex data integration.
 The methodology used is superior to standard machine learning models providing only production indicators, as it gives the user the possibility to evaluate economics and completion design sensitivity for future well activities. The methodology can be further extended as a proxy model for well schedule optimization in planning or for better insight into well refrac selection.","Day 2 Wed, November 03, 2021",2021,10.2118/201786-ms,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
15fe17152151c755c95a2eac9783ea02e44e594e,https://www.semanticscholar.org/paper/15fe17152151c755c95a2eac9783ea02e44e594e,Flight Delay Prediction: Data Analysis and Model Development,"Flight delays in air transportation are a major concern that has adverse effects on the economy, the passengers, and the aviation industry. This matter critically requires an accurate estimation for future flight delays that can be implemented to improve airport operations and customer satisfaction. Having said that, a massive volume of data and an extreme number of parameters have restricted the way to build an accurate model. Many existing flight delay prediction methods are based on small samples and/or are complex to interpret with little or no opportunity for machine learning deployment. This paper develops a prediction model by analysing the data of domestic flights within the United States of America (USA). The proposed model gains insight into factors causing flight delays, cancellations and the relationship between departure and arrival delay using exploratory data analysis. In addition, Random Forest (RF) algorithm is used to train and test the big dataset to help the model development. A web application has also been developed to implement the model and the testing results are presented with the limitation discussed.",2021 26th International Conference on Automation and Computing (ICAC),2021,10.23919/icac50006.2021.9594260,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
0b0647b68b332057635fd1cd9b9387a0d7e6fd89,https://www.semanticscholar.org/paper/0b0647b68b332057635fd1cd9b9387a0d7e6fd89,Real-Time Lime Quality Control through Process Automation,"In the steel industry Tata steel, India, most of the lime produced in the lime plant is used in the steel-making process at LD shops. The quality of steel produced at LD shops depends on the quality of lime used. Moreover, the lime also helps in the crucial dephosphorization process during steel-making. The calcined lime produced in the lime plant goes to the laboratory for testing its final quality (CaO%), which is very difficult to control. To predict, control and enhance the quality of lime during lime making process, five machine-learning-based models such as multivariate linear regression, support vector machine, decision tree, random forest and extreme gradient boosting have been developed using different algorithms. Python has been used as a tool to integrate the algorithms in the models. Each model has been trained on the past 14 months’ data of process parameters, collected from level 1 sensor devices, to predict the future quality of lime. To boost the model’s prediction performance, hyper-parameter tuning has been performed using grid-search algorithm. A comparative study has been done among all the models to select a final model with the least root mean square error in predicting and control future lime quality. After the comparison, results show that the model incorporating support vector machine algorithm has least value of root mean square error of 1.23 in predicting future lime quality. In addition to this, a self-learning approach has also been incorporated into support vector machine model to enhance its performance further in realtime. The result shows that the performance has been boosted from 85% strike-rate in +/-2 error range to 90% of strike-rate in +/-1 error range in real-time. Further, the above predictive model has been extended to build a control model which gives prescriptions as output to control the future quality of lime. For this purpose, a golden batch of good data has been fetched which has shown the best quality of lime (≥ 94% of CaO%). A good range of process parameters has been extracted in the form of upper control limit and lower control limit to tune the set-points and to give the prescriptions to the user. The integration of these two models (Predictive model and control model) helps in controlling the quality of lime 12 hours before its final production of lime in lime plant. Results show that both models (Predictive model and control model) have 90% of strike-rate within +/-1 of error in real-time. Finally, a human machine interface has been developed to facilitate the user to take action based on control model’s output. Eventually this work is deployed as a lime making process automation to predict and control the lime quality.",,2021,10.35940/IJESE.B2502.057221,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
137873d22d49760b9469c63dad4211f3083653b8,https://www.semanticscholar.org/paper/137873d22d49760b9469c63dad4211f3083653b8,"Real Time Cloud-Based Automation for Formation Evaluation Optimization, Risk Mitigation and Decarbonization","
 Recent developments in artificial intelligence (AI) have enabled upstream exploration and production companies to make better, faster and accurate decisions at any stage of well construction, while reducing operational expenditure and risk, increasing logistic efficiencies. The achieved optimization through digitization at the wellsite will significantly reduce the carbon emissions per well drilled when fully embraced by the industry. In addition, an industry pushed to drill in more challenging environments, they must embrace safer and more practical methods.
 An increase in prediction techniques, to generate synthetic formation evaluation wellbore logs, has unlocked the ability to implement a combination of predictive and prescriptive analytics with petrophysical and geochemical workflows in real time. The foundation of the real time automation is based on advanced machine learning (ML) techniques that are deployed via cloud connectivity.
 Three levels of logging precision are defined in the automated workflow based on the data inputs and machine learning models. The first level is the forecasting ahead of the bit that implements advanced machine learning using historical data, aiding proactive operational decisions. The second level has improved precision by incorporating real time drilling measurements and providing a credible contingency to for wellbore logging program. The last level incorporates petrophysical workflows and geochemical measurements to achieve the highest precision for logging prediction in the industry. Supervised and unsupervised machine learning models are presented to demonstrate the path for automation.
 Precision above 95% in the real time automated workflows was achieved with a combination of physics and advanced machine learning models. The automation of the workflow has assisted with optimization of logging programs utilizing technology with costly lost in hole charges and high rate of tool failures in offshore operations.
 The optimization has reduced the requirement for logistics associated with logging and eliminated the need for radioactive sources and lithium batteries.
 Highest precision in logging prediction has been achieved through an automated workflow for real time operations. In addition, the workflow can also be deployed with robotics technology to automate sample collection, leading to increased efficiencies.","Day 1 Tue, September 07, 2021",2021,10.2118/205402-ms,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
7401b1fb764bf71a4342e0888e25268a8b11009f,https://www.semanticscholar.org/paper/7401b1fb764bf71a4342e0888e25268a8b11009f,Automatic lane change scenario extraction and generation of scenarios in OpenX format from real-world data,"Autonomous Vehicles (AV)’s wide-scale deployment appears imminent despite many safety challenges yet to be resolved. The modern autonomous vehicles will undoubtedly include machine learning and probabilistic techniques that add significant complexity to the traditional verification and validation methods. Road testing is essential before the deployment, but scenarios are repeatable, and it’s hard to collect challenging events. Exploring numerous, diverse and crucial scenarios is a time-consuming and expensive approach. The research community and industry have widely accepted scenario-based testing in the last few years. As it is focused directly on the relevant critical road situations, it can reduce the effort required in testing. The scenario-based testing in simulation requires the realistic behaviour of the traffic participants to assess the System Under Test (SUT). It is essential to capture the scenarios from the real world to encode the behaviour of actual traffic participants. This paper proposes a novel scenario extraction method to capture the lane change scenarios using point-cloud data and object tracking information. This method enables fully automatic scenario extraction compared to similar approaches in this area. The generated scenarios are represented in OpenX format to reuse them in the SUT evaluation easily. The motivation of this framework is to build a validation dataset to generate many critical concrete scenarios. The code is available online at https://github.com/dkarunakaran/ scenario_extraction_framework",ArXiv,2022,10.48550/arXiv.2203.07521,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
b9230e5d90babb478fb6fc1eca69398d2553b75a,https://www.semanticscholar.org/paper/b9230e5d90babb478fb6fc1eca69398d2553b75a,An Approach towards Deployable Hybrid Product Recommendation Systems for E-Commerce,"As India is moving fast towards digital economy, E-commerce industry has been on rise. Many platforms provide their customers with a shopping experience better than actual physical stores. Several E-commerce websites use different methods to improve the customer engagement and revenue. One such technique is the use of personalized recommendation systems, which uses customer’s data like interests, purchase history, ratings to suggest new products, which they may like. Recom-mendation systems are used by E-commerce websites to suggest new products to their users. The products can be suggested based on the top merchants on the website, based on the interests of the user or based the past purchase pattern of the cus-tomer. Recommender systems are machine learning based systems that help users discover new products. Due to the recent pandemic situation of 2020 and 2021, many of the local retail stores have been trying to shift their business to online plat-forms such as dedicated websites or social media. The proposed methodology based on Machine Learning aims to enable local online retail business owners to enhance their customer engagement and revenue by providing users with personalized recommendations using past data using methods such as Collaborative Filtering, Popularity-based and Content-Based Filtering.","International Journal of Scientific Research in Computer Science, Engineering and Information Technology",2022,10.32628/cseit228332,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
b3b2d9bc2af4bb0698f14712742a7859a72cb206,https://www.semanticscholar.org/paper/b3b2d9bc2af4bb0698f14712742a7859a72cb206,Edge computing implementation of safety monitoring system in frame of IIoT,"Intelligent automation of manufacturing processes is one of the topics of the fourth industrial revolution - Industry 4.0. Edge Computing, Cloud Computing and inference of machine learning algorithms on equipments in the frame of Industrial Internet of Things is therefore of high interest. In this paper the implementation of a machine learning algorithm in the form of neural network is used for detection of wearing protective equipment in production environment. Furthermore, the edge computing technology is used for deployment of the proposed application on the edge device using video data from the Closed-Circuit Television system. The Edge device is communicating with the Cloud, where streaming platforms are running, taking necessary safety precautions to prevent accidents at work and also record these situations for reporting purposes.",2022 23rd International Carpathian Control Conference (ICCC),2022,10.1109/iccc54292.2022.9805918,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
0bf6ef7ae4aaaecae885c456c983d361336ab3df,https://www.semanticscholar.org/paper/0bf6ef7ae4aaaecae885c456c983d361336ab3df,An Efficient YOLO Algorithm with an Attention Mechanism for Vision-Based Defect Inspection Deployed on FPGA,"Industry 4.0 features intelligent manufacturing. Among them, the vision-based defect inspection algorithm is remarkable for quality control in parts manufacturing. With the help of AI and machine learning, auto-adaptive instead of manual operation is achievable in this field, and much progress has been made in recent years. In this study, considering the demand of inspection features in industrialization, we made further improvement in smart defect inspection. An efficient algorithm using Field Programmable Gate Array (FPGA)-accelerated You Only Look Once (YOLO) v3 based on an attention mechanism is proposed. First, because of the relatively fixed camera angle and defect features, an attention mechanism based on the concept of directing the focus of defect inspection is proposed. The attention mechanism consists of three improvements: (a) image preprocessing, which is to tailor images for selectively concentrating on the defect relevant things. Image preprocessing mainly includes cutting, zooming and splicing, named CZS operations. (b) Tailoring the YOLOv3 backbone network, which is to ignore invalid inspection regions in deep neural networks and optimize the network structure. (c) Data augmentation. First, two improvements can be made to efficiently reduce deep learning operations and accelerate the inspection speed, but the preprocessed images are similar and the lack of diversity will reduce network accuracy. So, (c) is added to mitigate the lack of considerable amounts of training data. Second, the algorithm is deployed on a PYNQ-Z2 FPGA board to meet the industrialization production requirements for accuracy, efficiency and extensibility. FPGA can provide a low-latency, low-cost, high-power-efficiency and flexible architecture that enables deep learning acceleration for industrial scenarios. A Xilinx Deep Neural Network Development Kit (DNNDK) converted the improved YOLOv3 to Programmable Logic (PL), which can be deployed on FPGA. The conversion process mainly consists of pruning, quantization and compilation. Experimental results showed that the algorithm had high efficiency, inspection accuracy reached 99.2%, processing speed reached 1.54 Frames per Second (FPS), and power consumption was only 10 W.",Micromachines,2022,10.3390/mi13071058,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
bff8a2d5ed940c76e1ca683111dacea356ab2388,https://www.semanticscholar.org/paper/bff8a2d5ed940c76e1ca683111dacea356ab2388,Securing Healthcare Data With Blockchain,"This chapter shows that blockchain has a lot of potential for revolutionizing the traditional healthcare industry. When attempting to completely integrate blockchain technology with existing EHR systems, however, a number of research and operational hurdles remain. The authors evaluated and discussed some of these issues in this chapter. After that, they discovered a variety of possible research topics, such as IoT, big data, machine learning, and edge computing. They offer a methodology for implementing blockchain technology in the healthcare industry for electronic health records (EHR). The goal of the proposed structure is to first integrate blockchain technology for EHR and then to enable safe storage of electronic data for users of the framework by setting access controls. They hope that this review will help us gain a better understanding of the development and deployment of future generation EHR systems that will benefit humankind.",Advances in Healthcare Information Systems and Administration,2022,10.4018/978-1-7998-9606-7.ch007,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
4e4310aa507659c88b0a58b15bc49f87747a6fc6,https://www.semanticscholar.org/paper/4e4310aa507659c88b0a58b15bc49f87747a6fc6,LOCAT: Low-Overhead Online Configuration Auto-Tuning of Spark SQL Applications,"Spark SQL has been widely deployed in industry but it is challenging to tune its performance. Recent studies try to employ machine learning (ML) to solve this problem, but suffer from two drawbacks. First, it takes a long time (high overhead) to collect training samples. Second, the optimal configuration for one input data size of the same application might not be optimal for others. To address these issues, we propose a novel Bayesian Optimization (BO) based approach named LOCAT to automatically tune the configurations of Spark SQL applications online. LOCAT innovates three techniques. The first technique, named QCSA, eliminates the configuration-insensitive queries by Query Configuration Sensitivity Analysis (QCSA) when collecting training samples. The second technique, dubbed DAGP, is a Datasize-Aware Gaussian Process (DAGP) which models the performance of an application as a distribution of functions of configuration parameters as well as input data size. The third technique, called IICP, Identifies Important Configuration Parameters (IICP) with respect to performance and only tunes the important ones. As such, LOCAT can tune the configurations of a Spark SQL application with low overhead and adapt to different input data sizes We employ Spark SQL applications from benchmark suites TPC-DS, TPC-H, and HiBench running on two significantly different clusters, a four-node ARM cluster and an eight-node x86 cluster, to evaluate LOCAT. The experimental results on the ARM cluster show that LOCAT accelerates the optimization procedures of the state-of-the-art approaches by at least 4.1× and up to 9.7×; moreover, LOCAT improves the application performance by at least 1.9× and up to 2.4×. On the x86 cluster, LOCAT shows similar results to those on the ARM cluster.",SIGMOD Conference,2022,10.1145/3514221.3526157,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
80b2d2596fbc7bbc945116295b2f229daf74cb44,https://www.semanticscholar.org/paper/80b2d2596fbc7bbc945116295b2f229daf74cb44,An in-hive soft sensor based on phase space features for Varroa infestation level estimation and treatment need detection,"Abstract. Bees are recognized as an indispensable link in the human food chain and general ecological system.
Numerous threats, from pesticides to parasites, endanger bees, enlarge the burden on hive keepers, and frequently lead to hive collapse.
The Varroa destructor mite is a key threat to bee keeping, and the monitoring of hive infestation levels is
of major concern for effective treatment. Continuous and unobtrusive monitoring of hive infestation levels along with other vital bee hive parameters is coveted, although there is currently no explicit sensor for this task. This problem is strikingly similar to issues such as
condition monitoring or Industry 4.0 tasks, and sensors and machine learning bear the promise of viable solutions (e.g., creating a soft sensor for the task).
In the context of our IndusBee4.0 project, following a bottom-up approach, a modular in-hive gas sensing system, denoted as BeE-Nose, based on common
metal-oxide gas sensors (in particular, the Sensirion SGP30 and the Bosch Sensortec BME680) was deployed for a substantial part of the 2020
bee season in a single colony for a single measurement campaign. The ground truth of the Varroa population size was determined by repeated conventional method application.
This paper is focused on application-specific invariant feature computation for daily hive activity characterization.
The results of both gas sensors for Varroa infestation level estimation (VILE) and automated treatment need detection (ATND), as a thresholded or two-class interpretation of VILE, in the order of up to 95 % are presented.
Future work strives to employ a richer sensor palette and evaluation approaches for several hives over a bee season.
",Journal of Sensors and Sensor Systems,2022,10.5194/jsss-11-29-2022,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
5f6584ceaabc7811238981870e2fde195760c883,https://www.semanticscholar.org/paper/5f6584ceaabc7811238981870e2fde195760c883,Real-time detection of uncalibrated sensors using Neural Networks,,Neural Comput. Appl.,2021,10.1007/s00521-021-06865-z,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
31b37b6f3ed31f4307000f4b0d8172f1bb9215d3,https://www.semanticscholar.org/paper/31b37b6f3ed31f4307000f4b0d8172f1bb9215d3,Online Monitoring of Inner Deposits in Crude Oil Pipelines,"
 The formation of deposits is a very common issue in oil and gas pipeline transportation systems. Such sediments, mainly wax and paraffine for crude oil, or hydrates and water for gas, progressively reduce the free cross-sectional area of the pipe, leading in some cases to the complete occlusion of the conduit. The overall result is a decrease in the transportation performance, with negative economic, environmental, and safety consequences. To prevent this issue, the amount of inner deposits must be continuously and accurately monitored, such that the corresponding cleaning procedures can be performed when necessary. Currently, the former operation is still dictated by best-practice rules pertaining to preventive or reactive approaches, yet the demand from the industry is for predictive solutions that can be deployed online for real-time monitoring applications. The paper moves toward this direction by presenting a machine learning methodology that leverages pressure measurements to perform online monitoring of the inner deposits in crude oil trunklines. The key point is that the attenuation of pressure transients within the fluid is dependent on the free cross-sectional area of the pipe. Pressure signals, collected from two or more distinct locations along a pipeline, can therefore be exploited to estimate and track in real time the presence and thickness of the deposits. Several statistical indicators, derived from the attenuation of such pressure transients between adjacent acquisition points, are fed to a data-driven regression algorithm that automatically outputs a numeric indicator representing the amount of inner pipe debris. The procedure is applied to the pressure measurements collected for one and a half years on discrete points at a relative distance of 40 and 60 km along an oil pipeline in Italy (100 km length, 16-in. inner diameter pipes). The availability of historical data prepipe and postpipe cleaning campaigns further enriches the proposed data-driven approach. Experimental results demonstrate that the proposed predictive monitoring strategy is capable of tracking the conditions of the entire conduit and of individual pipeline sections, thus determining which portion of the line is subject to the highest occlusion levels. In addition, our methodology allows for real-time acquisition and processing of data, thus enabling the opportunity for online monitoring. Prediction accuracy is assessed by evaluating the typical metrics used in the statistical analysis of regression problems.",SPE Production &amp; Operations,2022,10.2118/209825-pa,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
bab1ae565c3c180cb4bacf3ea7154ae101bc01e4,https://www.semanticscholar.org/paper/bab1ae565c3c180cb4bacf3ea7154ae101bc01e4,Building a generalisable ML pipeline at ING,"Advances in data science have caused an increase in the use of Artificial Intelligence (AI), specifically Machine Learning (ML), throughout various fields. Not only in research but in the industry as well, has ML been receiving increasing amounts of interest. Many companies rely on ML models to increase the efficiency of existing processes or offer new services and products. The industry, however, is facing several additional challenges compared to the academic context. One of those challenges is applying the Development Operations (DevOps) model to an ML application, also referred to as MLOps. This thesis sets out to find the specific challenges that practitioners encounter while operationalising ML models. To do so, we perform a single-case case study on an ML pipeline built by the Trade & Communication Surveillance team at the ING bank. This case study consists of conducting a set of interviews and performing a manual code inspection of the pipeline. The team faces challenges ranging from having insufficient time for operationalising each ML project individually to operating in the highly-regulated fintech context. Their pipeline is able to deploy a single ML model but it does not generalise well to other projects. We present the first version of an application that mitigates these challenges. The application is able to deploy ML models to the development environment at ING and can be operated by data scientists to reduce the effort of operationalising an ML model.",,2022,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
599ec5049fe2401f4ce42619b25f1aae2dfa4025,https://www.semanticscholar.org/paper/599ec5049fe2401f4ce42619b25f1aae2dfa4025,Implementing a Microservices Architecture for Predicting the Opinion of Twitter Users on COVID Vaccines,"A strong trend in the software industry is to merge the activities of deployment and operationalization through the DevOps approach, which in the case of artificial intelligence is called Machine Learning Operations (MLOps). We present here a microservices architecture containing the whole pipeline (frontend, backend, data predictions) hosted in Docker containers which exposes a model implemented for opinion prediction in Twitter on the COVID vaccines. This is the first description in the literature of implementing a microservice architecture using TorchServe, a library for serving Pytorch models.",MIE,2022,10.3233/SHTI220417,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
caf1eb17b91b0f1fb450703c9e81ecd6931924ca,https://www.semanticscholar.org/paper/caf1eb17b91b0f1fb450703c9e81ecd6931924ca,Accelerating the Uptake of Advanced Digital Technologies for Health and Safety Management within Construction: Small and Medium Enterprises (SMEs),"Health and safety problems are essential for the construction industry, and such problems are more pronounced in small and medium enterprises (SMEs) due to the lack of financial resources and skilled personnel. Researchers have explored the feasibility and viability of addressing such constraints using artificial intelligence-enhanced, low-cost sensor systems. Our previous studies have investigated both conventional machine learning and deep neural network models for recognizing workers' postures from low-cost wearable sensors and assessing the ergonomics risks for injury prevention. In the next steps for this research, we are investigating adoption drivers and diffusion barriers for the scaled deployment of AI-enhanced sensor networks and other emerging digital technologies for construction health and safety in a real-work setting. Although the COVID-19 pandemic has brought unprecedented challenges, it has also sped up the digital technology adoption. The discussion in this paper is directed at building on this momentum to advance the use of emerging digital technologies at construction SMEs. The authors conducted a systematic review of literature on digital technologies at construction SMEs and how COVID-19 affected the digital transformation at SMEs. After an initial screening of a total of 170 articles, the key publications based on the research questions were selected for a more in-depth analysis. It emerged that although construction SMEs have embraced the use of several digital technologies during the current pandemic, there is still a large digital divide between these companies and larger companies. The research discussed in this paper contributes to efforts directed at addressing this problem through the design and deployment of SME-centric digital technologies for construction health and safety. © 2021 Computing in Civil Engineering 2021 - Selected Papers from the ASCE International Conference on Computing in Civil Engineering 2021. All rights reserved.",Computing in Civil Engineering 2021,2022,10.1061/9780784483893.103,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
5a5fa1f3b8355cd9913e8ae3a15a583ee91532cf,https://www.semanticscholar.org/paper/5a5fa1f3b8355cd9913e8ae3a15a583ee91532cf,Silicon Photonics,,Computer,2022,10.1007/978-1-4419-9335-9_3,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
d8ffef877579dffb1d3de19741e58774f65cf8df,https://www.semanticscholar.org/paper/d8ffef877579dffb1d3de19741e58774f65cf8df,"The Pransky interview: Harry Kloor, PhD, PhD – CEO and Co-Founder, Beyond Imagination Inc.; scientist; entrepreneur; inventor; filmmaker","
Purpose
The following article is a “Q&A interview” conducted by Joanne Pransky of Industrial Robot Journal as a method to impart the combined technological, business and personal experience of a prominent, robotic industry PhD-turned successful innovator and entrepreneur regarding turning his lifelong dream into an invention and commercialized product. This paper aims to discuss these issues.


Design/methodology/approach
Harry Kloor is a successful serial entrepreneur, scientist, technologist, educator, policy advisor, author and Hollywood filmmaker. He is the CEO and co-founder of Beyond Imagination, a company that has developed a suite of exponential technology solutions that deploys artificial intelligence (AI), AR, robotics, machine learning and human–computer interaction technology to enhance and revolutionize the world’s workforce. The company early in 2021 completed BEOMNI 1.0, the world’s first fully functional humanoid robotic system with an AI evolving brain, enabling remote work at a high level of fidelity to be done from around the globe. Kloor describes how he transformed his childhood dream into his brainchild and tangible reality.


Findings
Kloor was born a groundbreaker who did not take no for an answer. He was born partially crippled with his legs facing backwards. The doctors said that he would spend his life in braces and would never be able to run. His parents told him not to let those ideas limit him and by the age of seven he ran for the first time and went on to become a martial arts master. Kloor’s childhood dream was to create ways to leave his body and inhabit a robotic body so that he could physically be free from his limited mobility. Kloor built his first computer at the age of seven and invented his first product at the age of eight. Kloor's inspiration to study science came largely from science fiction and his 20,000-plus collection of comic books. Knowing the nature of exponential growth, he spent the next 40 years building the expertise, relationships, networks and experience in all areas of exponential technology. Kloor obtained a BA from Southern Oregon State College, an MEd from Southern Oregon University and two simultaneous PhDs, one in chemistry and one in physics, from Purdue University. Kloor co-founded the company Universal Consultants, where he served as chief science consultant, providing guidance to clients in the development of new technological products, patents and policy positions. Kloor was the founder of Stem CC Inc. – a stem cell company that was sold in 2018 to Celularity, one of the world’s most cutting edge clinical-stage cell therapeutics company. Kloor is also the founder and president of Jupiter 9 Productions and is a credited film writer, director and producer. Since his graduation from Purdue University, he has written for Star Trek: Voyager and was the story editor for Gene Roddenberry’s Earth: Final Conflict, a series he co-created/developed. Kloor helped create Taiwan’s animation industry, bringing Quantum Quest: A Cassini Space Odyssey, the first big animation film that starred major Hollywood stars, to Taiwan. Kloor also sits on the board of Brain Mapping and Therapeutics Society and serves as their Chief Scientific Advisor and Educational Outreach Coordinator.


Originality/value
Kloor is a “creative consultant and universal problem solver, with an emphasis in technology and education.” Kloor has worked with Dr Peter Diamandis since the first class of the International Space University in 1988. Kloor was one of the five founding team members of XPRIZE serving as its CSO until 2005 and was one of the founders of the Rocket Racing League. He was on the founding team of Singularity University and taught at Singularity’s first summer program. In 2016 he created the $10m Avatar XPRIZE, and in 2018 he co-created the Carbon Extraction XPRIZE which obtained the largest incentive prize in history, a $100m, funded by Elon Musk and the Musk Foundation. Kloor is the only person in world history to earn two PhDs simultaneously in two distinct academic disciplines. In recognition of this achievement, he was named ABC World News’ Person of the Week in August 1994. Kloor has received numerous awards, including The Golden Axon Award from the Society for Brain Mapping & Therapeutics. He has recently created the Kloor Cycle, a four-stage experiential autonomous learning process within Beomni’s “AI Brain,” adapted from Kolb’s Learning Cycles.
",Industrial Robot: the international journal of robotics research and application,2022,10.1108/ir-06-2022-0148,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
bf2fc9a43cba9ac7e7ce4d872f3a08fd220cf65f,https://www.semanticscholar.org/paper/bf2fc9a43cba9ac7e7ce4d872f3a08fd220cf65f,Scientific AI in materials science: a path to a sustainable and scalable paradigm,"Recently there has been an ever-increasing trend in the use of machine learning (ML) and artificial intelligence (AI) methods by the materials science, condensed matter physics, and chemistry communities. This perspective article identifies key scientific, technical, and social opportunities that the materials community must prioritize to consistently develop and leverage Scientific AI (SciAI) to provide a credible path towards the advancement of current materials-limited technologies. Here we highlight the intersections of these opportunities with a series of proposed paths forward. The opportunities are roughly sorted from scientific/technical (e.g. development of robust, physically meaningful multiscale material representations) to social (e.g. promoting an AI-ready workforce). The proposed paths forward range from developing new infrastructure and capabilities to deploying them in industry and academia. We provide a brief introduction to AI in materials science and engineering, followed by detailed discussions of each of the opportunities and paths forward.",Mach. Learn. Sci. Technol.,2020,10.1088/2632-2153/ab9a20,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
3855da29f4f6a2abba13ebc7ab4da1c611a37ad4,https://www.semanticscholar.org/paper/3855da29f4f6a2abba13ebc7ab4da1c611a37ad4,Security And Privacy Issues In A Knowledge Management System Epub Download,"Summarizes the current state and upcoming trends within the area of fog computing Written by some of the leading experts in the field, Fog Computing: Theory and Practice focuses on the technological aspects of employing fog computing in various application domains, such as smart healthcare, industrial process control and improvement, smart cities, and virtual learning environments. In addition, the Machine-toMachine (M2M) communication methods for fog computing environments are covered in depth. Presented in two parts—Fog Computing Systems and Architectures, and Fog Computing Techniques and Application—this book covers such important topics as energy efficiency and Quality of Service (QoS) issues, reliability and fault tolerance, load balancing, and scheduling in fog computing systems. It also devotes special attention to emerging trends and the industry needs associated with utilizing the mobile edge computing, Internet of Things (IoT), resource and pricing estimation, and virtualization in the fog environments. Includes chapters on deep learning, mobile edge computing, smart grid, and intelligent transportation systems beyond the theoretical and foundational concepts Explores real-time traffic surveillance from video streams and interoperability of fog computing architectures Presents the latest research on data quality in the IoT, privacy, security, and trust issues in fog computing Fog Computing: Theory and Practice provides a platform for researchers, practitioners, and graduate students from computer science, computer engineering, and various other disciplines to gain a deep understanding of fog computing. How the enabling technologies in 5G as an integral or as a part can seamlessly fuel the IoT revolution is still very challenging. This book presents the state-of-the-art solutions to the theoretical and practical challenges stemming from the integration of 5G enabling technologies into IoTs in support of a smart 5G-enabled IoT paradigm, in terms of network design, operation, management, optimization, privacy and security, and applications. In particular, the technical focus covers a comprehensive understanding of 5G-enabled IoT architectures, converged access networks, privacy and security, and emerging applications of 5G-eabled IoT. This book constitutes the refereed proceedings of the International ECML/PKDD Workshop on Privacy and Security Issues in Data Mining and Machine Learning, PSDML 2010, held in Barcelona, Spain, in September 2010. The 11 revised full papers presented were carefully reviewed and selected from 21 submissions. The papers range from data privacy to security applications, focusing on detecting malicious behavior in computer systems. This timely book provides broad coverage of security and privacy issues in the macro and micro perspective. In macroperspective, the system and algorithm fundamentals of next-generation wireless networks are discussed. In micro-perspective, this book focuses on the key secure and privacy techniques in different emerging networks from the interconnection view of human and cyber-physical world. This book includes 7 chapters from prominent international researchers working in this subject area. This book serves as a useful reference for researchers, graduate students, and practitioners seeking solutions to wireless security and privacy related issues Recent advances in wireless communication technologies have enabled the large-scale deployment of next-generation wireless networks, and many other wireless applications are emerging. The next generation of mobile networks continues to transform the way people communicate and access information. As a matter of fact, next-generation emerging networks are exploiting their numerous applications in both military and civil fields. For most applications, it is important to guarantee high security of the deployed network in order to defend against attacks from adversaries, as well as the privacy intrusion. The key target in the development of next-generation wireless networks is to promote the integration of the",,2022,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
c3db9ce159bb2b4e5931aed6958df600dfd7005d,https://www.semanticscholar.org/paper/c3db9ce159bb2b4e5931aed6958df600dfd7005d,Enhancing Cognition for Digital Twins,"In the era of Industry 4.0, Digital Twins (DTs) pave the way for the creation of the Cognitive Factory. By virtualizing and twinning information stemming from the real and the digital world, it is now possible to connect all parts of the production process by having virtual copies of physical elements interacting with each other in the digital and physical realms. However, this alone does not imply cognition. Cognition requires modelling not only the physical characteristics but also the behavior of production elements and processes. The latter can be founded upon data-driven models produced via Data Analytics and Machine Learning techniques, giving rise to the so-called Cognitive (Digital) Twin. To further enable the Cognitive Factory, a novel concept, dubbed as Enhanced Cognitive Twin (ECT), is proposed in this paper as a way to introduce advanced cognitive capabilities to the DT artefact that enable supporting decisions, with the end goal to enable DTs to react to inner or outer stimuli. The Enhanced Cognitive Twin can be deployed at different hierarchical levels of the production process, i.e., at sensor-, machine-, process-, employee- or even factory-level, aggregated to allow both horizontal and vertical interplay. The ECT notion is proposed in the context of process industries, where cognition is particularly important due to the continuous, non-linear, and varied nature of the respective production processes.","2020 IEEE International Conference on Engineering, Technology and Innovation (ICE/ITMC)",2020,10.1109/ICE/ITMC49519.2020.9198492,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
97224dfaa9517c51e86250cd4443958d37c17220,https://www.semanticscholar.org/paper/97224dfaa9517c51e86250cd4443958d37c17220,Tools and Benchmarks for Automated Log Parsing,"Logs are imperative in the development and maintenance process of many software systems. They record detailed runtime information that allows developers and support engineers to monitor their systems and dissect anomalous behaviors and errors. The increasing scale and complexity of modern software systems, however, make the volume of logs explodes. In many cases, the traditional way of manual log inspection becomes impractical. Many recent studies, as well as industrial tools, resort to powerful text search and machine learning-based analytics solutions. Due to the unstructured nature of logs, a first crucial step is to parse log messages into structured data for subsequent analysis. In recent years, automated log parsing has been widely studied in both academia and industry, producing a series of log parsers by different techniques. To better understand the characteristics of these log parsers, in this paper, we present a comprehensive evaluation study on automated log parsing and further release the tools and benchmarks for easy reuse. More specifically, we evaluate 13 log parsers on a total of 16 log datasets spanning distributed systems, supercomputers, operating systems, mobile systems, server applications, and standalone software. We report the benchmarking results in terms of accuracy, robustness, and efficiency, which are of practical importance when deploying automated log parsing in production. We also share the success stories and lessons learned in an industrial application at Huawei. We believe that our work could serve as the basis and provide valuable guidance to future research and deployment of automated log parsing.",2019 IEEE/ACM 41st International Conference on Software Engineering: Software Engineering in Practice (ICSE-SEIP),2018,10.1109/ICSE-SEIP.2019.00021,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
8222cbd972f34652a5952ff9633709f886303c83,https://www.semanticscholar.org/paper/8222cbd972f34652a5952ff9633709f886303c83,MicroNets: Neural Network Architectures for Deploying TinyML Applications on Commodity Microcontrollers,"Executing machine learning workloads locally on resource constrained microcontrollers (MCUs) promises to drastically expand the application space of IoT. However, so-called TinyML presents severe technical challenges, as deep neural network inference demands a large compute and memory budget. To address this challenge, neural architecture search (NAS) promises to help design accurate ML models that meet the tight MCU memory, latency and energy constraints. A key component of NAS algorithms is their latency/energy model, i.e., the mapping from a given neural network architecture to its inference latency/energy on an MCU. In this paper, we observe an intriguing property of NAS search spaces for MCU model design: on average, model latency varies linearly with model operation (op) count under a uniform prior over models in the search space. Exploiting this insight, we employ differentiable NAS (DNAS) to search for models with low memory usage and low op count, where op count is treated as a viable proxy to latency. Experimental results validate our methodology, yielding our MicroNet models, which we deploy on MCUs using Tensorflow Lite Micro, a standard open-source NN inference runtime widely used in the TinyML community. MicroNets demonstrate state-of-the-art results for all three TinyMLperf industry-standard benchmark tasks: visual wake words, audio keyword spotting, and anomaly detection.",MLSys,2020,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
0a29101ca20edb01f403787844f455b317d34d62,https://www.semanticscholar.org/paper/0a29101ca20edb01f403787844f455b317d34d62,Taking ROCKET on an Efficiency Mission: Multivariate Time Series Classification with LightWaveS,"—Nowadays, with the rising number of sensors in sec- tors such as healthcare and industry, the problem of multivariate time series classiﬁcation (MTSC) is getting increasingly relevant and is a prime target for machine and deep learning approaches. Their expanding adoption in real-world environments is causing a shift in focus from the pursuit of ever-higher prediction accuracy with complex models towards practical, deployable solutions that balance accuracy and parameters such as prediction speed. An MTSC model that has attracted attention recently is ROCKET, based on random convolutional kernels, both because of its very fast training process and its state-of-the-art accuracy. However, the large number of features it utilizes may be detrimental to inference time. Examining its theoretical background and limitations enables us to address potential drawbacks and present LightWaveS: a framework for accurate MTSC, which is fast both during training and inference. Speciﬁcally, utilizing wavelet scattering transformation and distributed feature selection, we manage to create a solution that employs just 2.5% of the ROCKET features, while achieving accuracy comparable to recent MTSC models. LightWaveS also scales well across multiple compute nodes and with the number of input channels during training. In addition, it can signiﬁcantly reduce the input size and provide insight to an MTSC problem by keeping only the most useful channels. We present three versions of our algorithm and their results on distributed training time and scalability, accuracy, and inference speedup. We show that we achieve speedup ranging from 9x to 53x compared to ROCKET during inference on an edge device, on datasets with comparable accuracy.",ArXiv,2022,10.48550/arXiv.2204.01379,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
cdb66a2f6de4f002b4a871ade24e6e61d321e566,https://www.semanticscholar.org/paper/cdb66a2f6de4f002b4a871ade24e6e61d321e566,Detecting Drill Failure in the Small Short-sound Drill Dataset,"Monitoring the conditions of machines is vital in the manufacturing industry. Early detection of faulty components in machines for stopping and repairing the failed components can minimize the downtime of the machine. This article presents an approach to detect the failure occurring in drill machines based on drill sounds from Valmet AB. The drill dataset includes three classes: anomalous sounds, normal sounds, and irrelevant sounds, which are also labeled as “Broken”, “Normal”, and “Other”, respectively. Detecting drill failure effectively remains a challenge due to the following reasons. The waveform of drill sound is complex and short for detection. Additionally, in realistic soundscapes, there are sounds and noise in the context at the same time. Moreover, the balanced dataset is small to apply state-of-the-art deep learning techniques. To overcome these aforementioned difficulties, we augmented sounds to increase the number of sounds in the dataset. We then proposed a convolutional neural network (CNN) combined with a long short-term memory (LSTM) to extract features from log-Mel spectrograms and learn global highlevel feature representation for the classification of three classes. A leaky rectified linear unit (Leaky ReLU) was utilized as the activation function for our proposed CNN instead of the rectified linear unit (ReLU). Moreover, we deployed an attention mechanism at the frame level after the LSTM layer to learn long-term global feature representations. As a result, the proposed method reached an overall accuracy of 92.35% for the drill failure detection system.",ArXiv,2021,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
475b343d4f42c8048aa7c98f8a6c9230298c6afa,https://www.semanticscholar.org/paper/475b343d4f42c8048aa7c98f8a6c9230298c6afa,Internet of Things (IoT) Security and Forensics: Challenges and Opportunities,"Internet of Things (IoT) devices are increasingly found around, in, and on us (e.g., smart home and other consumer devices) in applications ranging from environmental monitoring to healthcare (e.g., healthcare or medical IoT devices) to surveillance to industry (e.g., industrial IoT IIoT), and battlefields / military (e.g., Internet of Battlefield / Military Things). Such devices are also generally capable of capturing a broad range of information, including digital artifacts that can be used for cyber threat intelligence and inform security mitigation strategy formulation. There are, however, a number of challenges associated with designing IoT cyber security and threat intelligence solutions. In addition to the technical challenges, there are also associated legal and policy challenges that need to be considered in the design and deployment of such solutions in practice. For example, how do we use machine/deep learning to facilitate detection of real-time attacks against IoT devices and systems, and how can we automatically identify and collect digital evidence in a forensically sound manner which can be subsequently used for cyber threat intelligence? In the event that the attackers use sophisticated tools to obfuscate their trails, can we design machine/deep learning techniques to unobfuscate and/or identify and exploit vulnerabilities to get access to digital evidence? What are the potential legal implications and challenges? Can we also design explainable AI techniques to facilitate the explanation and inclusion of such digital evidence and cyber threat intelligence in court proceedings or presentations to C-level or boards in organizations? Based on these discussed challenges, we will identify potential opportunities for stakeholders in academia (e.g., students and researchers), industry and government.",CPSIOTSEC@CCS,2021,10.1145/3462633.3484691,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
f34300f0cd9a8c1c2cf7add559d4e453f37e95af,https://www.semanticscholar.org/paper/f34300f0cd9a8c1c2cf7add559d4e453f37e95af,Single-cell dispensing and ‘real-time’ cell classification using convolutional neural networks for higher efficiency in single-cell cloning,,Scientific Reports,2020,10.1038/s41598-020-57900-3,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
320e3a66dbc51a1331f65b5ac42b937a39d5e051,https://www.semanticscholar.org/paper/320e3a66dbc51a1331f65b5ac42b937a39d5e051,Physical Activity Recognition With Statistical-Deep Fusion Model Using Multiple Sensory Data for Smart Health,"Nowadays, enhancing the living standard with smart healthcare via the Internet of Things is one of the most critical goals of smart cities, in which artificial intelligence plays as the core technology. Many smart services, deployed according to wearable sensor-based physical activity recognition, have been able to early detect unhealthy daily behaviors and further medical risks. Numerous approaches have studied shallow handcrafted features coupled with traditional machine learning (ML) techniques, which find it difficult to model real-world activities. In this work, by revealing deep features from deep convolutional neural networks (DCNNs) in fusion with conventional handcrafted features, we learn an intermediate fusion framework of human activity recognition (HAR). According to transforming the raw signal value to pixel intensity value, segmentation data acquired from a multisensor system are encoded to an activity image for deep model learning. Formulated by several novel residual triple convolutional blocks, the proposed DCNN allows extracting multiscale spatiotemporal signal-level and sensor-level correlations simultaneously from the activity image. In the fusion model, the hybrid feature merged from the handcrafted and deep features is learned by a multiclass support vector machine (SVM) classifier. Based on several experiments of performance evaluation, our fusion approach for activity recognition has achieved the accuracy over 96.0% on three public benchmark data sets, including Daily and Sport Activities, Daily Life Activities, and RealWorld. Furthermore, the method outperforms several state-of-the-art HAR approaches and demonstrates the superiority of the proposed intermediate fusion model in multisensor systems.",IEEE Internet of Things Journal,2021,10.1109/JIOT.2020.3013272,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
79f2626046fdc56edfaca840874e355cac734b9a,https://www.semanticscholar.org/paper/79f2626046fdc56edfaca840874e355cac734b9a,Ad click prediction: a view from the trenches,"Predicting ad click-through rates (CTR) is a massive-scale learning problem that is central to the multi-billion dollar online advertising industry. We present a selection of case studies and topics drawn from recent experiments in the setting of a deployed CTR prediction system. These include improvements in the context of traditional supervised learning based on an FTRL-Proximal online learning algorithm (which has excellent sparsity and convergence properties) and the use of per-coordinate learning rates. We also explore some of the challenges that arise in a real-world system that may appear at first to be outside the domain of traditional machine learning research. These include useful tricks for memory savings, methods for assessing and visualizing performance, practical methods for providing confidence estimates for predicted probabilities, calibration methods, and methods for automated management of features. Finally, we also detail several directions that did not turn out to be beneficial for us, despite promising results elsewhere in the literature. The goal of this paper is to highlight the close relationship between theoretical advances and practical engineering in this industrial setting, and to show the depth of challenges that appear when applying traditional machine learning methods in a complex dynamic system.",KDD,2013,10.1145/2487575.2488200,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
64dfb21fc22523f2fbb1128cc9bd32a904df5e92,https://www.semanticscholar.org/paper/64dfb21fc22523f2fbb1128cc9bd32a904df5e92,Accelerating the discovery of materials for clean energy in the era of smart automation,,Nature Reviews Materials,2018,10.1038/s41578-018-0005-z,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
53c7ba5e970f1fa628d6edda01b3e12751d0a85d,https://www.semanticscholar.org/paper/53c7ba5e970f1fa628d6edda01b3e12751d0a85d,Improving the Accuracy and Transparency of Underwriting with AI to Transform the Life Insurance Industry,"Life insurance provides trillions of dollars of financial security for hundreds of millions of individuals and fami­lies worldwide. To simultaneously offer affordable products while managing this financial ecosystem, life-insurance companies use an underwriting process to assess the mortality risk posed by individual applicants. Traditional underwriting is largely based on examining an applicant’s health and behavioral profile. This manual process is incompatible with expectations of a rapid customer experience through digital capabilities. Fortunately, the availability of large historical data sets and the emergence of new data sources provide an unprecedented opportunity for artificial intelligence to transform under­writing in the life-insurance industry with standard measures of mortality risk. We combined one of the largest application data sets in the industry with a responsible artificial intelligence framework to develop a mortality model and life score. We describe how the life score serves as the primary risk-driving engine of deployed algorithmic underwriting systems and demonstrate its high level of accuracy, yielding a nine-percent reduction in claims within the healthiest pool of applicants. Additionally, we argue that, by embracing transparency, the industry can build consumer trust and respond to a dynamic regulatory environment focused on algorithmic decision-making. We present a consumer-facing tool that uses a state-of-the-art method for interpretable machine learning to offer transparency into the life score.",AI Mag.,2020,10.1609/aimag.v41i3.5320,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
b53571c7448822ae2fc719c78d13fc1c451b3996,https://www.semanticscholar.org/paper/b53571c7448822ae2fc719c78d13fc1c451b3996,Detecting Lateral Movement in Enterprise Computer Networks with Unsupervised Graph AI,"In this paper we present a technique for detecting lateral movement of Advanced Persistent Threats inside enterpriselevel computer networks using unsupervised graph learning. Our detection technique utilizes information derived from industry standard logging practices, rendering it immediately deployable to real-world enterprise networks. Importantly, this technique is fully unsupervised, not requiring any labeled training data, making it highly generalizable to different environments. The approach consists of two core components: an authentication graph, and an unsupervised graph-based machine learning pipeline which learns latent representations of the authenticating entities, and subsequently performs anomaly detection by identifying low-probability authentication events via a learned logistic regression link predictor. We apply this technique to authentication data derived from two contrasting data sources: a small-scale simulated environment, and a large-scale real-world environment. We are able to detect malicious authentication events associated with lateral movement with a true positive rate of 85% and false positive rate of 0.9%, compared to 72% and 4.4% by traditional rule-based heuristics and non-graph anomaly detection algorithms. In addition, we have designed several filters to further reduce the false positive rate by nearly 40%, while reducing true positives by less than 1%.",RAID,2020,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
15ae651f2a8da18405dc1dc6e75a608234b23b64,https://www.semanticscholar.org/paper/15ae651f2a8da18405dc1dc6e75a608234b23b64,Human machine synergies in intra-logistics: Creating a hybrid network for research and technologies,"The purpose of the article is to outline the futuristic vision of Industry 4.0 in intra-logistics by creating a hybrid network for research and technologies thereby providing a detailed account on the research centre, available technologies and their possibilities for collaboration. Scientific challenges in the field of Industry 4.0 and intra-logistics are identified due to the new form of interaction between humans and machines. This kind of collaboration provides new possibilities of materials handling that can be developed with the support of real-time motion data tracking and virtual reality systems. These services will be provided by a new research centre for flexible human-machine cooperation networks in Dortmund. By the use of various reference and experiment systems various real-time scenarios can be emulated including digital twin simulation concepts. Big data emerges as an important paradigm in this research project where all systems are made flexible in terms of networking for all the systems to consume the data produced and also to combine all the data to arrive at new insights using concepts from machine learning and deep learning networks. This leads to the challenge of finding a common syntax for inter-operating systems. This paper describes the design and deployment strategies of research centre with the possibilities and the design insights for a futuristic Industry 4.0 material handling facility.",2017 Federated Conference on Computer Science and Information Systems (FedCSIS),2017,10.15439/2017F253,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
0fba4314e5cd72dee9144b804c2231d61ae65bb0,https://www.semanticscholar.org/paper/0fba4314e5cd72dee9144b804c2231d61ae65bb0,DEEP LEARNING ALGORITHM IMPLEMENTATION FOR SHIP DETECTION IN SPOT SATELLITE IMAGES,"Marine industry is a large industry especially in the economy sector. Not limited to commercial, this industry also includes naval sector and the small and medium industry of fisheries all over the world. The huge development throughout the industry has also develop many kinds of unlawful act such as piracy and illegal cargo transportation. This has become the call for action for the officials of the sovereignty area to monitor the activities to control the situation and prevent them from become an epidemic that effects the whole industry. In this study, we propose to implement a deep-learning approach for detection of ships on satellite images in various conditions. The deep-learning algorithm to be deployed is Faster R-CNN and to be implemented using MATLAB. The project is carried out with the objective to implement the algorithm on SPOT satellite images that can accurately localize the region of interest (ROI) of the ship. The implementation of the algorithm consists of three stages which are pre-processing, network training and accuracy evaluation. The output of this project will be the localization of ships within the image with confidence scores of the prediction. Based on the results obtained, the deployment of the Faster R-CNN algorithm on ship class objects from SPOT satellite images has achieved a noteworthy performance despite the limitations in the amount of training dataset and specifications of the machines used. We can conclude that the project was able to achieve its objectives within the stipulated timeframe.",,2019,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
76f8f6dc3c82139d02111ec0c5fdc89f994b927d,https://www.semanticscholar.org/paper/76f8f6dc3c82139d02111ec0c5fdc89f994b927d,Hybrid neurofuzzy wind power forecast and wind turbine location for embedded generation,"Wind energy uptake in South Africa is significantly increasing both at the micro‐ and macro‐level and the possibility of embedded generation cannot be undermined considering the state of electricity supply in the country. This study identifies a wind hotspot site in the Eastern Cape province, performs an in silico deployment of three utility‐scale wind turbines of 60 m hub height each from different manufacturers, develops machine learning models to forecast very short‐term power production of the three wind turbine generators (WTG) and investigates the feasibility of embedded generation for a potential livestock industry in the area. Windographer software was used to characterize and simulate the net output power from these turbines using the wind speed of the potential site. Two hybrid models of adaptive neurofuzzy inference system (ANFIS) comprising genetic algorithm and particle swarm optimization (PSO) each for a turbine were developed to forecast very short‐term power output. The feasibility of embedded generation for typical medium‐scale agricultural industry was investigated using a weighted Weber facility location model. The analytical hierarchical process (AHP) was used for weight determination. From our findings, the WTG‐1 was selected based on its error performance metrics (root mean square error of 0.180, mean absolute SD of 0.091 and coefficient of determination of 0.914 and CT = 702.3 seconds) in the optimal model (PSO‐ANFIS). Criteria were ranked based on their order of significance to the agricultural industry as proximity to water supply, labour availability, power supply and road network. Also, as a proof of concept, the optimal location of the industrial facility relative to other criteria was X = 19.24 m, Y = 47.11 m. This study reveals the significance of resource forecasting and feasibility of embedded generation, thus improving the quality of preliminary resource assessment and facility location among site developers.",International Journal of Energy Research,2020,10.1002/er.5620,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
8294698a449b6f701aaf6d2d8f8af03ce948392b,https://www.semanticscholar.org/paper/8294698a449b6f701aaf6d2d8f8af03ce948392b,NSML: Meet the MLaaS platform with a real-world case study,"The boom of deep learning induced many industries and academies to introduce machine learning based approaches into their concern, competitively. However, existing machine learning frameworks are limited to sufficiently fulfill the collaboration and management for both data and models. We proposed NSML, a machine learning as a service (MLaaS) platform, to meet these demands. NSML helps machine learning work be easily launched on a NSML cluster and provides a collaborative environment which can afford development at enterprise scale. Finally, NSML users can deploy their own commercial services with NSML cluster. In addition, NSML furnishes convenient visualization tools which assist the users in analyzing their work. To verify the usefulness and accessibility of NSML, we performed some experiments with common examples. Furthermore, we examined the collaborative advantages of NSML through three competitions with real-world use cases.",ArXiv,2018,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
aaa5bfbeae4086956d9213fd7ce802de138f22cf,https://www.semanticscholar.org/paper/aaa5bfbeae4086956d9213fd7ce802de138f22cf,Μελέτη ναυτικών ατυχημάτων και πρόβλεψη δαπανών μέσω αλγορίθμων μηχανικής μάθησης,"Like all business entities, shipping companies are called upon to act for profitability, which is how well a company utilizes its resources in the purpose to generate profit and shareholder value. The onboard human resources are the most essential factor that ensures a smooth and planned course of a business voyage, which is the main activity of a shipping company. Due to the complex environment and unforeseen circumstances that characterize a voyage, the shipping company is constantly on the alert and must be ready to manage, the new data and the new situation. Sudden accidents that happen on merchant ships are part of the unforeseen and unexpected events and can have a direct impact on the ship's course as well as the overall performance of the voyage. The main goal of this master thesis is to study the theoretical and experimental algorithms of supervised machine learning using the R programmatic environment. The final goal is to develop and deploy valuable forecasting models that predict the importance of accidents occurring during a voyage of a merchant ship, which is a key feature of predicting the required total treatment cost. In general, prediction is one of the core points of interest in machine learning field with a wide range of research. The application of the forecast is carried out in various scientific majors such as medicine, meteorology, biology as well as the industry, such as the automobile industry, insurance companies, banks, etc. with various practical applications. In the shipping industry, the use of forecasting models is also widespread, but there is no prior application to the subject 
of the present master thesis. Primarily we will study the behavior and the performance of the applied machine learning algorithms and then enhance its performance through optimization techniques. Finally, we will compare the results and extract the analogous results.",,2020,10.26267/UNIPI_DIONE/389,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
431a95eaf0c8920659b9fa8f12adc2d3c1098552,https://www.semanticscholar.org/paper/431a95eaf0c8920659b9fa8f12adc2d3c1098552,CableMon: Improving the Reliability of Cable Broadband Networks via Proactive Network Maintenance,"Cable broadband networks are one of the few “last-mile” broadband technologies widely available in the U.S. Unfortunately, they have poor reliability after decades of deployment. Cable industry proposed a framework called Proactive Network Maintenance (PNM) to diagnose the cable networks. However, there is little public knowledge or systematic study on how to use these data to detect and localize cable network problems. Existing tools in the public domain have prohibitive high false-positive rates. In this paper, we propose CableMon, the first publicdomain system that applies machine learning techniques to PNM data to improve the reliability of cable broadband networks. CableMon uses statistical models to generate features from time series data and uses customer trouble tickets as hints to infer abnormal thresholds for these generated features. We use eight-month of PNM data and customer trouble tickets from an ISP to evaluate CableMon’s performance. Our results show that 81.9% of the abnormal events detected by CableMon overlap with at least one customer trouble ticket. This ticket prediction accuracy is four times higher than that of the existing public-domain tools used by ISPs. The tickets predicted by CableMon constitute 23.0% of the total networkrelated trouble tickets, suggesting that if an ISP deploys CableMon and proactively repairs the faults detected by CableMon, it can preempt those customer calls. Our current results, while still not mature, can already tangibly reduce an ISP’s operational expenses and improve customers’ quality of experience. We expect future work can further improve these results.",NSDI,2020,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
7fb365d6a956c0dc1d992dd3ba97704683e6cd65,https://www.semanticscholar.org/paper/7fb365d6a956c0dc1d992dd3ba97704683e6cd65,Deep Learning Algorithms for Detection of Lymph Node Metastases From Breast Cancer: Helping Artificial Intelligence Be Seen.,"Artificial intelligence (AI), the theory and development of computer systems able to perform tasks that normally require human intelligence, is creeping into almost every facet of modern life. Familiar examples include computer chess games, speech recognition, intelligent routing in content delivery networks, and autonomous driving cars. In the financial sector, AI is routinely used for fraud detection, algorithmic trading, and chatbots (ie, computer programs that appear to conduct conversations via auditory or textual methods, such as with online virtual assistants). Health care has been slower to adopt AI but the pace of implementation is accelerating at an impressive rate. In 2014, the acquisition of AI startups in health care was about $600 million; in 2021, it is anticipated to be $6.6 billion or a 40% compound annual growth rate.1 One reason health care is ripe for AI is “big data”: the health care industry has rich data sets that are ideal for AI given the requirement for large test sets of data with which the computer can “learn.” Most computer modeling enhancements in health care, particularly in the image analysis field, have focused on feature engineering, essentially asking a computer to evaluate explicit features specified by experts. This permits the algorithms to detect abnormalities or predict specified lesions. In contrast, deep learning is a form of AI that includes machine learning techniques that perform iterative optimization strategies that are based on pixel-by-pixel evaluation of the data from images.2 The promise of AI in health care is the delivery of improved quality and safety of care and the potential to democratize expertise. For example, in a study by Esteva et al,3 the authors compared the ability of a deep convolutional neural network (CNN) to discriminate the most common skin cancers including malignant melanoma. They compared and demonstrated at least equivalence in the performance of their algorithm against 21 board-certified dermatologists in evaluating biopsy-proven clinical images. In this example, AI was used to discriminate whether skin lesions were malignant. The authors suggested that mobile devices, like smartphones, could be deployed with similar algorithms, permitting potentially low-cost universal access to vital diagnostic care anywhere in the world. In another study, Gulshan et al4 applied a deep CNN approach to a test set of more than 128 000 retinal fundus images from adult patients with diabetes to identify referable diabetic retinopathy. The algorithm developed had very high sensitivity and specificity for detecting referable diabetic retinopathy and macular edema.4 This study established a clear path toward use of AI not to replace physicians, but rather to perform simple, cost-effective, and widely available examinations and analyses that could help identify at-risk patients who require referral for specialty care while reassuring other patients that potential retinal manifestations of their diabetes are not present or are stable. Radiology, having converted to digital images more than 25 years ago, is well-positioned to deploy AI for diagnostics. Several studies have shown considerable opportunity to support radiologists in evaluating a variety of scan types including mammography for breast lesions, computed tomographic scans for pulmonary nodules and infections, and magnetic resonance images for brain tumors including the molecular classification of brain tumors.5-9 In contrast to radiology, pathology has been late to adopt digital imaging and thus computer-assisted diagnostic technologies. In part, this is the result of practical and financial obstacles. With conversion to digital images, radiology eliminated film, chemicals, developers, and storage of the films. Radiology departments also solved problems related to loss of films and transport of films to where they are needed, for example, in operating rooms, emergency departments, and intensive care units. Unforeseen at the time, although anticipated by some, was the inherent value within these images for greater learning using computers to improve the quality, safety, and efficiency of radiologists. Many, if not most, of the practical benefits realized by radiology would not be achieved with pathology digitization. An anatomic pathology workflow that includes digital pathology will not reduce or remove the need to produce and ultimately store glass slides of pathology specimens. Instead of any reductions, digital pathology will require additional workflows, personnel, equipment, and importantly storage of data (it is estimated that digital pathology images are at least 10 times larger files than radiology images), all on top of an already financially and operationally stressed health care system. Certainly, the adoption of digital pathology will bring some advantages, particularly in areas such as rapid teleconsultations with experts and in quality and safety. Nonetheless, Author Video Interview and JAMA Report Video",JAMA,2017,10.1001/jama.2017.14580,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
98ae4b12ab518b5eb4d36e4ec40fca5d5167e3d3,https://www.semanticscholar.org/paper/98ae4b12ab518b5eb4d36e4ec40fca5d5167e3d3,A Comprehensive Review on the Emerging IoT-Cloud based Technologies for Smart Healthcare,"Internet of Things (IoT) has redefined the operation of next-generation technologies by offering an intelligent framework. Cloud and IoT based system has been widely used in numerous applications for providing automated solutions and services. One of the areas in which the IoT and cloud have been widely used in the healthcare industry. To strengthen the healthcare system many initiatives are undertaken using advanced technologies such as IoT and cloud. So in this paper, a comprehensive review of the different IoT health model that are available are presented. The paper focuses on the various deployment strategies for building a cloud and IoT based system for smart home and smart hospital environment for handling different health-related issues. The paper highlights the various benefits that the IoT offers and also presents the challenges that are most prevalent in realizing the full automation of the healthcare system. The paper also discusses the integration of machine learning techniques for processing health data in the cloud to provide quality healthcare and modernize the healthcare system.",2020 6th International Conference on Advanced Computing and Communication Systems (ICACCS),2020,10.1109/ICACCS48705.2020.9074457,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
7317d046b4d3315e639b3690c375af619f99d5f1,https://www.semanticscholar.org/paper/7317d046b4d3315e639b3690c375af619f99d5f1,Security Analysis of Networked 3D Printers and their Deployments (CMU-CyLab-20-001),"Networked 3D printers are an emerging trend, enabling agile manufacturing. However, they are simultaneously increasing the security threats to manufacturing by creating new ways for attackers to cause physical hazards, steal proprietary data, create defective parts, or halt operations. Prior work has given limited attention to the security implications of adding these machines to a network. In this work, we present C3PO, an open-source network security analysis toolfor systematically identifying security threats to networked 3D printers. C3PO’s design is guided by industry standards and best practices. It identifies potential vulnerabilities in data transfer, the printing application, availability, and exposed network services. Furthermore, C3PO analyzes the security implications of a 3D printer’s network deployment, such as an attacker compromising a camera to modify printing instructions “on-the-wire.” We use C3PO to analyze 13 networked 3D printers and 5 realworld manufacturing network deployments. We identified 33 network security trends in networked 3D printers such as a susceptibility to low-rate denial of service attacks (all 13), transmitting unencrypted data (12/13), and being deployed on publicly accessible networks (2/5). We leverage these findings to provide recommendations on securing networked 3D printers and their deployments.",,2021,10.1184/R1/16709635.V1,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
ddc364373ffc205a21cf39d7192bc5c230b364ba,https://www.semanticscholar.org/paper/ddc364373ffc205a21cf39d7192bc5c230b364ba,Image-based wheat grain classification using convolutional neural network,,Multim. Tools Appl.,2021,10.1007/S11042-020-10174-3,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
f6dab84c2c00ab92d8ee9d9359d7e530512114f9,https://www.semanticscholar.org/paper/f6dab84c2c00ab92d8ee9d9359d7e530512114f9,"Finance Big Data: Management, Analysis, and Applications","Big Data is an emerging paradigm in almost all industries. Finance big data (FBD) is becoming one of the most promising areas of management and governance in the financial sector. It is significantly changing business models in financial companies. Many researchers argue that Big Data is fueling the transformation of finance and business at-large in the ways that we cannot as yet assess. A new research area is evolving to study quantitative models and econometric approaches for financial studies that can bridge the gap between empirical finance research and data science. In this fascinating area, experts and scientists can propose novel finance business models by using the Big Data methods, present sophisticated methods for risk control with machine learning tools, provide visualization tools for financial markets analysis, create new finance sentiment indexes by mining public feelings from the massive textual data from social networks, and deploy the information-based tools in other creative ways. Due to the 4V characteristics of Big Data—volume (large data scale), velocity (real-time data streaming), variety (different data formats), and veracity (data uncertainty)—a long list of challenges for FBD management, analytics, and applications exists. These challenges include (1) to organize and manage FBD in effective and efficient ways; (2) to find novel business models from FBD analytics; (3) to handle traditional finance issues like high-frequency trading, sentiments, credit risk, financial analysis, risk management and regulation, and others, in creative Big Data–driven ways; (4) to integrate the variety of heterogeneous data from different sources; and (5) to ensure the security and safety of finance systems and to protect the individual privacy in view of the availability of Big Data. To meet these challenges, we need fundamental research on both data analytics technology and finance business. This special issue, “Finance Big Data: Management, Analysis, and Applications,” of International Journal of Electronic Commerce, is motivated by the need to meet the challenges of the fast development of finance big data. The papers brought together in this special issue highlight research efforts focused on the development of methods, tools, and techniques for the handling of various aspects of FBD from academia and industries. Viktor Manahov and Hanxiong Zhang, in “Forecasting Financial Markets Using High-Frequency Trading Data: Examination with Strongly Typed Genetic Programming,” develop an artificial futures market populated with high-frequency (HF) traders and institutional traders using Strongly Typed Genetic Programming trading algorithm. The authors simulate real-life futures trading at the millisecond time frame by applying Strongly Typed",Int. J. Electron. Commer.,2019,10.1080/10864415.2018.1512270,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
ecc6d11de4afa98687fb99bdd04b2fcda72805bd,https://www.semanticscholar.org/paper/ecc6d11de4afa98687fb99bdd04b2fcda72805bd,UFO: Unified Feature Optimization,"This paper proposes a novel Unified Feature Optimization (UFO) paradigm for training and deploying deep models under realworld and large-scale scenarios, which requires a collection of multiple AI functions. UFO aims to benefit each single task with a large-scale pretraining on all tasks. Compared with existing foundation models, UFO has two points of emphasis, i.e., relatively smaller model size and NO adaptation cost: 1) UFO squeezes a wide range of tasks into a moderatesized unified model in a multi-task learning manner and further trims the model size when transferred to down-stream tasks. 2) UFO does not emphasize transfer to novel tasks. Instead, it aims to make the trimmed model dedicated for one or more already-seen task. To this end, it directly selects partial modules in the unified model, requiring completely NO adaptation cost. With these two characteristics, UFO provides great convenience for flexible deployment, while maintaining the benefits of large-scale pretraining. A key merit of UFO is that the trimming process not only reduces the model size and inference consumption, but also even improves the accuracy on certain tasks. Specifically, UFO considers the multi-task training and brings a two-fold impact on the unified model: some closely-related tasks have mutual benefits, while some tasks have conflicts against each other. UFO manages to reduce the conflicts and preserve the mutual benefits through a novel Network Architecture Search (NAS) method. Experiments on a wide range of deep representation learning tasks (i.e., face recognition, person re-identification, vehicle re-identification and product retrieval) show that the model trimmed from UFO achieves higher accuracy than its single-task-trained counterpart and yet has smaller model size, validating the concept of UFO. Besides, UFO also supported the release of 17 billion parameters computer vision (CV) foundation model which is the largest CV model in the industry.",ArXiv,2022,10.48550/arXiv.2207.10341,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
0c700703c9aac049f60f288a6c399a7ee2ef8f1c,https://www.semanticscholar.org/paper/0c700703c9aac049f60f288a6c399a7ee2ef8f1c,On Reverse Engineering Neural Network Implementation on GPU,,IACR Cryptol. ePrint Arch.,2021,10.1007/978-3-030-81645-2_7,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
452f2cfd54e771742a4b39299615d2f41fa2caf2,https://www.semanticscholar.org/paper/452f2cfd54e771742a4b39299615d2f41fa2caf2,Galactic Air Improves Airline Ancillary Revenues with Dynamic Personalized Pricing,"Ancillaries are a rapidly growing source of revenue for airlines, yet their prices are currently statically determined using rules-of-thumb and are matched only to the average customer, or to customer groups. Offering ancillaries at dynamic and personalized prices based on flight characteristics and customer needs could greatly improve airline revenue and customer satisfaction. Through a startup (Deepair) that builds and deploys novel machine learning techniques to introduce such dynamically priced ancillaries to airlines, we partnered with a major European airline, Galactic Air, to build models and algorithms for improved pricing. These algorithms recommend dynamic personalized ancillary prices for a stream of features (called context) relating to each shopping session. Our recommended prices are restricted to be lower than the human-curated prices for each customer group. We designed and compared multiple machine learning models and deployed the best performing ones live on the airline's booking system in an online A/B testing framework. Over a six-month live implementation period, our dynamic pricing system increased ancillary revenue per offer by 25% and conversion rate by 15% compared to the industry standard of human-curated rule-based prices.",,2021,10.2139/SSRN.3836941,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
bc547c278ae9b87c2cf4770f2883e37c48e30c62,https://www.semanticscholar.org/paper/bc547c278ae9b87c2cf4770f2883e37c48e30c62,Improving the Accuracy and Transparency of Underwriting with Artificial Intelligence to Transform the Life-Insurance Industry,"Copyright © 2020, Association for the Advancement of Artificial Intelligence. All rights reserved. ISSN 0738-4602 78 AI MAGAZINE Life insurance is a critical financial tool for millions of households, providing security to families by reducing the financial impact of an untimely death. In the United States alone, life-insurance companies collectively manage trillions of dollars of protection while annually disbursing billions of dollars to beneficiaries; according to the American Council of Life Insurers, at the end of 2018, there was nearly $12.1 trillion of active coverage for individuals and $57 billion in payments to their beneficiaries.1 To support this large-scale financial ecosystem while simultaneously offering affordable prices, insurers must estimate the mortality risk of individual life-insurance applicants through an underwriting process. The accuracy of this underwriting ultimately drives the long-term stability of the life-insurance industry because the collective sum of incoming premiums, which are fixed post-underwriting, must be sufficient to offset future payouts from guaranteed death benefits. Unlike most types of insurance that are renewed and reassessed annually (such as property and health), nearly all life-insurance policies are one-time, long-duration contractual agreements. Thus, the veracity and completeness of health and behavioral data used for mortality-risk assessment is paramount. For the past few decades, life underwriting has been guided by manual review and point-based systems that predominately consider factors independently. Consequently, traditional underwriting limits the degree to which insurers can accurately estimate risk from data and achieve optimal price efficiency of products.  Life insurance provides trillions of dollars of financial security for hundreds of millions of individuals and fami lies worldwide. To simultaneously offer affordable products while managing this financial ecosystem, life-insurance companies use an underwriting process to assess the mortality risk posed by individual applicants. Traditional underwriting is largely based on examining an applicant’s health and behavioral profile. This manual process is incompatible with expectations of a rapid customer experience through digital capabilities. Fortunately, the availability of large historical data sets and the emergence of new data sources provide an unprecedented opportunity for artificial intelligence to transform underwriting in the life-insurance industry with standard measures of mortality risk. We combined one of the largest application data sets in the industry with a responsible artificial intelligence framework to develop a mortality model and life score. We describe how the life score serves as the primary risk-driving engine of deployed algorithmic underwriting systems and demonstrate its high level of accuracy, yielding a nine-percent reduction in claims within the healthiest pool of applicants. Additionally, we argue that, by embracing transparency, the industry can build consumer trust and respond to a dynamic regulatory environment focused on algorithmic decision-making. We present a consumer-facing tool that uses a state-of-the-art method for interpretable machine learning to offer transparency into the life score. Improving the Accuracy and Transparency of Underwriting with Artificial Intelligence to Transform the Life-Insurance Industry",,2020,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
bacfb8d5f6f75da1f5fde6f92e38d0de9fb13d04,https://www.semanticscholar.org/paper/bacfb8d5f6f75da1f5fde6f92e38d0de9fb13d04,Explainable AI in Industry: Practical Challenges and Lessons Learned,"Artificial Intelligence is increasingly playing an integral role in determining our day-to-day experiences. Moreover, with the proliferation of AI based solutions in areas such as hiring, lending, criminal justice, healthcare, and education, the resulting personal and professional implications of AI have become far-reaching. The dominant role played by AI models in these domains has led to a growing concern regarding potential bias in these models, and a demand for model transparency and interpretability [11]. Model explainability is considered a prerequisite for building trust and adoption of AI systems in high stakes domains such as lending and healthcare [1] which require reliability, safety, and fairness. It is also critical to automated transportation, and other industrial applications with significant socio-economic implications such as predictive maintenance, exploration of natural resources, and climate change modeling. As a consequence, AI researchers and practitioners have focused their attention on explainable AI to help them better trust and understand models at scale [14, 15, 25]. The challenges for the research community include: (i) defining model explainability, (ii) formulating explainability tasks for understanding model behavior and developing solutions for these tasks, and finally (iii) designing measures for evaluating the performance of models in explainability tasks. In this tutorial, we will first motivate the need for model interpretability and explainability in AI [6] from societal, legal, enterprise, end-user, and model developer perspectives, and present techniques & tools for providing explainability as part of AI/ML systems [13]. Then, we will focus on the real-world application of explainability techniques in industry, wherein we present practical challenges & implications for using explainability techniques effectively and lessons learned from deploying explainable models for several web-scale machine learning and data mining applications. We will present case studies across different companies, spanning application domains such as search and recommendation systems, hiring, lending, sales, and fraud detection. Finally, based on our experiences in industry, we will identify open problems and research directions for the WWW community.",WWW,2020,10.1145/3366424.3383110,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
9c1e79d8633399ac07dca5a87d12f2785c244336,https://www.semanticscholar.org/paper/9c1e79d8633399ac07dca5a87d12f2785c244336,ADAPTIVE PID-LIKE CONTROL USING BROAD LEARNING SYSTEM FOR NONLINEAR DYNAMIC SYSTEMS,"This paper presents a new learning control structure using broad learning system (BLS) for adaptive PID-like control of unknown digital nonlinear dynamic systems with time delays. The proposed control method, abbreviated as BLS-APIDLC, is novel in combining BLS and model predictive control to develop a new PID-like control law for high-performance setpoint tracking control and disturbance rejection. Comparative simulations on two renowned nonlinear digital time-delay systems are well used to show the effectiveness and superiority of the proposed method by comparing to four existing methods. INTRODUCTION In the past decades, conventional proportional-integralderivative (PID) controllers have gained widespread use in numerous control applications due to its simplicity of design and efficiency in the industrial applications (Astrom and Hagglund, 1995; Silva et al., 2005; O’Dwyer, 2009; Vilanova and Visioli, 2012). Although the PID controller has only three parameters to be tuned, it is surprisingly difficult to find the right tuning for them without systematic procedures. As such, the tuning of the PID gains is always a challenge in the state of the art of PID controller design. In other words, the main problem with a PID controller is the fact that the parameters of the PID controller must be adjusted properly to meet desired performance. This problem becomes more important when considering issues that include stability and control performance. Recently, the area of adaptive PID control and its related control approaches have still been developed by researchers (Oliveira and Lemos, 2000; Pan et al., 2007; Fahmy et al., 2014; Yang et al., 2015). Adaptive PID controller designs with controller parameters updated online by the neural network models were also presented. Pan et al. (2007) developed a two-layer supervised control method for tuning PID controller parameters based on model parameters estimated by the lazy learning technique. Fahmy et al. (2014) proposed an adaptive PID controller using the recursive least square algorithm which updates the PID gains automatically online to force the actual system to behave like a desired reference model. Oliveira and Lemos (2000) proposed a comparison of some fuzzy-modelbased adaptive-predictive control strategies. Yang et al. (2015) presented an adaptive predictive control strategy based on Laguerre functions in the chopper cascade control system, and examined by experiments. More recently, machine learning algorithms have made significant progress, especially deep learning technologies that have been made in wide areas (Tsai et al., 2014; Rosa and Yu, 2016; Ghazia et al., 2017; Andò et al., 2018; Y. Kang et al., 2019). By successively adjusting the weights between neurons over many input-output pairs, the function computed by the network is refined over time so that it provides more accurate predictions. The lately presented broad learning system (BLS) is an emerging way for efficient and effective modeling of complex systems (Chen and Liu, 2018; Jin and Chen, 2018; Chen et al., 2019). Chen and Liu (2018) developed a very fast and efficient BLS based on the random vector functional-link neural networks (RVFLNN) (Pao et al., 1994) to offer an alternative way for deep learning and structure. The designed model can be expanded in wide fashion when new feature nodes and enhancement nodes are needed. Moreover, the corresponding incremental learning algorithm is also designed. The BLS offers an alternative to deep learning because it has a fast and broad expansion without the need for retraining through incremental learning. The input signals are passed to the mapped feature layer and then Paper submitted 01/12/20; revised 03/25/20; accepted 05/03/20. Author for correspondence: Ching-Chih Tsai (e-mail: cctsai@nchu.edu.tw). 1 Department of Electrical Engineering, National Chung Hsing University, Taichung, Taiwan. 2 Department of Electronic Engineering, Hsiuping University of Science and Technology, Taichung, Taiwan. 358 Journal of Marine Science and Technology, Vol. 28, No. 5 (2020) passed to the enhancement layer via a nonlinear transformation. Although NNs possess good function approximation capabilities for nonlinear dynamic systems, the training process is time-consuming. On the other hand, the BLS system has been shown to preserve good function approximation capabilities and has been illustrated the feasibility and benefits of BLS-based control techniques for identification and control of nonlinear dynamic systems (Chen and Liu, 2018; Jin and Chen, 2018; Vong et al., 2020; Xu et al., 2018; and Feng and Chen, 2018a; Chen et al., 2019). Conventional PID controllers have been regarded as the simplest and the most deployed controller in industry. To extend the robustness and adaptability of the conventional PID controller, by integrating the simplicity and effectiveness of the conventional PID controller and the learning and automatic adjustment capabilities of the intelligent control strategy based on the PID-like controller for the nonlinear dynamic system have been proposed (Wang et al., 1997; Tsai et al., 2005; Cong and Liang, 2009; Fu and Chai, 2011). For example, Wang et al. (1997) presented an adaptive PID-like controller using a Modified Neural Network (MNN) for learning the characteristics of a dynamic system. Tsai et al. (2005) proposed an adaptive PID-Like fuzzy-neural controller and applied to the nonlinear model reference control system. Fu and Chai (2011) presented a robust self-tuning PID-like controller by combining a pole assignment selftuning PID controller with a filter. Cong and Liang (2009) proposed a PID-like neural network nonlinear adaptive controller for motion control systems by using a mix locally recurrent neural network. The gradient descent method is used for online adjustment and the initial PID parameters are needed which can operate the closed-loop stably. Kumar et al. (2014) proposed a hybrid neural network-based PID like adaptive controller for precise position control of a permanent-magnet synchronous motor (PMSM) servo drive. So far, many adaptive PID control for industrial applications have been proposed (Tung, 2012; Tsai et al., 2017; Tsai et al., 2019). Feng and Chen (2018b) presented a PID-like control method using BLS; however, this kind of PID-like control method was limited to a class of nonlinear dynamic systems without time-delays. Inspired by the above-mentioned surveys, the objective of this paper is to propose a BLS-based adaptive predictive PIDlike control, called BLS-APIDLC, of a class of unknown nonlinear discrete-time time-delay systems not only for disturbance rejection but also for precise tracking and guaranteed stability. The presented contents of the paper are written in two principal contributions. One is the theoretical derivation and proof of a more general adaptive PID-like control approach using BLS for unknown nonlinear time-delay dynamic systems by comparing to the result (Feng and Chen, 2018b), and the other is comparative investigation of the proposed BLS-APIDLC in comparison with four existing adaptive PID control methods. Fig. 1. Topological structure of the used BLS model. The rest of this paper is organized as follows. The basic ideas of the BLS and BLS identifier design are described in Sections II and III, respectively. Section IV is devoted to proposing a BLS-APIDLC control law, investigating its closed-loop stability and iterative control algorithm. Section V uses computer simulations to explore the effectiveness and superiority of the proposed BLS-APIDLC method for two illustrious nonlinear time-delay systems. Section VI is finished with the conclusions and future work of the paper. II. BROAD LEARNING SYSTEM (BLS)",,2020,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
4b4210bb7d2359e0d7ae39489d6ab31e7082158f,https://www.semanticscholar.org/paper/4b4210bb7d2359e0d7ae39489d6ab31e7082158f,IoT for Smart Learning/Education,"Internet of Things, has opened many new possibilities in the world of technology. The interconnection among sensors, wearables, mobile devices, etc. is used to collect data. The collected data is processed and analyzed on fog and cloud through various algorithms to obtain an efficient solution to diverse problems. IoT has a wide range of spectrum for its implementation, and several industries have already started deploying a smart and efficient ecosystem. The Education sector is among these industries. There are many implementations of IoT for Smart Learning such as the use of smart boards in the classrooms, mobile applications allowing impromptu access to learning resources anywhere. These advancements are only the tip of the iceberg. The network of smart things can be utilized to make the learning process even smarter and more efficient. Even though the classes are equipped with smart devices, it is hard for educators to attend each student individually and find out the areas where they are facing problems, as every individual has a different learning pattern. Keeping track of the students and to assist them individually requires a lot of time and effort. The level of difficulty increases further when face-to-face learning is switched to online learning. The research focuses on enhancing the online learning and teaching experience through the implementation of IoT using available devices, sensors, and other technologies such as machine learning and artificial intelligence.",2020 5th International Conference on Innovative Technologies in Intelligent Systems and Industrial Applications (CITISIA),2020,10.1109/CITISIA50690.2020.9371774,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
f2ff46f13f6707be0295e1d6f6d00df9f3d4ce86,https://www.semanticscholar.org/paper/f2ff46f13f6707be0295e1d6f6d00df9f3d4ce86,Ethics and Creativity in Computer Vision,"This paper offers a retrospective of what we learnt from organizing the workshop Ethical Considerations in Creative applications of Computer Vision at CVPR 2021 conference and, prior to that, a series of workshops on Computer Vision for Fashion, Art and Design at ECCV 2018, ICCV 2019, and CVPR 2020. We hope this reflection will bring artists and machine learning researchers into conversation around the ethical and social dimensions of creative applications of computer vision. 1 CVFAD: Computer Vision for Fashion, Art and Design In 2018, we organized, in conjuction with ECCV, the first workshop on Computer Vision for Fashion, Art and Design. The workshop concentrated on generating, analyzing and processing visual content and invited the computer vision community to use generative model as a tool. As part of this workshop, we organized the Fashion-Gen [4] challenge for language to visual fashion design. The workshop also included a Computer Vision Art Gallery, organized by Luba Elliot and Xavier Snelgrove, to reflect the growing community of digital artists. Overall, the workshop brought together researchers in computer vision, artists and professionals from creative domains to discuss open problems in the areas of computer vision for fashion and creative visual content generation. Our ICCV 2019 and CVPR 2020 workshops broadened the scope of focus to include economical and industrial applications of creative computer vision tools. We hosted two fashion oriented challenges, FashionIQ [8] on multimodal fashion image retrieval [8] by Wu et al and DeepFashion2 [1] on a variety of fashion and clothing tasks such as fashion landmark detection by Ge et al. In ICCV 2019, Adriana Kovashka [7] brought up discussions on biases in creative advertisement content creation, and in political campaigns. We also had discussions during the panel discussions on potential harms that can be created by lack of representation in fashion industry and datasets. 2 EC3V: Ethical Considerations in Creative Applications of Computer Vision In 2021, we organized, in conjunction with CVPR, the first workshop on Ethical Considerations in Creative applications of Computer Vision (E3CV) . This workshop built upon 3 years of prior workshop organizing experience — 4 creativity-oriented challenges, as well as a successful Creativity in AI workshop series at NeurIPS and multiple generative art symposiums at the intersection of machine learning research and fashion industry — and oriented our focus around the under-explored ethical dimensions of creative computer vision work. With this workshop, we brought together a team of computer vision researchers, artists, and sociotechnical researchers to address growing number of questions on broader impact of this research. At a high level, the workshop focused on (a) the recognition of creative computer vision technologies as a new form of art , (b) influence of these technologies on society and representations [2] and (c) the areas that requires greater attention and discussion, and can create potential harms [3, 5]. 35th Conference on Neural Information Processing Systems (NeurIPS 2021), Sydney, Australia. We also hosted the Computer Vision Art Gallery this year to bring more artists as one of the major stakeholders to the discussion. The computer vision art gallery showcased the work of 60+ artists addressing a range of topics and leveraging range of computer vision methods. For example, Jake Elwes’s work ‘Zizi and Me’ 11 showcased a double act between drag queen Me and a deep fake clone of Me. In doing so, the work aims to both demystify AI and queer the process of AI development. Nouf Aljowaysir’s project, ‘Salaf’, leveraged generative techniques to make visible the patterns of inclusion and exclusion operative in AI systems and express her personal frustrations with the Western colonial gaze so frequently embedded within the systems in use today. Many fruitful conversations came out of the workshop including the oral presentation 14 by Prabhu and Isiain [3] critiquing our implementation of art gallery and submission process. Two main anchors that covered major conversations during our breakout sessions were around cultural appropriation, and ownership as well as issues integrated in training data. Cultural appropriation vs inspiration Algorithmic techniques offer new routes for adopting and transferring aesthetic styles in ways that can be beautiful and creative and even shed light on cultures and art that individuals might not otherwise engage with. However, these same tools risk enabling new forms of cultural appropriation as they can make it even easier to extract from marginalized cultures without any accompanying investment in that culture, understanding of the significance of the artefact or aesthetics, or meaningful engagement with or say from the community. Cultural appropriation [2], distributed art and eventually generative art, all raise once again the question of intellectual property. If a traditional African pattern is re-contextualised in western fashion, is it fair to share the profit? If an artwork is the result of millions of people interacting with a website, should everyone get a fraction of the credit? The question becomes more complex when considering techniques based on deep learning, where a model is trained by exposure to thousands of images. These models can later be deployed by artists as aid to the creative process. How much credit goes to the ideator of the algorithm in this case? And how much to the creators of the content that was used to create the content? While ethical considerations for what concerns the work of a specific artist are starting to be discussed in depth, the aspect of broader cultural appropriation is still relatively unexplored. Fundamental challenges arise when trying to define ownership and copyrights in the context of Traditional Cultural Expression, where the intellectual contribution can’t be attributed to a single individual, but results from, and often defines, the cultural evolution of specific groups of people. Generative art, training data: a source of inspiration or memorization? The ever-increasing ability of AI models to generate very realistic images introduces new challenges. AI-generated content can be carefully tailored to specific generations, creating (supposedly) new content that resembles the training data. Considerations related to the training data are: biases inherently present in the data, memorization of training data, and insufficient transparency around some dataset creation processes [5]. When using models trained on specific datasets to generate art, any bias present in the training data is unavoidably learned by the model and revealed in its generations [6]. Many contemporary artists have begun to engage with ML researchers to find these biases. Some have done this for the visual aesthetics that the techniques begin to allow, others engage with them more critically in order to understand and reveal the algorithmic processes that are beginning to have great social and political power. These groups often notice the social dimension of the algorithms which can be overlooked by the computer science community.",ArXiv,2021,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
e1c148b1ef2859e70bdbb65b18092b0335879127,https://www.semanticscholar.org/paper/e1c148b1ef2859e70bdbb65b18092b0335879127,Editorial: Artificial Intelligence and Human Movement in Industries and Creation,"Recent advances in human motion sensing technologies and machine learning have enhanced the potential of Artificial Intelligence to improve our quality of life, increase productivity and reshapemultiple industries, including cultural and creative industries. In order to achieve this goal, humansmust remain at the center of Artificial Intelligence and AI should learn from humans and collaborate effectively with them.Human-CentredArtificial Intelligence (HAI) is expected to create new opportunities and challenges in the future, which cannot yet be foreseen. Any type of programmable entity (e.g., robots, computers, autonomous vehicles, drones, Internet of Things, etc.) will have different layers of perception and sophisticated HAI algorithms that will detect human intentions and behaviors (Psaltis et al., 2017) and learn continuously from them. Thus, every single intelligent system will be able to capture human motions, analyze them (Zhang et al., 2019), detect poses and recognize gestures (Chatzis et al., 2020; Stergioulas et al., 2021) and activities (Papastratis et al., 2020; Papastratis et al., 2021; Konstantinidis et al., 2021), including facial expressions and gaze (Bek et al., 2020), enabling natural collaborationwith humans. Different sensing technologies, such as opticalMocap systems, wearable inertial sensors, RGB or depth cameras and other modality type sensors, are employed for capturing humanmovement in the scene and transforming this information into a digital representation. Most of the researchers usually focus on the use of a single modality sensor due to the simplicity and low cost of the final system and the design of either conventional machine learning algorithms or complex deep learning network architectures for analyzing humanmotion data (Konstantinidis et al., 2018; Konstantinidis et al., 2020). Such cost-effective approaches have been applied to a wide range of application domains, including entertainment (Kaza et al., 2016; Baker, 2020), health (Dias et al.; Konstantinidis et al., 2021), education (Psaltis et al., 2017; Stefanidis et al., 2019), sports (Tisserand et al., 2017), robotics (Jaquier et al., 2020; Gao et al., 2021), art and cultural heritage (Dimitropoulos et al., 2018), showing the great potential of AI technology. Based on the aforementioned, it is evident that HAI is currently at the center of scientific debates and technological exhibitions. Developing and deploying intelligent machines is definitely both an economic challenge (e.g., flexibility, simplification, ergonomy) as well as a societal challenge (e.g., safety, transparency), not only from a factory perspective, but also for the real-world in general. The papers in this Research Topic adopt different sensing technologies, such as depth sensors, inertial suits, IMU sensors and force-sensing resistors (FSRs) to capture human movement, while they present diverse approaches for modeling the temporal data. More specifically, Sakr et al. investigate the feasibility of employing FSRs worn on the arm to measure the Force Myography (FMG) signals for isometric force/torque estimation. A two-stage Edited and reviewed by: Astrid Marieke Rosenthal-von Der Pütten, RWTH Aachen University, Germany",Frontiers in Robotics and AI,2021,10.3389/frobt.2021.712521,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
9b1d8df753cec5e9f010cc79c866a4458865d0f5,https://www.semanticscholar.org/paper/9b1d8df753cec5e9f010cc79c866a4458865d0f5,Knowledge Discovery from Data Mining Techniques,"An evolving topic in today’s era is Data Mining and Knowledge Discovery. Data mining and knowledge discovery in databases is attracting a lot of researchers, industry persons, academicians. Why this area is so emerging? This article provides an overview of this emerging field, gives an overview that how data mining and knowledge discovery in databases are related to each other and also to other related fields, such as machine learning, statistics, and databases. The article also mentions particular real-world applications, specific data-mining techniques, challenges involved in realworld applications of knowledge discovery, and current and future research directions in the field",,2019,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
d07e91bc1c6be69f4e142174b06afe7387cc42c9,https://www.semanticscholar.org/paper/d07e91bc1c6be69f4e142174b06afe7387cc42c9,Analytics and AI for Industry- Specific Applications Minitrack,"The purpose of this minitrack is to explore case studies of applications of data analytics and artificial intelligence based smart services and digital solutions across industries. We will discuss reports that improve our understanding of how analytics and AI are currently used across industries influencing digital transformation of economies. We are interested in getting answers to the question “Where can AI be applied in an industry specific manner (a task with open access data and code) to benchmark and to improve industry standard performance, and grow more opportunities for value creation?” We are also interested in open tech AI applications for manufacturing, agriculture, healthcare as well as for other industry-specific applications. We will emphasize research on the design, analysis, implementation, adoption, and evaluation of real-life cases that provide opportunities to design, develop, and deploy these capabilities as micro-services that solve customer needs, especially those with startup potential. Opening presentation “Business Analytics for Sales Pipeline Management in the Software Industry: A Machine Learning Perspective” proposes a model designed to help sales representatives in the software industry manage the complex sales pipeline. By integrating business analytics in the form of machine learning into lead and opportunity management, datadriven qualification support reduces the high degree of arbitrariness caused by professional expertise and experiences. Through the case study of a software provider, authors developed three models to map the end-to-end sales pipeline process using real business data from the company’s CRM system. The results show a superiority of the CATBoost and Random Forest algorithm over other supervised classifiers such as Support Vector Machine and XGBoost. The study also reveals that the probability of either winning or losing a sales deal in the early lead stage is more difficult to predict than analyzing the lead and opportunity phases separately. In the paper “FraudMemory: Explainable Memory-Enhanced Sequential Neural Networks for Financial Fraud Detection” authors propose a novel fraud detection algorithm. It adopts state-of-art feature representation methods to better depict users and multimodal logs in financial systems. The proposed method uses a sequential model to capture the sequential patterns of each transaction and leverage memory networks to improve both the performance and interpretability. Also, with the incorporation of memory components, new algorithm called “FraudMemory” possesses high adaptability to the existence of concept drift. The paper “Deep Learning for Improved Agricultural Risk Management” authors investigate potential of deep learning in predicting agricultural yield in time and space under weather/climate uncertainty. They evaluate the predictive power of deep learning, benchmarking its performance against more conventional approaches. The findings reveal that deep learning offers the highest predictive accuracy, outperforming all the other approaches. Authors infer that it also has great potential to reduce underwriting inefficiencies and insurance coverage costs associated with using more imprecise yield-based metrics of real risk exposure. In the last paper “Holistic System-Analytics as an Alternative to Isolated Sensor Technology: A Condition Monitoring Use Case” authors propose a system-oriented concept of how to monitor individual components of a complex technical system without including additional sensor technology. By using already existing sensors from the environment combined with machine learning techniques, authors can infer the condition of a system component, without actually observing it. In consequence condition monitoring or additional services based on the component's behavior can be developed without overcoming the challenges of sensor implementation. We hope you will enjoy the papers and their presentation at the conference and we thank the authors for submitting excellent results of their work to make this minitrack successful. Proceedings of the 52nd Hawaii International Conference on System Sciences | 2019",,2018,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
566a003dc195b7fbe4d7dabdb70900f68c3aa235,https://www.semanticscholar.org/paper/566a003dc195b7fbe4d7dabdb70900f68c3aa235,Large-scale detection of non-technical losses in imbalanced data sets,"Non-technical losses (NTL) such as electricity theft cause significant harm to our economies, as in some countries they may range up to 40% of the total electricity distributed. Detecting NTLs requires costly on-site inspections. Accurate prediction of NTLs for customers using machine learning is therefore crucial. To date, related research largely ignore that the two classes of regular and non-regular customers are highly imbalanced, that NTL proportions may change and mostly consider small data sets, often not allowing to deploy the results in production. In this paper, we present a comprehensive approach to assess three NTL detection models for different NTL proportions in large real world data sets of 100Ks of customers: Boolean rules, fuzzy logic and Support Vector Machine. This work has resulted in appreciable results that are about to be deployed in a leading industry solution. We believe that the considerations and observations made in this contribution are necessary for future smart meter research in order to report their effectiveness on imbalanced and large real world data sets.",2016 IEEE Power & Energy Society Innovative Smart Grid Technologies Conference (ISGT),2016,10.1109/ISGT.2016.7781159,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
6ea59f72f3239e524b7fb5d09eaf2fc9638cb018,https://www.semanticscholar.org/paper/6ea59f72f3239e524b7fb5d09eaf2fc9638cb018,Ai Technology Achieving General Purpose Ai That Can,"Artificial IntelligenceArtificial Intelligence in Cyber Security: Impact and ImplicationsTechnological Progress, Artificial Intelligence, and Inclusive GrowthEnergy Research AbstractsCan Artificial Intelligence ImproveHandbook of Pharmaceutical Granulation TechnologyArtificial Intelligence for BusinessArtificial Intelligence in EducationThe Democratization of Artificial IntelligenceAdvances in Artificial Intelligence, Software and Systems EngineeringFinancing Our FutureProceedings of the Future Technologies Conference (FTC) 2021, Volume 1Artificial IntelligenceNew Technologies in Dermatological Science and PracticeArtificial IntelligenceGame Theory and Machine Learning for Cyber SecurityRegulatory Aspects of Artificial Intelligence on BlockchainConcise Encyclopedia of Software EngineeringEuropean Artificial Intelligence (AI) Leadership, the Path for an Integrated VisionAI In The Age Of Cyber-DisorderReadings in Artificial Intelligence and DatabasesHuman decisionsConstitution 3.0Artificial Intelligence and IoT-Based Technologies for Sustainable Farming and Smart AgricultureArtificial Intelligence and Deep Learning for Decision MakersThe Regional Economics of Technological TransformationsArchitects of IntelligenceArtificial IntelligenceArtificial Intelligence and Integrated Intelligent Information SystemsECIAIR 2019 European Conference on the Impact of Artificial Intelligence and Robotics Intelligence UnboundHow to Achieve Inclusive GrowthArtificial Intelligence for Business OptimizationArtificial Intelligence in SocietyThe Myth of Artificial IntelligenceThe Digital Innovation RaceAI-First HealthcareProject Management Best Practices: Achieving Global ExcellenceConnected WorldBiotechnology: Concepts, Methodologies, Tools, and Applications The interaction of database and AI technologies is crucial to such applications as data mining, active databases, and knowledge-based expert systems. This volume collects the primary readings on the interactions, actual and potential, between these two fields. The editors have chosen articles to balance significant early research and the best and most comprehensive articles from the 1980s. An in-depth introduction discusses basic research motivations, giving a survey of the history, concepts, and terminology of the interaction. Major themes, approaches and results, open issues and future directions are all discussed, including the results of a major survey conducted by the editors of current work in industry and research labs. Thirteen sections follow, each with a short introduction. Topics examined include semantic data models with emphasis on conceptual modeling techniques for databases and information systems and the integration of data model concepts in high-level data languages, definition and maintenance of integrity constraints in databases and knowledge bases, natural language front ends, object-oriented database management systems, implementation issues such as concurrency control and error recovery, and representation of time and knowledge incompleteness from the viewpoints of databases, logic programming, and AI.The artificial intelligence (AI) landscape has evolved significantly from 1950 when Alan Turing first posed the question of whether machines can think. Today, AI is transforming societies and economies. It promises to generate productivity gains, improve well-being and help address global challenges, such as climate change, resource scarcity and health crises.As technology continues to saturate modern society, agriculture has started to adopt digital computing and data-driven innovations. This emergence of “smart” farming has led to various advancements in the field, including autonomous equipment and the collection of climate, livestock, and plant data. As connectivity and data management continue to revolutionize the farming industry, empirical research is a necessity for understanding these technological developments. Artificial Intelligence and IoT-Based Technologies for Sustainable Farming and Smart Agriculture provides emerging research exploring the theoretical and practical aspects of critical technological solutions within the farming industry. Featuring coverage on a broad range of topics such as crop monitoring, precision livestock farming, and agronomic data processing, this book is ideally designed for farmers, agriculturalists, product managers, farm holders, manufacturers, equipment suppliers, industrialists, governmental professionals, researchers, academicians, and students seeking current research on technological applications within agriculture and farming.This book constitutes the refereed proceedings of the Second International Conference, SLAAI-ICAI 2018, held in Moratuwa, Sri Lanka, in December 2018. The 32 revised full papers presented were carefully reviewed and selected from numerous submissions. The papers are organized in the following topical sections: ?intelligence systems; neural networks; game theory; ontology engineering; natural language processing; agent based system; signal and image processing.After a long time of neglect, Artificial Intelligence is once again at the center of most of our political, economic, and socio-cultural debates. Recent advances in the field of Artifical Neural Networks have led to a renaissance of dystopian and utopian speculations on an AI-rendered future. Algorithmic technologies are deployed for identifying potential terrorists through vast surveillance networks, for producing sentencing guidelines and recidivism risk profiles in criminal justice systems, for demographic and psychographic targeting of bodies for advertising or propaganda, and more generally for automating the analysis of language, text, and images. Against this background, the aim of this book is to discuss the heterogenous conditions, implications, and effects of modern AI and Internet technologies in terms of their political dimension: What does it mean to critically investigate efforts of net politics in the age of machine learning algorithms?Intelligence Unbound explores the prospects, promises, and potential dangers of machine intelligence and uploaded minds in a collection of stateof-the-art essays from internationally recognized philosophers, AI researchers, science fiction authors, and theorists. Compelling and intellectually sophisticated exploration of the latest thinking on Artificial Intelligence and machine minds Features contributions from an international cast of philosophers, Artificial Intelligence researchers, science fiction authors, and more Offers current, diverse perspectives on machine intelligence and uploaded minds, emerging topics of tremendous interest Illuminates the nature and ethics of tomorrow’s machine minds—and of the convergence of humans and machines—to consider the pros and cons of a variety of intriguing possibilities Considers classic philosophical puzzles as well as the latest topics debated by scholars Covers a wide range of viewpoints and arguments regarding the prospects of uploading and machine intelligence, including proponents and skeptics, pros and consCompanies that don't use AI to their advantage will soon be left behind. Artificial intelligence and machine learning will drive a massive reshaping of the economy and society. What should you and your company be doing right now to ensure that your business is poised for success? These articles by AI experts and consultants will help you understand today's essential thinking on what AI is capable of now, how to adopt it in your organization, and how the technology is likely to evolve in the near future. Artificial Intelligence: The Insights You Need from Harvard Business Review will help you spearhead important conversations, get going on the right AI initiatives for your company, and capitalize on the opportunity of the machine intelligence revolution. Catch up on current topics and deepen your",,2022,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
f298c610975987727b3005a4d3cfa26fcf7fbbce,https://www.semanticscholar.org/paper/f298c610975987727b3005a4d3cfa26fcf7fbbce,A Decade of Internet of Things: Analysis in the Light of Healthcare Applications,"Impressive growth in the number of wearable health monitoring devices has affected global health industry as they provide rapid and intricate details related to physical examinations, such as discomfort, heart rate, and blood glucose level, which enable doctors to efficiently diagnose sensitive heart troubles. The Internet of Medical Things (IoMT) is a phenomenon wherein computer networks and medical equipment are connected through the Internet to provide real-time interaction between physicians and patients. In this article, we present a comprehensive view of the IoMT and its related Machine Learning (ML)-based developed frameworks designed, or being utilized, in the last decade, i.e., from 2010 to 2019. The presented techniques are designed for monitoring limbs, controlling rural healthcare, identifying e-health applications, monitoring health through mobile apps, classifying heart sounds, detecting stress in drivers, monitoring cardiac diseases, making the decision to predict heart attacks, recognizing human activities, and classifying breast cancer. The aim is to provide a clear picture of the existing IoMT environment so that the analysis may pave the way for the diagnosis of critical disorders such as cancer, heart attack, and blood pressure among others. In the end, we also provide some unresolved challenges that are confronted in the deployment of the secure IoMT-based healthcare systems.",IEEE Access,2019,10.1109/ACCESS.2019.2927082,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
ad3ce37737fdbab5e4834a161d755249b0c2ee24,https://www.semanticscholar.org/paper/ad3ce37737fdbab5e4834a161d755249b0c2ee24,A Survey of Opinion Mining in Arabic,"Opinion-mining or sentiment analysis continues to gain interest in industry and academics. While there has been significant progress in developing models for sentiment analysis, the field remains an active area of research for many languages across the world, and in particular for the Arabic language, which is the fifth most-spoken language and has become the fourth most-used language on the Internet. With the flurry of research activity in Arabic opinion mining, several researchers have provided surveys to capture advances in the field. While these surveys capture a wealth of important progress in the field, the fast pace of advances in machine learning and natural language processing (NLP) necessitates a continuous need for a more up-to-date literature survey. The aim of this article is to provide a comprehensive literature survey for state-of-the-art advances in Arabic opinion mining. The survey goes beyond surveying previous works that were primarily focused on classification models. Instead, this article provides a comprehensive system perspective by covering advances in different aspects of an opinion-mining system, including advances in NLP software tools, lexical sentiment and corpora resources, classification models, and applications of opinion mining. It also presents future directions for opinion mining in Arabic. The survey also covers latest advances in the field, including deep learning advances in Arabic Opinion Mining. The article provides state-of-the-art information to help new or established researchers in the field as well as industry developers who aim to deploy an operational complete opinion-mining system. Key insights are captured at the end of each section for particular aspects of the opinion-mining system giving the reader a choice of focusing on particular aspects of interest.",ACM Trans. Asian Low Resour. Lang. Inf. Process.,2019,10.1145/3295662,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
535909196d81fff052a8563c639c6ad96c5eb9e3,https://www.semanticscholar.org/paper/535909196d81fff052a8563c639c6ad96c5eb9e3,An artiﬁcial intelligence approach to pro-actively inspire drug discovery with recommendations.,"Artiﬁcial intelligence is becoming established in drug discovery. For example, many in the industry are applying machine learning approaches to target discovery or to optimize compound synthesis. While our organization is certainly applying these sorts of approaches, we propose an additional approach: using AI to augment human intelligence. We have been working on a series of recommendation systems that take advantage of our existing laboratory processes, both wet and computational, to provide inspiration to our chemists, suggest next steps in their work, and automate existing workﬂows. We will describe ﬁve such systems in various stages of deployment within the Novartis Institutes for BioMedical Research. While each addresses diﬀerent stages in the discovery pipeline, all share three common features: a trigger that initiates the recommendation, an analysis which leverages our existing systems with AI, and a recommendation delivery. The goal of all of them is to inspire and accelerate the drug discovery process.",Journal of medicinal chemistry,2020,10.1021/acs.jmedchem.9b02130,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
78da8c6893734e7ceb9fcddf566a11ce7ebd8f7c,https://www.semanticscholar.org/paper/78da8c6893734e7ceb9fcddf566a11ce7ebd8f7c,Privacy for All: Demystify Vulnerability Disparity of Differential Privacy against Membership Inference Attack,"Machine learning algorithms, when applied to sensitive data, pose a potential threat to privacy. A growing body of prior work has demonstrated that membership inference attack (MIA) can disclose specific private information in the training data to an attacker. Meanwhile, the algorithmic fairness of machine learning has increasingly caught attention from both academia and industry. Algorithmic fairness ensures that the machine learning models do not discriminate a particular demographic group of individuals (e.g., black and female people). Given that MIA is indeed a learning model, it raises a serious concern if MIA ``fairly'' treats all groups of individuals equally. In other words, whether a particular group is more vulnerable against MIA than the other groups. This paper examines the algorithmic fairness issue in the context of MIA and its defenses. First, for fairness evaluation, it formalizes the notation of vulnerability disparity (VD) to quantify the difference of MIA treatment on different demographic groups. Second, it evaluates VD on four real-world datasets, and shows that VD indeed exists in these datasets. Third, it examines the impacts of differential privacy, as a defense mechanism of MIA, on VD. The results show that although DP brings significant change on VD, it cannot eliminate VD completely. Therefore, fourth, it designs a new mitigation algorithm named FAIRPICK to reduce VD. An extensive set of experimental results demonstrate that FAIRPICK can effectively reduce VD for both with and without the DP deployment.",ArXiv,2020,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
c14ba2bd9b185ffe28c581a292050a596f3e672c,https://www.semanticscholar.org/paper/c14ba2bd9b185ffe28c581a292050a596f3e672c,CNSM 2019 special issue: Embracing the new wave of artificial intelligence,"In recent years, we observe a growing trend in the area of artificial intelligence (AI) for network and service management. Approaches such as statistical analysis, data mining and machine learning have become promising to harness the immense stream of operational data and to improve the management of networks and system. Thus, CNSM 2019 was focused on what matters most to managing networks and services. Many researchers from the field of network and service management have explored how AI can help us not only sense but also shape the future of management as AI technologies have disrupted numerous industries. To manage the configuration, performance, resilience, availability and security of the networks and services, traditional measures such as log/event analysis, intrusion detection/prevention and monitoring and deployment have taken a new dimension. New techniques and mechanisms from AI, machine learning and data mining are explored for designing, developing and operating networks and systems. In summary, there are a lot of research challenges in this emerging field of embracing the new wave of AI. The purpose of this special issue is to explore and highlight the promising capabilities of AI in managing the operational data on the networks and services. Among the top research papers presented at the 2019 International Conference on Network and Service Management, four of them are selected and the extended versions of those papers went under extensive reviews and discussions. These four papers were finally selected for publication in this special issue. The authors of these papers were given the time to update their papers based on the review comments and suggestions provided. The selected papers address topics that play a central role in embracing the new wave of AI for network and service management and presenting novel theoretical and/or experimentation results. The first paper, “On Accounting for Screen Resolution in Adaptive Video Streaming: QoE Driven Bandwidth Sharing Framework,” by Belmoukadam et al. analyses the problem of bandwidth allocation for multiple video streaming sessions over a shared link and proposes an optimization framework to maximize the overall quality of experience (QoE) taking into consideration terminal display capabilities. The approach relies on a Lagrangian relaxation heuristic and Karush–Kuhn–Tucker (KKT) conditions to efficiently solve the optimization problem, showing an increase of up to 20% of the overall QoE compared with an allocation with a TCP look-alike strategy implementing max–min fairness. The second paper, “Towards Distributed Emergency Flow Prioritization in SDN Networks,” by Moeyersons et al. proposes a microservices-based framework, which is able to run both as a centralized and distributed system. Moreover, the proposed framework guarantees the required bandwidth for the emergency flows and maximizes the best-effort flows over the remaining bandwidth based on their priority. The offline linear model part of the framework is evaluated through simulation, the distributed model is evaluated through emulation while the online approach is validated through physical experiments with SDN switches. Finally, as a proof of concept, a prototype with Zodiac switches validates the feasibility of the centralized framework. The third paper, “OpenBNG: Central Office Network Functions on Programmable Data Plane Hardware,” by Kundel et al. explores to offer residential network access with programmable packet processing architectures. The authors present the design and open-source implementation of a Broadband Network Gateway data plane using P4 programming language. Moreover, they introduce a concept of hybrid openBNG design, realizing the required hierarchical quality of service functionality in a subsequent FPGA. Evaluation results show the desired performance characteristics towards highest performance NFV network access. Finally, the fourth paper, “De-anonymizing Ethereum Blockchain Smart Contracts through Code Attribution,” by Linoy et al. is on analysing the pseudo-anonymity of blockchain technologies. The paper proposes to leverage a stylometry approach to investigate the extent to which a deployed contract's source code can contribute to the affiliation of the deployers' account addresses. Authors have prepared a dataset of real-world contract data; design and implement feature selection, extraction techniques and data refinement heuristics; and examine their effect on attribution accuracy. They further evaluate the proposed system to test the classification of real-world scammer data. Received: 18 November 2020 Accepted: 26 November 2020",Int. J. Netw. Manag.,2020,10.1002/nem.2149,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
98f40c015bd9f5b5a90f888f677aa9759a82c3a2,https://www.semanticscholar.org/paper/98f40c015bd9f5b5a90f888f677aa9759a82c3a2,Artificial Intelligence potential within airlines: a review on how AI can enhance strategic decision-making in times of COVID-19,"Purpose: Airline strategy relies on the competitive environment analysis and the management of resources. Artificial Intelligence (AI) algorithms are being increasingly deployed throughout several industries. COVID-19 has further stressed a sector where firms have historically struggled to sustain profitability.The purpose is to explore the potential of AI applications regarding strategic decision-making in airlines in times of crisis and to depict a roadmap to encourage scholars and practitioners to jointly implement these tools within corporations.Design/methodology/approach: This study firstly reviews the state-of-the-art regarding transport organization trends with focus on airline strategy and finance as well as AI tools, supported by the collaboration of a former airline digitalization strategist. Secondly, the potential of the latter to be applied in those functions is analyzed, considering different Machine Learning (ML) methods and algorithms.Findings: Some applications or pathways are identified as of particular interest for the airlines’ strategic decision-making process. Most of them are based on ML algorithms and training methods that are currently underused or disregarded in certain business areas, such as Neural Network models for unsupervised market analysis or supervised cost estimation.Research limitations/implications: Focus is on airline strategy and finance, keeping engineering or operational applications out of the scope.Practical implications: Proposed guidance may promote the deployment of AI tools which currently lack practical implementation in certain business areas.Social implications: Showcased guidance may revert into a closer collaboration between business and academia.Originality/value: Comprehensive review of current airlines’ strategic levers and identification of promising AI pathways to be further explored.",Journal of Airline and Airport Management,2021,10.3926/jairm.189,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
0c2252772a087a323fee9e1edde5c3052511cf9a,https://www.semanticscholar.org/paper/0c2252772a087a323fee9e1edde5c3052511cf9a,Managing Artificial Intelligence Technology for Added Value,"Many industrial sectors are in the midst of a digital transformation that has emerged from the advancement of information and data technology, enhancing the use of computers and automation with smart and autonomous systems powered by data and machine learning. This revolution has been broadly adopted in industry by initiating the use of digital technologies, sensor systems, intelligent machines, and smart material in its processes. Some examples of industrial innovation are the invention of artificial intelligence (AI), the deployment of the Internet of Things (IoT)/Internet of Services (IoS), 3D printing/additive manufacturing, machine learning, and the use of Big Data. These have enables the digitization, automation, or integration of service and product value chains. Implementing digitization and automation is believed to help construction transform into a technology-driven industry and keep pace with other industries.",International Journal of Technology,2020,10.14716/ijtech.v11i1.3889,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
fc65d91ca5f06584113d18fb271a1d6bc9573a85,https://www.semanticscholar.org/paper/fc65d91ca5f06584113d18fb271a1d6bc9573a85,The Wireless Solution to Realize Green IoT: Cellular Networks with Energy Efficient and Energy Harvesting Schemes,"With the tremendous increase of heterogeneous Internet of Things (IoT) devices and the different service requirements of these IoT applications, machine-type communication (MTC) has attracted considerable attention from both industry and academia. Owing to the prominent advantages of supporting pervasive connectivity and wide area coverage, the cellular network is advocated as the potential wireless solution to realize IoT deployment for MTC, and this creative network paradigm is called the cellular IoT (C-IoT). In this paper, we propose the three-layer structured C-IoT architecture for MTC and review the challenges for deploying green C-IoT. Then, effective strategies for realizing green C-IoT are presented, including the energy efficient and energy harvesting schemes. We put forward several strategies to make the C-IoT run in an energy-saving manner, such as efficient random access and barring mechanisms, self-adapting machine learning predictions, scheduling optimization, resource allocation, fog computing, and group-oriented transmission. As for the energy harvesting schemes, the ambient and dedicated energy harvesting strategies are investigated. Afterwards, we give a detailed case study, which shows the effectiveness of reducing power consumption for the proposed layered C-IoT architecture. Additionally, for real-time and non-real-time applications, the power consumption of different on-off states for MTC devices is discussed.",Energies,2020,10.3390/en13225875,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
c86e78458721a0d02ece7ca376e60cb0ed076377,https://www.semanticscholar.org/paper/c86e78458721a0d02ece7ca376e60cb0ed076377,Explainable AI in Industry,"Artificial Intelligence is increasingly playing an integral role in determining our day-to-day experiences. Moreover, with proliferation of AI based solutions in areas such as hiring, lending, criminal justice, healthcare, and education, the resulting personal and professional implications of AI are far-reaching. The dominant role played by AI models in these domains has led to a growing concern regarding potential bias in these models, and a demand for model transparency and interpretability. In addition, model explainability is a prerequisite for building trust and adoption of AI systems in high stakes domains requiring reliability and safety such as healthcare and automated transportation, and critical industrial applications with significant economic implications such as predictive maintenance, exploration of natural resources, and climate change modeling. As a consequence, AI researchers and practitioners have focused their attention on explainable AI to help them better trust and understand models at scale. The challenges for the research community include (i) defining model explainability, (ii) formulating explainability tasks for understanding model behavior and developing solutions for these tasks, and finally (iii) designing measures for evaluating the performance of models in explainability tasks. In this tutorial, we will present an overview of model interpretability and explainability in AI, key regulations/laws, and techniques/tools for providing explainability as part of AI/ML systems. Then, we will focus on the application of explainability techniques in industry, wherein we present practical challenges/ guidelines for using explainability techniques effectively and lessons learned from deploying explainable models for several web-scale machine learning and data mining applications. We will present case studies across different companies, spanning application domains such as search and recommendation systems, sales, lending, and fraud detection. Finally, based on our experiences in industry, we will identify open problems and research directions for the data mining/machine learning community.",KDD,2019,10.1145/3292500.3332281,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
a8182921b9da7a4cf8cff0422a1545be0a5ed76e,https://www.semanticscholar.org/paper/a8182921b9da7a4cf8cff0422a1545be0a5ed76e,Advancing from Predictive Maintenance to Intelligent Maintenance with AI and IIoT,"As Artificial Intelligent (AI) technology advances and increasingly large amounts of data become readily available via various Industrial Internet of Things (IIoT) projects, we evaluate the state of the art of predictive maintenance approaches and propose our innovative framework to improve the current practice. The paper first reviews the evolution of reliability modelling technology in the past 90 years and discusses major technologies developed in industry and academia. We then introduce the next generation maintenance framework - Intelligent Maintenance, and discuss its key components. This AI and IIoT based Intelligent Maintenance framework is composed of (1) latest machine learning algorithms including probabilistic reliability modelling with deep learning, (2) real-time data collection, transfer, and storage through wireless smart sensors, (3) Big Data technologies, (4) continuously integration and deployment of machine learning models, (5) mobile device and AR/VR applications for fast and better decision-making in the field. Particularly, we proposed a novel probabilistic deep learning reliability modelling approach and demonstrate it in the Turbofan Engine Degradation Dataset.",ArXiv,2020,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
7e5b2d6ecd340766f6eba8298eeb1493b69653d7,https://www.semanticscholar.org/paper/7e5b2d6ecd340766f6eba8298eeb1493b69653d7,Adaptive AR/MR Output Security on a Physical Device using Reinforcement Learning,"Augmented Reality (AR) and Mixed Reality (MR) are becoming increasingly ubiquitous. Research has shown that AR/MR will be a $100 industry by 2020 [7] and companies are chasing this market by making their own solution to AR/MR. Some solutions, such as Apple’s ARKit [18] and Google’s ARCore [19], use smartphones to create an AR/MR experience through the smartphone camera and display. However, these experiences are not fully immersive since users must hold up their smartphone in front of them. Microsoft’s HoloLense [17] and Magic Leap’s Magic Leap One [16] Head Mounted Display (HMD) AR/MR devices enable a much more immersive AR/MR experience since users do not have to hold a device in front of them and objects (i.e. “holograms”) are project right in front of them to augment their field of view. Although these devices are limited in their current field of view, their immersiveness is promising for the future of AR/MR. Although current immersive AR/MR technologies are focused on the use of proprietary devices, HoloKit [20] enables immersive AR/MR experiences with the use of a smartphone and simple cardboard mounting device (similar to Google Cardboard [21] which is for Virtual Reality experiences). In addition, researchers have suggested that AR car windshields may be a good use case for AR/MR technologies [23] through applications such as left-turn driving aids [24]. While the increasing proliferation of AR/MR devices will undoubtedly enable many new applications, issues of privacy and security cannot be ignored. Much of the previous work on AR/MR privacy has focused on the inputs to AR/MR devices (i.e. input security) [14,15]. Noting this gap in AR/MR security research, Lebek et al. suggested in a position paper that there should also be a focus on output security to secure the output of AR/MR devices [4]. Visual AR/MR output security is concerned with two issues pertaining to the user’s visual field: • Regulating visual content displayed to the user to reduce distraction and obstruction of the real-world context. • Preventing holograms from obscuring other holograms with a higher priority. To understand why these two issues are a concern for AR/MR security, take the case of an AR windshield in a car. Suppose the AR display in the car had a hologram to display the current speed and a hologram with the name of the song which is currently playing. It would be dangerous if either of the holograms obstructed an important real-world object such as a stop sign (the first security concern). In addition, it would also be dangerous if the name of the song hologram obstructed the current speed hologram (the second security concern). While one could leave these output security concerns to the application developer, it is much safer to have the OS guarantee the security of AR/MR device outputs. In this vein, previous work has investigated what an OS level AR/MR device output security module may look like by allowing developers to write policies [3]. However, as noted by Surin et al., these hand-coded policies are cumbersome and impractical for real-world use [1]. For example, specifying a policy to move holograms that are obstructing an important real-world object while not moving too far from its original location and not obscuring other important holograms at the same time is a very difficult task. Surin et al. proposed the use of reinforcement learning (RL) to solve this problem to automatically generate policies and demonstrated its effectiveness [1]. Although previous work has demonstrated the importance of output security and demonstrated its feasibility in simulation [1,3], they have not deployed output security policies on a physical AR/MR device to ascertain its viability in the realworld. In particular, whether a RL based output security model can be deployed without degrading performance was left an open question [1]. To fill this gap, this work investigates whether RL models can be deployed on a physical AR/MR device without a noticeable degradation in performance and test the deployment of an output security policy trained using RL. It is important to note that while this work focuses on visual output security, there are concerns that other AR/MR output, such as audio and haptic output, may need to be regulated as well [1,3]. In addition, one may also want to limit distracting and uncomfortable AR/MR output such as blinking holograms much like web browsers have evolved to block popups and the blink tag [3].",,2019,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
ad1301752d55a2dbd855c979c0b0d15f3ce986cc,https://www.semanticscholar.org/paper/ad1301752d55a2dbd855c979c0b0d15f3ce986cc,Semantic ML for Manufacturing Monitoring at Bosch,"Motivation. Technological advances that come with Industry 4.0, in e.g. sensoring and communication, unlock unprecedented large volumes of manufacturing data. This opens new horizons for data-driven methods like Machine Learning (ML) in analysis of manufacturing processes for a wide range of industries. An important scenario here is monitoring of manufacturing processes, including e.g. analysing the quality of the manufactured products and predicting the health state of machines and equipment. Consider an example of welding quality monitoring at Bosch, where welding is performed with automated machines that connect pieces of metal together by pressing them and passing high current electricity through them. Development of ML approaches for welding quality monitoring used in Bosch follows an iterative workflow that includes data collection (Step 1), task negotiation (Step 2), data preparation (Step 3), ML model development (Step 4), result interpretation and model selection (Step 5), model deployment (Step 6).",SEMWEB,2020,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
a2cbeadd90cbaabca6ac0e1aeb9352b829f80e3f,https://www.semanticscholar.org/paper/a2cbeadd90cbaabca6ac0e1aeb9352b829f80e3f,Assessing and mitigating unfairness in credit models with the Fairlearn toolkit,"As AI plays an increasing role in the financial services industry, it is essential that financial services organizations anticipate and mitigate unintended consequences, including fairness-related harms, such as denying people services, initiating predatory lending, amplifying gender or racial biases, or violating laws such as the United States’ Equal Credit Opportunity Act (ECOA). To address these kinds of harms, fairness must be explicitly prioritized throughout the AI development and deployment lifecycle. To help organizations prioritizing fairness in AI systems, Microsoft has released an open-source toolkit called Fairlearn. This toolkit focuses on the assessment and mitigation of fairness-related harms that affect groups of people, such as those defined in terms of race, sex, age, or disability status. Using a dataset of loan applications, we illustrate how a machine learning model trained with standard algorithms can lead to unfairness in a loan adjudication scenario, and how Fairlearn can be used to assess and mitigate this unfairness. The model, which is obtained by thresholding the predictions of probability of default (PD), leads to an uneven distribution of adverse events for the “male” group compared to the “female” group even though this model does not use sex as one of its inputs. Fairlearn’s mitigation algorithms reduce this disparity from 8 percentage points to 1 percentage point without any (statistically significant) impact on the to the financial services organization. We emphasize that fairness in AI is a sociotechnical challenge, so no software toolkit will “solve” fairness in all AI systems. However, software toolkits like Fairlearn can still play a valuable role in developing fairer AI systems—as long as they are precise and targeted, embedded within a holistic risk management framework, and supplemented with additional resources and processes.",,2020,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
1a51198caa98796094b3c472005ac91641cee7a5,https://www.semanticscholar.org/paper/1a51198caa98796094b3c472005ac91641cee7a5,A Primer on Large Intelligent Surface (LIS) for Wireless Sensing in an Industrial Setting,,CrownCom,2020,10.1007/978-3-030-73423-7_10,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
b69f1ab94acbc5ba91239f89851c1ac93206cf00,https://www.semanticscholar.org/paper/b69f1ab94acbc5ba91239f89851c1ac93206cf00,A Distributed ML Framework for Service Deployment in the 5G-based Automotive Vertical,"5G is the convergence technology for the new generation of mobile networks, expected to be massively deployed in the coming years. Building on network slicing and edge computing capabilities, 5G promises to address the diverse and quite demanding performance requirements of a wide range of use cases (UCs). As a result of these technological transformations, vertical industries will have enhanced technical capacity available to trigger the development of new products and services. Driven by these advances, Machine learning (ML) applications are headed towards collaborative distributed (CDL) schemes, to exploit the abundance of clients’ data. Contrary to the traditional cloud-based centralized solutions (CML), in CDL schemes, computational load is shifted to the intelligent edge and extends further beyond, to the user-equipment (including connected vehicles). Here, we present a distributed ML (DML) framework, that will provide functionalities for simplified management and orchestration of collections of ML service components and will allow ML-based applications to penetrate the Automotive world.",2021 IEEE International Mediterranean Conference on Communications and Networking (MeditCom),2021,10.1109/MeditCom49071.2021.9647693,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
63263cefaffd056a826f1d9734256c91780b7923,https://www.semanticscholar.org/paper/63263cefaffd056a826f1d9734256c91780b7923,"Transitioning from Legacy Air Traffic Management to Airspace Management through Secure, Cloud-Native Automation Solutions","Advancements in Cloud-native services, Machine-Learning (ML), Artificial Intelligence (AI), and Rapid Application Development (RAD) using the Agile methodology has led countless industries to achieving desirable levels of automation while reducing cost and improving quality software deployments, timely / iterative delivery, and accountability. Coupling this framework with the principle of security as a shared responsibility further enhances the efficacy of an integrated Development, Security, and Operations (DevSecOps) Team within organizations to deliver secured digital solutions. Air Navigation Service Providers (ANSPs) around the world are currently exploring and embracing the digital evolution shifting from monolithic, legacy automation platforms to an application framework of microservices to allow for flexible operations as capabilities and airspace operations evolve. Specific to the US, the ATM automation system of today is comprised of both safety and non-safety critical systems, with mission-essential, efficiency-critical, and mission-support services that are predominately maintained and evolved through multi-year, one contractor-led programs. Although the system has proven resilient, it has not proven to be agile and flexible to allow for advances in capabilities on-board aircraft or in the data integration and sharing with other NAS automation systems. This creates significant overhead in development, sustainability, and operations of the current automation system, and leaves modernization efforts—in terms of new capabilities—in constant investment decision planning cycles, costing agencies not just money, but more time to innovate. To advance aviation into a new generation of interoperability leveraging collaborative frameworks and application specific capabilities, ANSPs must adapt to innovative methods to collect, process, and deliver critical and essential aeronautical, weather, and flight information to air traffic control operators and ultimately to airspace users. Doing so can not only lead to sustaining NAS automation systems while reducing the costs to develop and operate these systems, but it also provides an opportunity to present strategies on how to dramatically reduce the time and integration efforts needed to deploy new capabilities. Leveraging cloud-native technologies and services is a way to realize this automation evolution vision for ANSPs.This paper examines the migration from today’s systems to secure, cloud-native platforms to prove that Mission Services and Mission Applications can be rapidly available / deployable to operators who provide separation and flow management services, using a cyber-secured cloud-native environment. Aeronautical data typically used for tactical decision making is now seen as crucial to the decision-making process in Air Traffic Management (ATM). Integrating global and localized datasets into a digital aviation data platform enhances the capabilities of the solutions and opens the possibilities of leveraging big data analytics and microservices to compute trajectory predictions (TP), demand capacity balancing (DCB), arrival and departure sequencing, airspace delay, among others, in real-time to achieve operator-driven mission objectives. Technology has reached a state of maturity, especially in cloud and hybrid cloud solutions, to support safety of life operations, like ATM. This paper identifies approaches that are being considered for that migration to support the integration of new airspace entrants, the use of application services to provide a dynamic, evolutionary ATM platform, and addresses some of the safety and security strategies that must be considered for this evolution.",2021 IEEE/AIAA 40th Digital Avionics Systems Conference (DASC),2021,10.1109/dasc52595.2021.9594313,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
2a1208fd6b4a7a314c90c2b4bf103e5993134e52,https://www.semanticscholar.org/paper/2a1208fd6b4a7a314c90c2b4bf103e5993134e52,Connecting Health Immersion of Digital into eHealth,,Introduction to Nursing Informatics,2021,10.1007/978-3-030-58740-6_2,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
1da1f40e379c4dd4303f0b769264191339afafea,https://www.semanticscholar.org/paper/1da1f40e379c4dd4303f0b769264191339afafea,"Building Efficient, Reliable, and Ethical Autonomous Systems","As autonomous systems rapidly grow in adaptability, effectiveness, and sophistication, their deployment has been accelerating in complex real-world domains that range from elder care robotics and autonomous driving to smart city management and military technology. However, while our ability to build efficient and reliable autonomous systems that integrate into our daily lives has expanded over the years, it has inevitably outstripped our ability to build ethical autonomous systems. Therefore, the goal of my research is to build autonomous systems that operate in natural, partially observable, stochastic domains for long durations in not only an efficient and reliable but also ethical way. Given the goal of my research, I have developed a range of approaches based on MDPs, POMDPs, and Dec-POMDPs along with their solution methods by using dynamic programming, mathematical programming, reinforcement learning, machine learning, deep learning, abstractions, heuristic search, and probabilistic graphical models. As a demonstration of my research, I have applied these approaches to autonomous vehicles (route navigation, obstacle handling, lane merging, and intersection negotiation), planetary exploration rovers, earth observation satellites, and standard general-purpose mobile robots. My research has fortunately led to a distinguished paper award (AAAI), an NSF Graduate Research Fellowship, and top publications (AAAI/IJCAI/ECAI/ICRA/IROS/AAMAS/AIES/SoCS). Moreover, I have had the opportunity to mentor students, write a grant proposal, and collaborate with industry. First, I have been a mentor for 6 BS, MS, and PhD students who have all had the experience as an author on multiple papers for top conferences. In fact, most recently, I am excited to say that one of my BS students will start a PhD in artificial intelligence at Brown University in the fall. Next, I was a main author on an NSF grant proposal on adaptive metareasoning for bounded rational agents that was awarded over $400,000. Finally, I am a key contributor to an industry collaboration with Nissan Research Center that has led to top publications and patents for over 3 years.",,2021,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
9cc247ed02bbd6eb3dfa04637e105958ec07d79d,https://www.semanticscholar.org/paper/9cc247ed02bbd6eb3dfa04637e105958ec07d79d,Design considerations introducing analytics as a “dual use” in complex industrial embedded systems,"Embedded systems are today often self-sufficient with limited and predefined communication. However, this traditional view of embedded systems is changing through advancements in technologies such as, communication, cloud technologies, and advanced analytics including machine learning. These advancements have increased the benefits of building Systems of Systems (SoS) that can provide a functionality with unique capabilities that none of the included subsystems can accomplish separately. By this gain of functionality the embedded system is evolving towards a “dual use” purpose11In this paper we define dual usage as a control system having two purposes. In other contexts such as politics, diplomacy and export control, the term “dual-use” refers to technology that can be used for both peaceful and military aims, e.g., nuclear power technology., The use is dual in the sense that the system still needs to handle its original task, e.g., control and protect of an asset, and it must provide information for creating the SoS. Larger installations, e.g., industry plants, power systems and generation, have in most cases a long expected life-cycle, some up to 30–40 years without significant updates, compared to analytical functions that evolve and change much faster, i.e., requiring new types of data sets from the subsystems, not know at its first deployment. This difference in development cycles calls for new solutions supporting updates related to new requirements inherent in analytical functions. In this paper, within the context of “dual usage” of systems and subsystems, we analyze the impact on an embedded system, new or legacy, when it is required to provide analytic data with high quality. We compare a reference system, implementing all functions in one CPU core, to three other alternative solutions: a) a multi-core system where we are using a separate core for analytics, b) using a separate analytics CPU and c) analytics functionality located in a separate subsystem. Our conclusion is that the choice of analytics information collection method should to be based on intended usage, along with resulting complexity and cost of updates compared to hardware cost.",2021 26th IEEE International Conference on Emerging Technologies and Factory Automation (ETFA ),2021,10.1109/ETFA45728.2021.9613273,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
14b879c55e0116fb1d9cd10837446a8cc1272fde,https://www.semanticscholar.org/paper/14b879c55e0116fb1d9cd10837446a8cc1272fde,An Efficient Intrusion Prevention System for CAN: Hindering Cyber-Attacks With a Low-Cost Platform,"The controller area network (CAN), which is still today the most used in-vehicle network, does not provide any security or authentication mechanism by design. Since current vehicles, which have numerous connectivity technologies, such as Bluetooth, Wi-Fi, and cellular radio, can be easily accessed from the exterior world, they can be easy targets of cyber-attacks. It is therefore urgently necessary to enhance vehicle security by detecting and stopping cyber-attacks. In this paper, we propose a novel unsupervised intrusion prevention system (IPS) for automotive CANs that detects and hinders attacks without modifying the architecture of the electronic control units (ECUs) or requiring information that is restricted to car manufacturers. We compare two machine learning algorithms’ ability to detect fuzzing and spoofing attacks, and evaluate which of them is most accurate with the fewest number of data bytes. The fewer data bytes required, the sooner detection can start and the sooner attacking frames can be detected. Experiment results show that our proposed detection mechanism achieves accuracy higher than 99%, F1-scores higher than 97%, and detection times shorter than $80 ~\mu s$ for the types of attacks considered. Moreover, when compared to four state-of-the-art intrusion detection systems, it is the only solution that is capable of discarding attacking frames before damage occurs while being deployed on inexpensive Raspberry Pi. Such an inexpensive deployment is particularly desirable, as cost is one of the automotive industry’s primary concerns.",IEEE Access,2021,10.1109/access.2021.3136147,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
cffb0a0ad3738aa06511a9ba484d9911ac4793a6,https://www.semanticscholar.org/paper/cffb0a0ad3738aa06511a9ba484d9911ac4793a6,Digitization of Drill Bit Inspections; User-Centered Design Methods to Automate Robotic Inspections,"
 One unique facet of digital technology is the merging of separate technologies for new workflows and products. Like other industries, energy is also doing this. This project will automate the bit inspection process and this system will reduce labor costs, increase product quality, and improve bit performance. The innovation center is working on various aspects of the project, which aims to join automation technologies with robotic capabilities.
 Industrial robots are used extensively in traditional high-volume manufacturing applications. The high-mix, low-volume nature of oil and gas manufacturing operations has impeded deployment of automation solutions. Recent advances in sensors, computers, and machine learning now enable integrating robotics and automation technologies into these flexible manufacturing workflows. Driven by digital transformation, an automated inspection system for polycrystalline diamond compact (PDC) drill bits has been developed. The system uses high-resolution robotic 3D scanning, 2D imaging, and artificial intelligence to improve inspection efficiency and product quality. In our user-experience- (UX-) focused approach, we streamlined the user interface (UI) research methods to develop the robotic inspection UI and successfully tested the design with end users. This paper introduces the inspection system and improved workflows for the PDC bits, illustrates the innovative UX/UI development process, and targeted evaluation with the end users, which is crucial before deploying the system in production. We also concluded with some recommended improvements to guide future work.","Day 2 Wed, September 22, 2021",2021,10.2118/206261-ms,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
a898efda601f315f7d0e91275d6cad3d93d27749,https://www.semanticscholar.org/paper/a898efda601f315f7d0e91275d6cad3d93d27749,Pipeline safety warning system based on distributed optical fiber sensing,"Pipeline safety early warning (PSEW) systems are designed to automatically identify and locate damage events on pipelines. They are intended to replace traditional manual inspections. However, current PSEW systems cannot achieve universality for various complex environments. Our research aims to improve the performance of long-distance PSEW systems through machine learning. With strong industrial demands, we proposed a 作者简介:杨毅远(1997 年-),男,河北石家庄,研究生,主要从事分布式光纤传感器信号处理与模式 识别的研究。E-mail:yangyy19@mails.tsinghua.edu.cn. 导师简介:李轶(1983 年-),男,广东深圳,博士,副教授,主要从事传感器数据融合以及多相流测量 领域的研究。E-mail:liyi@sz.tsinghua.edu.cn. novel long-distance PSEW system based on distributed optical fiber sensing. Specifically, we presented two novel features and built a deep learning network for action recognition. Validated on two operational energy transportation pipelines, the results confirm that the proposed features can effectively characterize signals in strong noise and weak signal environments with good visualization, and the model can rapidly identify and locate third-party events under various hardware conditions with real-time performance and model size fully meeting industry requirements. In addition, the system has been deployed to an 85km pipeline in CNPC's northern pipeline network at the end of 2020. After half a year of testing, the system has been operating stably and successfully applied in the field, and the solution proposed in this paper has been integrated into CNPC's smart pipeline system. Our work provides a new perspective on the practical application of PSEW systems in industrial scenarios. key words: pipeline safety early warning; distributed optical fiber sensing; pattern recognition; machine learning; industrial signal processing",,2021,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
01b0b16dbb4fbef6fb44bdb40886a49dcb43a8a6,https://www.semanticscholar.org/paper/01b0b16dbb4fbef6fb44bdb40886a49dcb43a8a6,A Big Bang-Big Crunch Type-2 Fuzzy Logic System for Explainable Predictive Maintenance,"The role of maintenance in modern manufacturing systems is becoming a more significant contributor to organizational benefit. World-class enterprises are pushing forward with “predict-and prevent” maintenance instead of embracing the drawbacks of reactive maintenance (or a “fail-and fix” approach). The advancement towards Artificial Intelligence (AI), Internet of Things (IoT) and cloud computing has led to a shift in maintenance paradigms with the rising interest in Machine Learning (ML) and in particular deep learning. However, opaque box AI models are complex and difficult to understand and explain to the lay user. This limits the use of these models in predictive maintenance where it is crucial to understand and analyze the model before deployment and it is imperative to understand the logic behind any given decision. This paper introduces a Type-2 Fuzzy Logic System (FLS) optimized by the Big-Bang Big-Crunch algorithm that allows maximizing the interpretability of a model as well as its prediction accuracy for the faults which may occur in future. We tested the proposed type-2 FLS model on water pumps where data was collected in real-time by our proprietary hardware deployed at Aquatronic Group Management Plc. The observations indicate that the proposed system provides a highly interpretable and accurate model for predicting the faults in equipment for building services, process and water industries. The system predictions are used to understand why a particular fault may occur, leading to improved and better-informed service visits for the customers thus reducing the disruptions faced due to equipment failures.",2021 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE),2021,10.1109/FUZZ45933.2021.9494540,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
bd602025d000e9a2ea6decfab14f94dd44a5db12,https://www.semanticscholar.org/paper/bd602025d000e9a2ea6decfab14f94dd44a5db12,Subsequent Visualizing in Swarm Robotics,"Our high-level conferences will bring together forward thinking brands, market leaders, AI & Swarm intelligence evangelists and hot start-ups to explore and debate the advancements in Artificial Intelligence & Swarm intelligence, the impacts within Enterprise & Consumer sectors as well as Development platforms and Digital Transformation opportunities. Topics covered include Business Intelligence, Deep Learning, Machine Learning, AI Algorithms, Data & Analytics, Virtual Assistants & Catboats as well as case study based presentations proving an insight into the deployment of AI across different verticals. 
 
On behalf of the International Journal of Swarm Intelligence and Evolutionary Computation I am gratified to present the journal. Our articles AI & Swarm intelligence is the latest trending technology in many fields especially in industries like Manufacturing, Automation, Control Systems, Healthcare, Energy, Transport, Defence, Space, Data Mining, etc.",,2021,10.35248/2376-130X.21.7.E168,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
a4a96a4725ab92ebb3a01be8eb1278ee85b76a88,https://www.semanticscholar.org/paper/a4a96a4725ab92ebb3a01be8eb1278ee85b76a88,Cost-effective filtering of unreliable proximity detection results based on BLE RSSI and IMU readings using smartphones,,Scientific reports,2022,10.1038/s41598-022-06201-y,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
f503213df8d2a819f9dbd4ac94ce21519e1ab13c,https://www.semanticscholar.org/paper/f503213df8d2a819f9dbd4ac94ce21519e1ab13c,Disparate Impact in Differential Privacy from Gradient Misalignment,"As machine learning becomes more widespread throughout society, aspects including data privacy and fairness must be carefully considered, and are crucial for deployment in highly regulated industries. Unfortunately, the application of privacy enhancing technologies can worsen unfair tendencies in models. In particular, one of the most widely used techniques for private model training, differentially private stochastic gradient descent (DPSGD), frequently intensifies disparate impact on groups within data. In this work we study the fine-grained causes of unfairness in DPSGD and identify gradient misalignment due to inequitable gradient clipping as the most significant source. This observation leads us to a new method for reducing unfairness by preventing gradient misalignment in DPSGD.",ArXiv,2022,10.48550/arXiv.2206.07737,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
a54a4ac24cd2a540fe0f845c77e6bdb95facd64a,https://www.semanticscholar.org/paper/a54a4ac24cd2a540fe0f845c77e6bdb95facd64a,Formalization and Improvement of Ambulance Dispatching in Brabant-Zuidoost,"Throughout the last years, performance of ambulance services in the Netherlands has been consistently below the nationally-set target (response time of less than 15 minutes for at least 95% of highly urgent requests). As an industry where performance improvements can literally save lives, but resources are often scarce, emergency medical services (EMS) providers are continuously looking for ways to deploy available resources more efficiently. In this thesis we developed an alternative dispatch policy with the objective to improve the on-time performance of highly urgent ambulance requests, given the available ambulance capacity. We used machine learning to capture current dispatch practices in the Dutch EMS region ‘Brabant-Zuidoost’ (BZO) by extracting human decision rules from historic data and refining them with domain knowledge in a unique post-processing phase. This effort resulted in a formal model that is both concise and able to accurately predict current dispatch decisions. Subsequently, we leveraged the captured dispatch process as a practically relevant basis to improve upon and evaluated the resulting alternative policy in an advanced simulation. Results show that complementing the captured current dispatch policy with two enhancements yields a significant improvement of the on-time performance of highly urgent ambulance requests, without the need for increasing ambulance capacity. Our main contributions are the following:",BNAIC/BENELEARN,2019,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
cce600d54ef32b3f6312b9166da097e0f9eded8a,https://www.semanticscholar.org/paper/cce600d54ef32b3f6312b9166da097e0f9eded8a,Designing a Visual Analytics System for Industry-Scale Deep Neural Network Models,"The complexity of industry-scale deep learning models and datasets pose unique design, visualization, and system challenges that are inadequately addressed by existing work. Through participatory design sessions with over 15 researchers and engineers at Facebook, we have designed, developed, and deployed ACTIVIS, a visual analytics system for interpreting industry-scale deep learning models and results. By tightly integrating multiple coordinated views, such as a computation graph overview of the model architecture, and a neuron activation view for pattern discovery and comparison, users can explore complex deep neural network models at both instanceand subset-level. ACTIVIS has been deployed on Facebook’s machine learning platform. This article is a summary for the VAST’17 paper (TVCG track) ActiVis: Visual Exploration of Industry-Scale Deep Neural Network Models [2]. 1 DESIGNING FOR INDUSTRY-SCALE MODELS Despite the increasing interest in developing visualization tools for deep learning interpretation [5–7], the complexity of large-scale models and datasets used in industry pose unique design challenges that are inadequately addressed by existing work. For example, while most existing visualization tools target image datasets, deep learning tasks in industry often involve different types of data, including text and numerical data. Furthermore, in designing tools for realworld deployment, it is a high priority that the tools be flexible and scalable, adapting to the wide variety of models and datasets used. These observations motivate us to design and develop ACTIVIS [2], a visual analytics system for deep neural network models, now deployed on Facebook’s machine learning platform. Since the ACTIVIS project started in April 2016, we have conducted participatory design sessions with over 15 Facebook engineers, researchers, and data scientists across multiple teams to learn about their visual analytics needs. We identified six key design challenges — for data, model, and analytics — that have not been adequately addressed by existing deep learning visualization tools. The challenges include the need to support: (1) diverse input data sources, (2) high data volume, (3) complex model architecture, (4) a great variety of models, (5) diverse subset definitions for analytics, and (6) both instanceand subset-level analyses. These challenges shape the main design goals of ACTIVIS. 2 ACTIVIS CONTRIBUTIONS ACTIVIS’s main contributions include: • A novel visual representation that unifies instanceand subsetlevel inspections of neuron activation, facilitating comparison of activation patterns for multiple instances. *e-mail: kahng@gatech.edu †e-mail: mortimer@fb.com ‡e-mail: adityakalro@fb.com §e-mail: polo@gatech.edu • An interface that tightly integrates an overview of graph-structured complex models and local inspection of neuron activations, allowing users to explore the model at different levels of abstraction. • A deployed system scaling to large datasets and models. • Case studies with Facebook engineers and data scientists that highlight how ACTIVIS helps them with their work. ACTIVIS’s multiple coordinated views help users get a high-level overview of the model from which the user can drill down to perform localized inspection of activations. ACTIVIS visualizes how neurons are activated by user-specified instances or instance subsets, to help users understand how a model derives its predictions. The subsets can be flexibly defined using data attributes, features, or output results, enabling model inspection from multiple angles. While many existing deep learning visualization tools support instancelevel exploration [6, 7], ACTIVIS is the first tool that simultaneously supports instanceand subset-level exploration. Both exploration strategies are common and effective, and they offer complementary analytics benefits. Instance-based analysis instructs how individual instances contribute to a model’s accuracy, but it is tedious to inspect many instances one by one. Subset-based analysis leverages input features or instance subsets to help reveal relationships between data attributes and machine learning algorithms’ outputs [3]. It is especially beneficial when dealing with huge datasets in industry, which may consist of millions or billions of data points. By exploring instance subsets and enabling their comparison with individual instances, users can learn how them models respond to many different slices of the data. We refer our readers to the longer version of our ACTIVIS [2] VAST’17 paper published in IEEE Transactions on Visualization and Computer Graphics.",,2017,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
aac2edf6a9a174fe181e8055ad9a43c9627f63c7,https://www.semanticscholar.org/paper/aac2edf6a9a174fe181e8055ad9a43c9627f63c7,AI Firm Ambyint’s New Bakken Deal With Equinor Moves the Industry Another Step Closer to the Edge,"In the face of leaner economic times, oil and gas companies want to be able to boost well counts while minimizing any new additions to their field workforce.
 Among the companies answering this call is Ambyint, which was just tapped to deploy its artificial intelligence (AI) systems to help optimize all of Equinor’s Bakken Shale wells running on sucker rod pumps—the oil field’s most common breed of artificial lift.
 The deal is understood to be one of Ambyint’s largest contracts to date. Specific numbers have not been shared, but public data show that Equinor operates more than 800 wells in North Dakota. Ambyint, which is headquartered in Calgary and has an office in Houston, said the project scope may eventually include all of the Norwegian-owned operator’s horizontal wells in the state as they transition to rod pump.
 The large-scale upgrade marks another milestone in the evolution of oilfield automation that for decades has been defined by a nearly-ubiquitous reliance on SCADA (supervisory control and data acquisition) systems.
 One of the drawbacks of the status quo is that it requires small armies of field personnel to interpret SCADA data and then adjust setpoints to get pumping units back into optimal operating ranges. This manual process can consume half an hour per well to complete; downtime that quickly adds up in a field of hundreds.
 “What we are talking about is having the machine do that entire workflow,” Chris Robart, Ambyint’s president of US operations said.
 By equipping wells with its cloud-based AI and edge computing technology, and an application the company calls an autonomous setpoint management system, the man-hours once spent re-setting pumps can hopefully be reallocated to other bottom-line drivers. “We are freeing up individuals to go do other things, like think about new technology, troubleshoot failed equipment, deal with workovers, or new well designs,” Robart added.
 Pilot Hits the Mark for Equinor The Bakken project comes after a pilot that included 50 of Equinor’s wells, which saw a net production increase of 6%—considerably larger uplift figures were seen from those wells suffering from underpumping.
 The encouraging results were realized with zero shutdowns in production and minimal “human interference,” according to the companies.
 “The Ambyint technology has improved the remote data visibility and has delivered a more accurate diagnostic of downhole conditions to our rod pump wells in the Bakken,” a production engineer for Equinor’s Bakken asset, Jack Freeman, said in a statement. “The autonomous speed range management tool has leveraged the power of machine learning to optimize our wells by identifying and acting on real opportunities”
 Last year, Ambyint’s Series A funding round raised $11.5 million from several venture capital groups including Equinor Technology Ventures (formerly known as Statoil Technology Invest).",Journal of Petroleum Technology,2018,10.2118/1018-0030-JPT,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
8ac7f5308673d9cde6bfba44584baeabec1b45d6,https://www.semanticscholar.org/paper/8ac7f5308673d9cde6bfba44584baeabec1b45d6,iPaaS in Agriculture 4.0: An Industrial Case,"Current automation approaches in the Industry 4.0 have generated increased interest in the utilization of Integration Platforms as a Service (iPaaS) cloud architectures in order to unify and synchronize several systems, applications, and services in order to build smart solutions for automated and adaptive industrial process management. Existing iPaaS solutions present several out-of-the-box connectors and automation engines for easier integration of customers’ projects, but show issues regarding overall adaptation outside their scope, brand locking, and occasionally high prices. Moreover, existing platforms fail to respond adequately to the needs of deploying multiple decision models capable of offering automated or semi-automated management of processes, thanks to the integration of the large diversity of data and event sources as well as the different physical or logical action entities. With the popularization of open-source software and applications such as BPM Engines, Machine Learning libraries, and Integration suites and libraries, it is possible to develop a fully customizable and adaptable, open-source iPaaS that can be used both in and outside industrial applications. In this paper, we propose a generic iPaaS architecture implemented on the basis of several open source solutions boasting integration, interoperability, and automated decision-making capabilities in the domain of Agriculture 4.0. A proof-of-concept based on these solutions is presented, as well as a case study on MAÏSADOUR’s grain storage process with a comparison with the currently human-operated tasks.",2020 IEEE 29th International Conference on Enabling Technologies: Infrastructure for Collaborative Enterprises (WETICE),2020,10.1109/WETICE49692.2020.00018,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
befc75f886724b0e4359e002094bfc4e68a2b6fe,https://www.semanticscholar.org/paper/befc75f886724b0e4359e002094bfc4e68a2b6fe,A Multidirectional LSTM Model for Predicting the Stability of a Smart Grid,"The grid denotes the electric grid which consists of communication lines, control stations, transformers, and distributors that aids in supplying power from the electrical plant to the consumers. Presently, the electric grid constitutes humongous power production units which generates millions of megawatts of power distributed across several demographic regions. There is a dire need to efficiently manage this power supplied to the various consumer domains such as industries, smart cities, household and organizations. In this regard, a smart grid with intelligent systems is being deployed to cater the dynamic power requirements. A smart grid system follows the Cyber-Physical Systems (CPS) model, in which Information Technology (IT) infrastructure is integrated with physical systems. In the scenario of the smart grid embedded with CPS, the Machine Learning (ML) module is the IT aspect and the power dissipation units are the physical entities. In this research, a novel Multidirectional Long Short-Term Memory (MLSTM) technique is being proposed to predict the stability of the smart grid network. The results obtained are evaluated against other popular Deep Learning approaches such as Gated Recurrent Units (GRU), traditional LSTM and Recurrent Neural Networks (RNN). The experimental results prove that the MLSTM approach outperforms the other ML approaches.",IEEE Access,2020,10.1109/ACCESS.2020.2991067,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
70451a7852d0a062e1f1f8ad1e131981ec77fff6,https://www.semanticscholar.org/paper/70451a7852d0a062e1f1f8ad1e131981ec77fff6,Computer vision system for Working time estimation by Human Activities detection in video frames,"The goal of the research is to develop and to test methods for detecting people, parametric points for their hands and their current working tools in the video frames. The following algorithms are implemented: humans bounding boxes coordinates detection in video frames; human pose estimation: parametric points detection for each person in video frames; detection of the bounding boxes coordinates of the defined tools in video frames; estimation of which instrument the person is using at the certain moment. To implement algorithms, the existing computer vision models are used for the following tasks: Object detection, Pose estimation, Object overlaying. Machine learning system for working time detection based on computer vision is developed and deployed as a web-service. Recall, precision and f1-score are used as a metric for multi-classification problem. This problem is defined as what type of tool the person uses in a certain frame of video (Object Overlaying). Problem solution for action detection for the railway industry is new in terms of work activity estimation from video and working time optimization (based on human action detection). As the videos are recorded with a certain positioning of cameras and a certain light, the system has some limitations on how video should be filmed. Another limitation is the number of working tools (pliers, wrench, hammer, chisel). Further developments of the work might be connected with the algorithms for 3D modeling, modeling the activity as a sequence of frames (RNN, LSTM models), Action Detection model development, time optimization for the working process, recommendation system for working process from video activity detection.",Proceedings of the Institute for System Programming of the RAS,2020,10.15514/ispras-2020-32(1)-7,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
f67d60c60b396599cadcce80a6955377fabdb697,https://www.semanticscholar.org/paper/f67d60c60b396599cadcce80a6955377fabdb697,Simulation-driven Deep Classification of Bearing Faults from Raw Vibration Data,"The industry is moving towards maintenance strategies that consider component health, which require extensive collection and analysis of data. Condition monitoring methods that require manual feature extraction and analysis, become infeasible on an industrial scale. Machine learning algorithms can be used to automatically detect and classify faults, however, obtaining sufficient data for training is required for deep learning and other data-driven classification approaches. Data from healthy machine operation is generally available in abundance, while data from representative faultand operating conditions is limited. This limits both development and deployment of deep learning-based CM systems on an industrial scale. This paper addresses both the challenges of automated analysis and lack of training data. A deep learning classifier architecture utilizing 1-dimensional dilated convolutions is proposed. Dilation of the convolution kernel allows for analysis of raw vibration signals while simultaneously maintaining the receptive field of the classifier enough to capture temporal patterns. The proposed method performs classification in time domain on signal segments of 1 second or shorter. With knowledge of the bearing specification, artificial vibration signals with similar characteristics as an actual bearing fault can be created. In this work, generated fault signals are combined with healthy operational data to obtain training data for a deep classifier. Parameters of the vibration Martin Hemmer et al. This is an open-access article distributed under the terms of the Creative Commons Attribution 3.0 United States License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited. model is chosen as distributions rather than fixed values. By using a range parameters in the vibration model, the classifier learns to recognize temporal features from the training data that generalize to unseen data. The effectiveness of the proposed method is demonstrated by training classifiers on generated data and testing on real signals from faulty bearings at both low and high speed. One dataset containing seeded faults and three run-to-failure tests are used for the demonstration.",,2020,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
69d699ca6ac62c759c6372aa86a10756c8f509ce,https://www.semanticscholar.org/paper/69d699ca6ac62c759c6372aa86a10756c8f509ce,Price Optimization in Fashion E-commerce,"With the rapid growth in the fashion e-commerce industry, it is becoming extremely challenging for the E-tailers to set an optimal price point for all the products on the platform. By establishing an optimal price point, they can maximize overall revenue and profit for the platform. In this paper, we propose a novel machine learning and optimization technique to find the optimal price point at an individual product level. It comprises three major components. Firstly, we use a demand prediction model to predict the next day demand for each product at a certain discount percentage. Next step, we use the concept of price elasticity of demand to get the multiple demand values by varying the discount percentage. Thus we obtain multiple price demand pairs for each product and we have to choose one of them for the live platform. Typically fashion e-commerce has millions of products, so there can be many permutations. Each permutation will assign a unique price point for all the products, which will sum up to a unique revenue number. To choose the best permutation which gives maximum revenue, a linear programming optimization technique is used. We have deployed the above methods in the live production environment and conducted several AB tests. According to the AB test result, our model is improving the revenue by 1 percent and gross margin by 0.81 percent.",ArXiv,2020,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
9cad5744a6853fc7e306494a0357896204b2e9b2,https://www.semanticscholar.org/paper/9cad5744a6853fc7e306494a0357896204b2e9b2,Getting Passive Aggressive About False Positives: Patching Deployed Malware Detectors,"False positives (FPs) have been an issue of extreme importance for anti-virus (AV) systems for decades. As more security vendors turn to machine learning, alert deluge has hit critical mass with over 20% of all alerts resulting in FPs and, in some organizations, the number reaches half of all alerts [1]. This increase has resulted in fatigue, frustration, and, worst of all, neglect from security workers on SOC teams. A foundational cause for FPs is that vendors must build one global system to try and satisfy all customers, but have no method to adjust to individual local environments. This leads to outrageous, albeit technically correct, characterization of their platforms being 99.9% effective. Once these systems are deployed the idiosyncrasies of individual, local environments expose blind spots that lead to FPs and uncertainty. We propose a strategy for fixing false positives in production after a model has already been deployed. For too long the industry has tried to combat these problems with inefficient, and at times, dangerous allowlist techniques and excessive model retraining which is no longer enough. We propose using a technique called passive-aggressive learning to alter a malware detection model to an individual's environment, eliminating false positives without sharing any customer sensitive information. We will show how to use passive-aggressive learning to solve a collection of notoriously difficult false positives from a production environment without compromising the malware model's accuracy, reducing the total number of FP alerts by an average of 23x.",2020 International Conference on Data Mining Workshops (ICDMW),2020,10.1109/ICDMW51313.2020.00074,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
9384b351456209a871095c0a11bf3999e7b1b1c6,https://www.semanticscholar.org/paper/9384b351456209a871095c0a11bf3999e7b1b1c6,Deploying Artificial Intelligence for Component-Scale Multi-physical Field Simulation of Metal Additive Manufacturing,,,2020,10.1007/978-3-030-54334-1_19,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
1ab1617ec95700f6dd5b313f4175d44dcf63903d,https://www.semanticscholar.org/paper/1ab1617ec95700f6dd5b313f4175d44dcf63903d,COLOR-BASED OPTICAL CAMERA COMMUNICATION USING DEEP LEARNING,"When a network is not available or allowed (e.g., air gap), all benefits of digital communication and the Internet disappear. To enable substantial data transmission, techniques are described herein for using Machine Learning to embed data into color images and to read it with camera-equipped devices. In addition to data transmission, this enables new applications such as embedding of content on packaging (instructions or runnable code and configurations directly on devices), physical documents with selective encryption (role-based permission to read parts of a document), protection from digital tracking, prevention of tampering on physical documents, and more. DETAILED DESCRIPTION Passing information to a device without cabled or radio-based wireless connectivity is an ongoing problem. One approach is to use a different bands of the radio spectrum, such as visible light. This is the approach taken in the fields of Visible Light Communication (VLC) and Optical Camera Communication (OCC). VLC has emerged as a promising technology for wireless communication owing to advantages such as bandwidth, license, coexistence, and security. However, VLC requires an active source (i.e., emitting light) to modulate the signals. On the other hand, OCC can also operate on passive sources, that is, data printed on pictures, such as Quick-Response (QR) codes. A QR code® is a type of two-dimensional matrix barcode first designed for the automotive industry in Japan. A barcode is a machine-readable optical label that contains information, typically about the item to which it is attached. A QR code uses four standardized encoding modes (numeric, alphanumeric, byte/binary, and Kanji) to efficiently store data and consists of black squares arranged in a square grid on a white background, which can be read by an imaging device such as a camera and processed using Reed-Solomon error correction until the image can be appropriately interpreted. The required data is then extracted from patterns that are present in both horizontal and vertical 2 Rohrseitz: COLOR-BASED OPTICAL CAMERA COMMUNICATION USING DEEP LEARNING Published by Technical Disclosure Commons, 2018 2 5660 components of the image. QR codes became popular outside the automotive industry due to their fast readability and greater storage capacity compared to standard Universal Product Code (UPC) barcodes. Applications include product tracking, item identification, time tracking, document management, and general marketing. The amount of data a standard QR code can carry is limited by the number of its pixels. To increase the payload, the number of pixels needs to be increased: either by making the QR code bigger or by making the pixels smaller. Both options are constrained by the measurement devices: to read more data, either the field of view or the pixel resolution must be larger. Another approach to increase the payload is to increase the amount of data per pixel. This can be accomplished by using more than the two black and white states, for instance greyscale and color. The more shades that can be robustly measured, the larger the payload. Accurately measuring color requires robustness to variations due to incoming light. Without compensation, the intensity and the color of the light source modify what the photoreceptors (“pixels”) of a camera can measure. This problem of “color constancy” remains largely unsolved by technical systems. Even though biological vision achieves color constancy, its precise mechanisms are not yet fully understood. Functionally, it is clear that there is an evolutionary advantage in maintaining colors constant in forests speckled by light or at different times of day. To mitigate for this effect, current techniques use a reduced color palette, for instance four colors. Given the ubiquity of QR codes, the application potential of using more colors to increase bandwidth in optical wireless transmission information is vast. For one, when standard networking technologies are unavailable or not allowed (e.g., air gaps), the enormous societal and business benefits of digital communication and the Internet disappear. Accurately measuring color would enable networking using, for instance, QR codes, pictures, or electronic displays as data emitters and cameras or other measurement devices as receivers. Such a communication system also carries important technical benefits in terms of latency and availability. The techniques described herein use a deep neural network trained specifically to measure color in the context of data transmission. Although some current techniques relevant to OCC make use of neural networks, their role is not to measure color. Empirical 3 Defensive Publications Series, Art. 1391 [2018] https://www.tdcommons.org/dpubs_series/1391 3 5660 results indicate very high accuracy (98%) in a task that had not yet been solved by a technical system. The deep neural network is capable of accurately measuring the color of a region of the field of view by including in the processing the adjacent areas (in contrast, cameras are designed to convey only the measured color of one specific region, the “pixel”). A Machine Learning process prior to deployment determines which locations and calculations the neural network needs to perform. To create the deep neural network capable of accurately measuring color, a workflow of three high-level steps is carried out: (1) creation of a relevant dataset, (2) training of the deep neural network, and (3) deployment on a camera-carrying device. The first step involves acquiring a most complete and diverse dataset. To be robust against the main problem of accurately measuring color namely changes in illumination a deep neural network is trained on a dataset that contains multiple pictures of the colors to be recognized, each under different light intensities and colors. The dataset is stored with metadata that describe the characteristics to be learned. In the second step, an appropriate deep learning architecture is designed and parametrized. It is typical of Machine Learning to have many variants of architecture and parameters that lead to a similarly effective systems. The dataset may also be different and still be effective. It is the practitioner’s role to find the right combination to achieve the intended goals. The third step involves transferring the learned model to a deployment system for inference (for instance, a smartphone or an industrial measurement system), whose camera characteristics must match those of the camera used for acquiring the dataset. For experimental verification, a dataset was created, the neural network trained, and a measurement application implemented on a mobile device. To capture the dataset, an array of colored elements were placed under a camera, surrounded by a set of programmable color light bulbs. An application was run which at fixed time intervals (two seconds) automatically changes the values of hue, brightness, and saturation by predefined amounts, while capturing pictures with the camera. To capture the metadata, the standard procedure was followed of storing each picture file in a folder corresponding to the incoming light characteristics. 4 Rohrseitz: COLOR-BASED OPTICAL CAMERA COMMUNICATION USING DEEP LEARNING Published by Technical Disclosure Commons, 2018 4 5660 To train the neural network on the dataset, the following characteristics were used:  Type: convolutional neural network  Activation function: Rectified Linear Unit (ReLU); If input <= 0, then output = 0, otherwise if input >0, then output = input)  Optimizer: Adam (ADAptive with Momentum https://arxiv.org/abs/1412.6980)  Loss function: categorical crossentropy (quantifies the difference between two probability distributions – http://deeplearning.net/software/theano/library/tensor/nnet/nnet.html#theano.tenso r.nnet.nnet.categorical_crossentropy)",,2018,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
2524e8ea6f100cfe50c794c9d599fecce443ff82,https://www.semanticscholar.org/paper/2524e8ea6f100cfe50c794c9d599fecce443ff82,P 01 – Deep learning based object detection and classification,"With the rapid advancement of deep learning and high performance computing technologies, data scientists can now use GPUs, GPU clusters or purpose-built hardware systems for deep learning to develop and deploy machine learning based applications such as massive image classification and video analytics. The aim of this project is to develop a deep learning based image classifier using the latest high performance platform for visual computing. The work will include assessing the existing deep learning software packages, deploy a selected package using the platform, and implement the image classification application using the deep learning package. The practical outcome from this work will be increased capabilities for automated fishery monitoring to support sustainable fisheries. The project has close links to computer science, mathematics and engineering, and can help bridge the gap between your courses and developing a real scientific application. The suitable candidate will have opportunities to have access to world class facilities and work alongside CSIRO senior scientists while you are enjoying generous personal development and learning opportunities. Skills required: • Programming language C or C++ preferred. • Image processing/image analysis or computer vision course or experience is preferred, but not compulsory. • Quick learner. • Good communication skills. Developmental outcomes for student: • Software that can be used by others. • Exposure to high-impact research, scientific expertise in multiple disciplines, and scientific infrastructure; • Improved image processing and computer vision skills; • Improved skills in scientific communication (e.g. writing scientific reports and oral presentations). Projects – Trustworthy Systems • Formal methods o Theorem proving o Protocols o Expressiveness o Blockchain • Systems o Kernels Middleware o Security • Programming Languages Formal Methods / Theorem Proving Project P02 Automating Formal Proofs Gerwin Klein, Daniel Matichuck Abstract: Isabelle is an interactive theorem prover which combines automated and human reasoning through the use of proof methods (or tactics). These methods allow users to make highlevel decisions on how to progress in a proof without worrying about the formal details. Powerful proof methods are required for large proof undertakings, such as the L4.verified project at Data61. Eisbach is an in-development prototype of a high-level language for writing proof methods in Isabelle, which is historically done in Standard ML. It is influenced by Coq’s Ltac, but distinguishes itself by leveraging Isabelle’s existing automation and backtracking infrastructure. The aim of this project is to investigate useful applications for Eisbach, with several possible domains to explore: An existing suite of Data61-developed separation logic tactics could be reimplemented and extended, or a similar investigation can be done against the verificationcondition generator used in the L4.verified proof; Or a novel proof method can be developed solve, for example, word arithmetic proofs that appear frequently in the verification of C programs. Novelty: This will be one of the first larger-scale uses of the new Eisbach proof language and the resulting feedback will contribute to its further development. Outcome: Automated proof tactics for previously manual proofs in one or more of the specified application domains, ideally reducing proof size and time for those applications. Reference Material Links: Trustworthy Systems Research Group (TS): http://trustworthy.systems seL4: http://ertos.nicta.com.au/research/sel4/ Isabelle: http://mirror.cse.unsw.edu.au/pub/isabelle/ P03 Formal Verification of multi-threaded embedded application software June Andronick, Corey Lewis Abstract: eChronos [0] is a small embedded OS for micro-controllers. It is commercially used in medical devices and is embedded in high-assurance autonomous flying vehicles (quadcopters) demonstrated in HACMS, a large DARPA-funded project, in collaboration with industry and university partners from the US. In Data61's eChronos verification project, we aim at proving strong guarantees about eChronos correct behavior, by means of formal (mathematical) proofs, machine-checked in the Isabelle/HOL theorem prover. The long-term goal is to provide developpers of embedded applications using eChronos with a formal and verified specification of the OS API functions used for synchronisation (semaphores, mutexes, etc). The challenge lies in providing the right abstraction level for the formal API and provide a usable framework for application code verification. This project will investigate eChronos-based application verification, via small case-studies, which could be derived or inspired from application code from the HACMS project, namely the SMACCMPilot open-source autopilot software from Galois [3]. [0] /projects/TS/echronos/ [1] ""Controlled owicki-gries concurrency: reasoning about the preemptible eChronos embedded operating system"". MARS'15. /publications/nictaabstracts/Andronick_LM_15.abstract.pml [2] ""Proof of OS scheduling behavior in the presence of interrupt-induced concurrency"". To appear in ITP'16. [3]http://smaccmpilot.org/ Novelty: Formal verification of real-world embedded application on a verified embedded OS API. Outcome:The expected outcome of the project is the experimental verification of a case-study application running on eChronos, exhibiting the required formalised API from eChronos. P04 Improving automation in concurrent software verification June Andronick, Corey Lewis Abstract: eChronos [0] is a small embedded OS for micro-controllers. It is commercially used in medical devices and is embedded in high-assurance autonomous flying vehicles (quadcopters) demonstrated in HACMS, a large DARPA-funded project, in collaboration with industry and university partners from the US. In Data61's eChronos verification project, we aim at proving strong guarantees about eChronos correct behavior, by means of formal (mathematical) proofs, machine-checked in the Isabelle/HOL theorem prover. The challenge lies in the concurrency due to eChronos running with interrupts enabled, including during scheduling operations, to ensure low latency. We have successfully proved, at a model level, the correctness of eChronos scheduling behavior in presence of interrupt-induced concurrency [1,2]. This project would look at increasing the automation and scalability of the framework. Opportunities for such improvements include (but are not limited to) increased reuse of already proven facts, investigation of more modular (but less fine-grain) approaches, more use of Isabelle automation, increased compositionality of the proof process, etc. [0] /projects/TS/echronos/ [1] ""Controlled owicki-gries concurrency: reasoning about the preemptible eChronos embedded operating system"". MARS'15. /publications/nictaabstracts/Andronick_LM_15.abstract.pml [2] ""Proof of OS scheduling behavior in the presence of interrupt-induced concurrency"". To appear in ITP'16. Novelty: Your work will contribute to the general feasibility and scalability of practical concurrent software verification. Outcome: Your work will directly impact the efficiency of the framework and proofs developed for the verification of eChronos. P05 Implement and Verify a CakeML Compiler Optimisation",,2016,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
7079788671d63a9061c01d85275530f2bcd46039,https://www.semanticscholar.org/paper/7079788671d63a9061c01d85275530f2bcd46039,Pandas DataFrames for a FAST binned analysis at CMS,"Binned data frames are a generalisation of multi-dimensional histograms, represented in a tabular format with one category per row containing the labels, bin contents, uncertainties and so on. Pandas is an industry-standard tool, which provides a data frame implementation complete with routines for data frame manipultion, persistency, visualisation, and easy access to “big data” scientific libraries and machine learning tools. FAST (the Faster Analysis Software Taskforce) has developed a generic approach for typical binned HEP analyses, driving the summary of ROOT Trees to multiple binned DataFrames with a yaml-based analysis description. Using Continuous Integration to run subsets of the analysis, we can monitor and test changes to the analysis itself, and deploy documentation automatically. This report describes this approach using examples from a public CMS tutorial and details the benefit over traditional methods.",EPJ Web of Conferences,2019,10.1051/epjconf/201921406035,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
3f4332ddbb561742f15149dacee438dbe871611e,https://www.semanticscholar.org/paper/3f4332ddbb561742f15149dacee438dbe871611e,Towards a Lightweight Detection System for Cyber Attacks in the IoT Environment Using Corresponding Features,"The application of a large number of Internet of Things (IoT) devices makes our life more convenient and industries more efficient. However, it also makes cyber-attacks much easier to occur because so many IoT devices are deployed and most of them do not have enough resources (i.e., computation and storage capacity) to carry out ordinary intrusion detection systems (IDSs). In this study, a lightweight machine learning-based IDS using a new feature selection algorithm is designed and implemented on Raspberry Pi, and its performance is verified using a public dataset collected from an IoT environment. To make the system lightweight, we propose a new algorithm for feature selection, called the correlated-set thresholding on gain-ratio (CST-GR) algorithm, to select really necessary features. Because the feature selection is conducted on three specific kinds of cyber-attacks, the number of selected features can be significantly reduced, which makes the classifiers very small and fast. Thus, our detection system is lightweight enough to be implemented and carried out in a Raspberry Pi system. More importantly, as the really necessary features corresponding to each kind of attack are exploited, good detection performance can be expected. The performance of our proposal is examined in detail with different machine learning algorithms, in order to learn which of them is the best option for our system. The experiment results indicate that the new feature selection algorithm can select only very few features for each kind of attack. Thus, the detection system is lightweight enough to be implemented in the Raspberry Pi environment with almost no sacrifice on detection performance.",,2020,10.3390/electronics9010144,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
9df5c75cbd9617d4b2855464835539ffe48ab58b,https://www.semanticscholar.org/paper/9df5c75cbd9617d4b2855464835539ffe48ab58b,On the Generation of Anomaly Detection Datasets in Industrial Control Systems,"In recent decades, Industrial Control Systems (ICS) have been affected by heterogeneous cyberattacks that have a huge impact on the physical world and the people’s safety. Nowadays, the techniques achieving the best performance in the detection of cyber anomalies are based on Machine Learning and, more recently, Deep Learning. Due to the incipient stage of cybersecurity research in ICS, the availability of datasets enabling the evaluation of anomaly detection techniques is insufficient. In this paper, we propose a methodology to generate reliable anomaly detection datasets in ICS that consists of four steps: attacks selection, attacks deployment, traffic capture and features computation. The proposed methodology has been used to generate the Electra Dataset, whose main goal is the evaluation of cybersecurity techniques in an electric traction substation used in the railway industry. Using the Electra dataset, we train several Machine Learning and Deep Learning models to detect anomalies in ICS and the performed experiments show that the models have high precision and, therefore, demonstrate the suitability of our dataset for use in production systems.",IEEE Access,2019,10.1109/ACCESS.2019.2958284,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
426f4f41ba3be6453a782cf5dd7f0c170d151ede,https://www.semanticscholar.org/paper/426f4f41ba3be6453a782cf5dd7f0c170d151ede,Welcome Message from SenSys-ML'20 Chairs,"We are very excited to welcome you to the second ACM/IEEE International Workshop on Machine Learning on Edge in Sensor Systems (SenSys-ML 2020). SenSys-ML’20 will be held in conjunction with the CPS-IoT Week 2020 and focuses on work that combines sensor signals from the physical world with machine learning, particularly in ways that are distributed to the device or use edge and fog computing. The development and deployment of ML at the very edge remains a technological challenge constrained by computing, memory, energy, network bandwidth, and data privacy and security limitations. This is especially true for battery-operated devices and always-on use cases and applications. In recent years this has gained attention from both academia and industry and many TinyML initiatives have been started focusing both in hardware and software advancements. This workshop will provide a forum for sensing, networking and machine learning researchers to present and share their latest research on building machine learning-enabled sensor systems. Sensys-ML focuses on providing extensive feedback on Work-In-Progress papers involving machine learning (TinyML/ UltraML) on sensor systems.",2020 IEEE Second Workshop on Machine Learning on Edge in Sensor Systems (SenSys-ML),2020,10.1109/sensysml50931.2020.00007,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
de3259f91d4ae41430b9a2da536b761e483d5bf6,https://www.semanticscholar.org/paper/de3259f91d4ae41430b9a2da536b761e483d5bf6,Auto Content Moderation in C2C e-Commerce,"Consumer-to-consumer (C2C) e-Commerce is a large and growing industry with millions of monthly active users. In this paper, we propose auto content moderation for C2C eCommerce to moderate items using Machine Learning (ML). We will also discuss practical knowledge gained from our auto content moderation system. The system has been deployed to production at Mercari since late 2017 and has significantly reduced the operation cost in detecting items violating our policies. This system has increased coverage by 554.8 % over a rule-based approach.",OpML,2020,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
93354b7f6aeaf7c312766134e75d9b395f602529,https://www.semanticscholar.org/paper/93354b7f6aeaf7c312766134e75d9b395f602529,Railroad semantic segmentation on high-resolution images,"Recent advances in machine learning research could significantly alter the railroad industry by deploying fully autonomous trains. To achieve effective interaction between self-driving trains and the environment, an accurate long-range railway detection should be provided. In this paper, we propose a framework for the rail tracks segmentation on high-resolution images ($2168\times 4096$). The announced approach accelerates inference speed 6 times, by using two neural networks. The proposed architecture and its training approach provide a long-range railway segmentation within 150 meters, achieving 20 fps. Also, we propose an auxiliary algorithm detecting possible paths among all the found ones. To determine which data labeling approach has a higher impact, additional experiments were performed. The proposed framework provides a balanced tradeoff between computing efficiency and performance in the railroad segmentation problem.",2020 IEEE 23rd International Conference on Intelligent Transportation Systems (ITSC),2020,10.1109/ITSC45102.2020.9294722,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
0a0f19426bdf6d0b5bb391fdf107b243fb94d1e6,https://www.semanticscholar.org/paper/0a0f19426bdf6d0b5bb391fdf107b243fb94d1e6,Improved Detection of Adversarial Images Using Deep Neural Networks,"Machine learning techniques are immensely deployed in both industry and academy. Recent studies indicate that machine learning models used for classification tasks are vulnerable to adversarial examples, which limits the usage of applications in the fields with high precision requirements. We propose a new approach called Feature Map Denoising to detect the adversarial inputs and show the performance of detection on the mixed dataset consisting of adversarial examples generated by different attack algorithms, which can be used to associate with any pre-trained DNNs at a low cost. Wiener filter is also introduced as the denoise algorithm to the defense model, which can further improve performance. Experimental results indicate that good accuracy of detecting the adversarial examples can be achieved through our Feature Map Denoising algorithm.",ArXiv,2020,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
7f6d2aa2f3b29184c67db03943f798304b6c7c7c,https://www.semanticscholar.org/paper/7f6d2aa2f3b29184c67db03943f798304b6c7c7c,Bridging the gap from AI ethics research to practice,"The study of fairness in machine learning applications has seen significant academic inquiry, research and publication in recent years. Concurrently, technology companies have begun to instantiate nascent program in AI ethics and product ethics more broadly. As a result of these efforts, AI ethics practitioners have piloted new processes to evaluate and ensure fairness in their machine learning applications. In this session, six industry practitioners, hailing from LinkedIn, Yoti, Microsoft, Pymetrics, Facebook, and Salesforce share insights from the work they have undertaken in the area of fairness, what has worked and what has not, lessons learned and best practices instituted as a result. • Krishnaram Kenthapadi presents LinkedIn's fairness-aware reranking for talent search. • Julie Dawson shares how Yoti applies ML fairness research to age estimation in their digital identity platform. • Hanna Wallach contributes how Microsoft is applying fairness principles in practice. • Lewis Baker presents Pymetric's fairness mechanisms in their hiring algorithm. • Isabel Kloumann presents Facebook's fairness assessment framework through a case study of fairness in a content moderation system. • Sarah Aerni contributes how Salesforce is building fairness features into the Einstein AI platform. Building on those insights, we discuss insights and brainstorm modalities through which to build upon the practitioners' work. Opportunities for further research or collaboration are identified, with the goal of developing a shared understanding of experiences and needs of AI ethics practitioners. Ultimately, the aim is to develop a playbook for more ethical and fair AI product development and deployment.",FAT*,2020,10.1145/3351095.3375680,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
53f16cd9c64248aea915c56390815f1652ea7da9,https://www.semanticscholar.org/paper/53f16cd9c64248aea915c56390815f1652ea7da9,Toward Smart Urban Development Through Intelligent Edge Analytics,,Integration of WSN and IoT for Smart Cities,2020,10.1007/978-3-030-38516-3_8,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
89f653f173fb57b8bfb4d66237d9a1c0ffec8fb6,https://www.semanticscholar.org/paper/89f653f173fb57b8bfb4d66237d9a1c0ffec8fb6,A Learning-Based Framework for Two-Dimensional Vehicle Maneuver Prediction over V2V Networks,"Situational awareness in vehicular networks could be substantially improved utilizing reliable trajectory prediction methods. More precise situational awareness, in turn, results in notably better performance of critical safety applications, such as Forward Collision Warning (FCW), as well as comfort applications like Cooperative Adaptive Cruise Control (CACC). Therefore, vehicle trajectory prediction problem needs to be deeply investigated in order to come up with an end to end framework with enough precision required by the safety applications’ controllers. This problem has been tackled in the literature using different methods. However, machine learning, which is a promising and emerging field with remarkable potential for time series prediction, has not been explored enough for this purpose. In this paper, a two-layer neural network-based system is developed which predicts the future values of vehicle parameters, such as velocity, acceleration, and yaw rate, in the first layer and then predicts the twodimensional, i.e. longitudinal and lateral, trajectory points based on the first layer’s outputs. The performance of the proposed framework has been evaluated in realistic cut-in scenarios from Safety Pilot Model Deployment (SPMD) dataset and the results show a noticeable improvement in the prediction accuracy in comparison with the kinematics model which is the dominant employed model by the automotive industry. Both ideal and nonideal communication circumstances have been investigated for our system evaluation. For non-ideal case, an estimation step is included in the framework before the parameter prediction block to handle the drawbacks of packet drops or sensor failures and reconstruct the time series of vehicle parameters at a desirable frequency.","2017 IEEE 15th Intl Conf on Dependable, Autonomic and Secure Computing, 15th Intl Conf on Pervasive Intelligence and Computing, 3rd Intl Conf on Big Data Intelligence and Computing and Cyber Science and Technology Congress(DASC/PiCom/DataCom/CyberSciTech)",2017,10.1109/DASC-PICom-DataCom-CyberSciTec.2017.39,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
bf4971e25af2ef23aabb873de937fd3c93c4f2cb,https://www.semanticscholar.org/paper/bf4971e25af2ef23aabb873de937fd3c93c4f2cb,Design of an Optimized Multicast Routing Algorithm for Internet of Things,"Internet of Things (IoT) is a fast- growing technology in on-going research field that includes wireless sensor networks, cloud computing, big data analytics, ubiquitous computing, distributed decentralized systems, pervasive computing, embedded systems, mobile computing, machine learning etc. The above mentioned fields are mainly connected with IoT smart portable devices such as smartphones, home appliances, healthcare device, smart vehicle devices automation industry devices, etc. Though IoT enabled devices has been increased in many fields, the industries still faces many problem with connectivity issues because of several factors like mobility nature of devices; limited processing power and resource availability which includes energy, bandwidth constraints, routing cost and end to end delay; communication between node to node via intermediate mobile nodes towards destination may also fail links frequently, there by affecting the network performance. These limitations of existing topology based on reactive tree and mesh based routing protocols create challenging task while designing an optimized stable routing algorithm for IoT. In such a situation, resource optimization is an essential task to be performed by the IoT networks. In the proposed work resource optimization was done by Designed Optimized Multicast Routing Algorithm (DOMRA) for IoT. The DOMR algorithm implemented has route discovery process with nodes positions, directions of nodes, velocities of nodes, and then the path stability bases to overcome the connectivity issues. The proposed algorithm focusing to deploy various real time IoT enabled applications such as smart home automation, smart cites, smart agriculture, automation industry etc. To finalize the simulation results shows maximized system throughput, goodput, packet delivery ratio, network lifetime, network routing performance and reduced control overheads. The proposed algorithm hence produced better routing performance when compared with other existing algorithm in wireless networks.",International Journal of Recent Technology and Engineering,2019,10.35940/ijrte.b3372.078219,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
b870a4141ce27edae65764f245331dc6e6c9de4b,https://www.semanticscholar.org/paper/b870a4141ce27edae65764f245331dc6e6c9de4b,2.4 Automated Anomaly Detection Through Assembly and Packaging Process,"In the semiconductor industry the desired quality and effectiveness in the process of assembling integrated circuits is nowadays at the limit and without safety margin. To achieve important competitive advantages, this process must be continuously optimized and adjusted. Such process is indeed strongly dependent on parameters that are distributed among various control technology assemblies, materials, and the environment. However, the current inspection tools deployed for defect detection through assembly and packaging process are mainly based on rigid and simple rules. The latter are handcrafted by engineers, which can only extract shallow features. Therefore, the accuracy of classiﬁcation by tools is quite low, which provides incomplete information for root cause investigation and can cause yield-loss costs due to over reject. Hence, automatic inspection tools for visual defect detection, acting as ﬁnal quality gate before shipping to end customers is very demanding. Therefore, a deviation detection model based on machine learning is developed. On the other side, due to the lack of existing labelled images, an anomaly detection is proposed, in some cases as an assistant tool for collecting defect images with less effort. Results show that artiﬁcial intelligent (AI) solutions can achieve a better performance than the classical tools and overcome the human ability in detecting the deviation in the data. Automated Anomaly Detection Through Assembly and Packaging Process Hence, AI can be used for decreasing the yield-loss, improving quality of the product and greatly reduce labour intensity.",,2021,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
4cb659cf5a8dea8b86263c65ec613f48c2426c51,https://www.semanticscholar.org/paper/4cb659cf5a8dea8b86263c65ec613f48c2426c51,"Analyzing Dilemmas Posed by Artificial Intelligence and 4IR Technologies Requires using all Available Models, Including the Existing International Human Rights Framework and Principles of AI Ethics","We are living in the epoch referred to as the ‘4th industrial revolution'. The 4th industrial Revolution (4IR) is a development characterized by a fusion of technologies that blur the digital, physical, and biological spheres (e.g., cyberspace, virtual and augmented reality, body-machine interface and robotics). 
 
Certain is the guaranteed ubiquitous adoption of these technologies, and futurism. Where the former is a reference to the increasing use and normalization of such technologies in everyday life, government service provision and industry. The latter is a reference to the philosophical/science fiction discussions that are emerging as a result of these changes (e.g. debates around the ‘singularity’, transhumanism, and posthumanism – often presented in utopian/dystopia terms). As such, the definition of digital ethics can be expanded and expressed in terms of the impacts of new digital technologies, through analysis of potential opportunities and risks in contemporary and future contexts. 
 
Many are working on forward‑looking policy frameworks and governance protocols, with broad multistakeholder engagement and buy‑in, to accelerate the adoption of emerging technologies in the global public interest, such as artificial intelligence (AI) and machine learning (ML) blockchain, 5G, data analytics, quantum computing, autonomous vehicles, synthetic biology, the internet of things (IoT), and killer robots or autonomous weapons systems (AWS). We have gained insight into the unequal distribution of the positive and negative impacts of AI on human rights throughout society, and have begun to explore the power of the human rights framework to address these disparate impacts. 
 
Although internationally recognized laws and standards on human rights provide a common standard of achievement for all people in all countries, more work is needed to understand how they can be best applied in the context of disruptive technology. 
 
AI systems raise myriad questions for society and democracy, only some of which are covered or addressed by existing laws. In order to fill these perceived gaps, a vocal group of governments, industry players, academics, and civil society actors have been promoting principles or frameworks for ethical AI. 
 
COVID-19 accelerated the use of AI in all countries and all fields. The pandemic accelerated the transition to a society that is increasingly based on the use of AI. This also increased the threats new risks related to human rights in the context of AI deployment. The human rights implications of governments' aggressive measures targeting the spread of COVID-19-related misinforation is also discussed. 
 
The question of whether corporations can act ethically is particularly relevant for Big Tech. Many of these firms are oligopolies that individuals and governments alike depend on completely, though they have little to no capacity to independently remedy issues when they arise, as Project Maven showed. Artificial intelligence and automated decision-making tools are increasing in power and centrality, and technology companies retain large troves of private data that it sells. These companies are at the forefront of technological innovation and may be caught up with the factual question of what can be done rather than the normative question of whether it should be done. All these issues arise in a field where there is little to no government regulation or intervention. The threats AI poses to society are so new, that the legal system is struggling to impose sufficient values and restrictions. Thus, a coherent approach to addressing AI ethics, values and consequences is, indeed, urgently needed. 
 
In May 2019, 42 countries adopted the Organization for Economic Co-operation and Development (OECD) AI Principles, a legal recommendation that includes five principles and five recommendations related to the use of AI. To ensure the successful implementation of the Principles, the OECD launched the AI Policy Observatory in February 2020. The Observatory publishes practical guidance about how to implement the AI Principles, and supports a live database of AI policies and initiatives globally. It also compiles metrics and measurement of global AI development and uses its convening power to bring together the private sector, governments, academia, and civil society. 
 
The AI ethics and governance initiatives discussed are cause for optimism that the global community will use all available models and brainpower for analysis and ultimately global governance of AI.",SSRN Electronic Journal,2021,10.2139/ssrn.3874279,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
6eea25b62ed1c6691d71e978206c96fe1de4a922,https://www.semanticscholar.org/paper/6eea25b62ed1c6691d71e978206c96fe1de4a922,Editorial: Advanced Industrial Networks with IoT and Big Data,,Mob. Networks Appl.,2019,10.1007/S11036-019-01228-4,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
3d5eae43c452fe387eb7c4eb3dbf42b9623d0462,https://www.semanticscholar.org/paper/3d5eae43c452fe387eb7c4eb3dbf42b9623d0462,DistDL: A Distributed Deep Learning Service Schema with GPU Accelerating,,APWeb,2015,10.1007/978-3-319-25255-1_65,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
472c758372469c8278256a0161ef8206adcb26d3,https://www.semanticscholar.org/paper/472c758372469c8278256a0161ef8206adcb26d3,An Assistive Bot for Healthcare Using Deep Learning: Conversation-as-a-Service,,Advances in Intelligent Systems and Computing,2018,10.1007/978-981-13-1708-8_10,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
64967f6503ecbfe1ccdaa14e607ecd7705c35f16,https://www.semanticscholar.org/paper/64967f6503ecbfe1ccdaa14e607ecd7705c35f16,"Tuplex: Robust, Efficient Analytics When Python Rules","Spark became the defacto industry standard as an execution engine for data preparation, cleaning, distributed machine learning, streaming and, warehousing over raw data. However, with the success of Python the landscape is shifting again; there is a strong demand for tools which better integrate with the Python landscape and do not have the impedance mismatch like Spark. In this paper, we demonstrate Tuplex (short for tuples and exceptions), a Pythonnative data preparation framework that allows users to develop and deploy pipelines faster and more robustly while providing bare-metal execution times through code compilation whenever possible. PVLDB Reference Format: Leonhard F. Spiegelberg and Tim Kraska. Tuplex: Robust, Efficient Analytics When Python Rules. PVLDB, 12(12): 1958-1961, 2019. DOI: https://doi.org/10.14778/3352063.3352109",Proc. VLDB Endow.,2019,10.14778/3352063.3352109,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
c77b908a48b9380765d968f6259f3d3d11fd9c30,https://www.semanticscholar.org/paper/c77b908a48b9380765d968f6259f3d3d11fd9c30,Big Data Intelligence Using Distributed Deep Neural Networks,"Large amount of data is often required to train and deploy useful machine learning models in industry. Smaller enterprises do not have the luxury of accessing enough data for machine learning, For privacy sensitive fields such as banking, insurance and healthcare, aggregating data to a data warehouse poses a challenge of data security and limited computational resources. These challenges are critical when developing machine learning algorithms in industry. Several attempts have been made to address the above challenges by using distributed learning techniques such as federated learning over disparate data stores in order to circumvent the need for centralised data aggregation. This paper proposes an improved algorithm to securely train deep neural networks over several data sources in a distributed way, in order to eliminate the need to centrally aggregate the data and the need to share the data thus preserving privacy. The proposed method allows training of deep neural networks using data from multiple de-linked nodes in a distributed environment and to secure the representation shared during training. Only a representation of the trained models (network architecture and weights) are shared. The algorithm was evaluated on existing healthcare patients data and the performance of this implementation was compared to that of a regular deep neural network trained on a single centralised architecture. This algorithm will pave a way for distributed training of neural networks on privacy sensitive applications where raw data may not be shared directly or centrally aggregating this data in a data warehouse is not feasible.",ArXiv,2019,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
c853b39793005ed00bc58f8903caebad9f3a3971,https://www.semanticscholar.org/paper/c853b39793005ed00bc58f8903caebad9f3a3971,Study on Deep Learning as a Powerful Technology that Revolutionizing Automation in Industries,": Smart production refers to the usage of superior records analytics to complement bodily technology for enhancing device performance and choice making. With the extensive deployment of sensors and Internet of Things, there is a growing need of managing large manufacturing facts characterized through excessive quantity, excessive velocity, and high range. Deep gaining knowledge of present’s superior analytics gear for processing and analyzing huge production facts. This paper affords a comprehensive survey of typically used deep mastering algorithms and discusses their programs in the direction of making production “clever”. The evolvement of deep mastering technologies and their benefits over traditional gadget gaining knowledge of are first of all mentioned. Subsequently, computational strategies based totally on deep getting to know are provided specifically purpose to improve device overall performance in manufacturing. Several consultant deep mastering models are comparably mentioned. Finally, emerging topics of research on deep learning are highlighted, and destiny trends and challenges related to deep getting to know for smart production are summarized. Abstract : Data is a very valuable asset in the world today. The economics of data are based on the idea that data value can be extracted through the use of analytics. Through Big data and analytics are still in their initial growth stage, their importance cannot be undervalued. As big data starts to expand and grow, Importance of big data analytics will continue to grow in everyday lives, both personal and business. In addition, the size and volume of data is increasing every single day, making it important to address the manner in which big data is addressed every day . A huge repository of terabytes of data is generated every day from modern information systems and digital technologies such as Internet of Things and cloud computing. Analysis of these massive data requires a lot of efforts at multiple levels to extract knowledge for decision making. Therefore, big data analysis is a current area of research and development. The basic objective of this paper is to explore the potential impact of big data technologies and challenges associated with it. This paper provides a platform to explore big data at numerous stages. Additionally, it opens a new horizon for researchers to develop the solution, based on the challenges and open research issues. Abstract : In this paper we focus on the initial-value problem of linear plate equations with memory in multi-dimensions, the decay structure of which is of regularity-loss property. We obtain fundamental solutions by using Fourier transform and Laplace transform. By virtue of the point-wise estimate of solutions in the Fourier space, we gain estimates and properties of solution operators, by utilizing which decay estimates of solutions to the linear problem are obtained and the decay rate can be as large as desired if the initial data are sufficiently smooth. A BSTRACT : I O T made the present generation to connect with the different network of devices for exchanging of the data. Nowadays it had made compulsion on wearing the helmet while riding. In our paper we introduce a helmet which is made smart using the latest IOT technologies. The papers’ main objective is to build a safety system that is integrated with the smart helmet and intelligent bike which reduce the chances of two-wheeler accident and drunk drive cases. The pressure sensor check whether the person is wearing helmet or not. Alcohol sensors (Gas sensor) which is installed at the bottom of the helmet detect the alcoholic content in riders’ breath. If the rider is not wearing the helmet or if there is any alcohol content found in rider’s breath, the bike remains off. The bike will start if and only if the rider wears the helmet provided with no alcoholic content present. When the rider crashes, helmet hits the ground, sensors detect the motion of helmet and reports the occurrence of an accident. It sends information of corresponding location and message to the registered number through GPS & GSM module respectively. And along with it we are implementing cooling system inside the helmet which would help the rider to stay cool during the climatic changes which in turn reduces the irritation that is created from the helmet. Abstract In order to safeguard their critical systems against network intrusions, organisations deploys multiple Network Intrusion Detection System (NIDS) to detect malicious packets embedded in network traffic based on anomaly and misuse detection approaches. The existing NIDS deal with a huge amount of data that contains null values, incomplete information, and irrelevant features that affect the detection rate of the IDS, consumes high amount of system resources, and slowdown the training and testing process of the IDS. In this paper, a new feature selection model is proposed based on hybrid feature selection techniques (information gain, correlation, chi squere and gain ratio) and Principal Component Analysis (PCA) for feature reduction. This study employed data mining and machine learning techniques on NSL KDD dataset in order to explore significant features in detecting network intrusions. The experimental results showed that the proposed model improves the detection rates and also speed up the detection process.",International Journal of Computer Applications Technology and Research,2020,10.7753/ijcatr0903.1001,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
3490086e67836072fe83a8cd49c56e4699c3b6c4,https://www.semanticscholar.org/paper/3490086e67836072fe83a8cd49c56e4699c3b6c4,Role of Artificial Intelligence in Shaping Consumer Demand in E-Commerce,"The advent and incorporation of technology in businesses have reformed operations across industries. Notably, major technical shifts in e-commerce aim to influence customer behavior in favor of some products and brands. Artificial intelligence (AI) comes on board as an essential innovative tool for personalization and customizing products to meet specific demands. This research finds that, despite the contribution of AI systems in e-commerce, its ethical soundness is a contentious issue, especially regarding the concept of explainability. The study adopted the use of word cloud analysis, voyance analysis, and concordance analysis to gain a detailed understanding of the idea of explainability as has been utilized by researchers in the context of AI. Motivated by a corpus analysis, this research lays the groundwork for a uniform front, thus contributing to a scientific breakthrough that seeks to formulate Explainable Artificial Intelligence (XAI) models. XAI is a machine learning field that inspects and tries to understand the models and steps involved in how the black box decisions of AI systems are made; it provides insights into the decision points, variables, and data used to make a recommendation. This study suggested that, to deploy explainable XAI systems, ML models should be improved, making them interpretable and comprehensible.",Future Internet,2020,10.3390/fi12120226,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
54ed60260eea374e3ab262282f1b923ab15b02d4,https://www.semanticscholar.org/paper/54ed60260eea374e3ab262282f1b923ab15b02d4,Planning for Goal-Oriented Dialogue Systems,"Generating complex multi-turn goal-oriented dialogue agents is a difficult problem that has seen a considerable focus from many leaders in the tech industry, including IBM, Google, Amazon, and Microsoft. This is in large part due to the rapidly growing market demand for dialogue agents capable of goal-oriented behaviour. Due to the business process nature of these conversations, end-to-end machine learning systems are generally not a viable option, as the generated dialogue agents must be deployable and verifiable on behalf of the businesses authoring them. 
In this work, we propose a paradigm shift in the creation of goal-oriented complex dialogue systems that dramatically eliminates the need for a designer to manually specify a dialogue tree, which nearly all current systems have to resort to when the interaction pattern falls outside standard patterns such as slot filling. We propose a declarative representation of the dialogue agent to be processed by state-of-the-art planning technology. Our proposed approach covers all aspects of the process; from model solicitation to the execution of the generated plans/dialogue agents. Along the way, we introduce novel planning encodings for declarative dialogue synthesis, a variety of interfaces for working with the specification as a dialogue architect, and a robust executor for generalized contingent plans. We have created prototype implementations of all components, and in this paper, we further demonstrate the resulting system empirically.",ArXiv,2019,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
6af4abe9f3360b2817db4a655e1d3486754ba6b0,https://www.semanticscholar.org/paper/6af4abe9f3360b2817db4a655e1d3486754ba6b0,Large-scale learning for media understanding,,EURASIP J. Image Video Process.,2015,10.1186/S13640-015-0080-7,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
69f91edaf5abc963a9b9b121d1f8bc851333e133,https://www.semanticscholar.org/paper/69f91edaf5abc963a9b9b121d1f8bc851333e133,Active Learning in the Era of Big Data,"Active learning methods automatically adapt data collection by selecting the most informative samples in order to accelerate machine learning. Because of this, real-world testing and comparing active learning algorithms requires collecting new datasets (adaptively), rather than simply applying algorithms to benchmark datasets, as is the norm in (passive) machine learning research. To facilitate the development, testing and deployment of active learning for real applications, we have built an open-source software system for large-scale active learning research and experimentation. The system, called NEXT, provides a unique platform for realworld, reproducible active learning research. This paper details the challenges of building the system and demonstrates its capabilities with several experiments. The results show how experimentation can help expose strengths and weaknesses of active learning algorithms, in sometimes unexpected and enlightening ways.",,2015,10.2172/1225849,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
7b4fd6bbd3933704f5f3c8afcea3950866d57a2d,https://www.semanticscholar.org/paper/7b4fd6bbd3933704f5f3c8afcea3950866d57a2d,What Can Digitization Do For Formulated Product Innovation and Development,"Digitization oﬀers signiﬁcant opportunities for the formulated product industry to transform the way it works and develop new methods of business. R&D is one area of operation that is challenging to take advantage of these technologies due to its high level of domain specialisation and creativity but the beneﬁts could be signiﬁcant. Recent developments of base level technologies such as artiﬁcial intelligence (AI)/machine learning (ML), robotics and high performance computing (HPC), to name a few, present disruptive and transformative technologies which could oﬀer new insights, discovery methods and enhanced chemical control when combined in a digital ecosystem of connectivity, distributive services and decentralisation. At the fundamental level, research in these technologies has shown that new physical and chemical insights can be gained, which in turn can augment experimental R&D approaches through physics-based chemical simulation, data driven models and hybrid approaches. In all of these cases, high quality data is required to build and validate models in addition to the skills and expertise to exploit such methods. In this article we give an overview of some of the digital technology demonstrators we have developed for formulated product R&D. We discuss the challenges in building and deploying these demonstrators.",,2020,10.26434/chemrxiv.11763864.v2,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
efff6c4c5c1f3677f0881a4a10556997d2fbe747,https://www.semanticscholar.org/paper/efff6c4c5c1f3677f0881a4a10556997d2fbe747,Distributed AI for Special-Purpose Vehicles,,SAFECOMP Workshops,2020,10.1007/978-3-030-55583-2_18,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
646b53d162cf6754aff28c6923663d0eeeafa427,https://www.semanticscholar.org/paper/646b53d162cf6754aff28c6923663d0eeeafa427,Special issue on quality management for information systems,,Software Quality Journal,2020,10.1007/s11219-020-09516-z,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
ceeddcc7f7f293b541e9813b9e9cd4a5e93ee5ed,https://www.semanticscholar.org/paper/ceeddcc7f7f293b541e9813b9e9cd4a5e93ee5ed,"Tutorial: Google Cloud for Beginners: Architecture, Storage, and Computation","Cloud computing is a powerful paradigm to support pervasive applications that require always-on computation and interaction. In this tutorial, we focus on the Google Cloud Platform (GCP). Even though it is not currently the leader in the cloud industry, GCP is one of the best choices for deploying machine learning-based applications. This is due to the advancement of Google's deep learning technology, e.g., TensorFlow and TPU. This tutorial consists of a series of hands-on exercises for beginners that have minimum or even no experience in cloud computing platforms. We will first give an overview of cloud computing. Then we cover some high-level architecture of GCP and how they affect application development and deployment. Finally, we will walk through exercises on using several services in GCP with the focus on storage and computation services. If time permits, we will build a simple application using TensorFlow.",2020 IEEE International Conference on Pervasive Computing and Communications Workshops (PerCom Workshops),2020,10.1109/percomworkshops48775.2020.9156238,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
41163344c4ea309bfb1cac527285ede1cf0b3809,https://www.semanticscholar.org/paper/41163344c4ea309bfb1cac527285ede1cf0b3809,Understanding Android Application Masquerades(Paper category: Preliminary),"Android has revolutionized the smartphone industry since its unveiling in 2008. It has become the major choice of mobile operating system, modelled after Linux open-source ecosystem. This openness and general end-user accessibility has made android an open-ground for privacy breach and data theft, through masquerading rogue android apps. In this research paper we put forward a novel method to recognise masquerading android applications by deploying supervised machine learning algorithms over Object-Oriented software metrics based dataset. First, android apps are collected and decompiled into a repository. Object-Oriented software metrics are then calculated for each app using it’s decompiled source codes, which forms the tuple of our dataset. Every tuple is tagged either as malware or benign using VirusTotal service. Consequently, this dataset is provided as input for machine learning algorithms. The malware recognising power for each machine-learned models can be evaluated using accuracy and AUC (area under ROC curve).",,2020,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
0138f0e45a79d004f5f7c1ff5a7a98c88dbfec80,https://www.semanticscholar.org/paper/0138f0e45a79d004f5f7c1ff5a7a98c88dbfec80,Artificial intelligence and autonomy in space: Balancing risks and benefits for deterrence and escalation control,"An overarching principle accepted by space-faring nations and industry alike is to maintain freedom of operations in a safe and secure environment, commensurate with national and commercial interests. Deterrence concepts and escalation control play key roles in realizing this principle in the increasingly congested, competitive and contested space environment. AI and autonomous machine learning are being pursued as critical enablers in commercial and military programs for space traffic management, routine space operations, space domain awareness (SDA), and space control. AI systems hold the potential to strengthen deterrence by improving both the speed and ability to assess threats and inform decision makers in times of crisis. However, issues that have arisen in terrestrial AI applications will be also present in these applications, with implications for space deterrence and escalation scenarios. Key among these are performance, explainability, and vulnerability. To date there are few if any international standards or regulations to guide best practices for choosing AI methods for space operations and developing a shared understanding of the risks and benefits to strategic stability. This paper explores trade-offs between explainability, performance, and vulnerability in AI methods applied to space control and SDA scenarios, and illustrates how choices on these trade-offs may affect deterrence signaling and escalation control in space. Copyright © 2020 Advanced Maui Optical and Space Surveillance Technologies Conference (AMOS) – www.amostech.com Introduction and Key Research Question Freedom of operations in a safe and secure environment in space, commensurate with national and commercial interests, is a fundamental principle of international policy as well as most space-faring nations. Preventing damage to space assets is tantamount to achieving a safe and secure space environment [6]. However, to date, there is less ability to respond to threats in space than to conventional threats, and countries with developed space assets are perceived to rely heavily on these assets for everything from civil uses to military use. As a result, deterrence concepts and escalation control play key roles in ensuring unimpeded use of space in the increasingly congested, competitive and contested space environment. Artificial Intelligence (AI) and autonomous machine learning are being pursued as critical enablers in commercial and military programs for space traffic management (STM), routine space operations, space domain awareness (SDA), and space control [5][9]. AI is generally defined as methods capable of rational and autonomous reasoning, action or decision making, or adaptation to complex environment, and to previously unseen circumstances [12]. AI systems hold the potential to strengthen deterrence by improving both the speed and ability to assess threats and inform decision makers in times of crisis. However, issues that have arisen in terrestrial AI applications will be also present in these applications, with implications for space deterrence and escalation scenarios. Key among these are performance, explainability, and vulnerability – all of which can vary depending on the AI algorithms, training data, and platforms. For example, deterrence signaling might be misconstrued and responses deemed escalatory if one nation does not fully understand the intentions and strategic goals of the other nation. AI systems could negatively affect signaling by compressing the timescale for making and communicating decisions, or incorrectly classifying observed behaviors. A result could be unintentional conflict escalation. To date there are few if any international standards and/or regulations to guide best practices for choosing AI methods for space operations and developing a shared understanding of the risks and benefits to strategic stability. This paper explores how AI deployed on critical space systems – and the design choices made about the characteristics of the AI methods may impact deterrence signaling and escalation control. Our key research question is, how might tradeoffs between explainability, performance, and vulnerability in AI methods applied to space control and SDA scenarios affect deterrence signaling and escalation control in space? The purpose is to stimulate dialogue on best practices for choosing AI methods for space operations and developing a shared understanding of the risks and benefits to strategic stability. AI in Space Operations: Exemplars and Use Cases Current and potential applications of AI in space operations are many. AI will be essential for managing megaconstellations (tens of thousands) of commercial telecommunications satellites in low Earth orbit (LEO); guiding functions such as scheduling and tasking; collision avoidance; and space debris mitigation. AI is also being explored for classification of observations from LEO constellations proposed to serve national security applications such as persistent overhead coverage and missile defense. Advancements in AI, in combination with increased availability of low-cost and secure cloud storage, have simultaneously led to improvements in SDA capabilities while decreasing costs. As databases grow with an increased number of objects to track and characterize, companies and countries will employ AI to make timely, cost-effective assessments for SDA, while reducing the role of the human-in-the-loop. For this research, we examine two exemplars of AI applications in space, with three different use cases for each (Table 1). The first exemplar involves scenarios for threat detection and response, in three different use cases that correspond to different phases of deterrence and escalation. In the scenarios for Exemplar 1, a national security satellite faces a potential threat that has yet to be realized. The primary functions of AI under these scenarios are to characterize and assess the threat, recommend (and potentially direct) options for defensive measures, and communicate a credible deterrence signal. The second exemplar involves scenarios to ensure resiliency of a meg-constellation. In these scenarios, a mega-constellation of satellites experiences failures of varying degrees. Here, primary functions of AI are to assess damage, attribute the cause, and recommend options for reconstitution and potential retaliation. These exemplars and scenarios are summarized in Table 1, along with functional roles of AI. The scenarios are viewed from the perspective of Country B. Copyright © 2020 Advanced Maui Optical and Space Surveillance Technologies Conference (AMOS) – www.amostech.com Table 1 Exemplars of AI in Space and Use Cases",,2020,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
739ca20817a8f07c4f3ec3fb5ebcbe6d50b55017,https://www.semanticscholar.org/paper/739ca20817a8f07c4f3ec3fb5ebcbe6d50b55017,Post-Processing of 3D Printed Metallic Micro-parts,"The fourth industrial revolution is transforming current manufacturing approaches. One of the main objectives of this Industry 4.0 is to mass produce highly customized products. Hence, this requires new types of production methods to manufacture parts with a large variety of dimensions and geometry. The present research project aims to contribute to this objective in collaboration with an industrial partner. They are a solution provider in the high-precision metal additive manufacturing (AM) field, combining AM, nanotechnology and machine learning providing unique freedom of design and material choice for high-precision parts by their developed 3D printer combining deposition of nanoparticle monolayers with laser sintering. However, most of their generated micro metal parts require post-processing steps before entering industrial applications. As additional process steps appear to be undesired, deploying post-processing could have an additional benefit of reducing the overall manufacturing time by balancing layer height and the polishing process to reduce the surface roughness of the generated metal micro-parts. Among other methods, electrochemical polishing (EP) seems a promising candidate for post-processing such parts. Nevertheless, EP is still unexplored in combination with this novel high-precision 3D printing method. This project aims to find a trade-off between layer thickness, printing speed and polishing. An optimum should be achieved to get smooth and flat surfaces to maintain high quality parts at reasonable fabrication time. Secondary, the project aims to acquire more knowledge of the printing technology and its interaction with post-processes on various alloys. Handling of these microparts, such as manual operations to attach electrodes for EP, is extremely challenging because of their small size (sub-mm) and relative brittleness. EP is the reverse of electroplating; hence, it dissolves metal on the surface of the part resulting in a smooth surface. This method does not apply forces on the brittle parts, requires very little handling and is able the reach very low surface roughness (submicron) on most geometries increasing fatigue resistance of the parts. However, EP of small micro-parts encounters some challenges. It requires the polishing medium (electrolyte) to be in contact with the part surface and a mechanical contact is required to close the electrical circuit (current flowing to the part). As generating a contact with typical metal parts is trivial, this is an issue for the printed micro-parts as contact points are relatively large compared to the parts to be polished. Second, EP produces gases that can interfere with medium contact of the part. This study presents a novel prototype for an EP setup to overcome such issues for polishing metallic micro-parts. It consists of a rotating drum designed to evacuate gases and vary while being mostly constant the contact point by tumbling the parts in a conductive drum. This ensures uniform and smooth surface roughness reduction on parts of all sizes and shapes. A preliminary study of the prototype impact shows that the printing time could be reduced by four times. Further, ongoing tests of EP on different alloys such as SS316, Ti and CuSn will be presented.",,2020,10.32393/csme.2020.1261,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
c6ce7fd941de2b1b9863cc8e43bc73a004343e52,https://www.semanticscholar.org/paper/c6ce7fd941de2b1b9863cc8e43bc73a004343e52,Human-Centered AI through Scalable Visual Data Analytics,"While artificial intelligence (AI) has led to major breakthroughs in many domains, understanding machine learning models remains a fundamental challenge. They are often used as ”black boxes,” which could be detrimental. How can we help people understand complex machine learning models, so that they can learn them more easily and use them more effectively? In this talk, I present my research that makes AI more accessible and interpretable, through a novel human-centered approach, by creating novel data visualization tools that are scalable, interactive, and easy to learn and to use. I present my work in two interrelated topics. (1) Visualization for Industry-scale Models: I present how to scale up interactive visualization tools for industry-scale deep learning models that use large datasets. I describe how the ActiVis system helps Facebook data scientists interpret deep neural network models by visually exploring activation flows. ActiVis is patent-pending, and has been deployed on Facebook’s ML platform. (2) Interactive Understanding of Complex Models: I show how visualization helps novices interactively learn complex concepts of deep learning models. I describe how I developed GAN Lab, a visual education system for Generative Adversarial Networks (GANs), one of the most popular, but hard-to-understand models. GAN Lab has been open-sourced in collaboration with Google Brain and used by over 30,000 people from 140 countries. I conclude with my vision to make AI more human-centered, to promote actionability for AI, stimulate a stronger ethical AI workforce, and foster healthy impacts of AI on broader society. Monday, April 1, 2019, 10:00 am Planetarium E300 MSC Computer Science Emory University",,2020,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
d89562ee18bce330f3cf72d793de5a921c08c5c4,https://www.semanticscholar.org/paper/d89562ee18bce330f3cf72d793de5a921c08c5c4,RETHINKING CALIBRATION FOR PROCESS SPECTROMETERS II,"Optical spectroscopy is a favored technology to measure chemistry and is ubiquitous in the hydrocarbon processing industry. In a previous paper, we focused on a generic, machine-learning approach that addressed the primary bottlenecks of mustering data, automating analyzer calibration, and tracking data and model performance over time. The gain in efficiency has been considerable, and the fact that the approach does not disturb any of the legacy (i.e., no changes or alterations to any analyzer or software in place) made deployment simple. We also standardized a procedure for doing calibrations that adheres to best practices, archives all data and models, provides ease of access, and delivers the models in any format. What remains is to assess the speed of processing and the quality of the models. To that end, a series of calibration experts were tasked with model optimization, restricting the work to selecting the proper samples to include in the computation and setting the number of factors in PLS. The amount of time and the quality of the models were then compared. The automated system performed the work in minutes rather than hours and the quality of the predictions at least matched the best experts and performed significantly better than the average expert. The conclusion is that there is a large amount of recoverable giveaway that can be avoided through automation of this process and the consistency it brings to the PLS model construction. RETHINKING SPECTROSCOPIC CALIBRATIONS II © 2020 ISA – The Instrumentation, Systems, and Automation Society. All rights reserved. 2 INTRODUCTION There is a lot of mundane work tied to the assembly of spectra and laboratory reference values to enable quality calibration work. There is also insufficient guidance when it comes to the model construction task. How much time should be spent on this task? How to best assess whether a spectrum-reference pair is an outlier or not? How many cycles of regression-sample elimination make sense? Where do we switch over from improving the model by adding PLS factors to overfitting and incorporating destabilizing noise? Why is this discussion important? Optical spectroscopy is one of the few instrumental technologies that allow us to assess the chemistry inside our manufacturing processes; the other being chromatography [1]. Spectroscopy measures the concentration of chemical functional groups, allowing us to infer a great number of properties with both accuracy and precision. Use of spectroscopy shifts the quality assessment from daily to minute-by-minute. In a petroleum refinery, where blending shifts can occur hourly, lab results are simply too infrequent and not timely. So, we adopt optical spectroscopy (NIR, IR or Raman) to solve the timeliness problem, but the quality of the reported values is completely controlled by the quality of the calibration. When we consider the costs of octane or RVP giveaway, the approach we take to optimize calibration models takes on a huge significance. There are automated optimization solutions that make sense to deploy as discussed in our earlier paper [2]. Are the costs of the solution justified? In this paper, we focus on two questions: one whether an automated calibration system can perform as well as true experts in the field, and the second to provide a mechanism for companies to objectively evaluate the quality of the models they produce in-house. To the second end, we propose a cost-free test to judge a company’s ability to generate optimal calibration performance. This paper builds on papers presented in the 2015, 2018, and 2019 ISA-AD Symposia [3, 1, 2]. The results reported in this paper supports our earlier contention that consistency in the approach to spectroscopic calibration is critical. For background on the multivariate calibration process, there are myriad articles and books. For the uninitiated or those interested in the organization of industrial projects, we recommend the text by Beebe et al. [4]; for more advanced practitioners interested in theory, Martens and Næs [5] is an excellent place to start.",,2020,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
5606767cff9249f2f65e2c04ea098e9d8a17470d,https://www.semanticscholar.org/paper/5606767cff9249f2f65e2c04ea098e9d8a17470d,A Novel PPA Method for Fluid Pipeline Leak Detection Based on OPELM and Bidirectional LSTM,"Pipeline leak detection has attracted great research interests for years in the energy industry. Continuous pressure monitoring is one of the most straightforward approaches in leak detection which utilizes pressure point analysis (PPA) algorithms to exploit the transient pressure characteristics and identify leak events. However, a critical issue that jeopardizes the deployment of PPA based methods is the high false alarm rate. In this paper, a novel PPA based leak detection method is proposed which can accurately detect the leak events and dramatically decrease the number of false alarms compared to existing methods. Firstly, the proposed method takes advantage of the good approximation ability and fast learning speed of optimally-pruned extreme learning machine (OPELM) to produce a preliminary leak detection result. Then, the strong memorizing ability of bidirectional long-short term memory (BiLSTM) network is exploited to identify the true positive from the preliminary detection result, hence significantly decrease the number of false alarms. Furthermore, a feature extraction mechanism is also proposed to obtain both the dynamic and static characteristics from raw pressure wave. Experiments and verifications are performed on different real world data sets obtained from pipeline leak tests. It shows that the proposed method can achieve higher detection accuracy with significantly less false alarms. It enhances the practicality of pressure monitoring based leak detection schemes.",IEEE Access,2020,10.1109/ACCESS.2020.3000960,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
a7307bf463f2911eea021d952a8d6b9d8eff1dcb,https://www.semanticscholar.org/paper/a7307bf463f2911eea021d952a8d6b9d8eff1dcb,Interactive Rule Refinement for Fraud Detection,"Credit card frauds are unauthorized transactions that are made or attempted by a person or an organization that is not authorized by the card holders. Fraud with general-purpose cards (credit, debit cards etc.) is a billion dollar industry and companies are therefore investing significant efforts in identifying and preventing them. It is typical to deploy mining and machine learning-based techniques to derive rules. However, such rules may not always capture the semantic reasons underlying the frauds that occur. For this reason, credit card companies often employ domain experts to manually specify rules that exploit general or domain knowledge for improving the detection process. Over time, however, as new (fraudulent and legitimate) transactions arrive, these rules need to be updated and refined to capture the evolving (fraud and legitimate) activity patterns. The goal of the RUDOLF system described in this paper is to guide and assist domain experts in this challenging task. RUDOLF automatically determines the “best” adaptation to existing rules to capture all fraudulent transactions and, respectively, omit all legitimate transactions. The proposed modifications can then be further refined by users and the process can be repeated until they are satisfied with the resulting rules. We show that the problem of identifying the best candidate adaptation is NP-hard in general and present PTIME heuristic algorithms for determining the set of rules to adapt. We have implemented our algorithms in RUDOLF and show, through experiments on real-life datasets, the effectiveness and efficiency of our solution.",EDBT,2018,10.5441/002/edbt.2018.24,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
ce143f809ecfff0179e7628967d0e104aa894def,https://www.semanticscholar.org/paper/ce143f809ecfff0179e7628967d0e104aa894def,Display Advertising with Real-Time Bidding (RTB) and Behavioural Targeting,"Online advertising is now one of the fastest advancing areas in the IT industry. In display and mobile advertising, the most significant technical development in recent years is the growth of Real-Time Bidding (RTB), which facilitates a real-time auction for a display opportunity. RTB essentially facilitates buying an individual ad impression in real time while it is still being generated from a users visit. RTB not only scales up the buying process by aggregating a large number of available inventories across publishers but, most importantly, enables direct targeting of individual users. As such, RTB has fundamentally changed the landscape of digital marketing. Scientifically, the demand for automation, integration and optimization in RTB also brings new research opportunities in information retrieval, data mining, machine learning and other related fields. Despite its rapid growth and huge potential, many aspects of RTB remain unknown to the research community for a variety of reasons. This monograph offers insightful knowledge of real-world systems, to bridge the gaps between industry and academia, and to provide an overview of the fundamental infrastructure, algorithms, and technical and research challenges of the new frontier of computational advertising. The topics covered include user response prediction, bid landscape forecasting, bidding algorithms, revenue optimization, statistical arbitrage, dynamic pricing, and ad fraud detection. This is an invaluable text for researchers and practitioners alike. Academic researchers will get a better understanding of the real-time online advertising systems currently deployed in industry. While industry practitioners are introduced to the research challenges, the state of the art algorithms and potential future systems in this field.",Found. Trends Inf. Retr.,2016,10.1561/1500000049,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
07f7680c35cbcf75134b6b2a2c0b69ab964c5a4b,https://www.semanticscholar.org/paper/07f7680c35cbcf75134b6b2a2c0b69ab964c5a4b,Investigating Challenges to SME Deployment of Operational Business Intelligence: A Case Study in the New Zealand Retail Sector,"Advances in machine learning and big data are leading to the employment of data analytics management systems in many organisations and industries. Generally, large organisations are able to put more resources towards this deployment, and the software used is more suited to larger entities. However, increasing business pressures are now driving small and medium-sized enterprises (SMEs) to deploy data analytics. This study examines the challenges faced by small retail businesses in New Zealand when deploying a self-service business intelligence (BI) system. The study applied qualitative methods including in-depth interviews with managers and staff of four retail stores, and observed that while respondents consistently agreed on the benefits and drawbacks of the new system, their levels of engagement with the system were uneven. It was discovered that users' real-time responses to dilemmas arising from the novelty of the new analytics management system compared to the existing system, significantly influenced the smooth adoption and implementation of the system. Analysis of a representative dilemma using the Theory of Constraints (TOC) and Evaporating Cloud (EC) method, leads to a possible resolution of the dilemma through TOC injections.",UCC Companion,2019,10.1145/3368235.3369371,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
36fc7d552c11ce1a04b273b30a4edeb14b70a4d5,https://www.semanticscholar.org/paper/36fc7d552c11ce1a04b273b30a4edeb14b70a4d5,An Approach for Adaptive Automatic Threat Recognition Within 3D Computed Tomography Images for Baggage Security Screening,"BACKGROUND
The screening of baggage using X-ray scanners is now routine in aviation security with automatic threat detection approaches, based on 3D X-ray computed tomography (CT) images, known as Automatic Threat Recognition (ATR) within the aviation security industry. These current strategies use pre-defined threat material signatures in contrast to adaptability towards new and emerging threat signatures. To address this issue, the concept of adaptive automatic threat recognition (AATR) was proposed in previous work.


OBJECTIVE
In this paper, we present a solution to AATR based on such X-ray CT baggage scan imagery. This aims to address the issues of rapidly evolving threat signatures within the screening requirements. Ideally, the detection algorithms deployed within the security scanners should be readily adaptable to different situations with varying requirements of threat characteristics (e.g., threat material, physical properties of objects).


METHODS
We tackle this issue using a novel adaptive machine learning methodology with our solution consisting of a multi-scale 3D CT image segmentation algorithm, a multi-class support vector machine (SVM) classifier for object material recognition and a strategy to enable the adaptability of our approach. Experiments are conducted on both open and sequestered 3D CT baggage image datasets specifically collected for the AATR study.


RESULTS
Our proposed approach performs well on both recognition and adaptation. Overall our approach can achieve the probability of detection around 90% with a probability of false alarm below 20%.


CONCLUSIONS
Our AATR shows the capabilities of adapting to varying types of materials, even the unknown materials which are not available in the training data, adapting to varying required probability of detection and adapting to varying scales of the threat object.",Journal of X-ray science and technology,2019,10.3233/XST-190531,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
5b38197884bacc6f3893e9320297639ccc89b47d,https://www.semanticscholar.org/paper/5b38197884bacc6f3893e9320297639ccc89b47d,Classifying Multivariate Signals in Rolling Bearing Fault Detection Using Adaptive Wide-Kernel CNNs,"With the developments in improved computation power and the vast amount of (automatic) data collection, industry has become more data-driven. These data-driven approaches for monitoring processes and machinery require different modeling methods focusing on automated learning and deployment. In this context, deep learning provides possibilities for industrial diagnostics to achieve improved performance and efficiency. These deep learning applications can be used to automatically extract features during training, eliminating time-consuming feature engineering and prior understanding of sophisticated (signal) processing techniques. This paper extends on previous work, introducing one-dimensional (1D) CNN architectures that utilize an adaptive wide-kernel layer to improve classification of multivariate signals, e.g., time series classification in fault detection and condition monitoring context. We used multiple prominent benchmark datasets for rolling bearing fault detection to determine the performance of the proposed wide-kernel CNN architectures in different settings. For example, distinctive experimental conditions were tested with deviating amounts of training data. We shed light on the performance of these models compared to traditional machine learning applications and explain different approaches to handle multivariate signals with deep learning. Our proposed models show promising results for classifying different fault conditions of rolling bearing elements and their respective machine condition, while using a fairly straightforward 1D CNN architecture with minimal data preprocessing. Thus, using a 1D CNN with an adaptive wide-kernel layer seems well-suited for fault detection and condition monitoring. In addition, this paper clearly indicates the high potential performance of deep learning compared to traditional machine learning, particularly in complex multivariate and multi-class classification tasks.",Applied Sciences,2021,10.3390/app112311429,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
1a0cf3eba22c053ad21503e034baf3178899258a,https://www.semanticscholar.org/paper/1a0cf3eba22c053ad21503e034baf3178899258a,Design and Deployment of a Data Lake at a Pilot Plant Scale for a Smart Electropolishing Process,"In order to remain competitive and satisfy the demands of today’s customers in a timely manner, manufacturing industries are embracing the Industry 4.0 philosophy where automation is pushed beyond robotics to new technologies emerging from data science and artificial intelligence. The aim is to reduce time spent on none added value tasks and help learning from past experience in order to enhance efficiency and quality of manufacturing processes.
 Traditional industries, such as electropolishing, need to find ways to automate their, often heavily artisanal-based techniques and develop an intelligent network of machines and processes taking advantage of information and communication technology such as Big Data, IoT (Internet of Things), or Artificial Intelligence (AI). This digital transition can be realized through the application of an IIoT (Industrial Internet of Things) platform that constructs a massive, sophisticated information network of interconnected sensors, equipment, and processes known as cyber-physical systems.
 Within this network, large amounts of data (for example process bath attributes such as temperature or viscosity and part characteristics such as roughness or brightness) can be collected automatically via sensors and through user-friendly applications from manual measurements and observations. All data are uploaded automatically into a cloud-based data storage system. In order for this collected information to be useful, the data needs to be processed to allow pattern discovery and extraction of useful information regarding the system performance, probable faults in the process, and product quality. Besides others, machine learning algorithms play a key role in extracting useful information.
 Classification and processing of such massive, diverse, and rapidly arriving data sets are known to be challenging. As a result, the concept of data lake has arisen in the last decade as an appealing and cost-effective approach for companies to manage large amounts of data. It consists of a large repository of datasets designed to transform raw and unstructured data into structured, usable information to allow further processing. A data lake, organized typically in four layers (ingestion, distillation, processing, and insights layers), stores both old and near real-time data in one location for initial assessment, with comprehensive data organization, analysis, and visualization being performed only when necessary 1,2. This promotes agility by allowing data to be accessed by everyone in the company. 2
 
 In this work, a data lake is designed and implemented in conjunction with a pilot plant to demonstrate how in the electropolishing process of stainless-steel samples in an aging electrolyte, data can be collected and organized for further processing using machine learning techniques in order to optimize the process and part quality based on the data analysis results.
 References: 
 
 N. Miloslavskaya and A. Tolstoy, Procedia Comput. Sci., 88, 300–305 (2016).
 
 
 H. Fang, in 2015 IEEE International Conference on Cyber Technology in Automation, Control, and Intelligent Systems (CYBER),, p. 820–824 (2015).
 
 
 
 
 
 
 
 
 Figure 1
",ECS Meeting Abstracts,2022,10.1149/ma2022-01251219mtgabs,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
6074ba443e2e468368257c9fb9b6dffaebaf6f6e,https://www.semanticscholar.org/paper/6074ba443e2e468368257c9fb9b6dffaebaf6f6e,DaDianNao: A Neural Network Supercomputer,"Many companies are deploying services largely based on machine-learning algorithms for sophisticated processing of large amounts of data, either for consumers or industry. The state-of-the-art and most popular such machine-learning algorithms are Convolutional and Deep Neural Networks (CNNs and DNNs), which are known to be computationally and memory intensive. A number of neural network accelerators have been recently proposed which can offer high computational capacity/area ratio, but which remain hampered by memory accesses. However, unlike the memory wall faced by processors on general-purpose workloads, the CNNs and DNNs memory footprint, while large, is not beyond the capability of the on-chip storage of a multi-chip system. This property, combined with the CNN/DNN algorithmic characteristics, can lead to high internal bandwidth and low external communications, which can in turn enable high-degree parallelism at a reasonable area cost. In this article, we introduce a custom multi-chip machine-learning architecture along those lines, and evaluate performance by integrating electrical and optical inter-chip interconnects separately. We show that, on a subset of the largest known neural network layers, it is possible to achieve a speedup of 656.63× over a GPU, and reduce the energy by 184.05× on average for a 64-chip system. We implement the node down to the place and route at 28 nm, containing a combination of custom storage and computational units, with electrical inter-chip interconnects.",IEEE Transactions on Computers,2017,10.1109/TC.2016.2574353,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
de25ccc983c712ffc7ee6825e9af84e7fc96774c,https://www.semanticscholar.org/paper/de25ccc983c712ffc7ee6825e9af84e7fc96774c,Democratizing algorithmic news recommenders: how to materialize voice in a technologically saturated media ecosystem,"The deployment of various forms of AI, most notably of machine learning algorithms, radically transforms many domains of social life. In this paper we focus on the news industry, where different algorithms are used to customize news offerings to increasingly specific audience preferences. While this personalization of news enables media organizations to be more receptive to their audience, it can be questioned whether current deployments of algorithmic news recommenders (ANR) live up to their emancipatory promise. Like in various other domains, people have little knowledge of what personal data is used and how such algorithmic curation comes about, let alone that they have any concrete ways to influence these data-driven processes. Instead of going down the intricate avenue of trying to make ANR more transparent, we explore in this article ways to give people more influence over the information news recommendation algorithms provide by thinking about and enabling possibilities to express voice. After differentiating four ideal typical modalities of expressing voice (alternation, awareness, adjustment and obfuscation) which are illustrated with currently existing empirical examples, we present and argue for algorithmic recommender personae as a way for people to take more control over the algorithms that curate people's news provision. This article is part of a theme issue ‘Governing artificial intelligence: ethical, legal, and technical opportunities and challenges’.","Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences",2018,10.1098/rsta.2018.0088,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
c9b43fcd195729f73e547287f73b44d1f55d9046,https://www.semanticscholar.org/paper/c9b43fcd195729f73e547287f73b44d1f55d9046,Automated game testing using computer vision methods,"Video game development is a growing industry nowadays with high revenues. However, even if there are many resources invested in the software development process, many games still contain bugs or performance issues that affect the user experience. This paper presents ideas on how computer vision methods can be used to automate the process of game testing. The goal is to replace the parts of the testing process that require human users (testers) with machines as much as possible, in order to reduce costs and perform more tests in less time by scaling with hardware resources. The focus is on solving existing real-world problems that have emerged from several discussions with industry partners. We base our methods on previous work in this area using intelligent agents playing video games and deep learning methods that interpret feedback from their actions based on visual output. The paper proposes several methods and a set of open-source tools, independent of the operating system or deployment platform, to evaluate the efficiency of the presented methods.",2021 36th IEEE/ACM International Conference on Automated Software Engineering Workshops (ASEW),2021,10.1109/ASEW52652.2021.00024,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
3194c62e80b7971d323e955b8623898886f0a605,https://www.semanticscholar.org/paper/3194c62e80b7971d323e955b8623898886f0a605,Prognostic Health Management for Turbofan Engines,"Prognostic Health Management (PHM) is an active area of research and a multibillion dollar industry in the field of reliability engineering. Complex sensor data exhibited by machines can be used to predict a bad omen (possible failure) beforehand, further saving downtime or loss of equipment and environment. We explore various deep learning solutions to model the spatio-temporal relationships exhibited by NASA Turbofans. Current approaches use CNN based models to predict Remaining Useful Life (RUL) of a system, we propose a novel CNN-LSTM architecture and explore the power of LSTMs to model sequential data. We further show the insights of deployment in terms of system tolerance statistics.",,2021,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
5391f8441ac5b637d728d138727c7b08d268dfb0,https://www.semanticscholar.org/paper/5391f8441ac5b637d728d138727c7b08d268dfb0,Data-Informed Duties in AI Development,"Law should help direct—and not merely constrain—the development of artificial intelligence (AI). One path to influence is the development of standards of care both supplemented and informed by rigorous regulatory guidance. Such standards are particularly important given the potential for inaccurate and inappropriate data to contaminate machine learning. Firms relying on faulty data can be required to compensate those harmed by that data use—and should be subject to punitive damages when such use is repeated or willful. Regulatory standards for data collection, analysis, use, and stewardship can inform and complement generalist judges. Such regulation will not only provide guidance to industry to help it avoid preventable accidents. It will also assist a judiciary that is increasingly called upon to develop common law in response to legal disputes arising out of the deployment of AI.",,2019,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10bbc5556fe6b79b18bb986af5e601b7ac7ab114,https://www.semanticscholar.org/paper/10bbc5556fe6b79b18bb986af5e601b7ac7ab114,LTF: A Label Transformation Framework for Correcting Target Shift,"Distribution shift is a major obstacle to the deployment of current deep learning models on realworld problems. Let Y be the target (label) and X the predictors (features). We focus on one type of distribution shift, target shift, where the marginal distribution of the target variable PY changes but the conditional distribution PX|Y does not. Existing methods estimate the density ratio between the sourceand target-domain label distributions by density matching. However, these methods are either computationally infeasible for large-scale data or restricted to shift correction for discrete labels. In this paper, we propose an end-to-end Label Transformation Framework (LTF) for correcting target shift, which implicitly models the shift of PY and the conditional distribution PX|Y using neural networks. Thanks to the flexibility of deep networks, our framework can handle continuous, discrete, and even multidimensional labels in a unified way and is scalable to large data. Moreover, for high dimensional X , such as images, we find that the redundant information in X severely degrades the estimation accuracy. To remedy this issue, we propose to match the distribution implied by our generative model and the target-domain distribution in a low-dimensional feature space that discards information irrelevant to Y . Both theoretical and empirical studies demonstrate the superiority of our method over previous approaches. UBTECH Sydney AI Centre, School of Computer Science, Faculty of Engineering, The University of Sydney, Darlington, NSW 2008, Australia School of Mathematics and Statistics, The University of Melbourne Department of Philosophy, Carnegie Mellon University. Correspondence to: Jiaxian Guo <jguo5934@uni.sydney.edu.au>. Proceedings of the 37 th International Conference on Machine Learning, Vienna, Austria, PMLR 119, 2020. Copyright 2020 by the author(s).",,2020,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
0e01b77290fa4e185bfdcaaccf75242732ab22c4,https://www.semanticscholar.org/paper/0e01b77290fa4e185bfdcaaccf75242732ab22c4,Artificial intelligence policy in India: a framework for engaging the limits of data-driven decision-making,"Artificial intelligence (AI) is an emerging focus area of policy development in India. The country's regional influence, burgeoning AI industry and ambitious governmental initiatives around AI make it an important jurisdiction to consider, regardless of where the reader of this article lives. Even as existing policy processes intend to encourage the rapid development of AI for economic growth and social good, an overarching trend persists in India, and several other jurisdictions: the limitations and risks of data-driven decisions still feature as retrospective considerations for development and deployment of AI applications. This article argues that the technical limitations of AI systems should be reckoned with at the time of developing policy, and the societal and ethical concerns that arise due to such limitations should be used to inform what policy processes aspire to achieve. It proposes a framework for such deliberation to occur, by analysing the three main stages of bringing machine learning (the most popular subset of AI techniques) to deployment—the data, model and application stage. It is written against the backdrop of India's current AI policy landscape, and applies the proposed framework to ongoing sectoral challenges in India. With a view to influence existing policy deliberation in the country, it focuses on potential risks that arise from data-driven decisions in general, and in the Indian context in particular. This article is part of the theme issue ‘Governing artificial intelligence: ethical, legal, and technical opportunities and challenges'.","Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences",2018,10.1098/rsta.2018.0087,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
71a978ce9878a26ba6e87b551f2bb7c987e47ebc,https://www.semanticscholar.org/paper/71a978ce9878a26ba6e87b551f2bb7c987e47ebc,Data Visualization for Air Quality Analysis on Bigdata Platform,"With the advances of industry, air pollution is increasingly becoming serious, and most of governments in the world has deployed many devices to monitor daily air quality. Monitoring and forecasting of air quality has also become an important issue to improve the quality of people's lives. As far as we know, bad air quality does not only affect the health of the respiratory tract, it may but also even cause mental illness. Many researchers have investigated different approaches to work on air quality forecast, and the visualization of forecasting becomes important. In this paper, we present an architecture for visualizing forecasted air quality on a big data platform. We implemented an ETL (Extract-Transform-Load) based framework in the platform, which includes computing nodes and storage nodes. Computational nodes are used for data collection and for air quality forecasting over the next 1 to 8 hours through machine learning and deep learning. Storage nodes are used to retrieve, analyze, and preprocess of collected data. We use the RESTful Web Service as an API, and finally we use the browser to get the data by predefined API and to present the forecasted and monitored results with Google Map API and D3 JavaScript library. It reveals that the visualization on big data framework can work well for air quality analysis.",2019 International Conference on System Science and Engineering (ICSSE),2019,10.1109/ICSSE.2019.8823437,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
3a20ed3521bd0041a22cf4dacd90b6f675751f27,https://www.semanticscholar.org/paper/3a20ed3521bd0041a22cf4dacd90b6f675751f27,"6G Wireless Communication: Its Vision, Viability, Application, Requirement, Technologies, Encounters and Research","The fast development of multiband ultrafast seamless network and super reliable data transmission system to support heavy traffic applications such as artificial intelligence, machine learning, deep learning, augmented reality, virtual reality, 3D media, Internet of Things, Enterprise Internet of Thing and the Internet of Nano-things that involves with the real time transfer of data, voice and video in terabytes per second (Tb/s), the current cellular network (5G Network is insufficient to meet the growth of usage of triple play services in fraction of time). To meet the expectation of heavy data users is a big challenge in today's generation. To handle the situation of drastic demand of data, the sixth generation of mobile technology (6G) should be deeply studied along with its potential in terms of bandwidth, low latency, channel capacity, channel modeling techniques, loss propagation models, energy spectrum efficiency, faster network connectivity and data security. In this paper the vision in terms intelligent computing and wireless massive connectivity, feasibility, requirement in terms of modifying the existing 5G network, technologies in terms of artificial intelligence, 3D networking, SM-MIMO and optical computing, challenges after deployment, research to promote good health for 6G and application of 6G in the field of industry, automation sector, health, and transport has been studied and presented.","2020 11th International Conference on Computing, Communication and Networking Technologies (ICCCNT)",2020,10.1109/ICCCNT49239.2020.9225680,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
ed6ce30c5bec85fc992bd9e814e0f77ea76cd18d,https://www.semanticscholar.org/paper/ed6ce30c5bec85fc992bd9e814e0f77ea76cd18d,AI explainability 360: hands-on tutorial,"This tutorial will teach participants to use and contribute to a new open-source Python package named AI Explainability 360 (AIX360) (https://aix360.mybluemix.net), a comprehensive and extensible toolkit that supports interpretability and explainability of data and machine learning models. Motivation for the toolkit. The AIX360 toolkit illustrates that there is no single approach to explainability that works best for all situations. There are many ways to explain: data vs. model, direct vs. post-hoc explanation, local vs. global, etc. The toolkit includes ten state of the art algorithms that cover different dimensions of explanations along with proxy explainability metrics. Moreover, one of our prime objectives is for AIX360 to serve as an educational tool even for non-machine learning experts (viz. social scientists, healthcare experts). To this end, the toolkit has an interactive demonstration, highly descriptive Jupyter notebooks covering diverse real-world use cases, and guidance materials, all helping one navigate the complex explainability space. Compared to existing open-source efforts on AI explainability, AIX360 takes a step forward in focusing on a greater diversity of ways of explaining, usability in industry, and software engineering. By integrating these three aspects, we hope that AIX360 will attract researchers in AI explainability and help translate our collective research results for practicing data scientists and developers deploying solutions in a variety of industries. Regarding the first aspect of diversity, Table 1 in [1] compares AIX360 to existing toolkits in terms of the types of explainability methods offered. The table shows that AIX360 not only covers more types of methods but also has metrics which can act as proxies for judging the quality of explanations. Regarding the second aspect of industry usage, AIX360 illustrates how these explainability algorithms can be applied in specific contexts (please see Audience, goals, and outcomes below). In just a few months since its initial release, the AIX360 toolkit already has a vibrant slack community with over 120 members and has been forked almost 80 times accumulating over 400 stars. This response leads us to believe that there is significant interest in the community in learning more about the toolkit and explainability in general. Audience, goals, and outcomes. The presentations in the tutorial will be aimed at an audience with different backgrounds and computer science expertise levels. For all audience members and especially those unfamiliar with Python programming, AIX360 provides an interactive experience (http://aix360.mybluemix.net/data) centered around a credit approval scenario as a gentle and grounded introduction to the concepts and capabilities of the toolkit. We will also teach all participants which type of explainability algorithm is most appropriate for a given use case, not only for those in the toolkit but also from the broader explainability literature. Knowing which explainability algorithms apply to which contexts and understanding when to use them can benefit most people, regardless of their technical background. The second part of the tutorial will consist of three use cases featuring different industry domains and explanation methods. Data scientists and developers can gain hands-on experience with the toolkit by running and modifying Jupyter notebooks, while others will be able to follow along by viewing rendered versions of the notebooks. Here is a rough agenda of the tutorial: 1) Overture: Provide a brief introduction to the area of explainability as well as introduce common terms. 2) Interactive Web Experience: The AIX360 interactive web experience (http://aix360.mybluemix.net/data) is intended to show a non-computer science audience how different explainability methods may suit different stakeholders in a credit approval scenario (data scientists, loan officers, and bank customers). 3) Taxonomy: We will next present a taxonomy that we have created for organizing the space of explanations and guiding practitioners toward an appropriate choice for their applications. 4) Installation: We will transition into a Python environment and ask participants to install the AIX360 package on their machines using provided instructions. 5) Example Use Cases in Finance, Government, and Healthcare: We will take participants through three use-cases in various application domains in the form of Jupyter notebooks. 6) Metrics: We will briefly showcase the two explainability metrics currently available through the toolkit. 7) Future Directions: The final segment will be to discuss future directions and how participants can contribute to the toolkit.",FAT*,2020,10.1145/3351095.3375667,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
1d104ace71010ec69bc3f1b775a90c2a7ee5c98f,https://www.semanticscholar.org/paper/1d104ace71010ec69bc3f1b775a90c2a7ee5c98f,"Artificial Intelligence-Based Techniques for Emerging Heterogeneous Network: State of the Arts, Opportunities, and Challenges","Recently, mobile networking systems have been designed with more complexity of infrastructure and higher diversity of associated devices and resources, as well as more dynamical formations of networks, due to the fast development of current Internet and mobile communication industry. In such emerging mobile heterogeneous networks (HetNets), there are a large number of technical challenges focusing on the efficient organization, management, maintenance, and optimization, over the complicated system resources. In particular, HetNets have attracted great interest from academia and industry in deploying more effective solutions based on artificial intelligence (AI) techniques, e.g., machine learning, bio-inspired algorithms, fuzzy neural network, and so on, because AI techniques can naturally handle the problems of large-scale complex systems, such as HetNets towards more intelligent and automatic-evolving ones. In this paper, we discuss the state-of-the-art AI-based techniques for evolving the smarter HetNets infrastructure and systems, focusing on the research issues of self-configuration, self-healing, and self-optimization, respectively. A detailed taxonomy of the related AI-based techniques of HetNets is also shown by discussing the pros and cons for various AI-based techniques for different problems in HetNets. Opening research issues and pending challenges are concluded as well, which can provide guidelines for future research work.",IEEE Access,2015,10.1109/ACCESS.2015.2467174,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
e3f7330da613248205705d1081ef210e3b5f1ac8,https://www.semanticscholar.org/paper/e3f7330da613248205705d1081ef210e3b5f1ac8,Introducing and Comparing Recent Clustering Methods for Massive Data Management in the Internet of Things,"The use of wireless sensor networks, which are the key ingredient in the growing Internet of Things (IoT), has surged over the past few years with a widening range of applications in the industry, healthcare, agriculture, with a special attention to monitoring and tracking, often tied with security issues. In some applications, sensors can be deployed in remote, large unpopulated areas, whereas in others, they serve to monitor confined busy spaces. In either case, clustering the sensor network’s nodes into several clusters is of fundamental benefit for obvious scalability reasons, and also for helping to devise maintenance or usage schedules that might greatly improve the network’s lifetime. In the present paper, we survey and compare popular and advanced clustering schemes and provide a detailed analysis of their performance as a function of scale, type of collected data or their heterogeneity, and noise level. The testing is performed on real sensor data provided by the UCI Machine Learning Repository, using various external validation metrics.",J. Sens. Actuator Networks,2019,10.3390/jsan8040056,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
00f204bacaa431b485ea70c4763f9e09733cbcf1,https://www.semanticscholar.org/paper/00f204bacaa431b485ea70c4763f9e09733cbcf1,On the Generation of Anomaly Detection Datasets in Industrial Control Systems,"In recent decades, Industrial Control Systems (ICS) have been affected by heterogeneous cyberattacks that have a huge impact on the physical world and the people’s safety. Nowadays, the techniques achieving the best performance in the detection of cyber anomalies are based on Machine Learning and, more recently, Deep Learning. Due to the incipient stage of cybersecurity research in ICS, the availability of datasets enabling the evaluation of anomaly detection techniques is insufficient. In this paper, we propose a methodology to generate reliable anomaly detection datasets in ICS that consists of four steps: attacks selection, attacks deployment, traffic capture and features computation. The proposed methodology has been used to generate the Electra Dataset, whose main goal is the evaluation of cybersecurity techniques in an electric traction substation used in the railway industry. Using the Electra dataset, we train several Machine Learning and Deep Learning models to detect anomalies in ICS and the performed experiments show that the models have high precision and, therefore, demonstrate the suitability of our dataset for use in production systems.",IEEE Access,2019,10.1109/ACCESS.2019.2958284,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
de6f93627b6e794c22c4ec7fceff478b02a04be1,https://www.semanticscholar.org/paper/de6f93627b6e794c22c4ec7fceff478b02a04be1,"PREMISES, a Scalable Data-Driven Service to Predict Alarms in Slowly-Degrading Multi-Cycle Industrial Processes","In recent years, the number of industry-4.0-enabled manufacturing sites has been continuously growing, and both the quantity and variety of signals and data collected in plants are increasing at an unprecedented rate. At the same time, the demand of Big Data processing platforms and analytical tools tailored to manufacturing environments has become more and more prominent. Manufacturing companies are collecting huge amounts of information during the production process through a plethora of sensors and networks. To extract value and actionable knowledge from such precious repositories, suitable data-driven approaches are required. They are expected to improve the production processes by reducing maintenance costs, reliably predicting equipment failures, and avoiding quality degradation. To this aim, Machine Learning techniques tailored for predictive maintenance analysis have been adopted in PREMISES (PREdictive Maintenance service for Industrial procesSES), an innovative framework providing a scalable Big Data service able to predict alarming conditions in slowly-degrading processes characterized by cyclic procedures. PREMISES has been experimentally tested and validated on a real industrial use case, resulting efficient and effective in predicting alarms. The framework has been designed to address the main Big Data and industrial requirements, by being developed on a solid and scalable processing framework, Apache Spark, and supporting the deployment on modularized containers, specifically upon the Docker technology stack.",2019 IEEE International Congress on Big Data (BigDataCongress),2019,10.1109/BigDataCongress.2019.00032,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
3f272a4f39355933dcbc25e8cad4fdf235fc35e2,https://www.semanticscholar.org/paper/3f272a4f39355933dcbc25e8cad4fdf235fc35e2,Natural language processing of MIMIC-III clinical notes for identifying diagnosis and procedures with neural networks,"Coding diagnosis and procedures in medical records is a crucial process in the healthcare industry, which includes the creation of accurate billings, receiving reimbursements from payers, and creating standardized patient care records. In the United States, Billing and Insurance related activities cost around $471 billion in 2012 which constitutes about 25% of all the U.S hospital spending. In this paper, we report the performance of a natural language processing model that can map clinical notes to medical codes, and predict final diagnosis from unstructured entries of history of present illness, symptoms at the time of admission, etc. Previous studies have demonstrated that deep learning models perform better at such mapping when compared to conventional machine learning models. Therefore, we employed state-of-the-art deep learning method, ULMFiT on the largest emergency department clinical notes dataset MIMIC III which has 1.2M clinical notes to select for the top-10 and top-50 diagnosis and procedure codes. Our models were able to predict the top-10 diagnoses and procedures with 80.3% and 80.5% accuracy, whereas the top-50 ICD-9 codes of diagnosis and procedures are predicted with 70.7% and 63.9% accuracy. Prediction of diagnosis and procedures from unstructured clinical notes benefit human coders to save time, eliminate errors and minimize costs. With promising scores from our present model, the next step would be to deploy this on a small-scale real-world scenario and compare it with human coders as the gold standard. We believe that further research of this approach can create highly accurate predictions that can ease the workflow in a clinical setting.",ArXiv,2019,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
b77587379550f88dbb9d21b850ee69dad9014358,https://www.semanticscholar.org/paper/b77587379550f88dbb9d21b850ee69dad9014358,Learning-Based Reference-Free Speech Quality Assessment for Normal Hearing and Hearing Impaired Applications,"Accurate speech quality measures are highly attractive and beneficial in the design, finetuning, and benchmarking of speech processing algorithms, devices, and communication systems. Switching from narrowband telecommunication to wideband telephony is a change within the telecommunication industry which provides users with better speech quality experience but introduces a number of challenges in speech processing. Noise is the most common distortion on audio signals and as a result there have been a lot of studies on developing high performance noise reduction algorithms. Assistive hearing devices are designed to decrease communication difficulties for people with loss of hearing. As the algorithms within these devices become more advanced, it becomes increasingly crucial to develop accurate and robust quality metrics to assess their performance. Objective speech quality measurements are more attractive compared to subjective assessments as they are cost-effective and subjective variability is eliminated. Although there has been extensive research on objective speech quality evaluation for narrowband speech, those methods are unsuitable for wideband telephony. In the case of hearing-impaired applications, objective quality assessment is challenging as it has to be capable of distinguishing between desired modifications which make signals audible and undesired artifacts. In this thesis a model is proposed that allows extracting two sets of features from the distorted signal only. This approach which is called reference-free (nonintrusive) assessment is attractive as it does not need access to the reference signal. Although this benefit makes nonintrusive assessments suitable for real-time applications, more features need to be extracted and smartly combined to provide comparable accuracy as intrusive metrics. Two feature vectors are proposed to extract information from distorted signals and their performance is examined in three studies. In the first study, both feature vectors are trained on various portions of a noise reduction database for normal hearing applications. In the second study, the same investigation is performed on two sets of databases acquired through several hearing aids. Third study examined the generalizability of the proposed metrics on benchmarking four wireless remote microphones in a variety of environmental conditions. Machine learning techniques are deployed for training the models in the three studies. The studies show that one of the feature sets is robust when trained on different portions of the data from different databases and it also provides good quality prediction accuracy for both normal hearing and hearing-impaired applications.",,2018,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
e0bed1fcb5629744927922f46ec7c3f02c87671c,https://www.semanticscholar.org/paper/e0bed1fcb5629744927922f46ec7c3f02c87671c,"Fakebusters strike back: How to spot deep fakes, the manipulated videos that are the newest form of “fake news” to hit the internet","BELOW: Raymond Joseph teaches Ethiopian journalists how to spot fake photos A BASIC INTERNET SEARCH for “deep fakes” plus the names of actresses Daisy Ridley, Emma Watson and Scarlett Johansson or singers Katy Perry and Taylor Swift is decidedly not safe for viewing at work. It returns multiple hits from a wide variety of pornography websites of these famous women allegedly involved in a variety of kinky and shocking sex acts. But not one of these videos is genuine. They are all fakes of varying degrees of sophistication, created with free – and freely-available – software used to substitute their faces on the bodies of actual porn actresses. Welcome to the world of deep fakes – a portmanteau of “deep learning” and “fake” – which uses machine learning and artificial intelligence to create videos portraying people saying or doing things they never said or did. It is not the first time that the multibillion-dollar porn industry has taken the lead in creating or mainstreaming new tech, including e-commerce, webcams and streaming video. Deep-fake pornography first surfaced on the internet in 2017 when videos were posted on Reddit by a user under the pseudonym “Deepfakes”. The trickle of deep-fake hardcore porn videos soon turned into a deluge with the release of free software that made them relatively easy for anyone with a basic understanding of artificial intelligence to create. The problem became so serious that several platforms, including Reddit and Twitter, banned them. Late last year, Google also cracked down, adding “involuntary synthetic pornographic imagery” to its ban list and allowing anyone falsely depicted in them as “nude or in a sexually explicit situation” to request searches to the content be blocked. So common has celebrity deep-fake porn become that actress Scarlett Johansson told the Washington Post: “Clearly this doesn’t affect me as much because people assume it’s not actually me in a porno, however demeaning it is. I think it’s a useless pursuit legally, mostly because the internet is a vast wormhole of darkness that eats itself. The fact is that trying to protect yourself from the internet and its depravity is basically a lost cause, for the most part.” But dismissing online deep fakes as just something in the world of porn ignores the very real potential for them to be deployed on the mainstream web and in politics, taking online misinformation and disinformation to a new level. You need only think of the damaging and divisive role played by social media in the US and other elections, and Brexit, to realise the potential damage well-crafted deep fakes could cause. In fragile democracies divided by strongman politics and cultural and tribal divides, the potential for using them to stir up hate and violence is a very real possibility. Deep fakes are yet to be widely deployed “in the wild” beyond the world of porn, although there is a trend, which began as a meme on social media and went viral, of",Index on Censorship,2019,10.1177/0306422019841326,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
1ddb12710818e871b37d9fddd7f185dd722f408f,https://www.semanticscholar.org/paper/1ddb12710818e871b37d9fddd7f185dd722f408f,Emerging Trends of ML-based Intelligent Services for Industrial Internet of Things (IIoT),"Intelligent information technology is a notable feature in the context of industry 4.0. A key factor in obtaining intelligent industrial Internet of things (IIoT) services is to integrate machine learning (ML) into IIoT. With the increasing scale of deployed terminals, IIoT becomes heterogeneous, diverse, and dynamically changeable. Traditional optimization methods are difficult to deal with the emerging network problems. This paper first proposes a ML-based IIoT architecture for intelligent IIoT services and expounds two ML methods for IIoT analysis, namely, deep learning (DL) and reinforcement learning (RL). Secondly, advanced applications and development trends of ML in industrial field are summarized. Opportunities and challenges of ML for IIoT analysis are discussed finally. The purpose of this paper is to point out the role of artificial intelligence (AI) technology in IIoT from the macroscopic view.","2019 Computing, Communications and IoT Applications (ComComAp)",2019,10.1109/ComComAp46287.2019.9018815,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
5065863acc6b5a105958d95270b4c2e8e7ae019f,https://www.semanticscholar.org/paper/5065863acc6b5a105958d95270b4c2e8e7ae019f,IPOD: An Industrial and Professional Occupations Dataset and its Applications to Occupational Data Mining and Analysis,"Occupational data mining and analysis is an important task in understanding today's industry and job market. Various machine learning techniques are proposed and gradually deployed to improve companies' operations for upstream tasks, such as employee churn prediction, career trajectory modelling and automated interview. Job titles analysis and embedding, as the fundamental building blocks, are crucial upstream tasks to address these occupational data mining and analysis problems. In this work, we present the Industrial and Professional Occupations Dataset (IPOD), which consists of over 190,000 job titles crawled from over 56,000 profiles from Linkedin. We also illustrate the usefulness of IPOD by addressing two challenging upstream tasks, including: (i) proposing Title2vec, a contextual job title vector representation using a bidirectional Language Model (biLM) approach; and (ii) addressing the important occupational Named Entity Recognition problem using Conditional Random Fields (CRF) and bidirectional Long Short-Term Memory with CRF (LSTM-CRF). Both CRF and LSTM-CRF outperform human and baselines in both exact-match accuracy and F1 scores. The dataset and pre-trained embeddings are available at https://www.github.com/junhua/ipod.",,2019,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
d827e7d7205093bb11d8acaa110bc5e71771b16f,https://www.semanticscholar.org/paper/d827e7d7205093bb11d8acaa110bc5e71771b16f,A Predictive Maintenance Solution for Bearing Production Line Based on Edge-Cloud Cooperation,"Effective and accurate maintenance of equipment is a difficult problem for the manufacturing industry. With the development of technologies such as the Internet of Things, artificial intelligence, and edge computing, predictive maintenance (PdM) has brought new solutions to this problem. Taking the bearing production line as the application object, this paper proposes a predictive maintenance solution of “edge cloud coordination”. By deploying sensors in the production line equipment to realize equipment data collection, data cleaning, data standardization and real-time control are completed at the edge end. The cloud analyzes the data through machine learning algorithms to predict equipment failure and remaining service life, thereby reducing equipment maintenance costs and improving equipment utilization. Experiments show that this solution provides a powerful means to ensure efficient and stable operation of the bearing production line, which can effectively predict equipment failure and equipment life, and reduce operation and maintenance costs, with feasibility and good market application prospects.",2019 Chinese Automation Congress (CAC),2019,10.1109/CAC48633.2019.8996482,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
62193342da8df71189ac3756b8f35836f71ef5de,https://www.semanticscholar.org/paper/62193342da8df71189ac3756b8f35836f71ef5de,A Vision and Framework for the High Altitude Platform Station (HAPS) Networks of the Future,"A High Altitude Platform Station (HAPS) is a network node that operates in the stratosphere at an of altitude around 20 km and is instrumental for providing communication services. Precipitated by technological innovations in the areas of autonomous avionics, array antennas, solar panel efficiency levels, and battery energy densities, and fueled by flourishing industry ecosystems, the HAPS has emerged as an indispensable component of next-generations of wireless networks. In this article, we provide a vision and framework for the HAPS networks of the future supported by a comprehensive and state-of-the-art literature review. We highlight the unrealized potential of HAPS systems and elaborate on their unique ability to serve metropolitan areas. The latest advancements and promising technologies in the HAPS energy and payload systems are discussed. The integration of the emerging Reconfigurable Smart Surface (RSS) technology in the communications payload of HAPS systems for providing a cost-effective deployment is proposed. A detailed overview of the radio resource management in HAPS systems is presented along with synergistic physical layer techniques, including Faster-Than-Nyquist (FTN) signaling. Numerous aspects of handoff management in HAPS systems are described. The notable contributions of Artificial Intelligence (AI) in HAPS, including machine learning in the design, topology management, handoff, and resource allocation aspects are emphasized. The extensive overview of the literature we provide is crucial for substantiating our vision that depicts the expected deployment opportunities and challenges in the next 10 years (next-generation networks), as well as in the subsequent 10 years (next-next-generation networks).",IEEE Communications Surveys & Tutorials,2020,10.1109/COMST.2021.3066905,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
c1e79df58927e7f428bddcfd66cd146c445ba33c,https://www.semanticscholar.org/paper/c1e79df58927e7f428bddcfd66cd146c445ba33c,Neural Network with Confidence Kernel for Robust Vibration Frequency Prediction,"Image-based measurement has received increasing attention as it can substantially reduce the cost of labor, measurement equipment, and installation process. Instead of using optical flow, pattern, or marker tracking to extract a displacement signal, in this study, a novel noncontact machine learning-based system was proposed to directly predict vibration frequency with high accuracy and good reliability by using image sequences acquired from a single camera. The performance of the proposed method was demonstrated through experiments conducted in a laboratory and under real-field conditions and compared with those obtained using a contacted sensor. The vibration frequency prediction results of the proposed method are compared with industry-level vibration sensor results in the frequency domain, demonstrating that the proposed method could predict the target-object-vibration frequency as accurately as an industry-level vibration sensor, even under uncontrollable real-field conditions with no additional enhancement or extra signal processing techniques. However, only the principal vibration frequency of a measurement target is predicted, and the measurement range is limited by the trained model. Nonetheless, if these limitations are resolved, this method can potentially be used in real engineering applications in mechanical or civil structural health monitoring thanks to the simple deployment and concise pipeline of this method.",J. Sensors,2019,10.1155/2019/6573513,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
c26b41ca6231f86ee7c3cbe99557ce75622cdca5,https://www.semanticscholar.org/paper/c26b41ca6231f86ee7c3cbe99557ce75622cdca5,Active-Code Replacement in the OODIDA Data Analytics Platform,,Euro-Par Workshops,2019,10.1007/978-3-030-48340-1_55,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
7d689a8c11cfc93b2921ce2b51cc8e2c16224337,https://www.semanticscholar.org/paper/7d689a8c11cfc93b2921ce2b51cc8e2c16224337,"iSTEP, an Integrated Self-Tuning Engine for Predictive Maintenance in Industry 4.0","The recent expansion of IoT-enabled (Internet of Things) devices in manufacturing contexts and their subsequent data-driven exploitation paved the way to the advent of the Industry 4.0, promoting a full integration of IT services, smart devices, and control systems with physical objects, their electronics and sensors. The real-time transmission and analysis of collected data from factories has the potential to create manufacturing intelligence, of which predictive maintenance is an expression. Hence the need to design new approaches able to manage not only the data volume, but also the variety and velocity, extracting actual value from the humongous amounts of collected data. To this aim, we present iSTEP, an integrated Self-Tuning Engine for Predictive maintenance, based on Big Data technologies and designed for Industry 4.0 applications. The proposed approach targets some of the most common needs of manufacturing enterprises: compatibility with both the on-premises and the in-the-cloud environments, exploitation of reliable and largely supported Big Data platforms, easy deployment through containerized software modules, virtually unlimited horizontal scalability, fault-tolerant self-reconfiguration, flexible yet friendly streaming-KPI computations, and above all, the integrated provisioning of self-tuning machine learning techniques for predictive maintenance. The current implementation of iSTEP exploits a distributed architecture based on Apache Kafka, Spark Streaming, MLlib, and Cassandra; iSTEP provides (i) a specific feature engineering block aimed at automatically extracting metrics from the production monitoring time series, which improves the predictive performance by 77% on average, and (ii) a self-tuning approach that dynamically selects the best prediction algorithm, which improves the predictive performance up to 60%. The iSTEP engine provides transparent predictive models, able to provide end users with insights into the knowledge learned, and it has been experimentally evaluated on a public unbalanced failure dataset, whose extensive results are discussed in the paper.","2018 IEEE Intl Conf on Parallel & Distributed Processing with Applications, Ubiquitous Computing & Communications, Big Data & Cloud Computing, Social Computing & Networking, Sustainable Computing & Communications (ISPA/IUCC/BDCloud/SocialCom/SustainCom)",2018,10.1109/BDCloud.2018.00136,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
a6c7b3c141f6c604e08fee902fa268665b8a4221,https://www.semanticscholar.org/paper/a6c7b3c141f6c604e08fee902fa268665b8a4221,Simple linear classifiers via discrete optimization: learning certifiably optimal scoring systems for decision-making and risk assessment,"Scoring systems are linear classification models that let users make quick predictions by adding, subtracting, and multiplying a few small numbers. These models are widely used in applications where humans have traditionally made decisions because they are easy to understand and validate. In spite of extensive deployment, many scoring systems are still built using ad hoc approaches that combine statistical techniques, heuristics, and expert judgement. Such approaches impose steep trade-offs with performance, making it difficult for practitioners to build scoring systems that will be used and accepted. In this dissertation, we present two new machine learning methods to learn scoring systems from data: Supersparse Linear Integer Models (SLIM) for decision-making applications; and Risk-calibrated Supersparse Linear Integer Models (RiskSLIM) for risk assessment applications. Both SLIM and RiskSLIM solve discrete optimization problems to learn scoring systems that are fully optimized for feature selection, small integer coefficients, and operational constraints. We formulate these problems as integer programming problems and develop specialized algorithms to recover certifiably optimal solutions with an integer programming solver. We illustrate the benefits of this approach by building scoring systems for realworld problems such as recidivism prediction, sleep apnea screening, ICU seizure prediction, and adult ADHD diagnosis. Our results show that a discrete optimization approach can learn simple models that perform well in comparison to the state-ofthe-art, but that are far easier to customize, understand, and validate. Thesis Supervisor: Cynthia Rudin Title: Associate Professor of Computer Science Duke University",,2017,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
6a2a534a9d01b76ecb5caba20032022a904cb50d,https://www.semanticscholar.org/paper/6a2a534a9d01b76ecb5caba20032022a904cb50d,COSMIC Semantic Segmentation Framework,"Deep space missions such as the Mars Reconnaissance Orbiter collect more data than can be sent back to Earth due to limited communications bandwidth. Machine learning algorithms can be deployed on board orbiters to prioritize the downlink of scientifically interesting images, such as those including fresh impact craters, recurring slope lineae, or dust devils. However, basic machine learning research is necessary to boost realworld performance, and numerous possible convolutional neural network architectures must be evaluated in terms of accuracy and compute requirements. A framework is designed to reduce redundant development, to standardize the algorithm testing process, and to allow developers to focus on the implementation details of novel machine learning algorithms. Three convolutional neural network implementations are included with the framework, pending use in future research. 1Content-based Onboard Summarization to Monitor Infrequent Change 2CL #18-465",,2019,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
cd51bbfd51cde0c089d9dfb7d30bfc124d9b7c55,https://www.semanticscholar.org/paper/cd51bbfd51cde0c089d9dfb7d30bfc124d9b7c55,"Summary for CIFE Seed Proposals for Academic Year 2020-21 Proposal number: 2020-04 Proposal title: Hybrid Physical-Digital Spaces: Transforming the Design, Operation, and Experience of Built Environments to Promote Health and Wellbeing","up to 150 words) Increasing evidence suggests built office features (e.g., lighting, materials, and ventilation) have substantial impacts on occupant wellbeing. A key next direction is field studies at industry partner sites to examine real-world workplaces. We propose to develop innovative Internet of Things (IoT) techniques that integrate data from building instrumentation, personal device sensors, and self-report interfaces and then deploy this platform in-the-wild to capture rich, longitudinal, ecologically-valid data about the status of office workers and the spaces they occupy. Insights will advance scientific knowledge of how buildings impact wellbeing as well as produce practical implications for building designers and operators. A timely component will explore how covid-19 has temporally or fundamentally changed occupant behaviors and operational decisions (e.g., physical distancing desks and ventilation settings that reduce pathogen spread). Overall, our proposed research has the potential to transform the industry’s thinking on how built environments can be designed, operated, and experienced. Hybrid Physical-Digital Spaces: Transforming the Design, Operation, and Experience of Built Environments to Promote Health and Wellbeing Problem and Significance Considering that people in the U.S. spend 87% of their time in indoor spaces , we assert that 1 buildings are powerful yet underleveraged loci for promoting human wellbeing. Imagine an intelligent office that could adapt soundscape systems to manage noise in open floor plans, optimize space reservation or utilization to foster collaborations and save energy, or provide digital information displays that promote employee connectedness and physical activity. Towards actualizing our vision of such hybrid physical-digital spaces, our proposal strives to develop, apply, and evaluate novel scientific and engineering approaches that will transform the industry’s thinking around how built environments can be designed, operated, and experienced. Increasingly, hypotheses suggest that built features of indoor environments (e.g., lighting, materials, and ventilation) have substantial impacts on occupants (e.g., employee recruitment and retention, absenteeism, cognition, creativity, productivity, social interactions, physical activity and health, and psychological wellbeing). In turn, these individual outcomes also drive pivotal organizational outcomes such as product innovation, workforce diversity, employee turnover, market share, and profitability. Examples illustrate how building interventions can have huge impacts : enhancing employee exposure to daylight can save businesses ~$2,000/yr per capita 2 , better air quality can raise cognitive scores of workers by 101% 3 , and increasing indoor access to biophilic elements could recoup $23 billion considering 10% of workplace absenteeism (a $226 billion dollar problem) is attributable to architecture that inadequately connects to nature 4 . However, few of these hypotheses have been tested at scale, over time, and in real world conditions . Instead, most prior efforts are small sample, short-term correlational studies based on potentially biased and sparse self-reported data. A more rigorous, scientific, and human-centered approach to study and engineer buildings that promote wellbeing can have major implications at individual, organizational, and societal levels (see Figure 1), offering both foundational theoretical knowledge as well as practical strategies for building designers and operators. Figure 1. Relations among building features and human outcomes at various levels. Further, “smart buildings” today typically focus on basic sensing and control for energy savings, thermal comfort, and security. Connecting to CIFE’s Vision for the Future of Building Users, we argue buildings of the future can go beyond such bottom line outcomes to be more interactive and human-centered: aware of and responsive to occupants’ cognitive, mental, and physical feelings and needs, while respecting privacy and promoting positive indoor experiences . 1 Klepeis, et al., 2001; 2 Heschong & Mahone, 2003; 3 Allen et al., 2016; 4 Elzeyadi, 2011. <Landay-Billington> < Hybrid Physical-Digital Spaces> 1 Theoretical and Practical Points of Departure It is imperative to increase understanding of exactly what built attributes have what impacts and on whom, in a scalable, longitudinal, and inclusive manner. Thus through technology-driven assessment and hybrid physical-digital interventions, we aim to (a) fundamentally advance the science on how built environments impact human wellbeing and, in turn, (b) generate guidelines that can revolutionize the way spaces are designed, operated, and experienced . Our current scope focuses on office spaces and workers; though an overarching goal is for our developed approaches and insights to establish a foundation that enables future research with additional populations and environments (e.g., physicians and patients in clinical settings, students and teachers in classrooms, and traditionally marginalized shift and temporary workers). In particular, our reusable platform will help others study this wider range of buildings and occupants; and combining these approaches with emerging endeavors such as biophilic design and precision interventions provides a novel opportunity to not only more deeply investigate but also address long-running public health challenges and systemic inequities facing society. In these ways, we hope to positively impact a broad cross-section of stakeholders at individual, organizational, and institutional levels. Moreover, this project will support interdisciplinary fertilization across engineering, computing, psychology, law, and medicine . Research Methods and Work Plan Our research agenda is to support the design and operation of built facilities that augment human capabilities and wellbeing — and have a fundamental positive change on the way indoor spaces are experienced by the people that occupy them. By introducing intelligent systems capable of gathering and interpreting building and occupant data as well as delivering adaptive interventions in response, novel roles will also emerge for managing buildings and the activities that take place inside them. To achieve these goals, our research will comprise three main activities: 1. Developing an extensible and secure data collection and machine learning platform . A key aim of this research is scientifically examining how built spaces impact human wellbeing. To pursue this investigation and develop methods that enable buildings to be more aware of occupants’ states and needs, we have been developing pattern detection software that integrates data from (a) personal devices (smartphones, smartwatches, fitness trackers), (b) building instrumentation or portable environmental sensors (light levels, air quality), and (c) experience sampling interfaces that prompt occupants for subjective information through quick, validated self-report techniques. Figure 2 illustrates examples of these assessment components. This work involves addressing a number of technical challenges, such as selecting sampling rates and window sizes to maximize efficiency, developing methods for analyzing asynchronous and sparse sensor data, and developing privacy-sensitive feature engineering strategies for detecting and predicting wellbeing outcomes of interest. We also plan to package our platform as a reusable toolkit that can be applied by other researchers and building operators. This work is ongoing and a basic version will be ready by summer. Once development is complete, CIFE support would allow us to move onto the next critical phase: moving out of the lab and into the field. <Landay-Billington> < Hybrid Physical-Digital Spaces> 2 Figure 2. Platform to integrate data from personal devices, building sensors, and subjective self-report. 2. Deploying the platform through a mixed-method study with industry partners . The next step in our research is to deploy this platform at field sites in partnership with View, Inc. (specifically, at TIAA offices in Manhattan, this summer/fall) to capture rich, longitudinal, ecologically-valid data about behavioral, psychological, and physiological states of occupants and their everyday work environments. Our plan is to recruit a sample of approximately 150 employees for a period of 18 weeks, which will involve a baseline phase followed by systematic variation of built features (Views/No Views, Plants/No Plants, and Diversity/No Diversity in artwork) and measurement of indicators hypothesized to promote both personal wellbeing and organizational performance, based on the literature and our formative online and lab studies, described below. In combination with the engineering-focused activities to implement and install the platform, deployment will occur in tandem with ethnographic work (e.g., observations, interviews, and surveys) to manually validate reliability of the system’s automated inferences as well as gain a more qualitative portrait of occupant experiences in various spaces. Privacy-centric engagements will additionally investigate stakeholders’ attitudes regarding the capture of various types of information to derive implications about informed consent and personal data management. Along similar lines, it will be critical to responsibly manage captured data, especially potentially sensitive and exploitable data about wellness or performance. Therefore all studies will be conducted with oversight and approval from the Stanford Institutional Review Board (IRB). In addition to obtaining participants’ informed consent, we will also design sensor and data collection mechanisms to use an opt-in model, including partial participation. Our data management systems can also allow individuals to view and delete their personal data, including if purging is desired in the event of study withdrawal. Our research team has exp",,2020,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
cb9539dbba61a6e97e9db30ba16b7542fc1d2a65,https://www.semanticscholar.org/paper/cb9539dbba61a6e97e9db30ba16b7542fc1d2a65,Technology Focus: Seismic (March 2020),"Technology Focus
 Two years ago, in this column, I focused on new, exciting technology advances that were being made with the seismic method. Unfortunately, 1 year ago, I had to change the theme to the deepening economic downturn that had occurred in the industry. Happily, though, in this past year, we witnessed several examples of marriage between these two themes. In other words, necessity for survival in hard times provided the impetus for innovation.
 For example, in one case of land seismic, the acquisition system was redesigned. This started with the stripping down of the nodal geophone, leaving only the very basic functionality that was needed. Ultimately, this led to a much lighter system, not only reducing power requirements but also the labor needed to deploy (and later retrieve) the ground equipment. In turn, this efficiency led to lower costs while still allowing advancement to the denser, spatial sampling enabled by higher channel counts.
 In another case, success was achieved not through changes in technology but rather through changes in business strategy. Historically, when downturns occur, one of the first line items to be cut is the acquisition of new seismic programs. However, the strategy reported by one courageous company was to continue with its usual level of advanced seismic acquisition and processing, while saving costs by reducing the exploration drilling. This strategy of protecting the ability to identify future, high-quality prospects is what led to success.
 Continued development of seismic applications occurred in many other disciplines, too, including unconventionals, microseismic, and imaging. This often took place through effective use of multidisciplinary teams and machine learning.
 That discussion is supported by the three synopsized papers included in this Technology Focus section, as well as the three papers that are recommended for additional reading. Enjoy.
 Recommended additional reading at OnePetro: www.onepetro.org.
 SPE 197289 Acquisition of an Ultrahigh-Density 3D Seismic Survey Using New Nimble Nodes Onshore Abu Dhabi by Hani Nehaid, Abu Dhabi National Oil Company, et al.
 SPE 197822 Application of Machine Learning To Estimate Sonic Data for Seismic Well Ties, Bongkot Field, Thailand by Nisa Sukkee, PTTEP, et al.
 SPE 195522 A Novel Approach to Discovery of Hidden Structures in Microseismic Data Using Machine-Learning Techniques by Maxim Yatsenko, Texas A&M University, et al.",,2020,10.2118/0320-0069-jpt,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
50b3e2fed21d1050ad38f495a41729dea67cc9ff,https://www.semanticscholar.org/paper/50b3e2fed21d1050ad38f495a41729dea67cc9ff,A tool for semi-automatic ground truth annotation of traffic videos,"We have developed a semi-automatic annotation tool – “CVL Annotator” – for bounding box ground truth generation in videos. Our research is particularly motivated by the need for reference annotations of challenging nighttime traffic scenes with highly dynamic lighting conditions due to reflections, headlights and halos from oncoming traffic. Our tool incorporates a suite of different state-of-the-art tracking algorithms in order to minimize the amount of human input necessary to generate high-quality ground truth data. We focus our user interface on the premise of minimizing user interaction and visualizing all information relevant to the user at a glance. We perform a preliminary user study to measure the amount of time and clicks necessary to produce ground truth annotations of video traffic scenes and evaluate the accuracy of the final annotation results. Introduction In the context of computer vision and machine learning algorithms for assisted/autonomous driving, the need for training and evaluation data in the automotive industry is increasing significantly. The goal is to ultimately deploy autonomous vehicles into traffic that is subject to unpredictable environmental influences, such as changing weather and lighting conditions. Various scientific groups and companies have created and published road scene ground truth datasets (e.g. Argoverse [2], CityScapes [3], BDD100K [4], KITTI [7], CamVid [5], D2-City [6], VIPER [8]) to further research on autonomous vehicles and machine learning. The work presented in this paper is embedded in the CarVisionLight (CVL) project, which aims to develop an object detection algorithm for night scenes with temporal consistency (see also [1]). To achieve this goal with a supervised machine learning algorithm, we defined the following requirements, which a training dataset should ideally meet: • non-urban roads (e.g. highway or country roads) • nighttime • at least 20FPS temporal density • realistic lighting in a real-world environment. Table 1 gives an overview of various datasets (both road scenes as well as general scenes) we have reviewed. Several datasets (e.g. GOT10k [18], VOT2017 [19], VIPER [8], D2-City [6], BDD100K [4]) have the temporal density needed for our application. While temporally dense night scenes are included in some of them ([2], [6]), we noticed a shortage of footage from nonurban roads. In the case of snythetically generated videos, such as in ([8]), we observed a lack of natural lighting variability (high dynamic contrast, glaring, halos or reflections). Dataset Non-Urban Night ≥ 20 FPS Real Argoverse [2] 7 3 3 3",,2020,10.2352/ISSN.2470-1173.2020.16.AVM-150,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
1da0bd6f22d7dd9a4e9c345d80afb40d008e7541,https://www.semanticscholar.org/paper/1da0bd6f22d7dd9a4e9c345d80afb40d008e7541,An Effort to Democratize Networking Research in the Era of AI/ML,"A growing concern within today's networking community is that with the proliferation of Artificial Intelligence/Machine Learning (AI/ML) techniques, a lack of access to real-world production networks is putting academic researchers at a significant disadvantage. Indeed, compared to a select few research groups in industry that can leverage access to their global-scale production networks in their data-driven efforts to develop and evaluate learning models, academic researchers not only struggle to get their hands on real-world data sets but find it almost impossible to adequately train and assess their learning models under realistic conditions. In this paper, we argue that when appropriately instrumented and properly managed, enterprise networks in the form of university or campus networks can serve as real-world production networks and can, because of their ubiquity, help create a more level playing field for academic researchers. Their various limitations notwithstanding, as real-world production networks, such enterprise networks can (i) serve as unique sources for some of the rich data that will enable these researchers to influence or advance the current state-of-the-art in AI/ML for networking and (ii) also function as much-needed test beds where newly developed AI/ML-based tools can be evaluated or ""road-tested"" prior to their actual deployment in the production network. We discuss new research challenges that arise from this proposed dual role of campus networks and comment on the opportunities our proposal affords for both academic and industry researchers to benefit from the advantages and limitations of their respective production environments in their common quest to advance the development and evaluation of AI/ML-based tools to the point where they can be deployed in practice.",HotNets,2019,10.1145/3365609.3365857,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
38d17764c33caffef5441decb36beac7beac3ff3,https://www.semanticscholar.org/paper/38d17764c33caffef5441decb36beac7beac3ff3,A Systematic Mapping Study of Innovative Cloud Applications,"Cloud computing is a business paradigm wherein computers and computing related services are provided by Cloud Service Providers to consumers either as software, development platform, or infrastructure. Innovative applications are growing in a productive manner on the cloud landscape. Innovative applications are being developed for use in the area of e-learning, automotive processes, cloud containers and machine learning. The objective of this paper is to conduct a systematic mapping study of innovative cloud applications and experiences. The systematic map provided a structured overview of research work carried out and the frequency of publications, presenting them pictorially in form of a map. The obtained results showed that 7.34% of the publications were on development of innovative cloud applications in terms of model. Architecture and modelling, and simulation in relation to model were both at 13.76%, 11.93% of the papers respectively, while 8.26% of the articles were on deployment in terms of process. Architecture had most publication in the area of solution research, with 15.2%. For articles published on deployment and development, most were on solution research with 8.80% and 14.40% respectively. The outcome of this study will be beneficial to practitioners in the industry and academic researchers alike.",IOP Conference Series: Materials Science and Engineering,2020,10.1088/1757-899X/811/1/012034,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
7ec15731613ac0dc5d09125d325e7970a5a6a1ea,https://www.semanticscholar.org/paper/7ec15731613ac0dc5d09125d325e7970a5a6a1ea,Communications for IoT: Connectivity and Networking,"The Internet of Things (IoT) is revolutionizing many industries by enabling machines to directly work with each other without human intervention, and communication and networking technologies are fundamental to enabling it. IoT systems are also creating a significant amount of new data which can now be processed to analyze and learn from using novel machine-learning techniques. However, the requirements for communications can vary significantly depending on the target applications, and they can range from ultra-low power for enabling a vast deployment of sensors with multi-year battery life to ultra-reliable low latency communications (URLLC) for smart factories and remote robot control. 5G, as an example, is expected to deliver a number of enhancements to support massive IoT deployments as well as URLLC capabilities for new verticals such as Smart Cities, Industrial IoT, E-Health, Public Safety, and Autonomous Vehicles (V2V, V2X). The purpose of this Special Issue is to provide researchers and practitioners working on Connectivity and Networking technologies for IoT systems a means to share experiences and disseminate successful applications of IoT technologies and identify recent trends and opportunities poised to revolutionize the IoT industry.",IEEE Internet of Things Magazine,2020,10.1109/MIOT.2020.9063399,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
a8c57f226e422461c2f50e886a990013e36ccc14,https://www.semanticscholar.org/paper/a8c57f226e422461c2f50e886a990013e36ccc14,"Digitising the Industry - Internet of Things Connecting the Physical, Digital and Virtual Worlds","This book provides an overview of the current Internet of Things (IoT) landscape, ranging from the research, innovation and development priorities to enabling technologies in a global context. A successful deployment of IoT technologies requires integration on all layers, be it cognitive and semantic aspects, middleware components, services, edge devices/machines and infrastructures. It is intended to be a standalone book in a series that covers the Internet of Things activities of the IERC Internet of Things European Research Cluster from research to technological innovation, validation and deployment. The book builds on the ideas put forward by the European Research Cluster and the IoT European Platform Initiative (IoT-EPI) and presents global views and state of the art results on the challenges facing the research, innovation, development and deployment of IoT in the next years. The IoT is bridging the physical world with virtual world and requires sound information processing capabilities for the ""digital shadows"" of these real things. The research and innovation in nanoelectronics, semiconductor, sensors/actuators, communication, analytics technologies, cyber-physical systems, software, swarm intelligent and deep learning systems are essential for the successful deployment of IoT applications. The emergence of IoT platforms with multiple functionalities enables rapid development and lower costs by offering standardised components that can be shared across multiple solutions in many industry verticals. The IoT applications will gradually move from vertical, single purpose solutions to multi-purpose and collaborative applications interacting across industry verticals, organisations and people, being one of the essential paradigms of the digital economy. Many of those applications still have to be identified and involvement of end-users including the creative sector in this innovation is crucial. The IoT applications and deployments as integrated building blocks of the new digital economy are part of the accompanying IoT policy framework to address issues of horizontal nature and common interest (i.e. privacy, end-to-end security, user acceptance, societal, ethical aspects and legal issues) for providing trusted IoT solutions in a coordinated and consolidated manner across the IoT activities and pilots. In this, context IoT ecosystems offer solutions beyond a platform and solve important technical challenges in the different verticals and across verticals. These IoT technology ecosystems are instrumental for the deployment of large pilots and can easily be connected to or build upon the core IoT solutions for different applications in order to expand the system of use and allow new and even unanticipated IoT end uses. Technical topics discussed in the book include: IntroductionDigitising industry and IoT as key enabler in the new era of Digital EconomyIoT Strategic Research and Innovation Agenda IoT in the digital industrial context: Digital Single MarketIntegration of heterogeneous systems and bridging the virtual,",,2016,10.13052/rp-9788793379824,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
71104a613283ae095345cdb4becc18303fcecff3,https://www.semanticscholar.org/paper/71104a613283ae095345cdb4becc18303fcecff3,Intelligent Data Engineering and Automated Learning – IDEAL 2013,,Lecture Notes in Computer Science,2013,10.1007/978-3-642-41278-3,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
d318a7e71faffee00acdd0c53709348370e908ba,https://www.semanticscholar.org/paper/d318a7e71faffee00acdd0c53709348370e908ba,Pushing the AI Envelope: Merging Deep Networks to Accelerate Edge Artificial Intelligence in Consumer Electronics Devices and Systems,"Deep neural networks (DNNs) are widely used by both academic and industry researchers to solve many long-standing problems in machine learning. There has been such a growth of research in this field, and it has been applied to so many varying problems, that it would be accurate to say that we may be living through the precursor of the singularity [1]. But regardless of one's views on artificial intelligence (AI), there is no doubt that there is a wealth of recent research that leverages the use of various DNNs to solve a broad range of pattern recognition and classification problems. Examples range from the introduction of smart speakers with intelligent assistants to the application of DNNs to solve recalcitrant problems in computer vision for autonomous vehicles. Many of these problems can have very useful applications in the design of smarter consumer electronics (CE) systems and devices. The question for CE engineers is how to leverage this wealth of academic and industry research efforts, turning them into practical DNN solutions suitable for deployment in practical devices and electronic systems.",IEEE Consumer Electronics Magazine,2018,10.1109/MCE.2017.2775245,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
6bdb186ec4726e00a8051119636d4df3b94043b5,https://www.semanticscholar.org/paper/6bdb186ec4726e00a8051119636d4df3b94043b5,Caffe: Convolutional Architecture for Fast Feature Embedding,"Caffe provides multimedia scientists and practitioners with a clean and modifiable framework for state-of-the-art deep learning algorithms and a collection of reference models. The framework is a BSD-licensed C++ library with Python and MATLAB bindings for training and deploying general-purpose convolutional neural networks and other deep models efficiently on commodity architectures. Caffe fits industry and internet-scale media needs by CUDA GPU computation, processing over 40 million images a day on a single K40 or Titan GPU (approx 2 ms per image). By separating model representation from actual implementation, Caffe allows experimentation and seamless switching among platforms for ease of development and deployment from prototyping machines to cloud environments. Caffe is maintained and developed by the Berkeley Vision and Learning Center (BVLC) with the help of an active community of contributors on GitHub. It powers ongoing research projects, large-scale industrial applications, and startup prototypes in vision, speech, and multimedia.",ACM Multimedia,2014,10.1145/2647868.2654889,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
e2a432f9dc80b20b273c1c2be1890b1eb6e7e9c0,https://www.semanticscholar.org/paper/e2a432f9dc80b20b273c1c2be1890b1eb6e7e9c0,Online Tool Wear Classification during Dry Machining Using Real Time Cutting Force Measurements and a CNN Approach,"The new generation of ICT solutions applied to the monitoring, adaptation, simulation and optimisation of factories are key enabling technologies for a new level of manufacturing capability and adaptability in the context of Industry 4.0. Given the advances in sensor technologies, factories, as well as machine tools can now be sensorised, and the vast amount of data generated can be exploited by intelligent information processing techniques such as machine learning. This paper presents an online tool wear classification system built in terms of a monitoring infrastructure, dedicated to perform dry milling on steel while capturing force signals, and a computing architecture, assembled for the assessment of the flank wear based on deep learning. In particular, this approach demonstrates that a big data analytics method for classification applied to large volumes of continuously-acquired force signals generated at high speed during milling responds sufficiently well when used as an indicator of the different stages of tool wear. This research presents the design, development and deployment of the system components and an overall evaluation that involves machining experiments, data collection, training and validation, which, as a whole, has shown an accuracy of 78 % .",Journal of Manufacturing and Materials Processing,2018,10.3390/JMMP2040072,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
96f7123a27fb52e18ff8d9542ea78a4c5196a65e,https://www.semanticscholar.org/paper/96f7123a27fb52e18ff8d9542ea78a4c5196a65e,Extreme Classification (Dagstuhl Seminar 18291),"Extreme classification is a rapidly growing research area within machine learning focusing on multi-class and multi-label problems involving an extremely large number of labels (even more than a million). Many applications of extreme classification have been found in diverse areas ranging from language modeling to document tagging in NLP, face recognition to learning universal feature representations in computer vision, gene function prediction in bioinformatics, etc. Extreme classification has also opened up a new paradigm for key industrial applications such as ranking and recommendation by reformulating them as multi-label learning tasks where each item to be ranked or recommended is treated as a separate label. Such reformulations have led to significant gains over traditional collaborative filtering and content-based recommendation techniques. Consequently, extreme classifiers have been deployed in many real-world applications in industry. 
 
Extreme classification has raised many new research challenges beyond the pale of traditional machine learning including developing log-time and log-space algorithms, deriving theoretical bounds that scale logarithmically with the number of labels, learning from biased training data, developing performance metrics, etc. The seminar aimed at bringing together experts in machine learning, NLP, computer vision, web search and recommendation from academia and industry to make progress on these problems. We believe that this seminar has encouraged the inter-disciplinary collaborations in the area of extreme classification, started discussion on identification of thrust areas and important research problems, motivated to improve the algorithms upon the state-of-the-art, as well to work on the theoretical foundations of extreme classification.",Dagstuhl Reports,2018,10.4230/DagRep.8.7.62,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
f93c7edee65ee73923d5110c2d17cbce1d4e544d,https://www.semanticscholar.org/paper/f93c7edee65ee73923d5110c2d17cbce1d4e544d,Manufacturing Execution Systems Mes Optimal Design Planning And Deployment,"The thesis work introduces an assessment framework consisting of decisive criteria and related indicators which describe qualitatively the suitability of AI for MES functions based on three criteria with related indicators. In addition, the researcher displays furthermore how the developed assessment framework can be used in order to assess the MES functions regarding their AI “readiness”. In order to cope the findings through the thesis work an inductive research approach has been applied. Existing literature in the fields of intelligent manufacturing, Manufacturing-Execution-Systems, machine learning, deep learning, intelligent manufacturing, digital twin, and assessment methodologies have been extensively studied in order to base the theoretical developed framework on grounded theory. A case study was carried out in order to test the validity and reliability of the developed assessment framework for industry. The outcome of this thesis work was an assessment framework consisting of decisive criteria and related indicators when evaluating a MES function in respect to its AI suitability. Furthermore, an assessment checklist has been provided for the industry in order to be able to assess a MES function regards AI support in a quick and pragmatic manner. To generate a more generalizable assessment framework criteria and indicators have to be adapted, likewise testing the outcome of analogue and digital assessment methodologies will provide material for future studies. Artificial Intelligence arises in the manufacturing field very rapidly. Implementing Artificial Intelligence (AI) solutions and algorithms in the manufacturing environment is a wellknown research field in academia. On the other hand, Manufacturing-Execution-System (MES) providers do not have a theoretical and pragmatic framework regarding the evaluation of MES functions in respect to their suitability for Artificial Intelligence. In order to be able to pre-evaluate whether a MES function shall be AI supported an intense literature research has been conducted. Academia shows few investigations regarding this field of research. Recent studies have been concerning about possible applications for MES functions in combination with AI. However, there is a lack of research in terms of preevaluating a MES function before embedding the function with AI",,2017,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
5fe27259ec7bd72e920f8e81441955cfe3c4de5e,https://www.semanticscholar.org/paper/5fe27259ec7bd72e920f8e81441955cfe3c4de5e,The role of unmanned aerial vehicles and mmWave in 5G: Recent advances and challenges,"Next‐generation wireless communication networks, in particular, the densified 5G will bring many developments to the existing telecommunications industry. The key benefits will be the higher throughput and very low latency. In this context, the usage of unmanned aerial vehicle (UAV) is becoming a feasible option for deploying 5G services on demand. At the same time, the immense bandwidth potential of mmWave has strengthened its performance in radio communication. In this article, we provide a consolidated synthesis on the role of UAVs and mmWave in 5G, emphasis on recent developments and challenges. The review focuses on UAV relay architectures, identifies the relevant problems and limitations in the deployment of UAVs using mmWave in both access and backhaul links simultaneously. There is a critical analysis of the optimum placement of the UAVs as a relay with a focus on the mmWave band. The distinctive rich characteristics of the mmWave propagation and scattering are presented. We also synthesis mmWave path loss models. Then, the scope of artificial intelligence and machine learning techniques as an efficient solution for combating the dynamic and complex nature of UAV‐based cellular communication networks are discussed. In the end, security and privacy issues in UAV‐based cellular network are spotlighted. It is believed that the literature discussed, and the findings reached in this article are of significant importance to researchers, application engineers and decision‐makers in the designing and deployment of UAV‐supported 5G network.",Trans. Emerg. Telecommun. Technol.,2021,10.1002/ett.4241,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
c5d2a523a209e4cb8b91c732e4019f1d2cfc3ba8,https://www.semanticscholar.org/paper/c5d2a523a209e4cb8b91c732e4019f1d2cfc3ba8,ACM Transactions on Internet of Things,"The Internet of Things (IoT) demands synergy among several research domains and incorporates a broad range of multidisciplinary topics, including low-power wireless networking, embedded systems, data streaming architectures, data analytics and machine learning, cloud and edge computing, service computing and middleware, and security and privacy, as well as social computing. ACM Transactions on Internet of Things (TIOT) publishes novel research contributions and experience reports broadly related to these topics and their interrelations in the context of IoT, with a focus on system designs, end-to-end architectures, and enabling technologies, covering in principle the entire spectrum from hardware devices up to the application layer. Along with this large breadth of scope, another defining element of TIOT is that the results and insights reported in it must be corroborated by a strong experimental component. This is expected to offer evidence of the proposed techniques in realistic scenarios (e.g., based on field deployments or user studies) or public datasets, with the intent to facilitate adoption and exploitation in the real world of the novel ideas published in TIOT. In the same light, experience reports about the use or adaptation of known systems and techniques in real-world applications are equally welcome, as these studies elicit valuable insights for researchers and practitioners alike. This first, inaugural issue bears witness to the aforementioned breadth of topics and emphasis on experimental validation, as it begins with articles proposing novel system-level techniques concerned with wearable computing and light-based positioning, continues with contributions concerned with security at the edge and IoT services in the cloud, and then ends with the definition of ontologies for IoT applications. Many other interesting papers have already been accepted and will appear in the upcoming issues. All of these high-quality contributions have been selected from an outstanding number of submissions from all over the world. We are very excited to see that the research field of IoT is increasingly gaining momentum. In this respect, we are fortunate to have an outstanding editorial board helping us with the process of reviewing and selecting from these many and diverse submissions. The associate editors on the board reflect the scientific mission and values of TIOT and comprise top-notch researchers from academia and industry, with a balanced mix of seniority, gender, and geography. We sincerely thank all of them for accepting to help us in the delicate task of bringing the first issues of TIOT to reality. Indeed, ACM TIOT is the result of the work of many people, some of whom we want to publicly thank in this inaugural editorial. We are very grateful to Steve Welch and the ACM Publications Board for kickstarting the process by contacting us and planting the seed of a new transaction on IoT in our heads. Lothar Thiele and Tarek Abdelzaher drafted the journal proposal alongside us, offering insights that were key in defining the current scope of TIOT; we are honored to have both",ACM Trans. Internet Things,2020,10.1145/3379599,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
8962bf88f3c3e9860421d2e7c982cf28385d4913,https://www.semanticscholar.org/paper/8962bf88f3c3e9860421d2e7c982cf28385d4913,Automatic Check-In Service at Businesses Enabled with Private Mobile Networks,"Private mobile networks such as private LTE/local 5G, which support flexibly configured and empowered innovative technologies that are not feasible in closed public mobile networks recently, have been catching much attention both in academia and in industries. In this paper, we design and implement the automatic check-in service as an example of value-added services of private mobile networks utilizing the flexibility of softwarization. To alleviate the inherent coverage problem of a private mobile network, we integrate our private mobile network with a public LTE by sharing the subscriber database so that a user can use the automatic check-in services deployed in various private mobile networks with only one SIM issued by a public network. We perform field tests in a private mobile network and also a private-public hybrid mobile network and disclose that the users' check-out behavior is predictable through numerical analyses. Based on the finding, we introduce two machine learning-based inference mechanisms that can predict a user's check-out behavior at an inference accuracy of 83% and 93% in a private network and a hybrid one separately. We believe this paper can provide valuable experience for those who are developing their private mobile networks.",GLOBECOM 2020 - 2020 IEEE Global Communications Conference,2020,10.1109/GLOBECOM42002.2020.9348115,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
97b1ea3015d5ccb48885981a31021fb330e58274,https://www.semanticscholar.org/paper/97b1ea3015d5ccb48885981a31021fb330e58274,Top-Down Model Development Using Data Generated from a Complex Numerical Reservoir Simulation with Water Injection,"Top-Down Model Development Using Data Generated from a Complex Numerical Reservoir Simulation with Water Injection Yvon Martinez Numerical simulation and data-driven modeling are two current approaches in engineering reservoir modeling. Numerical reservoir simulation attempts to match past production history by modifying reservoir properties of the model. After multiple computationally intensive trial and error efforts, accurate history matches are identified. These history matches are used by project management for production forecasting purposes. Data-driven reservoir modeling utilizes measured data and is, therefore, free of assumptions that are often included in numerical reservoir simulations. Artificial intelligence and machine learning algorithms are technologies implemented in the development of a data-driven reservoir model with efforts to learn fluid flow through porous media from the datasets provided to the system. Training, calibration, and validation datasets ensure the success during the teaching process. Models, such as oil, gas, and water production, reservoir pressure and water saturation, are trained, calibrated, verified to ensure the success in the teaching process. After appropriate hyperparameter tuning, well-trained models are tested on blind datasets. This leads to the model being deployed on new datasets to again test the model’s performance during forecasting. The accuracy is based upon the model achieving a similar result to what numerical simulation found on the same dataset. The objective of this thesis is to use a 22-year dataset from a reservoir model generated by a numerical simulator that undergoes many complexities to approach the reality that takes place in the industry through the use of operational constraints, workover events requiring shut-in, random bottom hole pressure (“BHP”) trend, etc. Should the Top Down Model (“TDM”) be able to accurately learn the relationships between input attributes and outputs from the data, a similar procedure can then be applied to real field data in the future. The TDM’s capabilities will attempt to be proved when the blind validation’s in-time results match the data from the numerical simulation reservoir model. This will be done by excluding the data from the training, calibration, and validation datasets used to create the TDM. This thesis’s results should demonstrate that datadriven modeling is capable of history matching and forecasting the complexities of a reservoir.",,2020,10.33915/etd.7571,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
2ff58257d18a2cfc43417df163c3222e0e2943f2,https://www.semanticscholar.org/paper/2ff58257d18a2cfc43417df163c3222e0e2943f2,IMS2020 Microwave Week Workshops,": This workshop showcases intelligent mixed-signal, RF/mm-wave, and microwave photonics systems that exploit machine learning and AI techniques in three focused application areas: advanced wireless communication, sensing, and computation. With an emphasis on wireless communication, the workshop explores machine-learning and AI techniques for RF signal conditioning, dynamic wireless-spectrum collaboration, wireless PA linearization, and massive MIMO– mm-wave phased-array beamforming. With a focus on sensing and imaging applications, the workshop presents machine-learning–based radar signal-processing techniques for autonomous navigation and their implementation in integrated frequency-modulated continuous-wave radar systems. Regarding computation, the workshop combines mixed-signal, RF/mm-wave, and microwave-photonics circuit techniques to accelerate energy-efficient, multidimensional signal processing for machine-learning and AI algorithms. In addition, the workshop discusses several applications of photonic deep-learning hardware accelerators in wireless communication, such as RF fingerprinting. Abstract: Advances in mm-wave CMOS technology have resulted in fully integrated mm-wave radar sensors that offer a cost-effective and robust solution to automotive safety, provide accurate industrial sensing, and enable gesture recognition. This workshop features technical experts from academia and industry who present the state of the art in mm-wave CMOS technology, such as all-digital architectures, higher carrier frequencies, advanced signal processing, and machine learning. These technologies promise to improve the achievable accuracy and push performance levels further. The speakers will also share their view of the Abstract: Coherent detection, where the optical carrier’s phase information provides higher signal-to-noise ratios, has ever-increasing momentum. Today, coherent communication dominates long-haul networks operating with data rates beyond 400 Gb/s per wavelength. Thanks to advancements in digital signal processing that leverage ultralow-power implementations in deep submicron technologies (for instance, 7 nm), the cost and power of coherent transponders are becoming competitive for short-reach networks (inter- and intra-data centers). Reducing the cost and enhancing the overall performance of such networks is only achievable through highly integrated solutions that encompass complex digital signal-processing algorithms, state-of-the-art transimpedance amplifiers and modulator drivers, and integrated silicon photonics. Codesign and co-optimization become the key factors in further scaling the power and performance of coherent transponders. They will be the focus of this workshop and addressed by several speakers from different backgrounds. Abstract: Quantum computing has spurred intense research to develop electronics to control quantum devices operating at cryogenic temperatures. Several applications beyond quantum computing require cryogenic electronics to be compatible with very low ambient temperatures or outperform their room-temperature counterparts. This workshop presents an overview of cryogenic electronics, from applications to device operation, focusing on ICs. Applications that must operate at cryogenic temperatures, such as quantum computing (first talk) and particle physics (second talk), will be presented to highlight requirements, current limitations, and future perspectives. The operation of silicon-germanium (SiGe) (third talk) and CMOS (fourth talk) at cryogenic temperatures will be discussed, and four examples of ICs that employ SiGe, bulk CMOS, and fully depleted silicon-on-insulator (SOI) CMOS and target low-noise Abstract: Low-noise amplifiers (LNAs), PAs, switches, and phase shifters can be integrated into one silicon RF front-end (RFFE) IC for mm-wave 5G, and multichannel integration is likely; however, the cost, robustness, and manufacturability advantages of the all-silicon RFFE IC approach are not clear compared to hybrid III-V/ silicon alternatives. mm-Wave 5G broadband PA power efficiency is considerably lower than for 4G equipment. Gallium-nitride (GaN)/ gallium-arsenide (GaAs) III-V–based PAs have a higher output power than silicon-based PAs and good efficiency, but the cost of hybrid-integration approaches rapidly increases with complexity, as will be covered in this workshop. mm-Wave PA linearity versus power-added efficiency at power backoff is always a design tradeoff, and novel RF linearization techniques are required to improve 5G mm-wave PAs. All-silicon solutions with superstrates for antennas will be investigated, and we will discuss the PA antenna and package codesign for 5G MIMO PAs. Design: Basics Abstract: This workshop by phased-array experts in academia and industry provides an in-depth learning experience and walks attendees through different aspects of mm-wave phased-array transceiver design. It covers the following topics: 1) silicon-based mm-wave phased-array basics; 2) phase- and gain-control circuits; 3) package, antenna, and module codesign and calibration; 4) phased-array measurements on-chip and over the air; 5) phased-array applications in commercial and defense systems; and 6) current 5G New Radio phased-array systems and limitations and an outlook Abstract: 5G communication in sub-6-GHz frequencies offers enhanced data rates, capacity, and flexibility but faces challenges, such as energy efficiency, linearity, integration, and scalability. To increase battery life, optimizing PA efficiency is of the utmost importance. This workshop investigates digitally intensive transmit architectures and predistortion techniques that enhance the efficiency of the transmitters and PAs used in these next-generation wireless systems. Experts from industry and academia will share their latest research on linearization techniques to build highly efficient linear PAs in various technologies, employing topologies such as Doherty, out-phasing, and polar. Circuit topologies and digital signal-processing algorithms for the predistortion of these PAs will also be covered. Abstract: 5G will be key to meeting an order-of-magnitude increase in the data-traffic demand on mobile networks. 5G massive MIMO technology will deliver high data rates to many users, helping to increase capacity. It will support real-time multimedia services and reduce energy consumption by targeting signals to individual users through digital beamforming. Element-level digital beamforming that supports emerging multibeam communications and directional sensing at the mm-wave frequency range will expand the use of mm-wave phased arrays and make them broadly applicable across U.S. Department of Defense systems. This workshop presents state-of-the-art radio circuits and systems exploiting MIMO and digital beamforming at sub-6-GHz and mm-wave bands for civilian 5G New Radio and defense applications. Abstract: Indoor positioning and localization will be critical to the next-generation Internet of Things (IoT). The technology obtains the location of a device or user in an indoor environment, which is a key function enabling various IoT applications, such as smart buildings, distance-bounded security, smart industrial applications, and so forth. In this workshop, several popular smartphone-based wireless technologies for locating people and objects will be discussed. Bluetooth Low Energy (BLE), ultrawideband (UWB), and WiFi are three popular standards-compliant localization approaches. BLE is the most widely adopted smartphone-based wireless protocol, so BLE-based localization has an advantage in densely deployed infrastructure. UWB is an emerging wireless-localization technology that is used in advanced smartphones (for example, the iPhone 11). The new UWB protocol, IEEE 802.15.4z, can provide centimeter-level accuracy thanks to its wide spectrum. Finally, WiFi, as a wireless technology deployed in most buildings, will play an important role in accurate positioning with the upcoming IEEE 802.11az protocol. Abstract: This vertically oriented workshop provides technical know-how, from the satellite to the device, by bringing together commercial and defense leaders in space hardware. It reviews satellite orbits and the demands on antenna systems and also provides a detailed overview of CubeSats and the drive for small-form-factor, high-reliability electronics, followed by a comprehensive examination of the market and challenges for satellite-communication terminals. The workshop covers RFICs for space in CMOS and III-V technology, including a special discussion of advanced, very-low-power CMOS for deep-space sensors. A technical review of radiation types and their effects on CMOS and the techniques to successfully design in space using a radiation-hard library or next-generation radiation-hard process on advanced bulk CMOS is offered. This is a great place for new and experienced engineers to learn about the adventure of space. Abstract: PAs do not fulfill all the requirements of linearity, energy efficiency, and bandwidth necessary for New Radio and mm-wave 5G operation and future communications, particularly in user equipment. New techniques are required to design ultrahigh-linearity PAs and yield improved linearization, efficiency enhancements, and bandwidth-extension methods to dramatically improve performance. All aspects of New Radio and mm-wave PA design become more challenging when equipment is placed in arrays with non-negligible element-to-element coupling. This workshop explores PA designs in the mm-wave spectrum as well as linearization techniques (digital predistortion, outphasing, envelope tracking, and so forth) and efficiency enhancements (load and supply modulation and so on) in user equipment and base stations. Abstract: In emerging 5G cellular communication and other mm-wave systems, the generation, distribution, and synchronization of local-oscillator (LO) signals remain a challenge. This ",IEEE Microwave Magazine,2020,10.1109/mmm.2020.2971397,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
03bfd278a4d314f07eec6949581fbcb4466d8e41,https://www.semanticscholar.org/paper/03bfd278a4d314f07eec6949581fbcb4466d8e41,Smart Concurrent Learning Scheme for 5G Network: QoS-Aware Radio Resource Allocation,"The continuous performance race led wireless industry to a ubiquitous adoption of heterogeneous architecture with small cells. Although, the extreme densification offers the largest increase in the network capacity, it also challenges more valuable system metrics like quality of service (QoS) for users with various traffic types. While previously used to boost capacity in the cellular system, radio resource management schemes now need to be refocused to address the requirements of the next generation services. In this paper, we propose a novel power profile construction framework designed specifically for scenarios with multiple traffic types and a smart way to adopt it to a distributed learning algorithm. The main goal is to provide each cell with the ability to make its decision autonomously while taking into account the QoS metrics of the surrounding cells. We illustrate the application of this distributed learning strategy for the subband allocation and propose several mechanisms to improve the convergence speed in the absence of communication. To address the arising convergence challenge we propose to additionally enhance the proposed algorithm with a smart model fitting stage. Taking advantage of this ideas, we were able to properly utilize flexibility and meet the strict requirements of machine learning algorithm for QoS scenarios. The performances of the proposed method are evaluated in the case of Long Term Evolution (LTE-A) setup and compared to a number of traditional resource allocation schemes. System level simulations show that it achieves a considerable improvement in system performance for heterogeneous deployment, without compromising the quality of service of the overall system.",2017 IVth International Conference on Engineering and Telecommunication (EnT),2017,10.1109/ICENT.2017.28,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
ec093100a59018a99e22bab4bed66e81759ca129,https://www.semanticscholar.org/paper/ec093100a59018a99e22bab4bed66e81759ca129,Real-World Conversational AI for Hotel Bookings,"In this paper, we present a real-world conversational AI system to search for and book hotels through text messaging. Our architecture consists of a frame-based dialogue management system, which calls machine learning models for intent classification, named entity recognition, and information retrieval subtasks. Our chatbot has been deployed on a commercial scale, handling tens of thousands of hotel searches every day. We describe the various opportunities and challenges of developing a chatbot in the travel industry.",2019 Second International Conference on Artificial Intelligence for Industries (AI4I),2019,10.1109/AI4I46381.2019.00022,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
357ea471a6702a8f338581392b949cbca21f3e22,https://www.semanticscholar.org/paper/357ea471a6702a8f338581392b949cbca21f3e22,Autopilot of Cement Plants for Reduction of Fuel Consumption and Emissions,"The cement manufacturing industry is an essential component of the global economy and infrastructure. However, cement plants inevitably produce hazardous air pollutants, including greenhouse gases, and heavy metal emissions as byproducts of the process. Byproducts from cement manufacturing alone accounts for approximately 5% of global carbon dioxide (CO2) emissions1. We have developed ”Autopilot” a machine learning based Software as a Service (SaaS) to learn manufacturing process dynamics and optimize the operation of cement plants in order to reduce the overall fuel consumption and emissions of cement production. Autopilot is able to increase the ratio of alternative fuels (including biowaste and tires) to Petroleum coke, while optimizing operation of pyro, the core process of cement production that includes the preheater, kiln and cooler. Emissions of gases such as NOx and SOx, and heavy metals such as mercury and lead which are generated through burning petroleum coke can be reduced through the use of Autopilot. Our system has been proven to work in real world deployments and an analysis of cement plant performance with Autopilot enabled shows energy consumption savings and a decrease of up to 28,000 metric tons of CO2 produced per year.",,2019,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
9087ea168f70b124eb988cddb2e9bec306d348c8,https://www.semanticscholar.org/paper/9087ea168f70b124eb988cddb2e9bec306d348c8,Impactos da indústria 4.0 na construção civil brasileira,"4.0 technology has now become the main focus of discussions around the world, looking for alternatives to improve productive performance and reduce costs using cutting-edge technology such as the internet of things, cloud computing, BIM ( Building Information Modeling), Machine learning and Big Date. In this context, the main objective of this article is initially to show what is industry 4.0 and the revolutions that preceded it, how it works in the Brazilian construction sector, the main advantages and disadvantages of using new technology in construction sites, such as will affect the worker and what are the challenges for deployment in Brazil. Given this, the article is characterized as an applied research, considered a basic research, and its approach is qualitative and quantitative, with the exploratory and descriptive objective. Thus, an online questionnaire was formulated for professionals, so the target audience was around 30 professionals, but only 18 respondents completed the submission of information, totaling the participation of 60% of professionals in the area, in order to To gather their opinion about the use of industry 4.0 in Brazilian civil construction, it is observed that most professionals work in the academic and business sectors. The survey results were compared with the current situation of the construction industry.",Brazilian Journal of Development,2019,10.34117/bjdv5n10-210,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
9190a60e8258caf1a449cbe5fe6c485ab8ab2f20,https://www.semanticscholar.org/paper/9190a60e8258caf1a449cbe5fe6c485ab8ab2f20,Anode Quality Monitoring Using Advanced Data Analytics,,Light Metals 2019,2019,10.1007/978-3-030-05864-7_152,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
bdd0608e39b7c1a30f39735dd0ed6d21dbc9ec02,https://www.semanticscholar.org/paper/bdd0608e39b7c1a30f39735dd0ed6d21dbc9ec02,Practical Experience Report: Engineering Safe Deep Neural Networks for Automated Driving Systems,,SAFECOMP,2019,10.1007/978-3-030-26601-1_16,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
cc07b98c2e475ad75d01ae8bc1b5cc56c99e79d7,https://www.semanticscholar.org/paper/cc07b98c2e475ad75d01ae8bc1b5cc56c99e79d7,An Edge Computing Visual System for Vegetable Categorization,"In self-service supermarket and retail industry, efforts to reduce customer wait time using automatic grocery item identification are challenged by low recognition accuracy, long response time and substantial requirement for equipment. In this paper, we propose a novel edge computing system named EdgeVegfru for vegetable and fruit image classification. While existing work on Vegfru dataset shows excellent performance, few of them have been deployed in real-world applications. We adopt an edge computing paradigm, design, implement and evaluate the whole system on the Android devices. The proposed deep learning model and quantization algorithm reduce the model size and inference time significantly. Our system has shown out-standing accuracy within limited time and computation resources, compared with other machine learning methods(such as Support Vector Machine(SVM), Random Forest(RF)), thus providing the potential path for automatic recognition and pricing in self-service retail stores.",2019 18th IEEE International Conference On Machine Learning And Applications (ICMLA),2019,10.1109/ICMLA.2019.00115,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
8eaa1e618c7a96640a97e81bdce9068451cef6cc,https://www.semanticscholar.org/paper/8eaa1e618c7a96640a97e81bdce9068451cef6cc,Concept for a Technical Infrastructure for Management of Predictive Models in Industrial Applications,,EUROCAST,2019,10.1007/978-3-030-45093-9_32,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
63b607cb3d2932ab576432e62467b24e4f6f8b34,https://www.semanticscholar.org/paper/63b607cb3d2932ab576432e62467b24e4f6f8b34,IoT/CPS Ecosystem for Efficient Electricity Consumption : Invited Paper,"Modern society relies on smart systems like internet of things (IoT) and cyber physical systems (CPS) to monitor and control physical processes. The widespread deployment of IoT and CPSs result in fast growth of sensor data as physical processes are constantly monitored by billions of IP-enabled sensors (44 zettabytes by 2020). Hence, fog nodes are deployed to make network edge rich in computing resources to enable real-time data analytics using artificial intelligence/machine learning (AI/ML) for Big data generated from IoT and CPSs. This paper proposes IoT/CPS ecosystem for smart grid (SG) utilizing industry 4.0 concept to manage and control the loads using an intelligent predictive controller based on artificial neural network (ANN). The ANN is trained to predict the loads in certain districts based on previous smart meter readings installed at consumers and substations. This is a novel approach which integrates IoT/CPSs ecosystem into electric power system to deliver energy to consumers with high efficiency, reduce the cost, optimize the energy consumption, improve the reliability and enable real-time monitoring of power consumption.",2019 Tenth International Green and Sustainable Computing Conference (IGSC),2019,10.1109/IGSC48788.2019.8957164,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
e11be2776b35f84acc9eec7ed590b39fac71038f,https://www.semanticscholar.org/paper/e11be2776b35f84acc9eec7ed590b39fac71038f,An Open 5G NFV Platform for Smart City Applications Using Network Softwarization,"Advanced wireless communication network testbeds are now widely being deployed around European and cross-continental. This represents an interesting opportunity for vertical industry and academia to perform experimentation and validation before a real deployment. In this paper, we present 5GinFIRE as a suitably flexible platform towards open 5G (Network Function Virtualization (NFV) ecosystem and playground. On top of this platform, we designed and deployed a smart city safety system as a vertical use case, exploring 5G capabilities through a combination of NFV and machine learning to provide end-to-end communication and low latency smart city service. This safety system helps detecting criminals along the city and sending a notification to the security center. A Virtual Network Function (VNF) has been developed to enable video transcoding, face detection and recognition at the cloud or the edge of the network. The validation of the overall system is performed through the deployment of the use case indoor (Smart Internet Lab) and outdoor (Millennium Square Bristol). We show the VNF specification and present a quantitative analysis in terms of bandwidth, response time, processing time and transmission speed in terms of Quality of Experience (QoE).",2019 IEEE Wireless Communications and Networking Conference Workshop (WCNCW),2019,10.1109/WCNCW.2019.8902853,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
3ae0caecdf5ac979bcc344240c3cd03c7c2fd8e0,https://www.semanticscholar.org/paper/3ae0caecdf5ac979bcc344240c3cd03c7c2fd8e0,A Tour of TensorFlow,"Deep learning is a branch of artificial intelligence employing deep neural network architectures that has significantly advanced the state-of-the-art in computer vision, speech recognition, natural language processing and other domains. In November 2015, Google released $\textit{TensorFlow}$, an open source deep learning software library for defining, training and deploying machine learning models. In this paper, we review TensorFlow and put it in context of modern deep learning concepts and software. We discuss its basic computational paradigms and distributed execution model, its programming interface as well as accompanying visualization toolkits. We then compare TensorFlow to alternative libraries such as Theano, Torch or Caffe on a qualitative as well as quantitative basis and finally comment on observed use-cases of TensorFlow in academia and industry.",ArXiv,2016,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
6d40f385d0e92e0db1241b63f1787eb0f5745e3c,https://www.semanticscholar.org/paper/6d40f385d0e92e0db1241b63f1787eb0f5745e3c,Bodhisattva - Rapid Deployment of AI on Containers,"Cloud-based machine learning is becoming increasingly important in all verticals of the industry as all organizations want to leverage ML and AI to solve real-world problems of emerging markets. But, incorporating these services into business solutions is a goliath task, mainly due to the sheer effort necessary to go from development to deployment. We present a novel idea that enables users to easily specify, create, train and rapidly deploy machine learning models through a scalable Machine-Learning-as-a-Service (MLaaS) offering. The MLaaS is provided as an end-to-end microservice suite in a container-based PaaS environment for web applications on the cloud. Our implementation provides an intuitive web-based GUI for tenants to consume these services in a few quick steps. The utility of our service is demonstrated by training ML models for various use cases and comparing them on factors like time-to-deploy, resource usage and training metrics.",2018 IEEE International Conference on Cloud Computing in Emerging Markets (CCEM),2018,10.1109/CCEM.2018.00025,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
44935233e13bff2fca0e85c076dd4ea24986d045,https://www.semanticscholar.org/paper/44935233e13bff2fca0e85c076dd4ea24986d045,Ensemble Based Real-Time Adaptive Classification System for Intelligent Sensing Machine Diagnostics,"The deployment of a sensor node to manage a group of sensors and collate their readings for system health monitoring is gaining popularity within the manufacturing industry. Such a sensor node is able to perform real-time configurations of the individual sensors that are attached to it. Sensors are capable of acquiring data at different sampling frequencies based on the sensing requirements. The different sampling rates affect power consumption, sensor lifespan, and the resultant network bandwidth usage due to the data transfer incurred. These settings also have an immediate impact on the accuracy of the diagnostics and prognostics models that are employed for system health monitoring. In this paper, we propose a novel adaptive classification system architecture for system health monitoring that is well suited to accommodate and take advantage of the variable sampling rate of sensors. As such, our proposed system is able to yield a more effective health monitoring system by reducing the power consumption of the sensors, extending the sensors' lifespan, as well as reducing the resultant network traffic and data logging requirements. We also propose an ensemble based learning method to integrate multiple existing classifiers with different feature representations, which can achieve significantly better, stable results compared with the individual state-of-the-art techniques, especially in the scenario when we have very limited training data. This result is extremely important in many real-world applications because it is often impractical, if not impossible, to hand-label large amounts of training data.",IEEE Transactions on Reliability,2012,10.1109/TR.2012.2194352,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
bab465b4c9748d5d3835effeb839b868b8cfbfb3,https://www.semanticscholar.org/paper/bab465b4c9748d5d3835effeb839b868b8cfbfb3,Distributed Osmotic Computing Approach to Implementation of Explainable Predictive Deep Learning at Industrial IoT Network Edges with Real-Time Adaptive Wavelet Graphs,"Challenges associated with developing analytics solutions at the edge of large scale Industrial Internet of Things (IIoT) networks close to where data is being generated in most cases involves developing analytics solutions from ground up. However, this approach increases IoT development costs and system complexities, delay time to market, and ultimately lowers competitive advantages associated with delivering next-generation IoT designs. To overcome these challenges, existing, widely available, hardware can be utilized to successfully participate in distributed edge computing for IIoT systems. In this paper, an osmotic computing approach is used to illustrate how distributed osmotic computing and existing low-cost hardware may be utilized to solve complex, compute-intensive Explainable Artificial Intelligence (XAI) deep learning problem from the edge, through the fog, to the network cloud layer of IIoT systems. At the edge layer, the C28x digital signal processor (DSP), an existing low-cost, embedded, real-time DSP that has very wide deployment and integration in several IoT industries is used as a case study for constructing real-time graph-based Coiflet wavelets that could be used for several analytic applications including deep learning pre-processing applications at the edge and fog layers of IIoT networks. Our implementation is the first known application of the fixed-point C28x DSP to construct Coiflet wavelets. Coiflet Wavelets are constructed in the form of an osmotic microservice, using embedded low-level machine language to program the C28x at the network edge. With the graph-based approach, it is shown that an entire Coiflet wavelet distribution could be generated from only one wavelet stored in the C28x based edge device, and this could lead to significant savings in memory at the edge of IoT networks. Pearson correlation coefficient is used to select an edge generated Coiflet wavelet and the selected wavelet is used at the fog layer for pre-processing and denoising IIoT data to improve data quality for fog layer based deep learning application. Parameters for implementing deep learning at the fog layer using LSTM networks have been determined in the cloud. For XAI, communication network noise is shown to have significant impact on results of predictive deep learning at IIoT network fog layer.",2018 IEEE First International Conference on Artificial Intelligence and Knowledge Engineering (AIKE),2018,10.1109/AIKE.2018.00042,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
e229a1f2d7edb2957c4cc1d0fed9179570c17791,https://www.semanticscholar.org/paper/e229a1f2d7edb2957c4cc1d0fed9179570c17791,5G Enabled Technologies for Smart Education,"5G technology use cases depicts the prospects of 5G network model to revolutionize Industry and Education is not an exception. The 5G model in general is made up of three main blocks: Enhanced Mobile Broadband, Massive Machine Type Communication and Ultra Reliable and Low Latency Communication. Within these blocks are the services 5G offers to users. In this paper, we focus on Educational users as beneficiaries of 5G technologies. The modern day Educational Institutions can benefit from the deployment of 5G-enabled services adapted to this sector. We proposed frameworks relating 5G and its disruptive technologies in advancing tools that will propel the idea of a Smart Educational System. This paper hence provides a comprehensive discussion on 5G technologies that will facilitate new teaching and learning trends in Educational environment.",International Journal of Advanced Computer Science and Applications,2019,10.14569/ijacsa.2019.0101228,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
3e646ac7ba2a3fc1027362331f93f66bd57a926d,https://www.semanticscholar.org/paper/3e646ac7ba2a3fc1027362331f93f66bd57a926d,Security and Privacy in the Internet of Things: Technical and Economic Perspectives,"For the last twenty years, the Internet extends from digital spheres into the physical world through applications such as smart homes, smart cities, and Industry 4.0. Although this technological revolution of the Internet of Things (IoT) brings many benefits to its users, such as increased energy efficiency, optimized and automated processes, and enhanced comfort, it also introduces new security and privacy concerns. In the first part of this thesis, we examine three novel IoT security and privacy threats from a technical perspective. As first threat, we investigate privacy risks arising from the collection of room climate measurements in smart heating applications. We assume that an attacker has access to temperature and relative humidity data, and trains machine learning classifiers to predict the presence of occupants as well as to discriminate between different types of activities. The results show the leakage of room climate data has serious privacy implications. As second threat, we examine how the expansion of wide-area IoT infrastructure facilitates new attack vectors in hardware security. In particular, we explore to which extent malicious product modifications in the supply chain allow attackers to take control over these devices after deployment. To this end, we design and build a malicious IoT implant that is inserted in arbitrary electronic products. In the evaluation, we leverage these implants for hardware-level attacks on safetyand security-critical products. As third threat, we analyze the security of ZigBee, a popular network standard for smart homes. We present novel attacks that make direct use of the standard’s features, showing that one of its commissioning procedures is insecure by design. In the evaluation of these vulnerabilities, we reveal that attackers are able to eavesdrop key material as well as take-over ZigBee products and networks from a distance of more than 100 meters. In the second part of this thesis, we investigate how IoT security can be improved. Based on an analysis of the root causes of ZigBee’s security vulnerabilities, we learn that economic considerations influenced the security design of this IoT technology. Consumers are currently not able to reward IoT security measures as an asymmetric information barrier prevents them from assessing the level of security that is provided by IoT products. As a result, manufacturers are not willing to invest into comprehensive security designs as consumers cannot distinguish them from insufficient security measures. To tackle the asymmetric information barrier, we propose so-called security update labels. Focusing on the delivering of security updates as an important aspect of enforcing IoT security, these labels transform the asymmetric information about the manufacturers’ willingness to provide future security updates into an attribute that can be considered during buying decisions. To assess the influence of security update labels on the consumers’ choice, we conducted a user study with more than 1,400 participants. The results reveal that the proposed labels are intuitively understood by consumers, considerably influence their buying decisions, and therefore have the potential to establish incentives for manufacturers to provide sustainable security support.",,2019,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
a3b740be485b9ad2296dbdc09c874c036e5619d4,https://www.semanticscholar.org/paper/a3b740be485b9ad2296dbdc09c874c036e5619d4,"Development, applications and benefits of the network digital twin","The network digital twin is the must-have technology of any advanced DSO. One of the main challenges for DSOs is deploying the most efficient and effective solutions for operation and maintenance of electricity network and infrastructures. New technologies that are flourishing in the market show high potential for the electricity industry, too. Examples are cloud and edge-computing, artificial intelligence (AI) with machine learning, IoT sensors, wearables, augmented and virtual reality, 3D modelling, LIDAR, drones and robotics, advanced simulation and modelling software. Their adoption by DSOs and integration with existing systems (such as GIS AMI, ADMS) offer new innovative approaches to distribution systems operation, epitomized by the network digital twin. This paper describes how existing and new technologies combine to realize the network digital twin, its relevant applications and benefits. Several practical examples and lessons learned from proof of concepts, pilots and innovation projects carried out with start-ups, technology providers and data scientists are illustrated. KEYWORKDS: Network Digital Twin, artificial intelligence, 3D-modelling, IoT sensors, augmented reality.",,2019,10.34890/974,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
c2f5c2cbebe52155b66cf280fb83866f51bc3a8e,https://www.semanticscholar.org/paper/c2f5c2cbebe52155b66cf280fb83866f51bc3a8e,Development of Ergonomic User Interfaces for the Human Integration in Cyber-Physical Systems,"Digitalization is transforming the worldwide economy. In such digital transformation the human integration in CPS assumes a crucial importance, complemented with some key enabling technologies like IoT, cloud computing, big data and machine learning. In this context, user interfaces are used to collect data and to monitor and control the systems, being required to use design and ergonomics guidelines, as well as user expirience, for their deployment in industrial enviroments. This paper describes the use of friendly and ergonomic user interfaces to support the human integration at operational and strategic level, aligned with the Industry 4.0 context, for an industrial metal stamping unit.",2019 IEEE 28th International Symposium on Industrial Electronics (ISIE),2019,10.1109/ISIE.2019.8781101,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
3fbd4d8a977f905a7a4bec77ff180d9926f6c817,https://www.semanticscholar.org/paper/3fbd4d8a977f905a7a4bec77ff180d9926f6c817,Analytics Solution Helps Identify Rod-Pump Failure at the Wellhead,"This article, written by JPT Technology Editor Chris Carpenter, contains highlights of paper SPE 192513, “Industrial Internet of Things Edge Analytics: Deploying Machine Learning at the Wellhead To Identify Rod-Pump Failure,” by Bartosz Boguslawski, Matthieu Boujonnier, Loryne Bissuel-Beauvais, Fahd Saghir, SPE, and Rajesh D. Sharma, SPE, Schneider Electric, prepared for the 2018 SPE Middle East Artificial Lift Conference and Exhibition, Manama, Bahrain, 28–29 November. The paper has not been peer reviewed.
 Industrial-Internet-of-things (IIOT) architecture provides an opportunity to improve asset uptime and maintenance of assets, reduce safety risks, and optimize operational costs. However, to turn data into meaningful insights, the industry must make full benefit of machine-learning (ML) models. This paper presents an analytics solution for identifying rod-pump failure capable of automated dynacard recognition at the wellhead that uses an ensemble of ML models. The proposed solution does not require Internet connectivity to generate alarms and meets confidentiality requirements.
 Rod-Pump-Control (RPC) Architecture
 Thanks to recent progress in microelectronics, the embedding of ML models in remote places with scarce connectivity, known as edge computing, is possible. Thanks to the insights generated at the oil field, onsite maintenance teams can apply immediate corrective responses, working efficiently and safely.
 RPC architecture enables automated control of sucker-rod pumps. A variable-speed drive (VSD) controls the pump by adjusting the speed of the motor to downhole conditions. The VSD is controlled by a remote terminal unit (RTU) that provides speed reference for improved rod-pump control. The RTU also collects sensor measurements. In the work covered in the paper, two sensors, namely the proximity sensor and the load cell, are of particular interest. The proximity sensor is mounted near the crank arm, while the load cell is a transducer mounted between the polished rod clamp and the bridle. The RTU sends the measurements to a touchscreen human/machine interface for a local user. It also communicates the measurements to a supervisory control and data-acquisition (SCADA) system. The communication between the RTU and the SCADA host is established through external wireless communication devices such as radios or cellular modems. Furthermore, in the ex-ample discussed in the paper, a local edge computing gateway is added to the architecture. Edge gateways have significant computing power and can run ML-based applications. The gateway retrieves the data from the RTU and performs onsite analytics, generates alarms, and communicates with the cloud sporadically.
 Using the measurements from the two sensors, the RPC produces a surface card (or dynacard), from which downhole pump conditions may be deduced, and consequently calculates a downhole card, from which downhole pump conditions may be inferred. The downhole card is, in essence, a translated surface card with oscillatory harmonics removed. An experienced operator is able to infer from the downhole card whether a pump is operating normally or has failed in some way.",Journal of Petroleum Technology,2019,10.2118/0519-0063-JPT,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
12d310588dc22bc5cea64b20fc95df64994cdc86,https://www.semanticscholar.org/paper/12d310588dc22bc5cea64b20fc95df64994cdc86,Smart IoT Monitoring System for Agriculture with Predictive Analysis,"The Internet of Things (IoT) technology has the means to shape the future of many industries. Data is the language of the communication between different nodes through the network; the networks are the communication channel. The cloud is the home and destination of the data which adds intelligence through data analytics software, Precision agriculture uses the IoT features to help in managing crops production, by optimizing the quality of the crops through applying required nutrients and reduce the harmful impacts on the environment due to the application of excess pesticides. In this paper, we deployed a sensing network to gather the field data of some crops (Potatoes, Tomatoes, etc.), then fed these data to a machine learning algorithm to get a warning message finally displaying both the data and the warning message through a Graphical User Interface (GUI).",2019 8th International Conference on Modern Circuits and Systems Technologies (MOCAST),2019,10.1109/MOCAST.2019.8741794,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
02a0356138f9929964343933b4bfa84dc5d55070,https://www.semanticscholar.org/paper/02a0356138f9929964343933b4bfa84dc5d55070,A View from Industry: Securing IoT with Azure Sphere,"Every year, 9 billion new devices powered by single-chip computers-MCUs-are deployed; all with little to no cybersecurity and most with no network connectivity. These devices are in your home, in your office, and in every industrial or commercial setting on the planet. Azure Sphere offers to improve MCU computing by bringing cloud connectivity, intelligence, and high security to these devices. The Azure Sphere solution consists of three components: a new class of cross-over MCUs incorporating Microsoft silicon security technology, a new OS built around a custom Linux kernel, and a cloud-based security service that guards every Azure Sphere-based device on the planet for its deployed lifetime. In this talk, I will explain the market scenarios Azure Sphere addresses, dig into the silicon and software architecture that compose the Azure Sphere solution, review some of the project's history, and demo the Azure Sphere experience. Finally, I will look towards the future and discuss research problems out on the horizon. Specifically, I'll explore some scenarios around the convergence mobility, IoT and facial recognition via machine learning where strong security and privacy guarantees are especially important.",HotMobile,2019,10.1145/3301293.3302378,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
63750e108273992df88c35ad476960c19b27a014,https://www.semanticscholar.org/paper/63750e108273992df88c35ad476960c19b27a014,Wide and Deep Learning for Recommendation System,"Low Birth Weight is the major problem for the new born. Low birth weight is a term used to describe babies who are born weighing less than 5 pounds, 8 ounces (2,500 grams). Low-birth weight babies are more likely than babies with normal weight to have health problems as a newborn. Almost 40 percent of the new born suffer from underweight. Predicting birth weight before the birth of the baby is the best way to help the baby get special care as early as possible. It helps us to arrange for doctors and special facilities before the baby is born. There are several factors that affect the birth weight. Through past studies, it has been observed that the factors which affect the child birth range from biological characteristics like the baby's sex, race, age of mother and father, weight gained by the mother during pregnancy to behavioral characteristics like smoking and drinking habits of the mother, the education and living conditions of the parents. This project focuses on developing a web application that predicts baby weight taking baby’s gender, plurality, gestation weeks and mothers age as inputs. Machine learning is one of the domains that plays important role in medical industry. Many machine learning models have been developed to predict diseases at the early stage. In this project wide and deep neural network model is developed using TensorFlow library in Google cloud environment. Wide and Deep Neural Network combines wide linear model and deep neural network. It provides both memorization and generalization. Pre-processing and training is done in the distributed environment using cloud Dataflow and Cloud ML Engine. The model is then deployed as REST API.A web application is developed to invoke the API with the user inputs and show the predicted baby weight to the users. It is scalable and provides high performance.",,2019,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
cc6a1d1934d0f972256066d163c87befa26ba6f2,https://www.semanticscholar.org/paper/cc6a1d1934d0f972256066d163c87befa26ba6f2,Integrated Cyber Physical Simulation Modelling Environment for Manufacturing 4.0,"This paper studied the recent development and applications of cyber physical system into manufacturing industry. It is observed in literature some models of system design and system development for Industry 4.0 but few were found specifically for manufacturing systems modelling and simulation. This paper proposed a novel system framework of Integrated Cyber Physical Simulation Modelling Environment for Manufacturing 4.0, which incorporated an architecture integrating Aggregate Cyber Space Controller (ACSC) with Physical Space Distributed Controller (PSDC). The concepts of digital twin, distributed artificial intelligence, machine learning, and distributed autonomous control are deployed in the framework with the intention to explore the future potential applications of systems modelling and simulation for manufacturing Industry 4.0 systems. The framework proposed is an extension of Cyber Physical Production System (CPPS) and provides a scenario of hybrid cyber space simulation and physical space discrete controller for manufacturing 4.0.",2018 IEEE International Conference on Industrial Engineering and Engineering Management (IEEM),2018.0,10.1109/IEEM.2018.8607696,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
4b11f3cb6d5f56771146d54022d28fc4711f0f49,https://www.semanticscholar.org/paper/4b11f3cb6d5f56771146d54022d28fc4711f0f49,Deep learning: from speech recognition to language and multimodal processing,"While artificial neural networks have been in existence for over half a century, it was not until year 2010 that they had made a significant impact on speech recognition with a deep form of such networks. This invited paper, based on my keynote talk given at Interspeech conference in Singapore in September 2014, will first reflect on the historical path to this transformative success, after providing brief reviews of earlier studies on (shallow) neural networks and on (deep) generative models relevant to the introduction of deep neural networks (DNN) to speech recognition several years ago. The role of well-timed academic-industrial collaboration is highlighted, so are the advances of big data, big compute, and the seamless integration between the application-domain knowledge of speech and general principles of deep learning. Then, an overview is given on sweeping achievements of deep learning in speech recognition since its initial success. Such achievements, summarized into six major areas in this article, have resulted in across-the-board, industry-wide deployment of deep learning in speech recognition systems. Next, more challenging applications of deep learning, natural language and multimodal processing, are selectively reviewed and analyzed. Examples include machine translation, knowledgebase completion, information retrieval, and automatic image captioning, where fresh ideas from deep learning, continuous-space embedding in particular, are shown to be revolutionizing these application areas albeit with less rapid pace than for speech and image recognition. Finally, a number of key issues in deep learning are discussed, and future directions are analyzed for perceptual tasks such as speech, image, and video, as well as for cognitive tasks involving natural language.",APSIPA Transactions on Signal and Information Processing,2016.0,10.1017/ATSIP.2015.22,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
09d6afe41249aedbb76fbb6ec55bcb8dc9ee4297,https://www.semanticscholar.org/paper/09d6afe41249aedbb76fbb6ec55bcb8dc9ee4297,Look Before You Leap! Designing a Human-Centered AI System for Change Risk Assessment,"Reducing the number of failures in a production system is one of the most challenging problems in technology driven industries, such as, the online retail industry. To address this challenge, change management has emerged as a promising sub-field in operations that manages and reviews the changes to be deployed in production in a systematic manner. However, it is practically impossible to manually review a large number of changes on a daily basis and assess the risk associated with these. This warrants the development of an automated system to assess the risk associated with a large number of changes. There are a few commercial solutions available to address this problem but those solutions lack the ability to incorporate domain knowledge and continuous feedback from domain experts into the risk assessment process. As part of this work, we aim to bridge the gap between model-driven risk assessment of change requests and the assessment of domain experts by building a continuous feedback loop into the risk assessment process. Here we present our work to build an end-to-end machine learning system along with the discussion of some of the practical challenges we faced related to extreme skewness in class distribution, concept drift, estimation of the uncertainty associated with the model’s prediction and the overall scalability of the system.",ICAART,2021.0,10.5220/0010877500003116,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
2762e0dc0e3f54eb28634803d7ea4d3de7c7421f,https://www.semanticscholar.org/paper/2762e0dc0e3f54eb28634803d7ea4d3de7c7421f,Computational decision support system in healthcare: a review and analysis,"A decision support system (DSS) may help to synchronize comprehensive computational aspects of computational problems like knowledge discovery, processing, and pattern visualization. It provides an effective channel for decision making by churning huge datasets. Primarily, DSS has been deployed in domains like business process management, health informatics, and even for managing smart devices. Numerous algorithms have been proposed to augment the efficacy of DSS and are undergoing refinement. Most recently, DSS has been trialed in the healthcare industry because there is a need for an intelligent decision support system. It may be helpful in the process of automation along with the solution generation and evaluation. In this paper, broad analysis and discussion on the applicability and suitability of the methods related to DSS and algorithms have been discussed along with the role of information and communications technology (ICT). It also includes problem-based discussion and suggested solutions along with different cases in the healthcare system. It covers the data analysis with the domain intelligence impact along with the information processing for knowledge extraction and discovery. It also covers some of the clinical decision aspects for understanding the impact of other correlated medical resource systems. The methodological and computational analysis includes data preprocessing, knowledge extraction, interpretation, decision-making model, and the influencing factors in the performance analysis. The main data mining methods considered here for the DSS system in case of healthcare informatics discussion were association rule mining, clustering, classification and optimization algorithms. The machine learning aspects covered here in three ways supervised, semi-supervised, and unsupervised for the decision analysis based on the healthcare system. Finally, based on the methodological and computational applicability different decision-making scenarios have been discussed and analyzed for the analysis of the combination and nature of applicability. Our study and analysis provide an analytical and computational perspective in terms of the health care system, influencing parameters, their applicability, a methodological perspective, decision-making process, traditional methods, and the challenges along with the suggested measures for the future. It's also helpful in the process of maintaining the internal and external aspects which is more reliable in performance aspects of DSS.",,2021.0,10.19101/IJATEE.2020.762142,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
3867a702aadb2497c83c4ef530ecc67b65ef9481,https://www.semanticscholar.org/paper/3867a702aadb2497c83c4ef530ecc67b65ef9481,The Vulnerabilities in Smart Contracts: A Survey,,Advances in Artificial Intelligence and Security,2021.0,10.1007/978-3-030-78621-2_14,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
a872ca8a5f87fd2a105cee47bba80366ef1fc2ab,https://www.semanticscholar.org/paper/a872ca8a5f87fd2a105cee47bba80366ef1fc2ab,EdgeEye: An Edge Service Framework for Real-time Intelligent Video Analytics,"Deep learning with Deep Neural Networks (DNNs) can achieve much higher accuracy on many computer vision tasks than classic machine learning algorithms. Because of the high demand for both computation and storage resources, DNNs are often deployed in the cloud. Unfortunately, executing deep learning inference in the cloud, especially for real-time video analysis, often incurs high bandwidth consumption, high latency, reliability issues, and privacy concerns. Moving the DNNs close to the data source with an edge computing paradigm is a good approach to address those problems. The lack of an open source framework with a high-level API also complicates the deployment of deep learning-enabled service at the Internet edge. This paper presents EdgeEye, an edge-computing framework for real-time intelligent video analytics applications. EdgeEye provides a high-level, task-specific API for developers so that they can focus solely on application logic. EdgeEye does so by enabling developers to transform models trained with popular deep learning frameworks to deployable components with minimal effort. It leverages the optimized inference engines from industry to achieve the optimized inference performance and efficiency.",EdgeSys@MobiSys,2018.0,10.1145/3213344.3213345,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
9ecf4af7a29186d4b09243469ea61c4126921df7,https://www.semanticscholar.org/paper/9ecf4af7a29186d4b09243469ea61c4126921df7,Multi-Source Spatial Entity Extraction and Linkage,"Web data sources contain large amounts of geo-social data, consisting of users, friendship/follower networks, check-ins, reviews, locations, etc., which are of great interest to academia and industry. There are publicly available datasets of samples of this web data, but they are very old (over ten years), not large, and not rich in attributes. Alternatively, one could use the public APIs to access and download web data. Unfortunately, this process is challenging due to the APIs limitations (e.g. the amount of data retrieved in a request, the number of requests performed within a timeframe, etc.). Thus, there is a need for algorithms that, given the limitations, are able to retrieve a good quality dataset from web sources, a need that the current state-of-the-art does not address. This thesis aims to provide algorithms and tools that can produce larger, recent, duplicate-free, and rich-in-attributes spatial entity data. To obtain larger and recent data from web data sources, we propose multi-source seed-driven (MSSD) algorithms that use the public free APIs to extract geosocial data. The MSSD algorithms aim to maximize the amount of data extracted while minimizing the number of requests and respecting each source's API limitations. The rationale behind the seeddriven algorithms is to perform some API requests for having an initial dataset and then use the points of the richest source as seed in the API requests for the rest of the sources. We propose different techniques for choosing the points and the radius of the search. We opt for a multi-source solution given that multiple sources provide independent information and diverse attributes as opposed to using only one source. Moreover, we experimentally demonstrate that using a single source algorithm sometimes converges to a dead end. The MSSD algorithms extract overall 14.3 times more data than the initial querying, and the optimized version MSSD* retrieves 90% of the data with less than 16% of the requests of the non-optimized version. When obtaining multi-source data, the same spatial entity might exist in different sources and sometimes even within the same source. These ""duplicates"" are not easy to detect since they have different attributes, they are expressed in different forms, and they might even contain contradicting attribute values. The problem of finding which pairs of spatial entities refer to a real physical entity is referred to as spatial entity linkage. We address this problem with several algorithms, which all share the same spatial blocking technique, and they use skylines to rank the compared pairs. The spatial blocking technique (QuadFlex) that we propose is a quadtree-inspired algorithm that groups the spatial entities based on the distance between them and the area's density. Moreover, it allows the assignment of spatial entities in more than one child to not miss any relevant comparisons. The spatial entities that fall into the same child are compared pairwise. To decide which pairs belong to the same physical entity, we propose novel skyline-based (SkyEx*) algorithms, which use preference functions to assign skylines to the pairs. The threshold-based SkyEx, SkyEx-F and SkyEx-FES require a threshold that is the number of skylines to separate the positive from the negative class, and they are able to achieve an F-measure of 0.72 on the whole dataset and 0.85 on a manually labeled sample. We introduce a fully unsupervised algorithm, SkyExD, which does not need a threshold and instead sets the cut-off based on the distance of the skylines. We demonstrate experimentally that SkyEx-D can reach a near-optimal F-measure (less than 0.01 loss). Additionally, we offer skyex, an R-package that implements the threshold-based and unsupervised skyline-based algorithms, supports the whole entity linkage pipeline with other stateof-the-art methods for entity blocking and comparisons, and provides a powerful Analysis and Visualization module to aid the explainability of the results. Besides the unsupervised algorithms, we propose a trained skyline-based algorithm, SkyEx-T, which is able to learn the preference function and the cut-off in tiny training sets (0.05%-1% of the dataset) and still achieve machine-learning-level accuracy. Moreover, the SkyEx-T model is fully explainable and readable, in contrast to the commonly-used black-box machine learning techniques. Furthermore, SkyEx-T has no weights nor layered architecture; consequently, it shows high robustness in deployment, while for the machine learning, some re-configuration and re-tuning of parameters might be needed when the new data arrives. Finally, we demonstrate that SkyEx-T cutoff closely approximates the optimal cut-off, even though it was learned on a tiny training set. With our algorithms in the spatial entity linkage, we ensure a duplicate-free dataset and rich-in-attribute spatial entities. Overall, this thesis contributes with effective and efficient algorithms for the initial and fundamental step of every geo-social research study: having recent, good-quality, rich-in-attributes datasets. We propose the MSSD-* algorithms that make the data extraction process more effective (14.3 times more data than the initial querying) while managing the requests carefully. We further improve the quality of the retrieved data by detecting pairs that refer to the same entity with high precision and recall while having an explainable and robust model (SkyEx-* algorithms). In the future, in the context of data extraction, we aim to work on hybrid algorithms that combine location-based with user-based and keyword-based API requests and use supervised techniques to learn the parameters of APIs. In the context of spatial entity linkage, we plan to work on hybrid blocking techniques that combine spatial attributes with textual and semantic ones, multi-class classification for the skyline-based algorithms, and crowdsourcing techniques for improving the labeling of the pairs. Members of the assessment committee are Associate Professor Manfred Jaeger, Aalborg University, Denmark, Associate Professor Maria Luisa Damiani, University of Milano, Italy, and Professor Konstantinos Stefanidis, University of Tampere, Finland. Professor Torben Bach Pedersen and Professor Esteban Zimányi are Suela Isaj’s supervisors. The moderator is Associate Professor Christian Thomsen.",,2021.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
09be29041382883652bb0ff5c34a19716b25a867,https://www.semanticscholar.org/paper/09be29041382883652bb0ff5c34a19716b25a867,Bridging the Gap Between Research and Production with CODE,,PAKDD,2019.0,10.1007/978-3-030-16142-2_22,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
aef2a783f4fb5d1ba876d4c5c9684ebb05e961c6,https://www.semanticscholar.org/paper/aef2a783f4fb5d1ba876d4c5c9684ebb05e961c6,Real-world Conversational AI for Hotel Bookings,"In this paper, we present a real-world conversational AI system to search for and book hotels through text messaging. Our architecture consists of a frame-based dialogue management system, which calls machine learning models for intent classification, named entity recognition, and information retrieval subtasks. Our chatbot has been deployed on a commercial scale, handling tens of thousands of hotel searches every day. We describe the various opportunities and challenges of developing a chatbot in the travel industry.",,2019.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
b1ec0fa804ad9d2b32678ab5286c6f1861cd3e2a,https://www.semanticscholar.org/paper/b1ec0fa804ad9d2b32678ab5286c6f1861cd3e2a,Editors' foreword,"During the last decade we have witnessed a rapid expansion of artificial intelligence (AI) applications and use of machine learning (ML) algorithms in an increasingly broad range of problems in finance. This development is fueled by a unique confluence of factors: an exponentially growing computational capacity that is available for enterprises, and similarly exponential growth in the amount of machine-readable data, along with improvements in the state of the art which allow ML and AI applications that were impractical ten or twenty years ago. There is an ambitious feeling emerging across industry and academia that some cognitive processes can be automated via ML and AI, radically expanding opportunities for automation especially in finance and the services industry. However, finance is very different from other domains, such as medical diagnosis, where ML and AI have been developed and successfully deployed. ML and AI require large and reliable training data sets to make machines ‘learn’ their models. But all financial data are not alike. We can characterize financial information as big data because of its large volume (financial time series data easily scales into petabytes), velocity (much of financial data is high-frequency), and variety (numerical, categorical, text, images, etc.). This data exhibits complex behavior: nonstationarity, nonlinear interactions, heteroscedasticity, and biases. The research goal in this domain is to find in this data relevant patterns that could be used for investment, risk management or trading decisions. Time series analysis and traditional statistics can facilitate the process of understanding, modeling, and forecasting the behavior of financial assets. Present day developments of ML and AI algorithms provide novel approaches and perspectives such as feature selection in high dimensional data that mixes large structured and unstructured datasets, and incorporates a",Quantitative Finance,2019.0,10.1080/14697688.2019.1638160,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
67c37b0e7ee1f23e1d6979cb3eb34d80fdd35fe0,https://www.semanticscholar.org/paper/67c37b0e7ee1f23e1d6979cb3eb34d80fdd35fe0,"Artificial-Intelligence-Based, Automated Decline Curve Analysis for Reservoir Performance Management: A Giant Sandstone Reservoir Case Study","
 Decline curve analysis (DCA) is one of the most widely used forms of data analysis that evaluates well behavior and forecasts future well and field production and reserves. Usually, this practice is done manually, making analysis of assets with a large number of wells cumbersome and time-consuming. Moreover, results are subject to alternate interpretations, mostly as a function of experience and objectives of the evaluator.
 In this work, despite the common practice of the industry, i.e. manual DCA, we developed and deployed cutting-edge technologies that intelligently apply DCA methods to any number of wells in an unbiased, systematic, intelligent, and automated fashion. The tool reads production data, and multidisciplinary well information (e.g., drilling and completion data, geological data, artificial lift information, etc.). Then it performs cluster analysis using unsupervised machine learning and pattern recognition to partition the dataset into internally homogeneous and externally distinct groups. This cluster analysis is later used for type-curve generation for wells with short production history. For wells with long enough history, the tool first detects production events through a fully automated event detection algorithm without any human interference. Since production events are highly correlated with real-time events, it also cross-validates with the operating conditions. Next, the last event is selected, and a decline curve is fitted using advanced nonlinear optimization and minimization algorithms. This leads to a reliable and unbiased prediction. For each cluster, a type curve is computed that truly captures the underlying production behavior of the wells that belong to the same group or cluster, and then is applied to the wells with short production history within that cluster. To capture the probabilistic nature of such analysis and quantify the inherent uncertainty, we extended the method to a probabilistic DCA using quantile regression.
 We successfully deployed this technology/tool to a giant Middle Eastern reservoir, with more than 2,000 wells and 70 years of production. Our predicted aggregated field decline rate is in good agreement with the client's reservoir simulation results run under the ""do-nothing"" scenario. While performing traditional DCA for such a field would require several weeks and significant resources, our automated solution integrates all real-life events/information and provides a comprehensive analysis in field, cluster and well level. In addition, our results are ""unbiased,"" as it is not subject to human errors or evaluator's interpretations.
 Our robust and intelligent DCA allows for exhaustive evaluation of production trends and opportunities in fields across time, production zones, well types, and any combinations of the above. The results demonstrate the effectiveness of the automated DCA to rapidly execute decline curve analysis for a large number of wells. The accuracy is improved significantly through automatic event detection, cross-validation of events, curve fitting optimization, quantile regression, and cluster-based type-curving.","Day 2 Tue, November 12, 2019",2019.0,10.2118/197142-ms,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
fd1b0aeec1c6142c71615e4a166b7db38f866a35,https://www.semanticscholar.org/paper/fd1b0aeec1c6142c71615e4a166b7db38f866a35,Empirical Study of Robust State Estimation for Power Systems,"This paper provides insights into the existence of spurious local optima for the nonlinear `1-norm state estimator in power system applications. The linear least-absolute-value (LAV) estimator has attracted considerable attentions over the past few years, especially in the machine learning community, mainly due to its convexity and robustness with respect to sparse noise. Due to these properties, recent studies have attempted to apply the nonlinear version of the LAV to problems such as topological error detection in power systems. However, there has been no study so far that provides theoretical guarantees for finding the global solution of the nonlinear least-absolute-value (NLAV) estimator. In this study, we analyze the performance of NLAV on different real-world scenarios and compare it with the nonlinear least-squares (NLS), which is the common practice in power industry. In our study, we consider various factors, such as the number of measurements, available prior information, and the presence of bad data, on the performance of these estimators by performing more than 260000 simulations on several IEEE benchmark systems. Additionally, we use a recent result on robust principal component analysis and low-rank matrix completion to justify our observations and provide recommendations for realworld applications of these estimators.",,2019.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
d58c5cfa88ee91a65ebe1d85ecfe80b4aadf23af,https://www.semanticscholar.org/paper/d58c5cfa88ee91a65ebe1d85ecfe80b4aadf23af,Convolutional neural network hyperparameters designed for highest object recognition accuracy in satellite imagery,"Geospatial data follows Moore's law. On the back of satellite hardware improvements and reduced costs of rocket launch there were more nanosatellites deployed to Lower Earth Orbit (LEO) in 2018 than in the previous 10 years combined allowing exponential satellite imagery data growth. Machine Learning tools enable us to process high resolution, multi-spectral satellite imagery data to recognize objects at large scale and generates insights with practical industry applications. It enables us to calculate global oil reserves, track tanker ships, forecast wheat yields or estimate retail revenue based on the car traffic, all exceptionally valuable financial information. In this paper, we research various computer vision techniques to develop and optimal technique for this unique type of the dataset: multi-spectral satellite imagery.",DATA '19,2019.0,10.1145/3368691.3368695,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
801f4d746d234f8f8c156095507fbf6d15d859e3,https://www.semanticscholar.org/paper/801f4d746d234f8f8c156095507fbf6d15d859e3,Optimal U-Net Architecture for Object Recognition Problems in Multispectral Satellite Imagery,"Geospatial data follows Moore's law. On the back of improvements of optical Earth observation satellite hardware [1] (weight, propulsion systems, signal transmission and resolution) as well as reduced costs of rocket launch carrying these satellites, number of nanosatellites deployed to Lower Earth Orbit (LEO) in 2018 was larger than in the previous 10 years combined [2]. This allowed an exponential growth in satellite imagery data production that is available for military and commercial use. Machine learning tools enable us to process high resolution, multi-spectral satellite imagery data to recognize objects at scale [3] and generate insights with practical industry applications. It allowa us to calculate global oil reserves, track tanker ships or estimate retail revenue based on the car count, all exceptionally valuable financial information. In this paper, we investigate various computer vision techniques to develop an optimal machine learning technique for object recognition problems at this unique type of the dataset: multi-spectral satellite imagery.",2019 IEEE/ACS 16th International Conference on Computer Systems and Applications (AICCSA),2019.0,10.1109/AICCSA47632.2019.9035305,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
bf663e046913d6e49ab3993db430537606dbe8d0,https://www.semanticscholar.org/paper/bf663e046913d6e49ab3993db430537606dbe8d0,MITIGATING SOFT FAILURES USING NETWORK ANALYTICS AND SDN TO SUPPORT DISTRIBUTED BANDWIDTH-INTENSIVE SCIENTIFIC INSTRUMENTS OVER INTERNATIONAL NETWORKS,"With the consolidation of high-speed networks and worldwide scientific deployments, new experiments are being conducted remotely. The control and data gathering of these bandwidth-intensive mission-critical instruments require a reliable network infrastructure capable of reacting in real-time to soft failures, such as packet loss. To address the mission-critical realtime instruments’ Service Level Agreement (SLA), streaming telemetry and data-driven analytics are required. In recent years, the industry has created many open consortiums and specifications, such as OpenConfig and Inband Network Telemetry (INT). As a result, we have new levels of interconnections, interoperation, and disaggregation allowing Software-Defined Networking (SDN) applications to use protocol agnostic, common APIs, Artificial Intelligence and Machine Learning to create reliable and adaptive networks. This paper aims to present the ongoing effort to create an adaptive network infrastructure capable of identifying and isolating soft failures in an automated approach to optimize bandwidth-intensive data transfers. Our approach leverages the most recent solutions offered by the optical and packet layers using SDN and network analytics.",,2019.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
090ee982beb2e5910fcbedbc9e1ecb02be76999b,https://www.semanticscholar.org/paper/090ee982beb2e5910fcbedbc9e1ecb02be76999b,Explainable Text Classification in Legal Document Review A Case Study of Explainable Predictive Coding,"In today’s legal environment, lawsuits and regulatory investigations require companies to embark upon increasingly intensive data-focused engagements to identify, collect and analyze large quantities of data. When documents are staged for review – where they are typically assessed for relevancy or privilege – the process can require companies to dedicate an extraordinary level of resources, both with respect to human resources, but also with respect to the use of technology-based techniques to intelligently sift through data. Companies regularly spend millions of dollars producing ‘responsive’ electronically-stored documents for these types of matters. For several years, attorneys have been using a variety of tools to conduct this exercise, and most recently, they are accepting the use of machine learning techniques like text classification (referred to as predictive coding in the legal industry) to efficiently cull massive volumes of data to identify responsive documents for use in these matters. In recent years, a group of AI and Machine Learning researchers have been actively researching Explainable AI. In an explainable AI system, actions or decisions are human understandable. In typical legal ‘document review’ scenarios, a document can be identified as responsive, as long as one or more of the text snippets (small passages of text) in a document are deemed responsive. In these scenarios, if predictive coding can be used to locate these responsive snippets, then attorneys could easily evaluate the model’s document classification decision. When deployed with defined and explainable results, predictive coding can drastically enhance the overall quality and speed of the document review process by reducing the time it takes to review documents. Moreover, explainable predictive coding provides lawyers with greater confidence in the results of that supervised learning task. The authors of this paper propose the concept of explainable predictive coding and simple explainable predictive coding methods to locate responsive snippets within responsive documents. We also report our preliminary experimental results using the data from an actual legal matter that entailed this type of document review. The purpose of this paper is to demonstrate the feasibility of explainable predictive coding in the context of professional services in the legal space.",2018 IEEE International Conference on Big Data (Big Data),2018.0,10.1109/BigData.2018.8622073,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
39556ff6eab180825269815a52c478b14cc9e69a,https://www.semanticscholar.org/paper/39556ff6eab180825269815a52c478b14cc9e69a,SparkBench: a comprehensive benchmarking suite for in memory data analytic platform Spark,"Spark has been increasingly adopted by industries in recent years for big data analysis by providing a fault tolerant, scalable and easy-to-use in memory abstraction. Moreover, the community has been actively developing a rich ecosystem around Spark, making it even more attractive. However, there is not yet a Spark specify benchmark existing in the literature to guide the development and cluster deployment of Spark to better fit resource demands of user applications. In this paper, we present SparkBench, a Spark specific benchmarking suite, which includes a comprehensive set of applications. SparkBench covers four main categories of applications, including machine learning, graph computation, SQL query and streaming applications. We also characterize the resource consumption, data flow and timing information of each application and evaluate the performance impact of a key configuration parameter to guide the design and optimization of Spark data analytic platform.",Conf. Computing Frontiers,2015.0,10.1145/2742854.2747283,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
a511a986af07804261d3b35badf764c622e3f502,https://www.semanticscholar.org/paper/a511a986af07804261d3b35badf764c622e3f502,Artificial Intelligence: A European Perspective,"We are only at the beginning of a rapid period of transformation of our economy and society due to the convergence of many digital technologies. Artificial Intelligence (AI) is central to this change and offers major opportunities to improve our lives. The recent developments in AI are the result of increased processing power, improvements in algorithms and the exponential growth in the volume and variety of digital data. Many applications of AI have started entering into our every-day lives, from machine translations, to image recognition, and music generation, and are increasingly deployed in industry, government, and commerce. Connected and autonomous vehicles, and AI-supported medical diagnostics are areas of application that will soon be commonplace. There is strong global competition on AI among the US, China, and Europe. The US leads for now but China is catching up fast and aims to lead by 2030. For the EU, it is not so much a question of winning or losing a race but of finding the way of embracing the opportunities offered by AI in a way that is human-centred, ethical, secure, and true to our core values. The EU Member States and the European Commission are developing coordinated national and European strategies, recognising that only together we can succeed. We can build on our areas of strength including excellent research, leadership in some industrial sectors like automotive and robotics, a solid legal and regulatory framework, and very rich cultural diversity also at regional and sub-regional levels. It is generally recognised that AI can flourish only if supported by a robust computing infrastructure and good quality data: â€¢ With respect to computing, we identified a window of opportunity for Europe to invest in the emerging new paradigm of computing distributed towards the edges of the network, in addition to centralised facilities. This will support also the future deployment of 5G and the Internet of Things. â€¢ With respect to data, we argue in favour of learning from successful Internet companies, opening access to data and developing interactivity with the users rather than just broadcasting data. In this way, we can develop ecosystems of public administrations, firms, and civil society enriching the data to make it fit for AI applications responding to European needs. We should embrace the opportunities afforded by AI but not uncritically. The black box characteristics of most leading AI techniques make them opaque even to specialists. AI systems are currently limited to narrow and well-defined tasks, and their technologies inherit imperfections from their human creators, such as the well-recognised bias effect present in data. We should challenge the shortcomings of AI and work towards strong evaluation strategies, transparent and reliable systems, and good human-AI interactions. Ethical and secure-by-design algorithms are crucial to build trust in this disruptive technology, but we also need a broader engagement of civil society on the values to be embedded in AI and the directions for future development. This social engagement should be part of the effort to strengthen our resilience at all levels from local, to national and European, across institutions, industry and civil society. Developing local ecosystems of skills, computing, data, and applications can foster the engagement of local communities, respond to their needs, harness local creativity and knowledge, and build a human-centred, diverse, and socially driven AI. We still know very little about how AI will impact the way we think, make decisions, relate to each other, and how it will affect our jobs. This uncertainty can be a source of concern but is also a sign of opportunity. The future is not yet written. We can shape it based on our collective vision of what future we would like to have. But we need to act together and act fast.",,2018.0,10.2760/11251,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
a381a0510d79fa3d869a398d29ead197c842efd0,https://www.semanticscholar.org/paper/a381a0510d79fa3d869a398d29ead197c842efd0,Towards emergent microservices for client-tailored design,"Contemporary systems are increasingly complex, with both large codebases and constantly changing environments which make them challenging to develop, deploy and manage. We consider two recent efforts to tackle this complexity: microservices and emergent software. Microservices have gained recent popularity in industry, in which monoliths of software are broken down into compositions of single-objective, end-to-end services running on HTTP which can be scaled out on cloud hosting systems. From the research community, the emergent systems concept demonstrates promise in using real-time learning to autonomously compose and optimise software systems from small building blocks, rapidly finding the best behavioural composition to match the current deployment conditions. We argue that emergent software and microservice architectures have strong potential for synergy in complex systems, offering mutually compatible lessons in dealing with complexity via scale-out design and real-time client-tailored behaviour. We explore self-designing microservices, built with emergent software, to demonstrate the complementary boundaries of both concepts - and how future intersections may offer novel architectures that lie at a compelling point between human- and machine-designed systems. We present the conceptual synergy and demonstrate a specific microservice architecture for a smart city example where scoped microservices are continually self-composed according to the demands of the applications and operating environment. For the purpose of reproducibility of the study, we make available all the code used in the evaluation of the proposed approach.",ARM@Middleware,2018.0,10.1145/3289175.3289177,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
e8cb5bd0dfa9bf5efa7a97af310575f251b7b46e,https://www.semanticscholar.org/paper/e8cb5bd0dfa9bf5efa7a97af310575f251b7b46e,Deep Learning-based New Methods of Images Processing for Criminal Investigation,"Image processing has been widely used in criminal investigation. However, the robustness and real-time is contradictory for image processing of criminal investigation. To address the issue, we propose a new idea that images of criminal investigation are proceed by using deep learning networks. On the one hand, it is necessary to extract complex image features from the original image; on the other hand, deep learning is able to efficiently improve recognition ratio of objects. Based on these advantages, we propose the applications of deep learning in face recognition and pedestrian detection for criminal investigation. The simulation experiments show that the proposed method are able to quickly recognize face, and detect multi-pedestrian under complex environment, which is the basis of improving clear-up rate. Introduction Currently, public safety has become a key area for construction in all countries. However, monitoring and photographing can only obtain massive amounts data of images and video, but it lack of effective and intelligent algorithms to analysis so much data. How to effectively use the massive amount of data which obtained from different camera and photo tools every day is a major challenge for public safety [1] .At present ,The main issues of the image processing of criminal investigation can be divided into the following categories: image blurring problems, which are difficult to effectively recover blurred images due to the rapid movement of the target (such as a vehicle) and the inevitable wind .in addition, for the images got in smoggy weather, how to make the images clearly is a problem. For fast-distant shooting of face and license plates, how to quickly and accurately identify is a problem. For multiple pedestrian targets in a complex environment, the problem that how to correctly detect and identify also needs to be resolved. There are a large number of image and video processing algorithms applied in industry and life, but it is difficult to get great results in criminal investigation image applications. Because the environment has a large impact for that. for example, Although the current face recognition algorithm can achieve a recognition rate of 99.9% in an ordinary indoor, the face recognition of criminal investigation is less than 50%. Deep Learning Deep Learning [2-3] originates from the research of artificial neural networks and is a new field in machine learning research. Data is abstracted at a high level by using complex structures or the multiple processing layers of multiple nonlinear transformations. It forms a more abstract high-level to represent attribute or feature by combining low-level features to discover the data’s representation in distributed feature. It is a method of Characteristic learning based on data. There are so many cities in China have deployed a large number of cameras and capture probes. There are so much data obtained by the Police system every day. How to effectively use these data and extract useful information from them to improve the efficiency of detection is a problem. With the rapid development of deep learning, solving this problem becomes possible. Because deep learning relies on massive data of sample, and the Police system exactly meets this condition, so the use of deep learning to improve the image processing quality and effectively identify the target is a",DEStech Transactions on Computer Science and Engineering,2018.0,10.12783/dtcse/iece2018/26640,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
e6cec9b4b33e1cee607b5954848a160de05f5e17,https://www.semanticscholar.org/paper/e6cec9b4b33e1cee607b5954848a160de05f5e17,AUTOMATIC RECOGNITION OF VIRTUAL REALITY SICKNESS BASED ON PHYSIOLOGICAL SIGNALS,"Virtual Reality (VR) sickness seems one of the main limitations to the large-scale adoption of VR technologies. This disturbance seems to induce physiological changes that affect the sympathetic and parasympathetic activities of the users. Thereby, it seems relevant to measure users’ physiological data in order to prevent and reduce VR sickness. This paper presents the results of an initial real-life experiment of VR sickness detection based on physiological data. The electrodermal, cardiac and subjective data of 27 participants was recorded during VR sessions. Machine Learning algorithms were trained and the best model (Gradient Boosting) explained 48% of the VR sickness variance. These results demonstrate the opportunity to develop an automatic and continuous tool to detect the appearance of VR sickness based on physiological signals. This tool will prove very valuable to the VR industry. INTRODUCTION Virtual Reality (VR) appears as a major technological breakthrough and a main business opportunity for the entertainment industry. The VR market is expected to expand exponentially with worldwide revenues for the AR/VR growing to more than $162 billion in 2020 [1]. However, one main limitation to its large-scale adoption is VR sickness especially because of health, ethical, legal and acceptability aspects. VR sickness is a common problem that could affect up to 60% of adult users [2]. The necessity to better detect and prevent the appearance of VR sickness is at the origin of this research cooperation between b<>com and Editorial user research lab, Ubisoft Paris aiming at the development of an automatic VR sickness detection tool. VIRTUAL REALITY SICKNESS: CAUSES AND SYMPTOMS Previous research proposed several hypotheses to explain VR Sickness. The most common theory is the sensory conflict theory [3]. According to this theory, VR sickness is the result of conflicts and inconsistency between the different sensorial information sent to the brain when the user evolves in the virtual world. In VR, the most common conflict is a discrepancy between the motion information coming from the vestibular system and the motion information coming from the visual system. Indeed, this information comes from two separate systems: the vestibular system located in the inner ear and the visual system [4]. This discrepancy will be detected by the brain and will induce, in sensitive participants, the symptoms of VR sickness. VR sickness is a complex phenomenon and while motion cues play a primary role, multiple factors are known to contribute to the appearance of the sickness. Three main categories of factors can be identified: 1) The factors related to the characteristics of the stimuli: the spatial frequency [5], the reactivity of the system [6], the wideness of the Field Of View (FOV) [7], etc. 2) The factors related to the predispositions of the users: gender [8], age [9], the predisposition to suffer from migraine attack [10], etc. 3) The users’ past experiences [11], according to the sensory conflict theory. In fact VR sickness appears only when a present set of vestibular and visual information is not congruent with what is expected from previous experiences [12]. The complex nature of VR sickness is not limited to its different causes, but it is reflected in its symptomatology as well. While the most evident and detrimental symptom of VR sickness is the nausea, the complete symptomatology of VR sickness includes other elements like general discomfort, headache, disorientation and eye strain. The intensity as well as the duration of the symptoms are quite variable. They depend on the characteristic of the stimulus and the user predisposition to VR sickness. In the majority of cases, the symptoms disappear some minutes after the end of the stimulation. Nevertheless, there are documented cases when the symptoms were still present 6 hours after the VR experience [13]. THE ASSESSMENT OF VIRTUAL REALITY SICKNESS The traditional way to evaluate VR sickness is based on subjective questionnaires. Various questionnaires have been previously developed to assess a sickness level. The most popular is probably the “Simulator Sickness Questionnaire” (SSQ) proposed by Kennedy in 1993 [14]. It is constituted of 16 questions to evaluate 3 categories of symptoms: nausea related symptoms, oculomotor symptoms and disorientation symptoms. While still widely used for the simplicity of its deployment and analysis, the SSQ has several limitations. Indeed, this measure is global, gathered a posteriori, intrusive and punctual. These limitations make the SSQ (and any other questionnaire) inadequate to be integrated in an automatic tool to evaluate VR sickness. Another method was proposed by Keshavarz and collaborators [15] which consists in requesting the participants to verbally give their evaluation of sickness level on a scale ranging from 0 to 20, with zero representing no discomfort and twenty representing barely supportable sickness. This approach seems to be a very good alternative to the SSQ since very strong correlation (r = .79) between verbal rating and SSQ scores were reported in this study. Nevertheless, this method is cognitively disruptive for the user experience in a context of VR-video games. To be effective and valuable for the VR industry, the assessment method must ideally be continuous, automatic and real time. Considering these constraints, physiological data related to the perceived VR sickness was used in this paper. The physiological measures of VR sickness The physiological response associated to VR sickness is due to the connections between the vestibular and the autonomic nervous system. The conflicting inputs from visual, vestibular and somatosensory afferents induce a vestibular autonomic response. This response involves both the sympathetic and the parasympathetic systems and affects for instance the heart rate variability and the skin conductance [16]. Various researches support the hypothesis of a measurable VR sickness by physiological response. For instance, Gavgani and collaborators [17] underline a correlation between phasic skin conductance activity and the reported nausea ratings. While Dennison [18] found relations between subscale scores of the SSQ and bradygastric power, breathing rate, pulse amplitude and blinking. Moreover, Ohyama and collaborators [19] investigated the cardiac responses associated to VR sickness. But, cardiac responses didn’t statically correlate with the user subjective evaluation. These findings suggest the existence of a relation between the physiological responses and VR sickness. Nevertheless, such relation seems too complex to be exploited using classical analyses. To handle such complex data, Machine Learning approach has spread in recent years to go beyond the limits of classical statistical approaches. Nam and collaborators [20] use such method to detect nausea in VR using various bio-signals (EEG, ECG, PPG, SCL, EOG, SKT). Results seem promising as they are able to partially detect nausea in real time [20]. Automatic recognition of VR sickness: the Machine Learning approach To automatically detect VR sickness using physiological data, the use of Machine Learning algorithms seem relevant. In this approach, physiological and declared VR sickness are mapped using Machine Learning algorithms. In others words, the training of Machine Learning algorithms, using the supervised techniques [21], aims to infer the function between the input data (i.e., physiological data) and the output data (i.e., subjective labels). Indeed, Machine Learning algorithms have the ability to learn without being explicitly programmed [22]. After training, the models should be able to automatically recognize in real time, for any new user, the VR sickness related to the physiological data without requesting a subjective response. Physiological signals and subjective VR sickness level were collected to investigate the possibility of detecting VR sickness automatically. A large and relevant labeled dataset (i.e., physiological data labeled with subjective evaluation of VR sickness) to train models is fundamental to obtain an efficient system of VR sickness detection. So, Editorial user research lab, Ubisoft Paris and b<>com decided to cooperate with the intent to collect and analyze consistent amounts of physiological data of test users viewing VR content. Contrary to previous studies [19], [20] the aim of the current paper is to develop and evaluate a new solution more adapted to the industrial context than previous approaches based on invasive and/or expensive sensors (e.g., EEG or eye tracking). METHOD 27 participants (22 men and 5 women average age: 25.93 years; standard deviation: 4.39) were recruited by Ubisoft. Participants were requested to test three VR game prototypes during 30 minutes. The prototypes were interactive games currently under development and they implied a relevant amount of camera’s movements. The first game was a space simulation using a third-person point of view. The second was an arcade car racing game (in first-person and third-person point of view) and the third was a space firstperson shooter. To increase the probability of VR sickness we didn’t implement in these prototypes any of the typical countermeasures (i.e. Reduction of the peripheral vision) applied in the gaming industry to prevent VR sickness. The participants were informed that they were free to stop the experiment at any time. In order to induce various levels of VR sickness, the game’s levels tested by the participants were designed to increase progressively the VR sickness induction. VR content was presented using the “Oculus Rift” ® or “HTC Vive” 2 ® devices. Measures Two types of measures were collected: the physiological data and subjective data of the users’ sickness. The physiological data was collected using the “Shimmer GSR+”®. This sensor measures the electrodermal activity (EDA) and the Blood Pulse Volume (B",,2018.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
bd723aa176753340ca9975f4d256d7b44983890c,https://www.semanticscholar.org/paper/bd723aa176753340ca9975f4d256d7b44983890c,Academic-Industrial Perspective on the Development and Deployment of a Moderation System for a Newspaper Website,"This paper describes an approach and our experiences from the development, deployment and usability testing of a Natural Language Processing (NLP) and Information Retrieval system that supports the moderation of user comments on a large newspaper website. We highlight some of the differences between industry-oriented and academic research settings and their influence on the decisions made in the data collection and annotation processes, selection of document representation and machine learning methods. We report on classification results, where the problems to solve and the data to work with come from a commercial enterprise. In this context typical for NLP research, we discuss relevant industrial aspects. We believe that the challenges faced as well as the solutions proposed for addressing them can provide insights to others working in a similar setting. Data and experiment code related to this paper are available for download at https://ofai.github.io/million-post-corpus.",LREC,2018.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
68b728fb56797b32b74ffcd6e2bec9a97f1d63a5,https://www.semanticscholar.org/paper/68b728fb56797b32b74ffcd6e2bec9a97f1d63a5,A Framework for Auditing Multilevel Models using Explainability Methods,": Multilevel models (MLMs) are increasingly deployed in industry across different functions. Applications usually result in binary classification within groups or hierarchies based on a set of input features. For transparent and ethical applications of such models, sound audit frameworks need to be developed. In this paper, an audit framework for technical assessment of regression MLMs is proposed. The focus is on three aspects: model, discrimination, and transparency & explainability. These aspects are subsequently divided into sub-aspects. Contributors, such as inter MLM-group fairness, feature contribution order, and aggregated feature contribution, are identified for each of these sub-aspects. To measure the performance of the contributors, the framework proposes a shortlist of KPIs, among others, intergroup individual fairness ( Diff Ind_MLM ) across MLM-groups, probability unexplained (PUX) and percentage of incorrect feature signs (POIFS) . A traffic light risk assessment method is furthermore coupled to these KPIs. For assessing transparency & explainability, different explainability methods (SHAP and LIME) are used, which are compared with a model intrinsic method using quantitative methods and machine learning modelling. Using an open-source dataset, a model is trained and tested and the KPIs are computed. It is demonstrated that popular explainability methods, such as SHAP and LIME, underperform in accuracy when interpreting these models. They fail to predict the order of feature importance, the magnitudes, and occasionally even the nature of the feature contribution (negative versus positive contribution on the outcome). For other contributors, such as group fairness and their associated KPIs, similar analysis and calculations have been performed with the aim of adding profundity to the proposed audit framework. The framework is expected to assist regulatory bodies in performing conformity assessments of AI systems using multilevel binomial classification models at businesses. It will also benefit providers, users, and assessment bodies, as defined in the European Commission’s proposed Regulation on Artificial Intelligence, when deploying AI-systems such as MLMs, to be future-proof and aligned with the regulation.",ArXiv,2022.0,10.48550/arXiv.2207.01611,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
4a1b1e54455ce6339f1ad5cf8b0ea2abed919cc2,https://www.semanticscholar.org/paper/4a1b1e54455ce6339f1ad5cf8b0ea2abed919cc2,Rotten-Fruit-Sorting Robotic Arm: (Design of Low Complexity CNN for Embedded System),": Industrial Automation has revolutionized the processing industry due to its high accuracy, the time it saves, and its ability to work without tiring. Being the most fundamental part of automation machines, robotic arms are being used as a fundamental component in many types of domestic as well as commercial automation units. In this paper, we proposed a low-complexity convolutional neural network (CNN) model and successfully deployed it on a locally generated robotic arm with the help of a Raspberry Pi 4 module. The designed robotic arm can detect, locate, and classify (based on fresh or rotten) between three species of Mangos (Ataulfo, Alphonso, and Keitt), on a conveyor belt. We generated a dataset of about 6000 images and trained a three-convolutional-layer-based CNN. Training and testing of the network were carried out with MatLab, and the weighted network was deployed to an embedded environment (Raspberry Pi 4 module) for real-time classiﬁcation. We reported a classiﬁcation accuracy of 98.08% in the detection of fresh mangos and 95.75% in the detection of rotten mangos. For the designed robotic art, the achieved angle accuracy was 93.94% with a minor error of only 2 ◦ . The proposed model can be deployed in many food- or object-sorting industries as an edge computing application of deep learning.","The 1st International Conference on Energy, Power and Environment",2022.0,10.3390/engproc2021012109,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
d37e9fab5a3e07af7f4a5d596d6e25b366cce464,https://www.semanticscholar.org/paper/d37e9fab5a3e07af7f4a5d596d6e25b366cce464,Introduction to the ACSAC’20 Special Issue,"The Annual Computer Security Applications Conference (ACSAC) brings together cutting-edge researchers, with a broad cross-section of security professionals drawn from academia, industry, and government, gathered to present and discuss the latest security results and topics. ACSAC’s core mission is to investigate practical solutions for computer and network security technology. The 36th Annual Computer Security Applications Conference was held virtually on December 7-11, 2020. As in the previ-ous year, ACSAC especially encouraged contributions in the area of Deployable and Impactful Security . Deployable and impactful security solutions aim to address key real-world challenges, which may include accuracy, run-time overhead, ground-truth labeling, human aspects, usability, and energy consumption. Having the deployability and impactfulness goals motivates one to focus on solving the most critical real-world challenges, which may otherwise be ignored by the fast-moving research community. In addition, ACSAC encourages authors of accepted papers to submit software and data artifacts and make them publicly available to the entire community. Releasing software and data artifacts represents an important step towards fa-cilitating the reproducibility of research results, and ultimately contributes to the real-world deployment of novel security solutions.Forthis special issue we invited authors of papers that appeared at ACSAC 2020 and that successfully passed an evaluation of their software and/or data artifacts to submit an extended version of their papers. This selection criteria ensured that the research has a high potential for being deployed in real-world environments and to be used to implement practical defense systems.Thisvolume contains three manuscripts on topics from three different areas: attacks on trusted execution environments, operations security, and usable security and privacy. In “Faulty Point Unit: ABI Poisoning Attacks on Trusted Execution Environments,” Alder et al. show that current Trusted Execution Environment (TEE) implementations do not properly sanitize Floating Point Unit (FPU) registers on enclave entry. This leads to potential security issues. For instance, the authors show that it is possible to change the prediction results of a machine learning classifier running in an enclave. The extended version of the ACSAC 2020",Digital Threats: Research and Practice,2022.0,10.1145/3534708,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
5e318590185f409494710510da425748eb6a3a51,https://www.semanticscholar.org/paper/5e318590185f409494710510da425748eb6a3a51,"AI Case Studies: Potential for Human Health, Space Exploration and Colonisation and a Proposed Superimposition of the Kubler-Ross Change Curve on the Hype Cycle","Abstract The development and deployment of artificial intelligence (AI) is and will profoundly reshape human society, the culture and the composition of civilisations which make up human kind. All technological triggers tend to drive a hype curve which over time is realised by an output which is often unexpected, taking both pessimistic and optimistic perspectives and actions of drivers, contributors and enablers on a journey where the ultimate destination may be unclear. In this paper we hypothesise that this journey is not dissimilar to the personal journey described by the Kubler-Ross change curve and illustrate this by commentary on the potential of AI for drug discovery, development and healthcare and as an enabler for deep space exploration and colonisation. Recent advances in the call for regulation to ensure development of safety measures associated with machine-based learning are presented which, together with regulation of the rapidly emerging digital after-life industry, should provide a platform for realising the full potential benefit of AI for the human species.",Studia Humana,2019.0,10.2478/sh-2019-0001,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
406289d08cbefcc4a10d2e0630b5e18778aec20f,https://www.semanticscholar.org/paper/406289d08cbefcc4a10d2e0630b5e18778aec20f,An SDN/NFV based Intelligent Fog Architecture for DDoS Defense in Cyber Physical Systems,"The Internet of Things (IoT) has garnered considerable interest in recent years as its technical capabilities have facilitated the creation of a diverse variety of commercial revenue models and services (e.g., medical cyber-physical systems, smart agriculture, Industry 4.0, smart cities, etc.). However, the security gaps in intelligent systems continue to be a significant source of concern. Malevolent attackers employ attacks like distributed denial of service (DDoS) to weaken system availability. To mitigate attack traffic, adopting a modern Software-defined Networked backbone and Network Function Virtualization (NFV) paradigm has proven to be a potential approach. This paper offers a comprehensive examination of the cutting-edge defense solutions driven by contemporary SDN/NFV technologies, highlighting the key limitations and future scope. The paper also presents the multi-dimensional view of the defense architecture by examining the Internet of Things (IoT) scenarios, defense strategies, the role of Fog to overcome research gaps, and leveraging SDN/NFV architecture for an intelligent defense solution. Furthermore, the paper examines the profound network backbone and offers the intelligent Machine and Deep learning-based distributed DDoS defense solution deployed in the Fog layer. The article concludes with an evaluation of open issues and research prospects for vulnerability apprehension in Smart systems. Keywords—SDN, Software Defined Network, NFV, Network Functions Virtualization, Cloud, IoT, Fog Computing, Distributed Denial of Service, DDoS",2021 10th International Conference on System Modeling & Advancement in Research Trends (SMART),2021.0,10.1109/SMART52563.2021.9676241,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
2697815fc45ca4875ace520c67d21df30992013e,https://www.semanticscholar.org/paper/2697815fc45ca4875ace520c67d21df30992013e,Novel Intrusion Detection System for 5G1,"The telecommunication industry is majorly transforming towards 5G networks. It needs to satisfy the needs of the new and existing users. The users and the customers need much better quality of the corresponding services and they need the corresponding security in order to secure the transmitting data and other services. Therefore, the mobile leading networks must provide much better quality of an experience and security, and they must improve the performance for the services they provide. Novel services envisioned by 5G, new networking, service deployment, the new processing technologies and the storage is required. The mentioned technologies will involve the new security problems for the 5G systems. The world scientists are seriously working on analyzing the 5G security. The researchers have identified the existing problems of 5G systems. Our analysis illustrates the basics reasons of security problems in 5G. The researchers have found the vulnerabilities in 5G, which give the attackers opportunity to integrate the malicious code and to run it. MiTM, MNmap and Battery drain attacks can be successfully implemented on 5G. Our paper analyzes an existing security problems of 5G. As the result, we offer the new Intrusion Detection System using machine-learning approaches. The paper offers an integration of this intrusion detection systems into an existing 5G architecture. We offer the methodology and a pseudo code of this system.",,2021.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
75fccf9eda7cf9c961efd3e9d42ba73ed37ab2e9,https://www.semanticscholar.org/paper/75fccf9eda7cf9c961efd3e9d42ba73ed37ab2e9,Tutorial: Edge Computing for Mobile Internet of Things,"Internet of things (IoT) has emerged as the enabling technology for smart applications in different domains, such as transportation, health-care, industry, smart homes and buildings, and education (e.g., [1-5]). IoT applications rely on the deployment of resourceconstrained devices that collect data from the environment it is immersed and control events of interest through actuators. One of the daunting challenges in many IoT applications is the need for the real-time processing of a large amount of produced data. Such processing is often impractical to be performed at the IoT devices, due to their resource-constrained nature and the incurred energy cost. In this regard, IoT data is often offloaded to be processed on distant powerful cloud servers, which return to IoT devices as the result of the heavy computations. This approach is well-suited for computation-intensive tasks in IoT applications. However, the process of task offloading to cloud servers incurs additional delays for the IoT application, in addition to the network overhead. Therefore, edge computing has been proposed to provide computation, communication, and storage resources closer to IoT devices. The general idea is to place resources in the proximity of IoT devices that will demand them. Thus, the latency involved in the IoT application is reduced since computation-intensive tasks are processed on edge devices rather than on distant cloud servers. One of the critical challenges in edge-aided IoT applications is that edge devices have limited resource capabilities when compared to cloud servers. In this regard, edge devices' resources must be managed and allocated in an efficient way, aimed at providing resources to IoT applications with guaranteed quality of service (QoS). This tutorial will motivate and explore the challenges, design principles, and goals of edge computing for IoT applications. It presents the building blocks for the design of optimization models for IoT task offloading to edge nodes. By doing so, it discusses the communication challenges between IoT and edge devices and highlights the different mathematical formulations commonly used in the literature to model IoT to edge communication. Furthermore, this tutorial discusses optimization-based and machine learning (ML)-based solutions for tackling the task offloading decision problem. Besides, this tutorial presents recent advancements in resource management solutions aimed at efficient resource allocation at edge devices. Finally, this tutorial shall conclude with a discussion of research opportunities and challenges in the edge-assisted Internet of things.",DIVANet,2021.0,10.1145/3479243.3494705,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
c78ff7ca730c85269065dd16ed6e8aea31e1634d,https://www.semanticscholar.org/paper/c78ff7ca730c85269065dd16ed6e8aea31e1634d,Agentless Insurance Model Based on Modern Artificial Intelligence,"Since past couple of years, Agents have been a crucial part of the financial sector, primarily focusing on the Auto Insurance sector, whose key responsibilities are centered around finding new prospective customers and maintaining a relationship with existing customers. But with every other company streamlining their business processes with the latest Technology, Insurance Industry is not too far behind. Currently, Insurance Industry has dived and started exploring the online space. Prospective customers can now get online insurance quotes, chat with an online robot and even purchase an Insurance policy online. Digitalization, Automation, and Streamlining are key buzzwords in every type of business sector. Given the above trends, Insurance Agents seem to be an unnecessary expense. In this paper, we propose an Artificial-Intelligence driven approach that eliminates the need for a human Insurance Agent that will ultimately reduce the overall cost for the end customer. As part of our contribution to the above problem statement, we have proposed a Software Application where four Statistical Models are deployed. These Models are tasked with determining prospective customers who will likely buy an Insurance Policy, identifying customers who are likely to cancel a policy so that we can provide them with something better, identifying customers submitting fraudulent insurance claims and finally a Recommendation System Model to recommend updates to current policy to existing policy of Customers. In our Experimentation Results, we identified a cluster of customers who were most likely to buy a product using an Unsupervised Statistical Machine Learning model.",2021 IEEE 22nd International Conference on Information Reuse and Integration for Data Science (IRI),2021.0,10.1109/IRI51335.2021.00013,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
7edc46c710e2ed4bb30a3e869f648012bcdc0a48,https://www.semanticscholar.org/paper/7edc46c710e2ed4bb30a3e869f648012bcdc0a48,Rapid Material Characterization using Smart Skin with functional Data Analysis,"Automotive industries in the last decade are demanding for lightweight, corrosion resistance and improved fatigue performance materials. Composites having all such properties quickly gain popularity. Due to complex fabrication methods, composites are known for various process defects. Thus, monitoring and characterizing the composite component before assembly is necessary to maintain the overall structural integrity. Further a rapid and reliable inspection is much needed addition in most automotive industries to save time. 
Non-Destructive Evaluation (NDE) is popular for component testing in the automobile industry because they do not cause any permanent alteration to components. Some NDE techniques require sensors (Piezo Electric transducers, PVDF films, Optical fiber, etc.) that need to be bonded to the structure for testing. Some of these sensors are expensive and cannot be reused once detached. This work aims at the reusability of sensors. Further, the reusable sensors can be deployed in an array configuration for multi-purpose NDE. SMART Skin shall be explained as a Multiple-Transmitter-Multiple-Receiver (MTMR) Piezo-ceramic based sensor (PZT) array which is embedded to a conformable skin. The bottom layer of the skin is coated with pressure-sensitive adhesive to be attached to most curved and non-curved structural surfaces. Each PZT sensor nodes are individually controlled by a MATLAB code that actuates and receive the GW waves signals. 
In this work, we have validated the ability of rapid material characterization using isotropic materials. 
Using experimental data collected by the proposed methodology, we have developed an automated material classification algorithm using various machine learning algorithms. A novel application of functional data analysis (fDA) which converts discrete samples into continuous curves is used. The curves are represented as linear combinations of basis functions. The advantage of this fDA method is that we use the shape of the signal instead of extracting features from the signal. Several choices of basis coefficients such as fourier series, wavelet and B-spline are considered of which spline basis gives the best results. These basis coefficients are then feed into machine learning models like Support Vector Machine (SVM), Random Forest (RF), k nearest neighbor (knn) for classification. The efficacy of this novel method is compared with conventional feature extraction techniques such as zero-crossing coefficients, absolute maximum value, different statistical features like mean and variance, energy of the signal. Encouraging results are obtained that shows the fDA methodology is efficient over the conventional feature extraction methods as it improves the prediction performance on the classifier and result in a faster and cost-effective model by reducing the predictor dimensionality.",,2021.0,10.36001/PHME.2021.V6I1.2884,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
e23e0c6529f205cdeb65f226a59fd996b8a25e8b,https://www.semanticscholar.org/paper/e23e0c6529f205cdeb65f226a59fd996b8a25e8b,A proposal to systematize introducing DevOps into the software development process,"The software development industry has been evolving with new development standards and service delivery models. Agile methodologies have reached their completion with DevOps, thereby increasing the quality of the software and creating greater speed in delivery. However, a gap regarding the formalization of its adoption and implementation doubts became relevant. My hypothesis is that, by systematizing the introduction of DevOps into the software development process and defining the function of the members of the DevOps team members, may well make it quicker to implement this process, thus reducing conflicts between the teams. As part of the investigation of this hypothesis, the result of the research will be applied in practical development environments i.e. in a Technology Agency of the State of the Brazilian Government and also at the Brazilian Company Neurotech in order to evaluate its effectiveness from metrics appropriate for DevOps environments. Keywords—DevOps, software development, maturity model I. PROBLEM DevOps is an emerging practice that has been adopted in the software development cycle. It focuses on the convergence of standards between the Development teams and the Operations teams and it seeks to improve cooperation between both teams, hence the origin of the term [1]. However, there is no consensus on the definition of what DevOps is. Wiedemann et al. [2] emphasize that one of the biggest challenges in the industry is the lack of a formal concept for DevOps. This conceptual flaw in what DevOps is, directly impacts the understanding of the objects and actions needed to overcome this flaw [3]. The adoption of Agile and DevOps Methodologies continues to grow, driven by the “need for speed”, agility and flexibility, evidenced in the World Quality Report of 2019 [4], in which 99% of the interviewees said they were using DevOps in at least some of their business. Implementing DevOps has become more difficult due to the lack of formalizing the concept and adoption processes. Although there is an abundance of information, practices and tools related to DevOps, it is still unclear how anyone could take advantage of this rich, yet diffuse information in an organized and structured way to properly adopt DevOps [6]. The study by Zulfahmi [7] provides evidence of this lack of standardization. The author also states that other major challenges are the lack of process and guidelines for implementing the practice of DevOps in Continuous Delivery. In the Systematic Literature Review conducted by Gasparaite; Naudziunaite and Ragaisis [8], 24 DevOps models were identified, but only 04 were considered applicable for practical use: the Focus Area, Bucena-Kirikova, Mohamed, and Radstaak models. However, in the Radstaak model, not all the steps necessary for its practical use have been described; the Mohamed model did not present the evaluation process, only how to apply the model; and the Focus Area and Bucena-Kirikova Models can be applied in practice, since their authors document the evaluation methods adopted in academic publications. Another aspect to be highlighted concerns what must be done or would be appropriate prior to an organization adopting DevOps. Leite et al. [9] ponder this aspect by considering a difficult question that the literature has not yet fully answered. They further point out that it is incomplete and even contradictory in relation to the subject. At the same time, the variety of DevOps tools seems to challenge the idea of there being a single person with the role of administering the entire process. Even mature teams, who have both knowledge of development and infrastructure operations, may find it difficult to be familiar with all these tools, thus making it necessary to define the roles of those involved in this process. The need to systematize the introduction of DevOps in the software development process is therefore necessary. Thus, the need for more research and empirical work is essential to put into practice and validate a proposal to systematize the introduction of DevOps. In view of the above, the following problems are in evidence: a) the lack of a conceptual definition and, consequently, the need for a proposal to systematize the introduction of DevOps in the software development process, that presents systematic improvements, evidenced by its adoption and the results of its effectiveness; and b) the lack of definition of the roles of those involved in this cycle. The present thesis should conduct a study that sets out to identify the sets of good practices already used in software development processes that use DevOps, and based on these to conduct an analysis, validation and standardization of their use, with a view to systematizing the introduction and execution of DevOps in this process. II. RESEARCH HYPOTHESIS A software production line that uses DevOps, has a welldefined automation cycle, starting with the developers' source code commits for the code version-control system. When the CI server identifies the completion of the commits, it performs the necessary tests and, if necessary, provides feedback to the developers [10]. In this thesis we will propose systematic practices for introducing DevOps and improving the efficiency of the software production process that uses DevOps. As a way of conducting the research and delimiting the scope of the study, the following hypothesis is proposed: Having adopted a set of systematized practices for introducing DevOps, and for formalizing and delimiting the role of the members of the DevOps team in the Software Production Process, Software Factories become faster in their implementation, thereby reducing conflicts between teams and providing quality deliverables. To validate the hypothesis, the following questions were constructed: RQ1: What are the gaps in the software development process that use DevOps practices? In order to propose solutions to the existing problems when introducing DevOps, one must first identify the gaps in the process. The purpose of RQ1 is to locate these gaps and the actions that must be taken to resolve such issues. RQ2: What are the practices that support the systematic introduction of DevOps in the software development process? The objective of RQ2 is to identify all the good practices used by the development and academic industry that support introducing it into the software development process. III. EXPECTED CONTRIBUTION The objective of my doctoral thesis is to improve the software development process using DevOps and to propose new ways of introducing it. The expected contributions of this project are summarized below: 1) to systematize the introduction of DevOps in the software development process; and 2) to improve the adoption of DevOps in the software development process. IV. METHODS AND PRELIMINARY FINDINGS As a preliminary study, a Systematic Mapping (SM) of Literature [11] was undertaken, in May 2020, with a view to analyzing the topic, and hence seeking to identify existing gaps in the area of DevOps and difficulties in the implementation process. The SM [11] will serve to underpin the initial conduct of the research, and thus provide a diagnosis of the software production process that uses DevOps, also providing initial data that can be used to build a set of systematic practices for adopting DevOps. The SM [11] showed how DevOps is highlighting the following issues: pipeline security, effective adoption of a cloud environment, adoption of microservices, infrastructure as code, use of container solutions, and tools to automate the development pipeline. Regarding best practices, DevOps was found to have: Infrastructure as Code, Continuous Integration, Continuous Delivery, Continuous Deployment. One of the questions raised in the MS [11] was about the existence of Maturity Models, in which 11 were found. However, only two were validated by companies, and one of them is applied only in IBM solutions. The roles of the actors involved in the process were found in 02 studies, with different names, but performing the same function. In a second phase, a Multi-Vocal Literature Review will be carried out to look for relevant information on emerging industry topics, which were not achieved by using SM. This type of Review has both academic publications and gray literature as input, its main objective being to close the gap between academic research and professional practice [12]. The methodology adopted for practical research will be Design Science Research (DSR), which is a method that establishes and operationalizes research when the desired objective is an artifact or a recommendation [13]. Given the above, there will be a need for an iterative cycle and the production of an artifact, which may be a set of systematic practices for adopting DevOps, which will be validated and improved throughout the research. The research will be applied in a Technology Agency of the State of the Brazilian Government, of which the author is Technical Director and has an Information Systems Coordination Unit with several applications under development using DevOps practices, and in the Brazilian company Neurotech, which develops advanced solutions of Artificial Intelligence, Machine Learning and Big Data, and has a portfolio of more than 100 customers. V. EVALUATION PROCEDURES I am starting my second year of doctorate and by its end, RQ1 and RQ2 will have been answered. The next step will be to propose a handbook that will systematize the introduction of DevOps, during which it will be applied in practice as previously mentioned in a State Technology Agency and Neurotech. The instrument for evaluating the results of the research proposed in the DevOps development process will be conducted by means of specific DevOps metrics, collected throughout the development process. Some studies have presented a set of metrics that can be used to assess the applicability of DevOps in the Software Production Process: Delivery ti",ICSE,2021.0,10.1109/ICSE-Companion52605.2021.00124,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
1460f1c20b4314cf5fa9a3d12bcf878614723f4c,https://www.semanticscholar.org/paper/1460f1c20b4314cf5fa9a3d12bcf878614723f4c,Technology Focus: Sand Management (October 2021),"Our industry is under pressure to produce cleaner energy. That is the mantra, more so than a few years ago. A recent report from the International Energy Agency suggested that all greenfield developments in the oil and gas sector should be stopped forthwith if we are to achieve the net-zero target by 2050. That essentially means that we squeeze what we can from the not-so-easy and mature reservoirs, many of which have sand-control problems. Perhaps that is the reason most operators are working ever harder to manage and produce such assets, a trend reflected in the number of papers written. More importantly, a large proportion of papers this year were on sand consolidation and through-tubing exclusion methods, which primarily target mature producing reservoirs.
 A few technology trends are becoming apparent. There is a move to gravel pack longer and longer horizontal sections. It is now possible to pack more than 7,000 ft with zonal isolation. Through-tubing sand-control remediation continues to evolve. Sand consolidation is moving toward nanoparticles, with a promise of better regained permeability. Further strides have been made in developing filters to achieve behind-screen compliance for better sand retention.
 Industry has been enchanted by what data analytics and machine learning can potentially offer, and perhaps rightly so. Several papers this year apply these tools to sand management. For those interested, I would recommend paper SPE 200949 and OTC 31234 as further reading. Unfortunately, from a sand-control perspective, I do not yet see a compelling narrative. One interesting statistic that I stole from a LinkedIn post is that the rising 3-year trend of papers in OnePetro on this subject has fallen dramatically between 2020 and 2021. I have not independently verified these figures, but it does tell a story. Is the excitement waning?
 Recommended additional reading at OnePetro: www.onepetro.org.
 SPE 203238 - Sanding Propensity Prediction Technology and Methodology Comparison by Surej Kumar Subbiah, Universiti Teknologi Malaysia and Schlumberger, et al.
 SPE 201768 - Using Artificial Intelligence for Determining Threshold Sand Rates From Acoustic Monitors by Srinivas Swaroop Kolla, The University of Tulsa, et al.
 OTC 30386 - Pioneering Slickline Deployed Through Tubing Gravel Pack in Malaysia: Successful Case Study and Lessons Learned by Ertiawati Mappanyompa, Petronas, et al.",Journal of Petroleum Technology,2021.0,10.2118/1021-0067-jpt,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
84e9fc77d419b09924e0ee644028ad360574bb59,https://www.semanticscholar.org/paper/84e9fc77d419b09924e0ee644028ad360574bb59,Editorial: Data-Driven Solutions for Smart Grids,"This research topic deals with innovative data-driven solutions for smart grids. The integration of communication systems and smart metering in power system has led, in recent years, to capability to measure quantities and communicate measurements with a precision, a sampling rate and a bandwidth that were unimaginable just a few years ago. The potential for this amount of information is huge but the power system industry and community have not yet fully exploited such a capability. In this complex data-rich, but information-limited domain, the data streaming generated by the pervasive grid sensors do not always provide smart grids operators with the necessary information to react to external disturbances in a timely manner. Even if fast computing algorithms are utilized to convert data into information, smart grid operators face the challenge of not having the full picture of the information context and, therefore, the obtained information cannot be deployed with a high degree of confidence. To address this complex issue, the most promising research directions are oriented toward the conceptualization of improved information processing paradigms and smart decision support systems aimed at enhancing standard operating procedures, based on pre-defined grid conditions and static operating thresholds, with a set of interactive information services, which could promptly provide the right information at the right moment to the right decision maker. To effectively support the deployment of these services in modern smart grids it will be incumbent upon the scientific community to develop advanced techniques and algorithms for reliable power system data acquisition and processing, which should support semantics and content-based data extraction and integration from heterogeneous sensor networks. This research topic contains four articles. The paperOptimal Balancing ofWind Parks with Virtual Power Plants by Omelčenko andManokhin addresses data-driven solutions in the context of optimization of virtual power plants. This work proposes the use of machine learning to process available data measurements. The goal is to balance the power production and at the same time maximize the revenue of a portfolio of power plants with different technologies (biogas, wind, batteries, etc.) considering uncertainty in both price and power production. The paper Supporting Regulatory Measures in the Context of Big Data Applications for Smart Grids by Mladin discusses the policy and regulatory aspects. This paper focuses in particular on big data applications to the ongoing “energy transition” process built on higher renewable energy integration and digitalization, and discusses how this can help regulatory measures through societal acceptance and involvement. The paper Data Consistency for Data-Driven Smart Energy Assessment by Chicco addresses the issue of data consistency and discusses data-versus model-based approaches. The latter is an emerging topic and a potential paradigm shift in power system analysis that have been traditionally based on modelEdited and reviewed by: Huan Liu, Arizona State University, United States",Frontiers in Big Data,2021.0,10.3389/fdata.2021.815686,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
4661974c05d6d46c7708b5bf7440675229487cab,https://www.semanticscholar.org/paper/4661974c05d6d46c7708b5bf7440675229487cab,Intrusion Detection System for 5G with a Focus on DOS/DDOS Attacks,"The industry of telecommunications is being transformed towards 5G technology, because it has to deal with the emerging and existing use cases. Because, 5G wireless networks need rather large data rates and much higher coverage of the dense base station deployment with the bigger capacity, much better Quality of Service - QoS, and the need very low latency [1–3]. The provision of the needed services which are envisioned by 5G technologies need the new service models of deployment, networking architectures, processing technologies and storage to be defined. These technologies will cause the new problems for the cybersecurity of 5G systems and the security of their functionality. The developers and researchers working in this field make their best to secure 5G systems. The researchers showed that 5G systems have the security challenges. The researchers found the vulnerabilities in 5G systems which allow attackers to integrate malicious code into the system and make the different types of the illegitimate actions. MNmap, Battery drain attacks and MiTM can be successfully implemented on 5G. The paper makes the analysis of the existing cyber security problems in 5G technology. Based on the analysis, we suggest the novel Intrusion Detection System - IDS by means of the machine-learning algorithms. In the related papers the scientists offer to use NSL-KDD in order to train IDS. In our paper we offer to train IDS using the big datasets of DOS/DDOS attacks, besides of training using NSL-KDD. The research also offers the methodology of integration of the offered intrusion detection systems into an standard architecture of 5G. The paper also offers the pseudo code of the designed system.",2021 11th IEEE International Conference on Intelligent Data Acquisition and Advanced Computing Systems: Technology and Applications (IDAACS),2021.0,10.1109/IDAACS53288.2021.9661021,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
0fd008b44d62266705871c77d70c069e375ee1fb,https://www.semanticscholar.org/paper/0fd008b44d62266705871c77d70c069e375ee1fb,5G AutoMEC – Boosting edge-to-edge service continuity for CAM in a sliced network,"Network function virtualization and edge computing in mobile networks are key enablers in the evolution of recent standards for a 5 Generation (5G) mobile communication system towards a comprehensive 5G ecosystem, which enables the deployment of customized networks and service access for various industry verticals by means of clearly defined and deployed network slices. In particular, the automotive industry can leverage low-latency access to services hosted along the distributed network edge, in support of a large variety of use cases. Network slices typically implement the requested service and network according to a set of defined requirements, as well as performance and latency bounds, while using a defined resources budget. With connected cars following different mobility patterns, the automotive sector represents a pretty agile customer of such network slices. This makes the management of network and edge computing resources a key challenge to tackle in order to balance the resource utilization, and the previously agreed and expected service levels. In this paper, we analyze the benefit of smart mobile edges and the use of machine learning to anticipate resource demand at distributed mobile edges in an automotive scenario. Finally, we experimentally show the feasibility to treat μ-slice resources efficiently by using an OSS-based prototype for the orchestrated edges, which is currently being developed for an automotive trial in Europe.",,2021.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
7e85a1462d85730543ef206a923fae64d9236b4f,https://www.semanticscholar.org/paper/7e85a1462d85730543ef206a923fae64d9236b4f,Digital Twin: Universal User Interface for Online Management of the Manufacturing System,"
 Industry 4.0 concept enables connecting a multitude of equipment to computer simulations through IoT and virtual commissioning, but using conventional interfaces for each separate piece of equipment for control and maintenance of Digital Twins is not always an optimal solution. Industrial Digital Twins software toolkits usually consist of simulation or offline programming tools. It can even connect real machines and controllers and sensors to feed a simulation with actual production data and later analyze it. Moreover, Virtual Reality (VR) and Augmented Reality (AR) are used in different ways for monitoring and design purposes. However, there are many software tools for the simulation and re-programming of robots on the market already, but those are a limited number of software that combine all these features, and all of those send data only in one way, not allowing to re-program machines from the simulations. The related research aims to build a modular framework for designing and deploying Digital Twins of industrial equipment (i.e., robots, manufacturing lines), focusing on online connectivity for monitoring and control. A developed use-case solution enables one to operate the equipment in VR/AR/Personal Computer (PC) and mobile interfaces from any point globally while receiving real-time feedback and state information of the machinery equipment. Gamified multi-platform interfaces allow for more intuitive interactions with Digital Twins, providing a real-scale model of the real device, augmented by spatial UIs, actuated physical elements, and gesture tracking.
 The introduced solution can control and simulate any aspect of the production line without limitation of brand or type of the machine and being managed and self-learning independently by exploiting Machine Learning algorithms. Moreover, various interfaces such as PC, mobile, VR, and AR give an unlimited number of options for interactions with your manufacturing shop floor both offline and online. Furthermore, when it comes to manufacturing floor data monitoring, all gathered data is being used for statistical analysis, and in a later phase, predictive maintenance functions are enabled based on it.
 However, the research scope is broader; this particular research paper introduces a use-case interface on a mobile platform, monitoring and controlling the production unit of three various industrial- and three various mobile robots, partially supported by data monitoring sensors. The solution is developed using the game engine Unity3D, Robot Operation System (ROS), and MQTT for connectivity. Thus, developed is a universal modular Digital Twin all-in-one software platform for users and operators, enabling full control over the manufacturing system unit.",Volume 2B: Advanced Manufacturing,2021.0,10.1115/imece2021-69092,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
f15eb7cf170f7939275b12176fdcfb6decf523d2,https://www.semanticscholar.org/paper/f15eb7cf170f7939275b12176fdcfb6decf523d2,CubeSat Communications: Recent Advances and Future Challenges,"Given the increasing number of space-related applications, research in the emerging space industry is becoming more and more attractive. One compelling area of current space research is the design of miniaturized satellites, known as CubeSats, which are enticing because of their numerous applications and low design-and-deployment cost. The new paradigm of connected space through CubeSats makes possible a wide range of applications, such as Earth remote sensing, space exploration, and rural connectivity. CubeSats further provide a complementary connectivity solution to the pervasive Internet of Things (IoT) networks, leading to a globally connected cyber-physical system. This paper presents a holistic overview of various aspects of CubeSat missions and provides a thorough review of the topic from both academic and industrial perspectives. We further present recent advances in the area of CubeSat communications, with an emphasis on constellation-and-coverage issues, channel modeling, modulation and coding, and networking. Finally, we identify several future research directions for CubeSat communications, including Internet of space things, low-power long-range networks, and machine learning for CubeSat resource allocation.",IEEE Communications Surveys & Tutorials,2019.0,10.1109/COMST.2020.2990499,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
7cc3becf71586189f9a2d32bc70a2843739d20d6,https://www.semanticscholar.org/paper/7cc3becf71586189f9a2d32bc70a2843739d20d6,Editorial: Big Data Innovation for Sustainable Cognitive Computing,,Mob. Networks Appl.,2019.0,10.1007/S11036-018-1198-5,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
8061d521fe7c62430e1ba2cb0b8f4d19f7366e6f,https://www.semanticscholar.org/paper/8061d521fe7c62430e1ba2cb0b8f4d19f7366e6f,Multi Uav Cooperative Surveillance With Spatio Temporal,"Deep Learning for Unmanned SystemsMultiple Heterogeneous Unmanned Aerial VehiclesAdvanced Mobile RoboticsSafe Robot Navigation Among Moving and Steady ObstaclesComputer Safety, Reliability, and SecurityAdvances in Swarm IntelligenceHolonic and Multi-Agent Systems for ManufacturingAdvances in Artificial Intelligence and Applied Cognitive ComputingUnmanned Aircraft SystemsIntelligent Computing Theories and ApplicationAutonomous Airborne Wireless NetworksAd Hoc NetworksEnabling Blockchain Technology for Secure Networking and CommunicationsUAV Sensors for Environmental MonitoringUnmanned Aerial Vehicles: Breakthroughs in Research and PracticeComputational Collective IntelligenceTime-Critical Cooperative Control of Autonomous Air VehiclesAdvances in Cooperative Control and OptimizationCooperative Robots and Sensor Networks 2015Artificial Intelligence and SecurityPRICAI 2016: Trends in Artificial IntelligenceClosing the Gap Between Research and Field Applications for Multi-UAV Cooperative MissionsMulti-rotor Platform Based UAV SystemsProceedings of the Future Technologies Conference (FTC) 2020, Volume 1Unmanned Aerial SystemsAdvanced Distributed Consensus for Multiagent SystemsCooperative Control of MultiAgent SystemsMulti-UAV Planning and Task AllocationMobile Internet SecurityCooperative Control of Multiple Unmanned Aerial Vehicles with Application to Forest Fire Detection and FightingMulti UAV Systems with Motion and Communication ConstraintsIntelligent Autonomy of UAVsIntelligent and Fuzzy Techniques in Big Data Analytics and Decision MakingIntelligent Autonomy of UAVsUAV Cooperative Decision and ControlCooperative Localization and NavigationAdvances in Guidance, Navigation and ControlMachine Learning and Intelligent CommunicationsUnmanned Aerial VehiclesThe Cognitive Approach in Cloud Computing and Internet of Things Technologies for Surveillance Tracking Systems Ad hoc networks, which include a variety of autonomous networks for specific purposes, promise a broad range of civilian, commercial, and military applications. These networks were originally envisioned as collections of autonomous mobile or stationary nodes that dynamically auto-configure themselves into a wireless network without relying on any existing network infrastructure or centralized administration. With the significant advances in the last decade, the concept of ad hoc networks now covers an even broader scope, referring to the many types of autonomous wireless networks designed and deployed for a specific task or function, such as wireless sensor networks, vehicular networks, home networks, and so on. In contrast to the traditional wireless networking paradigm, such networks are all characterized by sporadic connections, highly error-prone communications, distributed autonomous operation, and fragile multi-hop relay paths. The new wireless networking paradigm necessitates reexamination of many established concepts and protocols, and calls for developing a new understanding of fundamental problems such as interference, mobility, connectivity, capacity, and security, among others. While it is essential to advance theoretical research on fundamental and practical research on efficient policies, algorithms and protocols, it is also critical to develop useful applications, experimental prototypes, and real-world deployments to achieve an immediate impact on society for the success of this wireless networking paradigm.A comprehensive review of the state of the art in the control of multi-agent systems theory and applications The superiority of multi-agent systems over single agents for the control of unmanned air, water and ground vehicles has been clearly demonstrated in a wide range of application areas. Their large-scale spatial distribution, robustness, high scalability and low cost enable multi-agent systems to achieve tasks that could not successfully be performed by even the most sophisticated single agent systems. Cooperative Control of Multi-Agent Systems: Theory and Applications provides a wide-ranging review of the latest developments in the cooperative control of multi-agent systems theory and applications. The applications described are mainly in the areas of unmanned aerial vehicles (UAVs) and unmanned ground vehicles (UGVs). Throughout, the authors link basic theory to multi-agent cooperative control practice — illustrated within the context of highly-realistic scenarios of high-level missions — without losing site of the mathematical background needed to provide performance guarantees under general working conditions. Many of the problems and solutions considered involve combinations of both types of vehicles. Topics explored include target assignment, target tracking, consensus, stochastic game theory-based framework, event-triggered control, topology design and identification, coordination under uncertainty and coverage control. Establishes a bridge between fundamental cooperative control theory and specific problems of interest in a wide range of applications areas Includes example applications from the fields of space exploration, radiation shielding, site clearance, tracking/classification, surveillance, search-and-rescue and more Features detailed presentations of specific algorithms and application frameworks with relevant commercial and military applications Provides a comprehensive look at the latest developments in this rapidly evolving field, while offering informed speculation on future directions for collective control systems The use of multi-agent system technologies in both everyday commercial use and national defense is certain to increase tremendously in the years ahead, making this book a valuable resource for researchers, engineers, and applied mathematicians working in systems and controls, as well as advanced undergraduates and graduate students interested in those areas.Time-Critical Cooperative Control of Autonomous Air Vehicles presents, in an easy-to-read style, the latest research conducted in the industry, while also introducing a set of novel ideas that illuminate a new approach to problem-solving. The book is virtually self-contained, giving the reader a complete, integrated presentation of the different concepts, mathematical tools, and control solutions needed to tackle and solve a number of problems concerning time-critical cooperative control of UAVs. By including case studies of fixed-wing and multirotor UAVs, the book effectively broadens the scope of application of the methodologies developed. This theoretical presentation is complemented with the results of flight tests with real UAVs, and is an ideal reference for researchers and practitioners from academia, research labs, commercial companies, government workers, and those in the international aerospace industry. Addresses important topics related to time-critical cooperative control of UAVs Describes solutions to the problems rooted in solid dynamical systems theory Applies the solutions developed to fixed-wing and multirotor UAVs Includes the results of field tests with both classes of UAVsThis book provides the state-of-the-art intelligent methods and techniques for solving realworld problems along with a vision of the future research. The fifth 2020 Future Technologies Conference was organized virtually and received a total of 590 submissions from academic pioneering researchers, scientists, industrial engineers, and students from all over the world. The submitted papers covered a wide range of important topics including but not limited to computing, electronics, artificial intelligence, robotics, security and communications and their applications to the real world. After a double-blind peer review process, 210 submissions (including 6 poster papers) have been selected to be included in these proceedings. One of the meaningful and valuable dimensions of this conference is the way it brings together a large group of technology geniuses in one venue to not only present breakthrough research in future technologies, but also to promote discussions and debate of relevant issues, challenges, opportunities and research findings. The authors hope that readers find the book interesting, exciting and inspiringAdvanced Distributed Consensus for Multiagent Systems contributes to the further development of advanced distributed consensus methods for different classes of multiagent methods. The book expands the field of coordinated multiagent dynamic systems, including discussions on swarms, multi-vehicle and swarm robotics. In addition, it addresses advanced distributed methods for the important topic of multiagent systems, with a goal of providing a high-level treatment of consensus to different versions while preserving systematic analysis of the material and providing an accounting to math development in a unified way. This book is suitable for graduate courses in electrical, mechanical and computer science departments. Consensus control in multiagent systems is becoming increasingly popular among researchers due to its applicability in analyzing and designing coordination behaviors among agents in multiagent frameworks. Multiagent systems have been a fascinating subject amongst researchers as their practical applications span multiple fields ranging from robotics, control theory, systems biology, evolutionary biology, power systems, social and political systems to mention a few. Gathers together the theoretical preliminaries and fundamental issues related to multiagent systems and controls Provides coherent results on adopting a multiagent framework for critically examining problems in smart microgrid systems Presents advanced analysis of multiagent systems under cyberphysical attacks and develops resilient control strategies to guarantee safe operationComplete with online files and updates, this cutting-edge text looks at the next generation of unmanned flying machines. Aerial robots can be considered as an evolution of the Unmanned Aerial Vehicl",,2021.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
af7fb5305e2c6dbbc08167a4d442e7449761de5d,https://www.semanticscholar.org/paper/af7fb5305e2c6dbbc08167a4d442e7449761de5d,Computerize the Race Problem?: Why We Must Plan for a Just AI Future,"1960s civil rights and racial justice activists tried to warn us about our technological ways, but we didn't hear them talk. The so-called wizards who stayed up late ignored or dismissed black voices, calling out from street corners to pulpits, union halls to the corridors of Congress. Instead, the men who took the first giant leaps towards conceiving and building our earliest ""thinking"" and ""learning"" machines aligned themselves with industry, government and their elite science and engineering institutions. Together, they conspired to make those fighting for racial justice the problem that their new computing machines would be designed to solve. And solve that problem they did, through color-coded, automated, and algorithmically-driven indignities and inumahities that thrive to this day. But what if yesterday's technological elite had listened to those Other voices? What if they had let them into their conversations, their classrooms, their labs, boardrooms and government task forces to help determine what new tools to build, how to build them and - most importantly - how to deploy them? What might our world look like today if the advocates for racial justice had been given the chance to frame the day's most preeminent technological question for the world and ask, ""Computerize the Race Problem?"" Better yet, what might our AI-driven future look like if we ask ourselves this question today?",AIES,2020.0,10.1145/3375627.3377140,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
b119b942a2eba94fc38655692f7c07e0044a8bbc,https://www.semanticscholar.org/paper/b119b942a2eba94fc38655692f7c07e0044a8bbc,"Finding Meaning, Application for the Much-Discussed “Digital Twin”","Among the many buzzy, digitally related words or terms bandied about the industry over the past year or two, “digital twin” serves as something of a confluence of them all.
 Populating many industry conference agendas are high-level presentations and discussions with descriptors such as digitization, digitalization, digital transformation, and the digital disruption, which involve big data, data analytics, advanced analytics, artificial intelligence (AI), machine learning, automation, the Internet of Things (IoT), and the ever-important, abundantly abstract cloud. Some of those terms are used rather broadly and inter-changeably, leading many to wonder: What exactly are we talking about here? 
 The definition of a digital twin is similarly less finite, but it is rather easy to conceptualize at a basic level. The technology links the physical world with the digital world, providing a digital model of a physical asset or process. It serves as a real-time data hub for its owner, allowing for reference to designed or expected performance and continuous offsite monitoring. 
 Roots of the idea can be traced back to a similarly challenging sector that is tasked with exploring and operating in harsh environments normally inaccessible by humans. Successful space travel requires complex modeling and simulations, which NASA for decades has employed from its field centers on the ground. Michael Grieves, currently executive director of Florida Institute of Technology, brought the term to the fore during the early 2000s while working in the University of Michigan’s college of engineering, but he credits the coinage to his one-time NASA colleague and technologist John Vickers.
 Multinational conglomerates such as GE and Siemens have deployed the twin in everything from jet engines to power plants. The architectural engineering and automotive manufacturing industries are also mature in their use of digital twin. The digital twin in the building industry is referred to as the building information model.
 Traditionally slow on the uptake when it comes to implementing new technologies, the oil and gas industry is beginning to put the twin to work on its equipment, facilities, and wells with the promise of more efficient engineering, procurement, construction (EPC), and operations. The virtual hub is envisioned as a way of breaking down silos between work phases, operators and contractors, and the different industry disciplines—such as geophysics and reservoir engineering—required to carry out upstream projects.",Journal of Petroleum Technology,2018.0,10.2118/0618-0026-JPT,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
ebe8750dba24792a45b598558c1f20a4265ce929,https://www.semanticscholar.org/paper/ebe8750dba24792a45b598558c1f20a4265ce929,Lead Engagement by Automated Real Estate Chatbot,"Recently, automated chatbot has been increasingly applied in real estate industry. Even though chatbots cannot fully replace the traditional relation between agents and home buyers, they can help to engage potential clients (or leads) in meaningful conversations, which is highly useful for lead capture. In this paper, we present an intelligent chatbot for this purpose. Various machine learning techniques, including multi-task deep learning technique for intent identification and frequent itemsets for conversation elaboration, have been employed in our system. Our chatbot has been deployed by CEO K35 GROUP JSC with daily updated data of real estate information at Hanoi and Ho Chi Minh cities, Vietnam.",2018 5th NAFOSTED Conference on Information and Computer Science (NICS),2018.0,10.1109/NICS.2018.8606862,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
aa30901b02e9923851d507e1f2726bcf9ef1b068,https://www.semanticscholar.org/paper/aa30901b02e9923851d507e1f2726bcf9ef1b068,OccuSpace: Towards a Robust Occupancy Prediction System for Activity Based Workplace,"Workplace occupancy detection is becoming increasingly important in large Activity Based Work (ABW) environments as it helps building and office management understand the utilisation and potential benefits of shared workplace. However, existing sensor-based technologies detect workstation occupancy in indoor spaces require extensive installation of hardware and maintenance incurring ongoing costs. Moreover, accuracy can depend on the specific seating styles of workers since the sensors are usually placed under the table or overhead. In this research, we provide a robust system called OccuSpace to predict occupancy of different atomic zones in large ABW environments. Unlike fixed sensors, OccuSpace uses statistical features engineered from Received Signal Strength Indicator (RSSI) of Bluetooth card beacons carried by workers while they are within the ABW environment. These features are used to train state-of-the-art machine learning algorithms for prediction task. We setup the experiment by deploying our system in a realworld open office environment. The experimental results show that OccuSpace is able to achieve a high accuracy for workplace occupancy prediction.",2019 IEEE International Conference on Pervasive Computing and Communications Workshops (PerCom Workshops),2019.0,10.1109/PERCOMW.2019.8730762,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
06cfb2e8a76fdcfaa0618ca223171a478d22535d,https://www.semanticscholar.org/paper/06cfb2e8a76fdcfaa0618ca223171a478d22535d,Application of deep neural network and generative adversarial network to industrial maintenance: A case study of induction motor fault detection,"As data visibility in factories has increased with the deployment of sensors, data-driven maintenance has become popular in industries. Machine learning has been a promising tool for fault detection, but the problem is that the amount of fault data is much less than that of normal data which causes a data imbalance. In this study, we designed a deep neural network for fault detection and diagnosis, and compared the oversampling by a generative adversarial network to standard oversampling techniques. Simulation results indicate that oversampling by the generative adversarial network performs well under the given condition and the deep neural network designed is capable of classifying the faults of an induction motor with high accuracy.",2017 IEEE International Conference on Big Data (Big Data),2017.0,10.1109/BigData.2017.8258307,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10fbedfdb59ae5b9b1ebe0aca41a02a83254232f,https://www.semanticscholar.org/paper/10fbedfdb59ae5b9b1ebe0aca41a02a83254232f,In situ cocoa beans quality grading by near-infrared-chemodyes systems,"Fermentation level is a key bean quality indicator in the cocoa industry. A colorimetric sensor e-nose (CS e-nose) and an innovatively designed near infrared chemo-intermediary-dyes spectra technique (NIR-CDS) combined with four chemometric algorithms – extreme machine learning (ELM), support vector machine (SVM), linear discriminant analysis (LDA) and k-nearest neighbors (k-NN) – were applied to classify 90 sampled cocoa beans into three quality grades – fully fermented, partially fermented and non-fermented. The CS e-nose (89% ≤ Rp ≤ 94%) and NIR-CDS (85% ≤ Rp ≤ 94%) achieved comparable classification rates, with the systems' data cluster analysis yielding cophenetic correlation coefficients of 0.85–0.89. Both systems combined with SVM and ELM achieved a high classification rate (Rp = 94%) and could be applied to cocoa bean quality classification on an in situ and nondestructive basis. This novel NIR-CDS technique proved a pragmatic approach for the selection of sensitive chemo-dyes used in the fabrication of e-nose colorimetric sensor arrays compared with the hitherto trial-and-error method, which is time-consuming and dye-wasteful. The technique could also be deployed in near-infrared systems for the detection of volatile (gaseous) compounds, which previously had been a limitation.",,2017.0,10.1039/C7AY01751K,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
2a68d1014ca060b9cd7024217e9c483f3cb4fb47,https://www.semanticscholar.org/paper/2a68d1014ca060b9cd7024217e9c483f3cb4fb47,"Detecting Sensor Faults, Anomalies and Outliers in the Internet of Things: A Survey on the Challenges and Solutions","The Internet of Things (IoT) has gained significant recognition to become a novel sensing paradigm to interact with the physical world in this Industry 4.0 era. The IoTs are being used in many diverse applications that are part of our life and is growing to become the global digital nervous systems. It is quite evident that in the near future, hundreds of millions of individuals and businesses with billions will have smart-sensors and advanced communication technology, and these things will expand the boundaries of current systems. This will result in a potential change in the way we work, learn, innovate, live and entertain. The heterogeneous smart sensors within the Internet of Things are indispensable parts, which capture the raw data from the physical world by being the first port of contact. Often the sensors within the IoT are deployed or installed in harsh environments. This inevitably means that the sensors are prone to failure, malfunction, rapid attrition, malicious attacks, theft and tampering. All of these conditions cause the sensors within the IoT to produce unusual and erroneous readings, often known as outliers. Much of the current research has been done in developing the sensor outlier and fault detection models exclusively for the Wireless Sensor Networks (WSN), and adequate research has not been done so far in the context of the IoT. Wireless sensor network’s operational framework differ greatly when compared to IoT’s operational framework, using some of the existing models developed for WSN cannot be used on IoT’s for detecting outliers and faults. Sensor faults and outlier detection is very crucial in the IoT to detect the high probability of erroneous reading or data corruption, thereby ensuring the quality of the data collected by sensors. The data collected by sensors are initially pre-processed to be transformed into information and when Artificially Intelligent (AI), Machine Learning (ML) models are further used by the IoT, the information is further processed into applications and processes. Any faulty, erroneous, corrupted sensor readings corrupt the trained models, which thereby produces abnormal processes or outliers that are significantly distinct from the normal behavioural processes of a system. In this paper, we present a comprehensive review of the detecting sensor faults, anomalies, outliers in the Internet of Things and the challenges. A comprehensive guideline to select an adequate outlier detection model for the sensors in the IoT context for various applications is discussed.",,2020.0,10.3390/electronics9030511,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
b0528c13b684547b8a13241a4d212c32f176cbff,https://www.semanticscholar.org/paper/b0528c13b684547b8a13241a4d212c32f176cbff,IRCETMS 2020 Proceedings,"Internet of Things (IoT), IoT-education, STEM, smartness and digitalization are new techniques amd methods used in the industry 4.0 and Society 5.0 that enable universities, educational institutions to better management of resources and flexibility to respond to the business educational requirements and conditions. Demonstrators of the IoT has an important role, these resources consist of physical implementation of highly innovative experiments of IoT technologies. The demonstrators intend to be didactic labs where the students can make practical experiences in controlled situation but using real equipment or platforms. This learning-by-doing methodology is fundamental when dealing with a topic such as the ones studied in these projects, where practical skills are essential. The demonstrators will be remotely controlled; this will allow the students to use them event distance, ensuring a wider audience. Nowadays, we passed the 1, 2, 3 and 4 waves (ages). Before 1970 various businesses, education and training could affect and improve IT and the other modern technologies. Since we reached the 70ies the IT technologies and smartness able to change and improve other business, education, learning and even impact our lives. Three technological revolutions are shaping the dawn of the 21st century: 1) Decarbonization, 2) Decentralization and 3) Digitization. At the age of communications and technology, education as an irrefutable factor plays a vital role in the developing countries to extend their knowledge in different areas. It is an enabler for the intelligence appended to many central features of the modern world. We provide a survey of some of the major issues challenging the widespread educational management. Keywords— Internet of Things (IoT), Education, Educational Management, IoT-Education, Industry 4.0, Society 5.0, IoT Applications in Education, Educational Challenges, Tomorrow Shocks and the 5th Wave Theory INTRODUCTION Internet of Things (IoT), IoT-education, STEM, smartness and digitalization are new techniques and methods used in the industry 4.0 and Society 5.0 that enable universities, educational institutions to better management of resources and flexibility to respond to the business educational requirements and conditions. Demonstrators of the IoT has an important role, these resources consist of physical implementation of highly innovative experiments of IoT technologies. The demonstrators intend to be didactic labs where the students can make practical experiences in controlled situation but using real equipment or platforms. This learning-by-doing methodology is fundamental when dealing with a topic such as the ones studied in these projects, where practical skills are essential. The demonstrators will be remotely controlled; this will allow the students to use them event distance, ensuring a wider audience. [1,2,3]. Today, IoT application is one of leaders’ new methods considered as an appropriate enable us infrastructure. It is an enabler for the intelligence appended to many central features of the tomorrow world. This paper deals with a new theory called the 5th wave/tomorrow age toward IoT technologies as solution for educational learning management challenges with forecasting, preventing and making readiness for facing educational management by sustainability impacts. In addition, this theory creates societies and business founded on high technologies, D3 (three 21st revolutions: digitalization, decarbonization and decentralization), appropriate business strategies concerned on sustainability which can create new concept and situation of business which is capable of tackled with future concerns entitled ''Tomorrow's Society and Business'' which is a super intelligent society with smart business environment by using AI. In this paper we will discuss how the 5th wave theory and IoB could be as solutions for learning challenges in the 4th industrial revolution. APPLIED RESEARCH INTERNATIONAL CONFERENCES (ARICON) U.K 1 Fig. 1. IoT reference model I will describe how IoT is capable of recognizing challenges in edge of tomorrow and help leaders to find educational sustainable digital solutions for today’s challenges tomorrow’s crises. In this regard, I will discuss about some two case studies and analysis them as experimental fullfilment in the field of IoT in education, learning and training. The proposed paper represents IoT applications to solve educational gap on IoT technologies, Human Resources Competencies (HRC), software programming and hardware equipment. [1,2,3,4,5]. Now, we passed the 1st, 2nd, 3rd and 4th waves (ages), before the 70's various businesses could influence and improve technologies, IT, IoT and after 1970 the digital technologies, smartness, IT and IoT became able to influence, change and improve the various education, training, business, economy, and even impact our lives. At the age of ICT, training as an irrefutable factor plays a vital role in the sustainable countries to extend their knowledge to forecast, prevent and face tomorrow shocks. It may be tempting right now, when it is hard to see beyond the next few weeks, to dismiss the 17 Sustainable Development Goals (SDGs) as a distraction. But they have been described as a “crowd-sourced purchase order from the future” precisely because they offer a tremendous business opportunity. SDGs are an urgent call for action by all countries developed and developing in a global partnership. According to SDGs, goal number is regarding to: Ensure inclusive and equitable quality education and promote lifelong learning opportunities for all. [1,5,9]. Based on the author’s point of view, a new definition for sustainability is required to deal with educational challenges and sustainable development. A new concept of sustainability composing of seven pillars (7PS model) is proposed by the author, instead of the three pillars that make up the traditional notion of sustainability-environmental, economic, and social sustainability. Environmental, economic, social, cultural, educational, political, and technical sustainability are the main components of high sustainability. It is required that these seven pillars are developed fairly equally. In particular, two points are important for high sustainability theory introduced by author: The first point is that seven pillars including education, culture, social, technique, politic, economic, and environment are vital to gain high sustainability. The second point is that these seven pillars of sustainability must develop almost equally Technology development creates new opportunities for business improvement. In recent decades, IoT has been introduced as the implementation of IoT technology into distributed educational systems to optimize the efficiency of sustainability infrastructure as solution for educational and learning management challenges. [12,18]. It can also be extended to more minor levels, manufacturing and operation units. IoT based economy and business make the training procedure faster, safer with higher productivity. People, machines, natural resources, production lines, logistics networks, consumption habits, recycling flows, and virtually every other aspect of economic and social life will be linked via sensors and software to IoT plat-form, continually feeding Big Data to every node busi-nesses, homes, and vehicles moment to moment, in real time. In truth, the IoT provides a nearly endless supply of opportunities to interconnect our devices and equipment. The economic importance of IoT is underlined in several studies: to give an order of magnitude, a recent study of the European Committee estimates that the market value of the IoT in the EU will exceed one trillion euros in 2020. APPLIED RESEARCH INTERNATIONAL CONFERENCES (ARICON) U.K 2 The huge expected growth of IoT in the next years and the planned investments in the sector foresee a high demand of professionals in the sectors. According to a report from the Vision Mobile projects, while in 2014 just 300,000 developers contributed to the IoT, 4.5 mil-lion developers by 2020 are expected, reflecting a 57% compound annual growth rate and a massive market opportunity. European Universities and VET providers are not ready to face this educational challenge. Very few European Universities and VET providers offer courses on IoT nowadays. On the contrary, US Universities and private companies regularly offer courses on IoT to face the demand of professionals At the age of Internet of Things (IoT), education as an irrefutable factor plays a vital role in the developing countries to extend their knowledge in different areas. So the educational institutions’ managers have more responsibilities to survey the situation to enhance the effectiveness. People, machines, natural resources, production lines, logistics networks, consumption habits, recycling flows, and virtually every other aspect of economic and social life will be linked via sensors and software to IoT platform, continually feeding Big Data to every node businesses, homes, and vehicles moment to moment, in real time. In truth, the IoT provides a nearly endless supply of opportunities to interconnect our devices and equipment. The economic importance of IoT is underlined in several studies: to give an order of magnitude, a recent study of the European Committee estimates that the market value of the IoT in the EU will exceed one trillion euros in 2020. The European Commission is aware of the great potential of IoT and in the past has supported several projects for the development of IoT-based applications, protocols and policies for the secure, safe and privacy preserving deployment, mainly in the FP7 and Horizon 2020 programs. Actually, the EC plans to unveil a strategy for IoT for the mid of 2016, launching a series of large-scale pilots with an investment of more than 100 million euros. The huge expected growth of IoT in the next years and the planned investments ",,2020.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
d72cdf30b449b2d3474d4830751bf21b81fb3093,https://www.semanticscholar.org/paper/d72cdf30b449b2d3474d4830751bf21b81fb3093,Technology Developments for Biodiversity Monitoring and Conservation,"Over the next 5 years major advances in the development and application of numerous technologies related to computing, mobile phones, artificial intelligence (AI), and augmented reality (AR) will have a dramatic impact in biodiversity monitoring and conservation. Over a 2-week period several of us had the opportunity to meet with multiple technology experts in the Silicon Valley, California, USA to discuss trends in technology innovation, and how they could be applied to conservation science and ecology research. Here we briefly highlight some of the key points of these meetings with respect to AI and Deep Learning. Computing: Investment and rapid growth in AI and Deep Learning technologies are transforming how machines can perceive the environment. Much of this change is due to increased processing speeds of Graphics Processing Units (GPUs), which is now a billiondollar industry. Machine learning applications, such as convolutional neural networks (CNNs) run more efficiently on GPUs and are being applied to analyze visual imagery and sounds in real time. Rapid advances in CNNs that use both supervised and unsupervised learning to train the models is improving accuracy. By taking a Deep Learning approach where the base layers of the model are built upon datasets of known images and sounds (supervised learning) and later layers relying on unclassified images or sounds (unsupervised learning), dramatically improve the flexibility of CNNs in perceiving novel stimuli. The potential to have autonomous sensors gathering biodiversity data in the same way personal weather stations gather atmospheric information is close at hand. ‡ © Kelling S. This is an open access article distributed under the terms of the Creative Commons Attribution License (CC BY 4.0), which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited. Mobile Phones: The phone is the most widely used information appliance in the world. No device is on the near horizon to challenge this platform, for several key reasons. First, network access is ubiquitous in many parts of the world. Second, batteries are improving by about 20% annually, allowing for more functionality. Third, app development is a growing industry with significant investment in specializing apps for machine-learning. While GPUs are already running on phones for video streaming, there is much optimism that reduced or approximate Deep Learning models will operate on phones. These models are already working in the lab, with the biggest hurdle being power consumption and developing energy efficient applications and algorithms to run complicated AI processes will be important. It is just a matter of time before industry will have AI functionality on phones. These rapid improvements in computing and mobile phone technologies have huge implications for biodiversity monitoring, conservation science, and understanding ecological systems. Computing: AI processing of video imagery or acoustic streams create the potential to deploy autonomous sensors in the environment that will be able to detect and classify organisms to species. Further, AI processing of Earth spectral imagery has the potential to provide finer grade classification of habitats, which is essential in developing fine scale models of species distributions over broad spatial and temporal extents. Mobile Phones: increased computing functionality and more efficient batteries will allow applications to be developed that will improve an individual’s perception of the world. Already AI functionality of Merlin improves a birder’s ability to accurately identify a bird. Linking this functionality to sensor devices like specialized glasses, binoculars, or listening devises will help an individual detect and classify objects in the environment. In conclusion, computing technology is advancing at a rapid rate and soon autonomous sensors placed strategically in the environment will augment the species occurrence data gathered by humans. The mobile phone in everyone’s pocket should be thought of strategically, in how to connect people to the environment and improve their ability to gather meaningful biodiversity information.",,2018.0,10.3897/BISS.2.25833,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
f44dcc99747bb5c52b688ea9e0077f8f7c65c71e,https://www.semanticscholar.org/paper/f44dcc99747bb5c52b688ea9e0077f8f7c65c71e,Classification of Bacterial Contamination Using Image Processing and Distributed Computing,"Disease outbreaks due to contaminated food are a major concern not only for the food-processing industry but also for the public at large. Techniques for automated detection and classification of microorganisms can be a great help in preventing outbreaks and maintaining the safety of the nation's food supply. Identification and classification of foodborne pathogens using colony scatter patterns is a promising new label-free technique that utilizes image-analysis and machine-learning tools. However, the feature-extraction tools employed for this approach are computationally complex, and choosing the right combination of scatter-related features requires extensive testing with different feature combinations. In this study, we used computer clusters to speed up the feature-extraction process, which enables us to analyze the contribution of different scatter-based features to the overall classification accuracy. A set of 1000 scatter patterns representing ten different bacterial strains was used. Zernike and Chebyshev moments as well as Haralick texture features were computed from the available light-scatter patterns. The most promising features were first selected using Fisher's discriminant analysis, and subsequently a support-vector-machine classifier with a linear kernel was used. With extensive testing, we were able to identify a small subset of features that produced the desired results in terms of classification accuracy and execution speed. The use of distributed computing for scatter-pattern analysis, feature extraction, and selection provides a feasible mechanism for large-scale deployment of a light scatter-based approach to bacterial classification.",IEEE Journal of Biomedical and Health Informatics,2013.0,10.1109/TITB.2012.2222654,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
6873b431c454b9249b0533f11527022b8e13b6a3,https://www.semanticscholar.org/paper/6873b431c454b9249b0533f11527022b8e13b6a3,Expanding search in the space of empirical ML,"As researchers and practitioners of applied machine learning, we are given a set of requirements on the problem to be solved, the plausibly obtainable data, and the computational resources available. We aim to find (within those bounds) reliably useful combinations of problem, data, and algorithm. An emphasis on algorithmic or technical novelty in ML conference publications leads to exploration of one dimension of this space. Data collection and ML deployment at scale in industry settings offers an environment for exploring the others. Our conferences and reviewing criteria can better support empirical ML by soliciting and incentivizing experimentation and synthesis independent of algorithmic innovation.",ArXiv,2018.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
81866bd56c685aa8f5f74012384a0bdfa2d84794,https://www.semanticscholar.org/paper/81866bd56c685aa8f5f74012384a0bdfa2d84794,A Novel Networking Box System Architecture and Design for Data Center Energy Efficiency,"As cloud service providers (CSPs) business has been growing exponentially in the past decades, energy consumption in data center has been increasing dramatically as ever-expanding demand in the industry. The recent machine learning based Artificial Intelligence (AI) technology echoes in the ecosystem and motivates CSPs competition on accelerating data center innovation and evolution. Besides pursuing high performance computing processor in high density server system, CSPs have also iterated that networking system is playing an increasingly crucial role for overall data center energy efficiency. This paper introduces a novel networking box system deployed in Baidu self-build data center, which is sparkled with innovated “All-In-One” concept that integrates four major networking ingredients: switch, line card, control panel and management in one 6U- height (1U equal to 44.5mm) box system. Box system design, contributing to energy efficiency, is described in detail, including centralized power and cooling solution, subsystem perpendicular interconnection, on-demand system disaggregated management, networking flow optimization and etc. This paper also introduces flow optimization using probe and ECMP in the box system for Baidu Clos topology data center networking. It breaks the limitation of commodity solution (e.g. IP router, switch and etc.) where the complexity leads to stasis, inconsistent policies, inability to scale and vendor dependence. And, the solution also improves network utilization and energy efficiency for data center. Finally, engineering comparison to commodity networking solution is provided to demonstrate networking box system’s contribution to Baidu data center energy efficiency for OpEx (Operational Expenditures) and CapEx (Capital Expenditure) TCO saving.",2018 17th IEEE Intersociety Conference on Thermal and Thermomechanical Phenomena in Electronic Systems (ITherm),2018.0,10.1109/ITHERM.2018.8419480,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
9efc2ecf2efc6121c6b3489f829b74ecaa446213,https://www.semanticscholar.org/paper/9efc2ecf2efc6121c6b3489f829b74ecaa446213,"Smart Manufacturing: State-of-the-Art Review in Context of Conventional and Modern Manufacturing Process Modeling, Monitoring and Control","With the advances in automation technologies, data science, process modeling and process control, industries worldwide are at the precipice of what is described as the fourth industrial revolution (Industry 4.0). This term was coined in 2011 by the German federal government to define their strategy related to high tech industry [1], specifically multidisciplinary sciences involving physics-based process modeling, data science and machine learning, cyber-physical systems, and cloud computing coming together to drive operational excellence and support sustainable manufacturing. The boundaries between Information Technologies (I.T.) and Operation Technologies (O.T.) are quickly dissolving and the opportunities for taking lab-scale manufacturing science research to plant and enterprise wide deployment are better than ever before. There are still questions to be answered, such as those related to the future of manufacturing research and those related to meeting such demands with a highly skilled workforce. Furthermore, in this new environment it is important to understand how process modeling, monitoring, and control technologies will be transformed. The aim of the paper is to provide state-of-the-art review of Smart Manufacturing and Industry 4.0 within scope of process monitoring, modeling and control. This will be accomplished by giving comprehensive background review and discussing application of smart manufacturing framework to conventional (machining) and advanced (additive) manufacturing process case studies. By focusing on process modeling, monitoring, analytics, and control within the larger vision of Industry 4.0, this paper will provide a directed look at the efforts in these areas, and identify future research directions that would accelerate the pace of implementation in advanced manufacturing industry.",Volume 3: Manufacturing Equipment and Systems,2018.0,10.1115/MSEC2018-6658,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
88e4a3871f8c65fc9a850a5179fd73838c33acb1,https://www.semanticscholar.org/paper/88e4a3871f8c65fc9a850a5179fd73838c33acb1,Distributed Monitoring of Smart Buildings with Cloud Backend Infrastructure,"As Internet of Things applications have increased significantly over the last period in both research and industry there is an increased need for end-to-end robust demonstrators which can drive further adoption and technology developments in this field. The paper presents the application of a wireless sensor network system for distributed sensing in a smart building. The main objective has been to allow the reliable collection, storage and presentation of the collected data aimed at improving the quality of life of occupants in office environments. This includes both comfort conditions as well as dangerous situations which can appear given high concentrations of gases in the indoor setting. Special consideration has also been given to enabling scalability for integration of a large number of heterogenous sensor nodes and development of a cloud-based processing pipeline for advanced data analytics. This ranges from data visualisation and statistical indicators for dynamic thresholding towards machine learning inspired approaches, deployed within the cloud infrastructure. The key advantages and challenges of the presented solution are also discussed.","2018 5th International Conference on Control, Decision and Information Technologies (CoDIT)",2018.0,10.1109/CoDIT.2018.8394917,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
fd4df295b1dad849490b78efe9a1671436df6d09,https://www.semanticscholar.org/paper/fd4df295b1dad849490b78efe9a1671436df6d09,Blockchain outlook for deployment of IoT in distribution networks and smart homes,"Nowadays, unlike depleting fossil fuel resources, the integration of different types of renewable energy, as distributed generation sources, into power systems is accelerated and the technological development in this area is evolving at a frantic pace. Thus, inappropriate use of them will be irrecoverably detrimental. The power industry will reach a turning point in the pervasiveness of these infinite energy sources by three factors. Climate changes due to greenhouse gas accumulation in the atmosphere; increased demand for energy consumption all over the world, especially after the genesis of Bitcoin and base cryptocurrencies; and establishing a comprehensive perspective for the future of renewable energy. The increase in the pervasiveness of renewable energy sources in small-scale brings up new challenges for the power system operators to manage an abundant number of small-scale generation sources, called microsources. The current structure of banking systems is unable to handle such massive and high-frequency transactions. Thus the incorporation of cryptocurrencies is inevitable. In addition, by utilization of IoT-enabled devices, a large body of data will be produced must be securely transferred, stored, processed, and managed in order to boost the observability, controllability, and the level of autonomy of the smart power systems. Then the appropriate controlling measures must be performed through control signals in order to serve the loads in a stable, uninterruptible, reliable, and secure way. The data acquires from IoT devices must be analyzed using artificial intelligence methods such as big data techniques, data mining, machine learning, etc. with a scant delay or almost real-time. These measures are the controversial issues of modern power systems, which are yet a matter of debate. This study delves into the aforementioned challenges and opportunities, and the corresponding solutions for the incorporation of IoT and blockchain in power systems, particularly in the distribution level and residential section, are addressed. In the last section, the role of IoT in smart buildings and smart homes, especially for energy hubs schemes and the management of residential electric vehicle supply equipment is concisely discussed.",International Journal of Electrical and Computer Engineering (IJECE),2020.0,10.11591/ijece.v10i3.pp2787-2796,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
eecdaedc181574469b6b3ffc9901a891497d9ef9,https://www.semanticscholar.org/paper/eecdaedc181574469b6b3ffc9901a891497d9ef9,Hands-On Recommendation Systems with Python,"With Hands-On Recommendation Systems with Python, learn the tools and techniques required in building various kinds of powerful recommendation systems (collaborative, knowledge and content based) and deploying them to the web
Key Features
Build industry-standard recommender systems

Only familiarity with Python is required

No need to wade through complicated machine learning theory to use this book


Book Description
Recommendation systems are at the heart of almost every internet business today; from Facebook to Net?ix to Amazon. Providing good recommendations, whether it's friends, movies, or groceries, goes a long way in defining user experience and enticing your customers to use your platform.


This book shows you how to do just that. You will learn about the different kinds of recommenders used in the industry and see how to build them from scratch using Python. No need to wade through tons of machine learning theory—you'll get started with building and learning about recommenders as quickly as possible..


In this book, you will build an IMDB Top 250 clone, a content-based engine that works on movie metadata. You'll use collaborative filters to make use of customer behavior data, and a Hybrid Recommender that incorporates content based and collaborative filtering techniques

With this book, all you need to get started with building recommendation systems is a familiarity with Python, and by the time you're fnished, you will have a great grasp of how recommenders work and be in a strong position to apply the techniques that you will learn to your own problem domains.
What you will learn
Get to grips with the different kinds of recommender systems

Master data-wrangling techniques using the pandas library

Building an IMDB Top 250 Clone

Build a content based engine to recommend movies based on movie metadata

Employ data-mining techniques used in building recommenders

Build industry-standard collaborative filters using powerful algorithms

Building Hybrid Recommenders that incorporate content based and collaborative fltering


Who this book is for
If you are a Python developer and want to develop applications for social networking, news personalization or smart advertising, this is the book for you. Basic knowledge of machine learning techniques will be helpful, but not mandatory.",,2018.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
9b59a8bce4772c9228c2bf8dd7e5f237a8f73ded,https://www.semanticscholar.org/paper/9b59a8bce4772c9228c2bf8dd7e5f237a8f73ded,A Novel Method for Intelligent EMC Management Using a “Knowledge Base”,"Management of electromagnetic compatibility (EMC) is a key aspect of product manufacturing. The fierce industry competition and numerous regulations (guidelines) make EMC architects, engineers, and manufacturers increasingly anxious to apply EMC management in each phase (design, validation, development, production, deployment, and monitoring) by depending mainly on their experience. To organize the knowledge, regulations, and implementations of EMC management, this study introduces a new knowledge base with a collection of entities, their properties, and a rich set of relationships among them. Then, a machine learning-based framework for EMC management automation and intelligence is proposed. Convolutional neural network is applied to establish and classify the entities, and the knowledge-based question-answering approach is used for question answering. Finally, the EMC management plan is generated automatically. Two experiments were designed to evaluate the proposed method, and results show that both good recall and precision are achieved in the comprehensive EMC management scenarios. This method is useful for the practical EMC management of industrial products and can enhance product quality, shorten the production cycle, and reduce production cost.",IEEE Transactions on Electromagnetic Compatibility,2018.0,10.1109/TEMC.2018.2797053,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
13b2e03de3e7e305a96d495b7367a31f461da1de,https://www.semanticscholar.org/paper/13b2e03de3e7e305a96d495b7367a31f461da1de,A Support Vector Machine-based Prediction Model for Electrochemical A Support Vector Machine-based Prediction Model for Electrochemical Machining Process Machining Process,"Abstract Manufacturing of quality products is one of the core measures to address competitiveness in industries. Hence, it is always necessary to accomplish quality prediction at early stages of a manufacturing process to attain high quality products at the minimum possible cost. To achieve this goal, the past researchers have developed and investigated the applications of different intelligent techniques for their effective deployment at various stages of manufacturing processes. In this paper, support vector machine (SVM), a supervised learning system based on a novel artificial intelligence paradigm, is employed for prediction of three responses, like material removal rate, surface roughness and radial overcut during an electrochemical machining (ECM) operation. Gaussian radial basis kernel function is adopted in this algorithm to provide higher prediction accuracy. Regression analyses are also carried out to validate the effectiveness of these prediction models. The SVM-based results show good agreement between the experimental and predicted response values as compared to linear and quadratic models. Among the four ECM process parameters, i.e. applied voltage, tool feed rate, electrolyte concentration and percentage of reinforcement of B 4 C particles in the metal matrix, tool feed rate is identified having the maximum influence on the considered responses.",,,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
c1c187df21a262e448affe425250f0bb3cebe258,https://www.semanticscholar.org/paper/c1c187df21a262e448affe425250f0bb3cebe258,Service Clustering for Autonomic Clouds Using Random Forest,"Managing and optimising cloud services is one of the main challenges faced by industry and academia. A possible solution is resorting to self-management, as fostered by autonomic computing. However, the abstraction layer provided by cloud computing obfuscates several details of the provided services, which, in turn, hinders the effectiveness of autonomic managers. Data-driven approaches, particularly those relying on service clustering based on machine learning techniques, can assist the autonomic management and support decisions concerning, for example, the scheduling and deployment of services. One aspect that complicates this approach is that the information provided by the monitoring contains both continuous (e.g. CPU load) and categorical (e.g. VM instance type) data. Current approaches treat this problem in a heuristic fashion. This paper, instead, proposes an approach, which uses all kinds of data and learns in a data-driven fashion the similarities and resource usage patterns among the services. In particular, we use an unsupervised formulation of the Random Forest algorithm to calculate similarities and provide them as input to a clustering algorithm. For the sake of efficiency and meeting the dynamism requirement of autonomic clouds, our methodology consists of two steps: (i) off-line clustering and (ii) on-line prediction. Using datasets from real-world clouds, we demonstrate the superiority of our solution with respect to others and validate the accuracy of the on-line prediction. Moreover, to show the applicability of our approach, we devise a service scheduler that uses the notion of similarity among services and evaluate it in a cloud test-bed.","2015 15th IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing",2015.0,10.1109/CCGrid.2015.41,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
679568c3e2b04a65da6c18c8f222ba54d39360f7,https://www.semanticscholar.org/paper/679568c3e2b04a65da6c18c8f222ba54d39360f7,AI In Lending: Key Challenges And Practical Considerations,"Artificial intelligence, or AI, has enormous potential to enable the financial services industry to make better decisions in lending-related activities. The U.S. Treasury Department’s recent report on nonbank financials, fintech and innovation discussed the use of AI in financial services and identified certain legal challenges presented by AI and related technologies.[1] Recognizing the potential benefits of AI, the Treasury Department recommended that regulators “not impose unnecessary burdens or obstacles to the use of AI” and related machine learning technologies, but instead “provide greater regulatory clarity” to promote “further testing and responsible deployment of these technologies” by regulated financial services firms.[2]",,2018.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
83754c3a0fbeef4f2b068729b3dd39ac9b80ab57,https://www.semanticscholar.org/paper/83754c3a0fbeef4f2b068729b3dd39ac9b80ab57,AI Based Performance Benchmarking & Analysis of Big Data and Cloud Powered Applications: An in Depth View,"Big data analytics platforms on cloud are becoming mainstream technology enabling cost-effective rapid deployment of customer's Big Data applications delivering quicker insights from their data. It is, therefore, even more imperative that we have high performant platform infrastructure and application at a reasonable cost. This is only possible if we make a transition from traditional approach to execute and measure performance by adopting new AI techniques such as Machine Learning (ML) & predictive approach to performance benchmarking for every application domain. This paper proposes a high-level conceptual model for automated performance benchmarking which includes execution engine that has been designed to support a self-service model covering automated benchmarking in every application domain. The automated engine is supported by performance scaling recommendations via prescriptive analytics from real performance data set. We furthermore extended the recommendation capabilities of our self-service automated engine by introducing predictive analytics for making it more flexible in addressing 'what-if' scenarios to predict 'Right Scale' with measurement of ""Performance Cost Ratio"" (PCR). Finally, we also present some real-world industry examples which have seen the performance benefits in their applications with the recommendations given by our proposed model.",ICPE,2019.0,10.1145/3297663.3309676,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
4f625529e916944a8e8b6e1fa528714ef4c88569,https://www.semanticscholar.org/paper/4f625529e916944a8e8b6e1fa528714ef4c88569,Artificial Intelligence in Digital Agriculture. Towards In-Field Grapevine Monitoring using Non-invasive Sensors,"Agriculture seeks for a reduction of costs and environmental impact, better sustainability and to increase crop yield and quality. It is necessary to deliver useful applications for farmers and industries, to help for greater efficiency and sustainability. To achieve this in digital viticulture, useful information about the vineyard is necessary so better decisions can be taken. Advances in non-invasive sensing technologies allow the acquisition of high amounts of data from the vineyard, but these data alone are not enough to be used when decisions need to be made, it needs to be transformed into information. Artificial intelligence is a revolution at different social, work and industrial levels to deal with data. Within artificial intelligence, machine learning has evolved greatly during the last decades providing tools to make computers learn, and these algorithms are used in many different fields due to their high versatility for many data-related tasks, generating knowledge and information, and improving the decision-making process. Therefore, the combination of non-invasive sensors and artificial intelligence needs to be explored to meet the requirements needed to apply digital agriculture, the data-driven agriculture. 
The main objective of this PhD Thesis is the combination of machine learning and non-invasive sensing technologies for the assessment of relevant agronomical, physiological and qualitative traits in digital agriculture and viticulture. Specifically, the following objectives have been pursued: i) to make use of different machine learning algorithms on data from spectroscopy for in-field grapevine phenotyping and monitoring; ii) the application of ensemble data analysis techniques for vineyard water status assessment with thermal imaging; and iii) to deploy hyperspectral imaging in the field, supported by intensive machine learning combinations, for the monitoring of different crop traits. The first objective, covered in Chapter 3, was the combination of machine learning algorithms and near-infrared spectroscopy for vineyard monitoring and phenotyping. A handheld spectrometer was used for two goals: the classification of grapevine varieties, from several vineyard plots and vintages; and water status assessment, using the same spectral signal. Accurate models were developed for both goals. The results allow to open new ways in digital viticulture for the quick grapevine phenotyping under field conditions, an useful tool for several actors in the wine industry. 
The application of ensemble machine learning algorithms to in-field thermal images acquired on-the-go for vineyard water status monitoring, the second objective, is addressed in Chapter 4. A thermal camera was mounted on an all-terrain vehicle for continuous acquisition. A combination of rotation forests and decision trees was used for the training of prediction models. The outcomes provided by the machine learning algorithms support the use of thermal imaging for fast, reliable estimation of a vineyard water status, even suppressing the necessity of supervised acquisition of reference temperatures. The new developed on-the-go method can be very useful in the grape and wine industry for assessing and mapping vineyard water status. 
The last objective was the use of on-the-go hyperspectral imaging under field conditions, modelled with machine learning techniques, and it is discussed in Chapter 5. Hyperspectral imaging is a powerful technology, but it has been classically used under laboratory conditions. Very few attempts on in-field hyperspectral imaging have been reported in the literature, due to the difficulties, like natural, irregular illumination or unknown a priori sample positioning in the recorded scene, that it is necessary to face. For this reason, a considerable amount of the work developed in this PhD Thesis has been focused on surpassing the challenges that come from deploying a hyperspectral camera in the field for the on-the-go vineyard monitoring. Also, as hyperspectral imaging involves the management of a high amount of data, advanced machine learning algorithms become appealing to be applied in this scenario. Three different applications were developed: varietal classification, grape composition assessment and yield estimation. In all of them, it was designed a mechanism for the automated identification of the different grapevine organs. Potent models were obtained for the monitoring of different key viticulture and agriculture parameters. The results suggest that machine learning and hyperspectral imaging can be used to accurately estimate several traits in vineyards and other crops, becoming a powerful and accurate tool in the decision making process. 
The results from the research work carried out in this PhD Thesis, also published in several scientific articles, demonstrated that artificial intelligence techniques are able to exploit the potential of data acquired using non-invasive sensing technologies for the monitoring and phenotyping of key crop traits. This can be of utmost importance in digital agriculture and viticulture as new solutions can be developed as decision support tools.",,2019.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
40f45033b18670b32698b050e93621d9f2044ec9,https://www.semanticscholar.org/paper/40f45033b18670b32698b050e93621d9f2044ec9,Business opportunities in 5G mobile technology,"The question of how actors perceive business opportunities has puzzled both researchers and practitioners for decades. In the era of artificial intelligence, machine learning, and the Internet of things, many actors of the technology-intensive industries question how to use new technology to create value, and how to monetize new service concepts. In this paper, we focus on the next mobile communications technology, 5G, as one of the potential value-creators for the future that holds business opportunities for its utilizers and deployers. The concept of business opportunities is strongly associated with research on entrepreneurship (cf. Carlsson et al., 2003). Entrepreneurial opportunities consist of a set of ideas, beliefs, and actions that enable the introduction of goods, services, raw materials, and organizing methods in the absence of current markets for them (Sarasvathy et al., 2003). The research stream of entrepreneurial opportunities (cf. Alvarez & Barney, 2007, 2010; Dimov, 2007, 2011; Eckhardt & Shane, 2003) can offer new insights into the development of opportunities in high technologyintensive fields, and especially as regards the development of 5G. Strategies for opportunity identification, exploitation, and value creation are vital in the 5G era, as non-ICT traditional business sectors begin to deploy wireless technologies (e.g., factories, automotive, etc.). Researchers expect that 5G will change the business models and business ecosystems; it will also better address the evolving needs of customers (cf. Kliks et al., 2018). Unlike already existing mobile communications systems, 5G allows integration of vertical industries, e.g., energy, media, health, factories, and the automotive industry (5G-PPP, 2016). Thus, specialized companies will be able to provide services and establish positions in the value chains and actor networks. This is a major transformation compared to an environment dominated by bilateral relationships between mobile operators and their customers...",,2019.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
c720b7dbbc679d1083aacd49e397ec167f81a1e6,https://www.semanticscholar.org/paper/c720b7dbbc679d1083aacd49e397ec167f81a1e6,Privacy-Preserving Scoring of Tree Ensembles: A Novel Framework for AI in Healthcare,"Machine Learning (ML) techniques now impact a wide variety of domains. Highly regulated industries such as healthcare and finance have stringent compliance and data governance policies around data sharing. Advances in secure multiparty computation (SMC) for privacy-preserving machine learning (PPML) can help transform these regulated industries by allowing ML computations over encrypted data with personally identifiable information (PII). Yet very little of SMC-based PPML has been put into practice so far. In this paper we present the very first framework for privacy-preserving classification of tree ensembles with application in healthcare. We first describe the underlying cryptographic protocols that enable a healthcare organization to send encrypted data securely to a ML scoring service and obtain encrypted class labels without the scoring service actually seeing that input in the clear. We then describe the deployment challenges we solved to integrate these protocols in a cloud based scalable risk-prediction platform with multiple ML models for healthcare AI. Included are system internals, and evaluations of our deployment for supporting physicians to drive better clinical outcomes in an accurate, scalable, and provably secure manner. To the best of our knowledge, this is the first such applied framework with SMC-based privacy-preserving machine learning for healthcare.",2018 IEEE International Conference on Big Data (Big Data),2018.0,10.1109/BigData.2018.8622627,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
802fdb1252dddefa7db7c3ceba49d7274e11426c,https://www.semanticscholar.org/paper/802fdb1252dddefa7db7c3ceba49d7274e11426c,An Architecture for and Fast and General Data Processing on Large Clusters,"Today, a myriad data sources, from the Internet to business operations to scientific instruments, produce large and valuable data streams. However, the processing capabilities of single machines have not kept up with the size of data. As a result, organizations increasingly need to scale out these computations to clusters of hundreds of machines. 
 
At the same time, the speed and sophistication required of data processing have grown. In addition to simple queries, complex algorithms like machine learning and graph analysis are becoming common. And in addition to batch processing, streaming analysis of real-time data is required to let organizations take timely action. Future computing platforms will need to not only scale out traditional workloads, but support these new applications too. 
 
This book, a revised version of the 2014 ACM Dissertation Award winning dissertation, proposes an architecture for cluster computing systems that can tackle emerging data processing workloads at scale. Whereas early cluster computing systems, like MapReduce, handled batch processing, our architecture also enables streaming and interactive queries, while keeping MapReduce's scalability and fault tolerance. And whereas most deployed systems only support simple one-pass computations (e.g., SQL queries), ours also extends to the multi-pass algorithms required for complex analytics like machine learning. Finally, unlike the specialized systems proposed for some of these workloads, our architecture allows these computations to be combined, enabling rich new applications that intermix, for example, streaming and batch processing. 
 
We achieve these results through a simple extension to MapReduce that adds primitives for data sharing, called Resilient Distributed Datasets (RDDs). We show that this is enough to capture a wide range of workloads. We implement RDDs in the open source Spark system, which we evaluate using synthetic and real workloads. Spark matches or exceeds the performance of specialized systems in many domains, while offering stronger fault tolerance properties and allowing these workloads to be combined. Finally, we examine the generality of RDDs from both a theoretical modeling perspective and a systems perspective. 
 
This version of the dissertation makes corrections throughout the text and adds a new section on the evolution of Apache Spark in industry since 2014. In addition, editing, formatting, drawing of illustrations, and links for the references have been added.",,2016.0,10.1145/2886107,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
1b985858ef2a888268bac0d6d07c3947254cbf2e,https://www.semanticscholar.org/paper/1b985858ef2a888268bac0d6d07c3947254cbf2e,How Big Data Analytics Can Help Future Regulatory Issues in Carbon Capture and Sequestration CCS Projects,"
 In this age of data, there is a significant need for tracking and prediction of non-compliance of rules & regulations in various industries including the oil & gas sector. In this paper, we will be reviewing some of the anticipated regulatory issues in commercial implementations of carbon capture & sequestration (CCS) and discuss how machine learning and big data analytics can diminish future non-compliance incidents.
 With the rising awareness of advanced data-driven technologies such as ""Big Data Analytics"" and ""Machine Learning"", a contemporary approach to regulation and compliance is developing. This emerging approach, called ""Algorithmic Regulation"", defines an alternative framework for systematic collection of data (real-time or near real-time) and continuous generation of knowledge through intelligent computational algorithms in order to regulate a domain of activities.
 In this study, we will look at some of the major data management challenges in the pilot CCS operations with regards to rules and regulations. We will then discuss how an algorithmic regulatory framework can help in conducting CCS operations in a manner that are compliant with environmental, safety and health policies and regulations.
 Field operators collect a lot of data which needs to be formatted and modelled in a fashion acceptable to understand the operator's compliance with regulation. Generally, such compliance qualification criteria are verified using human intellect and basic querying software. In other industries, the idea of converting rules & regulations in a format understandable by machines is gaining momentum and great strides have been taken. The technological progresses made possible by data-driven analytical techniques can create a paradigm shift in the way rules and regulations are designed and implemented. Large-scale deployment of CCS projects is bound to bring with it a number of regulatory issues, making it a necessity to proactively explore and address the anticipated issues. These technologies can equip regulated entities as well as regulators with advanced tools for managing complexity in CCS projects. These improved solutions will help companies to better meet the regulatory data collection, reporting and governance requirements in large scale CCS operations.
 This paper looks into advanced data management and modeling techniques like ""algorithmic regulations"" to increase compliance in carbon capture and sequestration projects. The concept of handling rules and regulations in the form of big data will change the outlook and compliance management will become increasingly more agile and responsive.","Day 3 Thu, April 25, 2019",2019.0,10.2118/195284-MS,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
88209f99007b88f47dd34bc2063c2c478c7092bd,https://www.semanticscholar.org/paper/88209f99007b88f47dd34bc2063c2c478c7092bd,Metodología de análisis y segmentación de clientes usando secuencias de comportamiento,"Traditional marketing analytic and segmentation models are constrained for grouping and 
describing the new consumer's omnichannel behavior. 
The brand-new approach provided by this Customer analysis and segmentation methodology 
based on Behavioral sequences, covers: 
Nicolas M. Casariego Sarasquete Master Universitario en Visual Analytics y Big Data 
Metodologia de analisis y segmentacion de clientes usando secuencias de comportamiento 3 
• Formal representation and storage structure, 
• Algorithms, 
• Machine learning based analytics, 
• Visualization and 
• Behavioral based segment creation. 
The aim of the methodology is to be applied in different industries and business sectors, and 
enable to segment and analyze customers, citizens and patients. 
The methodology is deployed in two completely different business scenarios to demonstrate 
the practical capabilities of its application. 
The results show that the methodology complements the traditional approach with additional 
capabilities to segment and analyze customer’s behavior, enabling customer value, loyalty and 
experience improvement.",,2019.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
edfe5dd6fd8170ec12b5aec875b8b67ab4b97dc2,https://www.semanticscholar.org/paper/edfe5dd6fd8170ec12b5aec875b8b67ab4b97dc2,Special Issue on Cyber Security and AI,"We are facing a big data world, embedded with interconnected IoT (Internet of Things) devices that generate large volumes of data. They pose a significant challenge to academia and industries focused on digital security: A variety of new malware and other threats are emerging at a fast pace, and existing preventive methods are struggling to deal with them within the golden time by depending solely on the known attack signature. Recently, there has been considerable advancement in computing, particularly in the field of Artificial Intelligence (AI). Advanced technologies such as Machine Learning and Deep Learning are being actively deployed in cyber security, and new results and issues have been reported. In this special issue, we have selected five publications that represent the state‐of‐the‐art research in AI‐based cyber security ranging from theory to practice. The first paper, entitled ""Supervised learning‐based DDoS attacks detection: Tuning hyperparameters"" by Meejoung Kim investigates and analyzes Distributed Denial‐of‐Service (DDoS) attacks using Machine Learning‐based approaches. She employs two different supervised Machine Learning‐ based Algorithms, namely Basic Neural Network and LSTM RNN. Her study focuses on four aspects ‐ 1) how various data preprocessing methods and values of hyperparameters can affect the performance of Machine Learning‐based techniques, 2) which suboptimal values of hyperparameters enable feature extraction algorithms to work efficiently, 3) whether learning from the former traffic and a particular dataset can affect the sequential traffic and another dataset, respectively, especially in case of DDoS attacks, and 4) whether Machine Learning algorithms crafted for older attacks can be reused to detect recent attacks. Her experiments incorporate three scenarios: 1) using mixed CAIDA and DARPA datasets, 2) using CAIDA dataset for training and DARPA dataset for testing and 3) using the more recent dataset for both training and testing. Besides this, she also considers different environments with different hyperparameter values, including but not limited to optimizers, learning rates, number of layers, hidden nodes, number of epochs, etc. The key contributions of her study are: 1) an investigation of the joint effects of hyperparameters and preprocessing methods on the performance of Machine Learning‐based techniques, 2) investigation of the effects of learning, previously conducted using former traffic and a particular dataset, on sequential traffic and a different dataset, 3) appraisal of the applicability of a Machine Learning‐based algorithm, trained on old features, in the detection of DDoS attacks with new features, and 4) comparison of two optimizers, namely Gradient Descent and Adam, used in the Machine Learning‐based models to detect DDoS attacks. The next paper by Ili Ko, Desmond Chambers and Enda Barrett, entitled ""Unsupervised learning with hierarchical feature selection for DDoS mitigation within the ISP domain"" presents a DDoS attack mitigation framework. Their proposed framework is an improved version of the previous system they had designed. The newly proposed framework comprises four improvements ‐ 1) it has an extended horizontal expansion process for global feature extraction to enhance the separability of data, 2) it incorporates two logical controllers to avoid incorrect map generation by the system, 3) it consists of one additional layer of SOM, and 4) it is equipped with an additional model to select appropriate sets of features for each layer of SOM. Further, they have also underlined some of the challenges that they faced while conducting their research. These include 1) reducing the size of the dataset without losing relevant information, 2) extraction of significant features without consuming too many resources, and 3) difficulty in evaluating the performance of the newly proposed framework on unlabeled datasets. They have made three main contributions to the field via this research. First, they leveraged the data collected by an ISP, thereby enhancing data separability to improve the performance of the system. Second, they have proposed a two‐layered SOM DDoS attack mitigation model deployed at an ISP domain. This model is capable of recognizing normal traffic and mitigating DDoS attacks more efficiently compared to their previously presented model. Third, their approach is not only capable of identifying 99% of the malicious traffic but also enables ISPs to resist",ETRI Journal,2019.0,10.4218/etr2.12236,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
f3dd7d67ac4d7d6843f326ac510d49926a7af934,https://www.semanticscholar.org/paper/f3dd7d67ac4d7d6843f326ac510d49926a7af934,Social Network Mining for Analysis of Social Phenomena,"In the last decade, a large amount of human interaction data has become available that either comes from online social networks or was captured using wearable devices. Traditional social science does not provide tools for conducting data-driven research using such data. Computational social science aims to close this gap by developing data mining and machine learning approaches for utilizing this data. Toward this end, in this thesis we explore different ways of studying social phenomena by applying data mining techniques to social network data. We use two different types of data: The first type is face-to-face interaction networks data obtained using novel wearable radio-frequency identification (RFID) technology, which represents high-resolution social networks. These networks allow researchers to study human interaction and social phenomena, such as community formation, on a micro level. The second type of data is the data available from online social networks and media. These data enable the study of social phenomena on a large scale, such as the study of changes in human mobility during disasters. It is important to consider both data types, as only in this way may we present a complete picture of the process of mining social network for analysis of social phenomena. We structure this work following the cross-industry standard process for data mining (CRISP-DM) framework. First, in CRISP-DM’s data understanding phase, we aim to analyze face-to-face interaction networks and especially the process of community formation in those networks. We also validate whether online social networks can be used as a proxy for offline activities. The CRISP-DM framework’s subsequent data preparation, modeling and evaluation phases are presented in three different types of studies. In this manner, we demonstrate the scope of what is possible in using social network mining to help us understand social phenomena and activities. The first study, conducted in collaboration with sociologists, shows how analysis of face-to-face interactions may enhance sociological studies. In the second study, which was carried out together with the United Nations Pulse Lab Jakarta, we show how forest fires are discussed in social media and how changes in the mobility of populations can be observed using online social media data. In the third study, we propose an improvement to the k-nearest neighbors classification algorithm for data with non-uniform geospatial distribution, a characteristic often observed in data from online social networks. These three studies show different possibilities when studying social phenomena and developing applications based on social network data. Finally, when discussing the deployment phase of the CRISP-DM framework, we present the Ubicon platform, which we used to collect data and to deploy some of our results. Altogether, this thesis shows different approaches for exploring social phenomena using data mining and machine learning techniques applied to social network data. The presented approaches have different potential use cases, from practical applications to more theoretical sociological work. The insights presented in this thesis are relevant for researchers from different disciplines, including computer science and sociology, interested in studying and developing methods to work with social network data.",,2019.0,10.17170/KOBRA-20190815628,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
0321ed628bb037b9fe6a1ba536e71d7bcf535971,https://www.semanticscholar.org/paper/0321ed628bb037b9fe6a1ba536e71d7bcf535971,Dynamic Data security for Hadoop Systems using Fuzzy Adaptive Security Profiles (FASP),"Hadoop is an efficient parallel processing platform with ability to handle large volumes of data termed as big data. Ability to handle large volume of data using parallel processing means has made Hadoop widely adapted by various academics and Industries. Hadoop was not designed with security considerations during initial times and as Hadoop got deployed in public clouds; its security vulnerabilities became important. Towards this end, a Fuzzy Adaptive Security Profiles (FASP) is proposed in this work. The FASP is able to provide dynamic security profiles adaptive to the security and processing requirements. The system ensures data security for the HDFS with machine learning based attack detection.",2019 1st International Conference on Advances in Information Technology (ICAIT),2019.0,10.1109/ICAIT47043.2019.8987332,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
4179a3dec237ca6b7a8174614be6451cba03dc32,https://www.semanticscholar.org/paper/4179a3dec237ca6b7a8174614be6451cba03dc32,An Innovative Prediction Technique to Detect Pedestrian Crossing Using ARELM Technique,,,2019.0,10.1007/978-3-030-35139-7_6,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
a59b6d38c1b473292a7a20dd8fe0be1390bfdae0,https://www.semanticscholar.org/paper/a59b6d38c1b473292a7a20dd8fe0be1390bfdae0,Performance Modelling of Partially Replicated In-Memory Transactional Stores,"This paper presents PROMPT, a PeRfOrmance Model for Partially replicated in-memory Transactional cloud stores. PROMPT combines white box Analytical Modelling and Machine Learning techniques, with the goal of achieving the best of the two methodologies: low training times, high extrapolation power, and portability across heterogeneous cloud infrastructures. We validate PROMPT via an extensive experimental study based on a popular open-source transactional in-memory data store (Red Hat's Infinispan), industry-standard benchmarks, and deployments on both public and private cloud infrastructures.","2014 IEEE 22nd International Symposium on Modelling, Analysis & Simulation of Computer and Telecommunication Systems",2014.0,10.1109/MASCOTS.2014.41,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
3b9ce2ee4028f5faea850e6e0cedb06b68b57ce4,https://www.semanticscholar.org/paper/3b9ce2ee4028f5faea850e6e0cedb06b68b57ce4,LEARNING THROUGH PRACTICE VIA ROLE-PLAYING: LESSONS LEARNT,"Software engineering is the establishment and use of sound engineering principles in order to obtain economically software that is reliable and works efficiently on real machine. Sound software engineering closely related with socio-technical activity that depends on several human issues which are communication, collaboration, motivation, work environment, team harmony, engagement, training and education. These issues affect everything for students to fully understand software engineering and be prepared for software development careers. Therefore courses offered in the university must also consider the sociological and communication aspects, often called the socio-technical aspects. One popular method is to use role-playing exercises. Role-playing is a less technologically elaborate form of simulation for learning interpersonal skills and is analogous to rehearsal. It is particularly helpful when students are having difficulties to relate lessons learnt in the university to the applicability of the knowledge in the real implementation. This is because many students view software engineering as meaningless bureaucracy and have little interest in the knowledge delivered in the lecture hall. This scenario impedes the expansion of current knowledge and inhibits the possibility of knowledge exploration to solve range of industry problems. Simply lecturing about software engineering will never engage students or convince them that software engineering has value. Given this student bias, the goal of teaching software engineering often becomes convincing students that it has value. To achieve this, students need to experience firsthand the sociological and communication difficulties associated with developing software systems. In this paper, we argue that in teaching software engineering we must cover two essential things; delivery of knowledge and skills required in the software engineering domain in a form of lecture and hands-on practice to experience the value of the knowledge and skills learnt. We report on our experiences gained in deploying role-playing in master degree program. Role-playing is used as pedagogical tool to give students a greater appreciation of the range of issues and problems associated with software engineering in real settings. We believe that the lessons learnt from this exercise will be valuable for those interested in advancing software engineering education and training.",,2012.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
1139d18c23c5d438488330e4f582ec67757dea15,https://www.semanticscholar.org/paper/1139d18c23c5d438488330e4f582ec67757dea15,Coflow: A Networking Abstraction for Distributed Data-Parallel Applications,"Over the past decade, the confluence of an unprecedented growth in data volumes and the rapid rise of cloud computing has fundamentally transformed systems software and corresponding infrastructure. To deal with massive datasets, more and more applications today are scaling out to large datacenters. These distributed data-parallel applications run on tens to thousands of machines in parallel to exploit I/O parallelism, and they enable a wide variety of use cases, including interactive analysis, SQL queries, machine learning, and graph processing. Communication between the distributed computation tasks of these applications often result in massive data transfers over the network. Consequently, concentrated efforts in both industry and academia have gone into building high-capacity, low-latency datacenter networks at scale. At the same time, researchers and practitioners have proposed a wide variety of solutions to minimize flow completion times or to ensure per-flow fairness based on the point-to-point flow abstraction that forms the basis of the TCP/IP stack. We observe that despite rapid innovations in both applications and infrastructure, application- and network-level goals are moving further apart. Data-parallel applications care about all their flows, but today’s networks treat each point-to-point flow independently. This fundamental mismatch has resulted in complex point solutions for application developers, a myriad of configuration options for end users, and an overall loss of performance. The key contribution of this dissertation is bridging this gap between application-level performance and network-level optimizations through the coflow abstraction. Each multipoint-to-multipoint coflow represents a collection of flows with a common application-level performance objective, enabling application-aware decision making in the network. We describe complete solutions including architectures, algorithms, and implementations that apply coflows to multiple scenarios using central coordination, and we demonstrate through large-scale cloud deployments and trace-driven simulations that simply knowing how flows relate to each other is enough for better network scheduling, meeting more deadlines, and providing higher performance isolation than what is otherwise possible using today’s application-agnostic solutions. In addition to performance improvements, coflows allow us to consolidate communication optimizations across multiple applications, simplifying software development and relieving end users from parameter tuning. On the theoretical front, we discover and characterize for the first time the concurrent open shop scheduling with coupled resources family of problems. Because any flow is also a coflow with just one flow, coflows and coflow-based solutions presented in this dissertation generalize a large body of work in both networking and scheduling literatures.",,2015.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
eb01ea5249c465e4cd08a1ee9520971924798b48,https://www.semanticscholar.org/paper/eb01ea5249c465e4cd08a1ee9520971924798b48,TensorFlow Based Website Click through Rate (CTR) Prediction Using Heat maps,"Web Heat Maps are used to identify the click patterns and activities visited by the users of the website. Using heat maps, one can make a manual decision based on the user's click activity. This paper proposes a framework using TensorFlow to identify and detect the users click activity in real time. Tensor Flow also suggest or take business decisions predicted through users clicks. This paper models Tensor Flow's machine learning library to take automated decisions like placement of suitable products, placement of advertisements and others based on the highest clicks recorded by the users. The results predict that the future businesses like e-commerce, fashion and retail industry can benefit more if this framework is deployed in such applications.",2018 International Conference on Recent Trends in Advance Computing (ICRTAC),2018.0,10.1109/ICRTAC.2018.8679129,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
5b0ffc40f91580ec9b5cff2b2e27b51b6f1a6c24,https://www.semanticscholar.org/paper/5b0ffc40f91580ec9b5cff2b2e27b51b6f1a6c24,Lawrence Berkeley National Laboratory Recent Work Title Accelerating the discovery of materials for clean energy in the era of smart automation Permalink,"| The discovery and development of novel materials in the field of energy are essential to accelerate the transition to a lowcarbon economy. Bringing recent technological innovations in automation, robotics and computer science together with current approaches in chemistry , materials synthesis and characterization will act as a catalyst for revolutionizing traditional research and development in both industry and academia. This Perspective provides a vision for an integrated artificial intelligence approach towards autonomous materials discovery , which, in our opinion, will emerge within the next 5 to 10 years. The approach we discuss requires the integration of the following tools, which have already seen substantial development to date: highthroughput virtual screening, automated synthesis planning, automated laboratories and machine learning algorithms. In addition to reducing the time to deployment of new materials by an order of magnitude, this integrated approach is expected to lower the cost associated with the initial discovery. Thus, the price of the final products (for example, solar panels, batteries and electric vehicles) will also decrease. This in turn will enable industries and governments to meet more ambitious targets in terms of reducing greenhouse gas emissions at a faster pace. volume 3 | mAY 2018 | 5 PERSPECTIVES © 2018 Macmillan Publishers Limited, part of Springer Nature. All rights reserved. NAture reviews | MAtEriAlS First, we briefly discuss advances in AI and then provide an overview of the main applications of materials for the clean energy sector. Next, we explore stateof-theart automated procedures for materials discovery, with a focus on machine learning. The field of organic materials is the farthest along in many of the areas required for an integrated platform, but along the way, we point out some of the notable advances in both inorganic materials and nanomaterials. Finally, we conclude and provide our vision for the next generation of integrated AI approaches towards autonomous materials discovery, which will emerge within the next 5–10 years. Advances in AI Scientific discoveries are usually associated with an insight: the act of intuitively seeing a phenomenon, which contrasts with systematic mechanistic learning. This is despite the fact that most of our discoveries are based on extensive preliminary studies. Insight is considered to be an exclusively human attribute, whereas systematic exploration is connected with automated platforms. However, this gap between a creative and intuitive targeted search and a systematic exploration continues to narrow as automated platforms become increasingly sophisticated and are able to process more complex information17,18. In addition to systematic screenings of large databases and building chemical structures according to a set of preprogramed rules, today's platforms can update the rules for analysing the available information and even search for more information that helps them to make specific decisions19. In this case, it is natural to expect that in the very near future such platforms will be able to not only predict the properties of materials but also test hypotheses by designing structures and characterizing them, becoming autonomous. It has already been demonstrated that combinatorial optimization procedures provide faster screening of the molecular space than traditional approaches based on intuition20. The pharmaceutical and chemical industries, as well as academic research environments, use these methods for the design of new molecules, reactions and materials21–26. However, combinatorial chemical synthesis makes an exhaustive search of the multidimensional molecular space out of reach26. As such, the community needs a more rational approach for exploration of this large space; this is where machine learning comes into play. The recent progress in machine learning and statistical inference methods can be viewed as a revolution in AI. Most machine learning methods, such as neural networks and Bayesian optimization, were developed decades ago but have not found widespread use until recently27,28. Basic research in AI continues to be backed by governments, industry, and public and private research institutes29. Today, machine learning methods are behind many commercial applications, such as Internet searches, natural language translation, and image and speech recognition. Recently, an upgraded version of AlphaGo, the Goplaying program from Google, which in part uses deep neural networks (DNNs) and reinforcement learning as key algorithms, beat the top human Go player30. Moreover, its playing style inspired other Go players. Since mid-2017, Cisco has used a multilayer supervised algorithm based on machine learning to analyse encrypted traffic (that is, HTTPS). This algorithm helps identify malware communication through passive monitoring and yields enhanced incident responses31. In economics, machine learning has also started to emerge, notably to predict economic growth32, to quantify predictive performance33 and to anticipate customer behaviour34. While recent progress is making its mark in nonscientific endeavours, the application of machine learning in science and medicine is also emerging: assisting physicians in interpreting computerbased medical images, processing biomedical signals and learning from patient data35–37. In late 2017, a DNN was successfully shown to enable the reconstruction of perceptual and subjective images from the activity of human brains38. Within the context of this Perspective, the application of several machine learning methods to computational chemistry has recently bourgeoned39–41. From the representation of aromaticity and conjugation in general42 to the prediction of protein–ligand affinities43, there is increasing interest in using DNNs and convolutional neural networks in a wide range of applications. Hybrid learning models, which combine different approaches to leverage their respective strengths, have shown great promise. Examples of hybrid learning models include Bayesian deep learning44, Bayesian conditional generative adversarial networks45 and deep Bayesian optimization46. The latter has been successfully applied to reverse engineer chemical reactions to quantitatively and qualitatively reproduce observed behaviours46. Machinelearning-based algorithms have also been intensively used to bypass expensive static47 and dynamic48–50 ab initio electronic structure calculations. Although exploration and discovery are more challenging than interpolation or optimization for AI, recent algorithmic developments show substantial promise for making advances in these areas. To overcome these challenges of inverse design in computational chemistry and to explore the openended chemical space, autoencoders and generative adversarial networks have emerged as powerful tools to generate novel molecular structures with desirable properties tailored to specific needs51–55. This progress is only the beginning of the integrated materials discovery revolution. The key component of an autonomous discovery approach lies in the synergy between machine learning and robotics. One might ask, “Why are robots better?” A cursory analysis of humans versus robots shows some clear advantages for the latter. First, robots can operate in more adverse conditions. This can be seen even outside chemistry; for example, robots were sent to Mars decades before humans. For chemistry, this ability can translate to procedures that are subject to high temperatures and/or pressures, toxic solvents and highly exothermic processes. Robots also excel at providing unbiased and reproducible routes towards materials discovery. For example, it was recently demonstrated that not only do machinelearning-based algorithms cover an application space of polyoxometalates approximately six times larger than a human approach but they also increase the accuracy of prediction by a relative value of 6.9%56. Recently, an autonomous infrastructure for the optimization of chemical reaction conditions through the Deep Reaction Optimizer (DRO), an algorithm based on deep reinforcement learning, was reported57. Furthermore, robots are better at recording reaction procedures independently of their outcome and reduce waste by rigorously following the stoichiometry of the experiments18. Robots also provide a natural platform for scaling chemical experiments, reducing the cost per experiment. In 2009, a hypotheticodeductive ‘robot scientist’, named Adam, was developed that could autonomously perform experiments, devise hypotheses and design experiments to validate the hypotheses in the area of functional genomics58. 6 | mAY 2018 | volume 3 © 2018 Macmillan Publishers Limited, part of Springer Nature. All rights reserved. www.nature.com/natrevmats P e r s P e c t i v e s",,2018.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
e70764052c7e2a9eb030528b81b2591f1ec60928,https://www.semanticscholar.org/paper/e70764052c7e2a9eb030528b81b2591f1ec60928,Clustering Nature of Base Station and Traffic Demand in Cellular Networks and the Corresponding Caching and Multicast Strategies,"Traditional cellular networks have evolved from the first generation of analog communications 
to the current fourth generation of digital communications where iteratively enhanced physical 
layer technologies have greatly increased the network capacity. According to Shannon’s theory, 
the technical gains brought by physical layer has gradually become saturated, which cannot 
match the rapid increase of user traffic demand in current mobile internet era, thus calls for 
another path of evolution, i.e., digging into the traffic demand of mobile users. In recent 
years, the academic communities have begun to use the real data to analyze the infrastructure 
deployment of wireless networks and the traffic demand of mobile users, in order to make 
benefits from the underlying statistical patterns. At the same time, along with the recent rise of 
machine learning technics, data-driven service is considered as the next economic growth point. 
Thus the industry is putting more and more attention on data accumulation and knowledge 
mining related services and telecommunication operators are coming to realize the increasing 
importance of the recorded data from their own networks. Therefore, the real-data-driven 
technology advancement is considered as a promising direction for the next evolution of cellular 
networks.",,2018.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
b112533294a1125b3e8d6beb02dc639bf46f9bc9,https://www.semanticscholar.org/paper/b112533294a1125b3e8d6beb02dc639bf46f9bc9,Modeling and fault diagnosis of broken rotor bar faults in induction motors,"Due to vast industrial applications, induction motors are often referred to as the “workhorse” of the industry. To detect incipient faults and improve reliability, condition monitoring and fault diagnosis of induction motors are very important. In this thesis, the focus is to model and detect broken rotor bar (BRB) faults in induction motors through the finite element analysis and machine learning approach. 
The most successfully deployed method for the BRB fault detection is Motor Current Signature Analysis (MSCA) due to its non-invasive, easy to implement, lower cost, reliable and effective nature. However, MSCA has its own limitations. To overcome such limitations, fault diagnosis using machine learning attracts more research interests lately. Feature selection is an important part of machine learning techniques. 
The main contributions of the thesis include: 1) model a healthy motor and a motor with different number of BRBs using finite element analysis software ANSYS; 2) analyze BRB faults of induction motors using various spectral analysis algorithms (parametric and non-parametric) by processing stator current signals obtained from the finite element analysis; 3) conduct feature selection and classification of BRB faults using support vector machine (SVM) and artificial neural network (ANN); 4) analyze neighbouring and spaced BRB faults using Burg and Welch PSD analysis.",,2018.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
3ad95c7857c53ded9ace3f015f6e52d20b0e537b,https://www.semanticscholar.org/paper/3ad95c7857c53ded9ace3f015f6e52d20b0e537b,Predicting motion sickness for crew transfer vessels using modelling and date mining techniques,"In recent years, the offshore wind industry has grown significantly. The wind turbines are constructed in wind farms, and are serviced with relatively small crew transfer vessels. These vessels transport repair crews from the mainland to the farms and back within a day. One of the big challenges is that these transits can cause motion sickness. If one crew member gets sick, the vessel is legally required to return to the harbour, without performing the repair. It is of interest to predict motion sickness, since both a failed repair and a broken wind turbines that could have been repaired are costly. This can be done by predicting the motion sickness incidence (MSI), which is an objective measure for motion sickness. The goal of this thesis is to predict MSI using the wave conditions measured in or near the wind farms, by means of an analytical and numerical approach using machine learning. Testing is done on two sites: the Greater Gabbard wind farm and the Westermost Rough wind farm. The analytical approach relates the recorded wave spectrum to the motion spectrum of the vessel, but proves infeasible as a stand-alone method. Gaussian regression, tree ensemble regression and neural networks with Bayesian regularization and backpropagation are the best performing machine learning techniques for MSI prediction. We conclude that the methods from this thesis have sufficient accuracy, by comparing our results with those from Orsted, whose model is already successfully deployed.",,2018.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
c103601dbf79c05c7f72b865ce05e6f82048c1ca,https://www.semanticscholar.org/paper/c103601dbf79c05c7f72b865ce05e6f82048c1ca,Towards a Framework for Ethical Audits of AI Algorithms,"In recent years there has been much talk about the advancements in artificial intelligence (AI). Large strides have been made particularly in the area of machines learning algorithms and their application. This research investigates the premise that our AI algorithms have flaws, may be biased, or perhaps are simply incomplete. We seek to start a conversation around the need for an ethical audit framework for algorithmic development and deployment. We suggest that algorithmic applications go through what O’Neil (2016) calls an “algorithm audit”, specifically an ethical algorithm audit. This ethical AI auditing mechanism would provide some external validation of “doing no harm” and call out potential biases or flaws, based on industry or globally accepted best practice auditing procedures.",AMCIS,2019.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
1ad9700bab038dfa16c58140550eb118d2a63c45,https://www.semanticscholar.org/paper/1ad9700bab038dfa16c58140550eb118d2a63c45,Predictive Models for Equipment Fault Detection in the Semiconductor Manufacturing Process,"Semiconductor manufacturing is one of the most technologically and highly complicated manufacturing processes. Traditional machine learning algorithms such as uni-variate and multivariate analyses have long been deployed as a tool for creating predictive model to detect faults. In the past decade major collaborative research projects have been undertaken between fab industries and academia in the areas of predictive modeling. In this paper we review some of these research areas and thus propose machine learning techniques to automatically generate an accurate predictive model to predict equipment faults during the wafer fabrication process of the semiconductor industries. This research paper aims at constructing a decision model to help detecting as quickly as possible any equipment faults in order to maintain high process yields in manufacturing. In this research, we use WEKA tool and R languages for implementing our proposed method and other five machine learning discovery techniques.",,2016.0,10.7763/IJET.2016.V6.898,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
4c6214529d52ac10c2bd9b2ff7c26db93e5f3ca8,https://www.semanticscholar.org/paper/4c6214529d52ac10c2bd9b2ff7c26db93e5f3ca8,A Survey on Industrial Internet of Things: A Cyber-Physical Systems Perspective,"The vision of Industry 4.0, otherwise known as the fourth industrial revolution, is the integration of massively deployed smart computing and network technologies in industrial production and manufacturing settings for the purposes of automation, reliability, and control, implicating the development of an Industrial Internet of Things (I-IoT). Specifically, I-IoT is devoted to adopting the IoT to enable the interconnection of anything, anywhere, and at any time in the manufacturing system context to improve the productivity, efficiency, safety, and intelligence. As an emerging technology, I-IoT has distinct properties and requirements that distinguish it from consumer IoT, including the unique types of smart devices incorporated, network technologies and quality-of-service requirements, and strict needs of command and control. To more clearly understand the complexities of I-IoT and its distinct needs and to present a unified assessment of the technology from a systems’ perspective, in this paper, we comprehensively survey the body of existing research on I-IoT. Particularly, we first present the I-IoT architecture, I-IoT applications (i.e., factory automation and process automation), and their characteristics. We then consider existing research efforts from the three key system aspects of control, networking, and computing. Regarding control, we first categorize industrial control systems and then present recent and relevant research efforts. Next, considering networking, we propose a three-dimensional framework to explore the existing research space and investigate the adoption of some representative networking technologies, including 5G, machine-to-machine communication, and software-defined networking. Similarly, concerning computing, we again propose a second three-dimensional framework that explores the problem space of computing in I-IoT and investigate the cloud, edge, and hybrid cloud and edge computing platforms. Finally, we outline particular challenges and future research needs in control, networking, and computing systems, as well as for the adoption of machine learning in an I-IoT context.",IEEE Access,2018.0,10.1109/ACCESS.2018.2884906,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
554ba78505be195c90648f64115c9a50bd22b091,https://www.semanticscholar.org/paper/554ba78505be195c90648f64115c9a50bd22b091,PrescStream: A Framework for Streaming Soft Real-Time Predictive and Prescriptive Analytics,,ICCSA,2017.0,10.1007/978-3-319-62392-4_24,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
1f5902c44c4d58c4b59a38605af8a9c8183eae4e,https://www.semanticscholar.org/paper/1f5902c44c4d58c4b59a38605af8a9c8183eae4e,SparkBench: a spark benchmarking suite characterizing large-scale in-memory data analytics,,Cluster Computing,2017.0,10.1007/s10586-016-0723-1,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
2c9136d01f26eaada9bae89e117cacb0fcb4c59b,https://www.semanticscholar.org/paper/2c9136d01f26eaada9bae89e117cacb0fcb4c59b,The Integration of an API619 Screw Compressor Package into the Industrial Internet of Things,"The Industrial Internet of Things (IIoT) is the industrial subset of the Internet of Things (IoT). IIoT incorporates big data technology, harnessing the instrumentation data, machine to machine communication and automation technologies that have existed in industrial settings for years. As industry in general trends towards the IIoT and as the screw compressor packages developed by Howden Compressors are designed with a minimum design life of 25 years, it is imperative this technology is embedded immediately. This paper provides the reader with a description on the Industrial Internet of Things before moving onto describing the scope of the problem for an organisation like Howden Compressors who deploy multiple compressor technologies across multiple locations and focuses on the critical measurements particular to high specification screw compressor packages. A brief analysis of how this differs from high volume package manufacturers deploying similar systems is offered. Then follows a description on how the measured information gets from the tip of the instrument in the process pipework or drive train through the different layers, with a description of each layer, into the final presentation layer. The functions available within the presentation layer are taken in turn and the benefits analysed with specific focus on efficiency and availability. The paper concludes with how packagers adopting the IIoT can not only optimise their package but by utilising the machine learning technology and pattern detection applications can adopt completely new business models.",,2017.0,10.1088/1757-899X/232/1/012088,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
ce97d6b77d2399bb2f1c278f1220e76e1dca51fd,https://www.semanticscholar.org/paper/ce97d6b77d2399bb2f1c278f1220e76e1dca51fd,Data Analytics Towards Predictive Maintenance for Industrial Ovens - A Case Study Based on Data Analysis of Various Sensors Data,,CAiSE Workshops,2019.0,10.1007/978-3-030-20948-3_8,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
be11e04807d9d689ad5194592c779e5a780d3fc6,https://www.semanticscholar.org/paper/be11e04807d9d689ad5194592c779e5a780d3fc6,Edge and Fog Computing Enabled AI for IoT-An Overview,"In recent years, Artificial Intelligence (AI) has been widely deployed in a variety of business sectors and industries, yielding numbers of revolutionary applications and services that are primarily driven by high-performance computation and storage facilities in the cloud. On the other hand, embedding intelligence into edge devices is highly demanded by emerging applications such as autonomous systems, human-machine interactions, and the Internet of Things (IoT). In these applications, it is advantageous to process data near or at the source of data to improve energy & spectrum efficiency and security, and decrease latency. Although the computation capability of edge devices has increased tremendously during the past decade, it is still challenging to perform sophisticated AI algorithms in these resource-constrained edge devices, which calls for not only low-power chips for energy efficient processing at the edge but also a system-level framework to distribute resources and tasks along the edge-cloud continuum. In this overview, we summarize dedicated edge hardware for machine learning from embedded applications to sub-mW “always-on” IoT nodes. Recent advances of circuits and systems incorporating joint design of architectures and algorithms will be reviewed. Fog computing paradigm that enables processing at the edge while still offering the possibility to interact with the cloud will be covered, with focus on opportunities and challenges of exploiting fog computing in AI as a bridge between the edge device and the cloud.",2019 IEEE International Conference on Artificial Intelligence Circuits and Systems (AICAS),2019.0,10.1109/AICAS.2019.8771621,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
d972ace9366c011c7189c762e6c8ec7ae4830326,https://www.semanticscholar.org/paper/d972ace9366c011c7189c762e6c8ec7ae4830326,A Framework for Comprehensive Fraud Management using Actuarial Techniques,"Fraud acts as a major deterrent to a company’s growth if uncontrolled. It challenges the fundamental value of “Trust” in an Insurance business. This concern must be addressed on priority else, it brings down the castle of the insurance business. The regulation provides powers to authorities to act on fraud. Currently, this effort within most organizations happens discretely, involving those unconnected with actuarial or technology. In fact, actuarial techniques are powerful tools that help to bring efficiency and to target the right areas to deploy the right level of resources for fraud investigation. An effective solution approach to tackle this challenging problem is provided in this work which is empirically tested. In this work, we propose comprehensive fraud management (CFM) framework using actuarial techniques and AI Technology that helps increase fraud detection rate in comparison with other proposed models available in the literature. This framework includes three stages: • Stage 1: Automate Fraud identification using triggers specific to individual LoB and rule engine. This is a prevention stage. • Stage 2: Integrate Statistical/ Actuarial Techniques and Technology to identify fraud. Statistical/ Actuarial techniques for fraud detection include techniques such as classification trees, logistic regression, suspicious scoring, significance testing, random sampling, clustering, linear regression, peak analysis, extreme value theory etc. Technologies that are effective in detecting fraud include machine learning, deep learning, blockchain and distributed systems etc. This is a fraud detection stage. • Stage 3: Further analyse results from stage 2 to create a new set of fraud identification triggers. This adds on to the existing set of triggers in Stage 1 and increases the fraud detection rate in the subsequent runs of CFM. Proof of concept presented here on Motor line of business can be tested and extended to other lines of business or industries. This should encourage companies to explore new opportunities in comprehensive fraud management by utilising actuarial skillset ""carclaims.txt.""",,2019.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
a812368fe1d4a186322bf72a6d07e1cf60067234,https://www.semanticscholar.org/paper/a812368fe1d4a186322bf72a6d07e1cf60067234,Gaussian processes for modeling of facial expressions,"Automated analysis of facial expressions has been gaining significant attention over the past years. This stems from the fact that it constitutes the primal step toward developing some of the next-generation computer technologies that can make an impact in many domains, ranging from medical imaging and health assessment to marketing and education. No matter the target application, the need to deploy systems under demanding, realworld conditions that can generalize well across the population is urgent. Hence, careful consideration of numerous factors has to be taken prior to designing such a system. The work presented in this thesis focuses on tackling two important problems in automated analysis of facial expressions: (i) view-invariant facial expression analysis; (ii) modeling of the structural patterns in the face, in terms of well coordinated facial muscle movements. Driven by the necessity for efficient and accurate inference mechanisms we explore machine learning techniques based on the probabilistic framework of Gaussian processes (GPs). Our ultimate goal is to design powerful models that can efficiently handle imagery with spontaneously displayed facial expressions, and explain in detail the complex configurations behind the human face in real-world situations. To effectively decouple the head pose and expression in the presence of large outof-plane head rotations we introduce a manifold learning approach based on multi-view learning strategies. Contrary to the majority of existing methods that typically treat the numerous poses as individual problems, in this model we first learn a discriminative manifold shared by multiple views of a facial expression. Subsequently, we perform facial expression classification in the expression manifold. Hence, the pose normalization problem is solved by aligning the facial expressions from different poses in a common latent space. We demonstrate that the recovered manifold can efficiently generalize to various poses and expressions even from a small amount of training data, while also being largely robust to corrupted image features due to illumination variations. State-of-the-art performance is achieved in the task of facial expression classification of basic emotions. The methods that we propose for learning the structure in the configuration of the muscle movements represent some of the first attempts in the field of analysis and intensity estimation of facial expressions. In these models, we extend our multi-view approach to exploit relationships not only in the input features but also in the multi-output labels. The structure of the outputs is imposed into the recovered manifold either from heuristically defined hard constraints, or in an auto-encoded manner, where the structure is learned automatically from the input data. The resulting models are proven to be robust to data with imbalanced expression categories, due to our proposed Bayesian learning of the target manifold. We also propose a novel regression approach based on product of GP experts where we take into account people’s individual expressiveness in order to adapt the learned models on each subject. We demonstrate the superior performance of our proposed models on the task of facial expression recognition and intensity estimation.",,2016.0,10.25560/44106,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
9652253a15284e90f40484010c3e308a70741738,https://www.semanticscholar.org/paper/9652253a15284e90f40484010c3e308a70741738,"Boda-RTC: Productive generation of portable, efficient code for convolutional neural networks on mobile computing platforms","The popularity of neural networks (NNs) spans academia [1], industry [2], and popular culture [3]. In particular, convolutional neural networks (CNNs) have been applied to many image based machine learning tasks and have yielded strong results [4]. The availability of hardware/software systems for efficient training and deployment of large and/or deep CNN models is critical for the continued success of the field [5] [1]. Early systems for NN computation focused on leveraging existing dense linear algebra techniques and libraries [6] [7]. Current approaches use low-level machine specific programming [8] and/or closed-source, purpose-built vendor libraries [9]. In this work, we present an open source system that, compared to existing approaches, achieves competitive computational speed while achieving significantly greater portability. We achieve this by targeting the vendor-neutral OpenCL platform [10] using a code-generation approach. We argue that our approach allows for both: (1) the rapid development of new computational kernels for existing hardware targets, and (2) the rapid tuning of existing computational kernels for new hardware targets. Results are presented for a case study of targeting the Qualcomm Snapdragon 820 mobile computing platform [11] for CNN deployment.","2016 IEEE 12th International Conference on Wireless and Mobile Computing, Networking and Communications (WiMob)",2016.0,10.1109/WiMOB.2016.7763217,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
567ebb9855c3d093c361ec7d650a0bd7dcff8719,https://www.semanticscholar.org/paper/567ebb9855c3d093c361ec7d650a0bd7dcff8719,A Survey on Recent Trends and Open Issues in Energy Efficiency of 5G,"The rapidly increasing interest from various verticals for the upcoming 5th generation (5G) networks expect the network to support higher data rates and have an improved quality of service. This demand has been met so far by employing sophisticated transmission techniques including massive Multiple Input Multiple Output (MIMO), millimeter wave (mmWave) bands as well as bringing the computational power closer to the users via advanced baseband processing units at the base stations. Future evolution of the networks has also been assumed to open many new business horizons for the operators and the need of not only a resource efficient but also an energy efficient ecosystem has greatly been felt. The deployment of small cells has been envisioned as a promising answer for handling the massive heterogeneous traffic, but the adverse economic and environmental impacts cannot be neglected. Given that 10% of the world’s energy consumption is due to the Information and Communications Technology (ICT) industry, energy-efficiency has thus become one of the key performance indicators (KPI). Various avenues of optimization, game theory and machine learning have been investigated for enhancing power allocation for downlink and uplink channels, as well as other energy consumption/saving approaches. This paper surveys the recent works that address energy efficiency of the radio access as well as the core of wireless networks, and outlines related challenges and open issues.",Sensors,2019.0,10.3390/s19143126,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
095aeceba84ba188a2b3bb6e086d3755f11e824a,https://www.semanticscholar.org/paper/095aeceba84ba188a2b3bb6e086d3755f11e824a,"Using Augmented Intelligence to Automate Subsea Inspection Data Acquisition, Processing, Analysis, Reporting and Access","
 Augmented Intelligence (AI2) involves fusing Analyst Intuition with Artificial Intelligence to deliver an optimised combination of human-machine decision support.
 AI2 is being incorporated by i-Tech Services / Leidos into the physical inspection of offshore Oil, Gas, and Renewables assets, delivering valuable data driven insights that contribute to greater efficiency, enhanced condition monitoring, improved asset integrity and asset life extension.
 The deployment of vehicular and diver assets to obtain such inspection data, with associated support vessels, remains a major cost challenge for Operators.
 We believe the industry needs to approach this challenge from two key directions. Firstly, through the application of autonomous systems for data acquisition and delivery, reducing vessel reliance, and secondly through automating the acquisition and processing of data and maximising the insight provided by the data.
 This paper will examine the use of Augmented Intelligence to optimise the Subsea Inspection data workflow as a key use case, to demonstrate the principles.
 The historic paradigm consists of a fragmented evolving approach, with insufficient consideration and design across all the sensors, processing analytical engines and data visualisation. The approach being adopted is to closely link all aspects of the data workflow, within the context of delivering the data and beyond in terms of harvesting additional insight and value.
 To achieve the optimum workflow a number of developmental initiatives are being knitted into a modular platform, each element providing standalone value but the sum of the parts generates the most significant value and cost reduction.
 The elements being combined are automatic data quality control at acquisition source and through the full workflow, automated processing, machine vision for object recognition and reporting and machine learning to optimise the system intelligence. All of these are designed to augment the expertise of the analyst / user, detecting change to learnt parameters, by using real time data and critically by referencing large historical data sets and as-built data.
 The outputs from a system holistic approach will be improved data acquisition with more efficient high quality right first time data reporting. In addition layers of analytics, with smart, intuitive data access and retrieval will optimise delivery of key information within large data sets, together with maximising value and insight.","Day 2 Tue, May 07, 2019",2019.0,10.4043/29335-MS,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
1c31d738fb1abac291cff3b67067b00c88740ff6,https://www.semanticscholar.org/paper/1c31d738fb1abac291cff3b67067b00c88740ff6,Autonomous diagnostics and prognostics in machining processes through competitive learning-driven HMM-based clustering,"A prerequisite to widespread deployment of condition-based maintenance (CBM) systems in industry is autonomous yet effective diagnostics and prognostics algorithms. The concept of ‘autonomy’ in the context of diagnostics and prognostics is usually based on unsupervised clustering techniques. This paper employs an unsupervised competitive learning algorithm to perform hidden Markov model (HMM) based clustering of multivariate temporal observation sequences derived from sensor signal(s). This method improves autonomy of the diagnostic engine over the more traditional classifier based diagnostics models. Classifier models, such as the model presented by Baruah and Chinnam [Baruah, P. and Chinnam, R.B., 2005. HMM for diagnostics and prognostics in machining processes. International Journal of Production Research, 43 (6), 1275–1293] employ ‘labelled’ feature vectors for supervised model building and subsequent health-state classification during condition monitoring. Improving the autonomy of the diagnostics model also improves the autonomy of the prognostics module that often builds upon information extracted through the diagnostics module. We have validated the proposed methods on a physical test-bed that involved monitoring drill-bits on a CNC machine outfitted with thrust-force and torque sensors. Experimental results demonstrate the ability of this method to estimate on-line the remaining-useful-life of a drill-bit with significant accuracy.",,2009.0,10.1080/00207540802232930,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
0825c5b573ade44a571e4011230425234c6b0de1,https://www.semanticscholar.org/paper/0825c5b573ade44a571e4011230425234c6b0de1,Keynote Abstract: Safety and Security at the Heart of Autonomous Driving,"The automotive industry is undergoing a revolution with connected, autonomous and electric vehicles and the benefits they can bring to the public. Drivers enjoying their daily commute, fewer road fatalities and less pollution are all possible thanks to new technologies. Car makers need to offer these features but at the same time make sure vehicles are safe and secure. In the coming years, there will be various levels of automation until we have fully autonomous vehicles. To achieve any level of automation, cars need to connect to other vehicles, connect to the infrastructure, sense the environment through various sensors such as camera and radar and then make maneuvering decisions based on all these inputs. Artificial intelligence is and will be deployed heavily to accomplish many of the tasks of autonomous driving. Perception and decision-making based on artificial intelligence introduces an entirely new set of challenges to car makers to ensure no security compromises as well as proving the decisions being made are functionally, behaviorally and environmentally safe. The challenge can be described in a simple question: ""If a machine learning based car system is accurate 99% of the time, are you willing to ride this car knowing that it will be wrong 1% of the time? What is the consequence of that incorrect decision?"" Deep expertise and research in the safety and security aspects of AI are needed to ensure future mass deployment and success in the area of autonomous driving.",2018 1st Workshop on Energy Efficient Machine Learning and Cognitive Computing for Embedded Applications (EMC2),2018.0,10.1109/EMC2.2018.00006,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
6ba2423fd1564e7c4a1fabc2680d85a358245e23,https://www.semanticscholar.org/paper/6ba2423fd1564e7c4a1fabc2680d85a358245e23,A Chemical IoT System for Flow Assurance - From Single-Well Applications to Field Implementation,"
 Asphaltene deposition is a significant flow assurance challenge in Abu Dhabi with over 100 onshore wells impacted. Until recently, there had been no applicable IoT device in the industry for direct measurement of the problem, which prompted the national oil company to sponsor a real-time sensor for asphaltene quantification. A second generation of that device is now available with enhanced capabilities and has been delivered in country, with preparation now ongoing for deployment across multiple onshore wells.
 Current ways of identifying asphaltene problems via accessibility checks with slickline are reactive, often too late, and thereby increase cost of clean-up and production losses. By quantifying in real-time the asphaltene as it flows through the wellhead, much earlier problem detection is made feasible. Such quantification has been made possible by resonating asphaltene molecules in applied magnetic and GHz fields as those molecules flow through the wellhead. The peak of the resonant signal is directly proportional to the asphaltene in the crude, with decrease in signal indicating potential deposition. Adding cloud-based, machine-learning to the system allows local in-country team to make efficient use of the data.
 First deployment of the resonant system in Abu Dhabi demonstrated the resolution of asphaltene signature was better than 0.1% when measured at atmospheric pressure and temperature. Subsequent testing in pressure vessels has shown remarkable independence to fluid pressure, even up to 2000 psi. Original system had a sensitivity sufficient to detect only the asphaltene peak. New system reveals multiple smaller peaks in the spectra which can be used for other flow assurance applications - for example, vanadyl porphyrins display a unique characteristic. By mixing solvents and precipitants with crude oil, we have confirmed that the asphaltene peak measures same value whether the asphaltene is in, or out, of solution which is precisely the feature that allows surface data to be representative of the cumulative precipitation downhole. The main advance, however, is that the system data will now be available on multiple wells in the field, which allows Operator to compare and contrast different flow assurance optimizations. This should lead to substantial cost savings in addition to minimizing well downtime and potential loss of production, all in alignment with the Operator’s 2030 ""Smart Growth"" strategy.
 The additional spectral information also allows spectrometer use for real-time analysis of water properties (dissolved solids) and rock typing (geochemistry). More generally, the system becomes a real-time platform for advanced chemical analysis at the wellhead. The result is an industrial Internet of Things (IoT) real-time monitoring device, the first of its kind, that not just detects asphaltene deposition but also makes possible the optimization of chemical programs by incorporating surface data into an integrated flow assurance management system.",,2020.0,10.2118/203286-ms,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
4df5e4c43e83cd2278435751d6e00eae275f7f77,https://www.semanticscholar.org/paper/4df5e4c43e83cd2278435751d6e00eae275f7f77,GeoAI 2019 workshop report: The 3nd ACM SIGSPATIAL International Workshop on GeoAI: AI for Geographic Knowledge Discovery,"Nowadays artificial intelligence (AI) is bringing tremendous opportunities and challenges to geospatial research. Big data enable computers to observe and learn the world from many different perspectives, while high performance machines support the development, training, and deployment of AI models within reasonable amount of time. Recent years have witnessed significant advances in the integration of geospatial study and AI in both academia and industry. There have already been many successful studies for both physical environment and human society. Focusing on modeling the physical nature, research has shown that deep learning can improve the representation of clouds that are smaller than the grid resolutions of climate models. Examining the human society, AI and natural language processing methods, such as word embeddings, help quantify changes in stereotypes and attitudes toward women and ethnic minorities over 100 years in the United States. There are also many other applications that effectively integrate AI with problems in geospatial studies, such as vehicle trajectory prediction, high-definition mapping and navigation, historical map digitizing, gazetteer conflation, geographic feature extraction, and place understanding. The 3nd International Workshop on AI for Geographic Knowledge Discovery (GeoAI 2019) builds on the success of the previous workshops in 2017 and 2018. GeoAI is bringing together geoscientists, computer scientists, engineers, entrepreneurs, and decision makers from academia, industry, and government to discuss the latest trends, successes, challenges, and opportunities in the field of artificial intelligence for data mining and geographic knowledge discovery.",ACM SIGSPATIAL Special,2020.0,10.1145/3383653.3383662,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
14684b45799f966c288f649b059dd2d346ccec3a,https://www.semanticscholar.org/paper/14684b45799f966c288f649b059dd2d346ccec3a,From Average Customer to Individual Traveler: A Field Experiment in Airline Ancillary Pricing,"Ancillaries in the travel industry are now a major stream for revenue and profitability. Ancillaries are optional products or services whose sales depend on an individual's personal preference and their trip context. Conventional pricing strategies for ancillaries based on poorly optimized or static business rules do not respond to changing market conditions or trip context.<br><br>We present a dynamic pricing model developed in conjunction with Deepair solutions, an AI technology provider for travel suppliers. Our models provide dynamic, customer-interaction-specific pricing recommendations, to increase revenue. The unique nature of personalized pricing provides the opportunity to search over the market space to find the optimal price-point for each customer, without violating customer privacy.<br><br>We present an A/B testing deployment framework on an airline's website. Embedded in it are three models for dynamic pricing of ancillaries, with increasing levels of sophistication: (1) a two-stage forecasting and pricing model using a logistic mapping function; (2) a two-stage model with a deep neural network for forecasting, followed by pricing using discrete exhaustive search; (3) a single-stage end-to-end deep neural network that recommends the optimal price. In an outer loop, we introduce an online adaptive model-selection framework that adaptively routes customer requests to the above models. This is modeled as multi-armed bandit problem, which we solve using Thompson sampling.<br><br>We evaluate the performance of these models based on offfine and online evaluations, and their real-world business impact. Offline experiments show that deep learning algorithms outperform traditional machine learning techniques for this problem. In online testing, our AI-driven pricing outperforms human rule-based approaches, improving conversion by 17% and revenue per offer by 25%. Additionally, our adaptive model-selection approach outperforms a uniformly random selection policy by improving the expected revenue per offer by 43% and conversion score by 58% in a simulation environment.",,2020.0,10.2139/ssrn.3518854,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
c3f91c57bbccc8a93b384ee475c7f69a92c2a178,https://www.semanticscholar.org/paper/c3f91c57bbccc8a93b384ee475c7f69a92c2a178,Transactional auto scaler: elastic scaling of in-memory transactional data grids,"In this paper we introduce TAS (Transactional Auto Scaler), a system for automating elastic-scaling of in-memory transactional data grids, such as NoSQL data stores or Distributed Transactional Memories. Applications of TAS range from on-line self-optimization of in-production applications to automatic generation of QoS/cost driven elastic scaling policies, and support for what-if analysis on the scalability of transactional applications.
 The key innovation at the core of TAS is a novel performance forecasting methodology that relies on the joint usage of analytical modeling and machine-learning. By exploiting these two, classically competing, methodologies in a synergic fashion, TAS achieves the best of the two worlds, namely high extrapolation power and good accuracy even when faced with complex workloads deployed over public cloud infrastructures.
 We demonstrate the accuracy and feasibility of TAS via an extensive experimental study based on a fully fledged prototype implementation, integrated with a popular open-source transactional in-memory data store (Red Hat's Infinispan), and industry-standard benchmarks generating a breadth of heterogeneous workloads.",ICAC '12,2012.0,10.1145/2371536.2371559,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
cdebf1de7b2ccb852b203708f9dc2e584a2abb0c,https://www.semanticscholar.org/paper/cdebf1de7b2ccb852b203708f9dc2e584a2abb0c,Comparing Human-Robot Proxemics between Virtual Reality and the Real World,"Virtual Reality (VR) is gaining more and more popularity as a research tool in the field of Human-Robot Interaction (HRI). To fully deploy the potential of VR and benefit HRI studies, we need to establish the basic understanding of the relationship between the physical, real-world interaction (Live) and VR. This study compared Live and VR HRI with a focus on proxemics, as proxemics preference can reflect comprehensive human intuition, making it suitable to be used to compare Live and VR. To evaluate the influence of different modalities in VR, virtual scenes with different visual familiarity and spatial sound were compared as well. Lab experiments were conducted with a physical Pepper robot and its virtual copy. In both Live and VR, proxemics preferences, the perception of the robot (competence and discomfort) and the feeling of presence were measured and compared. Results suggest that proxemic preferences do not remain consistent in Live and in VR, which could be influenced by the perception of the robot. Therefore, when conducting HRI experiments in VR, the perceptions of the robot need be compared before the experiments. Results also indicate freedom within VR HRI as different VR settings are consistent with each other. Comparing Human-Robot Proxemics between Virtual Reality and the Real World Rui Li KTH Royal Institute of Technology Stockholm, Sweden Rui3@kth.se ABSTRACT Virtual Reality (VR) is gaining more and more popularity as a research tool in the field of HumanRobot Interaction (HRI). To fully deploy the potential of VR and benefit HRI studies, we need to establish the basic understanding of the relationship between the physical, real-world interaction (Live) and VR. This study compared Live and VR HRI with a focus on proxemics, as proxemics preference can reflect comprehensive human intuition, making it suitable to be used to compare Live and VR. To evaluate the influence of different modalities in VR, virtual scenes with different visual familiarity and spatial sound were compared as well. Lab experiments were conducted with a physical Pepper robot and its virtual copy. In both Live and VR, proxemics preferences, the perception of the robot (competence and discomfort) and the feeling of presence were measured and compared. Results suggest that proxemic preferences do not remain consistent in Live and in VR, which could be influenced by the perception of the robot. Therefore, when conducting HRI experiments in VR, the perceptions of the robot need be compared before the experiments. Results also indicate freedom within VR HRI as different VR settings are consistent with each other.Virtual Reality (VR) is gaining more and more popularity as a research tool in the field of HumanRobot Interaction (HRI). To fully deploy the potential of VR and benefit HRI studies, we need to establish the basic understanding of the relationship between the physical, real-world interaction (Live) and VR. This study compared Live and VR HRI with a focus on proxemics, as proxemics preference can reflect comprehensive human intuition, making it suitable to be used to compare Live and VR. To evaluate the influence of different modalities in VR, virtual scenes with different visual familiarity and spatial sound were compared as well. Lab experiments were conducted with a physical Pepper robot and its virtual copy. In both Live and VR, proxemics preferences, the perception of the robot (competence and discomfort) and the feeling of presence were measured and compared. Results suggest that proxemic preferences do not remain consistent in Live and in VR, which could be influenced by the perception of the robot. Therefore, when conducting HRI experiments in VR, the perceptions of the robot need be compared before the experiments. Results also indicate freedom within VR HRI as different VR settings are consistent with each other. INTRODUCTION Virtual Reality (VR) is gaining more and more popularity as a research tool in the field of HumanRobot Interaction (HRI) [1][2][3][4]. VR has been used to test teleoperation and collect demonstration data to train machine learning algorithms, which showcased the effectiveness of learning visuomotor skills using data collected by consumer-grade devices [1]. VR teleoperation systems were proposed to crowdsource robotic demonstrations at scale [2]. A VR simulation framework was also proposed to replace the physical robot, as VR can enable high level abstraction in embodiment and multimodal interaction [3]. VR has also been used as a rapid prototyping tool to design in-vehicle interactions and interfaces for self-driving cars, which showed the evocation to genuine responses from test participants [4]. Compared to other HRI experiment methods, VR as an emerging interactive media provides unique advantages. VR HRI has the potential of having higher immersion and fidelity than picture based HRI, video-based HRI and simulated HRI. In situations where the perception of the robot is challenging, compared to on-screen viewing, VR display showed significant improvement on collaborative tasks [5]. When comparing VR HRI to the physical, realworld interaction (Live HRI), there is a trade-off between the two. VR experiences still cannot replace physical experiences due to system limitation, and limited interaction modalities etc. [6]. For example, system limitations such as limited field of view and low display resolution could reduce immersion and presence of the VR experience, resulting in different behaviors from Live experiments. Limited interaction modalities, such as the absence of touch, means that the participant could not feel the robot or even go through the robot, which could potentially break the entire interaction. Figure 1: Photograph of the Live experiment setting However, with the help of the distribution of consumer-grade VR devices and online crowdsourcing platforms, VR HRI has the potential to gain massive data for training robotic behavior and studying HRI related issues. Data collection through VR can also reduce noise and improve the data quality [1], which help to ease data processing and algorithm training. Furthermore, VR HRI experiments can test concepts and interactions without physical robots, making it more resource efficient and less expensive than Live HRI. Less hardware also means that the experiment will be less cumbersome to set up, easier to be reproduced and to ensure experiment quality. In this study, HRI Proxemics (the preferred personal space between a human and a robot) was compared to give a better justification and more basic understanding of the relationship between Live and VR. Proxemics preferences rely on lower level intuition [7], therefore, reflect the differences in the perceptions between Live and VR better. Compared to other HRI subject such as conversational (audio) or gaze behavior (visual), which are more modality dependent, proxemics can give a comprehensive understanding of the human responses. In addition, variations of modalities in VR can greatly influence human perception. For example, a higher visual familiarity of the physical environment in VR can decrease the effect of distance distortion [8]. Auditory inputs play another important role in VR, the addition of spatial sound can increase the sense of presence in VR and provide sound localization [9]. Thus, this work also compares VR settings with variance in modalities to evaluate the impacts of visual familiarity and spatial sound on VR HRI experiments. A 2 x 3 mixed design experiment was conducted to evaluate the differences between Live and VR HRI, as well as the influence of visual familiarity and spatial sound in VR. For the Live HRI, the pepper robot from Softbank Robotics was used (Figure 1). In the VR HRI, a 3D model of the same robot was used. To measure visual familiarity, the VR scene was created in Blender based on a 3D scan of the physical lab. The spatial sound was created by enabling the movement of the physical robot, due to the difficulties of engineering spatial sound. The interaction was implemented in Unity. As an objective measurement for proxemics preference, the minimum comfort distance (MCD) was measured. In addition, for the psychological perception of the experience, the feeling of presence was measured with the SUS questionnaire. For the perception of the robot, two relevant factors, competence and discomfort was measured with the ROSAS questionnaire.",,2018.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
d853fbaee7d7a0e2e3155044ec9c3e9d8e85d0c4,https://www.semanticscholar.org/paper/d853fbaee7d7a0e2e3155044ec9c3e9d8e85d0c4,TPCx-HS v2: Transforming with Technology Changes,,TPCTC,2017.0,10.1007/978-3-319-72401-0_9,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
4625aeeec7a7b7e6874219627d32a0e7e522e4e0,https://www.semanticscholar.org/paper/4625aeeec7a7b7e6874219627d32a0e7e522e4e0,Cross-Method-Based Analysis and Classification of Malicious Behavior by API Calls Extraction,"Data-driven public security networking and computer systems are always under threat from malicious codes known as malware; therefore, a large amount of research and development is taking place to find effective countermeasures. These countermeasures are mainly based on dynamic and statistical analysis. Because of the obfuscation techniques used by the malware authors, security researchers and the anti-virus industry are facing a colossal issue regarding the extraction of hidden payloads within packed executable extraction. Based on this understanding, we first propose a method to de-obfuscate and unpack the malware samples. Additional, cross-method-based big data analysis to dynamically and statistically extract features from malware has been proposed. The Application Programming Interface (API) call sequences that reflect the malware behavior of its code have been used to detect behavior such as network traffic, modifying a file, writing to stderr or stdout, modifying a registry value, creating a process. Furthermore, we include a similarity analysis and machine learning algorithms to profile and classify malware behaviors. The experimental results of the proposed method show that malware detection accuracy is very useful to discover potential threats and can help the decision-maker to deploy appropriate countermeasures.",Applied Sciences,2019.0,10.3390/APP9020239,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
59e10d1d4cd454635914cfd0ac5160a318fd0473,https://www.semanticscholar.org/paper/59e10d1d4cd454635914cfd0ac5160a318fd0473,UB09 Session 9,"In the domain of Wireless Sensor Networks (WSN), providing an effective security solution to protect the motes and their communications is challenging. Due to the hard constraints on performance, storage and energy consumption, normal network-security related techniques cannot be applied. Focusing on the ""Intrusion Detection"" problem, we propose a realworld application of our WSN Intrusion Detection System (WIDS). WIDS exploits the Weak Process Models to classify potential security issues in the WSN and to notify the operators when an attack tentative is detected. In this demonstration, we show how our IDS works, how it detects some basic attacks and how the IDS can evolve to fullfil the needs of secure WSN deployments. Download Paper (PDF) UB09.2 RESCUE: EDA TOOLSET FOR INTERDEPENDENT ASPECTS OF RELIABILITY, SECURITY AND QUALITY IN NANOELECTRONIC SYSTEMS DESIGN Authors: Cemil Cem Gürsoy1, Guilherme Cardoso Medeiros2, Junchao Chen3, Nevin George4, Josie Esteban Rodriguez Condia5, Thomas Lange6, Aleksa Damljanovic5, Raphael Segabinazzi Ferreira4, Aneesh Balakrishnan6, Xinhui Anna Lai1, Shayesteh Masoumian7, Dmytro Petryk3, Troya Cagil Koylu2, Felipe Augusto da Silva8, Ahmet Cagri Bagbaba8 and Maksim Jenihhin1 1Tallinn University of Technology, EE; 2Delft University of Technology, NL; 3IHP, DE; 4BTU Cottbus-Senftenberg, DE; 5Politecnico di Torino, IT; 6IROC Technologies, FR; 7Intrinsic ID B.V., NL; 8Cadence Design Systems GmbH, DE Abstract The demonstrator will introduce an EDA toolset developed by a team of PhD students in the H2020-MSCA-ITN RESCUE project. The recent trends for the computing systems include machine intelligence in the era of IoT, complex safety-critical applications, extreme miniaturization of technologies and intensive interaction with the physical world. These trends set tough requirements on mutually dependent extra-functional design aspects. RESCUE is focused on the key challenges for reliability (functional safety, ageing, soft errors), security (tamper-resistance, PUF technology, intelligent security) and quality (novel fault models, functional test, FMEA/FMECA, verification/debug) and related EDA methodologies. The objective of the interdisciplinary cross-sectoral team from Tallinn UT, TU Delft, BTU Cottbus, POLITO, IHP, IROC, Intrinsic-ID, Cadence and Bosch is to develop in collaboration a holistic EDA toolset for modelling, assessment and enhancement of these extra-functional design aspects. Download Paper (PDF)The demonstrator will introduce an EDA toolset developed by a team of PhD students in the H2020-MSCA-ITN RESCUE project. The recent trends for the computing systems include machine intelligence in the era of IoT, complex safety-critical applications, extreme miniaturization of technologies and intensive interaction with the physical world. These trends set tough requirements on mutually dependent extra-functional design aspects. RESCUE is focused on the key challenges for reliability (functional safety, ageing, soft errors), security (tamper-resistance, PUF technology, intelligent security) and quality (novel fault models, functional test, FMEA/FMECA, verification/debug) and related EDA methodologies. The objective of the interdisciplinary cross-sectoral team from Tallinn UT, TU Delft, BTU Cottbus, POLITO, IHP, IROC, Intrinsic-ID, Cadence and Bosch is to develop in collaboration a holistic EDA toolset for modelling, assessment and enhancement of these extra-functional design aspects. Download Paper (PDF) UB09.3 ASAM: AUTOMATIC SYNTHESIS OF ALGORITHMS ON MULTI CHIP/FPGA WITH COMMUNICATION CONSTRAINTS Authors: Amir Masoud Gharehbaghi, Tomohiro Maruoka, Yukio Miyasaka, Akihiro Goda, Amir Masoud Gharehbaghi and Masahiro Fujita, The University of Tokyo, JP Abstract Mapping of large systems/computations on multiple chips/multiple cores needs sophisticated compilation methods. In this demonstration, we present our compiler tools for multi-chip and multi-core systems that considers communication architecture and the related constraints for optimal mapping. Specifically, we demonstrate compilation methods for multi-chip connected with ring topology, and multi-core connected with mesh topology, assuming fine-grained reconfigurable cores, as well as generalization techniques for large problems size as convolutional neural networks. We will demonstrate our mappings methods starting from data-flow graphs (DFGs) and equations, specifically with applications to convolutional neural networks (CNNs) for convolution layers as well as fully connected layers. Download Paper (PDF) UB09.4 HEPSYCODE-MC: ELECTRONIC SYSTEM-LEVEL METHODOLOGY FOR HW/SW CO-DESIGN OF MIXED-CRITICALITY EMBEDDED SYSTEMS Authors: Luigi Pomante1, Vittoriano Muttillo1, Marco Santic1 and Emilio Incerto2 1Università degli Studi dell'Aquila DEWS, IT; 2IMT Lucca, IT Abstract Heterogeneous parallel architectures have been recently exploited for a wide range of embedded application domains. Embedded systems based on such kind of architectures can include different processor cores, memories, dedicated ICs and a set of connections among them. Moreover, especially in automotive and aerospace application domains, they are even more subjected to mixed-criticality constraints. So, this demo addresses the problem of the ESL HW/SW co-design of mixed-criticality embedded systems that exploit hypervisor (HPV) technologies. In particular, it shows an enhanced CSP/SystemC-based design space exploration step, in the context of an existing HW/SW co-design flow that, given the system specification is able to (semi)automatically propose to the designer: a custom heterogeneous parallel HPV-based architecture; an HW/SW partitioning of the application; a mapping of the partitioned entities onto the proposed architecture. Download Paper (PDF)Heterogeneous parallel architectures have been recently exploited for a wide range of embedded application domains. Embedded systems based on such kind of architectures can include different processor cores, memories, dedicated ICs and a set of connections among them. Moreover, especially in automotive and aerospace application domains, they are even more subjected to mixed-criticality constraints. So, this demo addresses the problem of the ESL HW/SW co-design of mixed-criticality embedded systems that exploit hypervisor (HPV) technologies. In particular, it shows an enhanced CSP/SystemC-based design space exploration step, in the context of an existing HW/SW co-design flow that, given the system specification is able to (semi)automatically propose to the designer: a custom heterogeneous parallel HPV-based architecture; an HW/SW partitioning of the application; a mapping of the partitioned entities onto the proposed architecture. Download Paper (PDF) UB09.5 CS: CRAZYSQUARE Authors: Federica Caruso1, Federica Caruso1, Tania Di Mascio1, Alessandro D'Errico1, Marco Pennese2, Luigi Pomante1, Claudia Rinaldi1 and Marco Santic1 1University of L'Aquila, IT; 2Ministry of Education, IT Abstract CrazySquare (CS) is an adaptive learning system, developed as a serious game for music education, specifically indicated for young teenager approaching music for the first time. CS is based on recent educative directions which consist of using a more direct approach to sound instead of the musical notation alone. It has been inspired by a paper-based procedure that is currently used in an Italian middle school. CS represents a support for such teachers who prefer involving their students in a playful dimension of learning rhythmic notation and pitch, and, at the same time, teaching playing a musical instrument. To reach such goals in a cost-effective way, CS fully exploits all the recent advances in the EDA domain. In fact, it is based on a framework composed of mobile applications that will be integrated with augmented reality HW/SW tools to provide virtual/augmented musical instruments. The proposed demo will show the main features of the current CS framework implementation. Download Paper (PDF)CrazySquare (CS) is an adaptive learning system, developed as a serious game for music education, specifically indicated for young teenager approaching music for the first time. CS is based on recent educative directions which consist of using a more direct approach to sound instead of the musical notation alone. It has been inspired by a paper-based procedure that is currently used in an Italian middle school. CS represents a support for such teachers who prefer involving their students in a playful dimension of learning rhythmic notation and pitch, and, at the same time, teaching playing a musical instrument. To reach such goals in a cost-effective way, CS fully exploits all the recent advances in the EDA domain. In fact, it is based on a framework composed of mobile applications that will be integrated with augmented reality HW/SW tools to provide virtual/augmented musical instruments. The proposed demo will show the main features of the current CS framework implementation. Download Paper (PDF) UB09.6 LABSMILING: A SAAS FRAMEWORK, COMPOSED OF A NUMBER OF REMOTELY ACCESSIBLE TESTBEDS AND RELATED SW TOOLS, FOR ANALYSIS, DESIGN AND MANAGEMENT OF LOW DATA-RATE WIRELESS PERSONAL AREA NETWORKS BASED ON IEEE 802.15.4 Authors: Carlo Centofanti, Luigi Pomante, Marco Santic and Walter Tiberti, University of L'Aquila, IT Abstract Low data-rate wireless personal area networks (LR-WPANs) are constantly increasing their presence in the fields of IoT, wearable, home automation, health monitoring. The development, deployment and testing of SW based on IEEE 802.15.4 standard (and derivations, e.g. 15.4e), require the exploitation of a testbed as the network grows in complexity and heterogeneity. This demo shows LabSmiling: a SaaS framework which connects testbeds deployed in a real-world-environment and the related SW tools that make available a meaningful (but still scalable) number of physical devices (sensor nodes) to developers. It provides a comforta",,2019.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
0a3e48c04d4f4b0418544c942e5344bd740590b9,https://www.semanticscholar.org/paper/0a3e48c04d4f4b0418544c942e5344bd740590b9,The best of both worlds: highlighting the synergies of combining manual and automatic knowledge organization methods to improve information search and discovery.,"Research suggests organizations across all sectors waste a significant amount of time looking for information and often fail to leverage the information they have. In response, many organizations have deployed some form of enterprise search to improve the ˜findability of information. Debates persist as to whether thesauri and manual indexing or automated machine learning techniques should be used to enhance discovery of information. In addition, the extent to which a Knowledge Organization System (KOS) enhances discoveries or indeed blinds us to new ones remains a moot point. The oil and gas industry is used as a case study using a representative organization. Drawing on prior research, a theoretical model is presented which aims to overcome the shortcomings of each approach. This synergistic model could help re-conceptualize the ˜manual versus ˜automatic debate in many enterprises, accommodating a broader range of information needs. This may enable enterprises to develop more effective information and knowledge management strategies and ease the tension between what is often perceived as mutually exclusive competing approaches. Certain aspects of the theoretical model may be transferable to other industries which is an area for further research. THE ARTICLE WAS SELECTED AS A BEST PAPER FROM THE INTERNATIONAL SOCIETY OF KNOWLEDGE ORGANIZATION (ISKO) CONFERENCE WHERE THE PAPER IS ALSO ONLINE AT: http://www.iskouk.org/content/best-both-worlds-highlighting-synergies-combining-knowledge-modelling-and-automated",,2015.0,10.5771/0943-7444-2015-6-428,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
2fe235d1b325bb6114540bc30ff79cc7c2d496b3,https://www.semanticscholar.org/paper/2fe235d1b325bb6114540bc30ff79cc7c2d496b3,An artificial neural network-based ensemble model for credit risk assessment and deployment as a graphical user interface,"Credit risk is a common threat to the financial industry since improper management of credit risk leads to heavy financial losses in banking and non-banking sectors. Data mining approaches have been employed in the past to assess the credit risk. This study utilises the German credit dataset sourced from UCI machine learning repository for generating an artificial neural network-based ensemble learning model for credit risk assessment. Eleven data mining algorithms have been applied on an open source tool Weka for performing credit ratings on the German credit dataset using supervised learning approach. The performance of each algorithm was evaluated, and algorithms with the most diverse false positive and false negative results and that are highly accurate were selected for generating an ensemble model. The predicted outcomes of the top five ranked algorithms were fed into a feed-forward artificial neural network by employing an 'nnet' package in R. The artificial neural network-based ensemble model attained an accuracy of 98.98%, performing better than the individual component algorithms. Based on this ANN-based ensemble model, an interactive graphical user interface was further developed in R. The user-friendly graphical user interface can be used by financial organisations as a decision-support system for assessing the credit risk.",Int. J. Data Min. Model. Manag.,2017.0,10.1504/IJDMMM.2017.10006638,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
8d299258291512b11750cd61746a4f24193f7c07,https://www.semanticscholar.org/paper/8d299258291512b11750cd61746a4f24193f7c07,It’s All About Data: How to Make Good Decisions in a World Awash with Information,"The rise of big and alternative data has created significant new business opportunities in the financial sector. As we start on this journey of fast-moving technology disruption, financial professionals have a rare opportunity to balance the exponential growth of artificial intelligence (AI)/data science with ethics, bias, and privacy to create trusted data-driven decision making. In this article, the authors discuss the nuances of big data sets that are critical when one considers standards, processes, best practices, and modeling algorithms for the deployment of AI systems. In addition, this industry is widely guided by a fiduciary standard that puts the interests of the client above all else. It is therefore critical to have a thorough understanding of the limitations of our knowledge, because there are many known unknowns and unknown unknowns that can have a significant impact on outcomes. The authors emphasize key success factors for the deployment of AI initiatives: talent and bridging the skills gap. To achieve a lasting impact of big data initiatives, multidisciplinary teams with well-defined roles need to be established with continuing training and education. The prize is the finance of the future. TOPICS: Simulations, big data/machine learning Key Findings • The rise of alternative data in finance is creating major opportunities in all areas of the financial industry, including risk management, portfolio construction, investment banking, and insurance. • To build trusted outcomes in AI/ML initiatives, financial professionals’ roles are critical. Given the many nuances in using big data, there is a need for vetted protocols and methods in selecting data sets and algorithms. Best practices and guidelines are effective in reducing the risks of using AI/ML, including overfitting, lack of interpretability, biased inputs, and unethical use of data. • Given the major shortage of talent in AI/data science in finance, practical training of employees and continued education are keys to scale roll out to enable future of finance.",,2020.0,10.3905/jfds.2020.1.025,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
45f3bc90e4c721a0a91a5083a6272552a10f4d78,https://www.semanticscholar.org/paper/45f3bc90e4c721a0a91a5083a6272552a10f4d78,How to Gain a Competitive Edge with the Digital Twin,"
 With the move towards IIoT and the Digital Age, companies in the process industry are looking to digitalize processes. One critical element in this development is the increased adoption of the Digital Twin (DT), which is being deployed in all phases of a product, plant, or process lifecycle from design to operations to maintenance.
 A DT is a virtual representation of a physical product or process used to understand and predict the physical counterpart's performance characteristics. They are used throughout the product lifecycle to simulate, predict, and optimize the product and production system before investing in physical prototypes.
 The concept is not new. For more than 30 years, product and process engineering teams have used 3D renderings and process simulation to validate manufacturability. What is new, however, is that several factors have now converged to bring the concept of the DT to the forefront as a disruptive trend in the process industry. This will be demonstrated using relevant use cases featuring organizations that have seen increases in efficiency improvements after employing digitalization and the DT
 By incorporating simulation, data analytics, and machine learning, the DT is able to demonstrate impacts of usage and training scenarios in a virtual setting. This enables identification of any potential issues in the design phase as opposed to the commissioning phase as is very common today. Process and sensor data from physical objects are collected and analyzed to determine real-time performance and operating changes over time. Feeding this data back into the product lifecycle, the DT is continuously updated to reflect changes to the physical counterpart. This creates a closed-loop of virtual feedback that makes products, production, and performance optimization possible at minimal cost.",,2020.0,10.4043/30801-ms,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
5f2836111f7b2e3c9c4df81a7b9b8f183bfb5301,https://www.semanticscholar.org/paper/5f2836111f7b2e3c9c4df81a7b9b8f183bfb5301,Accelerating Artificial Intelligence on the Grid,"In this work, we present an ongoing, three year project funded by the Advanced Research Project Agency (ARPA-E) to develop an infrastructure to accelerate the development and deployment of artificial intelligence solutions for the electric grid. The project addresses the critical issues that we have identified as hampering the deployment of AI on electric grid measurements. These issues have stymied the potential for insight from high resolution grid measurements and generally hindered artificial intelligence innovation in the utility industry. The project consists of three main components. The first is the deployment of a diverse set of grid sensors to capture a variety of grid behaviour, both from the field and in simulation. The second is the deployment of a highly performant, scalable, cloud-based data management and AI platform designed for time series data to enable the easy storage, processing and analysis of grid sensor data. The third is the cultivation of an open research community of experts around the platform and data through useful educational material, code and data sharing, and data science competitions. Overall, the project will accelerate the development of analytics, machine learning, and AI by addressing existing gaps in data, tools, and people, with the aim of improving the electric grid.",2020 Clemson University Power Systems Conference (PSC),2020.0,10.1109/PSC50246.2020.9131317,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
58dc374e2669cd54638605c336334c9aff818095,https://www.semanticscholar.org/paper/58dc374e2669cd54638605c336334c9aff818095,Selected Papers from the Eleventh ITU Kaleidoscope Academic Conference,"This special section contains two updated papers, originally presented at the eleventh International Telecommunication Union’s (ITU) Kaleidoscope academic conference. The title of the conference was “ICT for Health: Networks, Standards and Innovation,” and took place in the United States, specifi cally in Atlanta, Georgia from 4-6 December 2019. The host was the Georgia Tech Research Institute with the collaboration of the World Health Organization (WHO). There were nearly 100 participants: 70 physically at the venue coming from 16 countries and over 30 on the web. The proceedings are available on the ITU website at https://www.itu.int/pub/T-PROC-KALEI-2018, and from the IEEE Xplore Digital Library. Pictorial highlights from the conference are available at https://www.fl ickr.com/photos/ itupictures/with/49237161532/. The topic of the next conference is “Industry-Driven Digital Transformation.” It was originally scheduled for 7-9 September 2020 in Hanoi, Viet Nam, in conjunction with the ITU Digital World 2020. However, due to the Covid-19 pandemic, it is now an online conference from 7–11 December 2020 running four hours per day to accommodate the various time zones. To access the conference, check its main page at https://www.itu.int/en/ITU-T/academia/kaleidoscope/2020. The ITU Kaleidoscope series of academic conferences started in 2008 to provide an interdisciplinary forum for the discussion of Information and Communication Technologies (ICTs) relevant to future telecommunication standards. Participants typically include researchers, academics, students, engineers, policymakers, regulators as well as futurists. The fi rst article in this issue deals with Machine Learning (ML) and Artifi cial Intelligence (AI) in medicine. The main challenge is that, because of the wide variety of patients and clinical conditions, ML/AI models must produce results that practitioners can rely on even when the algorithms process previously unseen data. Another diffi culty is that these algorithms are black boxes because their exact workings are unknown. Some bioethicists have suggested that applying trust to AI is a corruption of language that can corrupt thought because it is “a category error, mistakenly assuming that AI belongs to a category of things that can be trusted” [1]. This is why international cooperation is indispensable because it allows substantial synergies in the selection of the training and test data sets as well as the validation of the software, from both engineering and clinical viewpoints. This invited article from the Fraunhofer Heinrich Hertz Institute and the Technische Universität Berlin, titled “Toward Global Validation Standards for Health AI,” covers these aspects. The authors, Markus A. Wenzel and Thomas Wiegand, present an overview of the work being carried out under the joint auspices of the ITU-T and WHO to address the use of machine learning and artifi cial intelligence in healthcare, and highlight what has been achieved in terms of guidelines. On the regulatory side, they mention the contributions of the National Health Service in the UK and the International Medical Device Regulators Forum. On the standardization side, they list activities by a variety of organizations such as the U.S. National Institute of Standards and Technology (NIST), the Chinese Electronics Standards Institute, the European Union High-Level Expert Group on AI, the German Deutsches Institut für Normung (DIN), the IEEE, and the International Organization for Standardization (ISO). The second article, “Converged Internet of Lights Network for Telecommunication, Positioning, Illumination and Medical Therapy,” is a joint contribution from several Chinese and British universities and research institutions. The authors are Jian Song, Xiaofei Wang, Jintao Wang, Hongming Zhang, Changyong Pan, Yue Zhang, and John Cosmas. They focus on the spectrum of the visible light from 380nm to 850nm, which is nearly one thousand times broader than the Radio Frequency (RF) spectrum. This is because Light Emitting Diodes (LEDs) can be deployed to modulate visible light for Visible Light Communication (VLC). Accordingly, lighting systems can be designed to combine information services using a network of LEDs integrated with sensors. This would constitute what the authors call the Internet of Lights (IoL). IoL, however, can have both positive and negative impact on human beings (as well as other animals), because of its eff ect on the circadian rhythms and hence body functions. On the positive side, it can be used as a non-intrusive intervention therapy to alleviate degenerative neurological diseases such as SeLecTeD Papers froM The ELeVenTh ITU KaLeiDoscope AcaDeMic Conference",IEEE Commun. Stand. Mag.,2020.0,10.1109/MCOMSTD.2020.9204601,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
ca1217621296f42d425c9f363ccb1e20540f54aa,https://www.semanticscholar.org/paper/ca1217621296f42d425c9f363ccb1e20540f54aa,Edge Analytics as Key Enabler for the Success of Oil & Gas 4.0,"
 
 
 Corporations & plant operators are focused on improving profitability by every possible $ (Dollar). With no control over the feed stock and finished material pricing, which are mostly governed by regulations, operators must look with in their Operation & Maintenance (O&M) for possible areas of optimization. In modern days, CIOs are greatly empowered to drive Digital Transformation strategy and contribute in improved profitability. Many of such initiatives fail to deliver the desired outcomes in timely manner. This paper discusses the essential elements of EDGE analytics embedding the fundamental aspects of plant O&M in such Digital Transformation initiatives, and thus enabling faster value realization of the investment.
 
 
 
 Every industry can be looked in to three different tiers as Assets, Process & systems and Business. Each of them has different outcomes to contribute in overall profitability. Moreover, technology, methodology, people and their collaborative adoption play a big role in achieving those outcomes.
 Asset performance is evaluated using KPIs like availability, reliability, cost & Safety. Process involves interaction of people & process fluids with those assets. However, the fundaments remain same as the source of information, based on which the O&M decisions are taken.
 Transducers & measurements around plant assets are the single biggest source of information for such decision making. Often, this information is not best utilized in terms of generating proactive insights. Information from these measurements can be converted into simple analytics. E.g. Proximity type vibration transducer measures vibration amplitude which is used for equipment safe operation and shutdown when exceed the defined threshold. Same transducer also provides the gap voltage and spectral data. Analytics definition using proximity probe gap voltage will identify rotor position within the bearing and then comparing it across the machine train along with orbit preload can give significant information about rotor dynamics. Such analytics available at EDGE are often overlooked in enterprise level predictive analytics deployed by Artificial Intelligence/ Machine Learning. One of the reasons for it is cyber security concern for the data exchange across IT-OT network layers.
 This paper talks about implementing comprehensive strategy utilizing the information from EDGE devices for EDGE analytics, various challenges and possible way forward in complying to cyber security requirements for data exchange from OT-IT systems and thus enhancing value of big data in Data Lakes.
 
 
 
 EDGE analytics adds immense value for assets in achieving best of performance KPIs. With increased adoption of cyber security practices in plant operations, there are many IT approved architectures exist today to share these analytics insight to L4 - business network. With focus on industry domain experience around systems of systems and progressive thinking, EDGE analytics will play inevitable role in the overall success of O&G 4.0 in coming years.
 
 
 
 ""Data is the new oil"". Corporates in energy sectors are driving forth industrial revolution with this belief and investing heavily in big data-based projects. Asset condition monitoring systems at EDGE in OT domain remains in silos. High resolution static & dynamic data in various operating modes collected in these Condition monitoring systems hasn't been historically much interfaced with such big data projects. Use cases of such interfaces with static & dynamic data, benefits, related workflow, data exchange architecture is relatively newer in the industry. Comprehensive discussion of these aspects in this paper will enable industries to realize faster outcomes.
",,2020.0,10.2118/203269-ms,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
0878f21553a04ae10ed1a3e55ddf4091e6238df2,https://www.semanticscholar.org/paper/0878f21553a04ae10ed1a3e55ddf4091e6238df2,ICIN 2020 Program,"For nearly fifty years, beginning with Kleinrock's pioneering work on using queueing theory to model packet flows in communication networks, network modeling has adopted the individual packet as primary level of granularity for network modeling and analysis. With the advent of terabit-switching capabilities, information-centric networking, and data centers with complex workloads and hundreds of thousands of components, the time would seem ripe to raise the level of abstraction beyond the packet. In this talk, we identify higher-level modeling abstractions are already proving useful as well as new needed abstractions. But also we identify cases where packet-level models are still crucial in providing important insights. Wednesday, February 26 10:00 11:00 K2: Keynote 2 Softwarization and IoT evolution Lefteris Mamatas (University of Macedonia, Greece) Room: La Grande Scène Chair: Alex Galis (University College London (UCL), United Kingdom (Great Britain)) Abstract: The Internet of Things (IoT), a main enabler for Industry 4.0, is considered as a system connecting myriads of people, things and services. IoT enables new large-scale applications with diverse constraints (e.g., limited resource availability or mobility) and requirements (e.g., ultra low delays). A main challenge is the evolution beyond large networks of sensing devices to multiple cooperating network deployments that implement context-sensitive communication and cloud processing strategies, through the seamless adoption of Softwarization technologies. The talk includes the following aspects: (i) a motivation of the above vision with two novel use-cases on smart-city and maritime contexts; (ii) a discussion on the evolutionary and clean-slate approaches to the IoT Softwarization; (iii) the missing elements and open issues in Software-Defined IoT and Edge Cloud technologies; and (iv) insights from our practical experience in relevant implementations and real experiments. The Internet of Things (IoT), a main enabler for Industry 4.0, is considered as a system connecting myriads of people, things and services. IoT enables new large-scale applications with diverse constraints (e.g., limited resource availability or mobility) and requirements (e.g., ultra low delays). A main challenge is the evolution beyond large networks of sensing devices to multiple cooperating network deployments that implement context-sensitive communication and cloud processing strategies, through the seamless adoption of Softwarization technologies. The talk includes the following aspects: (i) a motivation of the above vision with two novel use-cases on smart-city and maritime contexts; (ii) a discussion on the evolutionary and clean-slate approaches to the IoT Softwarization; (iii) the missing elements and open issues in Software-Defined IoT and Edge Cloud technologies; and (iv) insights from our practical experience in relevant implementations and real experiments. Wednesday, February 26 11:30 12:30 TS3: Network Slicing Room: La Grande Scène Chair: Prosper Chemouil (Orange Labs (retired), France) TS3.1 A Lightweight Policy-aware Broker for Multi-domain Network Slice Composition Xuan-Thuy Dang (Technische Universität Berlin & DAI Labor, Germany); Fikret Sivrikaya (GT-ARC gGmbH & Technische Universität Berlin, Germany) TS3.2 Enhancing the performance of 5G slicing operations via multi-tier orchestration Miquel Puig Mena (i2cat Foundation, Spain); Apostolos Papageorgiou, Leonardo Ochoa-Aday and Muhammad Shuaib Siddiqui (Fundació i2CAT, Internet i Innovació Digital a Catalunya, Spain); Gabriele Baldoni (ADLINK Technology, France) TS3.3 An Efficient Online Heuristic for Mobile Network Slice Embedding Katja Ludwig (University of Augsburg, Germany); Andrea Fendt (Nokia Bell Labs & University of Augsburg, Germany); Bernhard Bauer (University of Augsburg, Germany) Wednesday, February 26 12:30 13:00 DPS: Demo/Poster ""Elevator Pitch"" Session Room: La Grande Scène Chair: Prosper Chemouil (Orange Labs (retired), France) DPS.1 A QUIC-based proxy architecture for an efficient hybrid backhaul transport Michele Luglio and Mattia Quadrini (University of Rome Tor Vergata Dip. Ing. Elettronica, Italy); Cesare Roseti and Francesco Zampognaro (University of Rome Tor Vergata, Italy); Simon Pietro Romano (University of Napoli Federico II, Italy) DPS.2 A Blockchain-based Brokerage Platform for Fog Computing Resource Federation Marco Savi, Daniele Santoro, Katarzyna Di Meo and Daniele Pizzolli (Fondazione Bruno Kessler, Italy); Miguel Pincheira (OpenIoT Research Area, FBK CREATE-NET & University of Trento, Italy); Raffaele Giaffreda (FBK CREATE-NET, Italy); Silvio Cretti (Fondazione Bruno Kessler, Italy); Seung-woo Kum (Korea Electronics Technology Institute, Korea (South)); Domenico Siracusa (Fondazione Bruno Kessler, Italy) DPS.3 Optimized Network Slicing Proof-of-Concept with Interactive Gaming Use Case José J Alves Esteves, Jr. (Orange Labs & Sorbonne Université, France); Amina Boubendir and Fabrice M. Guillemin (Orange Labs, France); Pierre Sens (Université de Paris 6, France) DPS.4 A Deployable Containerized 5G Core Solution for Time Critical Communication in Smart Grid Van Giang Nguyen, Karl-Johan Grinnemo, Javid Taheri and Anna Brunstrom (Karlstad University, Sweden) DPS.5 FogGuru: a Fog Computing platform based on Apache Flink Davaadorj Battulga (University of Rennes 1 & U-Hopper, Italy); Daniele Miorandi (U-Hopper, Italy); Cedric Tedeschi (University of Rennes I / INRIA, France) DPS.6 5G Experimentation Framework: Architecture Specifications, Design and Deployment Louiza Yala (Orange Labs, France); Sihem Cherrared (University of Rennes 1 & Orange Labs and INRIA, France); Grzegorz Panek (Orange Polska, Poland); Sofiane Imadali and Ayoub Bousselmi (Orange Labs, France) DPS.7 A New Service Management Framework for Vehicular Networks Jose Ramirez, Onyekachukwu Augustine Ezenwigbo, Gayathri Karthick and Ramona Trestian (Middlesex University, United Kingdom (Great Britain)); Glenford E Mapp (MIddlesex University & Cantego Limited, United Kingdom (Great Britain)) DPS.8 Creating trust in automation in intent-based mobile network management Ville Vartiainen (Aalto University, Finland); Dmitry Petrov and Vilho Räisänen (Nokia Bell Labs, Finland) DPS.9 Interoperable and discrete eHealth Data Exchange between Hospital and Patient Andreea Ancuta Corici, Olaf Rode, Ben Kraufmann, Andreas Billig, Jörg Caumanns and Markus Deglmann (Fraunhofer FOKUS, Germany); Viktoria Walter, Janina Rexin and Gunther Nolte (Vivantes Netzwerk für Gesundheit GmbH, Germany) Wednesday, February 26 14:00 15:00 K3: Keynote 3 Network Operations and AI Rafia Inam (Ericsson, Sweden) Room: La Grande Scène Chair: Diego Lopez (Telefonica I+D, Spain) Abstract: The Fifth Generation Mobile Networks (5G) are seen as a key enabler for diverse-natured industry verticals (such as automotive, manufacturing, mining, utility, health, etc.) by providing a platform to support heterogeneous sets of network quality requirements. The presentation will discuss how Artificial Intelligence and automation can support Telecom industry to manage the increased complexity, scalability, and diversity in its use cases. The work presents different aspects of the network operations of the future, done in an automated, proactive, and intent-driven fashion using different AI techniques. The Fifth Generation Mobile Networks (5G) are seen as a key enabler for diverse-natured industry verticals (such as automotive, manufacturing, mining, utility, health, etc.) by providing a platform to support heterogeneous sets of network quality requirements. The presentation will discuss how Artificial Intelligence and automation can support Telecom industry to manage the increased complexity, scalability, and diversity in its use cases. The work presents different aspects of the network operations of the future, done in an automated, proactive, and intent-driven fashion using different AI techniques. Wednesday, February 26 15:00 16:10 TS4: Improving Service Performance Room: La Grande Scène Chair: Amina Boubendir (Orange Labs, France) TS4.1 Multimedia Service Management with Virtualized Cache Migration Reza Shokri Kalan (Ege UniversityTurkey, Turkey); Muge Sayit (Ege University, Turkey); Stuart Clayman (University College London (UCL), United Kingdom (Great Britain)) TS4.2 Proposal of Profile and Event Sharing by Agent Communication Masafumi Katoh (Fujitsu Labotatories Ltd., Japan); Tomonori Kubota and Eiji Yoshida (Fujitsu Laboratories, Japan); Yuji Kojima (Fujitsu Limited, Japan); Yuuichi Yamagishi (FUJITSU LIMITED, Japan) TS4.3 Double Mask: An Efficient Rule Encoding for Software Defined Networking Ahmad Abboud (University of Lorraine, France); Abdelkader Lahmadi (INRIA Nancy Grand Est, France); Michael Rusinowitch (INRIA Nancy-Grand Est, France); Miguel Couceiro (University of Lorraine, France); Adel Bouhoula (Higher School of Communication of Tunis & University of Carthage, Tunisia); Mondher Ayadi (Numeryx, France) Wednesday, February 26 16:35 18:00 TS5: Network Security Room: La Grande Scène Chair: Ved P. Kafle (National Institute of Information and Communications Technology, Japan) TS5.1 Neural network based anomaly detection for SCADA systems Lenhard Reuter (AIT Austrian Institute of Technology, Austria); Oliver Jung (AIT Austrian Institute of Technology GmbH, Austria); Julian Magin (AIT Austrian Institute of Technology, Austria) TS5.2 DDoS Detection System Using Feature Selection and Machine Learning Algorithms in a Distributed System Amjad Alsirhani (Dalhousie University, Faculty of Computer Science & Canada, Canada); Geetanshu Grover and Srinivas Sampalli (Dalhousie University, Canada); Peter Bodorik (Dalhousie University, Faculty of Computer Science, Canada) TS5.3 Configuration of the Detection Function in a Distributed IDS Using Game Theory Clement Weill (Institut Polytechnique de Paris & CEA LIST, France); Alexis Olivereau (CEA, LIST, France); Djamal Zeghlache (Insti","2020 23rd Conference on Innovation in Clouds, Internet and Networks and Workshops (ICIN)",2020.0,10.1109/icin48450.2020.9059372,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
14581d947f9856f442af4f707a704cd2cb2a34dc,https://www.semanticscholar.org/paper/14581d947f9856f442af4f707a704cd2cb2a34dc,Supercomputing-supported COVID-19 CT image comprehensive analysis assistant system,"Objective: The Coronavirus Disease 2019 (COVID-19) has become a global pandemic, causing millions of people to be infected worldwide Imaging analysis based on computed tomography (CT) data is an important means of clinical diagnosis A supercomputing-supported method is proposed for the construction of a new comprehensive CT analysis auxiliary system dealing with pneumonia Method: The system consists of four parts: input processing module, preprocessing module, imaging analysis subsystem, and artificial intelligence(AI) analysis subsystem Among the four parts, the imaging analysis subsystem detects the pneumonia features, distinguishes typical new coronary pneumonia by analyzing the typical imaging features, such as lung consolidation, ground-glass opacities, and crazy-paving pattern, and then comes to a conclusion of pneumonia The AI analysis subsystem uses a deep learning model to classify typical viral pneumonia and COVID-19, which enhances the screening ability of pneumonia Convolutional neural network is widely used as an effective algorithm for medical image analysis, particularly in image classification It is also widely utilized in the CT image screening and has achieved good results, which has attracted the attention of domestic and foreign scholars and industry The seasonable result derived from deep learning relies largely on the number and quality of training samples Given the lack of training samples, the system selects transfer learning as the technical direction for model construction Considering the quick response to the epidemic, the quality of easy maintenance and dynamic system updating is required Thus, after comparing and analyzing the performance and classification effect indexes of many common image classification models, we build a transfer learning neural network model on the basis of inception The entire neural network can be roughly divided into two parts: the first part uses a pre-trained inception network;the role of which is to convert image data into a one-dimensional feature vector The second part uses a fully connected network to improve classification prediction The imaging analysis method analyzes the image features of COVID-19, extracts the pneumonia feature areas, and carries out semantic analysis to achieve the delineation of the pneumonia target area Simultaneously, the typical imaging characteristics of COVID-19 (such as ground glass shadow, infiltration shadow, and lung consolidation) are targeted With regard to the pneumonia target area, a multi-level dynamic threshold segmentation is first used to determine the minimum lung tissue area (rectangular region of interest (ROI)) The extraction of the lung tissue area is designed as a normal workflow For each ROI, pixel statistics, threshold segmentation, regional dissolution and expansion, and abnormal proofreading are used to obtain the pneumonia target area Aiming at the relationship between the sizes of the pneumonia target area, a logical filter is established to detect the segmented distribution features and spatial relationship with the outer contour of the lung Then, based on the characteristic relationship of typical new coronary pneumonia, the typical characteristics of new coronary pneumonia are outlined The entire comprehensive analysis platform is built on the basis of the Tianhe artificial intelligence innovation-integrated platform The Tianhe artificial intelligence innovation-integrated platform is based on the hardware fusion supporting the environment of Tianhe supercomputing, cloud computing, and big data, upon which realizes the existing mainstream deep learning framework It is highly encapsulated with the processing model algorithm, forming a visual interactive template development environment covering multiple links such as data loading, model construction, training, verification, and solidified deployment As a service on this supporting platform, CT image comprehensive analysis AI auxiliary system has access to the computing resources, data resources, and external service capabiliti s of the platform and finally achieves the rapid integration and dynamic update of the system during the pandemic Result: After its release, the system has continuously and steadily provided new COVID-19 auxiliary diagnostic services and scientific research support for more than 30 hospitals and more than 100 scientific research institutions at home and abroad, providing important support for combating the epidemic Conclusion: The supercomputing-supported new coronary pneumonia CT image comprehensive analysis auxiliary system construction method proposed in this paper has achieved important application on diagnosis and research It is an effective way to achieve rapid deployment services and provide efficient support for emergencies The system applies artificial intelligence technology using CT imaging to screen for COVID-19 By applying artificial intelligence to the screening of COVID-19 with pneumonia and giving reference opinions for auxiliary diagnosis, the marking and area statistics of the inflammatory regions are improved The system achieves the combination of artificial intelligence traditional machine vision and deep learning technology to distinguish COVID-19 by using CT images The combined route of viral pneumonitis feature extraction based on traditional machine vision and the COVID-19 image classification based on artificial intelligence technology has achieved a comprehensive analysis of medical image features and COVID-19 screening The fast implementation mode of the fusion platform scenario is based on computing power and data support Relying on the Tianhe artificial intelligence innovation-integrated service platform, the platform supports intelligent frontier innovation on the basis of computing power and data, implements an open model of simultaneous research and application, and has a multi-industry training resource model library and large-scale distributed training sources With regard to rapid deployment and other service capabilities, this comprehensive analysis system is also the first public COVID-19 AI-assisted diagnostic system deployed online Analysis based directly on digital imaging and communications in medicine(DICOM) data and video data will effectively improve the analysis efficiency, but it will involve data ethics and security-related issues;however, it is the developing direction that needs to be resolved in the future © 2020, Editorial and Publishing Board of Journal of Image and Graphics All right reserved",,2020.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
5fb2d1b370d268a0761b684de2cacb2a221c44b0,https://www.semanticscholar.org/paper/5fb2d1b370d268a0761b684de2cacb2a221c44b0,Transactional Auto Scaler: Elastic Scaling of Replicated In-Memory Transactional Data Grids,"In this article, we introduce TAS (Transactional Auto Scaler), a system for automating the elastic scaling of replicated in-memory transactional data grids, such as NoSQL data stores or Distributed Transactional Memories. Applications of TAS range from online self-optimization of in-production applications to the automatic generation of QoS/cost-driven elastic scaling policies, as well as to support for what-if analysis on the scalability of transactional applications.
 In this article, we present the key innovation at the core of TAS, namely, a novel performance forecasting methodology that relies on the joint usage of analytical modeling and machine learning. By exploiting these two classically competing approaches in a synergic fashion, TAS achieves the best of the two worlds, namely, high extrapolation power and good accuracy, even when faced with complex workloads deployed over public cloud infrastructures.
 We demonstrate the accuracy and feasibility of TAS’s performance forecasting methodology via an extensive experimental study based on a fully fledged prototype implementation integrated with a popular open-source in-memory transactional data grid (Red Hat’s Infinispan) and industry-standard benchmarks generating a breadth of heterogeneous workloads.",TAAS,2014.0,10.1145/2620001,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
d91b1b996678425418321d9de5b251778eb506d7,https://www.semanticscholar.org/paper/d91b1b996678425418321d9de5b251778eb506d7,Knowledge engineering with semantic web technologies for decision support systems based on psychological models of expertise,"Machines that provide decision support have traditionally used either a representation of human expertise or used mathematical algorithms. Each approach has its own limitations. This study helps to combine both types of decision support system for a single system. However, the focus is on how the machines can formalise and manipulate the human representation of expertise rather than on data processing or machine learning algorithms. It will be based on a system that represents human expertise in a psychological format. The particular decision support system for testing the approach is based on a psychological model of classification that is called the Galatean model of classification. The simple classification problems only require one XML structure to represent each class and the objects to be assigned to it. However, when the classification system is implemented as a decision support system within more complex realworld domains, there may be many variations of the class specification for different types of object to be assigned to the class in different circumstances and by different types of user making the classification decision. All these XML structures will be related to each other in formal ways, based on the original class specification, but managing their relationships and evolution becomes very difficult when the specifications for the XML variants are text-based documents. For dealing with these complexities a knowledge representation needs to be in a format that can be easily understood by human users as well as supporting ongoing knowledge engineering, including evolution and consistency of knowledge. The aim is to explore how semantic web technologies can be employed to help the knowledge engineering process for decision support systems based on human expertise, but deployed in complex domains with variable circumstances. The research evaluated OWL as a suitable vehicle for representing psychological expertise. The task was to see how well it can provide a machine formalism for the knowledge without losing its psychological validity or transparency: that is, the ability of end users to understand the knowledge representation intuitively despite its OWL format. The OWL Galatea model is designed in this study to help in automatic knowledge maintenance, reducing the replication of knowledge with variant uncertainties and support in knowledge engineering processes. The OWL-based approaches used in this model also aid in the adaptive knowledge management. An adaptive assessment questionnaire is an example of it, which is dynamically derived using the users age as the seed for creating the alternative questionnaires. The credibility of the OWL Galatea model is tested by applying it on two extremely different assessment domains (i.e. GRiST and ADVANCE). The conclusions are that OWLbased specifications provide the complementary structures for managing complex knowledge based on human expertise without impeding the end users’ understanding of the knowledgebase. The generic classification model is applicable to many domains and the accompanying OWL specification facilitates its implementations.",,2016.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
9e0090f244d3da646aa472691dad616c2e59475f,https://www.semanticscholar.org/paper/9e0090f244d3da646aa472691dad616c2e59475f,Financial forecasting with Neural Networks and Reservoir Computing,"Financial forecasting has emerged as a key field of study in the last decades as budget planning, and decision-making processes play a vital part in establishing and maintaining a healthy and sturdy business. Last years, Machine Learning and Deep Learning’s concepts have become prominent in the financial industry due to their ability to handle vast amounts of data and extract time-dependent patterns from non-linear relationships along with the -nowadaysexponentially growing availability of computing power. More and more industries and business operations tend to deploy artificial intelligence technology to automize their processes, minimize their risks and optimize their development in terms of increasing their revenues as a consequence of high quality and robust productivity. The wide range of Deep Learning usage in the finance industry extends from security and fraud detection, underwriting, stock marketing predictions, and chatbot advisory. [1][2][3] The possible outcomes and variations of Machine Learning applications are fully capable to cover a wide spectrum of needs and ideas and in this work we have developed a theoretical framework to acquire fundamental understanding of the Deep Learning philosophy and concepts, studying a very popular Neural Network type by the name of Recurrent Neural Networks, and another promising one by the name Reservoir Computing and their predictive abilities on time series of data. In addition to this, our work aims to study systematically and evaluate different ways to convert a dataset into a more «friendly» form, to optimize the predicting ability of the models presented. Conversion of data series to stationary sequences proves to be an inevitable process to carry out time-series analysis efficiently and the second chapter of results is delving into this issue. Finally, this work is also enhanced by a short analysis and description of a «shifting» problem we encountered during different evaluation",,,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
bd368b2deea4d8342f1a3d227df929880d459c6c,https://www.semanticscholar.org/paper/bd368b2deea4d8342f1a3d227df929880d459c6c,Dynamic Pricing for Airline Ancillaries with Customer Context,"Ancillaries have become a major source of revenue and profitability in the travel industry. Yet, conventional pricing strategies are based on business rules that are poorly optimized and do not respond to changing market conditions. This paper describes the dynamic pricing model developed by Deepair solutions, an AI technology provider for travel suppliers. We present a pricing model that provides dynamic pricing recommendations specific to each customer interaction and optimizes expected revenue per customer. The unique nature of personalized pricing provides the opportunity to search over the market space to find the optimal price-point of each ancillary for each customer, without violating customer privacy. In this paper, we present and compare three approaches for dynamic pricing of ancillaries, with increasing levels of sophistication: (1) a two-stage forecasting and optimization model using a logistic mapping function; (2) a two-stage model that uses a deep neural network for forecasting, coupled with a revenue maximization technique using discrete exhaustive search; (3) a single-stage end-to-end deep neural network that recommends the optimal price. We describe the performance of these models based on both offline and online evaluations. We also measure the real-world business impact of these approaches by deploying them in an A/B test on an airline's internet booking website. We show that traditional machine learning techniques outperform human rule-based approaches in an online setting by improving conversion by 36% and revenue per offer by 10%. We also provide results for our offline experiments which show that deep learning algorithms outperform traditional machine learning techniques for this problem. Our end-to-end deep learning model is currently being deployed by the airline in their booking system.",KDD,2019.0,10.1145/3292500.3330746,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
3af91b1be29551c1cf19866f47e7035b7e139343,https://www.semanticscholar.org/paper/3af91b1be29551c1cf19866f47e7035b7e139343,Objective Image Quality Assessment: Facing The Real-World Challenges,"There has been a growing interest in recent years in the development of objective image quality assessment (IQA) models, whose roles are not only to monitor image quality degradations and benchmark image processing systems, but also to optimize various image and video processing algorithms and systems. While the past achievement is worth celebrating, a number of major challenges remain when we apply existing IQA models in realworld applications. These include obvious ones such as the challenges to largely reduce the complexity of existing IQA algorithms and to make them easy-to-use and easy-to-understand. There are also challenges regarding the applicability of existing IQA models in many real-world problems where image quality needs to be evaluated and compared across dimensionality, across viewing environment, and across the form of representations − specific examples include quality assessment for image resizing, color-togray image conversion, multi-exposure image fusion, image retargeting, and high dynamic range image tone mapping. Here we will first elaborate these challenges, and then concentrate on a specific one, namely the generalization challenge, which we believe is a more fundamental issue in the development, validation and application of IQA models. Specifically, the challenge is about the generalization capability of existing IQA models, which achieve superior quality prediction performance in lab testing environment using a limited number of subject-rated test images, but the performance may not extend to the real-world where we are working with images of a much greater diversity in terms of content and complexity. We will discuss some principle ideas and related work that might help us meet the challenges in the future. Introduction Over the past decades, a growing number of researchers and engineers in the image processing community have started to realize the importance of image/video quality assessment (IQA/VQA) [40, 29, 4]. This is not surprising because no matter what image/video processing problems we are working on, the same issues repeatedly come up − How should we evaluate the images generated from our algorithms/systems? How do we know our algorithm/system is creating an improvement between the input and output images, and by how much? How can we know one algorithm/system performs better than another, and by how much? What should be the quality criterion for which the design of our algorithms/systems should be optimized? Since the human eyes are the ultimate receivers in most image processing applications, human subjective visual testing would be a reliable solution. However, with the exponential increase of the volume of image/video data being generated daily, it becomes impossible to address these quality issues in a timely manner by subjective visual testing, which is slow, cumbersome and expensive. Instead, only trusted objective IQA models may potentially meet these needs. In academia, objective IQA has been a hot research topic, especially in the past 15 years [35, 4, 29]. First, the commonly used numerical disotrtion/quality measures in the past − the mean squared error (MSE) and the peak signal-to-noise ratio (PSNR) − have been shown to correlate poorly with perceived image quality [28, 30]. Second, a large number of perceptually more meaningful IQA models have been proposed, including full-reference (where a perfect quality reference image is available when evaluating a distorted image) [35, 4, 29], no-reference (where the reference image is not accessible) [34, 24, 31], and reduced-reference (where only partial information about the reference image is available) models [39, 36, 31, 29]. Third, several design principles have been discovered and repeatedly demonstrated to be useful in the design and improvement of IQA models. These include psychophysical and physiological visibility models [35, 4], the structural similarity (SSIM) approaches [28, 32, 33, 20, 49], the natural scene statistics (NSS) and information theoretic approaches [36, 39, 21, 31], the visual saliency based approaches [50], and the machine learning based approaches [6]. Fourth, a number of subject-rated image quality databases have been created and made publicly available [22, 7, 8, 17, 16, 47]. They provide a common benchmark platform for the evaluation and comparison of IQA models, among which several algorithms have achieved high correlations with the subjective mean opinion scores (MOSs) of the test images [23, 38, 33, 49]. In the video delivery industry, perceptual objective IQA methods such as the SSIM algorithm have been incorporated into many practical hardware and software systems to monitor image/video quality degradations and to test/compare image/video encoders and transcoders [27, 25, 26]. The wide use of SSIM has resulted in a Primetime Engineering Emmy Award given by the Academy of Television Arts and Sciences [1]. The remarkable development and successful deployment of modern IQA methods are definitely worth celebrating. Nevertheless, this does not necessarily mean that the existing IQA models have already met the real-world challenges. Otherwise, they should have made a much stronger impact and become a gamechanging factor in the industry. Using the video delivery industry as an example, even now most practitioners are still equating bitrate with quality in the practical design of video delivery architectures. However, using the same bitrate to encode different video content could result in dramatically different visual quality. Clearly, the perceptual quality of the video itself, which is presumably the ultimate evaluation criterion of the whole video delivery system, has not been placed at the driver’s seat. While it is understandable that quality degradation is inevitable at many stages in the video delivery chain due to practical constraints, the real concern here is that there is no existing protocol to monitor and control such quality degradation. As a result, various tricks have been used to manipulate the video content and network resources are allocated in suboptimal ways, leaving the creative intent of the content producers unprotected. While it is certain that the industry needs to be better informed about the great potentials of making the best use of IQA/VQA models, we believe that an equally important aspect that slows down the process is that the existing IQA/VQA models still do not meet many real-world challenges. In the following sections, we will elaborate some of these challenges and then focus on a specific one, namely the generalization challenge. We wish our discussions on some fundamental ideas could provide some useful insights for the future development of IQA models that may meet these real-world challenges. The Real-World Challenges Here we make a list of real-world challenges, many of which are described in more details through examples of practical scenarios. 1. It is highly desirable to reduce the complexity of the IQA/VQA algorithms so that they can be computed in realtime or in an even faster speed. This is especially useful in time-sensitive applications such as live broadcasting and videoconferencing. Many existing models are far from meeting this challenge. 2. It is essential to make the IQA/VQA scores easy-to-use and easy-to-understand. For example, the raw SSIM score does not have an explicit perceptual meaning, making it difficult to determine what level of SSIM index can warrant an excellent video quality and how much improvement in the SSIM index is sufficient to create visible quality improvement. Mapping the raw scores into a perceptually linear domain that is easily linked to human expressions about image quality is desirable. 3. The same video stream shown on different display devices could result in very different perceptual quality. For example, a strongly compressed video that exhibits very annoying artifacts on a large TV could appear to have fine quality when viewed on the screen of a smartphone. The quality may also change significantly when the video is watched on the same TV but at two different viewing distances, one at the default distance and the other at a very close distance. However, existing IQA/VQA models give the same score based on the video stream only, completely ignorant of the viewing device and viewing condition. 4. In a video-on-demand application, a high-quality highresolution (e.g., 4K) source video may be encoded into multiple video streams of different resolutions (e.g., 1080p, 720p, 360p, 240p, etc.) and different bit rates, aiming for satisfying a variety of user needs. In order to measure the quality of the encoded videos, most existing VQA models cannot be computed because the source (reference) and test videos have different spatial resolutions. 5. An image or video may need to be displayed on a screen that has a spatial resolution higher than that of the image resolution. As a result, spatial interpolation is performed. Again, most existing VQA models are not applicable because the reference and test images have different spatial resolutions. 6. An image or video of imperfect quality (e.g., being compressed at an earlier stage) is received and then transcoded to multiple images or videos with different bitrates and resolutions. Most existing IQA/VQA models are not applicable not only because they do not allow for cross-resolution quality assessment, but also because they assume the original reference image/video to have perfect quality, which is not the case here. How to carry out “degraded reference” IQA/VQA is a major challenge. 7. A high dynamic range (HDR) image (e.g., the pixels are in 10 or more bit depths) is tone mapped to a standard dynamic range (SDR) image (8 bits per pixel) in order to be visualized on an SDR display. There is certainly information loss that we would like to capture. However, most existing IQA models do not apply because they cannot compare images/videos with different dynamic ranges.",IQSP,2016.0,10.2352/ISSN.2470-1173.2016.13.IQSP-205,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
ec4ce9470af03e607d88b5480c32b972516c45ea,https://www.semanticscholar.org/paper/ec4ce9470af03e607d88b5480c32b972516c45ea,"Woodside Energy Ltd: pioneer in cognitive computing, artificial intelligence and robotics","Cognitive computing is a new disruptive technology with the potential to reshape the oil and gas industry across the entire value chain. For Woodside Energy Ltd (Woodside), embracing this technology is an opportunity to save time, drive efficiency and reduce costs. In 2015, Woodside collaborated with IBM and deployed a cognitive computing system (IBM’s Watson) into its business. The system focuses on capturing the vast proprietary database of knowledge on Woodside’s major capital projects. Today, the Watson proof-of-concept has been successfully deployed by the science function into the business and is now under the care of the projects function. Moreover, it is undergoing continuous advances through further machine learning, additional ingestion of documentation and features linking the cognitive computer system with existing subject matter experts. Given the success of the first pilot program, Woodside is continuing to rapidly leverage cognitive technologies in other areas of the business. In mid-2016, Woodside deployed Watson for drilling events using IBM’s Watson Explorer – Advanced Addition. This program identifies and classifies a wide variety of geological drilling events allowing Woodside’s geoscience team to provide more timely and accurate assessment of potential risks for well design. Woodside continues to develop several other business solutions using these platforms in areas as diverse as continuous improvement, business management, maintenance campaigns, legal advice, general management and robotics. This presentation shares Woodside’s lessons and insights derived from its journey across multiple forms of cognitive technology and provides insights as to the state-of-the-art and adaptation of these systems to achieve specific goals.",,2017.0,10.1071/AJ16142,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
1cb1bb3243b2dcb12002e7aa380b860868a6a08d,https://www.semanticscholar.org/paper/1cb1bb3243b2dcb12002e7aa380b860868a6a08d,Dynamic Selection of Virtual Machines for Application Servers in Cloud Environments,,Research Advances in Cloud Computing,2016.0,10.1007/978-981-10-5026-8_8,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
505195a1db461b083ee30141923b1610ec2cb8ee,https://www.semanticscholar.org/paper/505195a1db461b083ee30141923b1610ec2cb8ee,Artificial intelligence and the dreaded 's.,"Numerous industries are being disrupted by growth in new technologies, especially information technologies, and healthcare is no exception. Advances in robotics, wireless sensor networks, 5D printing, and cloud technologies are reshaping countless industries. I am intrigued by the increasing importance of automation, machine learning, and artifi cial intelligence (AI) in healthcare. Let us explore three questions together: • Where are common applications of AI and automation in healthcare? • What implications for physician assistants (PAs) arise from increased automation and AI in caring for patients? • Did AI bring back the ’s that causes any self-respecting PA to cringe? I nearly panicked recently when I caught sight of the following two headlines from online articles about new healthcare technologies, which might lead a person to think the PAs of the future are not people at all. At the very least, I was ready to e-mail the AAPA communications team to combat those pesky apostrophes. The articles actually detailed advances in automation and AI within healthcare. Bright.MD raises another $8M for “virtual physician’s assistant” SmartExam (www.mobihealthnews.com/content/ brightmd-raises-another-8m-virtual-physicians-assistantsmartexam) Healthcare Chatbots: The Physician’s Assistant of the Future? (http://blog.kantarhealth.com/blog/brian-mondry/ 2016/11/28/healthcare-chatbots-the-physician’s-assistantof-the-future) Next, let us sort out AI and automation. According to Merriam Webster, artifi cial intelligence is the capability of a machine to imitate intelligent human behavior. Automation, on the other hand, is the automatically controlled operation of an apparatus, process, or system by mechanical or electronic devices that take the place of human labor. COMMON APPLICATIONS A widely adopted automation in healthcare is appointment reminder software that automatically reminds patients of their upcoming scheduled appointments, with options to customize the message and/or time it is delivered for patient preference. Similarly, missed appointment notifi cation systems can alert a PA to a potentially worrisome pattern of missed appointments for a patient identifi ed as high-risk. Robotics, commonly deployed in areas such as pharmacy and surgery, are automations proven to increase effi ciency and safety. According to CB Insights, about 86% of healthcare provider organizations, life science companies, and healthcare technology vendors are using AI technology. The most common applications seem to fall into one of ten categories: managing medical records and other data; doing repetitive jobs such as analyzing tests, interpreting radiologic studies, and data entry; helping design treatment plans; digital consultation (such as the Babylon app); virtual nurses (such as the Molly app), medication management (such as the AiCure app); drug development; precision medicine; health monitoring; and healthcare system analysis.1 Numerous tech giants are investing heavily in AI applications for healthcare as well, such as Microsoft’s Healthcare NExT initiative and Google’s Deepmind Health.",JAAPA : official journal of the American Academy of Physician Assistants,2018.0,10.1097/01.JAA.0000530302.23280.25,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
fd360f941d85123a026c78504e350ff2073cbc1e,https://www.semanticscholar.org/paper/fd360f941d85123a026c78504e350ff2073cbc1e,Recent Advances in Metamaterials and Metasurfaces,"submission website: https://mc.manuscriptcentral.com/tap-ieee Authors should indicate in their cover letter that their submission is intended for the special issue on “Recent Advances in Metamaterials and Metasurfaces” and indicate “Special Issue” as the type of their manuscript during the submission process. description: In the 15 years since the last (and first) IEEE T-AP Special Issue on Metamaterials guest-edited by R. Ziolkowski and N. Engheta, the fields of metamaterials and, recently, metasurfaces, have evolved tremendously: they have seen development across the electromagnetic spectrum from RF to optical frequencies and found useful counterparts in acoustics and mechanics; they have pushed the boundaries of fabrication technologies and gained a foothold in a multitude of related disciplines from materials science to machine learning; concepts and terminology have been continuously refined, and the resolution of old controversies through rigorous theoretical and experimental validations has yielded a wealth of new and useful perspectives; they have enabled novel functionalities in a host of devices, with intriguing developments in antenna design; and they are now seeing successful practical deployments in a variety of industries. The purpose of this special issue is to draw attention to the latest progress in the understanding, development, and deployment of electromagnetic metamaterials and metasurfaces. Potential topics include, but are not limited to, the following:",IEEE Antennas and Propagation Magazine,2018.0,10.1109/map.2018.2875252,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
3f3f00fc84c5ea2f6fba2dea24d139dfcc2278bb,https://www.semanticscholar.org/paper/3f3f00fc84c5ea2f6fba2dea24d139dfcc2278bb,Visual Model Interpretation for Epidemiological Cohort Studies,"Epidemiological cohort studies investigate the cause and development of diseases in human populations. Conventional analyses are challenged by recently increasing study sizes, which is why the incorporation of machine learning gains popularity. State-of-the-art classifiers are however often hard to interpret – an important requirement in medical applications. This thesis addresses the gap between predictive power and interpretability in the context of cohort study analysis. Main contribution is the development of an interactive visual interface for the interpretation and comparison of probabilistic classifiers. It supports the analysis of important features at both global and individual level, computation of partial dependence, and iterative construction of meaningful feature groups. To analyse the longitudinal influence of features, the user can modify the feature set by removing a feature or replacing its value by a previous examination record. The developed visual interface is evaluated in two case studies in order to test its effectiveness for the generation and validation of research hypotheses. The case studies include a realworld epidemiological cohort study and synthetic data. The results indicate the interface’s usefulness for epidemiological research, but also reveal necessary further work for the deployment into a productive environment.",,2018.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
fd7245e86067764eb59356ce43bb7c14d9685e92,https://www.semanticscholar.org/paper/fd7245e86067764eb59356ce43bb7c14d9685e92,Dynamic management of resources and workloads for RDBMS in cloud: a control-theoretic approach,"As cloud computing environments become explosively popular, dealing with unpredictable changes, uncertainties, and disturbances in both systems and environments turns out to be one of the major challenges facing the concurrent computing industry. My research goal is to dynamically manage resources and workloads for RDBMS in cloud computing environments in order to achieve ``better performance but lower cost"", i.e., better service level compliance but lower consumption of virtualized computing resource(s).
 Nowadays, although control theory offers a principled way to deal with the challenge based on feedback mechanisms, a controller is typically designed based on the system designer's domain knowledge and intuition instead of the behavior of the system being controlled. My research approach is based on the essence of control theory but transcends state-of-the-art control-theoretic approaches by leveraging interdisciplinary areas, especially from machine learning. While machine learning is often viewed merely as a toolbox that can be deployed for many data-centric problems, my research makes efforts to incorporate machine learning as a full-fledged engineering discipline into control-theoretic approaches for realizing my research goal.
 My PhD thesis work implements two solid systems by leveraging machine learning techniques, namely, ActiveSLA and SmartSLA. ActiveSLA is an automatic controller featuring risk assessment admission control to obtain the most profitable service-level compliance. SmartSLA is an automatic controller featuring cost-sensitive adaptation to achieve the lowest total cost. The experimental results show that both of the two systems outperform the state-of-the-art methods.",PhD '12,2012.0,10.1145/2213598.2213614,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
929e14c3a2b367fd906593d32bd6c470f3b1776a,https://www.semanticscholar.org/paper/929e14c3a2b367fd906593d32bd6c470f3b1776a,Predictive analytics methodology for smart qualification testing of electronic components,,J. Intell. Manuf.,2019.0,10.1007/s10845-018-01462-9,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
230c161b7b6a44eb53e4431d1372859d52c65a03,https://www.semanticscholar.org/paper/230c161b7b6a44eb53e4431d1372859d52c65a03,Accelerating Industrial Adoption of Metal Additive Manufacturing Technology,,,2016.0,10.1007/S11837-015-1794-9,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
d94aa70613f5714ec0162ec6a7ef79ea9fdf4f81,https://www.semanticscholar.org/paper/d94aa70613f5714ec0162ec6a7ef79ea9fdf4f81,Towards Effective Prioritizing Water Pipe Replacement and Rehabilitation,"Water pipe failures can not only have a great impact on people's daily life but also cause significant waste of water which is an essential and precious resource to human beings. As a result, preventative maintenance for water pipes, particularly in urban-scale networks, is of great importance for a sustainable society. To achieve effective replacement and rehabilitation, failure prediction aims to proactively find those 'most-likely-to-fail' pipes becomes vital and has been attracting more attention from both academia and industry, especially from the civil engineering field. This paper presents an already-deployed industrial computational system for pipe failure prediction. As an alternative to risk matrix methods often depending on ad-hoc domain heuristics, learning based methods are adopted using the attributes with respect to physical, environmental, operational conditions and etc. Further challenge arises in practice when lacking of profile attributes. A dive into the failure records shows that the failure event sequences typically exhibit temporal clustering patterns, which motivates us to use the stochastic process to tackle the failure prediction task. Specifically, the failure sequence is formulated as a self-exciting stochastic process which is, to our best knowledge, a novel formulation for pipe failure prediction. And we show that it outperforms a baseline assuming the failure risk grows linearly with aging. Broad new problems and research points for the machine learning community are also introduced for future work.",IJCAI,2013.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
2f470cfddc970e8d134aacfa42600f0c399eed48,https://www.semanticscholar.org/paper/2f470cfddc970e8d134aacfa42600f0c399eed48,Evaluating Intrusion Detection Systems for Energy Diversion Attacks,"The widespread deployment of smart meters and ICT technologies is enabling continuous collection of high resolution data about consumption behavior and health of grid infrastructure. This has also spurred innovations in technological solutions using analytics/machine learning methods that aim to improve efficiency of grid operations, implement targeted demand management programs, and reduce distribution losses. One one hand, the technological innovations can potentially lead large-scale adoption of analytics driven tools for predictive maintenance and anomaly detection systems in electricity industry. On the other hand, private profit-maximizing firms (distribution utilities) need accurate assessment of the value of these tools to justify investment in collection and processing of significant amount of data and buy/implement analytics tools that exploit this data to provide actionable information (e.g. prediction of component failures, alerts regarding fraudulent customer behavior, etc.) In this thesis, the focus on the value assessment of intrusion/fraud detection systems, and study the tradeoff faced by distribution utilities in terms of gain from fraud investigations (and deterrence of fraudulent customer) versus cost of investigation and false alarms triggered due to probabilistic nature of IDS. Our main contribution is a Bayesian inspection game framework, which models the interactions between a profit-maximizing distribution utility and a population of strategic customers. In our framework, a fraction of customers are fraudulent they consume same average quantity of electricity but report less by strategically manipulating their consumption data. We consider two sources of information incompleteness: first, the distribution utility does not know the identity of fraudulent customers but only knows the fraction of these consumers, and second, the distribution utility does not know the actual theft level but only knows its distribution. We first consider situation in which only the first source of information incompleteness is present, i.e., the distribution utility has complete information about the actual theft level. We present two simultaneous game models, which have same assumption 3 about customer preferences and fraud, but differ in the way in which the distribution utility operates the IDS. In the first model, the distribution utility probabilistically chooses to use IDS with a default (fixed) configuration. In the second model, the distribution utility can configure/tune the IDS to achieve an optimal operating point (i.e. combination of detection probability and false alarm rate). Throughout, we assume that the theft level is greater than cost of attack. Our results show that for, the game with default IDS configuration, the distribution utility does not use the IDS in equilibrium if the fraction of fraudulent customers is less than a critical fraction. Also the distribution utility realizes a positive “value of IDS” only if one or both have the following conditions hold: (a) the ratio of detection probability and false alarm probability is greater than a critical ratio, (b) the fraction of fraudulent customers is greater than the critical fraction. For the tunable IDS game, we show that the distribution utility always uses an optimal configuration with non-zero false alarm probability. Furthermore, the distribution utility does not tune the false alarm probability when the fraction of fraudulent customers is greater than a critical fraction. In contrast to the game with fixed IDS, in the game of tunable IDS, the distribution utility realizes a positive value from IDS, and the value increases in fraction of fraudulent customers. Next, we consider the situation in which both sources of information incompleteness are present. Specifically, we present a sequential game in which the distribution utility first chooses the optimal configuration of the IDS based on its knowledge of theft level distribution (Stage 1), and then optimally uses the configured IDS in a simultaneous interaction with the customers (Stage 2). This sequential game naturally enables estimation of the “value of information” about theft level, which represents the additional monetary benefit the distribution utility can obtain if the exact value of average theft level is available in choosing optimal IDS configuration in Stage 1. Our results suggest that the optimal configuration under lack of full information on theft level lies between the optimal configurations corresponding to the high and low theft levels. Interestingly enough, our analysis also suggests that for certain technical (yet realistic) conditions on the ROC curve that characterizes achievable detection probability and false alarm probability configurations, the value of information about certain combination of theft levels can attain negligibly small values. Thesis Supervisor: Saurabh Amin Title: Robert N. Noyce Career Development Assistant Professor",,2016.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
8f2a47cd8a04aa7985d36c1106b01f35290be30f,https://www.semanticscholar.org/paper/8f2a47cd8a04aa7985d36c1106b01f35290be30f,Development of 80- and 100- Mile Work Day Cycles Representative of Commercial Pickup and Delivery Operation,"When developing and designing new technology for integrated vehicle systems deployment, standard cycles have long existed for chassis dynamometer testing and tuning of the powertrain. However, to this day with recent developments and advancements in plug-in hybrid and battery electric vehicle technology, no true “work day” cycles exist with which to tune and measure energy storage control and thermal management systems. To address these issues and in support of development of a range-extended pickup and delivery Class 6 commercial vehicle, researchers at the National Renewable Energy Laboratory in collaboration with Cummins analyzed 78,000 days of operational data captured from more than 260 vehicles operating across the United States to characterize the typical daily performance requirements associated with Class 6 commercial pickup and delivery operation. In total, over 2.5 million miles of realworld vehicle operation were condensed into a pair of duty cycles, an 80-mile cycle and a 100-mile cycle representative of the daily operation of U.S. class 3-6 commercial pickup and delivery trucks. Using novel machine learning clustering methods combined with mileage-based weighting, these composite representative cycles correspond to 90th and 95th percentiles for daily vehicle miles traveled by the vehicles observed. In addition to including vehicle speed vs time drive cycles, in an effort to better represent the environmental factors encountered by pickup and delivery vehicles operating across the United States, a nationally representative grade profile and key status information were also appended to the speed vs. time profiles to produce a “work day” cycle that captures the effects of vehicle dynamics, geography, and driver behavior which can be used for future design, development, and validation of technology. Introduction Under DOE-FOA-0001349 FY15 Award for Mediumand Heavy-Duty Vehicle Powertrain Electrification, Cummins and PACCAR jointly proposed the development of a range-extending plug-in hybrid electric Class 6 pickup and delivery truck. The goal of this project is to demonstrate an electrified vehicle that would deliver a minimum of 50% reduction in fuel consumption across a range of representative drive cycles. In addition to achieving the 50% fuel reduction target, the vehicle also needs to demonstrate as good or better drivability and performance while still meeting emissions requirements when compared to existing conventionally fueled baseline vehicles. Most existing duty cycles used to test conventional internal combustion powered vehicles are of a limited time duration. For example, the Hybrid Truck Utility Forum Class 6 Pickup and Delivery cycle is slightly more than one hour. When testing a system using only fuel as its energy source, this is acceptable; a onehour duty cycle can be used to represent the vehicle operation for the entire work day (e.g., fuel consumption in the middle of the day is very similar to fuel consumption at the end of the day). However, with plug-in electric vehicles, the system (battery characteristics and thermal management systems) may operate differently throughout the work day (especially near the end of the day). For example, the available battery energy may be completely spent prior to the completion of the route. A short duty cycle cannot simply be extrapolated. Evaluating the vehicle over the entire work day also provides the ability to interject appropriate stops that are typical of the Class 6-7 pickup and delivery application. These stops can range from several minutes to much longer and can have significant thermal effect on the vehicle and powertrain systems. These stops may also have a large impact on overall duty cycle mileage (and other duty cycle characteristics such as average speed) as the stops may account for roughly half of the work day. As part of the research and development team, the National Renewable Energy Laboratory (NREL) was been NREL/CP-5400-70943. Posted with permission. Presented at WCX 18: SAE World Congress Experience, 10-12 April 2018, Detroit, Michigan.",,2018.0,10.4271/2018-01-1192,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
201a04f7b9d3d4a112ff1e14bc9f5f82de97c631,https://www.semanticscholar.org/paper/201a04f7b9d3d4a112ff1e14bc9f5f82de97c631,The Effectiveness Study of ML-based Methods for Protocol Identification in Different Network Environments,"Due to the wide use of encrypted protocols and random ports, traditional methods that based on port number or packet payload have gradually lose their effectiveness. To address this issue, new methods that based on machine learning techniques become the research hotspots. With many further studies, some research institutions show that MLbased protocol identification methods can generally achieve over 95% accuracy. However, different from most research studies, industry claims that ML-based techniques are hardly to be deployed for practical use due to their high false positives and false negatives. In this paper, different Machine Learning techniques are evaluated for the actual accuracy under different network environments, and a variety of features are tested on different encrypted protocols. The results show that the identification accuracy will go down due to the changed network scale and network environment while the same MLbased models are used under different network environments, and the choices among different Machine Learning techniques, protocol types or statistical features are not critical.",,2015.0,10.14257/IJFGCN.2015.8.2.16,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
25aaf36140d57dcdad739072d562b7d373090d24,https://www.semanticscholar.org/paper/25aaf36140d57dcdad739072d562b7d373090d24,Smart real-time meeting room,"The monumental changes happening in present infrastructure industry can mostly be attributed to developments in Internet of Things. Developing smarter conference rooms in offices have a great scope for innovation and digital transformation. In this paper, we have developed a Smart Meeting Room (SMR) architecture and addressed various imminent issues pertaining to a meeting room environment by providing automation. The Smart Room requirements at real-time are automatically customized and act according to the customer's needs. Due to the developments in Machine to Machine (M2M) communication, we can establish a connection between the devices and the customers on a real-time basis. This enables the smart rooms to interact with the users and act according to their needs as well as automatically establish communication with other meeting rooms. Various types of sensors like temperature sensor, humidity sensor, PIR (Passive Infrared) based motion detector etc. are deployed in the smart room to monitor and enable the actuators. Ericsson's APPIoT framework facilitates smooth discovery, attachment and data sharing between devices in a close proximity. Gnokii and Pidgin systems are used with our architecture to ease the customization and execution of different events. A microcontroller serves as the brain and gateway for the control operations and provides users with additional features such as setting up of the room environment, loading presentations on the display in advance etc. By continuously learning from the environment within the room, the actuators trigger a responsive event automatically to control the room's ambience and the bookings.",2017 IEEE Region 10 Symposium (TENSYMP),2017.0,10.1109/TENCONSPRING.2017.8070069,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
a11b6a988b1342c928f400d229207a5d6ffb586c,https://www.semanticscholar.org/paper/a11b6a988b1342c928f400d229207a5d6ffb586c,Extracting Value from Data Using an Industrial Data Platform to Provide a Foundational Digital Twin,"
 Oil & Gas data currently exists within a world of data silos. Lack of data is not the challenge. A wide variety of data is collected, including sensor values, P&IDs, ERP, and depth-based trajectories. Rather, the challenge pertains to data usefulness. The root of the problem is a combination of factors, including poor data infrastructure, incompatible operational data systems, and restricted data access. All this translates to a low maturity of digitalization across the Oil & Gas industry. To date, digitalization efforts have been limited to pilot projects, proofs of concept and case studies, with no large-scale operationalized projects.
 Aker BP, one of Europe's largest independent Oil & Gas companies, has broken through the typical roadblocks by deploying an industrial data platform across all five of its operational assets. The platform aggregates and processes data from sensors and contextualizes it, structuring it in relation to process diagrams, production information, 3D-models, and event data (maintenance, incidents). Everything linked in the real world is also linked in the platform. This has dramatically reduced the cost of integration and maintenance, while simultaneously enabling scalability, speed of development, and data openness throughout the Aker BP organization. The data platform handles live and historical data for close to 200,000 sensors, with a peak transfer of 800,000 data points per second. Internal and external experts are able to apply state-of-the-art algorithms to visualize and solve critical business problems. A range of third-party applications and data scientists also use the 1+ trillion data points in the platform to create value and support Aker BP's strategy for day-to-day operations and long-term digital transformation.
 To realize the promise of digitalization, unlocking the value of data must be made a priority within the Oil & Gas industry. This paper will describe the implementation of the industrial data platform, explaining how data streamed from many, disparate, underlying systems is contextualized in the data platform to provide a holistic view of all processes and operations, thus creating a foundational digital twin for each asset, ready to empower machine learning applications for optimization and automatization, as well as human-facing applications, such as advanced visualizations and apps for the digital field worker.","Day 1 Mon, May 06, 2019",2019.0,10.4043/29576-MS,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
667d1da01344286cf6f0f8104d1d6ddc2a61eb31,https://www.semanticscholar.org/paper/667d1da01344286cf6f0f8104d1d6ddc2a61eb31,Handbook of sustainable engineering,,,2013.0,10.1007/978-1-4020-8939-8,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
6af5028a8944432e0ddc1ce6203783ae91587525,https://www.semanticscholar.org/paper/6af5028a8944432e0ddc1ce6203783ae91587525,The Interdisciplinary Nature Of Knowledge Discovery Databases And Data Mining,": Data mining and knowledge discovery in databases have been attracting a significant amount of research, industry, and media attention of late. What is all the excitement about? This article provides an overview of this emerging field, clarifying how data mining and knowledge discovery in databases are related both to each other and to related fields, such as machine learning, statistics, and databases. In the last few years, knowledge discovery and data mining tools have been used mainly in experimental and research environments, business user etc. A large degree of the current interest in KDD is the result of the media interest surrounding successful KDD applications, for example, the focus articles within the last two years in Business Week, Newsweek, Byte, PC Week, and other large-circulation periodicals. Unfortunately, it is not always easy to separate fact from media hype. Nonetheless, several well documented examples of successful systems can rightly be referred to as KDD applications and have been deployed in operational use on large-scale real-world problems in science and in business.",,2016.0,10.18535/IJECS/V4I11.16,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
70c4c8f4ad8c0bb89f3d774d0e7a12cfb87f4010,https://www.semanticscholar.org/paper/70c4c8f4ad8c0bb89f3d774d0e7a12cfb87f4010,Modeling Semantic Web Services with OPM/S A Human and Machine-Interpretable Language,"The World-Wide-Web is now a ubiquitous, global tool, used for finding information, communicating ideas, carrying out distributed computation, and conducting business, learning and science. Web services and the Semantic Web are emerging as a powerful infrastructure for distributed computing. However, even though standard methods that define semantics of Web services, such as OWL-S, may aid in the development and deployment of these services, they are hardly designed to be easily understandable and usable by developers. Complexity and lack of accessibility of Web services and the Semantic Web hinders their usage by the information industry. OPM/S, which is based on ObjectProcess Methodology (OPM), offers a bi-modal visuallingual representation that is both intuitive for humans and formal for machines. Utilization of ontologies and interoperability are two issues addressed by the OPM/S modeling environment. Ontologies are expressed as meta-libraries, which are specified in OPM or OWL, and can be dynamically linked to semantic Web services in a distributed environment. Interoperability is achieved using a transparent reuse method that enables dynamic development of Web services and their integration into more complex Web services. Using a running example, the paper presents OPM/S and its mapping to OWL-S. The benefits and shortcomings are discusses and compared with other OWL-S modeling methods.",WebDyn@WWW,2004.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
8b4876609e8fb0cb7b7de41c6a7eb2cda551e17c,https://www.semanticscholar.org/paper/8b4876609e8fb0cb7b7de41c6a7eb2cda551e17c,Developing an Integrated Real-Time Drilling Ecosystem to Provide a One-Stop Solution for Drilling Monitoring and Optimization,"
 The paper provides a technical overview of an operator's Real-Time Drilling (RTD) ecosystem currently developed and deployed to all US Onshore and Deepwater Gulf of Mexico rigs. It also shares best practices with the industry through the journey of building the RTD solution: first designing and building the initial analytics system, then addressing significant challenges the system faces (these challenges should be common in drilling industry, especially for operators), next enhancing the system from lessons learned, and lastly, finalizing a fully integrated and functional ecosystem to provide a one-stop solution to end users.
 The RTD ecosystem consists of four subsystems as shown in architecture Figure 1. (I) The StreamBase RTD streaming system, which is the backbone of the ecosystem. It takes the real-time streaming log data as well as other contextual well data (for example, OpenWells), processes it through analytical models, generates results, and delivers them to the web-based user interface; (II) The analytics models, which include the Machine Learning (ML)/Deep Learning (DL) models, the physics-based models and the stream analytical/statistical models; (III) The digital transformation solution, which wasdesigned to address contextual well data digitization issues to enable real-time physics-based modeling. Contextual well data like bottom hole assemblies (BHAs) and casing programs are challenging to aggregate and deliver to models, as this data is often stored in locations across multiple systems and in various formats. The digital transformation applications are designed to fit into the drilling teams' workflows and collect this information during the course of normal engineering processes, enhancing both the engineering workflow and the data collection process; (IV) the cloud based ML pipeline, which streamlines the original ML workflows, as well as establishes an anomaly detection and re-training mechanism for ML models in production.
 Figure 1 RTD ecosystem architecture
 All of these subsystems are fully integrated and interact with each other to function as one system, providing a one-stop solution for real-time drilling optimization and monitoring. This RTD ecosystem has become a powerful decision support tool for the drilling operations team. While it was a significant effort, the long term operational and engineering benefits to operators designing such a real-time drilling analytics ecosystem far outweighs the cost and provides a solid foundation to continue pushing the historical limitations of drilling workflow and operational efficiency during this period of rapid digital transformation in the industry.","Day 2 Tue, October 01, 2019",2019.0,10.2118/196228-ms,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
913024763dc84e1dfb6830d40ddeb5cd0e8d6cba,https://www.semanticscholar.org/paper/913024763dc84e1dfb6830d40ddeb5cd0e8d6cba,Can One Laptop per Child Save the World's Poor?,"The One Laptop per Child (OLPC) program is one of the most ambitious educational reform initiatives the world has ever seen. The program has developed a radically new low-cost laptop computer and aggressively promoted its plans to put the computer in the hands of hundreds of millions of children around the world, including in the most impoverished nations. Though fewer than 2 million Of OLPC's XO computers have been distributed as of this writing, the initiative has caught the attention of world leaders, influenced developments in the global computer industry and sparked controversy and debate about the best way to improve the lot of the world's poor. With six years having passed since Nicholas Negroponte first unveiled the idea, this paper appraises the program's progress and impact and, in so doing, takes afresh look at OLPC's assumptions. The paper reviews the theoretical underpinnings Of OLPC, analyzes the program's development and summarizes the current state of OLPC deployments around the world. The analysis reveals that provision of individual laptops is a utopian vision for the children in the poorest countries, whose educational and social futures could be more effectively improved if the same investments were instead made on more sustainable and proven interventions. Middle- and high-income countries may have a stronger rationale for providing individual laptops to children, but will still want to eschew OLPC's technocentric vision. In summary, OLPC represents the latest in a long line of technologically utopian development schemes that have unsuccessfully attempted to solve complex social problems with overly simplistic solutions. ********** The One Laptop per Child (OLPC)program is one of the most ambitious educational reform initiatives to date. The program has developed a radically new low-cost laptop computer and aggressively promoted its plans to put laptops in the hands of millions of children around the world, including those in the most impoverished nations. The program's founder and chairman, Nicholas Negroponte, has argued that children can use this new computer to not only teach themselves, but also their family members. (1) This paper argues that the premises and approach of OLPC articulated by Negroponte are fundamentally flawed. The poorest countries targeted by OLPC cannot afford laptop computers for all their children and would be better off building schools, training teachers, developing curricula, providing books and subsidizing attendance. Middle- and high-income countries may benefit from educational use of laptops. However, this can only happen if they devote substantial effort and funding to the kinds of infrastructure development, teacher training, curriculum development, assessment reform and formative evaluation necessary for school laptop programs to work. Unlike Negroponte's approach of simply handing computers to children and walking away, there needs to be large-scale integrated education improvement efforts. (2) OLPC's VISION OCPC's vision is strongly shaped by Negroponte's background and views. Having been the founder of the Massachusetts Institute of Technology's (MIT) Media Lab and an initial investor in Wired magazine, he is not bashful about asserting his idealistic views on the transformative power of new technologies. As he wrote in an influential 1995 book, ""like a force of nature, the digital age cannot be denied or stopped."" (3) The OLPC program represents a marriage of Negroponte's digital utopianism and the constructionist learning theory of Seymour Papert, Negroponte's longtime colleague at MIT. Papert views learning as highly dependent on students constructing ideas and individual laptop computers as essential for carrying out such construction in today's world. He argues that having several students share a computer is as inadvisable as having multiple students share a single pencil. (4) In the OLPC program, Negroponte, Papert and others sought to develop and distribute a low-cost ""children's machine"" that would empower youth to learn without, or in spite of, their schools and teachers. …",,2011.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
f876f5fa530f45c31f5c5a9b739f17c816f88122,https://www.semanticscholar.org/paper/f876f5fa530f45c31f5c5a9b739f17c816f88122,Enterprise trends and opportunities in the age of semantic computing,"There is increasing awareness within enterprises of the many opportunities becoming available through adoption of emerging technologies and tools that utilize machine intelligence. A transition from reliance on conventional technologies and legacy data stores to more knowledge oriented semantic computing capabilities that make use of natural language processing and text analytics, video analytics, ontologies, reasoners, machine learning, Big Data and graph data stores is beginning to occur. Does this signal the beginning of a major shift in enterprise computing? What are some of these evolving trends and business opportunities? What are some of the major enterprise pain points that semantic computing can help to mitigate? How can enterprises best position themselves for successful deployment of these technologies to maximize business value? How is the financial industry embracing semantic computing?",ICSC,2015.0,10.1109/ICOSC.2015.7050763,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
3dac1931e00a8a4a3ee8e7061e99152eb39eb8ed,https://www.semanticscholar.org/paper/3dac1931e00a8a4a3ee8e7061e99152eb39eb8ed,"NICTA ATP lab: Eveleigh, Sydney, Australia","N ICTA (National ICT Austra-lia) is Australia's Information and Communications Technology Research Centre of Excellence. NICTA develops technologies that generate economic, social, and environmental benefits for Australia. NICTA collaborates with industry on joint projects, creates new companies, and provides new talent to the ICT sector through a NICTA-enhanced Ph.D. program. With six laboratories around Aus-tralia and more than 700 people, NICTA is the largest organization in Australia dedicated to ICT research. NICTA's research focuses on the following areas: computer vision, control and signal processing, machine learning, networks, optimization, and software systems. I am a Ph.D. student in the School of Engineering and Information Technologies at the University of Syd-ney. As well as a graduate researcher at NICTA Australian Technology Park (ATP) Laboratory in the Networks Research Group, which aims to improve overall user experience in accessing information and services by looking into ways to leverage the existing and researching into new infrastructure. NICTA provides many resources and other great opportunities that enhance research. My research focuses on large-scale networking systems. In particular we are developing Moana, a new Internet service abstraction. The Internet was originally designed for efficient data communication between end devices, but this is an increasingly ill-fitting model for today's information cen-tric services. We therefore argue for a network service model that instead focuses on interlinking information providers and consumers through a global information network. Moana is a global information storage and dissemination network, which simplifies developing and deploying of information rich applications. Moana loosely couples applications through a global information space that uses a Pub/ Sub communication model to communicate by publishing and subscribing to information. We are aiming to close the gap between the information model of an application and the supporting network service. We hope Moana will inspire the development of new types of information-centric applications by simplification of storage and dissemination and retrieval of information on an Internet-scale. NICTA has strong collaborations with leading research institutions around the world; we frequently get visiting researchers and intern students from all over Australia and the globe. This is a great way to extend our professional network and collaborate with other research institutions. In addition, students are offered a range of short courses for free. These courses are lectured by some of the best LABZ NICTA is all about growth, collectively as a lab and individually as a researcher.",XRDS,2014.0,10.1145/2590779,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
630feefad9164e33cef0e4e51b56ca79f238a2ee,https://www.semanticscholar.org/paper/630feefad9164e33cef0e4e51b56ca79f238a2ee,"Swarm robotics: Robustness, scalability, and self-X features in industrial applications","Abstract Applying principles of swarm intelligence to the control of autonomous systems in industry can advance our ability to manage complexity in prominent and high-cost sectors—such as transportation, logistics, and construction. In swarm robotics, the exclusive use of decentralized control relying on local communication and information provides the key advantage first of scalability, and second of robustness against failure points. These are directly useful in certain applied tasks that can be studied in laboratory environments, such as self-assembly and self-organized construction. In this article, we give a brief introduction to swarm robotics for a broad audience, with the intention of targeting future industrial applications. We then present a summary of four examples of our recently published research results with simple models. First, we present our approach to self-reconfiguration, which uses collective adjustment of swarm density in a dynamic setting. Second, we describe our robot experiments for self-organized material deployment in structured and semi-structured environments, applicable to braided composites. Third, we present our machine learning approach for self-assembly, motivated as a simple model developing foundational methods, which generates self-organizing robot behaviors to form emergent patterns. Fourth, we describe our experiments implementing a bioinspired model in a robot swarm, where we show self-healing of damage as the robots collectively locate a resource. Overall, the four examples we present concern robustness, scalability, and self-X features, which we propose as potentially relevant to future research in swarm robotics applied to industry sectors. We summarize these approaches as an introduction to our recent research, targeting the broad audience of this journal.",it Inf. Technol.,2019.0,10.1515/itit-2019-0003,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
804b626457e7dc365ac15653eab29315b47df9f6,https://www.semanticscholar.org/paper/804b626457e7dc365ac15653eab29315b47df9f6,Anomaly Detection in BACnet/IP managed Building Automation Systems,"Building Automation Systems (BAS) are a collection of devices and software which manage the operation of building services. The BAS market is expected to be a $19.25 billion USD industry by 2023, as a core feature of both the Internet of Things and Smart City technologies. However, securing these systems from cyber security threats is an emerging research area. Since initial deployment, BAS have evolved from isolated standalone networks to heterogeneous, interconnected networks allowing external connectivity through the Internet. The most prominent BAS protocol is BACnet/IP, which is estimated to hold 54.6% of world market share. BACnet/IP security features are often not implemented in BAS deployments, leaving systems unprotected against known network threats. This research investigated methods of detecting anomalous network traffic in BACnet/IP managed BAS in an effort to combat threats posed to these systems. This research explored the threats facing BACnet/IP devices, through analysis of Internet accessible BACnet devices, vendor-defined device specifications, investigation of the BACnet specification, and known network attacks identified in the surrounding literature. The collected data were used to construct a threat matrix, which was applied to models of BACnet devices to evaluate potential exposure. Further, two potential unknown vulnerabilities were identified and explored using state modelling and device simulation. A simulation environment and attack framework were constructed to generate both normal and malicious network traffic to explore the application of machine learning algorithms to identify both known and unknown network anomalies. To identify network patterns between the generated normal and malicious network traffic, unsupervised clustering, graph analysis with an unsupervised community detection algorithm, and time series analysis were used. The explored methods identified distinguishable network patterns for frequency-based known network attacks when compared to normal network traffic. However, as stand-alone methods for anomaly detection, these methods were found insufficient. Subsequently, Artificial Neural Networks and Hidden Markov Models were explored and found capable of detecting known network attacks. Further, Hidden Markov Models were also capable of detecting unknown network attacks in the generated datasets. The classification accuracy of the Hidden Markov Models was evaluated using the Matthews Correlation Coefficient which accounts for imbalanced class sizes and assess both positive and negative classification ability for deriving its metric. The Hidden Markov Models were found capable of repeatedly detecting both known and unknown BACnet/IP attacks with True Positive Rates greater than 0.99 and Matthews Correlation Coefficients greater than 0.8 for five of six evaluated hosts. This research identified and evaluated a range of methods capable of identifying anomalies in simulated BACnet/IP network traffic. Further, this research found that Hidden Markov Models were accurate at classifying both known and unknown attacks in the evaluated BACnet/IP managed BAS network.",,2019.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
04c3c17f734bc5fad75aa6983c6de3e3c455b0a3,https://www.semanticscholar.org/paper/04c3c17f734bc5fad75aa6983c6de3e3c455b0a3,Intelligent quality planning for quality deployment based on the balanced score card: application to the mobile communication industry,"In this paper, we suggest an intelligent quality measures method for the quality deployment based on BSC (Balanced Score Card). More specifically, we unfold them by selecting critical elements from Service Survey data in the mobile communication industry using Factor Analysis and Partial Correlation. Then, we find potential customers who fit into the demographic features of the target customer group identified as a result of applying machine learning modules such as Self Organising Map and Classifiers to the abovementioned data. Finally, we perform loyalty analysis that distinguishes the loyalty group from the potentially profitable one by using the same machine learning modules. The first step relates to the internal perspective of BSC, the second step relates to BSC's customer perspective and the third step relates to the financial perspective of BSC.",Int. J. Manuf. Technol. Manag.,2003.0,10.1504/IJMTM.2003.002530,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
050b64c2343ef3c7f0c60285e4429e9bb8175dff,https://www.semanticscholar.org/paper/050b64c2343ef3c7f0c60285e4429e9bb8175dff,"Automotive big data: Applications, workloads and infrastructures","Data is increasingly affecting the automotive industry, from vehicle development, to manufacturing and service processes, to online services centered around the connected vehicle. Connected, mobile and Internet of Things devices and machines generate immense amounts of sensor data. The ability to process and analyze this data to extract insights and knowledge that enable intelligent services, new ways to understand business problems, improvements of processes and decisions, is a critical capability. Hadoop is a scalable platform for compute and storage and emerged as de-facto standard for Big Data processing at Internet companies and in the scientific community. However, there is a lack of understanding of how and for what use cases these new Hadoop capabilities can be efficiently used to augment automotive applications and systems. This paper surveys use cases and applications for deploying Hadoop in the automotive industry. Over the years a rich ecosystem emerged around Hadoop comprising tools for parallel, in-memory and stream processing (most notable MapReduce and Spark), SQL and NOSQL engines (Hive, HBase), and machine learning (Mahout, MLlib). It is critical to develop an understanding of automotive applications and their characteristics and requirements for data discovery, integration, exploration and analytics. We then map these requirements to a confined technical architecture consisting of core Hadoop services and libraries for data ingest, processing and analytics. The objective of this paper is to address questions, such as: What applications and datasets are suitable for Hadoop? How can a diverse set of frameworks and tools be managed on multi-tenant Hadoop cluster? How do these tools integrate with existing relational data management systems? How can enterprise security requirements be addressed? What are the performance characteristics of these tools for real-world automotive applications? To address the last question, we utilize a standard benchmark (TPCx-HS), and two application benchmarks (SQL and machine learning) that operate on a dataset of multiple Terabytes and billions of rows.",2015 IEEE International Conference on Big Data (Big Data),2015.0,10.1109/BigData.2015.7363874,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
6fda516e3cce22ddfc31f8267587a28d3b71de8f,https://www.semanticscholar.org/paper/6fda516e3cce22ddfc31f8267587a28d3b71de8f,Guest Editorial: Countering Security Issues in the Digital World,"Guest editorial
 The adoption of digital technologies in the oil and gas industry is generating exciting new opportunities to improve performance, profitability, and sustainability, but brings new safety and security challenges across all operations.
 According to a 2017 survey by the Ponemon Institute, cyber attacks are on the rise. Successful breaches per company each year have risen more than 27%, from an average of 102 to 130. The energy and utilities industry, including oil and gas, suffered average annualized losses from cyber crime of $17.2 million per sampled organization, just behind the financial services sector at $18.2 million.
 In June 2017, the NotPetya computer virus affected many companies around the world, including the Russian oil and gas giant, Rosneft. In the same year, another report stated that almost three-quarters of US oil and gas companies had a cyber incident, yet only a handful cited cyber risk as a major concern in their annual reports.
 Cyber Concerns
 As the global oil and gas industry grasps the benefits that digitalization, automation, machine learning, and artificial intelligence can bring to production and profitability, its relatively immature cyber systems are making it an attractive soft target for hackers.
 The traditional focus of cyber security has been on IT, such as the office IT infrastructure. Now, there is an increasing trend for networks on production sites to be connected to wider corporate networks, to allow remote monitoring and control. This increases vulnerability. Managing operational technology, such as control and automation systems, requires both oil and gas operational domain competence, as well as proficiency in general information security.
 The level of threat depends on the level of communications. Cybersecurity can be simplest when data is moving in just one direction, for instance, from the production system into the corporate network. However, if using a remote or centralized control room, the need for protection becomes more pertinent and problematic, as control rooms must be able to alter critical offshore systems. The complexity and challenge for fail-safe security deepens if vendors are able to access and perhaps control equipment on the plant via corporate systems.
 As each rig operation involves a significant number of suppliers and contractors, all deploying safety critical systems, it is vital that the industry introduces controls and security barriers to eliminate any weaknesses.
 Without the understanding and knowledge of how to implement and integrate such systems securely, unnecessary risk and expense can be added to a project. Breaches can lead to lost production; raised health, safety, and environmental risk; costly damages claims; breach of insurance conditions; negative reputational impacts; and loss of licence to operate. Therefore, cybersecurity needs to be a consideration throughout the lifecycle of any project, especially across digital transition activity.",Journal of Petroleum Technology,2019.0,10.2118/0619-0014-JPT,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
c7e9385480c11725f0643d07cce9a9bd39d0a4f6,https://www.semanticscholar.org/paper/c7e9385480c11725f0643d07cce9a9bd39d0a4f6,A Non-Intrusive Appliance Recognition System,"Depleting energy resources and the unstable supply of raw materials call for innovations in the energy industry, such as in energy generation, distribution, and management. Moreover, increasing electricity prices take a toll on consumers that operates within a strict budget. This study in particular focuses on the proper management and utilization of energy from the consumer's perspective. The objective is to develop a non-intrusive appliance recognition system that can identify the appliances that are being used and calculate how much each of these appliances contribute to the total electricity consumption. Installation of the monitoring system, which utilizes a single sensor clamped to the main power line and used to measure the total energy consumption, does not alter the electrical system, thus, non-intrusive. With this system, the homeowner can monitor which appliances are in use and how much energy they consume. Also, this translates to savings for the household when data provided by the system lead to smarter energy-management choices. For this study to be deployable in households, a data-acquisition system to streamline the data-gathering procedure was needed. Also, a machine learning algorithm was trained and implemented to perform the appliance recognition task given input features from the frequency domain of the measured aggregate data from the main power line. Lastly, the system was tested for prediction accuracy and characterized; and then necessary optimizations were implemented.",2019 IEEE International Conference on Internet of Things and Intelligence System (IoTaIS),2019.0,10.1109/IoTaIS47347.2019.8980438,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
8246bcab39de6760227a02e306a0823d86bcb1ad,https://www.semanticscholar.org/paper/8246bcab39de6760227a02e306a0823d86bcb1ad,Introduction to the Creation and Appropriation of Knowledge Systems Minitrack,"The objective of this minitrack is to contribute to the body of knowledge that helps scholars and practitioners increase their collective understanding of (1) how knowledge systems are planned, designed, constructed, implemented, used, evaluated, supported, upgraded, and evolved; (2) how knowledge systems impact the context in which they are embedded; and (3) the human behaviors reflected within and induced through both (1) and (2). By knowledge system, we mean a system in which human participants and/or machines perform work (processes and activities) related to the creation, retention, transfer and/or application of knowledge using information, technology, and other resources to produce informational products and/or services for internal or external customers. Such systems may include, but are not limited to, knowledge management systems, decision systems, social media, expert systems, machine learning systems, and other AI systems as well as any other IT-enabled knowledge management processes. It is the 6th year of the minitrack. We received eleven papers this year and after a rigorous review process, we accepted five papers for publication in the proceedings and presentation at the conference. The first paper, co-authored by Nico Wunderlich and Roman Beck, looks at how the concept of a digital business strategy leads to increased organizational innovativeness and firm performance. The research results reveal that IT knowledge of business employees has a higher positive impact on organizational innovativeness in organizations giving the digital business strategy high importance, whereas the top management team IT knowledge plays a greater role when digital business strategy is given low priority. The second paper, co-authored by Hans-Georg Fill and Felix Haerer, looks at how blockchain technologies can be applied to the domain of knowledge management. To validate the technical feasibility of the approach a first technical implementation is described and applied to a fictitious use case. The third paper, co-authored by Dimitris Karagiannis and Robert Andrei Buchmann, proposes a deployment approach for hybrid knowledge bases using agile diagrammatic means. The proposal is based on the RDF-semantics variant of OWL and leads to a particular type of hybrid knowledge bases hosted by the GraphDB system. The approach aims for complementarity and integration, providing means of creating semantic networks that are amenable to ontology-based reasoning. The fourth paper, co-authored by Kevin Lumbard, Ammar Abid, Christine Toh, and Matt Germonprez, focuses on understanding the types of information utilized in industry with the aim of enabling the development and sharing of methodologies that support the design of competitive products. As such, this paper proposes a framework of information archetypes utilized by designers in industry. The research results reveal two archetypes of information utilized by decision-makers within these companies during the development of new products and services. The fifth paper, co-authored by Olivia Hornung and Stefan Smolnik, presents a systematic review of existing evidence on the role of emotions in KM research. The review shows that despite KM’s long tradition, there is only limited evidence as to how emotions are related to KM, most of which mention emotions as motivation for KM. As a study’s result, the authors identify four research opportunities to further examine certain aspects of the role of emotions in KM. We wish to thank all of the authors who submitted work for consideration in this minitrack. We also thank the dedicated reviewers for the time and effort they invested in reviewing the papers. We believe that the accepted papers contribute to furthering our understanding on the creation and appropriation of knowledge systems. We look forward to discuss these further during our sessions in January 2018. Proceedings of the 51 Hawaii International Conference on System Sciences | 2018",,2017.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
9f3f6ab0c372325af61a43f114070343b36f7dde,https://www.semanticscholar.org/paper/9f3f6ab0c372325af61a43f114070343b36f7dde,Adaptive Autonomous Agent Responses to Targeted Malware Attacks,"We describe the application of machine learning and data mining techniques for defensive autonomous agent responses to targeted cyberattacks. This approach clusters and classifies captured enterprise malware, and fuses the inferred classes with other relevant threat data to detect targeted attacks indicative of malware delivery attempts by advanced persistent adversaries. Defensive autonomous agents, whose behaviors are learned through specialized process modeling algorithms, are then improved through our enhanced situational knowledge of targeted attacks. The autonomous agents are guided by high-level process models, with which human operators interact for orchestrating lower-level autonomous agents. Agent responses are focused on protecting critical cyber assets, leveraging knowledge of potential paths of exploitation through the network. We employ agent-based simulation for rapid testing and refinement of process orchestration and agent behaviors. 1.0 RESEARCH CHALLENGES Organizations undergo continual attacks from a range of threat actors with varying capabilities and intent. When defenders are better able to understand their adversaries, they are better able to respond. Indeed, it has been recognized that incident response would greatly benefit from improved capabilities for malware analysis [1]. Among the most serious threats are advanced adversaries who are targeting a specific organization. Attackers morph their malware to help evade detection by anti-malware systems, and target different individuals in the organization with different malware delivery methods. Since malware campaigns are tracked using hashes computed using byte code of malware, morphing of the malware also makes it difficult to recognize targeted campaigns. Security operations suffer from largely manual processes for correlating attack indicators, making it difficult to keep pace with adversary activities. There are numerous open problems in recognizing targeted malware attacks and mounting adaptive autonomous responses against them. While advanced methods exist for computing malware semantic similarity, malware is readily shared through cybercrime industry, so that features derived from other malware artifacts and context are needed for distinguishing among adversaries. Knowledge about defensive responses by human operators needs to be captured by autonomous agents, and continually updated as defensive processes improve; operators also need to interrogate and orchestrate autonomous agents when needed. Ideally, attack responses should ideally be guided by paths of potential adversary movement through the network, and focused on protecting critical cyber assets. Overall, this problem involves rich webs of interrelated data, which requires a flexible and manageable knowledge base to be maintained and shared among defensive agents. There are also scalability issues, since the space of malware is large and their correlations scale quadratically, as do other kinds of relationships such as potential adversary paths. Adaptive Autonomous Agent Responses to Targeted Attacks PAPER NBR x STO-MP-IST-148 2.0 APPROACH We propose an intelligent autonomous agent architecture for recognizing and responding to targeted malware attacks. Through unsupervised learning on malware features, this discover clusters of related malware indicative of targeted attacks, and then classifies those clusters according to known adversaries. For this, we can leverage our previous work in symbolic interpretation to extract generalized semantics from binaries, for fast similarity matching against large malware repositories [2]. These malware inferences can be fused with other threat information (delivery mechanisms, social engineering employed, known threat actor behaviors, etc.), for more accurate and fine-grained classification of adversary campaigns. This enhanced situational awareness can guide the responses of our defensive autonomous agents, e.g., applying similar responses for similar attacks. We propose to automate the learning of response behaviors through process mining [3]. This extracts hierarchical Markov models from cyber defender event logs, for learning patterns in defender operational processes (e.g., adversary hunters). We then map the discovered lower-level processes to autonomous agents, which communicate with high-level process models for organizational vetting and agent orchestration. The autonomous agents are informed by our knowledge of targeted attacks. Through Monte Carlo simulation and machine learning, the agent models are trained to adapt to different targeted attack situations. Through machine learning of optimal processes, we thus adapt the autonomous agents to best respond to targeted attacks. We define orchestration processes that capture the high-level flow of an organization’s security operations. An agent-based simulation framework then simulates attacker and defender agents, which generate simulation event logs for further iterations of process refinement. The agent responses are guided by an understanding of potentially exploitable paths through the enterprise network [4], as well as historical patterns of communication among mission-critical cyber assets [5]. For knowledge management and situational awareness within this complex web of interrelated information, we leverage our previous work in representing such interrelationships as a knowledge graph [6], with flexible schema-free design and ad hoc query-based analysis and visualization. Figure 2 shows our architecture for responding to targeted malware attacks via autonomous agents. Agents deployed on enterprise endpoint nodes and gateways monitor and detect malware attempts (phishing emails, malicious web sites, etc.). Detected malware are sent to a “malware jail” for quarantine and dynamic behavioral observation and analysis. Malware are unpacked and subjected to deep semantic analysis of binary code [7], which extracts generalized semantics for machine learning features. From these features (along with semantic features from trusted partners sharing malware intelligence), an intelligent response orchestrator compares malware instances with historical archives, combines malware similarity inferences (e.g., cluster matches) with other relevant information (e.g., mined rules for response processes, enterprise security posture, mission dependencies, and external threat intelligence) for deciding optimal responses. The response orchestrator then pushes response decisions to agents deployed on network hosts. Figure 1: Clusters of Related Malware [10].",,2017.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
73bc9526341bc3445dfc48adfcf32d90b5167859,https://www.semanticscholar.org/paper/73bc9526341bc3445dfc48adfcf32d90b5167859,IT troubleshooting with drift analysis in the DevOps era,"Over the past few years, DevOps practices have led to many changes in the software industry. The need for agility has resulted in continuous development and deployment of frequent small updates in IT production systems. However, the ever-changing applications and their IT operations environments challenge existing IT troubleshooting approaches, which generally depend on prebuilt domain knowledge and ignore the frequent changes in the DevOps era. Moreover, the complexity and diversity of application architectures exacerbate the challenges. In this paper, we propose an unsupervised learning based drift analysis tool named CHASER to detect and analyze abnormal changes (referred to as “drifts,” which include configuration errors, processes hanging, etc.), with learned change models and patterns in real time as well as in the root cause analysis. First, we categorize the changes into two distinct groups (static and dynamic state changes) and periodically collect the finer grained changes. Then, we extract the time-series and structural features from these changes and apply statistical and machine learning algorithms to learn models and patterns from historical data. Furthermore, we apply these models and patterns to detect drifts in real time and infer possible root causes of reported errors based on a multidimensional correlation approach to improve the precision. Through experiments and case studies, we demonstrate the capability of CHASER.",IBM J. Res. Dev.,2017.0,10.1147/JRD.2016.2630478,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
9fc7df98f93adb929359512fb2903ad9f10129b8,https://www.semanticscholar.org/paper/9fc7df98f93adb929359512fb2903ad9f10129b8,"Tina Yu, David Davis, Cem Baydar, Rajkumar Roy (eds): Evolutionary Computation in Practice: Studies in Computational Intelligence",,Genetic Programming and Evolvable Machines,2008.0,10.1007/s10710-008-9068-8,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
41f95df93e0d113b2e8a5aa94a73156755ddd649,https://www.semanticscholar.org/paper/41f95df93e0d113b2e8a5aa94a73156755ddd649,"Generalized Net Model of an Automated System for Monitoring, Analysing and Managing Events Related to Information Security","A R T I C L E I N F O : RECEIVED: 02 JUL 2019 REVISED: 08 SEP 2019 ONLINE: 22 SEP 2019 K E Y W O R D S : security information and event management, information security, security tools, security services Creative Commons BY-NC 4.0 Introduction Every day cyber attackers break into networks disguising as employees and delete their tracks as they go. With time against you and with inadequate tools it can take an average of eight months to filter through such massive volumes of data in order to detect and contain the attack. The IBM QRadar Security Intelligence Platform is designed to automatically identify and analyse threads earlier in the attack cycle providing the necessary time to respond. Ivelina Vardeva, ISIJ 43, no. 2 (2019): 257-263 258 Methods Is computer security a problem? Today we are completely dependent on computer networks and information. Business, industry, utilities, and strategic sites bind their processes to computer networks and the Internet. Technology alone cannot solve the problem; they are the only tool we manage. People creating technology and managing information systems and computer networks are not mature, human errors create prerequisites for security breaches. The use of security information and event management (SIEM) systems increases the level of information security in already existing architectures that provide the ability to manipulate the flow of information and manage incidents and events in real-life mode of these systems. In order to man-age realtime security incidents, it is necessary to make a decision before the situation becomes critical. To perform such control and analysis, automated forecasting mechanisms are used based on the accumulated data for the normal operating state of these systems. The automation of real-time decision making is based on mechanisms that determine the state of information security. To enable security analysts to perform investigations, SIEM correlates information such as these examples: Point in time, Offending users, Origins, Targets, Vulnerabilities, Asset information, Known threats. Overview of key SIEM capabilities The key SIEM capabilities include: • Ability to process security-relevant data from a wide variety of sources, such as: o Firewalls o User directories o Proxies o Applications o Routers; • Collection, normalization, correlation, and secure storage of raw events, vulnerabilities, network flows, assets, and threat intelligence data; • Layer 7 payload capture up to a configurable number of bytes from un-encrypted traffic; • Comprehensive search capabilities; • Monitor network and host behaviour changes that could indicate an attack or policy breach such as these examples; • Off hours or excessive usage of an application or network activity patterns inconsistent with historical user profiles; • Prioritization of suspected attacks and policy breaches; • Notification by email, SNMP, and others; GN Model of an Automated System for Managing Information Security Events 259 • Provision of a variety of generic reporting templates. Based on these key capabilities of SIEM, intelligent automated security solutions are taken. They also include automation, dashboard, visualizations, workflows, reporting capabilities. Security intelligence platforms incorporate: • use cases – advanced threat detection, insider threat detection, risk and vulnerability management, critical data and GDPR, incident response, cloud security, compliance; • analytics engine – security analytics, real time detection and user driven analytics (machine learning, powerful search, behavioural analytics, artificial intelligence, threat hunting); • unlimited logging – date store (endpoint network, applications identity vulnerabilities, configuration assets 3th party data stores); • deployment model, that can be on the premise, as a service, cloud, or hybrid. All SIEM tools are an important part of the data security: they aggregate data from multiple systems (described above) and analyse that data to catch abnormal/unconventional behaviour or potential cyberattacks. What the SIEM processes involve is shown in Figure 1.",Information & Security: An International Journal,2019.0,10.11610/isij.4319,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
0f8239f1f300dd4e82abc303e8fc6f9830f01598,https://www.semanticscholar.org/paper/0f8239f1f300dd4e82abc303e8fc6f9830f01598,Application of Music Emotion Recognition for Design of Audio Content in Terms of Affective Gaming,"Music is a medium of communicating human emotion. In fact, a research area for studying on musicality features and variations of perceived emotions in accordance has been unfolded due to this proven fact. It is referred to as Music Emotion Recognition (MER). Various researches are being conducted on determining the relationships among music structural features and perceived emotions. This research in fact, will be focusing on a practical implementation based on MER. The paper interestingly, describes on deploying a basic prototype of applying MER when playing car racing video games. This is performed with the aim of increasing the user interactivity of game players and improving the state of current video game industry to “Affective Gaming” state. The emotions experienced during racing gameplay was analyzed through player annotations. Results depicted that players frequently experience various degrees of Arousal values when playing video games. Hence, Music structural features that contribute to Arousal factor of emotions were selected. Music features are to be extracted from Electronic/Rock music genre and a machine learning model is to be built to predict emotions that will be perceived. The emotions utilized to classify music are “Happy”, “Sad”, “Anger”, “Tension” and stored in a database. This will be connected to the integrated software that will play music accordingly afterwards. The integrated software consists of the classified database, the game engine and a hardware component that captures EEG signals and analyze according to Arousal & Valence dimensions of emotions.",,2019.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
066894fb19cbee73495b0eded1f33fa3c61c1d96,https://www.semanticscholar.org/paper/066894fb19cbee73495b0eded1f33fa3c61c1d96,Mobile sensing and social computing,"With the rapid development of social networks and social environments, mobile sensing has increasingly emerged as one of the most important technologies to develop social computing solutions. Social computing is a general term for an area of computer science that is concerned with the intersection of social behavior and computational systems, providing a programmable combination of contributions from both humans and computers. A key factor for social computing is how social information is collected from the ubiquitous environments and can be widely used to provide social services in mobile environments. Mobile sensing is increasingly becoming part of everyday life, as smartphones are becoming the central personal computational device in people’s lives. Mobile sensing presents several challenges related to wireless sensor networks, machine learning, human–computer interaction, and mobile systems. Sensor-equipped mobile phones can be combined with wireless sensor networks installed in the environment to develop social machines in many sectors of our economy, including business, healthcare, social networks, environmental monitoring, and transportation. Some research efforts on social computing and mobile sensing have been in progress, including mobile sensing algorithms, applications and systems, and methods and techniques to develop virtual societies. This IJDSN Special Issue is an opportunity to bring multi-disciplinary experts, academics, and practitioners together to exchange their experience in the development and deployment of mobile sensing and social computing systems. This Special Issue brings together researchers and developers from industry and academy to report on the latest scientific and technical advances on the application of mobile sensing and social computing and to showcase the latest systems using these technologies. Filipe et al. compile and compare technologies and protocols published in the most recent researches, seeking Wireless Body Area Network (WBAN) issues for medical monitoring purposes to select the most useful solutions for this area of networking. The most important features under consideration in our analysis include wireless communication protocols, frequency bands, data bandwidth, transmission distance, encryption, authentication methods, power consumption, and mobility. WBAN supporting healthcare applications are in early development stage, but offer valuable contributions at monitoring, diagnostic, or therapeutic levels. They cover real-time medical information gathering obtained from different sensors with secure data communication and low power consumption. Filipe et al. demonstrate that some characteristics of surveyed protocols are very useful to medical appliances and patients in a WBAN domain. Marcelino et al. present a solution to overcome barriers between elderlies and their information and communication technology (ICT) usage in order to potentiate all the benefits provided from mobile sensing and social computing. They present a survey on guidelines, standards, and advices regarding usability and accessibility issues when developing solutions for elderly people made having in mind that senior population have singular requirements due to age-related changes and also frequently technological illiteracy. The authors have identified and applied the most important guidelines to their own solution. A prototype was made using responsive design in order to be adaptable to any type of devices. Zong and Wen propose a new approach to calculate the smartphone orientation by detecting the vehicle starting action and then establish the coordinate system relationship between vehicle and smartphone. Furthermore, they trained the classified model offline to match the acceleration characteristics with traveling speed. In the model training process we compared different classification algorithms. Due to enclosed areas and intensive energy consumption, GPS or WiFi sometime are invalid. In this paper, Zong and Wen propose a new approach to estimate the traveling speed after analyzing the acceleration characteristics in time domain and frequency domain. Shuyun et al. propose a method used to calculate the link importance degree index, and the index is used to evaluate the link’s information. Besides, a multiobjective optimization model is proposed, its aim is to minimize the total cruise time under detecting as many important links as possible and minimize the information value undetected by unmanned aerial vehicles (UAVs), and the fuzzy operator is introduced to the constraint conditions. Finally, a case study is used to",Int. J. Distributed Sens. Networks,2016.0,10.1177/1550147716665512,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
920b236f530422313ebccf5d43f7d3dd9103beba,https://www.semanticscholar.org/paper/920b236f530422313ebccf5d43f7d3dd9103beba,"International Conference on Computing and Artificial Intelligence ( ICCAI 2019 ) April 19-22 , 2019 Bali , Indonesia","Recently, deep learning (DL) plays important roles in many academic and industrial areas especially in computer vision and image recognition. Deep learning uses a neural network with deep structure to build a high-level feature space. It learns data-driven, highly representative, hierarchical image features, which have proven to be superior to conventional hand-crafted low-level features and mid-level features. In ILSVRC2015 (an Annual competition of image classification at large scale), higher recognition accuracy by deep learning than human has been achieved. Deep learning (DL) has also been applied to medical image analysis. Compared with DL-based natural image analysis, there are several challenges in DL-based medical image analysis due to their high dimensionality and limited number of labeled training samples. We proposed several deep learning techniques for medical image analysis including medical image segmentation, medical image detection and medical image recognition. In this keynote talk, I will talk about current progress and futures of medical image analysis with deep learning. ICCAI 2019 CONFERENCE ABSTRACT 12 Keynote Speaker IV Prof. Qijun Zhao Sichuan University, China Prof. Qijun Zhao is currently a professor in the College of Computer Science at Sichuan University. He obtained his B.Sc. and M.Sc. degrees in computer science both from Shanghai Jiao Tong University, and his Ph.D. degree in computer science from the Hong Kong Polytechnic University. He worked as a post-doc research fellow in the Pattern Recognition and Image Processing Lab at Michigan State University from 2010 to 2012. His recent research interests lie in 3D face modeling and recognition, with applications to forensics, intelligent video surveillance, mobile security, healthcare, and human-computer interactions. Dr. Zhao has published more than 60 papers in academic conferences and journals, including CVPR, ECCV, AAAI, ICB, IEEE Trans., and PR. He is the principal investigator for two projects funded by NSFC, one project funded by the National Key Research and Development Program of China, and many projects funded by companies. Dr. Zhao is a reviewer for many renowned field journals and conferences, such as IEEE TPAMI, IEEE TIFS, IJCV, PR, PRL, ICCV, CVPR, ECCV, and FG. He served as a program committee co-chair in organizing the 11th Chinese Conference on Biometric Recognition (CCBR 2016), the 2018 IEEE International Conference on Identity, Security and Behavior Analysis (ISBA), and the 2018 6th International Conference on Bioinformatics and Computational Biology (ICBCB 2018), and as a face recognition area co-chair for the 9th IEEE International Conference on Biometrics: Theory, Applications, and Systems (BTAS 2018). Topic: ""3D Face Reconstruction in Recognition Perspective"" Abstract—The face reveals a lot of information of humans, for example, identity, race, gender, age, emotion, intention, and health. 3D face models are thus widely studied in many disciplines. Yet, acquisition of 3D faces is still much more expensive and less convenient than acquisition of 2D face images, making it unaffordable to deploy 3D face technology in many real-world applications. Our research aims to reconstruct 3D face shapes from either single or multiple uncalibrated 2D face images from a perspective of identity recognition. This talk will introduce our recent progress along this direction. The methods we propose enable not only efficient generation of 3D face models when only 2D imaging devices are available, but also effective exploration of 3D face information for improving face recognition accuracy. We believe that 3D faces will play increasingly important roles in many applications with the rapid development of both 3D face acquisition techniques and 3D face modeling methods.The face reveals a lot of information of humans, for example, identity, race, gender, age, emotion, intention, and health. 3D face models are thus widely studied in many disciplines. Yet, acquisition of 3D faces is still much more expensive and less convenient than acquisition of 2D face images, making it unaffordable to deploy 3D face technology in many real-world applications. Our research aims to reconstruct 3D face shapes from either single or multiple uncalibrated 2D face images from a perspective of identity recognition. This talk will introduce our recent progress along this direction. The methods we propose enable not only efficient generation of 3D face models when only 2D imaging devices are available, but also effective exploration of 3D face information for improving face recognition accuracy. We believe that 3D faces will play increasingly important roles in many applications with the rapid development of both 3D face acquisition techniques and 3D face modeling methods. ICCAI 2019 CONFERENCE ABSTRACT 13 Keynote Speaker V Assoc. Prof. Ken‘ichi Morooka Kyushu University, Japan Assoc. Prof. Ken’ichi Morooka received his M.S. and Ph.D. degrees from Kyushu University, in 1997 and 2000, respectively. He was a visiting researcher with Institute of Systems & Information Technologies/KYUSHU. From 2000 to 2006, he was an associate professor in Graduate School of Science and Engineering, Tokyo Institute of Technology. He was an associate professor in Digital Medicine Initiative (2006-2010) and Department of Medical Sciences, Kyushu University (2010). Currently, he is an associate professor in Graduate School of Information Science and Electrical Engineering, Kyushu University. Also he was a visiting researcher, Illinois Institute of Technology, U.S. (2016). He has published more than 100 journal and conference articles. He has served as a member of organizing and program committees at numerous conferences, e.g. he has been program committes of MLMI 2018 and 2017, IFMIA 2017, CARS 2014 and EMBC 2013. His research interests cover computer-aided support system for therapy and surgery by image information processing and machine learning. Topic: ""Computer Aided System for Minimally Invasive Surgery Using Deep Learning"" Abstract—Recently, deep neural networks (DNNs) have been paid attention by various research fields including vision, audio and natural language. Of course, there are many DNN-based systems for therapy and diagnosis. Our research group has been doing research about computer-aided support systems for safe and accurate minimally invasive surgeries. Especially, to provide useful information for surgeons, our support systems use stereo endoscopic images, DNNs and 3D shapes and deformations of organs. I will present the fundamental techniques of our support system.Recently, deep neural networks (DNNs) have been paid attention by various research fields including vision, audio and natural language. Of course, there are many DNN-based systems for therapy and diagnosis. Our research group has been doing research about computer-aided support systems for safe and accurate minimally invasive surgeries. Especially, to provide useful information for surgeons, our support systems use stereo endoscopic images, DNNs and 3D shapes and deformations of organs. I will present the fundamental techniques of our support system. ICCAI 2019 CONFERENCE ABSTRACT 14 Invited Speaker Assoc. Prof. Sugiono Sugiono Brawijaya University, Indonesia Sugiono, Ph.D was born in Blitar, Indonesia, in 1978. He finished Bachelor degree in Mechanical Engineering Department at Brawijaya University in 2001, received Master Degree in Industrial Engineering at Sepuluh Nopember Institute of Technology, Surabaya in 2004, and graduated Ph.D. degree of Art, Design and Technology from University of Derby, UK, in 2012. Title of his thesis (PhD) is: Investigating an Intelligent Concept Design Tool for Automotive Car Body Design. His research interests lie in bioengineering ergonomics and intelligent product design. He worked as project analyser in investigating of fuel distribution for industry at PT. Surveyor Indonesia from 2001 to 2002. He also worked as purchasing vice leader at PT. Mitra Saruta (Textile) from 2004 to 2005. Currently, he is working as a lecturer at Department of Industrial Engineering, Brawijaya University start from 2005. He is a head of Work Design and Ergonomics Laboratory and head of Research Committee at Brawijaya University. He is an international reviewer of research, certificated by ISO 17024. He is also working as editor in chief of the Indonesian Journal of Disability Studies (IJDS). He is a senior member of Hong Kong Chemical, Biological and Environmental Engineering Society (HKCBEES), member of Indonesian Ergonomics Society (Perhimpunan Ergonomi Indonesia – PEI) and Member of International Association of Engineers (IAENG). Topic: ""The Importance of Open Innovation Concept to Improve Health and Safety Factors in Transportation"" Abstract—Controlling driver stress level is going to be popular research and put it a very important factor to reduce the risk of a road accident. Understanding the role of road complexity and information technology in transportation issues and their relationship with humans psychophysiological is a good challenge and profitable prospect for the future. Images from the Electrocardiograph (ECG) and Electroencephalography (EEG) are the important tools to identify the driver stress as part of a safety alert system. The Electrocardiograph (ECG) is to monitor every heart rate change and Electroencephalography (EEG) is to record brain signal change correlated with brain functions (thinking, visual, decision, etc.) from three different road types (city road, rural road, and motorway). In this speech, I will deliver a potential open innovation of health and safety factors in transportation (car, train) from the perspective of interaction among human, car, and environment.Controlling driver stress level is going to be popular research and put it a very important factor to reduce the risk of a road accident. Understanding the role of road complexity and information t",,2019.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
97ea92535b20ce388f0a1541cd0d8f66602cc93c,https://www.semanticscholar.org/paper/97ea92535b20ce388f0a1541cd0d8f66602cc93c,CoSAC: Coordinated Session-Based Admission Control for Multi-Tier Internet Applications,"Popular Internet applications deploy a multi-tier architecture, with each tier provisioning a certain functionality to its preceding tier. The session-based admission control approach (SBAC) designed for a single Web server is not effective for a multi-tier architecture. This is due to the fact that the bottleneck in a multi-tier website dynamically shifts among tiers as client access patterns change. Admission control based on only the bottleneck tier is not efficient as different sessions impose different resource consumptions at the different tiers. In this paper, we propose a multi-tier measurement based admission control (MBAC), which pro-actively accepts different session mixes based on the utilization state of all tiers. More significantly, we design a novel coordinated session-based admission control approach (CoSAC) based on a machine learning technique. It uses a Bayesian network to correlate the states of all tiers. The probability with which a session is admitted is determined by the probabilistic inference of the network after applying the evidence in terms of utilization and processing time at each tier to the network. We compare CoSAC with MBAC and a black- box approach tailored from SBAC, using the industry standard TPC-W benchmark in a typical three-tier e-commerce website. Experimental results demonstrate the superior performance of CoSAC with respect to the effective session throughput. I. INTRODUCTION Today, popular Internet applications deploy a multi-tier architecture, with each tier provisioning a certain functionality to its preceding tier and making use of the functionality",2009 Proceedings of 18th International Conference on Computer Communications and Networks,2009.0,10.1109/ICCCN.2009.5235319,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
2a9a13b305ee3d41a2061ae9ae22cbdea8636b38,https://www.semanticscholar.org/paper/2a9a13b305ee3d41a2061ae9ae22cbdea8636b38,"The Impact of Interactive Whiteboards on teaching, Learning and Attainment","During the past three years the UK government has provided increasing funds for schools to invest in interactive whiteboard technology, together with research initiatives to evaluate the effectiveness of practitioner use. MirandaNet has been involved in the evaluation of interactive whiteboard technology and use for the past five years. This paper examines studies led and managed by MirandaNet, through the MirandaNet International Research Centre and MirandaNet Academy, into the ways in which interactive whiteboards have been used as essential tools to develop and implement school teaching and learning policies. The paper describes schools’ ICT diffusion and integration strategies to enhance all aspects of teaching and learning through school improvement policies. The impact of the work on the schools will be outlined, together with an assessment of the changes in attitudes and attainment. The Impact of Interactive Whiteboards on Teaching, Learning and Attainment During the past three years the UK government has provided increasing funds for schools to invest in interactive whiteboard (IWB) technology. This technology has an impact on whole-class, group and individual teaching and learning, with the ability to extend the range of teaching and learning styles deployed in a classroom. This investment has been paralleled with a number of research initiatives to evaluate the effectiveness of practitioner use of interactive whiteboards in raising attainment and transforming learning (Becta 2003, 2004; ImpaCT2, 2002; Review Project, 2004). MirandaNet, a collaborative online community of teachers, teacher educators, researchers and industry partners, has been involved in the evaluation of interactive whiteboard technology and use for the past five years. It has involved teachers, schools and pupils in research into effective IWB use through its practitioner-led action research projects (Cuthell, 2002; 2003; 2004) published on the MirandaNet website. This paper examines one series of such studies led and managed by MirandaNet, through the MirandaNet Academy, into the ways in which interactive whiteboards have been used as essential tools to develop and implement school teaching and learning policies. The schools range from Early Years (K – 2) through to Sixth Form (Grades 12 & 13) and cover the United Kingdom from the North East to the South Coast. Further, the impact on teaching and learning in each of these schools, and the ways in which Information and Communication Technology (ICT) diffusion and integration within the curriculum is enhanced, is described, The collaborative creation of innovative curriculum materials and the optimisation of the range of learning styles is outlined, together with differentiation of materials and presentation to facilitate an inclusive classroom. The use of IWB as a tool in Assessment for Learning is explored. Pupil and teacher attitudes and perspectives are evaluated. The impact of IWB pedagogies on the schools’ Teaching and Learning policies will be outlined, together with an assessment of the changes in attitudes and attainment. The context for this work is provided by an innovative Continuous Professional Development programme of action research (Work-Based Learning) that accredits teachers at Masters level for units of work that they complete. The MirandaNet Academy provides the collaborative online environment and Bath Spa University College provides the academic framework for the scheme within the UK. MirandaNet and Promethean offer similar schemes worldwide through the Promethean Ambassadors programme. Current projects involve schools in the UK, the US, China, South Africa and Mexico. Teaching and Learning Policies During the past three years a number of schools in the United Kingdom have developed policies on teaching and learning that involve all curriculum areas. The focus of these policies is the involvement of the learner: teachers and their departments have worked to develop strategies to involve all learners, and to produce differentiated materials that support a range of learning styles and abilities. The use of IWB means that teachers can develop teaching resources that are available across the school local area network (LAN) and that can easily be modified to reflect the learning needs of pupils. Collaborative work by teachers is enhanced: resources are developed more quickly; departments can consolidate materials for each curriculum segment and the teachers themselves are presented with a greater range of pedagogical possibilities than they would otherwise have developed individually. Whilst it is tempting to react to what may be considered the imposition of a certain uniformity in teaching styles – many teachers hold an idealised Hollywood image of the teacher as a charismatic individual changing people’s lives – the reality is that the availability of such a wide range of resources in fact sets teachers free to interact with learners. Formulaic though it may be, the ability to pull up a list of learning aims at the beginning of each lesson, identify them in the plenary section of the lesson to consolidate elements for the learner and then use these as the start of the following lesson provides a structure to which many of the learners can relate. When these are made available in the learners’ section of the school LAN learners are placed in control of their own learning. They can revise previous lessons, catch up on work they have missed and place their learning in an overall curriculum context. The concept of school teaching and learning policies is one of rights and responsibilities, and schooling is thus placed in a context that emphasises social responsibility for one’s own, and others’, learning, as well as individual rights. The impact of ACTIVboards on teaching and learning There has been considerable debate in the United Kingdom over the importance of ‘interactive’ in the phrase ‘interactive whiteboards’. Some would hold that the key determinant is the use of the board itself by pupils – their ability to respond to the materials presented on the board in an active fashion. By these criteria interactivity is judged by the number of times pupils come to the board and interact with it. Whether this is real interactivity is a moot point because, much as pupils enjoy the opportunity to leave their desk and activate the board, the reality is that when this happens it is under the control of the teacher. Maybe the key factor is interactivity with the process of the lesson. In this reading what is important is the intellectual and emotional engagement of the learner. One can observe an apparently traditional classroom, in which desks are laid out in rows and face the IWB and teacher at the front of the room, and see that learners are engaged in learning that is communally constructed (Holmes et al, 2001). The teacher is a central part of this process, but the questions and responses of the learners provide the interactivity with the learning materials. The boards provide the focus for visualisation; the pedagogical software (in this case ACTIVstudio and flipcharts) provides the scaffolding and both teachers and learners are able to move between the different stages of learning. The effects of IWB use on ICT diffusion How can teachers maximise their use of Information and Communications Technology? That question is predicated on the assumption that resources and facilities are readily available to all teachers. Are ICT facilities available to all teachers at all times? Many schools have adopted the policy of concentrating resources in computer suites: integrating ICT into lessons then becomes problematic. The facilities are ‘owned’ by someone other than the class teacher: the room has to be booked; the network is managed to minimise the unexpected; control of the machines is vested elsewhere – whether at district, school or departmental level – and the net result is to make ICT a discrete activity, rather than one that is woven into the fabric of teaching and learning. Other schools have adopted a wireless policy, embedding a Wi-Fi network into the framework of the building and providing laptops for both staff and students. Whilst this approach can be liberating – computing power can be taken to wherever it is needed, whether in the laboratory, the sports field or the external environment – there are limitations in its use. Old buildings often have dead spots; at times of peak demand the network can prove less than reliable; re-charging laptop batteries requires careful management. Perhaps the most problematic issue is that of laptop ownership. Students often treat them as another consumer good; there are issues of security when connecting to the network; reliability is difficult to guarantee. (Cuthell & Preston, 2005) Whatever solution is adopted by teachers and schools, however, the central issue is the integration into the lesson. Are the computers used for task completion? If they are, then what is the balance between learning and activity in the classroom? Are the computers used for learning? If so, what is the role of the teacher – and is learning an individual, or communal activity? The use of an Interactive Whiteboard, on the other hand, provides a way in which the board becomes the focus of learning for the whole class, and the teacher is able to deploy all of the affordances of ICT to facilitate the learning of her students. The computer network becomes the repository of teaching and learning materials; the materials prepared by the teacher can be modified and updated; there is no reliance on old, paper-based worksheets and, most importantly of all, the teacher finds that her learning and that of the pupils comes together through the focus of ICT. ICT diffusion is therefore existential, a way of thinking, rather than the physical manifestation of an inventory. Assessment for Learning Teachers have always used assessment as a way of judging the success o",,2005.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
0ab9bb2ca47ee53e124bd19261f0bc92a6bb20ec,https://www.semanticscholar.org/paper/0ab9bb2ca47ee53e124bd19261f0bc92a6bb20ec,Proceedings of the 2016 on International Symposium on Physical Design,"On behalf of the organizing committee, we are delighted to welcome you to the 2016 ACM International Symposium on Physical Design (ISPD), held at Santa Rosa, California. Continuing the great tradition established by its twenty-four predecessors, which includes a series of five ACM/SIGDA Physical Design Workshops held intermittently in 1987-1996 and nineteen editions of ISPD in the current form since 1997, the 2016 ISPD provides a premier forum to present leading-edge research results, exchange ideas, and promote research on critical areas related to the physical design of VLSI and other related systems. 
 
The regular papers in the ISPD 2016 program were selected after a rigorous, month-long, doubleblind review process and a face-to-face meeting by the Technical Program Committee (TPC). The papers selected exhibit latest advancements in a variety of topics in physical design, including 3D integration, directed self-assembly, timing and clock optimization, reliability, adaptability, and the latest machine-learning based techniques for computer-aided design. 
 
The ISPD 2016 program is complemented by two keynote addresses, twelve invited talks and a tribute session, all of which are delivered by distinguished researchers from both industry and academia. Dr. Kevin Zhang, vice president at Intel Corporation, will talk about circuit design in nano-scale CMOS technologies in the Monday keynote speech. In the Tuesday keynote speech, Dr. Antun Domic, executive vice president at Synopsys, will present his observations on the physical design problems of the next decade. A commemorative session on Tuesday afternoon will pay tribute to Prof. Ralph Otten. His collaborators will share with us Dr. Otten's exceptional contributions to EDA research, including his pioneering effort in automated floorplanning and layout generation. There will be two special sessions and other invited talks interspersed with the presentations of the regular papers. The topics of the invited papers range from emerging technologies such as 3D ICs, directed self-assembly, neurosynaptic chips, sub-7nm designs, as well as the latest challenges in physical design of ICs and FPGAs. 
 
Since 2005, ISPD has organized highly competitive contests to promote and advance research in placement, global routing, clock network synthesis, discrete gate sizing, and detailed routingdriven placement. Unlike previous years' contests which are mostly on ASIC designs, this year's contest, organized by Xilinx, is on routability-driven FPGA placement. This is the first large-scale worldwide contest on FPGA placement optimizing towards important real-world objectives based on a realistic heterogeneous FPGA architecture. The contest evaluates the quality of an FPGA placement with an advanced commercial FPGA routing tool making the problem even more challenging and practical. It is expected to lead and motivate more research and contributions on FPGA physical design. Continuing the tradition of all the past contests, a new large-scale realworld benchmark suite for FPGA circuits based on an advanced heterogeneous FPGA architecture will be released in the ISPD website: http://www.ispd.cc",ISPD,2016.0,10.1145/2872334,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
4cb7a7865d4e529a570f8f42bfe6943699b5d1fa,https://www.semanticscholar.org/paper/4cb7a7865d4e529a570f8f42bfe6943699b5d1fa,Developing Engaging Content,"Companies need training that is not only flexible and affordable, but can be deployed and consumed quickly. Enter eLearning. Trainers have the dual challenge of helping employees meet compliance or other industry requirements and respond to changing customer needs. These pressures mean trainers must create new online courses on the fly while ensuring the content is easily consumable to reinforce rapid learning. By using existing material and frequently measuring feedback, among other ideas, you can transform your online training program into a highly efficient machine for learning and competing.",ELERN,2014.0,10.1145/2591688.2594794,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
faa59797f53cd706359f1b9dde2d54197186e40c,https://www.semanticscholar.org/paper/faa59797f53cd706359f1b9dde2d54197186e40c,IMECE 2007-43720 Sensor Fusion for Machine Condition Monitoring,"Machinery maintenance accounts for a large proportion of plant operating costs. Compared with the conventional scheduled maintenance strategy which is to stop the machine at pre-determined intervals, modern condition-based maintenance strategy stops the machine only before there is evidence of impending failure. With the development of cheaper sensors, more and more sensors are designed for machine condition monitoring. It is now possible to use multi-modal sensor input to monitor machine condition in a collaborative and distributed manner. In this paper, three categories of methods for condition monitoring are reviewed – 1. knowledge based, 2. model based 3. data based methods. Knowledge-based systems are derivations from expert systems that use rules and inference engines to determine failures and their causes. Data-driven methods use machine fault data, typically derived during experiments to train a monitoring system. Pattern recognition algorithms then attempt to classify actual sensor data using the results of the training phase. However it is often impractical to obtain data for every type of fault. Model-based techniques on the other hand use mathematical models to predict machine performance. We propose to combine the model based and data based method for machine condition monitoring. The data is used to train the model and derive its parameters. The various fault modes are then identified and simulated. The output is then input to the classification schemes that can be then used to identify and classify real-time data. We apply the technique to condition monitoring of electrical motors. INTRODUCTION Machinery maintenance accounts for a large proportion of plant operating costs. It has been clearly demonstrated that the use of appropriate condition monitoring and maintenance management techniques can give industries significant improvements in efficiency and directly enhance profitability [1]. Condition monitoring or CBM (Condition Based Maintenance) is an effective form of predictive maintenance (PdM) where, as the name stated, people monitor the condition of specific areas of plant and equipment. CBM involves the observation of a system over time using periodically sampled dynamic response measurements from an array of sensors, the extraction of fault-sensitive features from these measurements, and the statistical analysis of these features to determine the current state of the system [2]. It is also referred to fault detection, fault isolation and identification. The use of CBM allows maintenance to be scheduled, or other actions to be taken to avoid the consequence of failure, before the failure occurs. It is typically much more cost effective than allowing the machinery to fail [1]. Over the past sixty years major improvements have occurred in the technology, practice and systems used for equipment condition measurement. Since 1939, vibration measurements have been used to judge the condition of machinery [3]. However, wireless technology [4] has only recently been developed and deployed for vibration-based condition monitoring. Besides sensor and signal processing technology, there have been significant developments in the architectures and methodologies to perform condition monitoring. Current existing techniques can be classified into three categories: knowledge based, model based, and data based methods. These methods are reviewed in the next section, followed by a review of popular frameworks of sensor fusion. A new combined method is proposed for machine condition monitoring. REVIEW OF CURRENT METHODS Knowledge based methods Knowledge based methods mainly perform automated reasoning to carry out situation assessment, consequence prediction and analysis [5]. One of the first machinery expert systems (Amethyst) was introduced by IRD in the mid 1980s. Also in the mid 1980s, Predict DLI developed an expert system for use with the US Navy aircraft carrier Condition Based Maintenance program for which they provided data and analysis [3]. This knowledge-based method is still popular for making expertise available to decision makers and technicians who need answers quickly. The most common form of expert systems is a program made up of a set of rules that analyze information (usually supplied by the user of the system) about a specific class of problems and recommend a course of action to implement corrections [6]. A typical representation of knowledge would be presented by rules that have the form: If (evidence exists for X) then do Y, where Y may involve performing a computation or updating a database. The rules, however, need to programmed into the system based upon opinions of human experts and can therefore be prone to subjectivity. Model based methods The basic principle of a model-based fault detection scheme is to generate residuals that are defined as the differences between the measured and the predicted variables. Ideally, these residuals are only affected by system faults and are not affected by any changes in the operating conditions, such as power quality changes or load variations [7, 8]. So the key point of this method is to find a variable which can be well modeled and measured to generate the residual signal which is not always available in some systems. Data based methods Many modern approaches to fault diagnosis and prognosis are based on the idea of pattern recognition (PR). In the broadest sense, a PR algorithm is simply one that assigns to a sample of measured data a class label, usually from a finite set. The appropriate class labels would encode damage type, location etc. In order to carry out the higher levels of identification using PR, it will almost certainly be necessary to construct examples of data corresponding to each class [9]. Classical Bayesian classifier, Linear Discriminant Analysis (LDA), Artificial Neural Network (ANN) and Support Vector Machine (SVM) are the major algorithms for pattern classification [10]. A feature is some characteristic of measurements (such as averages, dominant frequencies etc.) that provides information to discriminate between various classes of input data. In fact, an ideal feature extraction can make the job of classifier trivial If the extracted features are good then simple methods would do a good job in classification. Good features are however usually domain dependent. Different applications will have different features of interest. SENSOR FUSION ARCHITECTURE A complex machine consists of many components which could be the potential fault sources. When a single sensor is not able to identify all the faults in a machine, multiple sensors are needed to fulfill this task. Multi-sensor based condition monitoring system collects data from different sensors. Data fusion is necessary to make good use of all the sensors’ data. Bedworth and O’Brien [11] described a popular framework called the Omnibus model. Figure 1 gives the general layout of this framework, which consists of four main modules and can be executed along the clockwise loop. These modules are used to address the various tasks in sensor fusion and its functional objectives. Fig. 1 Omnibus data fusion model In pattern recognition module, there are two kinds of learning method which are supervised learning and unsupervised learning. Supervised learning is a type of learning algorithm in which the diagnostic is trained by showing it the desired label for each data set [10]. Unsupervised learning doesn’t require the labeled faults data, but it can only be used for detection which is called anomaly detection methods. If supervised learning is required, there will be serious demands associated with it; data from every conceivable damage situation should be available. The two possible sources of such data are computation or modeling, and experiment. In order to accumulate enough training data, it would be necessary to make copies of the system of interest and damage it in all the ways that might occur naturally; in reality, this is simply a waste of money. Modeling requires understanding of physics of the system. The more physics we know about the system, the less faults data we need for the training process. PROPOSED METHOD As the purpose of condition monitoring, not only the diagnosis of the machine faults but also some prognosis of the machine life are expected. Although the prognosis task of life time prediction is difficult at present due to the lack of the understanding of complex system and the lack of access to the faults data, the method used now should have the potential to achieve the prognosis task in the future. Supervised learning methods have the potential to predict the time to failure. The difficulty of this kind of methods is always the access to the faults data. Modeling of the machine measurable signal under normal condition and faults condition is vital to achieve this method. With the development of the physics which describes the complexity of the machine system, more and more measurable signal can be modeled mathematically. Figure 2 shows an ideal system diagram for machine online condition monitoring which uses the measurement of the system operation under normal condition to first identify the system parameters. The model with its parameters thus identified is used to simulate the system under faulty conditions. The simulated data can then be used to extract features and train classifiers. When the system is deployed and operational, sensor data from the machinery is used to extract features that are then classified by the classifiers obtained from the simulated results. The concept is applied to fault monitoring of 3-phase induction motors. Motor Current Signature Analysis (MCSA) is a commonly used technique for fault monitoring of large induction motors. It is a noninvasive, on-line monitoring technique for the diagnosis of problems in large induction motors. Specific harmonic components are located to detect different faults such as broken rotor bars",,2007.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
e58fabd249c4bf13c46a9c1750d1fc901d14e4ae,https://www.semanticscholar.org/paper/e58fabd249c4bf13c46a9c1750d1fc901d14e4ae,The RIMcomb research project: Towards the application of building information modeling in Railway Equipment Engineering,"This paper presents our research towards the utilization of Building Information Modeling (BIM) methods in Railway Equipment Engineering. While BIM is already applied in several railway infrastructure construction projects, so far, it is mainly used for construction engineering tasks, i.e. for modeling the alignment, bridges or tunnels. The RIMcomb research project targets the further application in scope of rail engineering subsections such as control and safety systems, train control systems, telecommunication systems, electric power systems, rail power supply or cable management. In this paper, we are going to give a general overview of the project and will discuss first findings of our approach to digitizing conventional technical drawings in order to translate their content into a machine-readable form. representing railway equipment. The paper ends with a summary and an outlook. 2 THE RIMCOMB PROJECT The research project “RIMcomb: Railway Information Modeling for the Equipment of Railway Infrastructure” was initiated by SIGNON Deutschland GmbH in 2016 in cooperation with Technical University of Munich and AEC3 Deutschland GmbH. The project is funded by the Bavarian Research Foundation and started in early 2017. The main focus of the research project is to develop and adapt new computer-supported methods for model-based collaboration between the different subsections of technical railway equipment in order to increase the efficiency of the planning process and the quality of the outcome. During the design, planning and construction of railway infrastructure and technical equipment, a multitude of domains experts are involved. Therefore, data exchange between these participants is an issue that needs to be addressed, as there a various specialized software tools available for different tasks that do not implement any common data standard. Also, most of railway construction projects involve the modernization or alteration of existing infrastructure and thus the industry has to rely on technical drawings from past decades that are not necessarily consistent with the real-world circumstances. As described in Section 2 we developed a method that allows the automatic recognition of plan symbols in technical drawings of railway infrastructure. Besides the use of this data described in Section 3, we also aim at comparing the generated data with realworld data in order to identify discrepancies. This use case may create a significant benefit for the railway companies as the manual comparison of the as-built drawings with real-world stock data requires a considerable amount of effort but is nonetheless necessary. Here, machine-learning and convolutional neural networks (as outlined in Section 2) will also be employed to process video files of railway tracks in order to identify objects in single frames for the mapping of objects such as signals, balises, switches or poles of overhead lines. Another aspect in the scope of the research project is the development of a method that allows the automated checking of technical rules and regulations. As such approaches have already been applied outside of the infrastructure domain (Preidel and Borrmann, 2016), we see a huge benefit in introducing them into the domain of railway equipment, as the amount of rules and regulations in this domain requires a large amount of manual work. 3 DITIALIZATION OF PLAN DATA One major topic in the research project is the digitalization of conventional drawings depicting railway equipment infrastructure. While most drawings are available digitally, the interpretation of these plans has to be undertaken manually. This is necessary when the accuracy of plans has to be compared to real-world circumstances or in case of stocktaking. Our approach aims at supporting this process in order to reduce the manual effort by automating at least parts of this image interpretation process. The first step towards this goal is the automatic recognition and highlighting of plan symbols on a given drawing and the subsequent storing of their count and location. 3.1 Theoretical background In a first step, three preexisting methods of image recognition are described. We evaluated those techniques in respect to the given problem. As none of those methods matched our requirements completely we also tested Convolutional Neural Networks, which are already widely used for image recognition, in respect of their ability to detect plan symbols. 3.1.1 Template Matching Template Matching is a well-known method for the searching of a template image in a larger image. This is made by sliding the template image over the input (larger) image and comparing them at every position. The result of this method is a grayscale image with a size of (W-w+1, H-h+1), where W and H are the width and the height of the input image, w and h are the width and the height of the template image. We investigated different comparison methods for each one of which there is a normalized version (Kaehler and Bradski, 2016). In this work two of these methods are used: Normalized Square Difference Matching Method: RR(xx,yy) = ∑ �TT(xx′,yy′) − II(xx + xx′,yy + xx′)� xx′,yy′ �∑ TT(xx′,yy′)2 ∙ ∑ II(xx + xx′,yy + yy′)2 xx′,yy′ xx′,yy′ Normalized Correlation Coefficient Matching Method: RR(xx,yy) = ∑ �TT′(xx′,yy′) ∙ II′(xx + xx′,yy + xx′)� xx′,yy′ �∑ TT′(xx′,yy′)2 ∙ ∑ II′(xx + xx′,yy + yy′)2 xx′,yy′ xx′,yy′ Here T is the template image, I the input image, R the result image and TT′(xx′,yy′) = TT(xx′,yy′) − 1/(ww ∙ h) ∙ � TT(xx′′,yy′′)","eWork and eBusiness in Architecture, Engineering and Construction",2018.0,10.1201/9780429506215-55,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
c74106c420c1481800c3c6a564df8b255a4600df,https://www.semanticscholar.org/paper/c74106c420c1481800c3c6a564df8b255a4600df,Good Vibrations: Artificial Ambience-Based Relay Attack Detection,"Relay attacks are passive man in the middle attacks, aiming to extend the physical distance of devices involved in a transaction beyond their operating environment, within the restricted time-frame. In the field of smartphones, proposals have been put forward suggesting sensing the natural ambient environment as an effective Proximity and Relay Attack Detection (PRAD) mechanism. However, these proposals are not in compliance with industry imposed constraints (e.g. EMV and ITSO) mandating that transactions should complete within a certain time-frame (e.g. 500ms for EMV contactless transactions). The generation of an artificial ambient environment (AAE) using peripherals of the transaction devices has shown positive results when using infrared light as an AAE actuator. In this paper we propose the use of vibration as an alternative AAE actuator. We empirically evaluated the effectiveness of the proposed solution as a PRAD mechanism on an experimental test-bed that we deployed. A total of 36,000 genuine and relay attack transaction pairs were analysed using well-known machine learning algorithms. The results of our analysis indicate that the proposed solution is highly effective.","2018 17th IEEE International Conference On Trust, Security And Privacy In Computing And Communications/ 12th IEEE International Conference On Big Data Science And Engineering (TrustCom/BigDataSE)",2018.0,10.1109/TrustCom/BigDataSE.2018.00075,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
514ce8be29824781ca0e5489c680502b0f31242f,https://www.semanticscholar.org/paper/514ce8be29824781ca0e5489c680502b0f31242f,Real Prediction Machines,"Predicting the future is no longer about the mystical reading of natural and celestial phenomena. Today it is all about data. 
The Real Prediction Machine (RPM) is a domestic product that uses big and small data, in combination with machine learning and predictive modelling to make predictions about specific future events. 
 
Contemporary use of digital networked technology, such as personal computers and smart phones, is effectively feeding a live global human behaviour laboratory with data scientists experimenting on an (often) unknowing pool of billions. The futures that emerge from this research are as yet mostly unknown, but there are hints – as this data accumulates it can be analysed, mined and used in algorithms; patterns or trends invisible to the human observer can be identified; and seemingly random events become predictable. At this time prediction algorithms are predominantly being exploited by big industries such as banking, insurance and commerce, or examined in massive research projects such as the EU funded FuturICT project. They are, however, making surreptitious steps into our lives through tailored internet browsing and predictive shopping with occasional Kafkaesque consequences. 
RPMs exploits the potential of this technology motivated not by the interests of industry and research but by the more emotive and personal needs/desires of people – this has the purpose of communicating the transformative potential of big data in domestic life, and asking if the future possibilities described by the project are desirable. 
 
The concept 
When things fail they rarely do so instantaneously but through a gradual process of deterioration. Based on this observation, predictive analytics, through the deployment of sensors in pertinent places and contexts, can determine the when things begin to fail. Such techniques are increasingly used in the mechanical and structural world - to predict for example when a vehicle or bridge might be in need of pre-emptive maintenance. 
 
The RPMs use similar techniques but in the context of human everyday life to predict anything from health related events such as a heart attack to more emotive forecasts such as a domestic argument. 
 
Once the event has been chosen the necessary and available data streams, from local sensors to RSS feeds, determined they are fed into the prediction algorithm - the output of which controls the visual display on the prediction machine. This informs the viewer if the chosen event is approaching, receding or impending. 
 
The Real Prediction Machines was commissioned by the Crafts Council for the exhibition Crafting Narrative. This explored how contemporary designers and makers use objects as mediums to tell stories. 
 
Project developed in collaboration with Subramanian Ramamoorthy and Alan Murray. Engineering by Nick Williamson.",,2015.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
4311542382900a2e0ee1a425dbe3dd6908102037,https://www.semanticscholar.org/paper/4311542382900a2e0ee1a425dbe3dd6908102037,The More the Merrier: Leveraging on the Bug Inflow to Guide Software Maintenance,"Issue management, a central part of software maintenance, requires much effort for complex software systems. The continuous inflow of issue reports makes it hard for developers to stay on top of the situation, and the threatening information overload makes activities such as duplicate management, Issue Assignment (IA), and Change Impact Analysis (CIA) tedious and error-prone. Still, most practitioners work with tools that act as little more than issue containers. Machine Learning encompasses approaches that identify patterns or make predictions based on empirical data. While humans have limited ability to work with big data, ML instead tends to improve the more training data that is available. Consequently, we argue that the challenge of information overload in issue management appears to be particularly suitable for ML-based tool support. While others have initially explored the area, we develop two ML-based tools, and evaluate them in proprietary software engineering contexts. We replicated [1] for five projects in two companies, and our automated IA obtains an accuracy matching the current manual processes. Thus, as our solution delivers instantaneous IA, an organization can potentially save considerable analysis effort. Moreover, for the most comprehensive of the five projects, we implemented automated CIA in the tool ImpRec [3]. We evaluated the tool in a longitudinal in situ study, i.e., deployment in two development teams in industry. Based on log analysis and complementary interviews using the QUPER model [2] for utility assessment, we conclude that ImpRec offered helpful support in the CIA task. (Less)",Tiny Trans. Comput. Sci.,2015.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
ead9b2621e17dddcfe87a81da4a7f01f284b6637,https://www.semanticscholar.org/paper/ead9b2621e17dddcfe87a81da4a7f01f284b6637,Predicting Customer Preferences in Non-experimental Retail Settings,"This thesis investigates the application of computational statistics and Machine Learning in consumer preference prediction, with specific reference to the challenges imposed by real world operational retail environments. Some retailers base their competitiveness on Machine Learning. For instance, Dunnhumby analyzes more than 400 million online consumer records for retailers, such as Tesco, to optimise business decisions. The experiments in this thesis investigate three main challenges commonly presented in operational scenarios that hinder the application of Machine Learning in retail environments: 1. The measurement of correlation of feature factors for Machine Learning in a noisy setting; 2. The exploration and exploitation balance for predicting purchase preferences on new products; 3. The model adaptability to the changing dynamics over time. A design of a distributed Machine Learning framework for building practical applications of consumer preference prediction is also presented. Experiment 1: Correlation between Contextual Information and Purchase Behaviours under a Non-experimental Retail Environment The first experiment applies statistical methodologies, namely odds ratio and Mantel-Haenszel method, to analyze contextual information in a retail business. More specifically, it investigates the correlation between customers’ recent online browsing behaviours on Boots.com and their in-store purchase behaviours at Boots’ retail stores nationally in a non-experimental noisy setting. Methodologies such as stratified analysis with K-means clustering are proposed to detect and eliminate confounding factors that affect the evaluation of the correlation. The dataset for this experiment, provided by Boots UK, is the first year of a 2-year anonymised real in-store and online purchase records data. It contains profiles of 10,217,972 unique consumers who are Advantage Card holders and 2,939 unique selected products under 10 different brands. Experiment 2: Resources Allocation of Exploration and Exploitation for New Products under Retail Constraints The second experiment provides a two-stage batch solution based on matrix factorization and binary integer programming to optimise the customer response rate to new products of a simulated group buying system. This experiment investigates how the balance between the exploration of new products and the exploitation of existing known model affects overall business gains through purchase prediction and recommendation. In this experiment, the products are new with no prior profile and the number of new products a retailer can recommend to each customer is limited. The effectiveness of one of the traditional experimental design techniques in improving the learning efficiency during the exploration process is evaluated. Experiment 3: Continuous Model Selection for a Changing Retail Environment The third experiment investigates, using root-mean-square error and mean average precision measures, the adaptability of data model for consumer purchase prediction in a non-static retail environment. In particular, it analyzes the prediction accuracy of data models with static parameters over time. A continuous model selection approach by using an automatic hyperparameter tuning technique, namely random search, is proposed and is evaluated. The results challenge the traditional assumption that a one-off initial model selection is sufficient. The dataset for this experiment is a 2-year anonymised real in-store and online purchase records data provided by Boots UK. System Design: A Distributed Machine Learning Framework with Automated Modeling This system design outlines the concept and system architecture. It also demonstrates scenarios of a distributed Machine Learning framework for (i) evaluating, comparing and deploying scalable learning algorithms, (ii) tuning hyperparameters of algorithms manually or automatically and (iii) evaluating model training status. The design has become the foundation of a popular open-source software project - PredictionIO. The project is followed by over 5000 data scientists and practitioners on Github. Contributions to Science The major contribution of this thesis is to offer robust research-based methodologies to handle prediction challenges in real world operational environment for retail businesses. Computational statistics and Machine Learning methodologies are proposed to 1) identify contextual factors that are relevant to consumer preference in noisy non-experimental setting; 2) determine the importance of exploration and exploitation for new products under real-world constraints; 3) adjust data model continuously to adapt to changes in retail environments. This thesis contributes to the existing literature in a number of ways. First, this research proposes a novel statistical method to isolates the influence of confounding factors in correlation analysis for consumer preference prediction. It is a topic that received little attention in empirical literature. Second, this research proves the existence of the correlation between consumer online browsing and in-store purchase behaviours in a real retail dataset. This is a significant finding for the retail industry to improve prediction accuracy in the future. Third, this research examines the influence of the balance between exploration and exploitation of new product profiles on maximising business gains. Forth, this research proves that random selection surprisingly outperforms D-optimal experimental design in some retail cases. Fifth, this research challenges the existing assumption that model selection is needed only once at the initial stage. It proves that prediction accuracy can be improved significantly by continuous model selection. Sixth, this thesis presents the implementation of a continuous model selection approach by using automatic hyperparameter tuning techniques. Finally, this thesis presents a design of a distributed system that can be used for building predictive retail applications.",,2015.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
ba4f683f484a69603f2a7714f9850b7eb81a3ff5,https://www.semanticscholar.org/paper/ba4f683f484a69603f2a7714f9850b7eb81a3ff5,Semantic Web-Based Systems,,SpringerBriefs in Computer Science,2018.0,10.1007/978-981-10-7700-5,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
ea89d5ab69627a9361a340d43866cc0950a50fa1,https://www.semanticscholar.org/paper/ea89d5ab69627a9361a340d43866cc0950a50fa1,Distributed Learning Algorithms for Sensor Networks,"Wireless sensor networks have received significant attention in the last decade owing to their widespread use not only in monitoring the physical world but also in surveillance. The energy and communication constraints of sensor nodes, coupled with distributed processing of sensed signals, lead to challenges in developing effective methods to perform desired inference tasks such as object detection or classification. Further, the lack of well-calibrated sensors is a major obstacle for the rapid deployment of sensor networks. This dissertation develops gossip-based learning algorithms for distributed signal processing in sensor networks. In gossip-based algorithms, sensor nodes share information with local neighbors to converge upon common knowledge about the sensed environment. Gossip-based methods allow for manageable communication among energy-constrained nodes and also accommodate changing network communication topologies. We consider three related problems and develop gossip-based processing solutions. We first consider the problem of joint signature estimation and node calibration using distributed measurements over a large-scale sensor network. We develop a new Distributed Signature Learning and Node Calibration algorithm, called D-SLANC, which estimates the signature of a commonly-sensed source signal and simultaneously estimates calibration parameters local to each sensor node. The approach we take is ii to model the sensor network as a connected graph and make use of the gossip-based distributed consensus to update the estimates at each iteration of the algorithm. We prove convergence of the algorithm to the centralized data pooling solution. We also compare its performance with the Cramér-Rao bound (CRB), and study the scaling performance of both the CRB and the D-SLANC algorithm. Secondly, we develop a gossip-based algorithm for distributed `1-optimization in a large-scale sensor network setting. Specifically, we consider sensor nodes which can measure only a part of the entire measurement vector. We formulate the `1optimization problem as quadratic optimization and develop a distributed, gossipbased algorithm using the projected-gradient approach. We analyze the performance of the proposed algorithm using synthetic data and compare it with a standard `1 solver. Third, we consider the problem of distributed classifier learning in a large-scale sensor network setting. We adopt a machine learning approach to the problem and develop a distributed, gossip-based algorithm that learns the optimal (large-margin) hyperplane separating the two classes, using the projected-gradient approach. We illustrate the performance of the proposed algorithm using both synthetic and realworld datasets.",,2010.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
1a8602d708d2f9ec7819230b9e5fb6be533111ba,https://www.semanticscholar.org/paper/1a8602d708d2f9ec7819230b9e5fb6be533111ba,Enabling the Best by Preparing for the Worst: Lessons from Disaster Response for Industrial IoT in Oil and Gas,"
 
 
 As more oil and gas companies develop Internet of Things (IoT) strategies and beginning their digital transformation to Industry 4.0 or Smart Manufacturing, they face challenges in adopting technologies due to regulatory restrictions for highly combustible atmospheres such as exist in some of the world's largest and most critical industries - oil & gas, chemical, pharmaceutical, energy and others. In Zone 1 classified hazardous areas worldwide, up to 15% of personnel do not have access to mobile computing devices unless they are certified ""intrinsically safe,"" or incapable of causing a spark that could ignite a combustible environment. Thus, the human ""sensor"" in hazardous area operations, who could conceivably detect perceived anomalies or problems in the maintenance, workflow, process or function of these operations, is relegated to recording observations with pencil and paper and then entering data manually into ERP systems hours or days later. Such lack of real-time communication and data management results in inefficiency, increased costs and elevated safety and asset risk, causing potential down-time and even loss of life in extreme cases.
 
 
 
 By deploying new IoT technologies that allow people to use technology inside Zone 1 hazardous areas, humans can actively interact with machines in real time to dramatically improve productivity, safety and the bottom line in hazardous operations. A new style of IoT platform built especially for oil & gas hazardous area operations, would need to include various and affordable types of sensors to cover vast spaces, real-time communications, cloud computing, machine learning, rights management, security, big data storage, analytics and user-friendly visualization, all functioning in highly explosive conditions. This paper considers the advantages for productivity and safety of an IoT Platform for Hazardous Locations, based on hands-on research conducted by AegexTechnologies, Verizon, Nokia and multiple technology partners that tested various edge technologies with first responders in realistic disaster scenarios during two annual events, Operation Convergent Response 2017 (#OCR2017) and Operation Convergent Response 2018 (#OCR2018 – to take place 5-8 November 2018)). The events provide unparalleled opportunities to test IoTunder extreme conditions with real people, such as a staged refinery collapse caused by an earthquake.
 
 
 
 #OCR2017 and #OCR2018 showed how enabling real-time communications and data management via cutting-edge technologies, such as intrinsically safe tablets and IoT sensors, can strategically assist first responders to better handle emergencies. The studies’ results give insight into the need for continued collaboration on IoT capabilities that can better manage not only emergency response, but everyday operations in hazardous industries such as oil and gas.
 
 
 
 Equipping oil and gas facilities with pervasive, smart IoT data-sensing capabilities, and equipping oil and gas personnel with real-time communications and data management tools, could result in dramatic improvements in productivity, safety, emergency response and disaster mitigation.
","Day 1 Mon, November 12, 2018",2018.0,10.2118/192614-MS,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
350c4282de22497acbab4bf900577751f83bd135,https://www.semanticscholar.org/paper/350c4282de22497acbab4bf900577751f83bd135,Diseño y construcción de un sistema de control de temperatura para la actualización de una máquina de recuperación de disolvente en residuos de serigrafía en la industria del plástico,"Waste generated in the screen printing industry plastics have led companies to use different mechanisms to treat and comply with regulations of the Ministry of Environment; the solvent it is recovered as raw material and the waste is sent to other companies for further treatment. 
Mr. Wilson Teran has built a machine for the treatment of solvent recovery, but it has not implemented a system that optimizes the processing time and is limited by the time of delivery. 
This project have finality the design and implementation for upgrade of three-phase system control and protection system for this has been integrated the variables of temperature and processing time; these are in a control card designed in various stages as: signal synchronization of zero crossing, conditioning of temperature signal, design power source, software for controlling the process variables. 
The upgrade of the control system is designed for the user to have a better control and monitoring of variables also it has the necessary assurances that comply with the rules governing in Ecuador, allowing reduce processing time and increased production capacity. 
Performance testing of all deployed elements is performed, allowing this project has basic and fundamental aspects of learning related to Instrumentation, Automatic Control, microcontrollers, etc., of the race of Electronic Engineering.",,2016.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
583b889e75b5766800e99215ae782f7d1834c47e,https://www.semanticscholar.org/paper/583b889e75b5766800e99215ae782f7d1834c47e,Data Mining on Grids,,IC3,2011.0,10.1007/978-3-642-22606-9_36,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
8c2b9c317190908351095d80dc45cf4027e229ff,https://www.semanticscholar.org/paper/8c2b9c317190908351095d80dc45cf4027e229ff,On the effectiveness of learning through the use of the web based Laboratories – the experience of using the solar e-lab,"This paper elaborates on the experience from the operation of the solar energy e-learning laboratory (Solar e-lab) in Cyprus, and analyses the results of the online evaluation submitted by the e-lab users. The aim of solar energy e-learning laboratory is to use web-based technology as a tool to make the laboratory facilities accessible to engineering students and technicians, including handicapped, located outside the e-lab premises anywhere in the world. In this way, the laboratory, its equipment and experimental facilities are available and can be shared by many people, thus reducing costs and widening educational experiences. Throughout its 5 years of operation, the solar e-lab has been accessed by users from over 400 locations from 75 countries spread all over the world. The total number of hits recorded on the site exceeded 1.2 million. Furthermore, a number of colleges and Universities are using the solar e-lab as part of their training programme. Judging from the online student evaluation reports sent to the solar e-lab administrator, it can be said that there is nearly excellent satisfaction by the users. The feedback provides useful information as to the students’ satisfaction with the e-lab interaction, its course content, and organisation. Introduction The role that web-based learning undertakes within the teaching and learning environments has gained widespread acceptance over the last number of years as remote engineering is becoming an important element in engineering education (Auer et al 2003, Benmohamed et al 2005, Agrawal and Srivastava 2007, Helander and Emani 2008). The development of web-based remote engineering experimentation laboratories can significantly enhance the students’ learning experience. A recent assessment study (Nickerson et al 2007) comparing versions of remote labs versus hands-on labs in a junior-level mechanical engineering course on machine dynamics and mechanisms, suggests that students learned lab content information equally well from both types of laboratories, and that they have a realistic understanding and appreciation of the practical advantages of remote laboratories. Using remote laboratories has the potential to significantly reduce obstacles related of cost, time-inefficient use of facilities, inadequate technical support and limited access. The growing importance of sharing engineering resources through web-based laboratories can be attributed to the growing complexity of engineering tasks, the highly specialized and expensive equipment as well as software tools and simulators. It can also be attributed to the necessary use and application of high tech equipment and the need of high qualified staff to control such equipment. There is evidence that the distance educated students are more motivated than on-site students and that there is significant relationship between the student attends class and the motivation to do well (Bisciglia et al 2005). As online education becomes an everyday part of engineering education in nowadays (Hutzel 2002, Lindsay and Good 2005, Aziz et al 2006, Machotka and Nedic 2006), online methods in engineering education will increase the breadth and scale of engineering education, thus extending the reach of institutions and the delivery of education to broader audiences (Lindsay and Good 2006). In contrast to “traditional” engineering, experts in remote engineering are deployed in a relatively broad range of activities that span to different sectors of industry. They typically work in locally distributed teams and coordinate their work amongst themselves. This requires not only competent handling of tools and methods for diagnosis, maintenance, monitoring and repair, but above all require the ability to communicate effectively with others (e.g. customers, users, installers) with the help of computer-aided means of communication. Skilled service technicians must solve the “mutual knowledge problem”, for example by integrating the know-how of others in order to accomplish their goals using appropriate tools (e.g. electronic conferencing or groupware applications). Special focus must be placed on accessing distributed information from suppliers, customers and manufacturers over the Internet. Because e-maintenance is primarily immaterial, the quality assessment made by customers is highly dependent on those employees who perform such services. For this reason, technicians and engineers must also be trained in customer orientation with an emphasis on communication training and customer-centred action. The solar e-lab features: architecture and organisation The solar energy e-learning laboratory is a good example of a web-based laboratory. It was developed within the MARVEL project of the Leonardo da Vinci programme (Müller and Ferreira 2003) and it focuses on experiential based learning arrangements allowing remote and distributed working with laboratories, workshops and real working-places to train students in remote engineering (Auer et al 2003). The solar e-lab comprises a pilot solar energy conversion plant which consists of two flat-plate solar collectors having a surface area of 3 m2 located on the roof of the laboratory, an insulated thermal storage tank located in the solar energy laboratory and other auxiliary equipment and accessories. It is also equipped with all necessary instrumentation, control and communication devices which are needed for remote access, control, and data collection and processing. A major goal of the solar e-lab is the usage of real worlds in virtual learning environments in order to support workprocess-oriented and distributed cooperative learning with real-life systems. Its aim is to use the Internet as a tool to make the laboratory facilities accessible to engineering students (especially handicapped) and technicians located outside the laboratory, including overseas. In this way, the solar energy e-learning lab and its equipment and experimental facility will be available and be shared by many people, thus increasing availability and reducing costs. The system architecture used in the solar energy e-learning lab is illustrated in fig. 1. The user can access the solar e-lab through a PC which acts as a web server. This server hosts the e-learning platform with all necessary extensions for PHP support as well as the database necessary for this platform. It also communicates with the machine hosting the application software (TestPoint). Whenever a user wishes to get into the system, the communication will be done through this server. That is, the user sends his/her request to the system, the web server communicates with the TestPoint web server and it collects the data and transfers them to the user. The actual running of the set-up is done via the TestPoint, which is an interface tool capable of acquiring data through various sensors, storing the data in a form that the user likes, and processing and handling the data in a meaningful manner. This particular software consists of two parts, the programming and the runtime parts. The programming is needed only to the system designer, while the runtime is necessary to run the particular experiment and is available to the interested user free of charge. Any collected data can be stored in popular programme formats (Word, Excel, etc.) allowing the user to print his own report formats and hand in a report of his choice. This tool is located on a dedicated server allowing faster data handling. Fig. 1. The solar e-lab system architecture A user may visit the laboratory website anytime from anyplace in the world. The only requirements are a computer connected to the internet and any of the standard web browsers. By typing the address of the solar energy e-learning laboratory (http://e-lab.hti.ac.cy ), the user can visit the initial page of the website. It is possible for visitors with little interest in solar energy to read and study on the subject with no requirements or registration or testing. So, not all of the pages require login. As a matter of fact, one can see most of the pages without the need of creating an account. Login, and thus creating an account, is only needed when the user decides to take the so called pre-lab test and conduct an online live experiment through the internet. A booking tool is available to control the access time to the system. In order to be able to make a booking, to have access to the system for conducting an experiment, a remote student has to attempt a pre-lab quiz and get a passing grade. In case the system is busy, because another user is online performing an experiment, the user is entitled and he/she can get into the e-lab as an observer. The system will open a new window and the remote user will be able to have a view of the system in operation and get the readings but he/she will not be allowed to intervene into the operation and control of the system. The “observer user” can, however, record the readings and use them for calculations if he/she wishes so. The learning experience The solar e-lab is designed for real time online live experiments in the field of solar engineering. The design of the learning scenarios comprises of a series of exercises of different degree of difficulty and complexity. The introductory work with all the notes, system explanations and glossary allow the student/interested party to get familiar with the system and the work to be followed. The subsequent exercises with the difficulties and unexpected problems of real life experimentation allow the student to realize the difficulties of the work (Kolb’s first and second steps). For each exercise, the student undergoes an online assessment and is allowed to proceed to a real experiment only if he/ she is successful to the pre-lab test. As a last step into the real world of experimentation the student may get access to the system and perform system control and data gathering. During this part of the work the student will get acquaint with the rem",,2009.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
6f070c05dbd342d3906ee2fbdd2c3529ed558851,https://www.semanticscholar.org/paper/6f070c05dbd342d3906ee2fbdd2c3529ed558851,Designing and evaluating techniques to mitigate misinformation spread on microblogging web services,"Online social media is a powerful platform for dissemination of information during important realworld events. Beyond the challenges of volume, variety and velocity of content generated on online social media, veracity poses a much greater challenge for effective utilization of this content by citizens, organizations, and authorities. Veracity of information refers to the trustworthiness / credibility / accuracy / completeness of the content. Over last few years social media has also been used to disseminate misinformation in the form of rumors, hoaxes, fake images, and videos. We aim to address this challenge of veracity or trustworthiness of content posted on social media. The spread of such untrustworthy content online has caused the loss of money, infrastructure and threat to human lives in the offline world. We focus our work on Twitter, which is one of the most popular microblogging web service today. We provide an in-depth analysis of misinformation spread on Twitter during real-world events. We propose and evaluate automated techniques to mitigate misinformation spread in real-time. The main contributions of this work are: (i) we analyzed how true versus false content is propagated through the Twitter network, with the purpose of assessing the reliability of Twitter as an information source during real-world events; (ii) we showed the effectiveness of automated techniques to detect misinformation on Twitter using a combination of content, meta-data, network, user profile and temporal features; (iii) we developed and deployed a novel framework for providing indication of trustworthiness / credibility of tweets posted during events. We evaluated the effectiveness of this real-time system with a live deployment used by real Twitter users. First, we analyzed Twitter data for 25+ global events from 2011-2014 for the spread of fake images, rumors, and untrustworthy content. Some of the prominent events analyzed by us are: Mumbai blasts (2011), England Riots (2011), Hurricane Sandy (2012), Boston Marathon Blasts (2013), Polar Vortex (2014). We identified tens of thousands of tweets containing fake images, rumors, fake websites, and by malicious user profiles for these events. We performed an in-depth characterization study of how this false versus the true data is introduced and disseminated in the Twitter network. Second, we showed how features of meta-data, network, event and temporat from user-generated content can be used effectively to detect misinformation and predict its propagation during realworld events. Third, we proposed and evaluated an automated methodology for assessing credibility of information in tweets using supervised machine learning and relevance feedback approach. We developed and deployed a real-time version in TweetCred, a system that assigns a credibility score to tweets. TweetCred, available as a browser plug-in, has been installed and used by 1,808 real Twitter users. During ten months of its deployment, the credibility score for about 12 million tweets was computed, allowing us to evaluate TweetCred in terms of accuracy, performance, effectiveness and usability. The system TweetCred built as part of this thesis work is used effectively by emergency responders, firefighters, journalists and general users to obtain credible content from Twitter. This thesis work has shown that measuring credibility of the Twitter content is possible using semi-automated techniques, and the results can be valuable to the real-world users. The insights obtained from this research and deployment provide a basis for building more sophisticated technology to tackle similar problems on different social media.",,2015.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
dbc0764f1c752ac6dc8dc2b468201fe362a2f037,https://www.semanticscholar.org/paper/dbc0764f1c752ac6dc8dc2b468201fe362a2f037,The moral imperative of artificial intelligence,"of a five-game match in Seoul, Korea. (AlphaGo is a program developed by DeepMind, a British AI company acquired by Google two years ago.) After Deep Blue’s victory against chess world champion Gary Kasparov in 1997, the game of Go was the next grand challenge for game-playing artificial intelligence. Go has defied the brute-force methods in game-tree search that worked so successfully in chess. In 2012, Communications published a Research Highlight article by Sylvain Gelly et al. on computer Go, which reported that “Programs based on Monte-Carlo tree search now play at human-master levels and are beginning to challenge top professional players.” AlphaGo combines tree-search techniques with searchspace reduction techniques that use deep learning. Its victory is a stunning achievement and another milestone in the inexorable march of AI research. By using deep–learning techniques to prune the search tree, AlphaGo can be said to augment bruteforce search with “intuition,” which it has developed by playing numerous games against itself. (Google said AlphaGo does not use Lee’s games as its training data.) By relying on learned “intuition,” AlphaGo is able to overcome the so-called Polanyi’s Paradox. The philosopher Michael Polanyi observed in 1966, “We can know more than we can tell ... The skill of a driver cannot be replaced by a thorough schooling in the theory of the motorcar.” Some labor economists have viewed Polanyi’s Paradox as a major barrier for AI, arguing it implies a limit on its potential to automate human jobs. AlphaGo’s victory demonstrates machine learning provides a path around that barrier. Indeed, the automation of driving has been a major challenge for AI research over the past decade. In 2004, economists argued driving was unlikely to be automated in the near future due to Polanyi’s Paradox. A year later, a Stanford autonomous vehicle won a DARPA Grand Challenge by driving over 100 miles along an unrehearsed desert trail. Now, more than a decade later, both technology companies and car companies vigorously pursue the automation of driving. I expect the technical challenges to be resolved in the coming decade. In 1843, Ada Lovelace, well known for her work on Charles Babbage’s Analytical Engine, wrote to Babbage that she wished to see computing technology developed for “the most effective use of mankind.” It is difficult for me to think of any computing technology other than automated driving that can be deployed in a decade or two and with such benefit for humanity. About 1.25 million people worldwide die from car accidents every year. Over 90% of these accidents are caused by human error. By automating driving, we could save over a million lives a year, as well as avoid countless injuries. At the same time, the automation of driving would have a huge disruptive effect on the global economy. Existing industries will shrivel, and whole new industries will rise. In the U.S., close to 10% of all jobs involve operating a vehicle and we can expect to see the majority of these jobs disappear. The human cost of such a profound change cannot be underestimated. As a precedent we can see what happened to U.S. manufacturing over the past 35 years. While manufacturing output in constant dollars is at an all-time high, manufacturing employment peaked around 1980 and is today lower than it was in 1947. The disappearance of millions of jobs due to automation may explain a recent, rather shocking, finding by economists Angus Deaton, winner of the 2015 Nobel Memorial Prize in Economic Science, and Anne Case, that mortality for white middle-aged Americans has been increasing over the past 25 years, due to an epidemic of suicides and afflictions stemming from substance abuse. Thus, the automation of driving would be hugely beneficial, saving lives and preventing injuries on a massive scale. At the same time, it would have a profoundly adverse impact on the labor market. In the balance, life saving and injury prevention must take precedence, and we have a moral imperative to develop and deploy automated driving. The solution to the labor problem will not be technical, but sociopolitical. As computing professionals, we also have a moral imperative to acknowledge the adverse societal consequences of the technology we develop and to engage with social scientists to find ways to address these consequences. Follow me on Facebook, Google+, and Twitter.",Commun. ACM,2016.0,10.1145/2903530,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
71b2e5ad0a259da10e61e97ac8f8ce1aa1019a5b,https://www.semanticscholar.org/paper/71b2e5ad0a259da10e61e97ac8f8ce1aa1019a5b,"Current Issue: November 2018, Volume 06, Number 3 --- Table of Contents","Software-defined networking (SDN) is reshaping the networking paradigm. Previous research shows that SDN has advantages over traditional networks because it separates the control and data plane, leading to greater flexibility through network automation and programmability. Small business and academia networks require flexibility, like service provider networks, to scale, deploy, and self-heal network infrastructure that comprises of cloud operating systems, virtual machines, containers, vendor networking equipment, and virtual network functions (VNFs); however, as SDN evolves in industry, there has been limited research to develop an SDN architecture to fulfil the requirements of small business and academia networks. This research proposes a network architecture that can abstract, orchestrate, and scale configurations based on academia and small business network requirements. Our results show that the proposed architecture provides enhanced network management and operations when combined with the network orchestration application (NetO-App) developed in this research. The NetO-App orchestrates network policies, automates configuration changes, secures container infrastructure, and manages internal and external communication between the campus networking infrastructure. We develop a new approach for distributed computing of the association rules of high confidence on the attributes/columns of a binary table. It is derived from the D-basis algorithm developed by K.Adaricheva and J.B.Nation (Theoretical Computer Science, 2017), which runs multiple times on sub-tables of a given binary table, obtained by removing one or more rows. The sets of rules retrieved at these runs are then aggregated. This allows us to obtain a basis of association rules of high confidence, which can be used for ranking all attributes of the table with respect to a given fixed attribute. This paper focuses on some algorithmic details and the technical implementation of the new algorithm. Results are given for tests performed on random, synthetic and real data. This study increased the overall knowledge of job satisfaction among non-tenured IT faculty by way of contributing to the body of management knowledge in the IT environment. The study results provided higher education institution IT leaders and management the vision and understanding to handle job satisfaction issues within the IT environment. This information is also crucial in helping higher education institutions perform at high levels of employee retention, flexibility, and employee job satisfaction by focusing on autonomy and the opportunity for advancement. This quantitative research examined the relationship between (a) the extrinsic motivators (predictor/independent variables), operationalized as autonomy; (b) the intrinsic motivators, operationalized as advancement opportunities; and (c) the job satisfaction level of IT faculty in higher educational institutions (dependent variable). This research has added to the body of knowledge regarding what is needed for IT educators to have job satisfaction by examining how much the independent variables affect job satisfaction of IT faculty in higher educational institutions. We have defined and analyzed methods and practices that companies could apply as they formulate the essential skills and resources for predicting job satisfaction among IT faculty. In this area of job satisfaction, additional understanding could support higher education institutions learning how to keep experienced IT faculty. To examine the extent to which autonomy and opportunity for advancement predict job satisfaction, a multiple linear regression was conducted. This study added to the existing body of knowledge regarding what is needed for IT educators to have job satisfaction by examining how much the independent variables of opportunity for advancement and autonomy affect job satisfaction of IT faculty in higher educational institutions. Additional knowledge in this area of job satisfaction may support higher educational institutions in learning how to retain experienced IT faculty. By addressing these factors related to job satisfaction, employers can better understand what motivates and keeps IT workers satisfied in their jobs. Keeping IT employees satisfied will help retain them and prevent them from job-hopping",,,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
9f9d5bacdbbe9d15278cb4072c5ab759deab44af,https://www.semanticscholar.org/paper/9f9d5bacdbbe9d15278cb4072c5ab759deab44af,"A brief description on measurement data from an operational microwave network in Gothenburg, Sweden","The idea of using operational wireless communication links as a widely deployed sensor network for environmental monitoring has been successfully accepted both in academia and industry since the first papers from the early 2000s. Over ten years of research has shown promising results in various applications, primarily rainfall but also e.g. evaporation and fog. In this paper we will present measurement data from a joint pilot project by Ericsson, Hi3G Sweden and SMHI (Swedish Meteorological and Hydrological Institute). It includes a set of over 20 months’ measurement of transmitted and received signal strength from an operational network of 364 bi-directional microwave hops in the Gothenburg area, Sweden. In total there are close to 7.5 billion measurement points with a time resolution of 10s. The purpose of this paper is to present key characteristics of the data set and to make some data available to research institutes around the world to encourage more cross disciplinary works in research topics such as environmental monitoring, sensor networks, big data analytics, machine learning etc. Researchers from different disciplines are encouraged to verify their models and algorithms on data from real communication networks.",,2017.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
6dee18d014022f02906169ff4a8a44a068cc7475,https://www.semanticscholar.org/paper/6dee18d014022f02906169ff4a8a44a068cc7475,On robust and adaptive soft sensors.,"In process industries, there is a great demand for additional process information such as the product quality 
level or the exact process state estimation. At the same time, there is a large amount of process data like temperatures, pressures, etc. measured and stored every moment. This data is mainly measured for process 
control and monitoring purposes but its potential reaches far beyond these applications. The task of soft 
sensors is the maximal exploitation of this potential by extracting and transforming the latent information 
from the data into more useful process knowledge. Theoretically, achieving this goal should be straightforward 
since the process data as well as the tools for soft sensor development in the form of computational learning methods, are both readily available. However, contrary to this evidence, there are still several obstacles which prevent soft sensors from broader application in the process industry. The identification of the sources of these obstacles and proposing a concept for dealing with them is the general purpose of this work. The proposed solution addressing the issues of current soft sensors is a conceptual architecture for the development of robust and adaptive soft sensing algorithms. The architecture reflects the results of two review studies that were conducted during this project. The first one focuses on the process industry aspects of soft sensor development and application. The main conclusions of this study are that soft sensor development is currently being done in a non-systematic, ad-hoc way which results in a large amount of manual work needed for their development and maintenance. It is also found that a large part of the issues can be 
related to the process data upon which the soft sensors are built. The second review study dealt with the same topic but this time it was biased towards the machine learning viewpoint. The review focused on the identification of machine learning tools, which support the goals of this work. The machine learning concepts which are considered are: (i) general regression techniques for building of soft sensors; (ii) ensemble methods; (iii) local learning; (iv) meta-learning; and (v) concept drift detection and handling. The proposed architecture arranges the above techniques into a three-level hierarchy, where the actual prediction-making models operate at the bottom level. Their predictions are flexibly merged by applying ensemble methods at the next higher level. Finally from the top level, the underlying algorithm is managed by means of metalearning methods. The architecture has a modular structure that allows new pre-processing, predictive or 
adaptation methods to be plugged in. Another important property of the architecture is that each of the levels can be equipped with adaptation mechanisms, which aim at prolonging the lifetime of the resulting soft sensors. 
The relevance of the architecture is demonstrated by means of a complex soft sensing algorithm, which can be seen as its instance. This algorithm provides mechanisms for autonomous selection of data preprocessing and predictive methods and their parameters. It also includes five different adaptation mechanisms, some of which can be applied on a sample-by-sample basis without any requirement to store the on-line data. Other, more complex ones are started only on-demand if the performance of the soft sensor 
drops below a defined level. The actual soft sensors are built by applying the soft sensing algorithm to three industrial data sets. The different application scenarios aim at the analysis of the fulfilment of the defined goals. It is shown that the soft sensors are able to follow changes in dynamic environment and keep a stable performance level by exploiting the implemented adaptation mechanisms. It is also demonstrated that, although the algorithm is rather complex, it can be applied to develop simple and transparent soft sensors. In another experiment, 
the soft sensors are built without any manual model selection or parameter tuning, which demonstrates the 
ability of the algorithm to reduce the effort required for soft sensor development. However, if desirable, the algorithm is at the same time very flexible and provides a number of parameters that can be manually optimised. Evidence of the ability of the algorithm to deploy soft sensors with minimal training data and as such to provide the possibility to save the time consuming and costly training data collection is also given in this work.",,2009.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
3a6c165565e0df0ec97609c24f7e2b025505076a,https://www.semanticscholar.org/paper/3a6c165565e0df0ec97609c24f7e2b025505076a,AI Infused Fragrance Systems for Creating Memorable Customer Experience and Venue Brand Engagement,,IHSI,2018.0,10.1007/978-3-319-73888-8_47,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
ad723fe26553d4667b16dc238212d617bcf05dcb,https://www.semanticscholar.org/paper/ad723fe26553d4667b16dc238212d617bcf05dcb,"A Smart Condition Monitoring System for HV Networks with Artificial Intelligence, Augmented Reality and Virtual Reality: Copyright Material IEEE, Paper No. PCIC-2018-37","The authors present a conceptual design for a SMART asset monitoring solution for high voltage (HV) networks in the petrochemical industry. The paper discusses the potential for incorporating artificial intelligence (AI), augmented reality (AR) and virtual reality (VR) into an application of the Industrial Internet of Things (IIoT) for condition monitoring. The paper is a continuation of the work presented by the authors at the IEEE-PCIC 2017 conference in Calgary. The proposed asset management system analyses condition monitoring (CM) data and assesses the risk of failure data across complete HV networks. Knowledge of deteriorating asset condition provides the operator with an advanced, early warning of incipient mechanical and electrical faults. With knowledge of the severity and source of such faults, pinpointed preventative maintenance interventions can then made during planned maintenance outages. The complete HV network asset monitoring solution described includes permanent sensors and monitoring nodes deployed at strategic locations across the network. Processed data is passed via a local area network to local servers and then via secure data cloud transmission to a centralized monitoring server located at the CM headquarters. This central server operates a CM database that logs, displays, benchmarks and trends the condition data with comparison to a statistically-significant database of measurements. It is proposed in the IIoT solution proposed that this database will be downloadable to a smartphone/tablet for use by the field engineer. The monitoring technology will likely also incorporate a number of AI machine learning software modules for the de-noising of raw signals and the diagnosis of different types of defects within different types of HV plant items. The proposed SMART CM system includes an advanced graphical user interface (GUI) for viewing HV asset CM data along with operational and maintenance (O&M) data. The GUI will also be able to display both condition criticality and operational criticality (on a color-coded range of 0-100%) for individual HV plant items on a digitized mimic of the HV network's single-line diagram (SLD). This could also be combined with geometric positioning data of assets across the facility (including HV cable routes and lengths) to provide a fully digitized SMART network diagram for use in the IIoT asset management solution. Asset management data, combined with the application of the developing techniques of AI, AR and VR, will greatly help the user to visualize the plant items in 3-D, their position within the network, their condition and operational criticality along with all related asset management information together on one dashboard screen, downloaded onto smartphone/tablet. The paper concludes with a case study showing the development of a specification for a SMART IIoT asset condition monitoring solution suitable for a large petrochemical refining facility.",2018 IEEE Petroleum and Chemical Industry Technical Conference (PCIC),2018.0,10.1109/PCIC31437.2018.9080444,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
32f97b529171cd8c527f8ca883f90e1fbc5795d9,https://www.semanticscholar.org/paper/32f97b529171cd8c527f8ca883f90e1fbc5795d9,IEEE Access Special Section Editorial: Health Informatics for the Developing World,"We live in a world with growing disparity in the quality of life available to people in the developed and developing countries. Healthcare in the developing world is fraught with numerous problems such as the lack of health infrastructure, and human resources, which results in very limited health coverage. The field of health informatics has made great strides in recent years towards improving public health systems in the developing world by augmenting them with state-of-the-art information and communication technologies (ICT). Through real-world deployment of these technologies, there is real hope that the health industry in the developing world will progress from its current, largely dysfunctional state to one that is more effective, personalized, and cost effective. Health informatics can usher a new era of personalized health analytics, with the potential to transform healthcare in the developing world. In conjunction with mHealth and eHealth, many other important health informatics trends—such as artificial intelligence (AI), machine learning (ML), big data, crowdsourcing, cloud computing—are also emerging. Exponentially growing heterogeneous data, with the help of big data analytics, has the potential to provide descriptive, predictive, and prescriptive health insights as well as enable new applications such as telemedicine and remote diagnostics and surgery. Such systems could enhance the overall process of monitoring, diagnosis, and prognosis of diseases.",IEEE Access,2017.0,10.1109/ACCESS.2017.2783118,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
0741a20d6548866ee4f958783f10db98c60cb421,https://www.semanticscholar.org/paper/0741a20d6548866ee4f958783f10db98c60cb421,Measuring Code Quality to Improve Specification Mining,"Every software Industry requires the quality of code. Formal specifications are mathematically based techniques whose purposes are to help with the implementation of systems and software. They are used to describe a system, to analyze its behavior, and to aid in its design by verifying key properties of interest through rigorous and effective reasoning tools. These specifications are formal in the sense that they have syntax, their semantics fall within one domain, and they are able to be used to infer useful information. Formal specifications can help with program testing, optimization, refactoring. However, they are difficult to write manually, and automatic mining techniques suffer from 90–99% false positive rates. To address this problem, we propose to augment a temporal-property miner by incorporating code quality metrics. We measure code quality by extracting additional information from the software engineering process, and using information from code that is more likely to be correct as well as code that is less likely to be correct. Keywords— Specification mining, machine learning, software engineering, code metrics, program understanding INTRODUCTION Incorrect and buggy behavior in deployed software costs up to $70 billion each year in the US [7]. Thus debugging, testing, maintaining, optimizing, refactoring, and documenting software, while time-consuming, remain critically important. Such maintenance is reported to consume up to 90% of the total cost of software projects .A key maintenance concern is incomplete documentation up to 60% of maintenance time is spent studying existing software(e.g.,[8]). Human processes and especially tool support for finding and fixing errors in deployed software often require formal specifications of correct program behavior(e.g.,[9]); it is difficult to repair a coding error without a clear notion of what ―correct‖ program behavior entails. Unfortunately, while low-level program annotations are becoming more and more prevalent, comprehensive formal specifications remain rare. Many large, preexisting software projects are not yet formally specified. Formal program specifications are difficult for humans to construct .and incorrect specifications are difficult for humans to debug and modify. Accordingly, researchers have developed techniques to automatically infer specifications from program source code or execution traces [2]. These techniques typically produce specifications in the form of finite state machines that describe legal sequences of program behaviors. Fig.1.Block Diagram Input code for the Quality Measuring Quality Measure Software Output with Percenta ge of Input Code International Journal of Engineering Research and General Science Volume 3, Issue 3, Part-2 , May-June, 2015 ISSN 2091-2730 448 www.ijergs.org LITERATURE SURVEY API-based and Information Theoretic Metrics for Measuring the Quality of Software Modularization [10]. This system is developed using Object oriented software system. Create a set design principles for code modularization and produce set of metrics. Modularization quality is calculated using metrics such as structural, architectural and notions. There are three contributions such as coupling, cohesion and complexity metrics to modularize the software. This metrics seek to characterize a body of software according to the enunciated principles. Provide two types of experiments to validate the metrics. Whaley et al. propose a static miner [1] that produces a single multi-state specification for library code. The JIST[2] miner refines Whaley et al.’s static approach by using techniques from software model checking to rule out infeasible paths. Gabel and Su [3] extend Perracotta using BDDs, and show both that two-state mining is NP-complete and some specifications cannot be created by composing two-state specifications. Lo et al. use learned temporal properties, such as those mined in this article, to steer the learning of finite state machine behavior models [4].Shoham et al. [5] mine by using abstract interpretation, where the abstract values are specifications MINING CHARACTERISTICS This section shows the underlying concepts of mining techniques and their limitations which encourage the researchers to step into incorporating code quality metrics. Specification mining techniques produces specifications but still they have high false positive rates. The Comparison between most of these approaches is provided in the Table 1. In WN miner [6] the specification mining was motivated by the observations of run-time error handling mistakes. In other approaches examining such mistakes, the code frequently violates simple API specifications in exceptional situations. Despite the proliferation of specification-mining research, there is not much report on issues pertaining to the quality of specification miners. This technique is same as that of Engler et al. but is based on assumptions about run time errors, chooses candidate event pairs differently, presents significantly fewer candidate specifications and ranks presented candidates differently. In a normal Table1.A Comparison study execution, events „a’ and „b’ may be separated by other events and difficult to discern as a pair. After an error has occurred, however, the cleanup code is usually much less cluttered and contains only operations required for correctness. The candidate specifications are filtered using varied criteria such as exceptional control flow, one error, data path etc. This highlights the practical importance of the algorithmic assumptions, in particular the use of exceptional control flow. It can serve as a requirement for acceptance. It can even assist inspections by helping to target effort at parts of a program that may need improvement. Though this miner select specifications from software artifacts and finds per-program specifications for error detection, it does not have profound results in bug finding. Strauss, ECC and WN technique were all good at yielding specifications that found bugs. The WN technique found all bugs reported by other techniques on these benchmarks and did so with the fewest false positives. QUALITY METRICS Code metrics like LOC and Cyclomatic Complexity examines the internal complexity of a procedure whereas this structure metrics examines the relationship between a section of code and the rest of the system. Process oriented metrics are used through the different phases of the software life cycle. Measurement on quality should concentrate on the early phases in the life cycle to improve the quality of software and decrease of development and maintenance costs. Defects must be tracked to the release origin which is the portion of the code that contains the defects and at what release the portion was added, changed, or enhanced. When calculating the defect rate of the entire product, all defects are used; when calculating the defect rate for the new and changed code, only defects of the release origin of the new and changed code are included. On the one hand, the process quality metrics simply means tracking defect arrival during formal machine testing for some organizations. On the other hand, some software organizations with well-established software metrics programs cover various parameters in each phase of the development cycle. International Journal of Engineering Research and General Science Volume 3, Issue 3, Part-2 , May-June, 2015 ISSN 2091-2730 449 www.ijergs.org ACKNOWLEDGMENT I would like to express my sincere gratitude towards my guide Prof.Umesh Kulkarni for the help, guidance an encouragement in the development of this methodology. They supported me with scientific guidance, advice and encouragement, and were always helpful and enthusiastic and this inspired me in my work. I have benefitted from numerous discussions with guide and other colleagues. CONCLUSION Formal specifications have a variety of applications including testing, maintenance, optimization, refactoring, documentation, and program repair. However, such specifications are difficult for human programmers to produce and verify manually, and existing Miners Characteristics Comment Engler et al. Use two state temporal properties. High false positive rates Whaley et al. Produces Single multi state specification Human intervention Strauss focuses on machine learning to learn a Single specification from traces Use of single specification is not sufficient JIST Refines Whaley et al. technique to mainly disregard infeasible paths Handles only simple subset of Java WN miner Selecting specifications from software artifacts Does not have profound results in finding bugs Claire Use measurements of trustworthiness of source code to mine specifications Does not give adequate results over precision. International Journal of Engineering Research and General Science Volume 3, Issue 3, Part-2 , May-June, 2015 ISSN 2091-2730 450 www.ijergs.org automatic specification miners that discover two-state temporal properties have prohibitively high false positive rates. The goal of this survey is to support the study on the legacy of generating specifications to the new automatic techniques. It helps to get an insight into this dynamic field of study in Specification Mining. Since the object orientation is emerging in all kinds of applications, it is also welcome in the specification mining process. It is mentioned to be dynamic, as these approaches are under development and it steps higher everyday to achieve efficiency in capturing specifications. REFERENCES: [1]J. Whaley, M. C. Martin, and M. S. Lam, ―Automatic extraction of object-oriented component interfaces,‖ in ISSTA, 2002. [2]R.Alur, P.Cerny, P.Madhusudan, and W. Nam, ―Synthesis of interface specifications for Java classes,‖ in POPL, 2005. [3]M. Gabel and Z. Su, ―Symbolic mining of temporal specifications, ―in ICSE, 2008, pp. 51–60. [4]D. Lo, L. Mariani, and M. Pezz`e, ―Automatic Steering of Behavioral Model Inference,‖ in FSE. ACM, 2009,",,2015.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
4867a63e5edd7ee10f28067d6dd5b07d69321de7,https://www.semanticscholar.org/paper/4867a63e5edd7ee10f28067d6dd5b07d69321de7,Thesis Title: A Framework for Improving the Performance of Signature-based Network Intrusion Detection Systems,"Network Intrusion detection systems (NIDSs) have been widely deployed in different network environments (e.g., banks, schools) to defend against a variety of network attacks (e.g., Trojans, worms). Generally, a network intrusion detection system can be classified into two categories: signature-based NIDS and anomaly-based NIDS. In realworld applications, the signature-based NIDS is more prevalent than the anomaly-based detection as the false alarm rate of the former is much lower than the latter. However, we identify three major issues that can greatly affect the performance of a signature-based NIDS. Expensive signature matching. The traditional signature matching in a signature-based NIDS is too expensive that the computing burden is at least linear to the size of an incoming string. Therefore, the operational burden of a signature-based NIDS could be significantly increased in a large-scale network environment. Overhead network packets. In a large-scale network environment, a signature-based NIDS usually has to drop lots of network packets since the number of incoming packets exceeds its maximum processing capability. Massive false alarms. Although the false alarm rate of a signature-based NIDS is much smaller than that of an anomaly-based NIDS. The number of false alarms generated by a signature-based NIDS can still increase the difficulty in analyzing true alarms and adversely affect the analysis results. To mitigate the above issues, in this thesis, we propose several approaches in improving the performance of a signature-based NIDS such as Snort in the following three aspects: Signature matching improvement.We design an exclusive signature matching scheme to help perform a more efficient signature matching with the purpose of enhancing the performance of signature matching in a heavy traffic environment. Network packet filtration and reduction. To mitigate this issue, we advocate the method of constructing a packet filter such as blacklist-based packet filter, list-based packet filter and trust-based packet filter to help filter out target network packets for a signature-based NIDS such as Snort in terms of IP reputation. This packet filter can be deployed in front of a signature-based NIDS and reduce its workload in an intensive traffic network. False alarm reduction. To resolve this issue, we design several false alarm filters such as machine-learning based false alarm filters, alarm filters using knowledge-based alert verification and context-based alarm filters to help reduce false alarms (or non-critical alarms) that are generated by a signature-based NIDS. A Framework. In addition, we further propose a framework by combining the above work to overall improve the performance of a signature-based NIDS such as Snort. As a case study of the framework, we implement an enhanced filter mechanism (shortly EFM) that consists of three major components: a context-aware blacklist-based packet filter, an exclusive signature matching component and a KNN-based false alarm filter. In particular, the component of context-aware blacklist-based packet filter is responsible for filtering out network packets in terms of IP reputation. The exclusive signature matching component is implemented in the context-aware blacklist-based packet filter and aims to speed up the signature matching. At last, the component of KNN-based false alarm filter is responsible for filtering out false alarms which are produced by the context-aware blacklist-based packet filter and the NIDS. In the evaluation, the experimental results demonstrate that our framework is promising and by deploying with the EFM, the performance of a signature-based NIDS such as Snort can be improved in the aspects of network packet filtration, signature matching improvement and false alarm reduction.",,2013.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
6fea2c3423dcaf2155a43adc8f3b4b9804da0e74,https://www.semanticscholar.org/paper/6fea2c3423dcaf2155a43adc8f3b4b9804da0e74,Attaining Situational Understanding in the Space Domain,"The information available today in the space domain consists primarily of basic positional, mission, and status data for ground and space based assets. This data provides a necessary, but not sufficient, basis for understanding the true situation of the overall space domain. Experts analyze this information, put it into context with other ongoing events, and then make assessments of the risks posed to allied assets. The potential for unknown, unexpected, and unprecedented situations to overwhelm this manual process is increasing as the number of space faring nations and orbiting objects increases. This paper describes a product family called iSpace that Lockheed has created, and continues to invest in, to help tackle the problems of attaining space information more timely, deriving deeper space situational understanding from the data, and integrating components and tools generated throughout the world-wide industry to contribute towards a comprehensive space solution. We will also update the group on iSpace’s usage in the USSTRATCOM Space Situational Awareness (SSA) Table Top Exercises (TTX), used to explore future SSA cooperation concepts and procedures among allied nations. 1. Background Technological improvements have dramatically expanded space accessibility for commercial and civil use, reduced active payload size, and diversified space-based and ground-based space sensing capabilities. With new SSA sensing capabilities, such as Space Fence, the number of Resident Space Objects (RSOs) will dramatically increase in the near future. Moreover, the natural of the space domain, as viewed by many countries of the world, is also changing. On September 20, 2016, Gen. John Hyten, the head of Air Force Space Command at that time, told the Senate Armed Services Committee in written testimony that space control programs and a battle management command and control system should be among the Defense Department’s top space priorities. Gen Hyten also said the Joint Interagency Combined Space Operations Center (JICSpOC1) will “change the warfighting culture of our space cadre as well as ensuring we have the ability to fully plan and employ our space control capabilities.” [1] General Raymond, the current lead of Air Force Space Command continues to pursue and expand the vision elaborated by Gen Hyten. It is these changes that are forcing Space Situational Awareness (SSA) and Command and Control (C2) functions to rapidly evolve to attain a better situational understanding of the space domain. 1 The JICSpOC has been renamed to the National Space Defense Center (NSDC) Copyright © 2017 Advanced Maui Optical and Space Surveillance Technologies Conference (AMOS) – www.amostech.com 2. Lockheed’s iSpace Product Family Lockheed Martin (LM) has been intimately involved in Space C2 since its inception. We developed one of the first Space C2 capabilities back in the late 1950s and have remained prime developers and maintainers of crucial U.S. Space C2 and sensor systems for Air Force Space Command. LM-corporate extensive programmatic and research & development efforts in all aspects of the space domain helps ensure our latest Space C2 and Battle Management Command & Control (BMC2) products address today’s, and are ready for tomorrow’s, space challenges. To address the evolving needs of the space domain, Lockheed has developed a product family called iSpace, shown in Figure 1, composed of three layers of products that can work together or independently. Starting at the bottom layer are a set of Sensor Mission Processor products that can be paired with sensors to provide local catalog management, net-centric communications to one or more C2/BMC2 centers as well as other sensors, policy-basedsensor-control and tasking and a number of other key features. At the middle layer in is the iSpace C2/BMC2 product. It allows you to: add sensors to the system dynamically, receive and process measurements from sensors and other entities, perform automated catalog management, designate a set of sensors as taskable, detect and alert to space events of interest, and to identify threat conditions and auto-generate Courses-OfActions (COAs). iSpace C2/BMC2 supports Radar, Passive RF and Optical space/ground based sensors. An open architecture including netcentric communications and displays provide a platform that is rapidly deployed, configured, and customized. This product is in operational use today and is the C2 SSA capability being used for the USSTRATCOM Global Sentinel 2017 (GS 17) international SSA Table-Top-Exercise (TTX), held in September 2017. The top iSpace product layer represents the Space Event Risk Assessment (SERA) tool. iSpace SERA accepts inputs from C2/BMC2 systems identifying the current space situation in terms of orbits and events of interest. SERA is also fed with various INTEL assessments. Using this information SERA employs Artificial Intelligence (AI) by means of a rule-based expert system engine, probabilistic reasoning, and machine learning to look-ahead for potential risks that may be indicated based on the current and predicted information. iSpace SERA was used by USSTRATCOM for GS-16 and is currently being used in the GS-17 exercise. 3. USSTRATCOM Global Sentinel SSA Experiment The SSA TTX III experimented with different C2 constructs to determine which construct enhanced SSA to the greatest degree. The United States, Australia, Canada, France, Germany, Japan, the United Kingdom, and a commercial consortium participated. The baseline construct was distributed space operations with each C2 center collaborating much as they do today. A variant was introduced to determine if having a central coordination center with representatives from each country and a commercial representative would enhance SSA. The center was called the FedSpOCC and iSpace SERA was employed. Scenario events included conjunctions, maneuvers, breakups, launches, and an “unusual” event where a rocket body cataloged as being inactive moved into a co-planar condition with a Federation asset. Figure 1 LM’s iSpace Product Family – Products can operate together or standalone Copyright © 2017 Advanced Maui Optical and Space Surveillance Technologies Conference (AMOS) – www.amostech.com The ability of the Federation players to respond to these events with different C2 structures was measured. SERA predicted risks induced to defended assets by red and grey geo-synchronous satellites that were moving along the geosynchronous belt. The risk of close proximity was predicted weeks and months in advance using SERA’s rules. No other Federation tools detected these events. As shown in Figure 2, iSpace will have a larger role in GS17. The iSpace C2 Center product will be used by approximately 40 SpOC operators from Australia, Canada, France, Germany, Italy, Japan, The United Kingdom, and the United States. iSpace-SERA will be employed by each SpOC as well. Table 1 Statistics Table 1 provides some key statistics about the scenario. It includes real world unclassified satellites and observations as well as simulated events. The events are comprised of breakups, conjunctions, maneuvers, launches, reentries, Rendezvous and Proximity Operations (RPO), ASATs, Lasing, and anomalous behavior. The iSpace modeling and simulation engine drives the events and also controls the sensor tasking using its Sensor Network Simulator. Another experiment is under planning for 2018 adding more events and more scenarios. 4. Impact of Enterprise Architectural Alternatives A successful space enterprise will consist of capabilities throughout industry, academia and government. Products must employ industry standards, have Software Development Kits (SDKs) supporting integration of third-party internal components, have key internal components swappable, provide the ability to dynamically integrate new sensors, and have integrated community XML standards and web services supporting flexible external interfacing. However, these steps by themselves do not ensure success at the enterprise-level. Industry off-the-shelf capabilities can be aided or hindered by the choices made for the enterprise architecture itself. When building a space enterprise, fundamental architectural considerations have a major influence on which thirdparty capabilities can be efficiently employed, the resiliency of the solution, and the overall probability of success of the enterprise itself. Consider two enterprise architectural extremes shown in Figure 3. At one extreme the enterprise consists of a system-of-systems where ideally the separate systems are loosely coupled from one another through the use of industry standard interface mechanisms and protocols. At the other extreme all capabilities are integrated into a single overall system that share one common infrastructure. There are lots of variations in-between the two extremes. In the case of a common infrastructure that includes just Virtual Machines (VMs) and a security infrastructure, the single system approach is not that far away from the Scenario Characterization",,2017.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
ec788f197ca6377cccf922d2bb0bfda90409ff3e,https://www.semanticscholar.org/paper/ec788f197ca6377cccf922d2bb0bfda90409ff3e,The Semantic Web. Latest Advances and New Domains,,Lecture Notes in Computer Science,2015.0,10.1007/978-3-319-18818-8,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
08e58aee98edff00e011189a2a44b8a0af8d9904,https://www.semanticscholar.org/paper/08e58aee98edff00e011189a2a44b8a0af8d9904,Dempster-Shafer evidence theory for automated bearing fault diagnosis,"Bearing failures can lead to a total machine dysfunction. Since several decades ago, various bearing faults diagnostic techniques have been developed such as envelops detection, bearing faults frequency monitoring, and empirical mode decomposition. However, the success of these methods to diagnose bearing faults depends on the knowledge and experience of the individual personnel that undertakes the task. Artificial intelligence methods such as support vector machine provide an alternative solution to automate the process of bearing fault diagnosis and thus reduced human intervention in the process. This paper studied the application of Dempster-Shafer theory algorithm with the objective to improve the classification accuracy of support vector machine for multibearing fault diagnosis. Analysis results showed that the combination of support vector machine and Dempster-Shafer theory had improved the support vector machine classification accuracy for bearing fault diagnosis from 82% to 94%. Therefore, the combination of support vector machine and Dempster-Shafer theory model is found to be a more effective artificial intelligence method as compared to sole support vector machine model for automated bearing fault diagnosis. Key words: Bearing Diagnosis, Dempster-Shafer, Support Vector Machine and Vibration. Cite this Article: K. H. Hui, M. H. Lim, M. S. Leong and S. M. A. Al-Obaidi, DempsterShafer Evidence Theory For Automated Bearing Fault Diagnosis. International Journal of Mechanical Engineering and Technology, 8(4), 2017, pp. 277–288. http://www.iaeme.com/IJMET/issues.asp?JType=IJMET&VType=8&IType=4 1. INTRODUCTION Bearing is one of the most common components of a machine. It plays a vital role in the operation of the machine. Bearing failures can lead to a total machine malfunction if it is not detected in time. Vibration spectra analysis has been proven to be the most effective diagnostic method for rotating machinery faults diagnosis [1]. A great number of vibration signal analysis techniques were introduced in recent years for bearing fault diagnosis such as empirical wavelet transform Dempster-Shafer Evidence Theory For Automated Bearing Fault Diagnosis http://www.iaeme.com/IJMET/index.asp 278 editor@iaeme.com (EWT) [2], [3], time-frequency manifold (TFM) sparse reconstruction [4], combination of EWT and operational modal analysis (OMA) [5], multivariate empirical mode decomposition [6], bond graph modeling [7], etc. These signal processing methods advanced from non-adaptive to selfadaptive signal analysis [8]. However, one of the greatest challenges of these diagnosis methods is over-relied on the knowledge and experience of the industrial machine operator. In recent years, there is an increased interest in adopting artificial intelligence (AI) approach in machinery faults diagnosis. This approach provides a consistent diagnostic result based on a synthesized AI model and thus leads to an automated faults diagnosis system which eliminates any human intervention. Although AI based machinery fault diagnosis provides a more consistent diagnostic result, its accuracy is highly dependent on the AI algorithm applied to analyze the input data. In other words, the accuracy of diagnostic based on artificial neural network (ANN) and support vector machine (SVM) could be distinctly different. Kankar et al. observed that the classification accuracy for SVM is better than of ANN in bearing fault diagnosis [9]. In addition, the ability of SVM in handling high number of input feature with small sampling data set had made SVM the preferred and more superior choice among all the AI algorithms typically used in faults diagnosis as the availability of real fault data set is generally very limited [10]–[12]. Although SVM was deemed to be a better option in automated diagnostic method, however, it is designed for binary classification (e.g. healthy and faulty bearing condition). Previous research studies showed that SVM can be adapted for multi-fault classification by decomposing the multi-fault problem into binary problem [13]–[15]. Even the decomposition of multi-fault problem is appeared to be the most promising approach in adapting SVM in multi-fault classification, but, the shortcoming of this approach is no decision can be made if there is contradicted decision made by multiple classifier [16]. In common practice, the decision is made based on first come first serve basis. For instance, once SVM found that the bearing condition has rolling element fault, the algorithm will be terminated, and the decision for the bearing condition will be made as rolling element fault. The accuracy of this approach highly dependent on the binary SVM architecture. Thus, the classification accuracy of SVM reduces significantly for multi-fault classification. The typical SVM classification accuracy is reported as 81% for multi-bearing fault diagnosis [17]. The hypothesis of this paper is the classification accuracy can be improved by comparing and reasoning the decision from each binary SVM for a better decision making process. This paper explores the application of Dempster-Shafer (DS) theory in improving the diagnostic accuracy of the rolling element bearing faults based on the SVM algorithm. 2. THEORETICAL BACKGROUND AI plays an important role to enable automated machinery fault diagnosis. It synthesizes the learning algorithm by samples (training data). In this study, SVM was used for fault classification purpose. Subsequently, the results produced by SVM were further refined by DS theory for ultimate decision-making purpose. The two AI techniques are described in the following sections. 2.1. Support Vector Machine SVM is a supervised machine learning method that relies on statistical learning theory. The ability of this learning method in handling high input features with low sample is a plus point for fault diagnosis [10]. SVM establishes a hyperplane that allocates the majority points of the same class in the same side, whilst maximizing the distance between the two classes to this hyperplane [18]. Figure 1 shows an example of hyperplane created by SVM for two classes (healthy and faulty) by two features (skewness and kurtosis) using Gaussian radial basis function (RBF) kernel. Hsu et al. K. H. Hui, M. H. Lim, M. S. Leong and S. M. A. Al-Obaidi http://www.iaeme.com/IJMET/index.asp 279 editor@iaeme.com proposed RBF kernel function to be the first try kernel function for a SVM model [19]. Chen et al. also found that RBF kernel gives a better test accuracy compared to polynomial kernel [17]. Thus, in this study, SVM with RBF kernel function was deployed for bearing fault classification purpose. Figure 1 SVM’s decision boundary In order to completely classify the four different experimental bearing conditions, four oneversus-all SVM models were developed. A voting strategy was used in for bearing conditions classification. The drawback of this approach is that no decision can be made if there is more than one class with identical votes. However, in this paper, the authors decided to choose the first class among all identical classes. The SVM practitioners commonly apply this practice for multi-classes classification. A SVM-DS model was proposed in this study to improve bearing conditions classification accuracy. DS algorithm essentially acted as the agent in combining results from four SVM models to achieve better classification accuracy. 2.2. Dempster-Shafer Evidence Theory The DS theory was the seminal work of Glenn Shafer (1976) and its conceptual forerunner by Arthur P. Dempster (1967). It is a mathematical theory that deals with uncertain information reasoning. It allows the combination of evidences from multiple sources and provides a measure of confidence (belief function, Bel) that a given event will occur. Let Θ be a finite set of possible answers and φ represents an empty set; the belief function should satisfy the three axioms represented by Eqs. (1) (3). 0 (1) Θ 1 (2) ⋃  ∑ 1 | | ⋂ ∈ ⊂ , ,..., (3) The DS theory consists of three important parameters, namely the mass function (m), belief function (Bel) and plausibility (Pl). Mass function (m) is a basic probability assignment that measures the belief that is committed exactly to a subset. Belief function (Bel) is a lower Dempster-Shafer Evidence Theory For Automated Bearing Fault Diagnosis http://www.iaeme.com/IJMET/index.asp 280 editor@iaeme.com probability that measures the total belief mass that is confined to a subset, whilst plausibility (Pl) is a higher probability that measures the total belief mass that can move into a subset. The most recent applications of DS theory can be found in the fields of medical diagnostic [20], aviation [21], machinery condition monitoring and fault diagnosis [22], [23], maintenance management [24], chemical engineering [25], defence [26], power generation industry [27] and engineering design [28], to name a few. To date, DS theory has been proven to be effective in combining evidences to provide a high level of confidence of the occurrence of an event. 3. FEATURE EXTRACTION The data used for this study was downloaded from the website of Case Western Reserve University Bearing Data Center specifically to represent ball bearing healthy and faulty conditions (rolling element, inner raceway, and outer raceway faults). The arrangement of the test rig used to simulate different conditions of bearing is shown in Figure 2. The test rig consists of a 2 hp motor, a torque transducer, and a dynamometer. A 178 μm fault diameter was introduced to the SKF bearing in order to simulate bearing faults. The motor was operating at approximately 1772 rpm with 1 hp load. Vibration data was collected at a sampling rate of 12,000 samples per second by accelerometers that were attached to the bearing housing. Figure 2 Test rig for the experiment A total of 400 sets of vibration time series were extracted from the raw continuous vibration signal. Then, the 400 se",,2017.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
712aa27546a025c55ed16b76bc2a6bd8b0743469,https://www.semanticscholar.org/paper/712aa27546a025c55ed16b76bc2a6bd8b0743469,Location-Aware Application Deployment in Multi-Cloud,"The client-centric multi-cloud has become a popular cloud ecosystem because it allows enterprise users to share the workload across multiple cloud providers to achieve high-quality services with lower operation costs and higher application resilience. From the perspective of application providers, the location of cloud resources for application deployment significantly impacts the deployment cost and performance of applications, e.g., request response time. This gives rise to the problem of location-aware application deployment in multi-cloud to select suitable cloud resources from widely distributed multi-cloud data centers to balance the cost and performance. Existing research works did not pay full attention to the key impact of the location for application deployment. Therefore, it is urgent to study the problem both theoretically and in practice. In this thesis, innovative optimization methods and machine learning techniques are proposed for three common scenarios, namely composite application deployment, application replication and deployment, and elastic application deployment.
First, this thesis studies the composite application deployment problem with the goal to minimize the average response time of composite applications subject to a budget constraint. We propose a Hybrid Genetic Algorithm (GA)-based approach, i.e., H-GA, for solving the problem with an extremely large search space. H-GA features a newly designed and domain-tailored service clustering algorithm, repair algorithm, solution representation, population initialization, and genetic operators. Experiments show that H-GA can outperform significantly several state-of-the-art approaches, achieving up to about 8% performance improvement in terms of response time, and 100% budget satisfaction in the meantime.
Second, this thesis studies the application replication and deployment problem with the goal to minimize the total deployment cost of all application replicas subject to a stringent requirement on average response time. We propose two approaches under different optimization frameworks to solve the problem. With user requests dispatched to the closest application replicas, we develop an approach under a GA framework for Application Replication and Deployment (ARD), i.e., GA-ARD. GAARD features problem-specific solution representation, fitness measurement, and population initialization, which are effective to optimize the deployment of application replicas in multi-cloud. The experiments show that GA-ARD outperforms common application replication and placement strategies in the industry. With user requests flexibly dispatched among different application replicas, we develop another approach under a two-stage optimization framework, i.e., MCApp. MCApp can optimize both replica deployment and request dispatching by combining mixed-integer linear programming with domain-tailored large neighborhood search. Our experiments show that MCApp can achieve up to 25% reduction in total deployment cost compared with several recently developed approaches.
Third, this thesis studies the elastic application deployment problem to minimize the deployment cost over a time span such as a billing day while satisfying the constraint on average response time. The goal of adapting resources for application deployment in response to dynamic and distributed workloads motivates us to adopt deep reinforcement learning (DRL) techniques. The proposed approach, namely DeepScale, applies a deep Q-network (DQN) to capture the optimal scaling policy that can perform online resource scaling. DeepScale also includes a long short-term memory-based prediction model to allow the DQN to consider predicted future requests while making cost-effective scaling decisions. Besides, we design a penalty-based reward function and a safety-aware action executor to ensure that any scaling decisions made by DRL can satisfy the response time constraint. The experiments show that DeepScale can significantly reduce the deployment cost of applications compared with the state-of-the-art baselines, including Amazon auto-scaling service and recently proposed RL-based algorithms. In the meanwhile, DeepScale can effectively satisfy the constraint on the average response time.
In summary, this thesis studies three new problems for location-aware application deployment in multi-cloud. We propose four novel approaches under different optimization and machine learning frameworks, i.e., HGA, GA-ARD, MCApp, and DeepScale, for solving these problems. New constraint handling techniques are developed to satisfy the practical deployment requirements of enterprise applications.",,,10.26686/wgtn.20099645,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
3a8c8b107db3ea114f98c3c8f08eb81c6be49cbd,https://www.semanticscholar.org/paper/3a8c8b107db3ea114f98c3c8f08eb81c6be49cbd,Analyzing Analytics: Part 1: A Survey of Business Analytics Models and Algorithms,"Many organizations today are faced with the challenge of processing and distilling information from huge and growing collections of data. We see examples of retailers trying to better serve their customers, telecommunications providers trying to more effectively manage their growing networks, cities trying to provide smarter infrastructure and services for their rapidly growing populations, and many more similar instances across multiple industries. These organizations are facing a deluge of information and are increasingly deploying sophisticated mathematical algorithms to model the behavior of their business processes to discover correlations in the data, to predict trends and ultimately drive decisions to optimize their operations. These techniques, are known collectively as ”analytics”, and draw upon multiple disciplines, including statistics, quantitative analysis, data mining, and machine learning. In this survey paper and the accompanying research report, we identify some of the key techniques employed in analytics both to serve as an introduction for the non-specialist and to explore the opportunity for greater optimization for parallel computer architectures and systems software. We are interested in isolating and documenting repeated patterns in analytical algorithms, data structures and data types, and in understanding how these could be most effectively mapped onto parallel systems. Scalable and efficient parallelism is critically important to enable organizations to apply these techniques to ever larger data sets for reducing the time taken to perform these analyses. To this end, we focus on analytical models (e.g. neural networks, logistic regression or support vector machines) that can be executed using different algorithms. For most major model types, we study implementations of key algorithms to determine common computational and runtime patterns. We then use this information to characterize and recommend suitable parallelization strategies.",,2011.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
eb417bd65e7c3d95e209e5310433cbfa704869e1,https://www.semanticscholar.org/paper/eb417bd65e7c3d95e209e5310433cbfa704869e1,AC 2012-4161: A WIRELESS SENSOR NODE POWERED BY SOLAR HAR- VESTER FOR MARINE ENVIRONMENT MONITORING AS A SENIOR DESIGN PROJECT,"Improving the design component in undergraduate engineering education has been an immediate and pressing concern for educators, professional societies, industrial employers and agencies concerned with national productivity and competitiveness. The projects are a valuable component of the science and engineering education. The design experience develops the students’ lifelong learning skills, self-evaluations, self-discovery, and peer instruction in the design’s creation, critique, and justification. Students learn to understand and make use of the manufacturer data sheets, application notes, and technical manuals when developing their design projects. The experience, which would be difficult to complete individually, gives the students a sense of satisfaction and the accomplishment that is often lacking in many engineering courses, using traditional teaching approaches. Furthermore, the design experience motivates student learning and develops skills required in industry. This paper discusses the development of a student project involving a number of senior undergraduate students at our engineering technology program. A simple, low cost solar power system to power a wireless sensor network (WSN) was developed during this project. The proposed WSN may be used for monitoring a coastal shallow water marine environment. It is composed of several sensor nodes or buoys. The description of this harvester, the system characteristics and performances are presented in details. Various aspects of the educational experience are examined such as the educational goals of the project, project organization, and outcomes. Innovative educational approaches are described such as brainstorming session and discussion with students of high-level choices described by a decision tree, component selections, simulations and system performance and characteristics computation. In the second part of the paper the design solution that was adopted is described in details. The adopted design solution includes: power electronics circuitry (DC-DC converter design and test), maximum power point tracking (MPPT) algorithms, control strategies, battery and super-capacitor selection as energy buffers, and overall system performances. Different MPPT and charging algorithms were analyzed and evaluated for their effectiveness in solar energy conversion system, as well as the control algorithms and implementation to maximize the power output. The project is a good example of multi-disciplinary cooperation as well as providing valuable hands-on experience. In addition to providing useful lessons in teamwork and project management, the project will provide a working demonstration wireless sensor network and solar energy system. The goal of the design project is to explore and enhance students understanding of the fundamental engineering principles, power circuit simulation capability and hands-on demonstration of system prototyping. Introduction and Project Rationale The wireless sensor networks (WSNs) is an autonomous network system which consists of large number of micro sensor nodes and has the characteristics of capability of sensing, calculation communication and low cost, and low power. It is a “smart” system that can accomplish various monitoring tasks, according to different environment conditions. Monitoring of water environment is one of its typical applications. Compared with existing real-time automatic water P ge 25120.2 environment monitoring systems, WSNs-based water environment monitoring system has strongpoint as follows : 1) Less effect of the system on ecological environment: nodes transmit water environment parameters to base station by low power and low radiation wireless channel and multi-hop communication protocol. Marine wireless sensor networks offer an unmatched option to a wide range of different domains. The significance of the aforementioned research lies in the fact that it opens the door for a variety of applications as well as new areas of relevant research in wireless networks. The possibility of having hundreds of thousands of sensor nodes diving in the ocean collecting data about the different inhabitants offers a unique opportunity for ocean studies and researchers in the field. The ability to seed wireless sensors that can dive deep in the ocean taking real-time pictures and reporting relevant data about the oceanic life can play a major role in bringing ocean research to new levels. In the following we present the development and designed of a solar energy harvester that can be used to provide power to a WSN for marine environment monitoring system. Coastal marine systems are particularly vulnerable to the effects of human activity attendant on industrial, tourist and urban development. Information and communications technologies offer new solutions for monitoring such ecosystems in real time. Therefore, during the past decade various initiatives emerged, from small-scale networks to complex coastal observation systems. Among small-scale networks, WSNs are a highly attractive solution for its easiness in deployment, operating and dismantling. WSNs are also relatively inexpensive. Energy harvesting or the process of acquiring energy from the surrounding environment has been a continuous human endeavor throughout history, e.g. the use of watermills in ancient Greece, and of sailboats by Phoenicians and Egyptians, circa 4000 B.C. Unlike the conventional electric power generation systems, in energy harvesting concept, fossil fuels are not used and the generation units might be decentralized. There are many sources for harvesting energy. Solar, wind, ocean, hydro, electromagnetic, electrostatic, thermal, vibration, and human body motion are renewable sources of energy. Even the energy of radio frequency waves, propagated due to television and radio broadcasting in the environment, can be harvested. Economic, environmental, and geopolitical constraints on global conventional energy resources started forcing the nations to accelerate energy harvesting from renewable energy sources. Thus, advanced technical methods should be developed to increase the efficiency of devices in harvesting energy from various environmentally friendly resources and converting them into electrical energy. These developments have sparked the interest in engineering community as well as in the engineering education community to develop more energy harvesting applications and new curriculums for renewable energy and energy harvesting topics. Nowadays, there is an increasing interest to harvest energy at a much smaller scale, for applications such as the ones found in many embedded systems where the power requirements are often small (less than 100 mW). Sustaining the power requirement for autonomous wireless and portable devices is an important issue. However, this progress has not been able to keep up with the development of microprocessors, memory storage, and wireless technology applications. For example, in wireless sensor networks, battery-powered sensors and modules are expected to last for a long period of time. However, conducting battery maintenance for a large-scale network consisting of hundreds or even thousands of sensor nodes may be difficult, if not impossible. Ambient power sources, as a replacement for batteries, come into consideration to minimize the maintenance and the cost of operation. Power scavenging may enable wireless and portable electronic devices to be completely self-sustaining, so that battery maintenance can be eventually removed. When P ge 25120.3 compared with energy stored in common storage elements, such as batteries, capacitors, and the like, the environment represents a relatively infinite source of available energy. Systems continue to become smaller, yet less energy is available on board, leading to a short runtime for a device or battery life. Researchers continue to build high-energy density batteries, but the amount of energy available in the batteries is not only finite but also low, which limits the life time of the systems. Extended life of the electronic devices is very important; it also has more advantages in systems with limited accessibility, such as those used in monitoring a machine or an instrument in a manufacturing plant used to organize a chemical process in a hazardous environment. The critical long-term solution should therefore be independent of the limited energy available during the functioning or operating of such devices. 1.1. Objectives of Project Work In response to these demands, universities offering baccalaureate and graduate degrees in electrical engineering or engineering technology must develop curricula to educate a workforce that is well equipped to meet these challenges. Unfortunately, US universities have not kept pace with the growing fields of power electronics, wireless networks or renewable energy, and they are not educating enough students in these recent technology applications, as needed by our industries. Rather this growth has been principally research and industry oriented. Little progress is being reported, with very few notable exceptions on the role of educational institutions in either keeping pace with this growth or addressing the importance of introducing new emerging engineering technologies, applications, and effective classroom and laboratory instruction. Engineering and engineering technology programs must offer a relevant and validated curriculum that prepares students for post-graduation success. Courses that cover traditional subject matter in mathematics, the sciences, engineering economics and other related topics provide the foundation of knowledge upon which specific skill sets are added. However, it is critical for engineering/technology to transition from theoretical work in the classroom towards experiential learning with applications of technology and design. The main objective of senior design courses in engineering and engineering technology",,2012.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
26cfc3a23ca457e094b8d037282ca8748488e2cf,https://www.semanticscholar.org/paper/26cfc3a23ca457e094b8d037282ca8748488e2cf,Portable lab modules on cloud computing,"Cloud computing is a highly scalable model for delivering information technology resources and applications, on demand, as a service, to end users through the network. In recent years, cloud computing has been adopted rapidly and extensively in industry. Notable public cloud services include Amazon Elastic Compute Cloud (EC2) and Google App Engine, among others. There is a fast growing demand for professionals with cloud computing skills. However, the demand is not being fulfilled, partially due to the inability of educational institutions to keep up with technological advancements, as well as a lack of hands-on educational material. We have been developing portable virtual lab modules which can be used to teach basic cloud computing concepts and skills early and often. The labs run on virtual machines and can be ported between different courses and between different platforms. The virtual labs can be deployed in centralized or decentralized ways. To meet the learning outcomes, the students are expected to: (1) comprehend the fundamental concepts of cloud computing; (2) identify the building blocks of cloud computing systems; (3) understand the basic operation of open source cloud infrastructures; and (4) recognize commonly used, commercial cloud computing services and applications.",2013 IEEE Frontiers in Education Conference (FIE),2013.0,10.1109/FIE.2013.6684860,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
3a9dbcbd447262bebf322781082e9913d802e871,https://www.semanticscholar.org/paper/3a9dbcbd447262bebf322781082e9913d802e871,"HcBench: Methodology, development, and characterization of a customer usage representative big data/Hadoop benchmark","Big Data analytics using Map-Reduce over Hadoop has become a leading edge paradigm for distributed programming over large server clusters. The Hadoop platform is used extensively for interactive and batch analytics in ecommerce, telecom, media, retail, social networking, and being actively evaluated for use in other areas. However, to date no industry standard or customer representative benchmarks exist to measure and evaluate the true performance of a Hadoop cluster. Current Hadoop micro-benchmarks such as HiBench-2, GridMix-3, Terasort, etc. are narrow functional slices of applications that customers run to evaluate their Hadoop clusters. However, these benchmarks fail to capture the real usages and performance in a datacenter environment. Given that typical datacenter deployments of Hadoop process a wide variety of analytic interactive and query jobs in addition to batch transform jobs under strict Service Level Agreement (SLA) requirements, performance benchmarks used to evaluate clusters must capture the effects of concurrently running such diverse job types in production environments. In this paper, we present the methodology and the development of a customer datacenter usage representative Hadoop benchmark ""HcBench"" which includes a mix of large number of customer representative interactive, query, machine learning, and transform jobs, a variety of data sizes, and includes compute, storage 110, and network intensive jobs, with inter-job arrival times as in a typical datacenter environment. We present the details of this benchmark and discuss application level, server and cluster level performance characterization collected on an Intel Sandy Bridge Xeon Processor Hadoop cluster.",2013 IEEE International Symposium on Workload Characterization (IISWC),2013.0,10.1109/IISWC.2013.6704672,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
7b590cad93653aee70915e105612492d70c591de,https://www.semanticscholar.org/paper/7b590cad93653aee70915e105612492d70c591de,Fall 2010 Colloquium,"Siemens is a global powerhouse in electronics and electrical engineering, employing more than 400,000 people world-wide and operating in industry, energy and healthcare sectors – driving innovation in key areas such as smart grid, high-speed rail, personalized medicine and many more. With the rise of sensing and storage technologies, data analytics in general, and data processing and statistical machine learning methods in particular, are becoming crucial for managing and optimizing large scale systems . Siemens Corporate Research is one of several Siemens Corporate Technology research and development centers committed to transforming research into practical, innovative solutions and services that support Siemens’ broad range of businesses. In this talk I will discuss several solutions developed by SCR that exemplify the transition of emerging technologies into mature software systems deployed worldwide to monitor and control processes and equipment. Bio: Mathaeus Dejori holds a degree in Electrical Engineering from the Technical University in Graz, Austria and received his PhD in Computer Science from the Technical University in Munich, Germany. He joined Siemens as a research scientist in his last year of his PhD continuing his research in computational biology, machine learning and text mining. In 2008 he joined Siemens Corporate Research (SCR) in Princeton NJ where he is now leading a global team of researchers developing new data driven applications and services.",,2010.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
412d785808c109cc4c9af1b4b45feb14c9c781f5,https://www.semanticscholar.org/paper/412d785808c109cc4c9af1b4b45feb14c9c781f5,A Comparative Study of Selected Classification Algorithms of Data Mining,"Data Mining is the process of extracting hidden knowledge from large volumes of raw data. Data mining has been defined as the nontrivial extraction of previously unknown, implicit and potentially useful information from data. Classification techniques are being used in different industry to easily identify the type and group to which a particular tuple belongs. This work explores the five classification algorithms (Zero, PART, OneR, Prism, J48) are compared on the bases of two techniques (cross validation and percentage split). Experimental comparison was performed by considering the car evaluation dataset and analyzing them using data mining open source WEKA tool. Keywords— ―Data Mining, Classification Algorithms, Car evaluation data set‖ INTRODUCTION Data mining is the motion of all methods and techniques, which allow analyzing very large data sets to extract and discover previously unknown structures and relations out of such huge heaps of details. This information is filtered, prepared and classified so that it will be a valuable aid for decisions and strategies [2]. With the enormous amount of data stored in files, databases, and other repositories, it is increasingly important, if not necessary, to develop powerful means for analysis and perhaps interpretation of such data and for the extraction of interesting that could help in decision-making [3]. The title Data mining is the process of extracting patterns from data. As more data are gathered, with the amount of data doubling every three years [1]. Data mining is the process of discovering knowledge from large amounts of data stored either in databases or warehouses [2]. Data mining is becoming an increasingly important tool to transform these data into information. Data mining can also be referred as knowledge mining or knowledge discovery from data. Many techniques are used in data mining to extract patterns from large amount of database [1]. Classification is a data mining (machine learning) technique used to predict group membership for data instances. Classification is a supervised procedure that learns to classify new instances based on the knowledge learnt from a previously classified training set of instances [5]. It takes a set of data already divided into predefined groups and searches for patterns in the data that differentiate those groups supervised learning, pattern recognition and prediction. Classification consists of predicting a certain outcome based on a given input. It is one of the Data Ashish Kumar Dogra et al, International Journal of Computer Science and Mobile Computing, Vol.4 Issue.6, June2015, pg. 220-229 © 2015, IJCSMC All Rights Reserved 221 Mining techniques that is mainly used to analyze a given dataset and takes each instance of it and assigns this instance to a particular class with the aim of achieving least classification error. It is used to extract models that correctly define important data classes within the given dataset. It is a two-step process. In first step the model is created by applying classification algorithm on training data set. Then in second step, the extracted model is tested against a predefined test dataset to measure the model trained performance and accuracy. So, classification is the process to assign class label for this dataset whose class label is unknown [7]. Find a model that describes the data classes or concepts. This derived model is based on analysis of set of training data. The derived model can be presented in the following forms Classification (IF-THEN) Rules, Decision Trees, Mathematical Formulae and Neural Networks. Classification Algorithm Classification is a classic data mining technique based on machine learning. Basically classification is used to classify each item in a set of data into one of predefined set of classes or groups. Classification method makes use of mathematical techniques such as decision trees, linear programming, neural network and statistics [9]. Classification algorithm finds relationships between the values of the predictors and the values of the target. Components of Data Mining Algorithms i) Model Representation (Knowledge Representation) is the language for describing discoverable patterns / knowledge ii) Model Evaluation is estimating the predictive accuracy of the derived patterns iii) Search methods we have two type of broadly classified search method one is parameter search when the structure of a model is fixed, search for the parameters which optimise the model evaluation criteria and another is model search when the structure of the model is unknown iv) Learning Bias consist of ffeature selection and Pruning algorithm [11]. Data classification is a two steps process in which first step is the training phase where the classifier algorithm builds classifier with the training set of tuples and the second phase is classification phase where the model is used for classification and its performance is analyzed with the testing set of tuples. The algorithm can differ with respect to accuracy, time to completion, and transparency. In practice, it sometimes makes sense to develop several models for each algorithm, select the best model for each algorithm, and then choose the best of those for deployment. Algorithms Evaluated Five Classification algorithms considered for our study are  ZeroR  Part  Prism  OneR  J48 ZeroR:ZeroR is the simplest classification method which relies on the target and ignores all predictors. ZeroR classifier simply predicts the majority category (class). Although there is no predictability power in ZeroR, it is useful for determining a baseline performance as a benchmark for other classification methods [10]. It is the simplest method which relies on the frequency of target. ZeroR is only useful for determining a baseline performance for other classification methods. PART:PART is a partial decision tree algorithm, which is the developed version of C4.5 and RIPPER algorithms .PART is a separate-and-conquer rule learner proposed by Eibe and Witten [54]. The algorithm producing sets of rules called decision lists which are ordered set of rules. A new data is compared to each rule in the list in turn, and the item is assigned the category of the first matching rule (a default is applied if no rule successfully matches). PART builds a partial C4.5 decision tree in its each iteration and makes the best leaf into a rule. The algorithm is a combination of C4.5 and RIPPER rule learning [2]. Prism:This simple and straightforward covering algorithm work by first picking a class from the dataset for which to create a new rule having the class as its conclusion, and selectively adding tests to the condition of the rule, striving for maximum number of instances covered and 100% accuracy. The accuracy of a test is measured by the ratio of the number of positive instances p to the total number of instances covered by the rule. The positive instances covered by the new rule then are removed from the dataset for further rule generation. Then, negative instances should remain in the data set to await a later iteration of the process. This process continues until no more instances remain to covered. Ashish Kumar Dogra et al, International Journal of Computer Science and Mobile Computing, Vol.4 Issue.6, June2015, pg. 220-229 © 2015, IJCSMC All Rights Reserved 222 OneR:OneR or “One Rule” is a simple algorithm proposed by Holt. The OneR builds one rule for each attribute in the training data and then selects the rule with the smallest error rate as its one rule. The algorithm is based on ranking all the attributes based on the error rate .To create a rule for an attribute, the most frequent class for each attribute value must be determined [54]. The most frequent class is simply the class that appears most often for that attribute value. A rule is simply a set of attribute values bound to their majority class. OneR selects the rule with the lowest error rate. In the event that two or more rules have the same error rate, the rule is chosen at random [8]. The OneR algorithm creates a single rule for each attribute of training data and then picks up the rule with the least error rate [4]. J48:J48 classifier is a simple C4.5 decision tree for classification. It creates a binary tree. The decision tree approach is most useful in classification problem. With this technique, a tree is constructed to model the classification process. Once the tree is built, it is applied to each tuple in the database and results in classification for that tuple[1][3]. Tool Used for Performance Evaluation Platform SOFTWARE USED  Operating system Window 8  JAVA version 7  Tool-WEKA 3.7 Introduction to WEKA Waikato Environment for Knowledge Analysis (WEKA) tool is used for the performance evaluation. WEKA is a collection of machine learning algorithms for data mining tasks. WEKA is created by researchers at University of Waikato in New Zealand. It is written in Java and runs on almost any platform . The algorithms in WEKA can either be applied directly to a dataset or called from own Java code. It is also well-suited for developing new machine learning schemes. WEKA is open source software issued under the GNU General public License. It is a collection of open source ML algorithms pre-processing, classifiers, clustering, association rule and visualization [6]. Data Set The dataset is car evaluation data set has been used for the experiment. In this data set their are 1728 number of instances and 7 number of attributes. Result and Discussion Cross Validation Testing Method Cross-Validation is a statistical method of evaluating and comparing learning algorithms by dividing data into two segments: one used to learn or train a model and the other used to validate the model. In cross validation, the training and validation sets must cross-over in successive rounds such that each data point has a chance of being validated against. The basic form of cross-validation is k-fold ",,2015.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
b0632ac81dac415a61be08fd09fa45c26fa5b769,https://www.semanticscholar.org/paper/b0632ac81dac415a61be08fd09fa45c26fa5b769,A Systematic Literature Review of the Personnel Assignment Problem,"Context: Personnel assignment (PA) is an important problem in industry. In general it is about assigning the right people to the right tasks. Operations research plays a big role in solving such problems. Objective: In this paper, we study the personnel assignment problem (PAP) and the proposed solutions to solve it. In addition to that, we aim to identify promising future works from the study results. Methods: We take a systematic approach towards studying the literature of the PAP. A general systematic review method, which has been recently used by a number of researchers in the field of software engineering, was modified and deployed in this study. Results: The analysis results reveal potential solution approaches, the trends in application of existing solution methods, and some potential future research areas. The review process is based on our variation of an existing literature review method. This variation is also presented in the paper. Conclusions: Although a concern in industry, PAP has not been widely studied when compared to other similar fields of research. It has been mainly studied in operations research and in the context of military personnel assignments. It seems that artificial intelligence and machine learning still have a good potential to contribute to this field of research in different applications. Application of PAP in software engineering (SE) is an open area of research. For instance, it looks promising for developer or bug assignments in software development projects.",,2013.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
97acadcfdef01a80431c532065efb5fddfba2dfb,https://www.semanticscholar.org/paper/97acadcfdef01a80431c532065efb5fddfba2dfb,Explanation Methods for Bayesian Networks,"The international maritime industry is growing fast due to an increasing number of transportations over sea. In pace with this development, the maritime surveillance capacity must be expanded as well, in order to be able to handle the increasing numbers of hazardous cargo transports, attacks, piracy etc. In order to detect such events, anomaly detection methods and techniques can be used. Moreover, since surveillance systems process huge amounts of sensor data, anomaly detection techniques can be used to filter out or highlight interesting objects or situations to an operator. Making decisions upon large amounts of sensor data can be a challenging and demanding activity for the operator, not only due to the quantity of the data, but factors such as time pressure, high stress and uncertain information further aggravate the task. Bayesian networks can be used in order to detect anomalies in data and have, in contrast to many other opaque machine learning techniques, some important advantages. One of these advantages is the fact that it is possible for a user to understand and interpret the model, due to its graphical nature. This thesis aims to investigate how the output from a Bayesian network can be explained to a user by first reviewing and presenting which methods exist and second, by making experiments. The experiments aim to investigate if two explanation methods can be used in order to give an explanation to the inferences made by a Bayesian network in order to support the operator’s situation awareness and decision making process when deployed in an anomaly detection problem in the maritime domain.",,2009.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
b42f4e46a6af7aa9dd98a76c39de417ce52ae461,https://www.semanticscholar.org/paper/b42f4e46a6af7aa9dd98a76c39de417ce52ae461,Language Resources as by-Product of Evaluation: The MULTITAG Example,"In this paper, we show how the paradigm of evaluation can function as language resource producer for high quality and low cost validated language resources. First the paradigm of evaluation is presented, the main points of its history are recalled, from the first deployment that took place in the USA during the DARPA/NIST evaluation campaigns, up to latest efforts in Europe (SENSEVAL2/ROMANSEVAL2, CLEF, CLASS etc.). Then the principle behind the method used to produce high-quality validated language at low cost from the byproducts of an evaluation campaign is exposed. It was inspired by the experiments (Recognizer Output Voting Error Recognition) performed during speech recognition evaluation campaigns in the USA and consists of combining the outputs of the participating systems with a simple voting strategy to obtain higher performance results. Here we make a link with the existing strategies for system combination studied in machine learning. As an illustration we describe how the MULTITAG project funded by CNRS has built from the by-products of the GRACE evaluation campaign (French Part-Of-Speech tagging system evaluation campaign) a corpus of around 1 million words, annotated with a fine grained tagset derived from the EAGLES and MULTEXT projects. A brief presentation of the state of the art in Part-Of-Speech (POS) tagging and of the problem posed by its evaluation is given at the beginning, then the corpus itself is presented along with the procedure used to produce and validate it. In particular, the cost reduction brought by using this method instead of more classical methods is presented and its generalization to other control task is discussed in the conclusion. 1. The paradigm of Evaluation Comparative evaluation in language engineering has been used as a basic paradigm in the USA DARPA program on human language technology since 1984. Activities similar in kind, have been pursued in Europe, both at national and at European level, but on a smaller scale and over a limited time (Mariani and Paroubek, 1999). The latest efforts concerning evaluation in Europe are CLEF (Cross Language Text Retrieval System Evaluation in collaboration with NIST and TREC conference), SENSEVAL2/ROMANSEVAL-2 (Kilgarriff, 1998) and CLASS (evaluation across FP5 project clusters). Comparative evaluation is a paradigm in which a set of participants compare the results of their systems using the same or similar control tasks (Bernsen et al., 1999) and related data with metrics that are agreed upon. More precisely, Comparative evaluation consists in (1) choosing or creating a control task, (2) in gathering system or component developers and integrators who are interested in testing their systems against those of others, (3) in organizing an evaluation campaign which necessarily involves distributing linguistic data for training and testing the systems, and (4) in defining the protocol and the metrics which will be used in the results assessment. A control task is the function that the participating systems have to perform during an evaluation together with the conditions under which this function must be performed (e.g. for parser evaluation, a control task can be the bracketing of the constituents). Every deployment of the paradigm of evaluation in the field of Language Engineering entails the production of linguistic data: the organizers build the reference and test data sets and the participants apply their systems on test data to produce evaluation data. When a quantitative black-box (Sparck-Jones and Galliers, 1995) evaluation methodology is applied the data produced is generally abundant and could easily be re-used as training material if the cost of filtering out the errors was not so high. 2. Combining to Improve In machine learning, it is well known that ensemble methods or committees of learning machines can often improve the performance of a system in comparison to a single learning machine. A very promising algorithm based on this principle now under investigation is Ada boost (Schwenk, 1999). In the same field, people have applied for a long time “winner take all” strategies to combine, inside the same system, the output of several basic processing units (Simpson, 1990). In the course of its evaluation program on speech recognition (S., 1998), NIST developed the ROVER (Recognizer Output Voting Voting Error Reduction) (Fiscus, 1997), to produce a composite Automatic Speech Recognition system output from the output of several ASR systems. Such composite system has an error rate inferior to the one of any of its components. In the ROVER, the output of several ASR systems is first combined into a single transition network using a modified version of the dynamic programming alignment technique used by NIST to score ASR systems1. This network is then explored and a simple voting strategy (highest number of votes) is used to select the best scoring word at each decision point. In (Fiscus, 1997), NIST reports a incremental 5.6% Word Error Rate reduction (12.5% relative) using voting by frequency of occurrence and maximum confidence (the output of the ASR systems was annotated with confidence measures (Chase, 1997)). Concerning, POS tagging, the principle of combination has been used in the past by (Marquez and Padro, 1998) who combines two taggers to annotate a corpus and by (Tufis, 1999) who uses several versions of same tagger but trained on different data. The SCLITE tool is freely available from http://www.itl.nist.gov/iaui/894.01/software.htm 3. The GRACE POS tagging evaluation campaign and its data. GRACE(Adda et al., 1999) was the first large scale evaluation campaign for Part-Of-Speech tagging for the French language. It was part of the French program CCIIL (Cognition, Intelligent Communication and Language Engineering), jointlypromoted by the Engineering Sciences and Human Sciences departments of the CNRS. The call for tenders was published in November 1995 and the first year has been devoted to bootstrapping the program by defining and installing the different organization committees. From the participants point of view, GRACE was made of 3 phases: training, dry-run and test. The first one was used by the participant to calibrate their systems on untagged data, the two others were complete runs of the evaluation protocol were the participants had to tag a large amount of text and to provide a mapping between their tagset and the reference tagset which had been derived with their collaboration from the EAGLES format (Leech and Wilson, 1995). The training corpus was distributed globally to all the participants in January 1996, while the dry run corpus was distributed individually to each participant in an encrypted form during the fall of 1996. The results were discussed during a workshop restricted to the participants, a satellite event to the Journees Scientifiques et Techniques du Reseau FRANCIL, in April 1997 (Adda. et al., 1997). The test corpus was distributed in the same manner as for the dry run, at the end of December 1997. The preliminary results of the tests were discussed with the participants in a workshop in May 1998. The final results were disclosed on the WEB2 during fall of 1998 as soon as they had been validated by the organizers (cross validation with two different processing chains based on different algorithms and developed at two different sites) and the participants. At the beginning there were 18 participants from 5 different countries (CA, USA, D, CH, FR), from both public research and industry, and 3 evaluators EPFL, INaLF-CNRS and Limsi-CNRS.The 2 corpus providers were Limsi and INaLF. Out of the 21 initial participants, 17 only took part in the dry run and only 13 completed the tests. The size of the training corpus was around 10 million words of untagged texts, evenly distributed between literary works and newspaper articles. For the dry run, the participants tagged a corpus of roughly 450,000 words with a similar genre distributionand the performance measure was computed over 20,000 words to which a reference description had been manually assigned. For the tests, the participants had to mark a corpus of 650,000 words and the measure was taken over 40,000 words. GRACE used the quantitative black box metrics: Decision and Precision, which were derived especially for GRACE from the metrics used in Information Retrieval (Precision and Recall). Precision measures the ability of a POS tagger to assign a correct tag to a given word form, and Decision measures the capacity of a POS tagger to restrict for a given word form the number of candidate tags with respect to a given tagset. One of the lessons to draw from the GRACE experience, is that ideally, results should be cross-validated with two different processing chains, based on different algorithms http://www.limsi.fr/TLP/grace (when this is possible) and developed at two different sites in order to ensure their accuracy and quality. The evaluation toolkit of GRACE has been packaged as a demonstration by the ELSE project and is freely available3. GRACE proved to be a success; its results are: a better knowledge of the existing systems in each domain and of their state of development; precise evaluation metrics defined in collaboration with the participants; an evaluation toolkit freely available, a new product on the market (one participant decided to add a tagger to his catalogue as a result of his participation); the creation of a community of actors interested in evaluation; and last of all, the initial data to build the new linguistic resource described here.",LREC,2000.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
98c88a8cb8c0c6e2e9092eedabf67814054c5b22,https://www.semanticscholar.org/paper/98c88a8cb8c0c6e2e9092eedabf67814054c5b22,Applying agent technology in Environmental Management Systems under real-time constraints,"Changes in the natural environment affect our quality of life. Thus, government, industry, and the public call for integrated environmental management systems capable of supplying all parties with validated, accurate and timely information. The 'near real-time' constraint reveals two critical problems in delivering such tasks: the low quality or absence of data, and the changing conditions over a long period. These problems are common in environmental monitoring networks and although harmless for off-line studies, they may be serious for near real-time systems. In this work, we discuss the problem space of near real-time reporting Environmental Management Systems and present a methodology for applying agent technology this area. The proposed methodology applies powerful tools from the IT sector, such as software agents and machine learning, and identifies the potential use for solving real-world problems. An experimental agent-based prototype developed for monitoring and assessing air-quality in near real time is presented. A community of software agents is assigned to monitor and validate measurements coming from several sensors, to assess air-quality, and, finally, to deliver air quality indicators and alarms to appropriate recipients, when needed, over the web. The architecture of the developed system is presented and the deployment of a real-world test case is demonstrated.",,2004.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
88ea92d8b618525ff2a7815c08f0fc2c68d8daa2,https://www.semanticscholar.org/paper/88ea92d8b618525ff2a7815c08f0fc2c68d8daa2,Human Activity Inference via physical sensing in support of Industrial Equipment Maintenance,"The paper describes an active research project at Intel’s High Volume Manufacturing (HVM) facility located at Leixlip, Co. Kildare, Ireland. The project explores the practical aspects of deploying RFID transponders, subtle sensing platforms and machine learning based inferencing in a harsh, realworld environment. The key features of the sensing platform, the data collection process and the translation of data into information using visualization and inferencing techniques are described.",,2006.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
919d6334d721970a2b2b9428e16013b1e9677715,https://www.semanticscholar.org/paper/919d6334d721970a2b2b9428e16013b1e9677715,Swarms in the Third Offset,"Advances in swarm technology is part of the Department of Defense’s Third Offset Strategy which is a plan for overcoming reduced military force structure and declining technological superiority against potential U.S. adversaries. The components of the Third Offset represent the enabling capabilities of swarm behavior which could be adopted in the future force. Therefore, this paper investigates whether the U.S. military should focus greater research and development efforts on swarm-capable systems that are low-cost, numerous, unmanned, and fast. The first area of discussion includes swarm initiatives that could allow the military to transition away from expensive and heavy weapons platforms. Second, self-driving vehicles, automated logistics, and aerial drones in industry could translate to autonomous supply trains, reduced soldier error, and targeting missions in the military. Third, adversaries are pursuing swarm capabilities. While swarms show great promise, there are some major legal and ethical obstacles to swarm-capable systems. Lastly, recommendations are offered as a way ahead for swarm initiatives. Swarms in the Third Offset The fiercest serpent may be overcome by a swarm of ants. —Admiral Isoroku Yamamoto During the 2017 Super Bowl 51 half-time performance, Intel demonstrated the control of 300 drones and broke the world record a few months earlier with 500 drones controlled by a single operator. In October of 2016, the U.S. Military conducted the largest deployment ever of micro-swarms. Dubbed the Perdix micro drone, these small, inexpensive, battery-powered, propeller-driven air vehicles were launched by three F/A18 Super Hornets. Just over a year prior in 2015, the Advanced Robotic Systems Engineering Laboratory at the Naval Postgraduate School in Monterey, California, held the record with 50 simultaneous airborne unmanned aerial vehicles controlled by a single operator. This author predicts that 500 drones will quickly increase to 1,000 and 10,000 agents in just a few a year’s time while being scalable, adaptable, distributed, and collective. 5 Advances in swarm technology is part of the Department of Defense’s (DoD) Third Offset Strategy which is a plan for overcoming (“offsetting”) reduced military force structure and declining technological superiority against potential U.S. adversaries. 6 Key components of this strategy include dominance in artificial intelligence, machine learning, robotics, unmanned systems, and increased autonomy. Collectively, the components of the Third Offset represent the enabling capabilities of swarm behavior which could be wholly adopted in the future force. Therefore, should the DoD focus research and development efforts on swarm-capable ISR (intelligence, surveillance, reconnaissance) and weapons systems that are lowcost, numerous, unmanned, and fast? This paper will provide an overview of swarms and explore three major areas to address this primary question. While the promise of 2 swarm behavior appears great, there are some major obstacles to swarm-capable systems which will be presented. Lastly, recommendations will be offered as a way ahead for DoD swarm initiatives. Swarms can offer multiple operational advantages in terms of speed, intelligence gathering, coordinated effects (kinetic and non-kinetic), and efforts. Humans typically cannot control more than four to five discrete elements at a time. Swarm-based systems allow for collective and distributed control of hundreds and thousands of agents where the operator is executing mission objectives and not focused on individual agent control. Humans will maintain overall operational control but low-level decisions from individual agents within the swarm could be made autonomously to fulfill mission objectives within pre-defined rules of engagement. The first area of discussion includes swarm initiatives that could allow the DoD to transition away from expensive, heavy, and human-centric weapons platforms such as legacy tanks, manned fighters, and submarines. These systems are difficult to deploy, require large amounts of maintenance and fuel, and may be out-matched in a many operator to one vehicle paradigm. Second, the advent of self-driving vehicles, automated logistics, and aerial drones in the commercial sector could translate to autonomous supply trains, reduced soldier fatigue and error, and targeting missions in the military. Academia partnered with private industry could propel these innovations since the technology has dual-use capabilities in transportation, search and rescue, agricultural monitoring, and homeland security. The open source community has enabled these advances but has also allowed for their proliferation and use among enemy forces and terrorist organizations.",,2017.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
4b24393c8eb60b5d7ab4b37a0637fd7eb49a8d0f,https://www.semanticscholar.org/paper/4b24393c8eb60b5d7ab4b37a0637fd7eb49a8d0f,Creating Sharable Learning Activities: Examples From A Manufacturing Engineering Curriculum,"Many engineering faculty have been involved in projects to improve teaching and learning using web-based resources. Information-based learning materials have proven to be adaptable and dynamic; they have enhanced the educational process. As the number of people involved in the development of IT-based educational materials expands, the engineering education landscape has become dotted with islands of innovationisolated areas where IT-based materials are available. However, these materials are not available to a large number of users, thereby reducing opportunities for synergy, discourse, and exchange. The NSF-funded Greenfield Coalition has developed a technology strategy to facilitate an ability to re-purpose web-based learning activities for a new context, enabling reuse and exchange. This paper describes Greenfields approach to share learning activities, and describes a suite of material that is available from the Coalition website. A Unique Educational Environment The Greenfield Coalition [1] is a coalition of five universities, three university affiliates, six manufacturing companies, the Society of Manufacturing Engineers, and Focus:HOPE. Focus:HOPE supports an amazing web of programs to underpin its educational objectives. Founded in 1968 after the urban riots in Detroit, it pledges intelligent and practical action to overcome racism, poverty and injusticeto make a difference within the city and its suburbs. Focus:HOPE began by feeding the undernourished needy (women with children and then adding senior citizens), but quickly added programs to enable inner city youth to acquire knowledge to seize opportunities for highly skilled and well paying jobs. Today, an individual may begin the journey by enrolling in First Step or FastTrack. These four and seven week programs use computer-based learning to build fundamental skills in mathematics and English. When the student graduates from FastTrack, they have skills certified at the ninth and tenth grade level in reading and math. This provides the appropriate prerequisite skills for entering the Machinist Training Institute (MTI). MTI is a thirty-one week program in which students earn certification in the operation of material processing equipment (machining), metrology, computer-aided design, computer numerical control, and the associated math, computer, and communication skills. Alternatively, students may also choose to pursue a career pathway through Focus: HOPE's Information Technologies Center. Greenfield presents an opportunity for graduates of MTI to cap their practical experience with further studies toward advanced university degrees. Those students who qualify, enter a 24 week pre-engineering program after completing MTIs basic machining program. After a series of diagnostic tests and interviews they become Candidates in the Center for Advanced 1 Coalition Members: Lawrence Technological University, Lehigh University, Michigan State University, University of Detroit Mercy, Wayne State University; Affiliate Partners: Ohio State University, University of Mich igan, Walsh College. 2 Cincinnati Machine, DaimlerChrysler, Detroit Diesel, Electronic Data Systems, Ford Motor Company, and General Motors Corporation. P ge 837.1 Proceedings of the 2003 American Society for Engineering Education Annual Conference & Exposition Copyright  2003, American Society for Engineering Education Technologies (CAT)Focus:HOPEs manufacturing facility. The Center for Advanced Technologies is a not-for-profit entity which is a first tier supplier of manufactured components and systems to Ford, General Motors, DaimlerChrysler, Detroit Diesel, and the U.S. Department of Defense. The Candidates are employed by Focus:HOPE and work in a broad range of manufacturing, production, and support activities. While this employment provides financial support, more importantly it becomes a real-world laboratory to support their learning. Focus: HOPE Mission Statement Recognizing the dignity and beauty of every person, we pledge intelligent and practical action to overcome racism, poverty and injustice. And to build a metropolitan community where all people may live in freedom, harmony, trust and affection. Black and white, yellow, brown and red, from Detroit and its suburbs of every economic status, national origin and religious Persuasion. We join in this covenant. (Adopted March 8, 1968) Greenfields Instructional Design Strategy The design strategy of the Greenfield Coalition is predicated on the set of beliefs about teaching and learning. In order to implement these beliefs, Greenfield has adopted a blended learning approach [2], where classroom exercises, manufacturing shop floor work experiences and technology merge to form a collaborative, reality-based learning environment. When designing a course, a team of subject matter experts from both academe and industry collaborate with an instructional designer, a programmer, and a media specialist in order to create instructionally-sound, technically-supported, engaging learning activities. The resulting material includes key manufacturing engineering concepts and directly applies these to realworld, on-the-job experiences. Often these materials include templates, tools and step-by-step instructions used by practicing engineers. Using Gagnes Nine External Events of Instruction as a guide [3], Greenfield is able to maximize the motivation for learning, add relevance to the content and foster an active learning atmosphere. Within our learning activities we present situations and pose questions to stimulate Greenfield Beliefs • Learning is a shared responsibility between learner and teacher. • Faculty play a key role guiding students in the learning process. • Learning is made real if it is integrated with real-world experience. • Learners must prepare to engage in classroom experiences. • Learning is a social process, which requires interaction with mentors and peers. • By actively participating in their learning, students achieve deeper understanding and enhanced skills. • Technology is not a silver bullet, which by itself promotes learning, but if used effectively, it can provide new capabilities to support learning.",,2003.0,10.18260/1-2--11707,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
a0987203d293e7ea9c99d76abd194a5679704a11,https://www.semanticscholar.org/paper/a0987203d293e7ea9c99d76abd194a5679704a11,Innovative approaches to urban data management using emerging technologies,"Many characteristics of Smart cities rely on a sufficient quantity and quality of urban data. Local industry and developers can use this data for application development that improves life of all citizens. Therefore, the handling and usability of this data is a big challenge for smart cities. In this paper we investigate new approaches to urban data management using emerging technologies and give an insight on further research conducted within the EC-funded smarticipate project. 
Geospatial data cannot be handled well in classical relational database environments. Either they are just put in as binary large objects or have to be broken down into elementary types which can be handled by the database, in many cases resulting in a slow system, since the database technology is not really tuned for delivery on mass data as classical relational databases are optimized for online transaction processing and not analytic processing. 
Document-based databases provide a better performance, but still struggle with the challenge of large binary objects. Also the heterogeneity of data requires a lot of mapping and data cleansing, in some cases replication can’t be avoided. 
Another approach is to use Semantic Web technologies to enhance the data and build up relations and connections between entities. However, data formats such as RDF use a different approach and are not suitable for geospatial data leading to a lack on usability. 
Search engines are a good example of web applications with a high usability. The users must be able to find the right data and get information of related or close matches. This allows information retrieval in an easy to use fashion. The same principles should be applied to geospatial data, which would improve the usability of open data. Combined with data mining and big data technologies those principles would improve the usability of open geospatial data and even lead to new ways to use it. By helping with the interpretation of data in a certain context data is transformed into useful information. 
In this paper we analyse key features of open geodata portals such as linked data and machine learning in order to show ways of improving the user experience. Based on the Smarticipate projects we show afterwards as open data and geo data online and see the practical application. We also give an outlook on piloting cases where we want to evaluate, how the technologies presented in this paper can be combined to a usefull open data portal. In contrast to the previous EC-funded project urbanapi, where participative processes in smart cities where created with urban data, we go one step further with semantic web and open data. Thereby we achieve a more general approach on open data portals for spatial data and how to improve their usability. 
The envisioned architecture of the smarticipate project relies on file based storage and a no-copy strategy, which means that data is mostly kept in its original format, a conversion to another format is only done if necessary (e.g. the current format has limitations on domain specific attributes or the user requests a specific format). A strictly functional approach and architecture is envisioned which allows a massively parallel execution and therefore is predestined to be deployed in a cloud environment. 
The actual search interface uses a domain specific vocabulary which can be customised for special purposes or for users that consider their context and expertise, which should abstract from technology specific peculiarities. 
Also application programmers will benefit form this architecture as linked data principles will be followed extensively. For example, the JSON and JSON-LD standards will be used, so that web developers can use results of the data store directly without the need for conversion. Also links to further information will be provided within the data, so that a drill down is possible for more details. 
The remainder of this paper is structured as follows. After the introduction about open data and data in general we look at related work and existing open data portals. This leads to the main chapter about the key technology aspects for an easy-to-use open data portal. This is followed by Chapter five, an introduction of the EC-funded project smarticipate, in which the key technology aspects of chapter four will be included.",,2016.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
ffea4feb73ced4ce7b2e36b587b9aac84d829b51,https://www.semanticscholar.org/paper/ffea4feb73ced4ce7b2e36b587b9aac84d829b51,The Age of Thrivability: Vital Perspectives and Practices for a Better World,"Solving Complex Industrial Problems Without Statistics. 2016. Ralph R. Pawlak. Boca Raton, FL: CRC Press. 143 pages. This book fills a unique gap. For those in our community who are not strong in mathematical and analytical methods (but who wish to contribute more fully to the “analyze” stage of improvement projects), Pawlak provides practical examples to demonstrate the value of qualitative reasoning. For those who want to add a layer of common-sense thinking to continuous improvement efforts that are already data driven, he provides the background and the basis for making sure the analytical and computational parts of projects are well-framed. Despite the title, this book does not suggest industrial problems can (or should) be solved without robust mathematical methods and statistics. Rather, Pawlak spends most of the book describing and explaining the qualitative processes that should accompany all problem-solving efforts in industry. Grounded in 14 short case studies, the techniques he shares are simple and immediately applicable: identifying what characteristics distinguish good output from bad, generating “clues,” visual observation, articulating differences, and staging assembly and operations trials, among others. Even without a complete and rigorous analysis, these “clues” may provide enough information for readers to identify and deploy an appropriate corrective action. The examples cover a range of cases, from the new product development process, to assembly, to design, to machining, to troubleshooting the causes for defects. In summary, this is the only book I have ever encountered that comprehensively explains the thought process of the “fuzzy front end” in Lean Six Sigma problem solving. As a result, it provides excellent case studies that can be used as a first step toward understanding problem solving in common industrial scenarios. Despite a sometimes repetitive feel, the conversational tone of this book makes the material particularly accessible to beginners who might benefit the most from that repetition. “It does not matter that some problems that you encounter will require knowledge beyond your capacity,” Pawlak advises. In fact, the author could have gone further: the primary weakness of this book is that each of the case studies could easily have been two or three times as long and still maintained reader interest. Overall, the “crime scene” approach he recommends helps to frame the process of discovery and learning in a straightforward, accessible way, and provides a unifying thread through all the cases.",,2016.0,10.1080/10686967.2016.11918491,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
396d0a8dc9ffd7f4530e1e0c900ac416024f3857,https://www.semanticscholar.org/paper/396d0a8dc9ffd7f4530e1e0c900ac416024f3857,Study of possible applications of AI Techniques to SEAT Manufacturing IT,"In order to survive in such an extreme competitive market as the car manufacturing industry is, 
automobile companies have traditionally invest a considerable amount of resources and effort in 
using innovative solutions to improve the efficiency of the manufacturing process in their plants. 
SEAT, being one of the innovation leading companies in the sector, aims to fully embrace the 
paradigms of the new industrial revolution known as “Industry 4.0”, based in the interconnection 
of machines, processes, systems and people. 
Artificial Intelligence techniques have a strong presence into the Industry 4.0 foundations, 
especially by means of exploiting the huge amount of digitalized data that is produced each 
second in a modern plant. But it is not only in data analysis where AI takes part in this new 
revolution but also in fields which are already mature enough to be deployed in industrial 
solutions like Computer Vision or Multi-Agent Systems. 
This work studies the evolution of the SEAT manufacturing process of its core plant in Martorell 
(Barcelona) together with the specific challenges that it faces nowadays and identifies a series of 
improvement opportunities for solving those challenges using Artificial Intelligence techniques. 
This identification is possible because it combined years-long working experience in the 
automobile industry with the learning acquired during the course of the Master in Artificial 
Intelligence of the Facultat d’Informatica de Barcelona. The work aims to analyze in general the 
implementation of Artificial Intelligence in a company, pointing out the real uses that the 
technology might have and it is written from a business strategic point of view. 
For each identified improvement opportunity the work conducts an analysis about the current 
situation, the most suitable AI technique to apply and the expected outcome of an improvement 
project. Such project would focus on implementing the selected technology for solving the 
specific problem or improving the described process. Both business and technological aspects 
are analyzed in detail for each improvement opportunity, setting the base for the execution of a 
full project in the future. 
Since the resources in a company are finite, both in terms of budget and personal, the next step 
is to carry out a prioritization of all the identified opportunities in order to start implementing 
the most affordable and interesting ones. For this purpose, the role of the business clients is 
essential to categorize the possible improvements. This is where the transference of the 
knowledge between IT and the manufacturing area occurs and the technology converts into a 
tool, a mean to achieve a goal and not a mean to itself. 
As a result of the prioritization, two projects are analyzed in detail for its future implementation. 
The first one consists on the application of process mining techniques for the optimization of the 
quality final revision workflow of the manufactured cars. The second one is a root cause analysis 
of the car painting failures that appear during the second part of the manufacturing process. 
Because of the bigger magnitude of the project, the data modeling part is to be implemented 
with external resources from the Volkswagen Group. Therefore the objective of the work is to 
reflect the project management part that I carry out as project leader at SEAT, including a 
detailed planning and exhaustive requirements specification, as well as the design of how the 
solution will be implemented and how the goals of the project will be accomplished. The work 
end by standing out the learning made during the whole process of the study and outlines the 
future work directions in order to transform SEAT manufacturing in one of the industry leaders of 
the upcoming industrial revolution brought by Artificial Intelligence.",,2016.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
f1f7e6bcdba65392e79661a8b952d0b622746ba3,https://www.semanticscholar.org/paper/f1f7e6bcdba65392e79661a8b952d0b622746ba3,Revenue assurance in electrodistribution using smart meters data,"The increasing complexity of services also encourages the complexity and heterogeneity of the systems that providers use for measuring and billing these services. The complexity may result in the occurrence of errors and consequently in a revenue leakage. For the telecom industry that strives to adapt their services not only to the needs of customers but also to new technological opportunities the biggest complexity issue is billing the services. They were the first to recognize the need to develop a systematic way to grapple the problem of errors that cause the revenue leakage. The sector prepared a comprehensive framework of procedures called ""revenue assurance"". With the liberalization of the electricity market, the services of electricity distribution became as complicated as the telecommunications services. This further significantly enhanced the deployment of smart grids, advanced metering infrastructures and smart meters that, with the abundance of data, give opportunities for new services. This master thesis presents the approach we took to grapple with the revenue assurance. We used general procedures and took into consideration the peculiarities of the electricity distribution domain as well. Particular attention was given to data quality issue. 
In the practical part of the thesis, we focused on acquiring knowledge from the data that would benefit us in detecting the revenue leakage, from the smart meters’ data collected by advanced metering infrastructure in particular. In the data warehouse, the data was combined with the billing system data and the geographic information system data. While building the data warehouse, we encountered some problems with data quality. After we had pointed out the problems, we indicated how to eliminate them and how to establish a mechanism for monitoring any possible recurrences of errors. 
The thesis focused on collecting information on the characteristics of consumers. Once we acquired this knowledge, we used it to look for any thefts of electricity. We made a comparison of machine learning methods for the classification of daily load curves of consumers into typical groups. Based on the analysis of the results obtained, we selected the best method, i.e. the expectation maximization method. At the same time, we determined the best number of clusters with the typical dynamics of daily consumption. Once all measuring points with the 15-minute consumption data were classified, we were determining the characteristics of a consumer that coincide the most with the dynamics of his electricity consumption. Again, we tested several machine learning methods and established that decision trees are the most appropriate tool for this task. With established behavior, we estimated daily consumption for all other measuring points. Thus prepared data were used to develop an analytical structure that proves to be an excellent base for discovering the revenue leakage. We automated the entire process of filling the warehouse, finding and applying knowledge and, last but not least, processing the analytical structure. We demonstrated the usefulness of this practice with a few examples of actual reports.",,2016.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
4d7819f1aeff718e590fe08ea944c28d1deda4a6,https://www.semanticscholar.org/paper/4d7819f1aeff718e590fe08ea944c28d1deda4a6,Solving Complex Industrial Problems Without Statistics,"Solving Complex Industrial Problems Without Statistics. 2016. Ralph R. Pawlak. Boca Raton, FL: CRC Press. 143 pages. This book fills a unique gap. For those in our community who are not strong in mathematical and analytical methods (but who wish to contribute more fully to the “analyze” stage of improvement projects), Pawlak provides practical examples to demonstrate the value of qualitative reasoning. For those who want to add a layer of common-sense thinking to continuous improvement efforts that are already data driven, he provides the background and the basis for making sure the analytical and computational parts of projects are well-framed. Despite the title, this book does not suggest industrial problems can (or should) be solved without robust mathematical methods and statistics. Rather, Pawlak spends most of the book describing and explaining the qualitative processes that should accompany all problem-solving efforts in industry. Grounded in 14 short case studies, the techniques he shares are simple and immediately applicable: identifying what characteristics distinguish good output from bad, generating “clues,” visual observation, articulating differences, and staging assembly and operations trials, among others. Even without a complete and rigorous analysis, these “clues” may provide enough information for readers to identify and deploy an appropriate corrective action. The examples cover a range of cases, from the new product development process, to assembly, to design, to machining, to troubleshooting the causes for defects. In summary, this is the only book I have ever encountered that comprehensively explains the thought process of the “fuzzy front end” in Lean Six Sigma problem solving. As a result, it provides excellent case studies that can be used as a first step toward understanding problem solving in common industrial scenarios. Despite a sometimes repetitive feel, the conversational tone of this book makes the material particularly accessible to beginners who might benefit the most from that repetition. “It does not matter that some problems that you encounter will require knowledge beyond your capacity,” Pawlak advises. In fact, the author could have gone further: the primary weakness of this book is that each of the case studies could easily have been two or three times as long and still maintained reader interest. Overall, the “crime scene” approach he recommends helps to frame the process of discovery and learning in a straightforward, accessible way, and provides a unifying thread through all the cases.",,2016.0,10.1080/10686967.2016.11918490,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
6eba9d9bd73349de9cd01e05f6b8e2ca0bf63a85,https://www.semanticscholar.org/paper/6eba9d9bd73349de9cd01e05f6b8e2ca0bf63a85,An Introduction to this Special Issue of AI Magazine,"Web. Indeed, the authors of each article included in this special issue have promised to create and maintain a URL pointing to a working prototype. Now, almost a year later, we have the fruit of this labor. A series of high quality articles describing a wide range of AI systems that are accessible on the Web right now (or so I hope). The articles describe a broad and diverse set of systems. The AI technologies used span the gamut from machine learning to natural language processing, from case-based reasoning to knowledge representation, and In AAAI-96 I gave an invited talk exhorting AI researchers to deploy their prototype systems on the World Wide Web. Deploying AI systems on the Web provides tangible evidence of the power and utility of AI techniques. Next time you encounter AI bashing, wouldn’t it be satisfying to counter with a handful of well-chosen URLs? Well, see the sidebar. At the conference, Jude Shavlik asked me to edit a special issue of AI Magazine describing AI systems that have the Web as their domain. I suggested that the authors make their prototype systems accessible on the more. Applications include Web page filtering, a grant finder, a FAQ finder, a home page finder, a shopping assistant, and more. Is the challenge of deploying systems on the Web a distraction from our long-term goal of understanding intelligence? I believe that the field benefits from a mixture of long-term and short-term goals and from both empirical and theoretical work. Work toward the goal of deploying systems on the Web is a valuable addition to the current mix for two reasons. Internally, the Web suggests novel research challenges and new constraints on existing techniques. Externally, intelligent Web “apps” will help to enhance the reputation of AI and to counter the popular misconception that “if it works, it ain’t AI.” Database and operating system research has spawned software industries with annual revenues in the billions. I look forward to the day when we have a comparable AI industry. Hopefully, Intelligence on the Web will be a step in that direction. An Introduction to This Special Issue of AI Magazine",,1997.0,10.1609/AIMAG.V18I2.1288,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
d4ef912f130eb2d791a1d65350e0d30d575f809d,https://www.semanticscholar.org/paper/d4ef912f130eb2d791a1d65350e0d30d575f809d,"Examining the Integration and Motivational Impact of Hands on Made4Me: Hands-on Machining, Analysis and Design Experiences for Mechanical Engineers","This paper reports our current progress towards introducing hands-on machining, analysis and design experiences in freshman, sophomore, and capstone design courses in the Department of Mechanical Engineering at the University of Massachusetts Lowell. The selection, assembly, and deployment of two low-cost, desktop computer-numerical-control (CNC) platforms is described along with our current plans for deploying desktop CNCs throughout a sequence of undergraduate engineering design courses. Finally, we present our proposed approach to evaluate the impact of curricular enhancement on our mechanical engineering students’ cognition, motivation and attitudes toward the profession. 1.0 Introduction The design of modern industrial products is an increasingly intricate process requiring a concomitant increase in the knowledge and skill-set of typical undergraduate engineering students who can successfully contribute to the product conception and creation process. The increased globalization of modern industry and its supply chain in new product development and manufacturing has resulted in the dispersion of the components and resources needed to realize new global products. The concept of “design anywhere – build anywhere” provides flexibility for U.S. manufacturers to remain globally competitive, yet results in a lack of proximity between the designer and the manufacturing elements of the product realization cycle. These evolving conditions create a pressing need for graduates capable of systems thinking and understanding the manufacturing and product development cycle, from making informed costand quality-based design decisions, analyzing these designs, to producing and ultimately testing these designs to ensure conformance with specifications. Our educational project aims to harness the recent proliferation of low-cost, multi-axis computernumerical-control machines to address these evolving market needs within the constraints of engineering design education. The lower-cost and lower-accuracy hobbyist CNC machines have largely benefited from the support of a growing hobbyist and open-source community eager to develop and capitalize on advanced machining and prototyping methods for home-use. The result has been a wide array of desktop tools that can be used for advanced prototyping with lowercost materials (machining wax, wood, PLA/ABS plastic filament, etc). These tools ultimately provide the end user with an opportunity for real, hands-on prototyping and advanced machining experiences at a fraction of the cost of commercially available machines. This project examines the selection, development and integration of desktop CNC technology throughout an undergraduate mechanical engineering curriculum and investigates how this technology can enhance student learning, motivation and attitudes towards engineering. Through the use of lower-cost desktop CNC machines, the students will be able to directly interface with advanced machining technology rather than treat these tools as expensive black boxes that demand more experienced operators. Moreover, through the desktop prototyping experiences, students will be able to explore multiple alternative conceptual designs and learn through iterative problem solving and design. As a result of this enhanced ability to quickly and efficiently implement ideas into physical models, we believe students will be encouraged to creatively approach engiP ge 24658.2 neering product design. With these lower-cost desktop CNC machines, we aim to introduce the science and engineering behind modern prototyping and manufacturing while enabling students to explore, experience and understand the role of prototyping in modern engineering within an integrated, hands-on, design oriented environment. This integration and expansion of hands-on, design-build-test coursework at the University of Massachusetts Lowell is expected to help undergraduate students to develop a more positive attitude, increased self-confidence and improved motivation towards design and manufacturing. We expect to produce future engineers with the increased skill-set necessary for the application of engineering and science principles in globalized product design and manufacturing. This paper and our poster highlight our preliminary project progress. We detail the prototyping platforms that have been selected and are actively being integrated into several core engineering courses (25.108 Introduction to Mechanical Engineering, 22.202 Mechanical Engineering Design Lab I and 22.423 Senior Capstone Design). 2.0 CNC Platform Selection This section presents a brief survey of desktop CNC machine platforms and the associated support hardware necessary to implement a safe and meaningful CNC machining laboratory experience. Platform selection in this first phase of the project is also described. 2.1 Desktop CNC Machines A broad range of commercial desktop CNC machines are now readily available in assembled and/or kit form. Numerous desktop CNC machine specifications were considered for student use within engineering curricula and include: overall dimensions, design extensibility, milling speed, milling area, machine footprint, and unit cost. Of the myriad desktop CNC machine designs available, we focused only on Cartesian linear-motion gantry designs that are more representative of high-end commercial CNC mills, rather than Delta-type designs that are more common in the 3D printing community. The characteristics of several candidate CNC machines were examined (Appendix A, Table A1) and a preliminary hands-on comparison of three selected machines has been performed. These three machines comprise two inexpensive, off-the-shelf, hobby CNC mills and our native first iteration of a University of Massachusetts, in-house design. A total of ten off-the-shelf CNC mills are being deployed in this initial phase of the project: 1. ShapeOko 2: Presently five ShapeOko 2 desktop CNC machines are deployed with two additional ShapeOko 2 CNCs under construction. The ShapeOko 2, shown in Figure 1, is an open source, x-, y-, z-axis, moving gantry design developed by Edward Ford. An aluminum extrusion MakerSlide rail and carriage system is used for both the axis structure and precision motion guide-rails – thereby reducing material use and costs. The drive system comprises a total of four stepper motors (dual motor x-axis). Timing pulleys and belts are coupled to the rail and carriage gantry to generate smooth motions. The ShapeOko 2 can be purchased as individual parts or in several self-assembly kit options from Inventables.com. The detailed unit deployment is presented in Appendix A, Table A2. Page 24658.3 2. Zen Toolworks 7” x 12”: Presently three Zen Toolworks desktop CNC machines are deployed. The Zen Toolworks CNC machine, shown in Figure 1, consists of a moving gantry (to generate motion in the vertical and one horizontal travel directions) and a translation table (to generate motion in the remaining orthogonal horizontal travel direction). The gantry and table structural components are constructed using quarter-inch PVC sheet stock. Motion in each Cartesian direction is generated using stepper motor-driven lead-screws. Only one stepper motor is required for each axis of motion, totaling three stepper motors per unit. The design is available in several kit options from ZenToolworks.com. The detailed unit deployment is presented in Appendix A, Table A3. These two desktop CNC mill designs were chosen to provide a comparison of machine durability, ease of lab-setup, design flexibility, machining efficiency, manufacturing precision and machine capability within a higher education laboratory setting. Our goal is to assess, via this initial deployment, whether a particular CNC machine design or material usage concept is preferable over another in a high-use academic environment. In addition to the two off-the-shelf machines, an affordable desktop CNC mill has been designed in-house and is being refined. This design maintains low deployment cost (less than $750 per unit) and is fabricated using a minimum of machinery and tools. A CAD rendering of the UMLdesigned low-cost CNC machine is shown in Figure 2. Figure 1: (left column) The Shapeoko 2 CNC machine with some custom rail-protection modifications. (right column) The Zen Toolworks CNC machine. The dimensions of the enclosure in the lower photos are 27” x 27” x 27”.",,2014.0,10.18260/p.24043,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
8b9082de1672c60a7114de9d315c9d61854c3ffa,https://www.semanticscholar.org/paper/8b9082de1672c60a7114de9d315c9d61854c3ffa,Network traffic classification : from theory to practice,"Since its inception until today, the Internet has been in constant transformation. The analysis and monitoring of data networks try to shed some light on this huge black box of interconnected computers. In particular, the classification of the network traffic has become crucial for understanding the Internet. During the last years, the research community has proposed many solutions to accurately identify and classify the network traffic. However, the continuous evolution of Internet applications and their techniques to avoid detection make their identification a very challenging task, which is far from being completely solved. 
This thesis addresses the network traffic classification problem from a more practical point of view, filling the gap between the real-world requirements from the network industry, and the research carried out. 
The first block of this thesis aims to facilitate the deployment of existing techniques in production networks. To achieve this goal, we study the viability of using NetFlow as input in our classification technique, a monitoring protocol already implemented in most routers. Since the application of packet sampling has become almost mandatory in large networks, we also study its impact on the classification and propose a method to improve the accuracy in this scenario. Our results show that it is possible to achieve high accuracy with both sampled and unsampled NetFlow data, despite the limited information provided by NetFlow. 
Once the classification solution is deployed it is important to maintain its accuracy over time. Current network traffic classification techniques have to be regularly updated to adapt them to traffic changes. The second block of this thesis focuses on this issue with the goal of automatically maintaining the classification solution without human intervention. Using the knowledge of the first block, we propose a classification solution that combines several techniques only using Sampled NetFlow as input for the classification. Then, we show that classification models suffer from temporal and spatial obsolescence and, therefore, we design an autonomic retraining system that is able to automatically update the models and keep the classifier accurate along time. 
Going one step further, we introduce next the use of stream-based Machine Learning techniques for network traffic classification. In particular, we propose a classification solution based on Hoeffding Adaptive Trees. Apart from the features of stream-based techniques (i.e., process an instance at a time and inspect it only once, with a predefined amount of memory and a bounded amount of time), our technique is able to automatically adapt to the changes in the traffic by using only NetFlow data as input for the classification. 
The third block of this thesis aims to be a first step towards the impartial validation of state-of-the-art classification techniques. The wide range of techniques, datasets, and ground-truth generators make the comparison of different traffic classifiers a very difficult task. To achieve this goal we evaluate the reliability of different Deep Packet Inspection-based techniques (DPI) commonly used in the literature for ground-truth generation. The results we obtain show that some well-known DPI techniques present several limitations that make them not recommendable as a ground-truth generator in their current state. In addition, we publish some of the datasets used in our evaluations to address the lack of publicly available datasets and make the comparison and validation of existing techniques easier.",,2014.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
598e392add0580da30ae50559e8361df27e34d0d,https://www.semanticscholar.org/paper/598e392add0580da30ae50559e8361df27e34d0d,Emotional System for Military Target Identification Prof,"The thought of man-made systems and machines having emotions sounds like science fiction, however, few decades ago the idea of machines with intelligence seemed also like fiction, but today we are developing intelligent machines and using them successfully in different applications. Would we accept the idea of machines that could “feel”? What role would emotions play in machine learning and decision making? How can we model artificial emotions within intelligent systems? Can a machine’s decision capability be improved if it had emotions? These are questions one may ask when hearing that machines may have emotions, albeit artificial emotions. In this paper, we discuss these questions, and review briefly some of the latest works on modeling emotions within intelligent systems; including our own model that is based on an emotional neural network (EmNN). The EmNN has emotional neurons, weights, and two embedded emotional responses; anxiety and confidence. These emotional parameters are updated during task learning, and used during decision making. The paper will also present an application of the EmNN to military target identification, in addition to discussing the potential of using the emotional system to improve information exploitation. 1.0 INTRODUCTION In our daily lives, the amount of information we receive, perceive and then react to is tremendous. Much of our reaction to input information is formed into decisions that we make. What makes us act in a certain way, or decide for, or against an act has its roots back in our previous experiences. Whether we choose anchovies or pineapple on our pizza is a decision we make based on a previous experience with these flavors that leads us to decide what to have. Such a simple process of deciding on our favorite topping is as important and complicated as the more critical decisions we need to make throughout our lives. All decisions involve a learning process, resulting in association, classification and then decision making. During the learning process information which can take different forms, is exchanged between our natural sensors and the main processor; the brain. This sounds very technical and parallel to describing a computing system. However, computing systems lack one aspect of a human processing system; emotions. Over the past decades machines and systems have been developed and deployed to aid us in decision making and taking action; spanning application areas from simple electronic toys, industry and manufacturing, intelligence and security, to more complicated tasks in medicine, military applications, and space navigation. As the creators (designers) of these machines, we aim to assure that the action or the decision taken by the machine is correct and complies with our way of making a decision. In simplistic terms, we tend to create machines that would make the decisions on our behalf, and as information age progresses, and powerful high-tech systems grow even faster, our expectations of the machines are increasing. Most of the systems that we develop, and use to make decisions on our behalf, do not go through the learning process and the experiences that we possess. With the exception of some artificial intelligent systems that could interact with their input stimuli, and adapt their output or decision accordingly, systems Emotional System for Military Target Identification 18 2 RTO-MP-IST-087 UNCLASSIFIED/UNLIMITED UNCLASSIFIED/UNLIMITED and machines rely entirely on a set of commands that we provide to govern their actions, and this has been fine until our demands started requiring more complicated decisions by machines. Therefore, more and more intelligent systems are being developed, based in particular on neural networks which form the brain of a machine. These systems imitate our learning process and decision making by repeatedly exposing a neural network to examples of input information and its corresponding output, response, action or decision; this process models the previous experience process in humans. The neural network-based systems have been popularly used, and have shown success in various application areas, where association, classification and decision making can be obtained based on accumulated memory of past experiences. Despite the success of such intelligent systems, there has been a major and vital difference between a human decision-maker and a machine decision-maker, namely emotionwe have it, and machines do not. The idea of machines with affection or feelings is controversial, and some works expressed doubt about this idea [1,2], however, the concept of machines with emotions has lately attracted the attention of many researchers, and is currently gaining momentum with novel architectures emerging to artificially model emotions in one way or another. Recent definitions of emotion have either emphasized the external stimuli that trigger emotion, or the internal responses involved in the emotional state, when in fact emotion includes both of those things and much more [3]. The effective role of emotions on cognitive processing, learning and decision making in animals and humans has been emphasized by several researchers [4-8]. Emotions play an important role in human decision-making process, and thus they should be embedded within the reasoning process when we try to model human reactions [9]. Although computers and machines do not have physiologies like humans, information signals and regulatory signals travel within them; there will be functions in an intelligent complex adaptive system, that have to respond to unpredictable, complex information that play the role that emotions play in people [1]. Such computers will have the same emotional functionality, but not the same emotional mechanisms as human emotions. We may think of machine emotions as machine intelligence; we do not expect machines to “feel” the way we feel, but we could simulate machine emotions just as we do machine intelligence [9]. There have been examples of research works that attempted to incorporate emotions in machines in one way or another [9-20]. It was concluded from these works that if emotions such as anxiety, fear, and stress are included in systems that aim to simulate the human behaviour in certain circumstances, the system will be more user-friendly and its responses will be more similar to human behaviour. Other recent research works suggested the use of emotional components within neural models and control systems. For example, Abu Maria and Abu Zitar [21] proposed and implemented a regular and an emotional agent architecture which is supposed to resemble some of the human behaviours. They noticed that artificial emotions can be used in different ways to influence decision-making. Gobbini and Haxby [22] proposed a model for distributed neural systems that participate in the recognition of familiar faces, highlighting that this spatially distributed process involves not only visual areas but also areas that primarily have cognitive and social functions such as person knowledge and emotional responses. Coutinho and Cangelosi [7] suggested the use of modelling techniques to tack into the emotion/cognition paradigm, and presented two possible frameworks that could account for their investigation, one of which explored the emergence of emotion mechanisms. Most of these previous attempts on incorporating emotions in to machine learning have shown successful results, and provided a positive trend to developing machines with emotions, albeit simulated. Lately, we proposed an emotional neural network (EmNN) which was based on the novel emotional back propagation (EmBP) learning algorithm [23], and used it to solve a facial recognition problem. In other works [24,25], we explored the potential of using emotional neural networks in different tasks, such as Emotional System for Military Target Identification RTO-MP-IST-087 18 3 UNCLASSIFIED/UNLIMITED UNCLASSIFIED/UNLIMITED more complicated face recognition tasks and in blood cell identification. The difference between an emotional system and the more traditional approaches; including intelligent systems, is the simulated artificial emotions of the system. These additions have several advantages over traditional approaches. The embedded artificial emotions narrow the gap between humans and systems; thus instead of “human and machine interaction” we have “human and human-like machine interaction”. The closer and more coherent communication of information between humans and emotional systems has the advantage of faster communication, since both systems (human and machine) have emotions. This is not the case with traditional systems, where often a human operator perceives information and makes decisions which could differ due to his/her emotions. In this paper, we present the emotional neural network (EmNN) and describe its emotional parameters. The EmNN will also be applied to identify potential military targets, such as navy ships, helicopters, jetfighters, tanks and other assorted military vehicles. One of the aims of this work is to mimic the way a human would recognize these targets, by: firstly, using different images of targets with random orientations, angles, and backgrounds, secondly, avoiding complicated image pre-processing phases, and using only global image pattern averaging to simulate a human’s “glance” or quick look at a target image, and finally, using the EmNN to perform the target identification, by repeatedly exposing random target images to the network during its training phase; this process simulates the human “getting familiar” with the objects, without the need to look into edges, local features, angles or colors of a potential target. The paper is organized as follows: Section 2 presents the EmNN and describes the differences between conventional neural networks and emotional neural networks. Section 3 presents the application of the EmNN to military target identification, describin",,2009.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
c1fe76025999ecbc22b7c721fffb4fb668bbf0f9,https://www.semanticscholar.org/paper/c1fe76025999ecbc22b7c721fffb4fb668bbf0f9,Google's Go and Dart: parallelism and structured web development for better analytics and applications,"Big Data -- the new buzz word in the IT world -- is synonymous with the concept of data-driven decision making. All across the industry, enterprises are collecting all sorts of data including client preferences, trends amongst social networks and information about competing businesses, at an unprecedented scale with the focus on making intelligent decisions. The decisions made from processing such data have a direct impact on both the businesses and its clients through higher profit margins and smarter services respectively. The combination of sophisticated analytics and data-driven decision making enables new types of solutions, from mining the human genome to deriving sentiments from social networks. This is all leveraged through consistent innovation of underlying technology, increased competition and cloud-based SaaS. Clouds are the frontiers for most business IT solutions as they idealize the ""as a service"" methodology, interoperability between services and correct billing metrics. Thus, it easily follows that most data analytic tools are designed to be run on cloud clusters rather than traditional data centres. 
 
In a world where there is an emergence of extensive use of analytics, data and fact-based decision making, spontaneous sorting of data becomes imperative. Industries like finance, pharmacy and others rely heavily on data to assess their strategies and customer requirements, and to respond quickly for the better. Hence, analytics are crucial for knowledge discovery, business growth and technological improvements. Every second, massive quantities of data are being generated and there is a need for data analytic tools that are easily integrated, scalable, and informative. While we are promisingly producing vast amounts of information, there is currently a wide gap between its potential and its realization. There are many problems with Big Data, including the heterogeneity of data, scale, readiness and complexity. The dynamic business environment requires a company to quickly adapt to newer technologies that can provide better solutions. A high throughput of services translates into improved business efficiency. It is often the case that an innocent shift in development tools can catapult a paradigm altering change. In this workshop, we discussed two emerging technologies recently launched by Google -- Go and Dart; which through their advents in scalability, improved parallelism, and structured web development greatly enhanced the capacity and quality of building applications designed to work from handheld mobile devices, to data crunching frameworks such as Google's Map Reduce, IBM's Big Insights and others. 
 
The initiative behind Go is to create a language that allows programmers to exploit concurrency in programs by providing simple yet powerful features built into the language that do not require complex code and high maintenance. With the introduction of GPGPU's for execution, it is essential to have concurrency dependent applications in order to improve their performance. Go provides simplicity and improvement in leveraging processing power while providing simplified features of traditional programming languages. Not following the traditional object oriented programming model, Go was initially targeted for system programming including applications for distributed systems, storage infrastructure, networking infrastructure and the RPC layer, however, its features make it an excellent language deploying application on concurrent systems. Go has higher throughput, as compared to Java or Python because of its dynamic stack- and core- focused features for speed and concurrency. Being a hybrid between C and Python, Go is type-safe and memory-safe, has fast start-ups, latency-free garbage collection, and high-speed compilation. Businesses gain the advantage of availability of services by using a language that is designed to be concurrent. Go's ""lightweight concurrency"" allows developers to create sets of lightweight communicating processes called goroutines. The programmer doesn't need to worry about synchronization, locking mechanisms such as semaphores and race conditions. This affects the cost of back-end and front-end instances which allows tiers to deliver more responses per instance. 
 
Moreover, Go uses significantly less memory as compared to Java which is JVM dependent. Go is predominantly used because of its easy integration with frameworks like Map Reduce. Go provides static typing and makes unit testing much easier to the languages like Java and C. Go is designed specifically to provide easy integration with Google App Engine and Google Compute Engine. Finally, Go compiles static binaries with no dependencies, so Go programs can simply be dropped onto a server and deployed. 
 
Today, the web is everywhere -- from handheld mobile devices to television sets to traditional desktops. It is very easy to write a web page that is universally accessible on any device via a browser. There are no installation or update procedures, making the user experience very pleasant. JavaScript is now supported by almost every browser because of its convenient functionality. However, alongside the beauty of JavaScript is its unpleasantness in debugging, performance across browsers, and security on client devices. Dart is a new web programming language and methodology in development by Google for creating structured web applications. It is developed with the aim to encompass aspects such as simplicity, efficiency, and scalability while combining new language features with familiar language constructs into a clear, fluent syntax. The fundamental reason for its creation is to ultimately replace JavaScript because of its unstructured and inconvenient development paradigm. Dart aims to tackle these issues by providing lexical scoping, closures, and optional static typing; not to mention integrated development and debugging are also provided in the Dart Editor and SDK, which supports major common interactions such as refactoring, breakpoints, code completion, code navigation and much more. In addition, the SDK provides a standalone virtual machine, package manager, and Chromium with an embedded Dart VM. Dart can also be compiled to JavaScript, which makes it usable on all modern desktop and mobile browsers with an additional plus of being able to run on a server in a stand-alone Dart VM. Fundamentally aiming to create structure and flexibility for the web, focusing on supporting a full range of devices --- including phones, tablets, laptops, and servers, and providing environmental and supportive tools to run efficiently across all major modern browsers, Go and Dart are developed specifically for a simplification of the coding paradigm and better integration with existing technology. Both Go and Dart aim to improve the modern day programming model and unstructured web source code. 
 
This full-day workshop focused on the core technologies of Go and Dart, including building and deploying applications integrated with analytic frameworks on cloud clusters. The workshop was split into three segments. In the first segment, the objective was to take a hands on approach to walk through the features of Go and Dart. The participants wrote code themselves and explored both the languages on a first hand basis. We introduced the languages and discussed their basic constructs and some advanced features. 
 
In the second half, we examined an in depth example of creating a web application using Go, Dart, Map Reduce, and Google App Engine. The goal was to demonstrate Go's simplicity, power of concurrency, and structured web development and integration with existing API's. In the last segment, we concluded with a discussion about the impact Go and Dart can have on different parts of the IT industry. As the shift to development for cross mobile platform web applications increases - we expose the usefulness of using Dart instead of JavaScript for building mobile web applications with frameworks like Apache Cordova and IBM Worklight. Furthermore, we discussed questions like how these languages are being used in the real world today; the learning curve, cost and complexity behind the languages; the value of the products that are produced by using such programming tools. 
 
There are many subtle challenges and opportunities associated with Big Data, which require restructuring the data management platforms to better suit the needs of businesses. When making a shift to newer technologies, there are always doubts and expectations. Hence, it is beneficial to look into the integration of emerging technologies with existing systems for the constant improvement.",CASCON,2012.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
3765d402a6e233d41e4cd4c4a0b99a8751a62c0c,https://www.semanticscholar.org/paper/3765d402a6e233d41e4cd4c4a0b99a8751a62c0c,From Bugs to Decision Support - Leveraging Historical Issue Reports in Software Evolution,"Software developers in large projects work in complex information landscapes and staying on top of all relevant software artifacts is an acknowledged challenge. As software systems often evolve over many years, a large number of issue reports is typically managed during the lifetime of a system, representing the units of work needed for its improvement, e.g., defects to fix, requested features, or missing
documentation. Efficient management of incoming issue reports requires the successful navigation of the information landscape of a project.

In this thesis, we address two tasks involved in issue management: Issue Assignment (IA) and Change Impact Analysis (CIA). IA is the early task of allocating an issue report to a development team, and CIA is the subsequent activity of identifying how source code changes affect the existing software artifacts. While IA is fundamental in all large software projects, CIA is particularly important to
safety-critical development.

Our solution approach, grounded on surveys of industry practice as well as scientific literature, is to support navigation by combining information retrieval and machine learning into Recommendation Systems for Software Engineering (RSSE). While the sheer number of incoming issue reports might challenge the overview of a human developer, our techniques instead benefit from the availability
of ever-growing training data. We leverage the volume of issue reports to develop accurate decision support for software evolution.

We evaluate our proposals both by deploying an RSSE in two development
teams, and by simulation scenarios, i.e., we assess the correctness of the RSSEs' output when replaying the historical inflow of issue reports. In total, more than 60,000 historical issue reports are involved in our studies, originating from the evolution of five proprietary systems for two companies. Our results show that
RSSEs for both IA and CIA can help developers navigate large software projects, in terms of locating development teams and software artifacts. Finally, we discuss how to support the transfer of our results to industry, focusing on addressing the context dependency of our tool support by systematically tuning parameters to a specific operational setting.",,2015.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
1d8e6b172411c53e967503b90d70bfecf8063cc4,https://www.semanticscholar.org/paper/1d8e6b172411c53e967503b90d70bfecf8063cc4,Special issue: societal aspects of synthetic biology,,Systems and Synthetic Biology,2009.0,10.1007/s11693-009-9043-6,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
50c1184569211e62e320bdea1d87b285e6cd1466,https://www.semanticscholar.org/paper/50c1184569211e62e320bdea1d87b285e6cd1466,Towards a New Safety Assurance Method for Complex Safety-Critical Systems,"Know your enemy and know yourself and you can fight a hundred battles without disaster. Sun Tzu, The Art of War This essay investigates the making of timely and acceptable safety assurance decision throughout a safety-critical system’s lifecycle in a complex environment. This ‘battle’ of assuring a safety-critical system is safe enough to avoid the ‘disaster’ of accident is a long and treacherous journey. It demands a structured and persistent method to support safety analysis despite challenges from the non-deterministic and non-linear environment throughout the system lifecycle. Learning from Sun Tzu, this journey begins by searching for clarity regarding the two important questions: ‘know yourself’ by understanding the current way of conducting safety management throughout a system lifecycle; and ‘know the enemy’ by examining the potential risk and hazards cause by the unique characteristics of complexity. This is followed by a survey of possible methods that can be used to analyse and mitigate the risk under a complex system. Systems Engineering Essay Competition 2015 Page 2 of 25 Jointly organised by Temasek Defence Systems Institute Department of Industrial and Systems Engineering, Faculty of Engineering, NUS The literature reviews make three observations. First, while there are established safety management processes to analyse safety at specific milestones of the system lifecycle, there is a lack of continuity in managing and utilising the system knowledge regarding safety throughout the system lifecycle. Secondly, safety assessment must adapt to changing social-technical context especially when a system acquisition lifecycle comprises multiple distinct phases that introduce different constraints to safety assessment. Lastly, uncertainty in the complex environment can never be eliminated but there should be more efforts to minimise the surprises due to incomplete and imperfect information so as to create the confidence in the safety analysis. The paper concludes by providing a preliminary approach to develop a safety assurance method that aims to be applied throughout the system lifecycle to provide a structured and continual way to support decision making. Introduction: Where are the Hazards? As technology advances, more and more systems in domains like defence, air traffic management, railway transport, nuclear power plant, offshore drilling and health care are becoming highly networked and complex. These are described as safety-critical systems as any failure can potentially lead to the loss of life and damage to property or environment. For example, in the defence industry, networked of large-scale safety-critical systems (often refers to as Network Centric Warfare or System-of-Systems 1 [1]) have been revolutionising the applications of military technology as machines and computers are used to carry out complex and time-critical tasks. These machines are inter-connected to form larger inter-dependent systems and continuing to expand in numbers and complexity as the armed forces attempt to accomplish more challenging missions. Unfortunately, while there are established safety management processes, accidents that led to the loss of Systems Engineering Essay Competition 2015 Page 3 of 25 Jointly organised by Temasek Defence Systems Institute Department of Industrial and Systems Engineering, Faculty of Engineering, NUS life and damage to properties continue to surface from such highly complex safety-critical systems. Some of the most notable accidents and incidents in recent years include:  (Space) NASA space shuttle Challenger (1986) [2] and Columbia (2003) [3] disasters  (Military Operation) B-1B Lancer bomber friendly fire on coalition soldiers in Afghanistan (2014) [4]  (Drilling) Piper Alpha offshore oil production explosion (1988) [5]  (Maintenance) F-111 (Fuel Tank) De-seal/Reseal program leading to chemical exposure (2001) [6]  (Health-care) Overdose of radiation during Therac-25 radiation therapy (1985) [7]  (Rail) Wenzhou high speed railway collision (2011) [8] While the faults for each of these disasters are unique, one common observation is that it is extremely difficult to narrow down to a specific failure mode. Unlike complicated but linear systems where traditional safety analysis method is capable of using linear reductionist approach to deduce the root causes, the failure modes in complex safety-critical systems are different. Reiman [9] observes that effects from a complex system have “several parallel contributing factors, instead of one or few causal chains 2 as in linear systems”. Dekker [10] also believes that “the behaviour of such complex system cannot be reduced to an aggregate of the behaviour of its constituent components”. Hence, even if one root cause has been identified, decision makers may face the frustrating but real challenge of not being able to fully comprehend the full casual chains of complex relations. One good example of a complex safety-critical system is Air Traffic Control (ATC). On 12 Dec 2014, an air traffic disruption at the Swanwick ATC resulted in numerous flight cancellations across Heathrow, Gatwick and London City [11]. The System Flight Servers failed when more workstations were being brought online during the transition between 1 The System-of-Systems (SoS) refers to a set of systems that are cooperating for a common purpose while simultaneously working as independent entities. 2 A causal chain refers to the path or sequence of events that runs from a root cause to problem symptoms in the real world. Systems Engineering Essay Competition 2015 Page 4 of 25 Jointly organised by Temasek Defence Systems Institute Department of Industrial and Systems Engineering, Faculty of Engineering, NUS normal and standby operation [12]. This affected air traffic control as it was impossible for controllers to access aircraft flight plans. While the final report has yet to be released, NATS has announced that the failure is unprecedented in its 13 years of operations. The incident highlights the difficulties in managing system safety in complex environment. There will be intense pressure when failures occur and rightfully so since safety-critical systems are utilised in critical situations (e.g. ATC keeping the airspace safe) where there are severe consequences (e.g. disruption to commercial flights and passengers’ safety) when the systems fail. It is also possible that certain failures may never manifest themselves during system development as it is impossible to predict and conduct safety assessment on all operational scenarios (e.g. overloading of the ATC System Flight Servers). Active sharing of knowledge regarding safety throughout the system lifecycle can help to better anticipate and identify such ‘blind-spots’ during operation. In terms of managing safety-critical projects, social, technical and organisation tensions continue to pose safety challenges when it comes to acquiring a system in such complex environment. The following shows a sample of such concerns highlighted by Atkinson [13] and Saunders [14] from their surveys on managing project uncertainty of complex safety-critical system:  Novelty of design and technology  Diverse and conflicting stakeholders expectations and belief  Failure to anticipate concurrency of activities and capture dependency relationships  Ineffective communication and knowledge management with changing stakeholders throughout system lifecycle  Lack of continuity in personal and responsibilities when managing different interoperating systems  Incomplete and imperfect information  Lack of systematic process to capture corporate knowledge and lessons learned While the list represents uncertainty in managing projects, it is equally relevant to safety management and highlights the diversity of social-technical context that would affect the Systems Engineering Essay Competition 2015 Page 5 of 25 Jointly organised by Temasek Defence Systems Institute Department of Industrial and Systems Engineering, Faculty of Engineering, NUS effectiveness of any safety analysis. Most safety-critical systems comprise human-machine interactions and it will be important to consider explicitly the impact of changing socio-technical context during safety analysis so as to create attention to potential hazards in such circumstances. The System Lifecycle – where it begins One approach to appreciate the safety hazards facing a safety-critical system is to consider its lifecycle. A system can be considered as a “combination, with defined boundaries, of elements that are used together in a defined operating environment to perform a given task or achieve a specific purpose”[15]. Taking reference from the military, a system is realised by following the system acquisition lifecycle. A full-scale system acquisition lifecycle includes multiple stages, milestones and decision points that shape the development of a system till its operationalisation. Two examples of military acquisition lifecycles are shown in Figure 1. They are the UK MoD CADMID 3 acquisition cycle and the US DoD Defence Acquisition Process. Figure 1 Categorisation of System Acquisition Lifecycle in defence A system lifecycle has two distinct phases: development and operation. Both phases are subjected to risk and safety hazards in the complex environment but exhibit different system characteristics. The following table provides a comparison of the two phases. Characteristics System Development System Operation 3 CADMID refers to the six phases of acquisition lifecycle: Concept, Assessment, Demonstration, Manufacture, In-service and Disposal. Systems Engineering Essay Competition 2015 Page 6 of 25 Jointly organised by Temasek Defence Systems Institute Department of Industrial and Systems Engineering, Faculty of Engineering, NUS Characteristics System Development System Operation Type of processes Design, plan, production, testing, and deployment Operation, maintenance and support, ret",,2015.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
9f314a8ed75dd3b5ed4f36b263804fc2357f5725,https://www.semanticscholar.org/paper/9f314a8ed75dd3b5ed4f36b263804fc2357f5725,Industrial requirements for ML application technology,"Given the fact that machine learning research exists almost half a century by now one may ask the question what the industrial value is of the things we have done. Well there is good news and bad news. The good news is that the research is flourishing. We have found a number of robust simple techniques that really seem to solve a class of interesting learning problems. In the next decade applications of ML will abound: intelligent cars, houses, wordprocessors, selfhealing networks, adaptive systems it all seems within reach. There is also bad news. Especially from an industrial point of view. ML is facing problems that look very much like the one we had in AI about a decade ago. The market is shattered. We offer point solutions in a variety of industrial settings, but no strong ML industry seems to be emerging. Our technology is shallow, i.e. easy to copy by our competitors. Application areas are, like in AI, divers. No strong vendors of AI tools currently exist in the market. In my opinion no strong vendors will emerge in the ML market. Still, ML is a very powerfull technology and we must start to develop methodologies for the deployment of ML in an industrial context. In the following I will draw on my experience as an R&D manager over the past few years.",,1997.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
6e52345184e660e82b2954c8195f8015ffe4a4e9,https://www.semanticscholar.org/paper/6e52345184e660e82b2954c8195f8015ffe4a4e9,"Proceedings of the seventh ACM International Workshop on Mobility in the Evolving Internet Architecture, MobiArch 2012, Istanbul, Turkey, August 22, 2012","Welcome to the seventh ACM International Workshop on Mobility in the Evolving Internet Architecture (MobiArch'12) in Istanbul, Turkey! We are delighted to see the workshop has attracted high quality submissions from international researchers in both academia and industry. 
 
This workshop provides the opportunity to participate in the exploration of the state of the art research results on mobile Internet computing and in particular, pricing and mobile cloud research. 
 
As the number of mobile users proliferates, there is an ever increasing appetite for innovative mobile services. Service providers are often under tremendous pressure of deploying new services to users quickly and cost effectively. This requires us to critically rethink the mobile services architecture that will lay the foundation for a futuristic mobile Internet and a growing revenue base. The aim of this proposed workshop is to have an open forum discussing cutting edge research contributions with a focus on economic and incentive issues in mobile networks and technologies pertaining to mobile cloud. Mobile operators cannot compete solely on higher bandwidths supported by the underlying technology (e.g. HSPA+, LTE). Service differentiation using innovations along other dimensions like pricing, incentives, QoS, etc. is turning out to be essential to compete. As mobile technology has matured tremendously in the past five years, we are moving into the domain of sophisticated mobile applications that encompass the regular World Wide Web, mobile centric web and cloud based services. However, new challenges have to be met: 
How do we create service differentiation with mobile economic incentives in mind? 
What are some of the new ""incentive-aware"" mobile network architectures? 
Are there new theories and models for understanding network pricing and congestion control? 
 
 
 
To meet with these challenges, researchers from a wide range of academic fields, including theory and algorithms, data mining and machine learning, computer systems and networks, statistical physics and complex systems, economics and managerial science, etc., are all actively studying various aspects concerning mobile networks. 
 
This workshop is intended to present such an opportunity and serve as a forum to bring together people from various fields to exchange their latest research results and to discuss new ideas and directions to properly understand these networks. 
 
Highlights of the workshop include a keynote speech delivered by Lili Qiu (Associate Professor at University of Texas, Austin), world-renowned researchers in the fields of mobile computing systems including mobile pricing, incentives and mobile cloud technology and services. 
 
The technical sessions consist of presentations and discussions of the 5 accepted full papers and 2 short papers on a wide variety of issues of mobile networking and architecture research from different angles. These papers were selected from 12 submissions. The selected papers were chosen by a technical program committee (TPC) of 19 experts in various fields related to mobile pricing and cloud technology. The selection process started shortly after the submission deadline. Each paper was reviewed by at least three independent reviewers, and evaluated based on scientific novelty, technical quality, relevance to the topics, and contribution to the field.",MobiArch@MobiCom,2012.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
5df7c5b7466b3ebb594cf28b4c9722b270744e22,https://www.semanticscholar.org/paper/5df7c5b7466b3ebb594cf28b4c9722b270744e22,A tecnologia de identificadores de rádio freqüência na logística interna industrial: pesquisa exploratória numa empresa de usinados para o setor aeroespacial,"Applications related to materials management are disseminating Radio-Frequency Identification (RFID) technology around the world. The scope of this technology enables managers to control several business processes and solutions for daily traceability. This article explores RFID applications in internal machining environment logistics where the use of barcode labels does not meet all the detection, identification and traceability needs. This study is classified as exploratory and was conducted in 2007, at a machined components industry for the aerospace sector. This study analyzes the deployment and use of this technology. Hopefully, the learning process that results from this study will disseminate experiences and practices in the use of RFID. Keywords: RFID; materials management; traceability; internal logistic.",,2009.0,10.15675/GEPROS.V2I2.750,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
7991b7b0317c0d29bc209f18acc18b5933ac71be,https://www.semanticscholar.org/paper/7991b7b0317c0d29bc209f18acc18b5933ac71be,"Part B 1 ] 1 ( to be evaluated in Step 1 ) Ubiquitous , spatiotemporal , multimodal action recognition Act Now","Action and activity recognition lie at the core of a panoply of scenarios in human machine interaction, ranging from gaming, mobile computing and video retrieval to health monitoring, surveillance, robotics and biometrics. The problem, however, is made really challenging by the inherent variability of motions carrying the same meaning, the unavoidable over-fitting due to limited training sets, and the presence of numerous nuisance factors such as locality, viewpoint, illumination, and occlusions that make real-world deployment extremely difficult. The most successful recent approaches, which mainly classify bags of local features, have reached their limits: only understanding the spatial and temporal structure of human activities can help us to successfully locate and recognise them in a robust and reliable way. We propose here to develop novel frameworks for the integration of spatiotemporal action structure in both generative and discriminative models, pushing for a breakthrough ripe with enormous exploitation potential. A new class of hierarchical part-based discriminative models originally developed for object recognition are reinvented for action localisation and recognition, as a fundamental way of coping with complex activities formed by series of simple actions and addressing the issues with locality and multiple actors. New manifold learning techniques for generative graphical models are developed to tackle the presence of nuisance factors and improve their generalisation power. Finally, novel classes of graphical models able to handle whole convex sets of probabilities are formulated in order to address the issue of overfitting due to the limited size of the training sets. As companies are heavily investing on virtual mice, smart TVs, phones and cars, and range sensors are changing clinical practice and the entertainment industry, the timeliness and potential impact of this project could not be understated.",,,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
6dd7a058eaa0b3a3a45bed62c41863c6bd762ab3,https://www.semanticscholar.org/paper/6dd7a058eaa0b3a3a45bed62c41863c6bd762ab3,Personalization of the AutomotiveInformation Environment,"Most research in machine learning applications has focussed on developing a knowledge base from training data, and deploying only the knowledge base in industry. This paper describes a new application of machine learning to personalization of complex devices. This task is diierent from some other learning applications because the learning component itself is deployed in the eld, and the application dynamically constructs the knowledge base through interaction with a user. A sample application currently under development is a personalized route advisor for automobile drivers. This system dynamically learns a driver's familiar routes for incorporation into route planning, route description, and destination prediction. We believe that this style of machine learning is an ideal solution to problems where a user needs to con-gure a device to improve its eeciency but lacks resources to do it manually.",,1997.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
6066ff4dd81f26710b654c4a28a3c246d724aba5,https://www.semanticscholar.org/paper/6066ff4dd81f26710b654c4a28a3c246d724aba5,"The Complex Challenge of Repairing the Gantry Steelwork on the First Generation Magnox Storage Pond at Sellafield: Legacy Waste Storage, First Generation Magnox Storage Pond","This paper puts into context the challenges that were faced when repairing the Gantry Steelwork of the First Generation Magnox Storage Pond (FGMSP). The First Generation Magnox Fuel Storage Pond (FGMSP) provided fuel storage and decanning capability from the early 1960’s until 1986. A significant programme of work has been underway since the completion of operational activities to support the programmes strategic intent of retrieving and storing all legacy wastes, and remediating the structure of the plant to support decommissioning activities. A key enabler to the retrievals programme is the Pond Skip Handler Machine (SHM), removed from service in 2002 following the discovery of significant signs of corrosion and distress, an inevitable consequence of being located in a coastal, salt laden environment. The SHM provides sole capability to access and retrieve the inventory of over 1000 fuel skips. It is also fundamental to future operations and the deployment of desludging equipment to recover significant bulk sludge’s from the pond floor. Failure of the SHM steelwork gantry at worst case could potentially result in the Skip Handler Machine being derailed. This has some potential to damage to the pond structure and at worst case may result in local radiological and environmental consequences. This paper will examine the challenges faced by the team as they successfully defined, planned and executed remedial work to a specific aspect of the civil structure, the SHM gantry rail system, using a purpose built refurbishment platform; the Gantry Refurbishment System. The paper will examine how an “innovative” approach was adopted to resolve the related issues of: • Refurbishing an aged structure to meet extended future operational demands. • The application of pragmatic engineering solutions against current codes and standards including seismic performance. • Provision of safe access for the workforce to undertake the refurbishment work against significant radiological and conventional safety constraints. • The use of off site test facilities to prove work methods. • Engagement of the multiple workforces including supply chain. • Development of challenging safety cases and management control arrangements to undertake the work. The paper will detail the arrangements established to engage all stakeholder groups aligned to a common goal, programme and end position, together with the arrangements put in place for managing the activities of delivery teams, operators and emergency response teams engaged in task execution over the five year period. Finally, the paper will also share the learning from the projects completion, so that the improvement opportunities flowing from this approach can be shared across the industry for the future benefit of all.Copyright © 2011 by ASME",,2011.0,10.1115/ICEM2011-59133,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
577363b3f2e593b349f409a8c6996b8359dfcf3c,https://www.semanticscholar.org/paper/577363b3f2e593b349f409a8c6996b8359dfcf3c,Robots in the home,"Robots are coming, but what does this mean for ordinary folks? First of all, don’t believe all the hype. Lots of hobbyists and small ventures would have you believe that robots are already here, capable of a wide variety of interactions, including healthcare and monitoring medication compliance, security monitoring, education, errands, and entertainment. Robots are, of course, used in manufacturing, in search-and-rescue missions, and in the military. But when we get away from industry and the military and discuss machines that are reasonably priced, most of these so-called applications are more imagination than reality, with unreliable mechanisms barely able to get through demonstrations. For everyday home applications, the use of robots is restricted to entertainment, vacuum cleaners, and lawn mowers. Note, however, that the definition of “robot” varies widely, often being used for anything mobile, even though controlled by a human. Personally, I would classify intelligent home appliances as robots: my coffee maker, microwave oven, dishwasher, and clothes washer and dryer have more intelligence and actuators than robot vacuum cleaners—and they are also a lot more expensive. But they don’t move around the room, which for many people disqualifies them from the label of “robot.” Given that any successful product for the home must be affordable, reliable, safe, and usable by everyday people, what might a home robot do? And what would it look like? In the home, form probably will follow function. A kitchen robot might be built into the counter space, with dishwasher, pantry, coffee maker, and cooking units all arranged so that they can communicate with one another and pass items readily back and forth. An entertainment robot might take on a humanoid appearance (as in Wow Wee’s Robosapien), or an animal-like one (as in Sony’s Aibo). And robots that vacuum or mow lawns will look like, well, vacuum cleaners and lawn mowers. Making robots work well is incredibly difficult. Their sensory apparatus is limited because sensors are expensive and interpretation (especially commonsense knowledge) is still more suited for research than deployment. Robotic arms are expensive to build and not very reliable. This limits the range of possibilities. Mowing and vacuuming? Sure. Sorting laundry? Hard, but do-able. Picking up dirty items around the home? Doubtful. How about assistants for the elderly or those who need medical supervision? This is a booming area of exploration, but I am skeptical. Today’s devices are not reliable, versatile, or intelligent enough-not yet, anyway. Moreover, the social aspects of the interaction are far more complex than the technical ones, something the technology-driven enthusiasts typically fail to recognize. Three likely directions for the future are entertainment, home appliances, and education. We can start with today’s existing devices and slowly add on intelligence, manipulative ability, and function. Start small and build. The market for robots that entertain by being cute and cuddly is already well established. The second generation of vacuum cleaners is smarter than the first. Sony’s dog gets smarter and less expensive with each new version. We don’t yet think of washing machines, microwave ovens, and coffee makers as robots, but why not? They don’t move around the house, but they are getting better and smarter every year. And when the coffee maker is connected to the pantry and dishwasher, that will be a home robot worthy of the name—same for the coupling of sorting, washing, drying, and storing clothes. Education is a powerful possibility. There is already a solid basis of educational devices that aid learning. Today’s robots can read aloud in engaging voices. They can be cute and lovable-witness the responses to the multiple quasi-intelligent animals on the toy market. A robot could very well interact with a child, offering educational benefits as well. Why not have the robot help the child learn the alphabet, teach reading, vocabulary, pronunciation, basic arithmetic, maybe basic reasoning? Why not music and art, geography and history? And why restrict it to children? Adults can be willing and active learners. Now this is a direction worthy of exploration: robot as teacher. Not to replace school, not to replace human contact and interaction, but to supplement them. The beauty here is that these tasks are well within the abilities of today’s devices. They don’t require much mobility or sophisticated manipulators. Many technologists dream of implementing Neil Stephenson’s children’s tutor in his novel The Diamond Age: Or, A Young Lady’s Illustrated Primer. Why not? Here is a worthy challenge. T H E W A Y I S E E IT",Interactions,2005.0,10.1145/1052438.1052473,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
39c3990d5c106dbe392dbb781d051904ecea0c33,https://www.semanticscholar.org/paper/39c3990d5c106dbe392dbb781d051904ecea0c33,Fish Inspection System Using a Parallel Neural Network Chip and the Image Knowledge Builder Application,"A generic image learning system, CogniSight, is being used for the inspection of fishes before filleting offshore. More than thirty systems have been deployed on seven fishing vessels in Norway and Iceland over the past three years. Each CogniSight uses four neural network chips (a total of 312 neurons) based on a natively parallel hardwired architecture performing real time learning and non-linear classification (RBF). These systems are trained by the ship crew using Image Knowledge Builder, a ”show and tell” interface for easy training and validation. Fishermen can reinforce the learning at anytime when needed. The use of CogniSight has reduced significantly the number of crewmembers on the boats (by up to six persons) and the time at sea has shortened by 15%. The prompt and strong return of the investment to the fishing fleet has increased significantly the market shares of Pisces Industries, the company integrating CogniSight systems to its filleting machines.",AI Mag.,2007.0,10.1609/aimag.v29i1.2084,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
32715b4400ac93d65cbb589a866abba79395ac73,https://www.semanticscholar.org/paper/32715b4400ac93d65cbb589a866abba79395ac73,New computational intelligence algorithms and their applications,"Several new computational intelligence algorithms and their applications are investigated in this thesis. First, a linear support vector machine decision tree (LSVM-DT) is developed by building a binary tree with a linear support vector machine in each tree node. It has built-in rare event detection mechanism, and allows efficient rule extraction. Secondly, an efficient recursive update algorithm when new data becomes available is derived for least squares support vector machines. This is very essential in online learning. Thirdly, a three-layered learning system is proposed. It consists of a random mapping stage and a learning stage with ordinary least squares, error-correcting least squares, or a linear support vector machine. Next, the three-layered system with ordinary least squares is further developed into a statistical, self-organizing learning system (SSOLS). It incorporates automatic determination of the enhancement nodes using validation, VC dimension, and efficient leave-one-out methods. The t-test pruning procedure and the gradient descent update are used to make the network more compact. The last past of this thesis investigates a real-world example of how computational intelligence algorithms can be applied to automate the decision making processes in manufacturing industries. Data are collected from a Six Sigma simulator that simulates an advanced TV production line. Several computational intelligence algorithms are then used to model this manufacturing process, and a global optimization technique is applied to obtain the optimum input settings that result in maximum overall % yield. Comparison with traditional methods such as Design of Experiments shows promise in deploying computational intelligence algorithms in manufacturing enterprises of the future.",,2003.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
f5bacc4e7d04edbff545c7efb44d80a83c4142bc,https://www.semanticscholar.org/paper/f5bacc4e7d04edbff545c7efb44d80a83c4142bc,Physical location awareness for enterprise IT assets,"In this paper we discuss techniques to automatically discover and track the physical location of networked devices in support of enterprise IT asset management. Recording the location of physical IT assets has long been the domain of manual change-tracking processes and wall-to-wall inventory activities. Meanwhile, increasing end-user device mobility, company growth and merger dynamics, adoption of outsourcing models, as well as transformation of formerly physical servers into virtual machines challenge the efficiency of manual location tracking. Dedicated sensor networks such as those used for RFID tags have been proposed as a replacement. In practice however, extensive infrastructure deployment costs seem to preclude industry adoption. Hence, we focus on location discovery techniques that leverage the ubiquitous wired and wireless enterprise networking infrastructure, to which an increasing percentage of IT assets is at least occasionally connected. Based on this assumption, we present a novel system architecture and algorithm for enterprise-network topology-centric IT asset location estimation based on adaptive learning techniques and spatial hierarchies. We use authentic and simulated data to show the functional and non-functional characteristics of this system.",IEEE INFOCOM Workshops 2008,2008.0,10.1109/INFOCOM.2008.4544617,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
d670959bcbd91ad38b0d5e46b98448a2d98708bd,https://www.semanticscholar.org/paper/d670959bcbd91ad38b0d5e46b98448a2d98708bd,The AI Business: The Commercial Uses of Artificial Intelligence,"Artificial intelligence has already been applied to many areas since its official birth in 1956, but most of the applications ended up in great disappointments as the benefits they reaped were very low. Due to this reason the vast interest in applying this relatively young technology to business calmed down in the late seventies when scientists recognized that the current intelligent systems were not yet plug-and-play solutions, hence mature enough to fully meet the business needs and requirements at that time. However, the limited commercial applicability of artificial intelligence in the past has to be rethought today as with the significant progress in artificial intelligence research and the growth of electronic commerce conducted over the World Wide Web new opportunities for business applications of artificial intelligence have emerged consequently. Nowadays horizontal and vertical electronic commerce is significantly driven by intelligent applications. Their employment in electronic businesses may well generate huge returns on investments, providing a technology-based response to increasing competition, the volatility of business models, and the pace of technology change . Despite the wide assumption that artificial intelligence will have a major impact on Internet-related businesses today and especially in the next years to come, it is uncertain to what extent it performs and will perform that way. The purpose of this thesis is to analyse, assess and evaluate the potential of commercial applications of artificial intelligence in electronic businesses. Therefore the main research question of this paper is whether artificial intelligence is reasonably applicable in Internet-related businesses, first in terms of effectiveness and second in terms of efficiency. In the assessment the application of artificial intelligence in electronic businesses is represented by the employment of intelligent agents. In harmony with the major research question emphasized above, the paper provides a thorough discussion about the economic impact of the most common and relevant application types of intelligent agents on electronic commerce environments. In addition the driving underlying technologies of intelligent agents are analysed with respect to artificial intelligence techniques and methods, and current standardisation efforts. [] Assessment of the Commercial Applicability of Artificial Intelligence in Electronic Businesses Thomas Kramer 2002-04-18 Abstract: Artificial intelligence has already been applied to many areas since its official birth in 1956, but most of the applications ended up in great disappointments as the benefits they reaped were very low. Due to this reason the vast interest in applying this relatively young technology to business calmed down in the late seventies when scientists recognized that the current intelligent systems were not yet plug-and-play solutions, hence mature enough to fully meet the business needs and requirements at that time. However, the limited commercial applicability of artificial intelligence in the past has to be rethought today as with the significant progress in artificial intelligence research and the growth of electronic commerce conducted over the World Wide Web new opportunities for business applications of artificial intelligence have emerged consequently. Nowadays horizontal and vertical electronic commerce is significantly driven by intelligent applications. Their employment in electronic businesses may well generate huge returns on investments, providing a technology-based response to increasing competition, the volatility of business models, and the pace of technology change . Despite the wide assumption that artificial intelligence will have a major impact on Internet-related businesses today and especially in the next years to come, it is uncertain to what extent it performs and will perform that way. The purpose of this thesis is to analyse, assess and evaluate the potential of commercial applications of artificial intelligence in electronic businesses. Therefore the main research question of this paper is whether artificial intelligence is reasonably applicable in Internet-related businesses, first in terms of effectiveness and second in terms of efficiency. In the assessment the application of artificial intelligence in electronic businesses is represented by the employment of intelligent agents. In harmony with the major research question emphasized above, the paper provides a thorough discussion about the economic impact of the most common and relevant application types of intelligent agents on electronic commerce environments. In addition the driving underlying technologies of intelligent agents are analysed with respect to artificial intelligence techniques and methods, and current standardisation efforts. [] Artificial intelligence has already been applied to many areas since its official birth in 1956, but most of the applications ended up in great disappointments as the benefits they reaped were very low. Due to this reason the vast interest in applying this relatively young technology to business calmed down in the late seventies when scientists recognized that the current intelligent systems were not yet plug-and-play solutions, hence mature enough to fully meet the business needs and requirements at that time. However, the limited commercial applicability of artificial intelligence in the past has to be rethought today as with the significant progress in artificial intelligence research and the growth of electronic commerce conducted over the World Wide Web new opportunities for business applications of artificial intelligence have emerged consequently. Nowadays horizontal and vertical electronic commerce is significantly driven by intelligent applications. Their employment in electronic businesses may well generate huge returns on investments, providing a technology-based response to increasing competition, the volatility of business models, and the pace of technology change . Despite the wide assumption that artificial intelligence will have a major impact on Internet-related businesses today and especially in the next years to come, it is uncertain to what extent it performs and will perform that way. The purpose of this thesis is to analyse, assess and evaluate the potential of commercial applications of artificial intelligence in electronic businesses. Therefore the main research question of this paper is whether artificial intelligence is reasonably applicable in Internet-related businesses, first in terms of effectiveness and second in terms of efficiency. In the assessment the application of artificial intelligence in electronic businesses is represented by the employment of intelligent agents. In harmony with the major research question emphasized above, the paper provides a thorough discussion about the economic impact of the most common and relevant application types of intelligent agents on electronic commerce environments. In addition the driving underlying technologies of intelligent agents are analysed with respect to artificial intelligence techniques and methods, and current standardisation efforts. [] The AI Book Ivana Bartoletti 2020-06-04 Written by prominent thought leaders in the global fintech space, The AI Book aggregates diverse expertise into a single, informative volume and explains what artifical intelligence really means and how it can be used across financial services today. Key industry developments are explained in detail, and critical insights from cutting-edge practitioners offer first-hand information and lessons learned. Coverage includes: · Understanding the AI Portfolio: from machine learning to chatbots, to natural language processing (NLP); a deep dive into the Machine Intelligence Landscape; essentials on core technologies, rethinking enterprise, rethinking industries, rethinking humans; quantum computing and next-generation AI · AI experimentation and embedded usage, and the change in business model, value proposition, organisation, customer and co-worker experiences in today’s Financial Services Industry · The future state of financial services and capital markets – what’s next for the real-world implementation of AITech? · The innovating customer – users are not waiting for the financial services industry to work out how AI can re-shape their sector, profitability and competitiveness · Boardroom issues created and magnified by AI trends, including conduct, regulation & oversight in an algo-driven world, cybersecurity, diversity & inclusion, data privacy, the ‘unbundled corporation’ & the future of work, social responsibility, sustainability, and the new leadership imperatives · Ethical considerations of deploying Al solutions and why explainable Al is so important The AI Book Ivana Bartoletti 2020-06-04 Written by prominent thought leaders in the global fintech space, The AI Book aggregates diverse expertise into a single, informative volume and explains what artifical intelligence really means and how it can be used across financial services today. Key industry developments are explained in detail, and critical insights from cutting-edge practitioners offer first-hand information and lessons learned. Coverage includes: · Understanding the AI Portfolio: from machine learning to chatbots, to natural language processing (NLP); a deep dive into the Machine Intelligence Landscape; essentials on core technologies, rethinking enterprise, rethinking industries, rethinking humans; quantum computing and next-generation AI · AI experimentation and embedded usage, and the change in business model, value proposition, organisation, customer and co-worker experiences in today’s Financial Services Industry · The future state of financial services and capital markets – what’s next for the real-world implementation of AITech? · The innovating customer – users are not waiting for the financial services industry to work out ",IEEE Transactions on Pattern Analysis and Machine Intelligence,1985.0,10.1109/TPAMI.1985.4767689,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
20830c4e6040b95aa686be94ddb1a268e6450016,https://www.semanticscholar.org/paper/20830c4e6040b95aa686be94ddb1a268e6450016,Impact Statement,"My work in the domain of computer security looks into the design, analysis, integration, and evaluation of methods for decision automation and attribution of malicious actors, and has had various direct impacts on the practice, whether it is in the industry or academia. The core concepts of several of those works have been patented (see patents in the attached resume), and their implementations have been used in production to facilitate attribution and risk assessment of domain names of malicious software, domain name, or even systems. In the following, I elaborate on some of such impacts. Behavior-based malware detection (pioneering) My work on behavior-based malware analysis, which is captured in my system AMAL, is one of the earliest works in this domain, initially published in 2013. AMAL, a tool to classify and cluster malware using autonomous feature extraction, sandboxes malicious binaries to collect fine-grained behavioral artifacts that characterize malware’s usage of the file system, memory, network, and registry. A key contribution of AMAL is the extrapolation of labels on unlabeled malware samples. Since its inception (2013), AMAL was incorporated into iDefense (then a subsidary of Verisign) malware analysis toolset. Today, AMAL reduces the manual analysis effort by two orders of magnitude, while providing practical accuracies. Academically, and given that AMAL one of the earliest works in this domain, the work has been well-cited with more than 150 follow-up studies on the subject. Personally, this work was the start of a productive journey on ML/DL-based detection systems as well as adversarial machine learning techniques with 30+ publications published in top conferences, e.g., ACM WiSec, ICDCS, INFOCOM, DSN, etc. Authorship attribution using deep learning (answering open questions) Code authorship attribution is an important problem where it is required to map a set of authors to their code snippets. The problem has witnessed a recent surge in interest, featured by various systems using hand-picked features for conducting this mapping, make the problem intractable for large-scale attribution. My work in this space, through DL-CAIS (CCS 2018) and Multi-X (PETS 2020) develops tools for automatically generating high-quality feature representations for large-scale authorship attribution. Compared the prior work, we were able to maintain a high accuracy for 10x the number of authors at a fraction of the overhead. Through Multi-X, we were able to attribute code segments in snippets with multiple authors. Our work in this domain closes key challenges in the area, by showing the feasibility of deep learning based code authorship attribution. Securing blockchain systems (knowledge systemization) One of the key areas of effort that I have worked on during the past 5 years is blockchain systems security. My work has provided the underpinnings for foundations-driven guarantees of blockchain systems security through a deeper understanding of their attack surface. In this direction, my work explored various attack possibilities on widely deployed blockchain systems (e.g., Bitcoin and Ethereum), including partitioning attacks (ICDCS 2019), optimized mining attacks (CCS 2021), DDoS attacks (ICBC 2019), fair mining (TPDS 2021), among many others. My work has established the need for a systematic understanding of blockchain systems security by demonstrating various vulnerabilities. The impact of this work is not only to practice, but also in academia: our work systematically analyzing the attack surface of blockchains (IEEE Communications Surveys & Tutorials, 2020) has been the go-to resource to reference, as evident by more than 65 citations to date, in less than one year. Mixing time of social graphs (paradigm shift) Until 2010, it was widely believed that social networks are “fast-mixing”, and many Sybil defenses made crucial use of this property. An experimental verification of this property was lacking, and my work explored mathematical tools and used them to measure the mixing time of several social graphs. My findings show that the mixing time of social graphs is much larger than used in literature, which leads to several striking results. First, designs based on the fast-mixing property utilize a weaker property concerning the average mixing in the social graph (as opposed to the worst mixing, making most theoretical provable guarantees of these systems inaccurate). Second, current security systems based on fast-mixing properties have weaker guarantees and have to be less efficient in to compensate for the slower mixing graphs. The work presented a breakthrough, shifting interest in the community to other assumptions for system design, and is highly cited (200+ citations as of 2020). Impact through standardization (beyond academia) Along with collaborators from TU Eindhoven, TU Darmstadt, genua GmbH, and Radboud University, I led the effort of specifying the implementation of a hash-based signature in the Internet Engineering Task Force (IETF) Request for Comments (RFC 8391). RFC 8391 provides a standard of the first hash-based signature algorithm, which is today incorporated into widely used software (OpenSSL), and is expected to be used on millions of devices. The impact of this work is rarely seen in an academic work, as such efforts are only done in industry. Students and placement (biggest impact) At UCF (since 2017), I have advised 15 doctoral students to candidacy (+3 who switched advisors before passing candidacy). Out of those 15, ten have either graduated (7), or are to graduate within 2021 (3). Out of those 10, only one PhD student went to industry, and the rest went/will go to academia: five as tenuretrack assistant professors (Wayne, Layola, Niagara, etc.) and four as postdoctoral researchers (Georgia Tech, TAMU, Northeastern). At Buffalo, I mentored two postdocs who went to academia as tenure-track assistant professors. At Buffalo and UCF, I mentored more than a dozen M.Sc. and B.Sc. students who joined government and industrial entities. Recognitions To date, my work has received more than 3500 citations, with h-index of 31 and i10-index of 86, with a healthy trajectory. My work won the best paper award at IEEE Systems Journal 2020, IEEE DSC 2019, IEEE ICDCS 2017, WISA 2014, and IEEE CNS 2013, and was featured in news articles in various outlets, including MIT Technology Review, Science Daily, Scientific American, Financial Express, Slashdot, CBS news, The Verge, New Scientist, etc.",,1997.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
13364ed14f828438d4b08136c92b2081799c971a,https://www.semanticscholar.org/paper/13364ed14f828438d4b08136c92b2081799c971a,Some educational implications of the microprocessor revolution,"The development of the microprocessor has enabled a whole new generation of sophisticated electronic devices to be produced. Microprocessors are cheap to make, versatile in their range of application, and can be used to produce powerful computers and sophisticated pieces of control apparatus, which are being widely deployed in industry and commerce. They are also cheap enough to be used in a variety of household devices, from washing machines to videogames. This thesis examines some of the effects which the microprocessor might have on people's lives in the future, and the educational implications of their proliferation. From the educational point of view there are, perhaps, two broad areas of interest; firstly, direct effects arising from the introduction of microprocessors into educational establishments and, secondly, the indirect effects arising from changes in society. Before attempting to predict future changes in the educational system it is useful to review some current areas of interest and, therefore, some aspects of educational theory which may be relevant are examined. One widely accepted view of the microprocessor revolution is that it will lead to massive unemployment, and the validity of this view is examined. Some of the predictions of job losses which may ensue are also considered, together with some of the possibilities for occupying increased leisure time which may result. The concept of ""education for leisure"" is examined vis-a-vis possible changes in employment patterns. The direct role of microprocessors in schools is considered, together with future implications for teaching and learning. Possible consequences on the curriculum are discussed.",,1984.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
a132844518e757c8c34d356c8f1f05a724d0aeab,https://www.semanticscholar.org/paper/a132844518e757c8c34d356c8f1f05a724d0aeab,Wideband Beamforming www.modernh.com for to for,"Real-time Narrowband and Wideband Beamforming Techniques Fully-digital RF ArraysApplication of Frequency Invariant Constraints Wideband BeamformingLow-cost Smart AntennasGeolocation of RF SignalsUltra-Wideband, Short-Pulse Electromagnetics 5Statistical Signal Processing in EngineeringMIMO Antennas Wireless CommunicationAdaptive Array SystemsWireless Communication SystemsUltra Wideband Wireless CommunicationVLSIConvex Optimization in Signal Processing and CommunicationsSignal Processing Algorithms for Communication and Radar SystemsAn Analysis of Wideband Beamforming Techniques and Hardware Requirements for Analog and Digital Radar ArchitecturesUltra-Wideband Antennas and PropagationPervasive Mobile and Ambient Wireless CommunicationsIssues in Electronic Circuits, Devices, and Materials: 2013 EditionAdvances in Neural Networks - ISNN 2005Academic Press Library in Signal ProcessingUWB Communication SystemsCooperative and Cognitive Satellite SystemsAll-Optical Signal ProcessingRobust Adaptive BeamformingNeural information processing [electronic resource]Multiple Access CommunicationsUltra Wideband Signals and Systems in Communication EngineeringAdvanced Wireless CommunicationsTrue-Time-Delay Beamforming für ultrabreitbandige Systeme hoher LeistungIssues in Electronic Circuits, Devices, and Materials: 2012 EditionSimplified Robust Adaptive Detection and Beamforming for Wireless CommunicationsImaging: Sensors and TechnologiesWideband Direction of Arrival Estimation and Wideband Beamforming for Smart Antenna SystemsAdvances in Acoustic Emission TechnologyWideband BeamformingSound Visualization and ManipulationDistributed Sensor NetworksCognitive Radio Communication and NetworkingAdvanced Antenna Systems for 5G Network DeploymentsSound Capture for Human / Machine InterfacesMicrowave and Millimeter Wave Circuits and Systems multi-path signals. Key Features: Unique book focusing on wideband beamforming Discusses a hot topic coinciding with the increasing bandwidth in wireless communications and the development of UWB technology Addresses the general concept of beamforming including fixed beamformers and adaptive beamformers Covers advanced topics including sub-band adaptive beamforming, frequency invariant beamforming, blind wideband beamforming, beamforming without temporal processing, and beamforming for multi-path signals Includes various design examples and corresponding complexity analyses This book provides a reference for engineers and researchers in wireless communications and signal processing fields. Postgraduate students studying signal processing will also find this book of interest.This practically-oriented, all-inclusive guide covers all the major enabling techniques for current and next-generation cellular communications and wireless networking systems. Technologies covered include CDMA, OFDM, UWB, turbo and LDPC coding, smart antennas, wireless ad hoc and sensor networks, MIMO, and cognitive radios, providing readers with everything they need to master wireless systems design in a single volume. Uniquely, a detailed introduction to the properties, design, and selection of RF subsystems and antennas is provided, giving readers a clear overview of the whole wireless system. It is also the first textbook to include a complete introduction to speech coders and video coders used in wireless systems. Richly illustrated with over 400 figures, and with a unique emphasis on practical and state-of-the-art techniques in system design, rather than on the mathematical foundations, this book is ideal for graduate students and researchers in wireless communications, as well as for wireless and telecom engineers.This volume collects the papers from the 2013 World Conference on Acoustic Emission in Shanghai. The latest research and applications of Acoustic Emission (AE) are explored, with particular emphasis on detecting and processing of AE signals, development of AE instrument and testing standards, AE of materials, engineering structures and systems, including the processing of collected data and analytical techniques as well as experimental case studies.This book presents an alternative and simplified approaches for the robust adaptive detection and beamforming in wireless communications. It adopts several systems models including DS/CDMA, OFDM/MIMO with antenna array, and general antenna arrays beamforming model. It presents and analyzes recently developed detection and beamforming algorithms with an emphasis on robustness. In addition, simplified and efficient robust adaptive detection and beamforming techniques are presented and compared with exiting Spectrum Sensing: Basic Techniques; Cooperative Sensing Wideband Transmission Orthogonal Frequency Division Multiplexing Multiple Input Multiple Output for Cognitive Radio; Convex Optimization for Cognitive Radio; Cognitive Core (I): Algorithms for Reasoning and Learning; Cognitive Core (II): Game Theory; Cognitive Radio Network IEEE The First Cognitive Radio Wireless Regional Area Network Standard, and Testbeds.This and of research ultra-wideband radar systems; ultra-wideband and transient antennas; pulsed power generation and propagation; ultra-wideband polarimetry; ultra-wideband and transient metrology; detection and identification studies; RF interactions and chaotic effects; Multiple (UWB) the transmission sequence, the combining s cheme and ML decision rule for two-branch transmit diversity scheme with one and M receivers. Ultra Wide Band Radio, UWB multiple access in Gaussian channels, the UWB channel, UWB system with M-ary modulation, M-ary PPM UWB multiple access, coded UWB schemes, multi-user detection in UWB radio, UWB with space time processing and beam forming for UWB radio. Adaptive Beamforming the and work ofleading researchers investigating various approaches in onecomprehensive uncertainty set of thearray steering vector. their the standard Capon beamformers with a spherical orellipsoidal uncertainty set of the array steering Diagonal loading for finite sample size beamforming * Mean-squared error beamforming for signal estimation * Constant modulus beamforming * Robust wideband beamforming using a steered adaptive beamformerto adapt the weight vector within a generalized sidelobe cancellerformulation Adaptive Beamforming up-to-date reference for engineers, researchers, and inthis rapidly expanding field.ULTRA including part of massive MIMO to provide the important aspects of emerging technology. Aimed at researchers, professionals and graduate students in electrical engineering, electromagnetics, communications and signal processing including antenna theory and design, smart antennas, communication systems, this book: Investigates real time MIMO antenna designs for WLAN/WiMAX/LTE applications. Covers effects of ECC, MEG, TARC, and equivalent circuit. Addresses the coupling and diversity aspects of antenna design problem for MIMO systems. Focus on the MIMO antenna designs for the real time applications. Exclusive chapter on 5G Massive MIMO along with case studies throughout the book.Geolocation of RF Signals—Principles and Simulations offers an overview of the best practices and innovative techniques in the art and science of geolocation over the last twenty years. It covers all research and development aspects including theoretical analysis, RF signals, geolocation techniques, key block diagrams, and practical principle simulation examples in the frequency band from 100 MHz to 18 GHz or even 60 GHz. Starting with RF signals, the book progressively examines various signal bands – such as VLF, LF, MF, HF, VHF, UHF, L, S, C, X, Ku, and, K and the corresponding geolocation requirements per band and per application – to achieve required performance objectives of up to 0o precision. Part II follows a step-by-step approach of RF geolocation techniques and concludes with notes on state-of-the-art geolocation designs as well as advanced features found in signal generator instruments. Drawing upon years of practical experience and using numerous examples and illustrative applications, Ilir Progri provides a comprehensive introduction to Geolocation of RF Signals, and includes hands-on real world labs and applications using MATLAB in the areas of: RF signals specifications, RF geolocation distributed wireless communications networks and RF geolocation. Geolocation of RF Signals—Principles and Simulations will be of interest to government agency program managers industry professionals and engineers, academic researchers, faculty and graduate students who are interested in or currently designing, developing and deploying innovative geolocation of RF Signal systems.Advanced Antenna Systems for 5G Network Deployments: Bridging the Gap between Theory and Practice provides a comprehensive understanding of the field of advanced antenna systems (AAS) and how they can be deployed in 5G networks. The book gives a thorough understanding of the basic technology components, the state-of-the-art multi-antenna solutions, what support 3GPP has standardized together with the reasoning, AAS performance in real networks, and how AAS can be used to enhance network deployments. Explains how AAS features impact network performance and how AAS can be effectively used in a 5G network, based on either NR and/or LTE Shows what AAS configurations and features to use in different network deployment scenarios, focusing on mobile broadband, but also including fixed wireless access Presents the latest developments in multi-antenna technologies, including Beamforming, MIMO and cell shaping, along with the potential of different technologies in a commercial network context Provides a deep understanding of the differences between mid-band and mm-Wave solutionsWith a continuously increasing desire for natural and comfortable human/machine interaction, the acoustic interface of any terminal for multimedia or telecommunication services is challenged to allow seamless and hands-free audio commun",,2022.0,10.1002/9780470661178,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
a95cd552d81a668f333d30b1c28f840b554d51fe,https://www.semanticscholar.org/paper/a95cd552d81a668f333d30b1c28f840b554d51fe,Six-Sigma Quality Management of Additive Manufacturing,"Quality is a key determinant in deploying new processes, products, or services and influences the adoption of emerging manufacturing technologies. The advent of additive manufacturing (AM) as a manufacturing process has the potential to revolutionize a host of enterprise-related functions from production to the supply chain. The unprecedented level of design flexibility and expanded functionality offered by AM, coupled with greatly reduced lead times, can potentially pave the way for mass customization. However, widespread application of AM is currently hampered by technical challenges in process repeatability and quality management. The breakthrough effect of six sigma (6S) has been demonstrated in traditional manufacturing industries (e.g., semiconductor and automotive industries) in the context of quality planning, control, and improvement through the intensive use of data, statistics, and optimization. 6S entails a data-driven DMAIC methodology of five steps—define, measure, analyze, improve, and control. Notwithstanding the sustained successes of the 6S knowledge body in a variety of established industries ranging from manufacturing, healthcare, logistics, and beyond, there is a dearth of concentrated application of 6S quality management approaches in the context of AM. In this article, we propose to design, develop, and implement the new DMAIC methodology for the 6S quality management of AM. First, we define the specific quality challenges arising from AM layerwise fabrication and mass customization (even one-of-a-kind production). Second, we present a review of AM metrology and sensing techniques, from materials through design, process, and environment, to postbuild inspection. Third, we contextualize a framework for realizing the full potential of data from AM systems and emphasize the need for analytical methods and tools. We propose and delineate the utility of new data-driven analytical methods, including deep learning, machine learning, and network science, to characterize and model the interrelationships between engineering design, machine setting, process variability, and final build quality. Fourth, we present the methodologies of ontology analytics, design of experiments (DOE), and simulation analysis for AM system improvements. In closing, new process control approaches are discussed to optimize the action plans, once an anomaly is detected, with specific consideration of lead time and energy consumption. We posit that this work will catalyze more in-depth investigations and multidisciplinary research efforts to accelerate the application of 6S quality management in AM.",Proceedings of the IEEE,2021.0,10.1109/JPROC.2020.3034519,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
5c51a7ca91a4c0b261fac9b0fd3628c55c55b03e,https://www.semanticscholar.org/paper/5c51a7ca91a4c0b261fac9b0fd3628c55c55b03e,Trends and Directions of Financial Technology (Fintech) in Society and Environment: A Bibliometric Study,"The contemporary innovations in financial technology (fintech) serve society with an environmentally friendly atmosphere. Fintech covers an enormous range of activities from data security to financial service deliverables that enable the companies to automate their existing business structure and introduce innovative products and services. Therefore, there is an increasing demand for scholars and professionals to identify the future trends and directions of the topic. This is why the present study conducted a bibliometric analysis in social, environmental, and computer sciences fields to analyse the implementation of environment-friendly computer applications to benefit societal growth and well-being. We have used the ‘bibliometrix 3.0’ package of the r-program to analyse the core aspects of fintech systematically. The study suggests that ‘ACM International Conference Proceedings’ is the core source of published fintech literature. China leads in both multiple and single country production of fintech publications. Bina Nusantara University is the most relevant affiliation. Arner and Buckley provide impactful fintech literature. In the conceptual framework, we analyse relationships between different topics of fintech and address dynamic research streams and themes. These research streams and themes highlight the future directions and core topics of fintech. The study deploys a co-occurrence network to differentiate the entire fintech literature into three research streams. These research streams are related to ‘cryptocurrencies, smart contracts, financial technology’, ‘financial industry stability, service, innovation, regulatory technology (regtech)’, and ‘machine learning and deep learning innovations’. The study deploys a thematic map to identify basic, emerging, dropping, isolated, and motor themes based on centrality and density. These various themes and streams are designed to lead the researchers, academicians, policymakers, and practitioners to narrow, distinctive, and significant topics.",Applied Sciences,2021.0,10.3390/app112110353,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
e5d522be50a596536c22c22c50807085154681a9,https://www.semanticscholar.org/paper/e5d522be50a596536c22c22c50807085154681a9,The NECOS Approach to End-to-End Cloud-Network Slicing as a Service,"Cloud-network slicing is a promising approach to serve vertical industries delivering their services over multiple administrative and technological domains. However, there are numerous open challenges to provide end-to-end slices due to complex business and engineering requirements from service and resource providers. This article presents a reference architecture for the cloud-network slicing concept and the practical realization of the slice-as-a-service paradigm, which are key results from the Novel Enablers in Cloud Slicing (NECOS) project. The NECOS platform has been designed to consider modularity, separation of concerns, and multi-domain dynamic operation as prime attributes. The architecture comprises a set of interworking components to automatically create, manage, and decommission end-to-end cloud-network slice instances in a lightweight manner. NECOS orchestrates slices at runtime, spanning across core/edge data centers and wired/wireless network infrastructures. The novelties of the multi-domain NECOS platform are validated through three proof-of-concept experiments: (i) a touristic content delivery service slice deployment featuring on-demand virtual infrastructure management across three countries on different continents to meet particular slice requirements; (ii) intelligent slice elasticity driven by machine learning techniques; and (iii) market-place-based resource discovery capabilities.",IEEE Communications Magazine,2021.0,10.1109/MCOM.001.2000702,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
6e0cdea54f6ef68adb7870b0777eaa1bc44de42b,https://www.semanticscholar.org/paper/6e0cdea54f6ef68adb7870b0777eaa1bc44de42b,2021 ACTIVITY REPORT Project-Team DIVERSE Diversity-centric Software Engineering,"modify the behavior or non-functional properties of a software. Deep software variability calls to investigate how to systematically handle cross-layer conﬁguration. The diversiﬁcation of the different layers is also an opportunity to test the robustness and resilience of the software layer in multiple environments. Another interesting challenge is to tune the software for one speciﬁc executing environment. In essence, deep software variability questions the generalization of the conﬁguration knowledge. Functional Description: Familiar is an environment for large-scale product customisation. From a model of product features (options, parameters, etc.), Familiar can automatically generate several million variants. These variants can take many forms: software, a graphical interface, a video sequence or even a manufactured product (3D printing). Familiar is particularly well suited for developing web conﬁgurators (for ordering customised products online), for providing online comparison tools and also for engineering any family of embedded or software-based products. Functional Description: PIT and Descartes are mutation testing systems for Java applications, which allows you to verify if your test suites can detect possible bugs, and so to evaluate the quality of your test suites. They evaluate the capability of your test suite to detect bugs using mutation testing (PIT) or extreme mutation testing (Descartes). Mutation testing does it by introducing small changes or faults into the original program. These modiﬁed versions are called mutants. A good test suite should able to kill or detect a mutant. Traditional mutation testing works at the instruction level, e.g., replacing "">"" by ""<="", so the number of generated mutants is huge, as the time required to check the entire test suite. That’s why Extreme Mutation strategy appeared. In Extreme Mutation testing, the whole body of a method under test is removed. Descartes is a mutation engine plugin for PIT which implements extreme mutation operators. Both provide reports combining, line coverage, mutation score and list of weaknesses in the source. reasoning, and (iii) the adaptations and the predictive model of their impact on the trade-off. We use this framework to build three languages with self-adaptive virtual machines and discuss the relevance of the abstractions, effectiveness of correctness envelopes, and compare their code size and performance results to their manually implemented counterparts. We show that the framework provides suitable abstractions for the implementation of self-adaptive operational semantics while introducing little performance overhead compared to a manual implementation. change propagation. This work leverages classical inconsistency repair mechanisms to explore the vast search space of change propagation. Our approach not only suggests changes to repair a given inconsistency but also changes to repair inconsistencies caused by the aforementioned repair. In doing so,our approach follows the developer’s intent where subsequent changes may not contradict or backtrack earlier changes. We argue that consistent change propagation is essential for effective model driven engineering. Our approach and its tool implementation were empirically assessed on 18 case studies from industry, academia, and GitHub to demonstrate its feasibility and scalability. A comparison with two versioned models shows that our approach identiﬁes actual repair sequences that developers that categorize experiments belonging to either safe or failure-prone states. We apply ChaT to a video streaming application use case. The simulation results show the effectiveness of ChaT to achieve our goals: identifying execution classes and detecting failure-prone experiments based on metamorphic relationships with high level of statistical scores. two standards and we provide TOSCA Studio, a model-driven tool chain for TOSCA that conforms to OCCI. TOSCA Studio allows to graphically design cloud applications as well as to deploy and manage them at runtime using a fully model-driven cloud orchestrator based on the two standards. Our contribution is validated by successfully transforming and deploying three cloud applications: WordPress, Node Cellar and Multi-Tier. These achievements constitute basic blocks upon which we will continue building our research and systems, extending for example the applicability to secure supply chains. • Abstract: The aim of the Falcon project is to investigate how to improve the resale of available resources in private clouds to third parties. In this context, the collaboration with DiverSE mainly aims at working on efﬁcient techniques for the design of consumption models and resource consumption forecasting models. These models are then used as a knowledge base in a classical autonomous loop. • Abstract: The GLOSE project develops new techniques for heterogeneous modeling and simulation in the context of systems engineering. It aims to provide formal and operational tools and methods to formalize the behavioral semantics of the various modeling languages used at system-level. These semantics will be used to extract behavioral language interfaces supporting the deﬁnition of coordination patterns. These patterns, in turn, can systematically be used to drive the coordination of any model conforming to these languages. The project is structured according to the following tasks: concurrent xDSML engineering, coordination of discrete models, and coordination of discrete/continuous models. The project is funded in the context of the network DESIR, and supported by the GEMOC initiative. The use case chosen for the demonstrator is the high-level description of a remote control drone system, whose the main objective is to illustrate the design and simulation of the main functional chains, the possible interactivity with the model in order to raise the level of understanding over the models built, and possibly the exploration of the design space. • Abstract: Debug4Science aims to propose a disciplined approach to develop domain-speciﬁc debugging facilities for Domain-Speciﬁc Languages within the context of scientiﬁc computing and numerical analysis. Debug4Science is a bilateral collaboration (2020-2022), between the CEA DAM/DIF and the DiverSE team Summary: The goal of the RESIST associate team is to explore the science of resilient software by foundational work on advanced a priori testing methods such as metamorphic testing and a posteriori continuous improvements through digital twins. resist web site • Abstract: Most modern software systems (operating systems such as Linux, Web browsers such as Firefox or Chrome, video encoders such as x264 or ffmpeg, servers, mobile applications, etc.) are subject to variation or come in many variants. Hundreds of conﬁguration options, features, or plugins can be combined, each potentially with distinct functionality and effects on execution time, memory footprint, etc. Among conﬁgurations, some of them are chosen and do not compile, crash at run time, do not pass a test suite, or do not reach a certain performance quality (e.g., energy consumption, security). In this JCJC ANR project, we follow a thought-provocative and unexplored direction: we consider that the variability boundary of a software system can be specialized and should vary when needs be. The goal of this project is to provide theories, methods and techniques to make variability vary. Speciﬁcally, we consider machine learning and software engineering techniques for narrowing the space of possible conﬁgurations to a good approximation of those satisfying the needs of users. Based on an oracle (e.g., a runtime test) that tells us whether a given conﬁguration meets the requirements (e.g., speed or memory footprint), we leverage machine learning to retroﬁt the acquired constraints into a variability that can be used to automatically specialize the conﬁgurable system. Based on a relative small number of conﬁguration samples, we expect to reach high accuracy for many different kinds of oracles and subject systems. Our preliminary experiments suggest that varying variability can be practically useful and effective. However, much more work is needed to investigate sampling, testing, and learning techniques within a variety of cases and application scenarios. We plan to further collect large experimental data and apply our techniques on popular, open-source, conﬁgurable software (like Linux, Firefox, ffmpeg, VLC, Apache or JHipster) and generators for media content (like videos, models for 3D printing, or technical papers written in LaTeX). • Abstract: in the context of this project, DGA-MI and the INRIA team DiverSE explore the existing approaches to ease the development of formal speciﬁcations of domain-Speciﬁc Languages (DSLs) dedicated to packet ﬁltering, while guaranteeing expressiveness, precision and safety. In the long term, this work is part of the trend to provide to DGA-MI and its partners a tooling to design and develop formal DSLs which ease the use while ensuring a high level of reasoning. • Abstract: The ONEWAY project aims at maturing digital functional bricks for the following capacities: 1) Digitalization, MBSE modeling and synthetic analysis by substitution model, of all the information and under all the points of view necessary for the design and validation across an extended enterprise of the complete aircraft system and at all its levels of decomposition, 2) Generic and instantiable conﬁguration management throughout the life cycle, on products and their support systems, in product lines or on aircraft programs, interactively in the context of an extended enterprise, 3) Decision support for launching, then controlling and steering a Product Development • Abstract: The IPSCO (Intelligent Support Processes and Communities) project aims to develop a new customer support platform for digital companies and public services. Both by implementing intell",,2022.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
5b68a02e18415725bc329ff850a9fd15b864f54a,https://www.semanticscholar.org/paper/5b68a02e18415725bc329ff850a9fd15b864f54a,Artificial Intelligence as a Service,,Bus. Inf. Syst. Eng.,2021.0,10.1007/s12599-021-00708-w,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
5fed6a995ce93f18bf4fb9793b44b436a2d4540b,https://www.semanticscholar.org/paper/5fed6a995ce93f18bf4fb9793b44b436a2d4540b,Computational Cost Analysis and Data-Driven Predictive Modeling of Cloud-based Online NILM Algorithm,"Online non-intrusive load monitoring methods have captivated academia and industries as parsimonious solutions for household energy efficiency monitoring as well as safety control, anomaly detection, and demand-side management. However, despite the promised energy efficiency by providing appliance specific consumption information feed-backs, the computational energy cost for running the load monitoring systems is not explored. This study analyzes whether the energy spent to execute the non-intrusive algorithms, out-weights the expected energy efficiency gain from using the algorithms. Furthermore, we present a study on the computation costs estimation and prediction of a Cloud-based online non-intrusive load disaggregation algorithm through data-driven models. Moreover, a generic framework for an automated algorithm computational cost monitoring and the modeling methodologies are devised and proposed for meeting extensive scaling load monitoring and deployment requirements. The proposed approach was examined and validated on ls and cvs running the disaggregation algorithm. The prediction models, developed using statistical and machine learning tools, demonstrate the promising applicability of the data-driven approach with a very high prediction accuracy without detailed knowledge of the computing systems and the algorithm.",,2021.0,10.1109/TCC.2021.3051766,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
8440f4751eb5ef87c41f95d969af473ed4149019,https://www.semanticscholar.org/paper/8440f4751eb5ef87c41f95d969af473ed4149019,Cloud Computing Based Real Time Vehicle Tracking And Speed Epdf Download,"This book addresses the emerging area of cloud computing, providing a comprehensive overview of the research areas, recent work and open research problems. The move to cloud computing is no longer merely a topic of discussion; it has become a core competency that every modern business needs to embrace and excel at. It has changed the way enterprise and internet computing is viewed, and this success story is the result of the long-term efforts of computing research community around the globe. It is predicted that by 2026 more than two-thirds of all enterprises across the globe will be entirely run in cloud. These predictions have led to huge levels of funding for research and development in cloud computing and related technologies. Accordingly, universities across the globe have incorporated cloud computing and its related technologies in their curriculum, and information technology (IT) organizations are accelerating their skill-set evolution in order to be better prepared to manage emerging technologies and public expectations of the cloud, such as new services. A critical part of ensuring that systems are advancing alongside technology without complications is problem solving. Practical applications of problem-solving theories can model conflict and cooperation and aid in creating solutions to real-world problems. Soft-Computing-Based Nonlinear Control Systems Design is a critical scholarly publication that examines the practical applications of control theory and its applications in problem solving to fields including economics, environmental management, and financial modelling. Featuring a wide range of topics, such as fuzzy logic, nature-inspired algorithms, and cloud computing, this book is geared toward academicians, researchers, and students seeking relevant research on control theory and its practical applications. This book explores the significant role of granular computing in advancing machine learning towards in-depth processing of big data. It begins by introducing the main characteristics of big data, i.e., the five Vs—Volume, Velocity, Variety, Veracity and Variability. The book explores granular computing as a response to the fact that learning tasks have become increasingly more complex due to the vast and rapid increase in the size of data, and that traditional machine learning has proven too shallow to adequately deal with big data. Some popular types of traditional machine learning are presented in terms of their key features and limitations in the context of big data. Further, the book discusses why granular-computing-based machine learning is called for, and demonstrates how granular computing concepts can be used in different ways to advance machine learning for big data processing. Several case studies involving big data are presented by using biomedical data and sentiment data, in order to show the advances in big data processing through the shift from traditional machine learning to granularcomputing-based machine learning. Finally, the book stresses the theoretical significance, practical importance, methodological impact and philosophical aspects of granular-computing-based machine learning, and suggests several further directions for advancing machine learning to fit the needs of modern industries. This book is aimed at PhD students, postdoctoral researchers and academics who are actively involved in fundamental research on machine learning or applied research on data mining and knowledge discovery, sentiment analysis, pattern recognition, image processing, computer vision and big data analytics. It will also benefit a broader audience of researchers and practitioners who are actively engaged in the research and development of intelligent systems. This book is a compilation of research work in the interdisciplinary areas of electronics, communication, and computing. This book is specifically targeted at students, research scholars and academicians. The book covers the different approaches and techniques for specific applications, such as particle-swarm optimization, Otsu’s function and harmony search optimization algorithm, triple gate silicon on insulator (SOI)MOSFET, micro-Raman and Fourier Transform Infrared Spectroscopy (FTIR) analysis, high-k dielectric gate oxide, spectrum sensing in cognitive radio, microstrip antenna, Ground-penetrating radar (GPR) with conducting surfaces, and digital image forgery detection. The contents of the book will be useful to academic and professional researchers alike. This volume contains the technical papers presented in the workshops associated with the European Conference on Service-Oriented and Cloud Computing, ESOCC 2016, held in Vienna, Austria, in September 2016: 4th International Workshop on Cloud for IoT, CLloT 2016, Second International Workshop on Cloud Adoption and Migration, CloudWays 2016, First International Workshop on Patterns and Pattern Languages for SOCC: Use and Discovery, PATTWORLD 2016, combined with the First International Workshop on Performance and Conformance of Workflow Engines, PEaCE 2016, IFIP WG SOS Workshop 2016 Rethinking Services ResearCH, ReSeRCH 2016. Furthermore, there is a topical section presenting the results of the PhD Symposium. The abstracts of the presentations held at the European Projects Forum, EU Projects 2016, are included in the back-matter of the volume. The 15 full papers included in this volume were carefully reviewed and selected from 49 submissions. They focus on specific topics in service-oriented and cloud computing domains such as limits and/or advantages of existing cloud solutions, future internet technologies, efficient and adaptive deployment and management of servicebased applications across multiple clouds, novel cloud service migration practices and solutions, digitization of enterprises in the cloud computing era, federated cloud networking services. Im Zeitalter des Internet of Things (IoT) erzeugen Edge-Geräte in jedem Sekundenbruchteil gigantische Datenmengen. Dabei besteht das Hauptziel dieser Netzwerke darin, aus den gesammelten Daten sinnvolle Informationen abzuleiten. Gleichzeitig werden gewaltige Datenmengen in die Cloud übertragen, was extrem teuer und zeitaufwändig ist. Es ist somit notwendig, effiziente Mechanismen für die Verarbeitung dieser gewaltigen Datenmengen zu entwickeln, und dafür sind effiziente Datenverarbeitungstechniken erforderlich. Nachhaltige Paradigmen wie Cloud Computing und Fog Computing tragen zu einem geschickten Umgang mit Themen wie Leistung, Speicherund Verarbeitungskapazitäten, Wartung, Sicherheit, Effizienz, Integration, Kosten, Energieverbrauch und Latenzzeiten bei. Allerdings werden ausgefeilte Analysetools benötigt, um die Anfragen in einer optimalen Zeit zu bearbeiten. Daher wird derzeit eifrig an der Entwicklung eines effektiven und effizienten Rahmens geforscht, um den größtmöglichen Nutzen zu erhalten. Bei der Verarbeitung der gewaltigen Datenmengen steht das maschinelle Lernen besonders hoch im Kurs und wird in zahlreichen Disziplinen angewandt, auch in den sozialen Medien. In Machine Learning Approach for Cloud Data Analytics in IoT werden sämtliche Aspekte des IoT, des Cloud Computing und der Datenanalyse ausführlich erläutert und aus verschiedenen Perspektiven betrachtet. Das Buch präsentiert den neuesten Stand der Forschung und fortschrittliche Themen. So erhalten die Leserinnen und Leser aktuelle Informationen und können das gesamte Spektrum der Anwendungen von IoT, Cloud Computing und Datenanalyse erfassen. International Conference on Bio-Inspired Computing: Theories and Applications (BIC-TA) is one of the flagship conferences on BioComputing, bringing together the world’s leading scientists from different areas of Natural Computing. Since 2006, the conferences have taken place at Wuhan (2006), Zhengzhou (2007), Adelaide (2008), Beijing (2009), Liverpool & Changsha (2010), Malaysia (2011) and India (2012). Following the successes of previous events, the 8th conference is organized and hosted by Anhui University of Science and Technology in China. This conference aims to provide a high-level international forum that researchers with different backgrounds and who are working in the related areas can use to present their latest results and exchange ideas. Additionally, the growing trend in Emergent Systems has resulted in the inclusion of two other closely related fields in the BIC-TA 2013 event, namely Complex Systems and Computational Neuroscience. These proceedings are intended for researchers in the fields of Membrane Computing, Evolutionary Computing and Genetic Algorithms, DNA and Molecular Computing, Biological Computing, Swarm Intelligence, Autonomy-Oriented Computing, Cellular and Molecular Automata, Complex Systems, etc. Professor Zhixiang Yin is the Dean of the School of Science, Anhui University of Science & Technology, China. Professor Linqiang Pan is the head of the research group of Natural Computing at Huazhong University of Science and Technology, Wuhan, China. Professor Xianwen Fang also works at the Anhui University of Science & Technology. Distributed systems intertwine with our everyday lives. The benefits and current shortcomings of the underpinning technologies are",,2021.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
68aff2731ad4ee1e83309d3b2f7ff240d7987208,https://www.semanticscholar.org/paper/68aff2731ad4ee1e83309d3b2f7ff240d7987208,Design Development And Fabrication Of Sugarcane Bud,"Design, Development and Fabrication of a Precision Autocollimating Solar Sensor /PASS/DESIGN, DEVELOPMENT, AND FABRICATION OF PROTOTYPE LOW-ENRICHED SUPERHEATER FUEL ELEMENTS (LESH).Design, Development and Fabrication of a Deployable/retractable Truss Beam Model for Large Space Structures ApplicationDesign, Development and Testing of Calspan/Chrysler Research Safety Vehicle Phase II. Volume I. Final Technical ReportDesign, Development, and Fabrication of a Sealed, Brushless Dc Motor Final ReportCompendium of Industrial Research, Design, and Development Facilities Available in Uttar PradeshDesign, Development and Fabrication of an Advanced High-Precision Robotic System for MicrosurgeryDesign, development, fabrication and testing of an operational prototype surge protectiveDesign, Development, and Fabrication of the FMU-26/B and FMU-26A/B Bomb FuzesDesign, Test, and Microfabrication of MEMS and MOEMSDesign and Development of RFID and RFID-Enabled Sensors on Flexible Low Cost SubstratesDesign for ManufacturingDesign and Fabrication of an Internally Insulated Filament Wound Liquid Hydrogen Propellant TankCalifornia. Court of Appeal (2nd Appellate District). Records and BriefsScientific and Technical Aerospace ReportsFurniture DesignDesign, Development and Fabrication of a Solar Experiment Alignment Sensor (SEAS)Axiomatic Design and Fabrication of Composite StructuresAdvanced Technology for Design and Fabrication of Composite Materials and StructuresRobotic Fabrication in Architecture, Art and Design 2014Proceedings of the U.S./U.S.S.R. Seminar on Problems of Design, Development, Fabrication and Test of Breeder Reactor ComponentsNuclear Science AbstractsRobotic Fabrication in Architecture, Art and Design 2018Research, Development, and Mechanization in the United States Post Office DepartmentDistributed Intelligence In DesignAeronautical Engineering ReviewDesign Methodologies for Space Transportation SystemsDesign, Development, and Fabrication of a Sealed, Brushless DC MotorDesign, Fabrication, Properties and Applications of Smart and Advanced MaterialsU.S. Government Research ReportsProcesses and Design for ManufacturingAEC Authorizing Legislation, Fiscal Year 1969Smart Material Systems and MEMSMaterials, Design and Manufacturing for Lightweight VehiclesDesign, Development and Fabrication of a New Generation Semiconductor X-ray DetectorNuclear Regulatory Legislation, 109th Congress, 2nd SessionDigital Design and FabricationDesign, Development, and Fabrication of a Electronic Analog Microminiaturized Electronic Analog Signal to Discrete Time Interval ConverterDesign, Development, Fabrication, and Testing of a Synchronous Condenser for a High-power Three-phase Traction DriveProduct Design for Manufacture and Assembly In response to tremendous growth and new technologies in the semiconductor industry, this volume is organized into five, information-rich sections. Digital Design and Fabrication surveys the latest advances in computer architecture and design as well as the technologies used to manufacture and test them. Featuring contributions from leading experts, the book also includes a new section on memory and storage in addition to a new chapter on nonvolatile memory technologies. Developing advanced concepts, this sharply focused book— Describes new technologies that have become driving factors for the electronic industry Includes new information on semiconductor memory circuits, whose development best illustrates the phenomenal progress encountered by the fabrication and technology sector Contains a section dedicated to issues related to system power consumption Describes reliability and testability of computer systems Pinpoints trends and state-of-theart advances in fabrication and CMOS technologies Describes performance evaluation measures, which are the bottom line from the user’s point of view Discusses design techniques used to create modern computer systems, including high-speed computer arithmetic and high-frequency design, timing and clocking, and PLL and DLL designDesign for Manufacturing assists anyone not familiar with various manufacturing processes in better visualizing and understanding the relationship between part design and the ease or difficulty of producing the part. Decisions made during the early conceptual stages of design have a great effect on subsequent stages. In fact, quite often more than 70% of the manufacturing cost of a product is determined at this conceptual stage, yet manufacturing is not involved. Through this book, designers will gain insight that will allow them to assess the impact of their proposed design on manufacturing difficulty. The vast majority of components found in commercial batch-manufactured products, such as appliances, computers and office automation equipment are either injection molded, stamped, die cast, or (occasionally) forged. This book emphasizes these particular, most commonly implemented processes. In addition to chapters on these processes, the book touches upon material process selection, general guidelines for determining whether several components should be combined into a single component or not, communications, the physical and mechanical properties of materials, tolerances, and inspection and quality control. In developing the DFM methods presented in this book, he has worked with over 30 firms specializing in injection molding, die-casting, forging and stamping. Implements a philosophy which allows for easier and more economic production of designs Educates designers about manufacturing Emphasizes the four major manufacturing processesPresenting unified coverage of the design and modeling of smart microand macrosystems, this book addresses fabrication issues and outlines the challenges faced by engineers working with smart sensors in a variety of applications. Part I deals with the fundamental concepts of a typical smart system and its constituent components. Preliminary fabrication and characterization concepts are introduced before design principles are discussed in detail. Part III presents a comprehensive account of the modeling of smart systems, smart sensors and actuators. Part IV builds upon the fundamental concepts to analyze fabrication techniques for silicon-based MEMS in more detail. Practicing engineers will benefit from the detailed assessment of applications in communications technology, aerospace, biomedical and mechanical engineering. The book provides an essential reference or textbook for graduates following a course in smart sensors, actuators and systems.This book introduces various advanced, smart materials and the strategies for the design and preparation for novel uses from macro to micro or from biological, inorganic, organic to composite materials. Selecting the best material is a challenging task, requiring tradeoffs between material properties and designing functional smart materials. The development of smart, advanced materials and their potential applications is a burgeoning area of research. Exciting breakthroughs are anticipated in the future from the concepts and results reported in this book.The work described in this document was performed in compliance with the scope of work as specified in Contract AF 08(635)-2850 tendered Honeywell Ordnance Division on 13 June 1962. All phases of a complete development program were carried out in order to achieve the goal of developing a safe, highly reliable fuze compatible with available subsonic and supersonic delivery systems. The final result of this development program was a multi-purpose fuze operable in three different modes: impact short-delay, impact medium-delay, and airburst. Fuzes were subjected to every environmental, functional, and safety test for development of fuzes required by the Air Force and by the contract. A program for the development of fuzes incorporating a retard-mode capability into the fuze was conducted, but the mode could not be included without extensive fuze redesign. Several recommendations were made by the contractor to expend additional efforts under the production program to effect the following: loading simplification, battery firing device simplification or integration, safing and arming mechanism simplification, and general safety improvements. (Author).The book presents research from Rob|Arch 2018, the fourth international conference on robotic fabrication in architecture, art, and design. In capturing the myriad of scientific advances in robotics fabrication that are currently underway – such as collaborative design tools, computerised materials, adaptive sensing and actuation, advanced construction, on-site and cooperative robotics, machine-learning, human-machine interaction, large-scale fabrication and networked workflows, to name but a few – this compendium reveals how robotic fabrication is becoming a driver of scientific innovation, cross-disciplinary fertilization and creative capacity of an unprecedented kind.pt.1: Considers S. 2880 and companion H.R. 14905, to authorize appropriations for AEC. Focuses on general budget and reactor development program; pt.2: Continuation of hearings on AEC FY69 authorization. Appendix includes reports. a. ""National Accelerator Laboratory, Design Report 1968, Universities Research Associates, "" prepared by AEC 1968 (p. 1223-1456). b. ""Report of Ad Hoc Panel on Low-Beta Toroidal Plasma Research, "" Sept. 1967 (p. 1459-1583). c. ""Bronco Oil Shale Study, "" prepared by AEC, Interior Dept, CER Geonuclear Corp., and Lawrence Radiation Laboratory, Oct. 13, 1967 (p. 1743-1813).Compiles statutues and materials relating to nuclear regulatory legislation through the 109th Congress, 1st Session.Hailed as a groundbreaking and important textbook upon its initial publication, the latest iteration of Product Design for Manufacture and Assembly does not rest on those laurels. In addition to the expected updating of data in all chapters, this third edition has been revised to provide a top-notch textbook for ",,2021.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
76e78bc520bdc550e762a5d82bd6aca84bd8c5bb,https://www.semanticscholar.org/paper/76e78bc520bdc550e762a5d82bd6aca84bd8c5bb,Data Driven Methods For Fault Detection And Diagnosis In Chemical Processes Advances In Industrial Control Epdf File,"Guaranteeing a high system performance over a wide operating range is an important issue surrounding the design of automatic control systems with successively increasing complexity. As a key technology in the search for a solution, advanced fault detection and identification (FDI) is receiving considerable attention. This book introduces basic model-based FDI schemes, advanced analysis and design algorithms, and mathematical and control-theoretic tools. This second edition of Model-Based Fault Diagnosis Techniques contains: • new material on fault isolation and identification and alarm management; • extended and revised treatment of systematic threshold determination for systems with both deterministic unknown inputs and stochastic noises; • addition of the continuously-stirred tank heater as a representative process-industrial benchmark; and • enhanced discussion of residual evaluation which now deals with stochastic processes. Model-based Fault Diagnosis Techniques will interest academic researchers working in fault identification and diagnosis and as a text it is suitable for graduate students in a formal university-based course or as a self-study aid for practising engineers working with automatic control or mechatronic systems from backgrounds as diverse as chemical process and power engineering. With pressure increasing to utilise wastes and residues effectively and sustainably, the production of biogas represents one of the most important routes towards reaching national and international renewable energy targets. The biogas handbook: Science, production and applications provides a comprehensive and systematic guide to the development and deployment of biogas supply chains and technology. Following a concise overview of biogas as an energy option, part one explores biomass resources and fundamental science and engineering of biogas production, including feedstock characterisation, storage and pre-treatment, and yield optimisation. Plant design, engineering, process optimisation and digestate utilisation are the focus of part two. Topics considered include the engineering and process control of biogas plants, methane emissions in biogas production, and biogas digestate quality, utilisation and land application. Finally, part three discusses international experience and best practice in biogas utilisation. Biogas cleaning and upgrading to biomethane, biomethane use as transport fuel and the generation of heat and power from biogas for stationery applications are all discussed. The book concludes with a review of market development and biomethane certification schemes. With its distinguished editors and international team of expert contributors, The biogas handbook: Science, production and applications is a practical reference to biogas technology for process engineers, manufacturers, industrial chemists and biochemists, scientists, researchers and academics working in this field. Provides a concise overview of biogas as an energy option Explores biomass resources for production Examines plant design and engineering and process optimisation This book provides a complete picture of several decision support tools for predictive maintenance. These include embedding early anomaly/fault detection, diagnosis and reasoning, remaining useful life prediction (fault prognostics), quality prediction and self-reaction, as well as optimization, control and self-healing techniques. It shows recent applications of these techniques within various types of industrial (production/utilities/equipment/plants/smart devices, etc.) systems addressing several challenges in Industry 4.0 and different tasks dealing with Big Data Streams, Internet of Things, specific infrastructures and tools, high system dynamics and non-stationary environments . Applications discussed include production and manufacturing systems, renewable energy production and management, maritime systems, power plants and turbines, conditioning systems, compressor valves, induction motors, flight simulators, railway infrastructures, mobile robots, cyber security and Internet of Things. The contributors go beyond state of the art by placing a specific focus on dynamic systems, where it is of utmost importance to update system and maintenance models on the fly to maintain their predictive power. In many industrial applications early detection and diagnosis of abnormal behavior of the plant is of great importance. During the last decades, the complexity of process plants has been drastically increased, which imposes great challenges in development of model-based monitoring approaches and it sometimes becomes unrealistic for modern largescale processes. The main objective of Adel Haghani Abandan Sari is to study efficient fault diagnosis techniques for complex industrial systems using process historical data and considering the nonlinear behavior of the process. To this end, different methods are presented to solve the fault diagnosis problem based on the overall behavior of the process and its dynamics. Moreover, a novel technique is proposed for fault isolation and determination of the root-cause of the faults in the system, based on the fault impacts on the process measurements. Reliability Analysis and Asset Management of Engineering Systems explains methods that can be used to evaluate reliability and availability of complex systems, including simulation-based methods. The increasing digitization of mechanical processes driven by Industry 4.0 increases the interaction between machines and monitoring and control systems, leading to increases in system complexity. For those systems the reliability and availability analyses are increasingly challenging, as the interaction between machines has become more complex, and the analysis of the flexibility of the production systems to respond to machinery failure may require advanced simulation techniques. This book fills a gap on how to deal with such complex systems by linking the concepts of systems reliability and asset management, and then making these solutions more accessible to industry by explaining the availability analysis of complex systems based on simulation methods that emphasise Petri nets. Explains how to use a monitoring database to perform important tasks including an update of complex systems reliability Shows how to diagnose probable machinery-based causes of system performance degradation by using a monitoring database and reliability estimates in an integrated way Describes practical techniques for the application of AI and machine learning methods to fault detection and diagnosis problems",,2021.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
1f2f6216100b3abf664a1f1385ad957e3f46efd7,https://www.semanticscholar.org/paper/1f2f6216100b3abf664a1f1385ad957e3f46efd7,Top 10 Read Article in Computer Science & Information Technology: June 2021,"Clouds provide a powerful computing platform that enables individuals and organizations to perform variety levels of tasks such as: use of online storage space, adoption of business applications, development of customized computer software, and creation of a “realistic” network environment. In previous years, the number of people using cloud services has dramatically increased and lots of data has been stored in cloud computing environments. In the meantime, data breaches to cloud services are also increasing every year due to hackers who are always trying to exploit the security vulnerabilities of the architecture of cloud. In this paper, three cloud service models were compared; cloud security risks and threats were investigated based on the nature of the cloud service models. Real world cloud attacks were included to demonstrate the techniques that hackers used against cloud computing systems. In addition,countermeasures to cloud security breaches are presented. ABSTRACT Big Data triggered furthered an influx of research and prospective on concepts and processes pertaining previously to the Data Warehouse field. Some conclude that Data Warehouse as such will disappear; others present Big Data as the natural Data Warehouse evolution (perhaps without identifying a clear division between the two); and finally, some others pose a future of convergence, partially exploring the possible integration of both. In this paper, we revise the underlying technological features of Big Data and Data Warehouse, highlighting their differences and areas of convergence. Even when some differences exist, both technologies could (and should) be integrated because they both aim at the same purpose: data exploration and decision making support. We explore some convergence strategies, based on the common elements in both technologies. We present a revision of the state-of-the-art in integration proposals from the point of view of the purpose, methodology, architecture and underlying technology, highlighting the common elements that support both technologies that may serve as a starting point for full integration and we propose a proposal of integration between the two technologies. ABSTRACT By applying RapidMiner workflows has been processed a dataset originated from different data files, and containing information about the sales over three years of a large chain of retail stores. Subsequently, has been constructed a Deep Learning model performing a predictive algorithm suitable for sales forecasting. This model is based on artificial neural network –ANN- algorithm able to learn the model starting from sales historical data and by pre-processing the data. The best built model uses a multilayer neural network together with an “optimized operator” able to find automatically the best parameter setting of the implemented algorithm. In order to prove the best performing predictive model, other machine learning algorithms have been tested. The performance comparison has been performed between Support Vector Machine –SVM-, k-Nearest Neighbor k-NN-,Gradient Boosted Trees, Decision Trees, and Deep Learning algorithms. The comparison of the degree of correlation between real and predicted values, the average absolute error and the relative average error proved that ANN exhibited the best performance. The Gradient Boosted Trees approach represents an alternative approach having the second best performance. The case of study has been developed within the framework of an industry project oriented on the integration of high performance data mining models able to predict sales using–ERP- and customer relationship management –CRM- tools. ABSTRACT Wireless network implementation is a viable option for building network infrastructure in rural communities. Rural people lack network infrastructures for information services and socio-economic development. The aim of this study was to develop a wireless network infrastructure architecture for network services to rural dwellers. A user-centered approach was applied in the study and a wireless network infrastructure was designed and deployed to cover five rural locations. Data was collected and analyzed to assess the performance of the network facilities. The results shows that the system had been performing adequately without any downtime with an average of 200 users per month and the quality of service has remained high. The transmit/receive rate of 300Mbps was thrice as fast as the normal Ethernet transmit/receive specification with an average throughput of 1 Mbps. The multiple output/multiple input(MIMO) point-to-multipoint network design increased the network throughput and the quality of serviceexperienced by the users ABSTRACT Although initially designed for co-located teams, agile methodologies promise mitigation to the challenges present in distributed software development with their demand for frequent communication. We examinethe application of agile practices in software engineering teams with low geographical distribution in Austria and Germany. To gather insights on challenges and benefits faced by distributed teams we conductinterviews with eleven representatives and analyse the interview transcripts using the inductive category formation method. As a result, we identify four major challenges, such as technical obstructions or theimpediments different language abilities have on communication, and four benefits, regardingcollaboration and information radiation, that agile methods yield in distributed teams. Based on ouranalysis of challenges and benefits, we deduct seven recommendations to improve collaboration, overcomedistance and avoid pitfalls. Key recommendations for teams with low geographical distance include thatteams should get together at certain points to build relationships and trust and share information face-to- face ABSTRACT The world is moving forward at a fast pace, and the credit goes to ever growing technology. One such concept is IOT (Internet of things) with which automation is no longer a virtual reality. IOT connects various non-living objects through the internet and enables them to share information with their community network to automate processes for humans and makes their lives easier. The paper presents the future challenges of IoT , such as the technical (connectivity , compatibility and longevity , standards , intelligent analysis and actions , security), business ( investment , modest revenue model etc. ), societal (changing demands , new devices, expense, customer confidence etc. ) and legal challenges ( laws, regulations, procedures, policies etc. ). A section also discusses the various myths that might hamper the progress of IOT, security of data being the most critical factor of all. An optimistic approach to people in adopting the unfolding changes brought by IOT will also help in its growth ABSTRACT Mobile payment allows consumers to make more flexible payments through convenient mobile devices. While mobile payment is easy and time save, the operation and security of mobile payment must ensure that the payment is fast, convenient, reliable and safety in order to increase the users’ satisfaction. Therefore, this study based on technology acceptance model to explore the impact of external variables through perceived usefulness and perceived ease of use on users’ satisfaction. The data analysis methods used in this study are descriptive statistical analysis, reliability and validity analysis, Pearson correlation analysis and regression analysis to verify the hypotheses. The results show that all hypotheses are supported. However, mobile payment is still subject to many restrictions on development and there are limited related researches. The results of this study provided insight into the factors that affect the users’ satisfaction for mobile payment. Related services development of mobile payment and future research suggestions are also offered. ABSTRACT Big Data is used in decision making process to gain useful insights hidden in the data for business and engineering. At the same time it presents challenges in processing, cloud computing has helped in advancement of big data by providing computational, networking and storage capacity. This paper presents the review, opportunities and challenges of transforming big data using cloud computing resources. ABSTRACT In this paper, we propose a new traffic flow model of the Long Term Evaluation (LTE) network for the Evolved Universal Terrestrial Radio Access Network (E-UTRAN). Here only one Evolve Node B (eNB)nearest to the Mobility Management Entity (MME) and Serving Gateway (S-GW) will use the S1 link tobridge the E-UTRAN and Evolved Packet Core (EPC). All the eNBs of a tracking area will be connected toeach other by the X2 link. Determination of capacity of a links of such a network is a challenging job sinceeach node offers its own traffic and at the same time conveys traffic of other nodes. In this paper, we applymaximum flow algorithm including superposition theorem to solve the traffic flow of radio network. Usingthe total flow per subcarrier, a new traffic model is also developed in the paper. The relation among the traffic parameters: ‘blocking probability’, ‘offered traffic’, ‘instantaneous capacity’, ‘average holdingtime’, and ‘number of users’ are shown graphically under both QPSK and 16 -QAM. The concept of thenetwork will be helpful to improve the SINR of the received signal ofeNBslocated long distance relative to MME/S-GW. ABSTRACT The huge amount of healthcare data, coupled with the need for data analysis tools has made data mining interesting research areas. Data mining tools and techniques help to discover and understand hidden patterns in a dataset which may not be possible by mainly visualization of the data. Selecting appropriate clustering method and optimal number of clusters in healthcare data can be confusing and difficult most times. Presently, a large number of clustering algorithm",,2021.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
75994ebb52094581dcb7d145795f6bafe6e276bb,https://www.semanticscholar.org/paper/75994ebb52094581dcb7d145795f6bafe6e276bb,Influence of artificial intelligence (AI) on firm performance: the business value of AI-based transformation projects,"The main purpose of our study is to analyze the influence of Artificial Intelligence (AI) on firm performance, notably by building on the business value of AI-based transformation projects. This study was conducted using a four-step sequential approach: (1) analysis of AI and AI concepts/technologies; (2) in-depth exploration of case studies from a great number of industrial sectors; (3) data collection from the databases (websites) of AI-based solution providers; and (4) a review of AI literature to identify their impact on the performance of organizations while highlighting the business value of AI-enabled projects transformation within organizations.,This study has called on the theory of IT capabilities to seize the influence of AI business value on firm performance (at the organizational and process levels). The research process (responding to the research question, making discussions, interpretations and comparisons, and formulating recommendations) was based on a review of 500 case studies from IBM, AWS, Cloudera, Nvidia, Conversica, Universal Robots websites, etc. Studying the influence of AI on the performance of organizations, and more specifically, of the business value of such organizations’ AI-enabled transformation projects, required us to make an archival data analysis following the three steps, namely the conceptual phase, the refinement and development phase, and the assessment phase.,AI covers a wide range of technologies, including machine translation, chatbots and self-learning algorithms, all of which can allow individuals to better understand their environment and act accordingly. Organizations have been adopting AI technological innovations with a view to adapting to or disrupting their ecosystem while developing and optimizing their strategic and competitive advantages. AI fully expresses its potential through its ability to optimize existing processes and improve automation, information and transformation effects, but also to detect, predict and interact with humans. Thus, the results of our study have highlighted such AI benefits in organizations, and more specifically, its ability to improve on performance at both the organizational (financial, marketing and administrative) and process levels. By building on these AI attributes, organizations can, therefore, enhance the business value of their transformed projects. The same results also showed that organizations achieve performance through AI capabilities only when they use their features/technologies to reconfigure their processes.,AI obviously influences the way businesses are done today. Therefore, practitioners and researchers need to consider AI as a valuable support or even a pilot for a new business model. For the purpose of our study, we adopted a research framework geared toward a more inclusive and comprehensive approach so as to better account for the intangible benefits of AI within organizations. In terms of interest, this study nurtures a scientific interest, which aims at proposing a model for analyzing the influence of AI on the performance of organizations, and at the same time, filling the associated gap in the literature. As for the managerial interest, our study aims to provide managers with elements to be reconfigured or added in order to take advantage of the full benefits of AI, and therefore improve organizations’ performance, the profitability of their investments in AI transformation projects, and some competitive advantage. This study also allows managers to consider AI not as a single technology but as a set/combination of several different configurations of IT in the various company’s business areas because multiple key elements must be brought together to ensure the success of AI: data, talent mix, domain knowledge, key decisions, external partnerships and scalable infrastructure.,This article analyses case studies on the reuse of secondary data from AI deployment reports in organizations. The transformation of projects based on the use of AI focuses mainly on business process innovations and indirectly on those occurring at the organizational level. Thus, 500 case studies are being examined to provide significant and tangible evidence about the business value of AI-based projects and the impact of AI on firm performance. More specifically, this article, through these case studies, exposes the influence of AI at both the organizational and process performance levels, while considering it not as a single technology but as a set/combination of the several different configurations of IT in various industries.",Bus. Process. Manag. J.,2020.0,10.1108/bpmj-10-2019-0411,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
0da04ad7ee5792897b455e6131478c9f2339ecaf,https://www.semanticscholar.org/paper/0da04ad7ee5792897b455e6131478c9f2339ecaf,Program verification,"I N 1 969, TON Y HOARE published a classical Communications’ article, “An Axiomatic Basis for Computer Programming.” Hoare’s article culminated a sequence of works by Turing, McCarthy, Wirth, Floyd, and Manna, whose essence is an association of a proposition with each point in the program control flow, where the proposition is asserted to hold whenever that point is reach. Hoare added two important elements to that approach. First, he described a formal logic, now called Hoare Logic, for reasoning about programs. Second, he offered a compelling vision for the program-verification project: “When the correctness of a program, its compiler, and the hardware of the computer have all been established with mathematical certainty, it will be possible to place great reliance on the results of the program, and predict their properties with a confidence limited only by the reliability of the electronics.” Hoare’s vision came under a scathing attack a decade later in an influential 1979 Communications’ article, “Social Processes and Proofs of Theorems and Programs,” by De Millo, Lipton, and Perlis. They argued that mathematical proofs are accepted through a social process. Program-correctness proofs will not be subject to a similar social process, due to their length and narrowness, so they will not be socially accepted. They concluded that “this makes the formal verification process difficult to justify and manage.” Hoare himself retracted, to some extent, his 1969 vision in 1995, writing “It has turned out that the world just does not suffer significantly from the kind of problems that our research was originally intended to solve.” In a parallel development, Amir Pnueli introduced the temporal logic of programs in 1977. Clarke and Emerson, and independently, Queille and Sifakis, then built on Pnueli’s work and developed, in the early 1980s, model checking, an algorithmic technique for checking properties of finite-state programs. That led to Pnueli receiving the ACM A.M. Turing Award in 1966, and Clarke-Emerson-Sifakis receiving the award in 2007. By the mid-1990s, several model checkers had been built and adopted for industrial usage by semiconductor and design-automation companies. Industrial temporal logics, such as PSL and SVA, based on Pnueli’s work, became industry standards in the early 2000s. The success of model checking in the semiconductor industry, where postproduction error correction is very difficult, points to an important insight that was missing in the early literature on program verification. Program verification is an expensive activity. Navigating the cost-benefit trade-off of program verification is ultimately a business decision. Model checking offered a different price point than full program verification: on one hand, model checking offers less— property checking and not full program verification, on the other hand, model checking costs less, due to higher level of automation. This cost-benefit trade-off suggests that how much verification should be done is context dependent. An operationsystem microkernel is probably a more appropriate target for a major verification effort than a dating app. Such an undertaking was initiated by an Australian team, who verified the seL4 microkernel using the Isabelle proof-assistant tool. Isabelle is based on a small logical core to increase the trustworthiness of proofs. This approach acknowledges that De Millo et al. were right—proofs do require a social process to be accepted—but in Isabelle (and similar tools) this social process can be confined to the small logic core. In fact, with the help of proof assistants, formal verification today is even bringing a new standard for rigor in mathematics. The emergence of cloud computing as the major context for much of today’s computing shifts the cost-benefit tradeoff of verification, due to its large scale. Because different users of the same cloud platform share hardware resources, security and privacy are of paramount interest. The Automated-Reasoning Group at Amazon Web Service (AWS) has been focusing on the development and use of formal-verification tools at AWS to increase the security assurance of its cloud infrastructure and to help customers secure themselves. At the same time, as the Spectre and Meltdown attacks have demonstrated, the large gap between the logical model (ISA) and the underlying microarchitecture of the X86 microprocessor not only provides side channels to attackers but also erects a major barrier to full verification. In 1969, Hoare wrote about mathematical certainty, great reliance, and confidence. In retrospect, the hope for “mathematical certainty” was idealized, and not fully realistic, I believe. Verification can give us great reliance and confidence, but at a cost that must be justified by the benefits. The deployment of autonomous systems with machine learning-based components brings new urgency and excitement to this important research area. Follow me on Facebook and Twitter.",Commun. ACM,2021.0,10.1145/3469113,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
75b21f9ef074a7385ef3837f63fcb0206006eb10,https://www.semanticscholar.org/paper/75b21f9ef074a7385ef3837f63fcb0206006eb10,COGNITIVE RISKS,"ABSTRACT Decision science have begun to enter the lexicon of risk professionals as the concepts from Prospect Theory become popular in media outlets who increasingly warn about the risk of human biases. Decision-making under uncertainty, popularized by Dan Kahneman, Amos Tversky, Paul Slovic, Herbert Simon and other economists, is more than an examination of human biases. Prospect Theory is a reexamination of the theory of choice and the causes of violations of utility theory that has blossomed into a broad and diverse body of research in behavioral and cognitive science. This paper is an outline for a proposed draft of a cognitive risk framework that will be developed to incorporate behavioral and cognitive science into an enterprise risk framework for cybersecurity and enterprise risk governance. Herbert Simon coined the term “Bounded Rationality” in his seminal book of the same name. “Broadly stated, the task is to replace the global rationality of economic man with the kind of rational behavior that is compatible with the access to information and the computational capacities that are actually possessed by organisms, including man, in the kinds of environments in which such organisms exist” (Simon 1955a: 99). Before the development of modern of PCs and even more powerful machine learning algorithms, Simon foresaw the opportunity at the intersection of human decision-making and technology. Since Simon, other economists and researchers have broaden insights from a multidisciplinary offering of academic studies into applied behavioral science. Notwithstanding these advances, only a few scientists have developed decision science solutions at scale at the enterprise level. Machine learning and other forms of artificial intelligence will require new rules of engagement and governance controls to ensure that bias and ethical use standards have been put in place. Data, the newest commodity in all digital strategies, must be better organized and structured in organizations to allow for efficacious information workflows needed to power organizations to higher performance. And lastly, the role of humans working with and alongside machines as decision-support tools are in the early stage of deployment. The research for the book, Cognitive Risks, will examine the last frontier in risk management – the role of human actors in a business environment that is transitioning to digital products and services. A new level of awareness is needed in a digital environment that differs from the physical world. We know this because of the advent of misinformation that now permeates the Internet. Nation states and Dark Web criminals have weaponized trust in the Internet through misinformation campaigns in social media sites by using behavioral science, or more specifically, cognitive hacks to change our behavior when surfing the web. These attacks are low cost and very effective because most observers are not aware of cognitive risks. There are many variations of “cognitive hacks” and “cognitive risks” which will be explained in detail in the book. Dimitry Kiselev, director general of Russia’s state-controlled Rossiyua Segodnya media conglomerate, “Objectivity is a myth which is proposed and imposed on us.” Today, thanks to the Internet and social media, the manipulation of our perception of the world is taking place on previously unimaginable scales of time, space and intentionality. Cognitive hacks and cognitive risks are part of a new lexicon of risks we must learn. Cognitive risks are commonly referred to as heuristic behavior. Heuristics is any approach to problem solving or self-discovery that employs a practical method that is not guaranteed to be optimal, perfect, or rational, but is nevertheless sufficient for reaching an immediate, short-term goal or approximation. Where finding an optimal solution is impossible or impractical, heuristic methods can be used to speed up the process of finding a satisfactory solution. Heuristics can be mental shortcuts that ease the cognitive load of making a decision. Large swaths of the economy have already misjudged the potential, and the threats, of digital transformation. The questions explored in this paper and the subsequent book, Cognitive Risks, that will follow is why? Why do some leaders see opportunity when others only see problems? Why has the retail industry been blindsided by firms like Amazon, Google, Apple, and so many others? The research for the book will also include an exhaustive review of how applied behavioral science can be used to enhance organizational performance, risk management and cybersecurity in all organizations. Few, if any studies to date, have combined a multidisciplinary approach to enterprise risk management and organizational performance. This will be the first study that builds on a 2020 study of advancements in enterprise risk and board governance to provide a comprehensive analysis of methods and processes to apply behavioral science to address a range of risks facing organizations as they transition to a digital economy.",EDPACS,2021.0,10.1080/07366981.2020.1840020,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
924420707edd1e75b735b577b7dc4ff2a2cf1932,https://www.semanticscholar.org/paper/924420707edd1e75b735b577b7dc4ff2a2cf1932,Cybertrust: From Explainable to Actionable and Interpretable AI (AI2),"To benefit from AI advances, users and operators of AI systems must have reason to trust it. Trust arises from multiple interactions, where predictable and desirable behavior is reinforced over time. Providing the system’s users with some understanding of AI operations can support predictability, but forcing AI to explain itself risks constraining AI capabilities to only those reconcilable with human cognition. We argue that AI systems should be designed with features that build trust by bringing decision-analytic perspectives and formal tools into AI. Instead of trying to achieve explainable AI, we should develop interpretable and actionable AI. Actionable and Interpretable AI (AI2) will incorporate explicit quantifications and visualizations of user confidence in AI recommendations. In doing so, it will allow examining and testing of AI system predictions to establish a basis for trust in the systems’ decision making and ensure broad benefits from deploying and advancing its computational capabilities. Decision Making: Humans and Artificial Intelligence (AI) “Can I trust the recommendation of an AI agent?” This question is difficult to answer, especially if the decision at stake is complex and spans different spatial and temporal scales. Such difficulty is exacerbated when the outcomes of an AI-influenced decision may heighten existing risks to humans or introduce new risks altogether. Yet such high-stakes situations have become routine within the diverse systems that currently incorporate AI, like controls for chemical plants, defense systems, and health insurance rate determinations. Stakeholders must be prepared not only to configure AI and its enabling technologies for a given industry or activity, but also to have tools and methodologies to examine and recognize its failures, limitations, and needs for quality control at various stages of its development and implementation. The ultimate goal of AI is to provide users with actionable recommendations that meet both implicit and explicit goals of the decision makers and stakeholders. Recommendations generated from AI-based This is a preprint version of the paper by Igor Linkov, Stephanie Galaitsi, Benjamin D. Trump, Jeffrey M. Keisler, and Alexander Kott, ""Cybertrust: From Explainable to Actionable and Interpretable Artificial Intelligence,"" IEEE Computer, Sept. 2020, pp. 91-96, vol. 53 DOI Bookmark: 10.1109/MC.2020.2993623 ................................. approaches hold advantages over human decision makers through their ability to analyze vast bodies of information in limited time in an objective and logic-centered fashion. In many situations, these benefits are clear and already implemented in practice, such as machine learning systems for the detection of phishing attempts [Khonji et al. 2013]. AI applications are also capable of providing multistep and adaptable strategies, as demonstrated by programs that compete in chess or Go, as well as AI-based cybersecurity systems [Al-Shaer et al. 2019; Kott et al. 2019] However, AI recommendations may not account for decision maker values or specific mission needs. For example, following a cyberattack, an AI-generated decision engine may recommend disabling an application on the compromised computer system. Such an action may neutralize the threat posed by the compromised system, but could simultaneous endanger a mission, negatively impact a critical user, or enable the adversary to extend the duration or scope of the cyberattack. The broader scale impacts of the recommended path forward may not have been incorporated into the AI’s design or scope, causing the AI decision processes to omit critical conditions that a human operator would implicitly account for. Such incomplete scoping of AI-driven analysis is especially problematic when unspoken, unacknowledged, or subjective variables influence or shape what a successful outcome looks like. For example, an AI system may not account for the need for a particular asset to be available to achieve a mission later on, or for psychological impact on the system’s users. The AI solves the problem it is given, but it the human’s role to ensure the recommendation’s suitability in context. Similarly, the human users making this judgment will benefit from understanding the factors that produced the AI’s decision, especially when that understanding helps the users see the value of factors they themselves could have overlooked. While AI-driven analysis enhances our decision making ability, providing insight into AI’s shifts in its analysis of needs, expectations, and mission requirements will ensure its decisions’ relevance and credibility and make its expectations for the future explicit. If AI’s analytical outputs do not account for these and other broader and potentially subjective concerns, an overly myopic focus upon a tactical decision can derail strategic mission requirements. As such, a more effective deployment of AI into decision making must resolve the ‘black box’ concerns of AI – in that it is unclear how to explain, interpret, or act upon AI’s conclusions as its underlying algorithm and parameters are difficult to decipher. Inevitable Disagreements: The Challenge of Fusing Human and AI Decision Making Capabilities We can expect AI recommendations to differ, in some percentage of circumstances, from the choice the operator alone would have made. There are three possibilities on how this plays out for any yes/no decision: the AI is more risk-averse than the human, the AI is more risk-tolerant than the human, or the AI and the human agree (Table 1).",ArXiv,2022.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
cc40cc3149c98212dcac10b897a79e997116f1c0,https://www.semanticscholar.org/paper/cc40cc3149c98212dcac10b897a79e997116f1c0,Voice Assistant based Movie Recommendation System utilizing User Filtered Correlation Factor,"Abstract: The project deals with the idea of a Movie Recommendation System, which itself is one of the key tools being deployed across various OTT and Social Media platforms that serve the purpose of engaging a customer of a particular service to stick to their platform service by consuming more relevant content. Primarily the crucial factor to boost the momentum of the Recommendation Systems date back to the emergence of Search Engines (as a part of everyday lives). These search engines make use of multiple algorithms to page and derive the useful results by indexing every page of a hosted/ live site. Due to these technological advancements, there has been a burgeoning of data, which is being generated at every mill-second from numerous streams and multitudes across various fields. While the utility of platforms - like the afore mentioned - give rise to Historical Data, where the behavioural traits of multiple users give scope to Clustering upon a set of distinct characteristics. One such method to suggest (or) recommend movies to online users is to utilize the already available historical data. Within its entirety the Recommendation Systems have become one of the most prominent tools within the Machine Learning framework, serving various industries and sectors. Our project uses the already available Historical Data downloaded from Kaggle and develops the Voice Assisted Approach to feed input to our Movie Recommendation System. Keywords: Movie Recommendation System, User-based Collaborative Filtering, Voice Assistant, Pearson’s Correlation, Historical Data, Speech Recognition, Tkinter, Data Visualization",International Journal for Research in Applied Science and Engineering Technology,2021.0,10.22214/ijraset.2021.38591,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
42c9527670182855ec693b469e58925e84998e29,https://www.semanticscholar.org/paper/42c9527670182855ec693b469e58925e84998e29,"""How Robotics are Revolutionizing Rehabilitation""","Capitalizing on the new understanding of brain plasticity, we introduced a paradigm shift in clinical practice in 1989 when we initiated the development of the MIT-Manus robot for neuro-rehabilitation and deployed it into the clinic. Since then we collected evidence to support the potential of enhancing and augmenting recovery following a stroke, first during the sub-acute and then the chronic phase. Our efforts and that of others led to the endorsements starting in 2010 from the American Heart Association, the American Stroke Association, and the Veterans Administration for the use of rehabilitation robots for the Upper Extremity, but not yet for the Lower Extremity. AHA recommendations were the same in the 2016 revision. Furthermore, it was demonstrated in the VA system that upper extremity robotic therapy has an economic advantage over manual therapy. More recently we completed a pragmatic study RATULS under the auspices of the National Health Service of the United Kingdom and its NIHR Health Technology Assessment Programme, which enrolled 770 stroke patients. Thus, we have developed novel robotic treatment and evaluation tools and have managed to collect the experimental evidence that demonstrates the unequivocal therapeutic benefits stemming from robot-aided rehabilitation for the upper extremity as well as present shortcomings. This talk will present an overview of our past rehabilitation robotics efforts and more recent efforts addressing the identified shortcomings. ""Novel Biomarkers: Robotics and Machine Learning "" Hermano Igo Krebs, PhD Abstract: In stroke, we demonstrated that robotic devices promoted upper extremity motor recovery. Those studies raised new questions focused on patients who were mildly or completely resistant to therapy, i.e., patients who did not improve, and prompted the hypothesis In stroke, we demonstrated that robotic devices promoted upper extremity motor recovery. Those studies raised new questions focused on patients who were mildly or completely resistant to therapy, i.e., patients who did not improve, and prompted the hypothesis that we could predict who are the responders, quasi-responders, and non-responders to behavioral therapy. There have been other attempts to create biomarkers to predict outcomes employing clinical scales such as the Fugl-Meyer assessment, the neurologic sensory exam, functional impairment scales, neurophysiology and neuro-imaging analysis; but these attempts have had mixed results and these measures are seldom used in practice to optimize therapy. To understand the variability of recovery, we examined the data collected with the robotic group on a recently completed studies. We investigated the potential for building a more sensitive biomarker, composed of robotic measurements collected during evaluation and training, to analyze the performance of patients recovering from stroke and to predict who will respond to movement-based treatment and who will not. We hypothesize that kinematic and kinetic measurements can predict the response to behavioral therapy in stroke and also determine how to optimize care for a particular patient. Here we will discuss our attempts to employ both linear and non-linear approaches and ascertain the correlation levels between our robot-based biomarker and clinical scales. We will discuss robot biomarker effect-size and compare it to clinical scales to determine whether there are some noticeable efficiencies in using the robotassay instead of the clinical scales. We will also present our efforts in developing an expert algorithm that employs data collected during the baseline assessment and during two consecutive training sessions in order to predict patient outcomes as well as to determine patterns of improvement in stroke patients so as to build an alternative machine learning predictor of outcomes. ""Starting a Venture Company"" Hermano Igo Krebs, PhD Abstract: “Imagine being present at the birth of a new industry. . . trends are now starting to converge and I can envision a future in which robotics devices will become a nearly ubiquitous part of our day-to-day lives. Technologies such as distributed computing, voice and visual recognition, and wireless broadband connectively will open the door to a new generation of autonomous devices that enable computers to perform tasks in the physical world on our behalf. We may be on the verge of a new era, when the PC will get up off the desktop and allow us to see, hear, touch and manipulate objects in places where we are not physically present.” Bill Gates Disruptive technology is a term coined to characterize an innovation that disrupts an existing market or way of doing things and creates a new value network. The concept was first described at Harvard Business School by Clayton M. Christensen, who described the concept in 1996 as: ""Generally, disruptive innovations were technologically straightforward, consisting of off-theshelf components put together in a product architecture that was often simpler than prior approaches. They offered less of what customers in established markets wanted and so could rarely be initially employed there. They offered a different package of attributes valued only in emerging markets remote from, and unimportant to, the mainstream."" Eventually with improvement, borrowing from Malcolm Gladwell, the moment of critical mass, the threshold, the boiling point is reached and the old practices and existing value network is abandoned in favor of the new one. Here I will discuss my experience as an entrepreneur and whether rehabilitation robotics has achieved its “tipping point.” “Imagine being present at the birth of a new industry. . . trends are now starting to converge and I can envision a future in which robotics devices will become a nearly ubiquitous part of our day-to-day lives. Technologies such as distributed computing, voice and visual recognition, and wireless broadband connectively will open the door to a new generation of autonomous devices that enable computers to perform tasks in the physical world on our behalf. We may be on the verge of a new era, when the PC will get up off the desktop and allow us to see, hear, touch and manipulate objects in places where we are not physically present.” Bill Gates Disruptive technology is a term coined to characterize an innovation that disrupts an existing market or way of doing things and creates a new value network. The concept was first described at Harvard Business School by Clayton M. Christensen, who described the concept in 1996 as: ""Generally, disruptive innovations were technologically straightforward, consisting of off-theshelf components put together in a product architecture that was often simpler than prior approaches. They offered less of what customers in established markets wanted and so could rarely be initially employed there. They offered a different package of attributes valued only in emerging markets remote from, and unimportant to, the mainstream."" Eventually with improvement, borrowing from Malcolm Gladwell, the moment of critical mass, the threshold, the boiling point is reached and the old practices and existing value network is abandoned in favor of the new one. Here I will discuss my experience as an entrepreneur and whether rehabilitation robotics has achieved its “tipping point.”",,2021.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
497c20a6d7ab043d2a07a50e19e27424c92fb003,https://www.semanticscholar.org/paper/497c20a6d7ab043d2a07a50e19e27424c92fb003,The PCOR Partnership Initiative: The State of the Region,"The PCOR Initiative, a 5-year project beginning in early 2020, is one of four newly selected regional initiative awards funded by the U.S. Department of Energy (DOE). The PCOR Initiative will accelerate the deployment of carbon capture, utilization, and storage (CCUS) in the central and northwestern region of North America, comprising ten U.S. states and four Canadian provinces. With extensive fossil fuel resources, large-scale anthropogenic CO2 sources, and geologic storage options, this region provides all of the essential elements necessary for infrastructure development and widespread CCUS deployment. The project builds on 16 years of applied research undertaken by the Plains CO2 Reduction (PCOR) Partnership throughout the northern Great Plains, including the successful assessment and monitoring of over 6 million metric tons of associated CO2 storage, incidental to enhanced oil recovery (EOR) operations at the Bell Creek oil field in southeastrn Montana. 
 
Managed by the Energy & Environmental Research Center (EERC) in Grand Forks, North Dakota, the PCOR Initiative is supported by the North Dakota Industrial Commission, through the Oil and Gas and Lignite Research Programs, and by research partners at the University of Alaska at Fairbanks and the University of Wyoming. Many of the 120+ previous PCOR Partnership member organizations are supporting this new initiative, with additional members also expected. Our team also includes the partners associated with Project Tundra, which constitutes the largest planned capture of CO2 in the world from a power generation facility. With many partners actively engaged in commercial projects, the PCOR Initiative draws together a powerful team to advance and accelerate CCUS deployment in the region and satisfy the objectives put forth by DOE. 
 
The PCOR Initiative will identify and address onshore regional storage and transport challenges facing commercial deployment of CCUS within its region. To achieve this, the PCOR Initiative will 1) address key technical challenges by advancing critical knowledge and capabilities; 2) facilitate data collection, sharing, analysis, and collaboration; 
3) evaluate regional infrastructure challenges and needs; and 4) promote regional technology transfer. 
 
The PCOR Initiative will engage federal and state regulators in the region to support the continued development and knowledge transfer of regulatory policies to accelerate the deployment of CCUS. The regulatory efforts will focus on understanding the federal and state permitting processes and timing and address major regulatory topics, including pore space ownership, practical approaches to defining the area of review, and management of the long-term liability associated with a closed storage site. The PCOR Initiative will facilitate dialogue regarding the status of CCUS projects and regulatory challenges, with an emphasis on knowledge transfer between states with active CCUS projects and Class VI primacy and states with less CCUS development and no CCUS regulations established. North Dakota is the only state currently with Class VI primacy, and Wyoming has a pending Class VI primacy application. These states can provide valuable insight to other states by sharing their learnings from the Class VI primacy application process and Class VI regulatory program implementation at the state level. 
 
The expected outcomes of the PCOR Initiative include: 
• Solving technical challenges through collaboration, and addressing issues including stacked storage; storage optimization; effective monitoring, verification, and accounting strategies; and risk assessment resulting in increased regulator and industry confidence. 
• Advancing knowledge transfer and data sharing to develop key technologies, including National Risk Assessment Partnership tools and machine-learning applications, through collaboration with the DOE National Laboratory Network complex. 
• Accelerating CCUS through the evaluation and promotion of infrastructure requirements, building stakeholder support through techno- and socioeconomic assessments. 
• Promoting regional technology transfer to accelerate infrastructure development and deployment of CCUS, and building regulatory confidence and supporting business case developments that support the CCUS goals of the DOE Fossil Energy Program.",SSRN Electronic Journal,2021.0,10.2139/ssrn.3812017,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
8a3ab92b1c64c932ac7d460e34222d6738539f19,https://www.semanticscholar.org/paper/8a3ab92b1c64c932ac7d460e34222d6738539f19,Program at a Glance,": In October, 2018, Verizon delivered the world's first 5G commercial use case in the form of fixed home broadband services in the U.S. This 5G FWA (Fixed Wireless Access) service dramatically reduced time and cost required to install optical cables, thus enabling wireless broadband speeds up to 1Gbps in many households. 2019 is another year of firsts for 5G, with the commercialization of NR-based 5G mobile service. In April of 2019, Korea's three mobile carriers, SK Telecom, KT, and LG U+, successfully launched 5G network services in Korea. That same month, Verizon also launched its 5G services in Minneapolis and Chicago. Other operators in the US are planning - or in some cases have already started - 5G services, while China and Japan are also set to launch their 5G commercial services soon. The successful execution of 5G commercialization demands prerequisites such as chips, devices, radio equipments (RAN, Core, S/W tools), S/W virtualization, AI, analytics, and containerization. Based on these, new 5G services will be excavated, from which users will reap the full extent of 5G's benefits. Thus far, Samsung has secured 5G network equipments (RAN, Core, S/W Tool) as well as obtained automation solutions Abstract: Concrete examples of what to expect when 5G comes to life will be presented. We will be looking at how 5G can support new consumer experiences, enhance healthcare, and transform industries like manufacturing and transport as well as how we work as an ecosystem to realize our 5G vision. Abstract: The Fourth Industrial Revolution is fundamentally changing all areas, and the entire industrial structure is expected to be completely changed in the not-to-distant future. AI, Big Data and High-Performance Computing will provide intelligent information services that mimic human capabilities. By implementing AI infrastructure for self-aware, judgment, evolution and collaboration, we will build a human-centered super-intelligent information society such as hyper-smart networking and ultra-realistic experiences. In this keynote presentation, Dr. Myung-Jun Kim provides the various AI researches carried out in ETRI and the future of AI expected by ETRI. Abstract: We are witnessing an exciting time for future wireless networks with the emergence of 5G. In contrast to 3G and 4G, which were mainly a continuation of their predecessors, 5G will represent a revolutionary leap and will have a huge impact on the transformation of wireless communications industries as well as vertical industries. In this talk, we will describe some of the important technologies and innovations ranging from air technologies and network design to services that are needed to meet the demands of the next-generation wireless networks and guarantee broadband ubiquitous communications of all things, including human-to-machine and machine-to-machine, for a connected living. Of particular interest is the use of machine learning, a powerful artificial intelligence approach, for supporting intelligent networks and optimizing resource allocation problems in wireless networks. Abstract: Disruption is happening in Manufacturing. Industrie 4.0 and IIOT programs promise to deliver better business outcome but resulted in early success pilots and projects but failed to scale to full deployment and many wrapped up within a year. Your IIOTl projects needs a Digital ready foundation. Cisco Secure, Intelligent and connected platform is the Digital Ready Foundation for Industrie 4.0 and your IIOT program. It addresses things/machine, Edge/Fog, Data, Security and Automation and management. It also create that ready foundation to customer to build future Digital capabilities and harvest real impact business outcome. lead for Manufacturing Industries, Digital Transformation Office at Cisco. He leads Cisco’s boardroom relevance in the Manufacturing industries to help our customers accelerate their digital transformation journeys. We do this by aligning Cisco’s unified technology platform & digital capabilities to our customer's business imperatives. Abstract: This research talk describes our recent work on “AI Connected Healthcare Platform” for accelerating innovations in Healthcare eco-system. This platform provides the capability to develop, deploy AI based Healthcare analytics and engage to provide real-time insights to patients. It allows healthcare providers to monitor, alert and engage patients through healthcare sensors. At the core, the platform consists of (i) a highly scalable monitor and provide alerts and insights to patients. Several use cases for the eco-system enablement are described to illustrate other important aspects of our platform. Abstract: Silicon Valley is the major breeding ground of a variety of technologies essential to 4th Industrial Revolution (4IR). One of the distinguished platforms on which 4IR technologies converge is autonomous vehicle (AV). Autonomous or Automated Driving (AD) is characterized by the convergence of key ICT technologies including AI, 5G, IoT, and Compute. Also, AD as part of mobility ecosystem is playing a key role in Mobility as a Service (MaaS) and the corresponding monetization business. All of the major players in Mobility as well as venture and legal entities have been establishing their presence in Silicon Valley. In this talk, the history and current trend of AV technology (sensors, computing, software stack, mapping, connectivity, etc.) in Silicon Valley are presented, along with the survey of the relevant SV companies, both the established and the startups. Also, current problems and obstacles to rapid advancement of AV to higher levels are discussed and suggested as future R&D directions. Abstract: With 3GPP R15 Frozen on June 15, 2018, 5G has achieved an important milestone. Major countries have announced 5G Republic of Korea has meaningfully launched of World’s First Commercial 5G network. In this presentation, I will introduce key technologies and services, which SK Telecom has been working on. Aligning to that, the 5G services such as immersive media (Virtual Reality)/AR (Augmented Reality), AI based media, smart factory and smart office will be addressed. In key member board member on Next Generation Media co., which is joint venture(JV) company between SK Telecom and Sinclair, the number one broadcasting company in US.He various ICT R&D projects SKT and deployed new technologies in the market, low latency live streaming, personalized recommendation, HEVC, and immersive AR/VR Optics, etc. Abstract: To support the expanded connectivity needs for the next decade, 5G is taking on a much bigger role than previous generations. Our vision for 5G is a unifying connectivity fabric and a platform for future innovations that will expand the value of mobile networks to connect new industries/devices, utilize new spectrum bands/types, and enable new deployments. mmWave is a set of higher spectrum bands that have so far been limited to fixed point-to-point link such as for wireless backhaul and home broadband delivery, but a new frontier has come to mobilize mmWave for the smartphone and make it available for the masses thanks to cutting edge R&D and extensive research/tests. 5G will serve as the foundational technology that makes it possible for everything and everyone to communicate and interact seamlessly, across connected cars and industrial IoT. This presentation will show how mmWave is enabling next gen mobile experiences, how we are solving the challenges, and also what new industry and services 5G will enable or improve. His roles include developing sound relationship with national and regional government officials and enhance Qualcomm’s corporate reputation. He advocates agreed corporate positions in coordination with internal/external stakeholders, introducing Qualcomm’s technology policies in the areas of mobile, automotive, IoT, and Smart X, and also spectrum policies. He has overall 20+ years of experience in the public policy and government relations areas. Business Applications: Research and Industry Proliferation of chatbots and their advances have been astounding. Almost every messaging platform has APIs to support chatbots. Businesses also have tremendous interests in chatbots for cost saving and better user experiences. However, despite the excitement and the prospect of chatbots, often in reality, their performance falls short of our expectations. This talk will (i) revisit the recent history of AI-enabled chatbots, (ii) present the current state of chatbot applications in the industry, and (iii) discuss the future direction of chatbots in the near term. State of the art technologies to improve chatbot performance and enable future advancement will be also briefly discussed. Abstract: Due to the ever increasing interest in mobile applications and services, the concept of mobile edge computing (MEC) has emerged. Due to the proximity between mobile user (MU) and MEC server located at base station (BS), i.e., the edge of radio access network (RAN), MEC can realize low-latency mobile application. For MEC, MU and MEC server need to exchange tasks using limited radio resources. Furthermore, when multiple MUs possess tasks, MEC server has to handle multiple tasks. Thus, the radio and computing resources need to be allocated to MUs by taking into account the wireless channel condition and the computing power of MUs and MEC server. In this research, a radio resource and computing resources allocation scheme is proposed to minimize the total processing completion time of all the tasks. Each task is assumed to be divided into local task and offload task. Local task is computed by each MU whilst offload task is computed by a MEC server. We first formulate the optimization problem to minimize the total processing completion time of all tasks. For the formulated optimization problem, we propose a two-step radio and computing resources allocation scheme which iteratively performs bisection search method a","2021 19th International Symposium on Modeling and Optimization in Mobile, Ad hoc, and Wireless Networks (WiOpt)",2013.0,10.1111/ajco.12153,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
8881dadf57a7515039c978bf3755c336fbb417af,https://www.semanticscholar.org/paper/8881dadf57a7515039c978bf3755c336fbb417af,Customer Relationship Management Customer Satisfaction Ebooks Download,"This book introduces a fuzzy classification approach, which combines relational databases with fuzzy logic for more effective and powerful customer relationship management (CRM). It shows the benefits of a fuzzy classification in contrast to the traditional sharp evaluation of customers for the acquisition, retention and recovery of customers in online shops. The book starts with a presentation of the basic concepts, fuzzy set theory and the combination of relational databases and fuzzy classification. In its second part, it focuses on the customer perspective, detailing the central concepts of CRM, its theoretical constructs and aspects of analytical, operational and collaborative CRM. It juxtaposes fuzzy and sharp customer classes and shows the implications for customer positioning, mass customization, personalization, customer assessment and controlling. Finally, the book presents the application and implementation of the concepts in online shops. A detailed case study presents the application and a separate chapter introduces the fuzzy Classification Query Language (fCQL) toolkit for implementing these concepts. In its appendix the book lists the fuzzy set operators and the query language’s grammar. This work offers a state-of-the art survey of information systems research on electronic customer relationship management (eCRM). It provides important new frameworks derived from current cases and applications in this emerging field. Each chapter takes a collaborative approach to eCRM that goes beyond the analytical and operational perspectives most often taken by researchers in the field. Chapters also stress integration with other enterprise information systems. The book is organized in four parts: Part I presents an overview of the role of CRM and eCRM in marketing and supply chain management; Part II focuses on the organizational success factors behind eCRM implementation; Part III presents cases of eCRM performance enhancement; and Part IV addresses eCRM issues in business-to-consumer commerce. Continuous improvements in digitized practices have created opportunities for businesses to develop more streamlined processes. This not only leads to higher success in day-today production, but it increases the overall success of businesses. Enterprise Information Systems and the Digitalization of Business Functions is a key resource on the latest advances and research for a digital agenda in the business world. Highlighting multidisciplinary studies on data modeling, information systems, and customer relationship management, this publication is an ideal reference source for professionals, researchers, managers, consultants, and university students interested in emerging developments for business process management. Research Paper (postgraduate) from the year 2019 in the subject Business economics Customer Relationship Management, CRM, grade: 1.5, Kwame Nkrumah University of Science and Technology, language: English, abstract: Customer Relationship Management (CRM) practices are business strategies designed to reduce costs and increase profitability by solidifying customer loyalty. With intense competition among insurance companies in Ghana, this study sought to assess Customer Relationship Management practices and Customer Retention in NSIA Insurance. The study was conducted to identify critical factors necessary for customer retention in carrying out customer relationship management practices in the selected insurance company and to develop effective customer relationship management practices to manage customer retention for sustainability within the insurance industry using NSIA Insurance as a case study. Well structured questionnaires and face-to-face interview were the methods adopted for the investigation of the study. A sample size of 40 respondents was considered, they were made up of customers and the staff who are fully involved in customer relationship management of the insurance company. Data collected from the completed questionnaires and the interviews were grouped into frequency tables and expressed in percentages. The researcher relied on the SPSS in interpreting the collected data. The study shows that even though NSIA insurance has policies on customer relationship management practices, these policies are not carried out fully to accomplish the ultimate goal of customer retention. The study recommends that for the insurance company to command an adequate number of loyal customers, NSIA Insurance should consistently improve on its quality of service to address the preference of the customers and consider the five service quality constructs of reliability, assurance, tangibility, empathy and responsiveness. The two-volume set CCIS 143 and CCIS 144 constitutes the refereed proceedings of the International Conference on Electronic Commerce, Web Application, and Communication, ECWAC 2011, held in Guangzhou, China, in April 2011. The 148 revised full papers presented in both volumes were carefully reviewed and selected from a large number of submissions. Providing a forum for engineers, scientists, researchers in electronic commerce, Web application, and communication fields, the conference will put special focus also on aspects such as e-business, e-learning, and e-security, intelligent information applications, database and system security, image and video signal processing, pattern recognition, information science, industrial automation, process control, user/machine systems, security, integrity, and protection, as well as mobile and multimedia communications. Customer Relationship Management, Fourth Edition, is a much-anticipated update of a bestselling textbook, including substantial revisions to bring its coverage up to date with the very latest in CRM practice. The book introduces the concept of CRM, explains its benefits, how and why it can be used, the technologies that are deployed, and how to implement it, providing you with a guide to every aspect of CRM in your business or your studies. Both theoretically sound and managerially relevant, the book draws on academic and independent research from a wide range of disciplines including IS, HR, project management, finance, strategy and more. Buttle and Maklan, clearly and without jargon, explain how CRM can be used throughout the customer life cycle stages of customer acquisition, retention and development. The book is illustrated liberally with screenshots from CRM software applications and case illustrations of CRM in practice. New to this Edition: Updated instructor support materials online Full colour interior Brand",,2021.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
2cc3338709ea9c14ff422025ae4a8ad09f9598ba,https://www.semanticscholar.org/paper/2cc3338709ea9c14ff422025ae4a8ad09f9598ba,Explainable AI: from black box to glass box,,Journal of the Academy of Marketing Science,2019.0,10.1007/s11747-019-00710-5,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
59638a384fde04f0af6faf7814cc6d31f941d82b,https://www.semanticscholar.org/paper/59638a384fde04f0af6faf7814cc6d31f941d82b,Next generation diagnostic pathology: use of digital pathology and artificial intelligence tools to augment a pathological diagnosis,,Diagnostic Pathology,2019.0,10.1186/s13000-019-0921-2,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
3d0e8c86b49bb4674e248ace106b4e943019edba,https://www.semanticscholar.org/paper/3d0e8c86b49bb4674e248ace106b4e943019edba,Smart Autopilot Drone System for Surface Surveillance and Anomaly Detection via Customizable Deep Neural Network,"
 Copter-based unmanned aerial vehicle (drone) systems are being utilized for surveillance, inspection and security purposes for well sites, gathering centers, pipelines, refineries, and other surface facilities. However, most of the practices largely rely on humans, including drone operation, data transfer, image analysis, etc. In this paper, we present a comprehensive, cloud-enabled, human-free autopilot drone system and its application in field surveillance and anomaly detection powered by customizable deep neural network and computer vision models.
 The proposed system consists of customized quadcopter drones equipped with high-definition cameras, thermal imaging and gas sensing devices, autopiloted by cloud-connected onboard computers. A series of advanced algorithms are developed and deployed onboard and over the Cloud for processing and diagnosing the image/thermal/gas sensing data collected by the drones in real-time or near real-time, including accurate 2D geospatial aerial mapping, anomaly detection and classification for events like oil leak, gas leak, facility failure, human activities, etc. Object detection deep learning models are customized and parallelized for low-profile multi-core single board computers.
 In our case study, a pre-configured drone flew along the same path twice at a 6-month gap. A robust, iterative image registering algorithm is developed to precisely align and overlay images taken at different days at the same or similar GPS locations, even with significant changes to the environment due to season shift, human activities, camera angles or height variations. Local changes are filtered and selected based on their sizes and magnitudes in the residual images by subtracting pairs of perfectly overlaid scenes. Pre-trained Residual Convolutional Neural network (He et al. 2015) is rapidly re-trained to further classify the type of changes using the techniques of transfer learning and data augmentation. An ROC of 99% was achieved in the multi-task binary classification, wherein the detected changes are divided into positive anomalies (such as oil/gas leak, facility failures, unauthored human activities) and negative (natural/insignificant) signals. Comparing against a support vector machine baseline with a ROC=92%, the ResNet model demonstrates significant, more promising detection accuracy at a faster training time.
 This innovative integrated platform is presented that combines physical drone, onboard imaging/sensing devices, cloud connectivity, onboard and back-end control system, deep learning and computer vision architecture for situational awareness of oil & gas fields and the mining industry. It achieves full automation of mass surveillance, data acquisition and storage, diagnostics and asset situational understanding. The system architecture, especially the onboard and cloud computation engines, can be readily transferred and applied to other common drone platforms.",,2020.0,10.2523/iptc-20111-ms,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
c27c4e652dcce5f8739285ca059bb6137764b6c7,https://www.semanticscholar.org/paper/c27c4e652dcce5f8739285ca059bb6137764b6c7,SyntheticNET: A 3GPP Compliant Simulator for AI Enabled 5G and Beyond,"The rapid evolution of cellular system design towards 5G and beyond gives rise to a need for investigation of the new features, design proposals and solutions in realistic settings for various deployments and use case scenarios. While many system level simulators for 4G and 5G exist today, there is particularly a dire need for a 3GPP compliant system level holistic and realistic simulator that can support evaluation of the plethora of AI based network automation solutions being proposed in literature. In this paper we present such a simulator developed at AI4networks Lab, called SyntheticNET. To the best of authors’ knowledge, SyntheticNET is the very first python-based simulator that fully conforms to 3GPP 5G standard release 15 and is upgradable to future releases. The key distinguishing features of SyntheticNET compared to existing simulators include: 1) a modular structure to facilitate cross validation and upgrading to future releases; 2) flexible propagation modeling using measurement based, ray tracing based or AI based propagation modeling; 3) ability to import data sheet based on measurement based realistic vendor specific base station features such as antenna and energy consumption pattern; 4) support for 5G standard based adaptive numerology; 5) realistic and user-specific mobility patterns that are yielded from actual geographical maps; 6) detailed handover (HO) process implementation; and 7) incorporation of database aided edge computing. Another key feature of the SyntheticNET is the ease with which it can be used to test AI based network automation solutions. Being the first python based 5G simulator, this ease, in part stems for SyntheticNET’s built-in capability to process and analyze large data sets and integrated access to Machine Learning libraries. Thus, SyntheticNET simulator offers a powerful platform for academia and industry alike to investigate not only new solutions for optimally designing, deploying and operating existing and emerging cellular networks but also for enabling AI empowered deep automation in the future.",IEEE Access,2020.0,10.1109/ACCESS.2020.2991959,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
65900342de4cc7cb6fc36af9862b99e21a98c2fb,https://www.semanticscholar.org/paper/65900342de4cc7cb6fc36af9862b99e21a98c2fb,Big tech and societal sustainability: an ethical framework,,AI & SOCIETY,2020.0,10.1007/s00146-020-00956-6,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
e884343e6132b445adae50f95dfc0a7d3f06e57a,https://www.semanticscholar.org/paper/e884343e6132b445adae50f95dfc0a7d3f06e57a,Non-Destructive Insect Infestation Monitoring Systems | Encyclopedia,"There developing new technologies that can improve the detection of insect infestation in and under using in last few years. The motivation stemmed from challenges in preventing insects of quarantine significance from getting into the supply chain, the economic benefits that ensue from having efficient detection systems, and the availability of improved technologies in this domain that encouraged studies that have produced efficient solutions. This review presents a general overview of current non-destructive techniques for the detection of insect damage in fruits and vegetables and discusses basic principles and applications. We also elaborated on the specific post-harvest fruit infestation detection methods, which include principles, protocols, specific application examples, merits, and limitations. The methods reviewed include those based on spectroscopy, imaging, acoustic sensing, and chemical interactions with greater emphasis on the noninvasive methods. This review also discusses the current research gaps as well as the future research directions for non-destructive methods’ application in the detection and classification of insect infestation in fruits and vegetables especially when compared to other noninvasive systems such as color vision, hyperspectral and multispectral imaging, and spectroscopic systems. Such problems include the high cost for building, running, and maintenance, and the large volume and heavy weight of the MRI systems [86,90]. The significance of ineffective insect infestation detection in fruits and vegetables is broad. It lies in the reduction in the value of produce that may ensue when they enter the supply chain without detection and control, the economic losses when infestation causes a ban of produce export, spread or damage occurring to high-quality produce, and the safety issues related to consuming or processing infested produce. This paper reviewed different methods that have been explored in the last few years for non-destructive detection and classification of fruits and vegetables infested with different types of insect pests. Agricultural production is at a scale and stage where subjective assessment is insufficient to meet the scale of quality needed by the industry. The development of highly sensitive and accurate technologies for performing the role typically done by human subjects is essential for quick turnover to meet regulatory and consumer demands. Several of the technologies available have prospects and limitations. Some of the challenges include the high cost of implementation, sensitivity, accuracy, feedback time, and in some cases, safety. Techniques such as hyperspectral imaging, electronic nose, and acoustic emission are emerging as the sensors needed for artificial intelligent system deployment to address this need. HSI especially has been applied as the baseline technology in some other industries, and its potential for success in insect infestation prediction is promising, so long as the accuracy is guaranteed. A lot of these techniques require a machine learning computational approach for development and deployment. Advanced machine learning approaches like sensor data fusion and ensemble machine learning have allowed for combining the strengths of different approaches, and models for better results have shown the potential benefits of improving the models for quality assessment of fruits, vegetables, and food products [147]. Current improvement in the analytical approach of big data and feedback speed will benefit these methods and make them more amenable.",,2020.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
09a4ab6a61eaa69d1ce496a4f291d565329fc8d5,https://www.semanticscholar.org/paper/09a4ab6a61eaa69d1ce496a4f291d565329fc8d5,eResearch Collaboration Projects-supporting CSIRO's digital science and research,"Background CSIRO is Australia’s largest research agency and is a recognised leader in a diverse set of
science domains: Agricultural Sciences, Environment/Ecology, Plant and Animal Sciences,
Geosciences, Chemistry and Materials Science. CSIRO also manages research infrastructure
like the Australia Telescope National Facility (ATNF), the Marine Research Vessel RV
Investigator and the Pawsey Supercomputing Centre. For many years in Australia, and also worldwide [2], research and science have undergone
transformational changes with the introduction of new instruments and advanced facilities
with matching increases in storage and computing capabilities. Individual researchers were
taking a bespoke approach to matching these technologies and capabilities to the way that
research and science were carried out. Wider adoption of new practices required social
change (in the practice of science and research) and these changes remained fragmented
and tailored to specific sciences or even projects. Organisations, by and large, varied
enormously in their support of these new practices.As far back as 2007 [1], CSIRO eResearch practitioners advocated that science and research
practices within CSIRO adapt to deal with these challenges. Much like the rest of the world,
practices matured over the years: in CSIRO’s health and biosecurity, oceanographic and
atmospheric research, radio astronomy, agriculture and food as well as geological and
other earth sciences. However, a significant shift occured in 2018, with a formal recognition by the CSIRO Board
of the need to support the new “digital” science and research at an organisational level.
CSIRO developed strategic digital transformation initiatives, including CSIRO’s Managed
Data Ecosystem (MDE), Missions and the Digital Academy [4].The aim of the MDE is to connect current and new platforms in a seamless way and improve
interoperability between datasets so users will be able to easily find and work on multiple
datasets. It will provide a set of tools and approaches enabling CSIRO and partners to
improve our collaboration, mining and analysis of data. CSIRO Missions are major scientific and collaborative research programs aimed at making
significant breakthroughs in one of six major challenges facing Australia. They include the
resilient and valuable environments, food security and quality, health and well-being, future
industries, sustainable energy and resources, and regional security. CSIRO's Digital Academy is focused on investing in the digital capability of our staff and
involves a rethink in planning for a digitally driven research environment. It provides a
learning opportunity for our staff, helping define the digital talent, skills and new ways of
working. The Academy will help attract and retain new digital talent within the Australian
innovation system, develop new digital skills and mindsets in Australian’s scientists and
facilitate digital talent accessibility and collaboration across Australia’s innovation system.Existing Support for “Digital” Science through “eResearch” initiativesCSIRO Scientific Computing Services group has been providing a dedicated eResearch service
since 2011 [3] This service is delivered through ""eResearch Collaboration Projects” (eRCPs)
which now delivers specialist capabilities that includes Machine Learning, Data Analytics,
Scientific Visualisation, Workflow Management and Science Data Handling into research and
science projects. The eRCP process is run as a competitive grant process and continues to be very successful. In the latest cycle, forty Scientific Computing Services specialists successfully completed and
delivered over sixty eRCPs outcomes from a total of eighty submissions. The underlying
capabilities are delivered by members from each of teams in the Scientific Computing
Services group: Technical Solutions; Data Analytics and Visualisation; Research Software
Engineering; and Modelling and Dataflow. The eRCP process also provides a mechanism to
promote and introduce new tools and frameworks for consumption to CSIRO’s research
community eg Jupyter and R/Shiny. In the latest cycle, forty Scientific Computing Services specialists successfully completed and
delivered over sixty eRCPs outcomes from a total of eighty submissions. The underlying
capabilities are delivered by members from each of teams in the Scientific Computing
Services group: Technical Solutions; Data Analytics and Visualisation; Research Software
Engineering; and Modelling and Dataflow. The eRCP process also provides a mechanism to
promote and introduce new tools and frameworks for consumption to CSIRO’s research
community eg Jupyter and R/Shiny. Specialists from the Scientific Computing program are then assigned to work on one or more
approved eRCPs. Over the six-month cycle, the resource allocation is around 0.2 FTE, with
each staff member allocated 3 eRCP projects per cycle. Importantly, eRCPs are provided to
CSIRO researchers and scientists at no additional charge.The eRCP has been enormously successful over the years, with demand outstripping
capability to allocate staff to the projects. The program has demonstrated a range of useful
outcomes including – including for example - an augmented reality tool for analysing
bushfire plumes over Tasmania; a dashboard to interrogate cotton crop physiological
measurements and an online platform to monitor algal blooms for multiple water bodies.Scientific Computing specialists also provide dedicated support to CSIRO researchers, based
around the same set of core capabilities, via an entirely separate funding models known as
“pan deployments” as well as secondments. In both cases, CSIRO projects fund the specialists’
time at larger allocations, often extending over 12 months or more. In a sense, this acts like a
contractor service for Business Units, providing them with highly specialised skills but without
the need to recruit new staff of their own.Future PlansCSIRO Scientific Computing will respond to the major initiatives – MDE, Digital Academy and
Missions as follows:MDE - Redirect Scientific Computing expertise currently working on eRCPs and pan
deployments to MDE related activities. In the first instance, these specialists
will apply their skills and domain knowledge to one of several nominated
pilots, helping design and build foundational components of the MDE. - Over time, it is anticipated that those same specialists will contribute to the
ongoing development and enhancement of additional MDE components in
line with its progressive organisational rollout. Digital Academy - Develop/adapt training content as appropriate for the Digital Academy. For
example, making use of existing Software Carpentry material for HPC usage,
but customising appropriate aspects for our own computing environment.- Delivering training content to CSIRO staff. This has already proven very
successful in the machine learning area – with hundreds of staff attending
sessions - and will no doubt continue to grow over time.Missions - Scientific Computing will continue to provide CSIRO researchers with the
eResearch support they need in response to the significant scientific
challenges tackling Missions. REFERENCES 1. J. A. Taylor, J. Zic, and J. Morrissey, “Building CSIRO e-Research Capabilities,” in eResearch Australasia 2008.2. T. Hey, S. Tansley, and K. Tolle, “The Fourth Paradigm: Data-Intensive Scientific
Discovery,” Data-Intensive Sci. Discov. Microsoft Res., 2009.3. S. Moskwa, “The Accelerated Computing Initiative,” in eResearch Australasia, 2012.4. CSIRO Chief Executive's Report 2018-19: https://www.csiro.au/en/About/Ourimpact/Reporting-our-impact/Annual-reports/18-19-annual-report/part-1/chiefexecutive-reportABOUT THE AUTHOR(S) Dr John Zic is the Executive Manager of CSIRO’s Science Computing Services Mr Justin Baker is Leader of the Scientific Computing Data Analytics and Visualisation
Team.",,2020.0,10.6084/M9.FIGSHARE.11929647.V1,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
aeefe6929553a42a9a6371ce80aea6094fdb8793,https://www.semanticscholar.org/paper/aeefe6929553a42a9a6371ce80aea6094fdb8793,Spectrum Efficiency and Energy Efficiency in Wireless Communication Networks,"In the October 2020 issue of IEEE Wireless Communications Magazine, we are pleased to present a special issue on “Spectrum and Energy Efficient Wireless Communications” with a collection of 12 articles. In this issue, we are also very glad to present 12 articles accepted from the open call. Driven by new-generation mobile devices and bandwidth consuming applications such as video streaming, wireless traffic volume is expected to continue expanding tremendously in the next few years. Sustaining this growing trend will in turn require higher spectrum capacity from the network side. Research has shown that capacity demand increases much faster than the current spectrum efficiency improvement, in particular at hot spot areas. From recent data, global mobile data traffic increased nearly 11-fold in the last few years. In contrast, the peak data rate from 3G wireless technology to 4G wireless technology only increased 55 percent in the last decade. Clearly there is a huge gap between the capacity growth of new wireless access technologies and the fast growth of wireless traffic volume for the next-generation wireless networks. In the meantime, energy efficiency, commonly defined as information bits per unit of energy, has become another essential requirement for the design of future wireless communication networks besides spectrum efficiency. Energy efficient communications have attracted great attention due to the ever-increasing demand to preserve energy resources and to protect the environment. Furthermore, mobile devices, such as smart phones and tablets, are widely used to conduct new applications such as video content distribution, location-aware advertisement, video chatting, video streaming, music and movie downloading, etc. In the year 2012, mobile video traffic exceeded 50 percent of the total wireless traffic volume for the first time. Mobile video has increased 14-fold since then, accounting for 69 percent of the total mobile data traffic by the end of last year. How to support energy and bandwidth consuming video applications with high QoE is becoming another challenging issue in future wireless networks. Clearly, there is an urgency for a new disruptive paradigm to bridge the gap between the increasing capacity, energy and QoE demands and the deficiency of radio spectrum resources. As wireless channel efficiency is approaching its fundamental limit, improvements in future wireless system capacity can be alternatively realized by networking technologies such as node density increase through underlay and overlay deployments, or by going to a higher spectrum such as millimeter wave to seek more spectrum bandwidth. In addition to delivering the required network spectrum efficiency, energy efficiency and QoE, the anticipated tremendous proliferation of machine-type devices and consumer-wearable devices also makes the underlay wireless network desirable. These small devices usually have limited onboard processing power and battery size. If they need powerful computing capability to process extensive content information, they will have to heavily rely on their surrounding local networks and computing platforms to facilitate these computing-intensive and thus power consuming applications. Using high performance and very low latency communication links to offload mobile device computing load into nearby powerful computing clouds becomes an essential direction to pursue. This paradigm shift in the next decade also calls for the cluster-based underlay networking technologies, in which a cluster head of a number of underlay devices can be selected as the representative of the entire cluster for both communication and control purposes. Spectrum and energy efficient wireless communications is one of the most important topics today in the next generation wireless networking area, and attracting more and more attention from industry, research, and academia. This special issue focuses on the challenges and novel solutions for spectrum and energy efficient wireless communication networks. Thanks to the guest editors, Q. Liang, T. S. Durrani, J. Koh, Q. Wu, and X. Wang, who did an excellent job in editing this special issue for our readers. Please stay tuned for new developments in the research area of spectrum and energy efficient wireless communications, and read the editorial for more details about the papers in this special issue. In addition to the 12 articles in the special issue, we have also included 12 accepted open call articles. The first article, “Dense Small Satellite Networks for Modern Terrestrial Communication Systems: Benefits, Infrastructure, and Technologies” by N. Hassan et al., demonstrates several components of dense small satellite networks (DSSN) infrastructure, including satellite formations, orbital paths, inter-satellite communication links, and communication architectures for data delivery from source to destination. It also reviews important technologies for DSSN as well as the challenges involved in the use of these technologies in DSSN. Several open research directions to enhance the benefits of DSSN for MTCS are also identified in the article. A case study showing the integration benefits of DSSN in mobile terrestrial communication systems is also included. The second article, “Overcoming the Channel Estimation Barrier in Massive MIMO Communication via Deep Learning” by Z. Liu et al., discusses the application of deep learning (DL) for massive MIMO channel estimation in wireless networks by integrating the underlying characteristics of channels in future high-speed cellular deployment. It develops important insights derived from the physical radio frequency channel properties and presents a comprehensive overview on the application of DL for accurately estimating channel state information (CSI) with low overhead. The article provides examples of successful DL application in CSI estimation for massive MIMO wireless systems and highlights several promising directions for future research. In the third article, “Energy-aware Task Offloading in Internet of Things”, J. Li et al. introduce a new energy-aware task offloading scheme in IoT to determine the optimal offloading strategy. First, they investigate the architecture of mobile edge computing (MEC) in IoT. Second, they discuss the challenges of task offloading in MEC for IoT. Third, they propose the framework of task offloading for MEC, the optimal offloading strategy for computing task is achieved. Finally, the article demonstrates experiment results to show that the proposed scheme can significantly improve the efficiency of task offloading compared with the conventional scheme. Yi Qian",IEEE Wirel. Commun.,2020.0,10.1109/mwc.2020.9241874,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
3b316c35b97f371d309c0aa521c6e835a85d1222,https://www.semanticscholar.org/paper/3b316c35b97f371d309c0aa521c6e835a85d1222,A Novel Assessment of Delayed Neutron Detector Data in CANDU Reactors,"
 A common opportunity for nuclear power plant operators is ensuring that routinely collected data are fully leveraged. Exploiting data analytics can enable improvements in anomaly detection and condition monitoring by identifying previously unseen data trends and correlations without major financial investment. One such opportunity is in facilitating the detection of fuel defects by augmenting the delayed neutron (DN) monitoring system deployed in the majority of Canada deuterium uranium (CANDU) reactors. In this paper, we demonstrate using archive data that the detection of fuel defects can be accelerated using this system in combination with the use of a deeper historical dataset and the introduction of a smoothing algorithm. The current defect identification process relies on the analysis of data of high variance and is subject to the judgment of a domain expert, resulting in variable defect identification periods. The proposed approaches seek to mitigate this and alleviate the variable identification time. Initial results presented here show that for an initial batch of 30 defects, identification periods can be meaningfully reduced compared to the current process, with defects potentially visible on an average of 11.4 days earlier. By shortening this identification period, fuel containing defects can be scheduled for earlier removal, reducing the risk of statutory shutdown obligations, protecting personnel, and promoting industry best practice. Exploring a historical dataset identifies previously undocumented trends and we discuss the potential to produce correlations with other reactor parameters. The application of this knowledge can lead to opportunities in the use of machine learning algorithms and, ultimately, more accurate predictions.",Journal of Nuclear Engineering and Radiation Science,2020.0,10.1115/1.4046824,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
981d1f317673c1ae3c0942e73207c85909e6490c,https://www.semanticscholar.org/paper/981d1f317673c1ae3c0942e73207c85909e6490c,Bringing RPA to the next level using AI,"Introduction: Artificial Perspicacity (AI) is transmuting the digital landscape in every field it reaches. The Robotic Process Automation (RPA) revolution goes hand in hand with the advances that Artificial Astuteness is making to transform industries ecumenical. Ultimately, Artificial Perspicacity augments RPA and its implements to surpass prospects. With this already established, it’s valid to verbally express that the world is gearing up for the robotic revolution. Albeit we are already experiencing many of its applications, there’s still plenty of potentials to tap into. What is RPA precisely? RPA or Robotic Process Automation is software technology, as dictated by business logic and structured inputs, aimed to program applications or robots to perform rudimental tasks, just like humans would, in an automated setting. RPA bots can mimic virtually any human action, emulating and integrating actions with digital systems to execute a business process. Robotic Process Automation (RPA) enables organizations to engender virtual workforces that increase process efficiency, truncate errors, and cut operational costs. However, even when enterprises implement RPA, many are challenged with scaling it across the organization and identifying all the potential business processes that can and should be automated for maximum ROI. This presentation will discuss how companies overcome these challenges utilizing AI powered process revelation. Automation has been transmuting the nature of work for over a decennium. It has superseded labour intensive tasks, undertaken perpetual ones, escalated the haste of engenderment and engendered incipient streams of work. Most organizations are already on RPA journey, which has resulted in productivity amendment, cost savings, process time amelioration with perpetual and rule-predicated processes. The commencement of the peregrination involves POCs, pilots, and initial automations. The next frontier is about scaling the  
deployment and adoption of cognitive technologies such as AI, analytics, machine learning that emulate human comportment. The transition involves transformation from running RPA on a few processes to scaling up RPA across the enterprise. This session would deal with strategies & challenges in enterprise wide adoption of Automation - crossing over from RPA to RPA+Cognitive , identifying the right operating model & establishing governance, Leadership & aptitude development , orchestrating stakeholders and board level endorsement. According to Investopedia, Robotic Process Automation is the “software that can be facilely programmed to do rudimentary tasks across applications just as human workers do.” Consequentiality of Robotic Process Automation According to insights developed by McKinsey&Company, RPA offers the potential ROI of 30-200% in the first year of avail alone. This staggering figure is met with the verbal expression made by Leslie Willcocks that “RPA takes the robot out of the human.” Companies and employees are taking notice, which is why everyone is so agog to invest in robotics and its emerging technologies. In essence, RPA is consequential because it is transforming the way businesses operate by availing automate perpetual tasks that are a component of a quotidian routine with a higher degree of efficiency than if performed by a human. Akin to cognitive automation, chatbots, and artificial perspicacity, RPA performs significantly more expeditious and more cost-efficaciously than human resources. Many fear that RPA implements and technologies can be perilous as they take jobs out of human hands. But you shouldn’t authentically worry as there is more to gain than lose when it comes to RPA. To put it into further context, implementing Robotic Process Automation into your workplace can avail with tasks such as monitoring customer activity to discover opportunities to upsell, monitoring client comportment to identify areas of opportunity, truncating cycle times in perpetual tasks to gain competitive advantage, capturing and analyzing bulks of information to provide more expeditious replication times, and more. In a wide range of industries that span healthcare, indemnification, finance, and more, there are many cumbersomely hefty-hitters that have already adopted RPA implements into their processes, including Wal-Mart, Ernst & Puerile, Walgreens, American Express, and more. These early-adopters have benefitted from minimizing staffing costs and human errors with the implementation of RPA technology.",,2020.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
db2b4ea57fde0b16ecd5973447bd502ac3ae99e0,https://www.semanticscholar.org/paper/db2b4ea57fde0b16ecd5973447bd502ac3ae99e0,FASTTRACK RECOMMENDER SYSTEM,"Network operators are overloaded with numerous recommendations coming from vendors, some of which come from automated recommender systems. Such automated recommendations may or may not apply to a customer’s specific environment, often lack an assessment of priority within the context of the other recommendations, and may or may not apply to an individual customer’s scenario. To address these challenges, techniques are presented herein that provide a novel approach to generating and ranking recommendations coming from a dynamic recommender system where rankings are based on enriched context from, for example, live data on real networks, activities performed by real customers, etc. Such techniques enhance the operational features of existing networks by recommending popular items to new customers, identifying critical items that can be proactively addressed in order to provide additional services, and reducing Technical Assistance Center (TAC) cases when patches exist for common issues. DETAILED DESCRIPTION Network operators are overloaded with numerous recommendations coming from vendors, some of which come from automated recommender systems. Such automated recommendations may or may not apply to a customer’s specific environment. Further, such recommendations often lack an assessment of priority within the context of the other recommendations that such a system provides as well as adoption trends of industry peers. Finally, such recommendations may or may not apply to an individual customer’s scenario. Even high priority recommendations are not of value to a customer who is not running a configuration that would benefit from the recommendations. 2 Jannetta et al.: FASTTRACK RECOMMENDER SYSTEM Published by Technical Disclosure Commons, 2020 2 6564 Recommendations in this context may be sourced from, for example, vendors, experts, expert systems, or machine learning pipelines. Ultimately the customer is presented with an unwieldy list of items that can cause them to miss the important recommendations or tune out the source of the items altogether. The general professional services industry presents a large volume of recommendations to customers, and while valuable, customer feedback suggests that it is difficult to parse and prioritize the insights and recommendations. To address these types of challenges techniques are presented herein that support a recommender system that is designed to cover any cases that meet the following criteria:  New items become available all of the time.  A large population is available to observe the adoption of those items.  When the items become part of a larger set, they are no longer needed if the set itself is used. This is a common scenario across many areas regarding customer recommendations. There is a need to add priority and ranking to the automated recommendations such that the network operator can make a decision whether to implement the recommendation in the short term, the longer term, or not at all. For purposes of exposition, in the narrative that follows aspects of the techniques presented herein will be described with reference to software patches for an operating system (OS). Aspects of the techniques presented herein support a method for recommending only the most relevant items to customers, or as candidates for automated deployment methods. Such ranking of the items ensures that only the highly relevant ones that are deployed in the largest and most active customers will be identified for deployment in customers that may be challenged (by, for example, a lack of skills, a lack of resources, etc.) in keeping up with software patch levels. It is important to note that software patching is only one example of how aspects of the techniques presented herein may be employed. Further aspects of the techniques presented herein may utilize routing protocol features. For example, perhaps most customers who run Open Shortest Path First (OSPF) (i.e., Feature A) on a particular device (i.e., Feature B) choose to change the way OSPF metrics are calculated (i.e., Feature C). 3 Defensive Publications Series, Art. 3788 [2020] https://www.tdcommons.org/dpubs_series/3788 3 6564 In addition, the rate of enabling the new feature C once it became available is relatively high (thus increasing the relevance). This creates a strong recommendation to enable this feature for customers that have not yet enabled it. The framework that is associated with the techniques presented herein may be applied to any feature recommendation, configuration best practice, security advisory remediation, etc. For simplicity of exposition the discussion that follows will focus on a software patching scenario. Aspects of the techniques presented herein include a series of stages to create a ranked list of recommendations. For the purposes of illustration, in the example that is presented below various of the stages are highlighted using a very common yet complex scenario for recommendations – i.e., a base software that has both patch and service pack capability. As will be readily apparent, of particular interest and note are stages one and two (e.g., leveraging off-the-shelf solutions for feature identification) and stages three and four (e.g., developing a score for each feature based on a rate of adoption and continued usage by peers). A first stage of the techniques presented herein encompasses a pairwise recommender system that is built using off-the-shelf algorithms. Pairwise feature comparisons with historical software patch data and lift metrics are used to provide an initial ranking of software patches, as well as an indication of which software patches are ""real"" because they have seen in real network data. For the general recommendations case, this could be classes of recommendations that have been actioned by other customers. A second stage of the techniques presented herein employs the initial rate of deployment to combine with the lift and support metrics from stage one. This is a second scoring method which adjusts the patch ranking to account for deployment statistics seen across thousands of other network devices. Again, for the general recommendations case this would be the relative pace of taking action once recommendations were first shown to a customer (e.g., was it immediate, thus having a high rank, or did it linger, thus having a lower rank). A third stage of the techniques presented herein utilizes network controllers or individual devices to collect the latest configuration data from unseen, new devices, and apply the ranked rules from stages one and two to generate a ranked list of candidate 4 Jannetta et al.: FASTTRACK RECOMMENDER SYSTEM Published by Technical Disclosure Commons, 2020 4 6564 patches for each entry. In the general recommendations case, this could be the current ""network improvement plan"" or ""action list"" from a cloud service. A fourth stage of the techniques presented herein is a filter stage that looks at userto-user similarity (as opposed to item-to-item similarity in stages one and two) to determine the applicability of the recommended patches. One example is configuration similarity (e.g., modeled features as a configuration representation) to determine possible usefulness based on configurations of the device population that has the software patch installed. In the general recommendations case, this could be an industry peer group. A fifth stage of the techniques presented herein integrates with an Information Technology Service Management (ITSM) system to open a ticket that includes, possibly among other things, the recommendation, the benefits to customers, and an option/services to make the change for the customer. This stage may also contain a final ""don't recommend"" list that is generated by expert systems matching (e.g., a patch is not relevant, a patch is in an installed service pack, etc.) as well as the input from stage six (e.g., a customer indicated no interest in the patch). A sixth stage of the techniques presented herein closes the loop by developing a deployment plan (e.g., manual, automated system, etc.) or removing items that are no longer relevant (e.g., based on information from stage five) and then applying the changes in the network via the appropriate process for the type of recommendation. Aspects of the narrative that was presented above may be illustrated through Figure 1, below. Figure 1: Fasttrack Recommender System 5 Defensive Publications Series, Art. 3788 [2020] https://www.tdcommons.org/dpubs_series/3788 5 6564 As described above, stage one encompasses a pairwise recommender system that looks at all of the items in the item sets as pairs. This identifies items that are seen together, and can also double as a reasoner to determine that a patch is contained in other items. For example, as illustrated in Figure 2, below, both patches are contained in the service pack three. In any case of the customer having one but not the other, the system can infer that a service pack is not installed, and the missing feature should be recommended. Figure 2: Illustrative Associations In Figure 2, above, the top graph shows a clear relationship between two items commonly seen together, in a diagram that shows the overall deployment percentage. Any 6 Jannetta et al.: FASTTRACK RECOMMENDER SYSTEM Published by Technical Disclosure Commons, 2020 6 6564 standard recommender system will pick these up as association rules. (Note that the Apriori algorithm was used in this instance.) Figure 3, below, shows another example of patches that show up pairwise together in a different version of software. There are many patches covered, but there are clearly four standouts that are deployed at a much higher rate than any others. Those standouts get scored higher in stage two of the system. From some level of minimum deployment seen in real networks (such as, for example, 20% of the peer devices have the feature), an evaluation of the initial deployment line",,2020.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
ebfc39892d9f86c16b3aecf728290e16e77feaf4,https://www.semanticscholar.org/paper/ebfc39892d9f86c16b3aecf728290e16e77feaf4,Determining Load Profiles for Customers in the Indicated DSO Area based on Smart Meters: A Summary Report for European Data Incubator,"The growth in the implementation of smart-meters presents a significant opportunity to improve operational processes in the energy sector. One such opportunity is with distribution system operators using the data to better anticipate the state of the different areas of their networks. However, large scale smart-meter rollouts often take years to complete, and having digital solutions that can realise outcomes as data becomes increasingly available can improve the return on investment of such rollouts. In this three month project conducted as part of the European Data Incubator program, data from over 2300 metered locations in the Torun region of Poland with varying levels of frequency ranging from every 15 minutes to readings every few months are examined to develop a methodology for creating hourly profiles of energy consumption for every location. Numerous patterns in the unavailability of the data are identified, and a methodology is proposed to address each pattern specifically. The proposed methodology tests multiple machine-learning based approaches including Random Forest, Decision Tree, Support Vector Regression, and clustering and assesses their effectiveness both against a generated baseline and other similar studies from academic literature. Additionally, functional considerations for deploying the algorithmic methodology in a full commercial use case are examined. The results achieved demonstrate comparable or superior performance against previous studies, leading to the conclusion that the methodology proposed is among the best in the industry, and several areas for future improvement are identified.",,2020.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
6d313954e6b7e5f83eb49dc05d68934aa1a627c8,https://www.semanticscholar.org/paper/6d313954e6b7e5f83eb49dc05d68934aa1a627c8,Blockchain and Anomaly Detection based Monitoring System for Enforcing Wastewater Reuse,"Industries, household communities consume a lot of water on regular basis, thereby increasing water crisis. There is continuous increase in water consumption by industries. Reusing wastewater can reduce water withdrawals from local water sources thus increasing water availability, lowering wastewater discharges and their pollutant load, reducing thermal energy consumption and, potentially, processing cost. Various ways have been implemented for recycling the generated wastewater. Wastewater must be reused for the benefit of mankind. In this paper we propose a wastewater recycle control system to efficiently manage the wastewater and coordinate it among the industries and the government. Blockchain technology has been deployed for storing data and developing an incentive model to encourage wastewater reuse. Tokens are provided to industries in proportion to quantity and quality of reused wastewater. Rules for issue and trade of these tokens are written as a smart contract. Unfortunately, providing such incentives also provides a motive for tampering the data on which these tokens are awarded. Anomaly detection algorithms are used to detect the potential frauds which take place in the system upon IoT meter data tampering. The system uses IoT meters that measure volume of wastewater generated and reused, along with quality metrics such as pH, hardness and oil content. Multiple machine learning algorithms are used to detect tampering- polynomial regression, DBSCAN, autoencoders and LSTM networks. Their performance has then been compared. A first implementation of this system and an evaluation of the system's performance are also presented.","2019 10th International Conference on Computing, Communication and Networking Technologies (ICCCNT)",2019.0,10.1109/icccnt45670.2019.8944586,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
021376f4cce9b32e196f7e7a5a3136c47a18be17,https://www.semanticscholar.org/paper/021376f4cce9b32e196f7e7a5a3136c47a18be17,Inference of development activities from interaction with uninstrumented applications,,2018 IEEE/ACM 40th International Conference on Software Engineering (ICSE),2018.0,10.1007/s10664-017-9547-8,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
73520f778e5d6033a05963a5309c248781d54a8f,https://www.semanticscholar.org/paper/73520f778e5d6033a05963a5309c248781d54a8f,Robots in Space: Sharing the Sidewalk with Autonomous Delivery Vehicles,"Industrial robots originated in mid-Twentieth Century factories where they increased the efficiency of manufacturing. Their implementation was an extension of earlier industrial automation such as the introduction of Henry Ford’s mechanized assembly line in 1913. In Ford’s assembly line, a rope-and-pully system advanced each vehicle from one worker to the next allowing each worker to remain stationary. Half a century later, in 1961, the first robotic arm, created by Unimate, was introduced to auto manufacturing, which further increased efficiency. More recently, following advancements in artificial intelligence and sensor technology, industrial robots have acquired greater autonomy and transformed the logistics and delivery industries. Like Ford’s assembly line, and Unimate’s robotic arm, Amazon’s fulfillment center robots, originally designed by Kiva Robotics, reduced the daily steps workers must take. Instead of walking through aisles to stock warehouse shelves or retrieve products for distribution, workers remain stationary, and the robots bring the products to them. Today, with even greater autonomy than their predecessors, robots are migrating out of factories, warehouses, and fulfillment centers and into neighborhood streets, sidewalks, and skies. The technological advancements that allowed robots to automate private industrial spaces, such as machine learning and sophisticated sensors, now enable autonomous delivery robots (ADVs) to travel independently in the outside world and deliver packages, meals, groceries, and other retail purchases to people’s homes.<br><br>This article focuses on the evolution of ADVs used for “last-mile delivery,” the final step of the delivery process that ends at the customer’s door. It breaks ADVs down into four different categories: unmanned aerial vehicles (UAVs or “drones”); self-driving cars; autonomous delivery pods; and sidewalk delivery robots, which are sometimes called personal delivery robots (PDRs). The article describes the risks and benefits of deploying ADVs for last-mile delivery and analyzes the laws and federal agencies that regulate them. Last mile delivery is generally thought to be “the most expensive and time-consuming part of the shipping process” because it is the most personalized and unpredictable. Industry estimates suggest that last-mile delivery can account for up to 53 percent of total shipping costs. ADV manufacturers claim they can reduce delivery time, increase efficiency, cut costs, improve the consumer experience, decrease traffic congestion, reduce carbon emissions, assist seniors and people with disabilities who may have decreased mobility, and democratize access to logistics and delivery resources for small businesses allowing them to compete with large corporations. Critics claim ADVs may negatively impact public health by encouraging inactivity, obstructing roads and sidewalks and impairing the mobility of seniors and people with disabilities, and endangering public safety due to their potential to collide with people who are not agile enough to get out of the way. ADVs may also reduce the need for human delivery workers, cause noise pollution, violate people’s privacy, and represent the increasing privatization of public spaces such as sidewalks. Though all ADVs will be discussed, my focus is primarily on sidewalk delivery robots because they are the newest and fastest growing segment of the ADV industry, and they face the fewest legal and regulatory hurdles. Particular attention will be paid to the differences between the laws that regulate sidewalk delivery robots and the laws that govern other types of ADVs. The article concludes by drawing lessons from the regulation of UAVs and self-driving cars to propose legislation to regulate sidewalk delivery robots that will increase their safety and utility while limiting the privatization of public spaces.",SSRN Electronic Journal,2019.0,10.2139/ssrn.3347466,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
a5d85748055ff954e38c4c1f24138d614d4323fa,https://www.semanticscholar.org/paper/a5d85748055ff954e38c4c1f24138d614d4323fa,SmartDashCam: Automatic Live Calibration for DashCams,"Dashboard camera installations are becoming increasingly common due to various Advanced Driver Assistance Systems (ADAS) based services provided by them. Though deployed primarily for crash recordings, calibrating these cameras can allow them to measure real-world distances, which can enable a broad spectrum of ADAS applications such as lane-detection, safe driving distance estimation, collision prediction, and collision prevention Today, dashboard camera calibration is a tedious manual process that requires a trained professional who needs to use a known pattern (e.g., chessboard-like) at a calibrated distance. In this paper, we propose SmartDash-Cam, a system for automatic and live calibration of dashboard cameras which always ensures highly accurate calibration values. Smart-DashCam leverages collecting images of a large number of vehicles appearing in front of the camera and using their coarse geometric shapes to derive the calibration parameters. In sharp contrast to the manual process we are proposing the use of a large amount of data and machine learning techniques to arrive at calibration accuracies that are comparable to the manual process. SmartDashCam implemented using commodity dashboard cameras estimates realworld distances with mean errors of 5.7 % which closely rivals the 4.1% mean error obtained from traditional manual calibration using known patterns.",2019 18th ACM/IEEE International Conference on Information Processing in Sensor Networks (IPSN),2019.0,10.1145/3302506.3310397,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
90b72219e05e2761108f84dfde558517e99e9109,https://www.semanticscholar.org/paper/90b72219e05e2761108f84dfde558517e99e9109,Cognitive Infotainment Systems for Intelligent Vehicles,"Lately, the automotive world is witnessing a trend related to the deployment and extensive use of Intelligent Transportation Systems (ITS) inside vehicles. Such systems generate large volumes of real-time data that need to be managed, communicated, interpreted, aggregated, and analyzed in order to support real-time decision-making capabilities. Furthermore, automotive industries have introduced In-Vehicle-Infotainment (IVI) systems in supporting drivers/passengers with a varying set of functions. The main purpose of this study is to present an IVI cognitive functionality that automatically and dynamically proposes the optimum music genre to the drivers /users when they want to travel with their vehicles. The proposed system utilizes (i) driver/user’s profile data and his/her current situation, (ii) driver/user’s personal preferences, (iii) external environment information obtained from sensor measurements, and (iv) previous knowledge, turned into experience, in an automated manner. Knowledge is obtained through the exploitation of a machine learning technique based on the Bayesian networking principles. Indicative simulation results showcase the behavior of the aforementioned IVI cognitive functionality in proactively identifying the optimal music genre and accordingly notifying the driver/user.","2019 10th International Conference on Information, Intelligence, Systems and Applications (IISA)",2019.0,10.1109/IISA.2019.8900712,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
33000cfad01184a29c7ff6fab57299c6f4d4fd9f,https://www.semanticscholar.org/paper/33000cfad01184a29c7ff6fab57299c6f4d4fd9f,Intelligent drying systems,"The era of Artificial Intelligence (AI) has become a reality. Computer technologies have reached a level where they are not only for information processing, but also as substitute for human being on routine tasks, which can be described as a set of programable operations. In such cases, computer is clearly superior to humans, because it is not affected by emotions and human errors amongst its multifarious advantages. Moreover, computers can do some tasks beyond the scope of human intelligence, e.g., goal-oriented control and optimization. Drying with its inherent transient nature, uncertainty and still poorly understood coupling of transport processes and materials science is an ideal playground for AI. High variability in thermo-physical properties of materials and relatively low energy efficiency of drying make this game even more exciting. Currently, each drying application requires much preliminary research due to unknown process-product interactions, very specific to each product and drying technology. We believe that future development of drying technologies will focus on the need in embedding AI into drying systems. So far, the progress is still very limited. Most published research deals with “black-box” computing, such as artificial neural network (ANN), fuzzy logic (FL) or evolutionary algorithms (EA). These black-box approaches clearly demonstrate ability to manage hidden uncertainty and non-linearity of process/product parameters, but they do not contribute to the improvement of our knowledge about drying process and optimal control strategies. Recent advances in computer vision and machine learning have opened up new avenues for future development of drying systems. Unfortunately, their applications in drying technologies are still very limited. One possible reason is that they still have not demonstrated their unique advantages in simplifying research and development of new knowledge. We also should realize that this shift in paradigm would require critical revision of the philosophy of drying R&D. Drying community needs to become aware of advances in the AI space and applications to realworld systems. So far, control strategies in drying are limited to the control of process parameters. Our knowledge of product quality transformation and critical control points in the process of drying are still not included in dryer control strategy. AI in the form of computer vision and soft sensors will fill this gap, delivering real-time information about the product quality attributes. It is similar to intelligence of experienced cook, who controls the process of food preparation by observing visible changes in food. Unfortunately, our drying technologies are still an art of blind processing. Hence, the concept of intelligent observer is the first step in the development of intelligent drying systems. Sharing of digital data through IoT could provide the basis for the development of unified methodology. The next goal is the development of digital twins of commonly used drying systems. Machine learning could be used as an excellent tool to validate drying models. Eventually it will make possible model predictive control (MPC) and multiobjective optimization (MOO). The first step in this direction is already made. We believe that in the coming decades AI will find important industrial applications in various drying systems, allowing cost-effective production of consistently high quality products. This will require both academics and industry to keep up with the rapidly developing field of AI and technology-push innovations in industrial drying. References below provide a valuable introduction and exposure to the status and future of AI in drying technologies. We",Drying Technology,2020.0,10.1080/07373937.2019.1650452,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
023dc669070168a27ea6c422a7e61b7fe673f4de,https://www.semanticscholar.org/paper/023dc669070168a27ea6c422a7e61b7fe673f4de,A DID for Everything,"The decentralized identifer (DID) is a new and open standard type of globally unique identifer that ofers a model for lifetime-scope portable digital identity that does not depend on any centralized authority and that can never be taken away by third-parties [14]. DIDs are supported by the W3C community [14] and the Decentralized Identity Foundation (DIF) [16]. They are the ""atomic units"" of a new layer of decentralized identity infrastructure. However, DIDs can be extended from identifers for people to any entity, thus identifying everything. We can use DIDs to help us identify and manage objects, machines, or agents through their digital twins; we can expand them to locations, to events, and even to pure data objects, which we refer to as decentralized autonomic data (DAD) items [1][3]. The paper will present novel use-cases for DIDs and DADs and propose a new cryptographic data structure that 01/17/19 A DID for Everything 1.0 1 is a self-contained blockchain of DADs. This enables the verifcation of the provenance of a given data fow. It builds on a prior paper [1] and an associated reading [2]. DIDs are only the base layer of decentralized identity infrastructure. The next higher layer (where most of the value is unlocked) is verifable claims. This is the technical term for a digitally signed electronic data structure that conforms to the interoperability standards being developed by the W3C Verifable Credentials Working Group [15]. When a DID and hence DADs of the resultant data are extended to machines and autonomic data, the provenance chain of the data fow can provide the basis for verifable claims and attestations about the data fow as well as the basis for a reputation. WHY THIS MATTERS Today, the Internet is probably best described as a network comprised of all interconnected entities, traditionally referring to human users and computers. When we add connected entities and devices in the so-called Internet of Things (IoT), the number of addressable elements is in the tens of billions, with an estimate of 75 bn connected IoT devices in 2025 [4]. Software services, such as algorithms and bots, further extend this universe of identifable entities. The resulting combinatorics of possible connections between any given set of entities is an impossibly large number. Yet in today's user journeys or business environments, agents (whether human, machine, or software) increasingly need to communicate, access or transact with a diverse group of these interconnected objects to achieve their goals in both the digital and physical worlds. This requires a straightforward and ubiquitous method to address, verify, and connect these elements together. Defnition of Entity: Something that has a distinct and independent existence either in the real or the digital world. Examples of an entity are: • Living Organism • Physical Object • Locations or Events • Machines and Devices in the Internet of Things (IoT) • Digital Asset, Data Set, or Agent Human or object identities are stored in multiple centralised or federated systems such as government, ERP, IoT, or manufacturing systems. From the standpoint of cryptographic trust verifcation, each of these centralised authorities serves as its own root of trust. An entity trailing along a value chain is interacting with multiple systems. Consequently, a new actor in any given value chain has no method to independently verify credentials of a human or attributes of either a physical object or data item (provenance, audit trail). This results in the existence of complex validation, quality inspection, and paper trail processes, and enormous hidden trust overhead costs are added to all value chains and services. 01/17/19 A DID for Everything 1.0 2 To be a truly global solution, easy to use and still safe from hacking and sovereign interference, such a scheme must include: • preservation of privacy • security from tampering • reliable trust verifcation • assurance of risk • independence from any vendor-defned naming API • one-to-one mappable onto each entity. Therefore, a universal addressing, trust-verifcation system and associated interoperable protocol must be utilised, empowering every form of entity. Why it Matters for People Today when entities want their identities to be confrmed they transfer information such as a birth certifcate, physical address, or social security number to multiple third parties, who start to validate the same data in diferent contexts for KYC and authentication processes. The parties to which they sent that information retains it, meaning the data is out there in silos, creating risks in terms of data loss, privacy breaches, and use of inconsistent data and forcing companies that might not want to be in that position to store that information. It also enables businesses to harvest people's personal data for commercial purposes, which does not necessarily refect the intentions of the individual people. This situation results in big problems for humans such as broken health care records. Patients will need a universally addressable healthcare record system that is controlled by the patient itself, that consistently stores all relevant verifed health care data, and that is able to share this data with doctors that need to connect with it. To enable the doctors or algorithms, they need a data-fow provenance to verify the integrity, quality, or reputation of a healthcare record to decide on treatments and give the patient confdence about the proposed treatments. Why it Matters for Businesses Defnition of a Digital Twin: A digital twin is a digital representation of either a real-world or digital entity. A digital twin exists over the life-cycle of an entity from planning, manufacturing, testing, birth, and operations to decommissioning and reuse. The more past and present data are related and analysed, the more knowledge can be deployed to drive signifcant improvements on an individual entity or system level. It is estimated that by 2022 the IoT powered by digital twins will save consumers and businesses worth $1 trillion a year in asset maintenance [5]. The notion of digital twinning for objects, machines, and agents is becoming relevant to an increasing number of human services and Industry 4.0 use cases. This is the result of the growth in digital services, connections, and 01/17/19 A DID for Everything 1.0 3 data streams from the Internet of Things (IoT) devices that increasingly drive integration with machine learning algorithms, resulting in graph-type data chains for processing the IoT data streams. Today, digital twins are captured in siloed, proprietary IoT solutions by individual corporates that do not own the physical object over the life-cycle and even do not interact with parties using the object further down a value chain. Decentralized solutions are liberating the digital twins from silos and establishing more valuable and interoperable verifable attributes about entities and the data chains they connect with. Why it Matters for Objects, Machines and Agents There is no widely adopted authentication or verifcation systems in place to provide the equivalent of KYC (know-your-customer) for non-human entities, that is: KYA (know-your-agent), KYB (know-your-bot), KYM (know-your-machine), or KYO (know-your-object). In a world when objects and machines are connected with datastreams and intelligent agents that perform transaction on behalf of the entity, the number of agent-to-agent transactions will outgrow the number of human transactions by many orders. An agent transacting with another party can independently verify the identifer of the this party, its attributes, and the provenance of the data sets that are involved in the transaction. Digital twins of 3D-printed objects for safety critical parts such as a turbine of an airplane provide an important example. For these parts, it is important to have an precise audit trail about the 3D printing process to prove that the object was manufactured in accordance to stringent specifcations. The digital twin stores design, manufacturing, post processing, and quality-assurance data about the 3D-printed object. These data are coming from multiple systems resulting in a variety of data chains. With DIDs and DADs the integrity of the data chains can be verifed. The verifcation of the datachains and the underlying data results in important proofs about the provenance of 3D printed object. Why it Matters for the World The diverse application of decentralized identifers (DIDs) will have substantial infuences in broader applications on a global scale. The seamless provenance of physical objects or data items through any value chain has major implications on the risk and value properties of the processed data. Within any dynamic process, participating entities have substantial interest in the authenticity and trustworthiness in any individual step. Data that is accumulated with an unforgeable audit trail that references decentralized identifying information (Person, Device or any other Entity) for any transformation step holds greater value then it would have without such properties. Managing the sustainability of the commons requires mechanisms to value natural capital and to account for the externalities that arise from human activities. This should attribute extractions from and contributions to the commons by organisations, organisms, machines and information. We need to identify these entities and must 01/17/19 A DID for Everything 1.0 4 identify both positive and negative impacts these entities are having on the commons. Knowing what these impacts are enables us to count what matters and to put a value on what counts. The promise of a overarching prevalence through the broad use of DIDs also provides the key component for achieving the vision of a circular economy: a regenerative system in which resource input and waste, emission, and energy leakage are minimized by slowing, closing, and narrowing energy an",,2019.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
048ef025ecc18d46a3d53951d87b02c48f3b5fa7,https://www.semanticscholar.org/paper/048ef025ecc18d46a3d53951d87b02c48f3b5fa7,Modeling Stabilization of Crane-Induced Ship Motion With Gyroscopic Control Using the Moving Frame Method,"This paper presents a new method in multibody dynamics and applies it to the challenge of stabilizing ship motion induced by onboard crane operations. Norwegian industries are constantly assessing new technologies for more efficient and safer production in the aquacultural, renewable energy, and oil and gas industries. They share a common challenge to install new equipment and transfer personnel in a safe and controllable way between ships, fish farms, and oil platforms. This paper deploys the moving frame method (MFM) to analyze the motion induced by a crane, yet controlled by a gyroscopic inertial device. We represent the crane as a simple two-link system that transfers produce and equipment to and from barges. We analyze how an inertial flywheel can stabilize the ship during the transfer. Lie group theory and the work of Elie Cartan are the foundations of the MFM. This, together with a restriction on the variation of the angular velocity used in Hamilton's principle, enables an effective way of extracting the equations of motion for an open-loop system. Furthermore, this work displays the results in three-dimensional (3D) on cell phones. The long-term results of this work lead to a robust 3D active compensation method for loading/unloading operations offshore. Finally, the simplicity of the analysis anticipates the impending time of artificial intelligence when machines, equipped with onboard central processing units and internet protocol addresses, are empowered with learning modules to conduct their operations.",Journal of Computational and Nonlinear Dynamics,2019.0,10.1115/1.4042323,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
231080b60ab52521b52308df165a307723956511,https://www.semanticscholar.org/paper/231080b60ab52521b52308df165a307723956511,Review evolution of cellular manufacturing system’s approaches: Human resource planning method,"32 Fig. 1. A Flow Diagram of worker assignment in a Cellular Manufacturing System 1.1 Optimum Number of Workers Perhaps, finding the optimal number of workers is the main idea of investigating HRM in CMS. To determine optimal number of operators and part assignment, Park and Lee (1995) developed a 2-stage model while in first stage, a Taguchi method was used to determine system performance which was then used as objective function of assigning model. The idea of maximizing saving costs between operation and outsourcing costs was investigated by Heady (1997). But their model did not investigate operator level, training, hiring and firing costs. Norman et al. (2002) proposed a model to assign workers in manufacturing cells in order to maximize the system profit. Ertay and Ruan (2005) developed the idea of determining number of operators for maximizing number of outputs. For this purpose, using weighted input data, a data envelopment analysis (DEA) was applied. But in the proposed model, the same skill for all operators and machines was considered. 1.2 Promoting and Assigning Skilled Workers Since in real industries, operator’s skill are not same, so their outputs will not be the same. The idea of considering operator levels was investigated by Suer and Cedeño (1996). For this purpose, a mixed integer programming method was used to generate alternative operator levels and then another integer programming is employed to find the optimal operator assignments to the cells. Askin and Huang (1997) used integer programming for assigning workers to cells in order to determine a training program for employees. Aryanezhad et al. (2009) considered 3 skill levels for workers, which can be promoted through the planning horizon by training. Then a multi-period scheduling model was developed for simultaneous cell forming and worker assignning. Jannes et al. (2005) focused on assiginings workers to team works with the aims of minimizing training and assigning costs as well as maximizing labor flexibility. In the same year, Fitzpatrick and Askin (2005) argued that elemens of a good team formation is not limited to personnal skills and characteristics but technological and human interactions. Hence, by using pre-determined skill level measures, they tried to select workers and assign them to appropriate teams in cells to maximize team performance. Cesaní and Steudel (2005) focused on some factors on deployment of labors. Then, they focused on work sharing, work balancing and leveling the operator assignments (in presence of bottleneck operations). To prevent overloading and over-assigning of operators, Satoglu and Suresh (2009) used goal programming in a mathematical model where the objectives were minimizing over assignment of workers, cross training, hiring and firing costs. 1.3 Cross-trained workers Note that cross-trained workers are refered to those workers that are trained to perofrm more than one task. Determinining best sets of crosstraining workers can improve system performance with more flexibility. Bartholdi and Eisenstein (1996) found that by using large work cells with multiple workstations and workers, a stable partition and assignment of work will spontaneously emerge that cause balance production lines and maximize the production rate. Kleiner et al. (1998) assumed a typical skilled workers, which can perform multi tasks with multifunctional machines, in a a computer based system. Other attributes of the proposed model were included cell lead time, part travel distance, A. Delgoshaei et al. / Journal of Project Management 4 (2019) 33 process yield, operator classification and labor efficiency. In continue, Gel et al. (2000) showed that cross-trained workers can achieve higher performance than normal workers. As a different point of view, Askin and Huang (2001) studied the performance of greedy, beam search, and simulated annealing for a multi-objective optimization model for the formation of worker teams and a cross-training plan for cellular manufacturing. Olorunniwo and Udo (2002) showed that top management role and employee cross-trained have significant impact on the successful implementation of CMS. Kher (2000b) focused on training schemes that obtained by using cross-trained workers under learning, relearning, and attrition conditions. The idea of distributing skilled workers within teams and the degree of workforce belongs to Molleman and Slomp (1999) where they indicated the mentioned items have significant impact on system performance. Their findings showed that a uniform distribution of workforce skill resulted better system performance and consequently each worker should master the same number of tasks. Later, Slomp and Molleman (2000) compared four cross-training policies based on the workload of the bottleneck worker in both static and dynamic circumstances. The results confirmed that better team performance can be expected by using higher levels of cross-training workers. Jensen (2000) involved with staffing level and shop layouts in departmental, strictly and hybrid cell layouts. By changing number of employees in each department and considering 3 levels of workload balance and 2 labor transferring rules, they evaluated flow time, mean of tardiness and square mean of job tardiness. Li et al. (2012) focused on minimizing average salary while maximizing average of satisfaction. For this purpose they developed a multi-objective mixed integer programming to determine number of cross-trained labors and also tasks that must be assigned to the labors in flexible assembly cell layout. Another contribution of their research was considering worker’s satisfaction and task redundancy levels. 1.4 Dual Resource Problems Dual constraint resource problems refers the problems where scheduling parts on machines and workers simultaneously. Kher (2000a) has investigated training schemes obtained by cross-trained workers under learning, relearning, and attrition conditions. Kher et al. (1999) further conclude that the effectiveness of cross-training depends significantly on the existing forgetting rate of the workers. In addition, they remarked on the significant relationship between batch size and worker flexibility cross-training include variability, labor interaction, resources utilization and transition efficiency. Molleman & Slomp (1999)indicate that the distribution of skill within teams and the degree of workforce multi-functionality have a significant impact on system performance. Their findings indicate that a uniform workforce skill distribution resulted in better system performance. In other words, each worker should master the same number of tasks. Xu et al. (2011) provided a novel research in dual resource systems. Hamedi et al. (2012) developed a model where parts, machines and workers are grouped and assigned to the generated virtual cells simultaneously. In continue, the developed model is solved through a multi-objective Tabu Search algorithm to find near optimum solutions. 1.5 Uncertain Market Demands The idea considering dynamic part demands in HRM-CMS which can cause system imbalance is less developed. To solve this problem, Mahdavi et al. (2010) developed an a multi-mode planning model for assigning workers to cells in a reconfigurable CMS. In the proposed model, hiring, firing and also salary costs were considered as a part of total system costs. Another contribution of their model was considering available time for workers. As described in pervious section, Mahdavi et al. (2012) focused on inter-cellular movements of workers and parts while processing on specific machine. Min and Shin (1993) considered the skilled human resource as a part of cell forming process. Their objective was finding machine operators with similar expertise and skills to produce similar part families. Black and Schroer (1993) investigated a case where multi-functional operators can walk within cells to complete operations. They reported that using portable work stations can increase the output rate. Morris and Tersine (1994) examined the impact of labor and equipment in a dual constraint resource planning to compare the process layouts and cell layouts. Hyer et al. (1999) carried out a filed study considering 8 human factors in cell systems to find the importance of different human factors may influence the CMS. As a result they concluded that communication and tem work ranked as the most important factors in 34 utilizing the cell systems.Cesaní and Steudel (2005) developed a 2 phase frame work for worker assignment in CMS based on human resource factors. In the first phase, they performed an empirical investigation to find important factors that affect the labor flexibility. In second phase they used these factors to find optimum worker assignment in cells. The contribution of their research is finding balance between the operators’ workload, the level and type of machine sharing to increase the performance of cell based systems. Chakravorty and Hales (2004) provided a case study to survey the impact of worker assignment on system performance in a manufacturer and supplier of residential and light commercial building products. Afterward, Chakravorty and Hales (2008) reported that during early stage of working after forming cells, both technical failures and human resource errors are existed. However, after spending a period although the technical problems may reduce but the human resource problems are still exists which must be managed to reduce the harms. Yu et al. (2014) focused on minimizing total labor hour while maximizing throughput time of products in a line-cell conversion problem. They found that implementing the proposed method can increase the workforce motivation. Jannes Slomp et al. (2005) proposed a new method which considered labor grouping as well as machine part grouping during the cell forming process. The contribution of their research is focusing on balanced loads for workers, minimization of inter-ce",Journal of Project Management,2019.0,10.5267/j.jpm.2018.7.001,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
2ddbabff6103e10e8735c30f3338357d8e620219,https://www.semanticscholar.org/paper/2ddbabff6103e10e8735c30f3338357d8e620219,On the Similarities of Aircraft and Humans: Monitoring CPS with StreamLAB,"Modern medicine increasingly relies on medical cyber-physical systems for diagnosis and treatment of patients. Yet, misbehavior can have dire consequences raising the need for formal guarantees on the runtime behavior. Static verification would provide such guarantees, but is infeasible due to the complexity of the systems and the human physiology. In these cases, more light-weight verification techniques such as runtime monitoring are still applicable. The monitor analyzes a single execution of the system and raises an alarm as soon as it detects unexpected behavior. The shape of unexpected behavior is described in a formal specification language such as the one provided by the STREAMLAB framework, which can express complex real-time constraints. Yet the language is sufficiently restrictive to allow for static analyses determining an upper bound on the memory consumption. As a result, the underlying monitoring framework STREAMLAB can synthesize an embedded runtime monitor on an FPGA. The practicality of this approach is validated by current case studies on avionics. Moreover, the low memory and energy consumption indicate that a deployment on implantable devices is possible. In this abstract we thus showcase the practicality of STREAMLAB in the medical domain and suggest it as a suitable candidate for runtime monitoring on medical devices. Medical cyber-physical systems (MCPS) are ubiquitous, ranging from large magnetic resonance imaging scanners to implantable artificial cardiac pacemakers. While they constitute an indisputable advancement in diagnosis and treatment, even minor mistakes can lead to unforeseen ramifications. This can be seen in the Therac-251 incident. A quick handling of the machine by experienced technicians triggered a data race that lead to severe radiation poisoning in patients, causing multiple cases of debilitation and three deaths. Formal verification of the system’s control software could have prevented the incident. However, the complexity of such This work was partially supported by the European Research Council (ERC) Grant OSARES (No. 683300) and by the DFG as part of TRR 248 (No. 389792660) https://www.bugsnag.com/blog/bug-day-race-condition-therac-25 systems coupled with incomplete knowledge about the patient’s physiology and the exact workings of the human body render a static verification infeasible. Moreover, the control logic is often based on empirical data. As an example, consider the detection and subsequent treatment of seizures based on brain waves collected in an electroencephalogram (EEG). Precise identification and description of seizure patterns can be a hard task even for specialists who rely on their long experience. Yet, treatment of epilepsy requires manual tweaking of parameters until they fit the patient. The resulting controllers perform reasonably well, but are based on the doctor’s experience and thus hard to explain and reason about. Similarly, recent successes in whole-brain seizure detection with a trained classifier [1] indicate that machine learning can help in seizure detected. However, the resulting controller is similarly hard to explain, as per usual for machine learned components. Static verification requires insight into the decision logic, so such incomprehensible controllers exacerbate the verification process further. While control logic based on empirical data — either designed by an experienced doctor or machine learned — performs well in practice, it is neigh impossible to verify it. However, controllers that are simple enough to verify perform poorly in practice. The avionics industry faces a similar problem. Software based on machine learning performs extraordinarily well but cannot be verified statically. This hinders the certification of the aircraft and thus prevents it from getting a permission to fly. As a remedy, dynamic verification techniques such as runtime monitoring are commonly used. Here, a specification written in a language with formal semantics describes the desired behavior of a system on an abstract level. A monitor then assesses the system’s state based on sensor values. As soon as the specification is violated, an alarm is raised and the system can initiate mitigation measures such as switching to a less effective, verified controller. The monitor treats the control logic as a blackbox: it sees which decision",,2019.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
1a6e847a75917d9f6c3d3c3d3ff6bc8505f50b86,https://www.semanticscholar.org/paper/1a6e847a75917d9f6c3d3c3d3ff6bc8505f50b86,Vehicular Communications And Networks Architectures Protocols Operation And Deployment Woodhead Publishing Series In Electronic And Optical Materials,"A Trust-driven Privacy Architecture for Vehicular Ad-hoc NetworksIntelligent Vehicular Networks and CommunicationsEmerging Wireless Communication and Network TechnologiesVehicular NetworksBio-inspired Routing Protocols for Vehicular Ad-Hoc NetworksInformation Security of Intelligent Vehicles CommunicationVehicular Cloud Computing for Traffic Management and SystemsVehicular ad hoc NetworksMedical Internet of ThingsNetworking and Telecommunications: Concepts, Methodologies, Tools, and ApplicationsInterand Intra-Vehicle CommunicationsIT Convergence and Security 2017Optimisation Des Communications V2V Et V2I Dans Un Réseau de Véhicules OpéréRoadside Networks for Vehicular CommunicationsCloud and IoT-Based Vehicular Ad Hoc NetworksVehicle-To-Vehicle and Vehicle-To-Infrastructure CommunicationsBlockchainenabled Fog and Edge Computing: Concepts, Architectures and ApplicationsWireless Device-to-Device Communications and NetworksInterference in Vehicle-to-vehicle Communication NetworksWireless Networks and Industrial IoTGreen Computing in Network SecurityVehicular Communications for Smart Cars5G-Enabled Vehicular Communications and NetworkingAdvances in Delay-Tolerant Networks (DTNs)Rail Vehicle MechatronicsQuality of Service Architectures for Wireless Networks: Performance Metrics and ManagementCommunication Technologies for VehiclesVehicular NetworkingVehicular-2-X Communication断言Telematics Communication Technologies and Vehicular Networks: Wireless Architectures and ApplicationsVehicular TechnologiesCharacterization, Avoidance and Repair of Packet Collisions in Inter-Vehicle Communication NetworksIntelligent Transportation SystemsResearch Anthology on Architectures, Frameworks, and Integration Strategies for Distributed and Cloud ComputingRoadside Networks for Vehicular Communications: Architectures, Applications, and Test FieldsVehicular-2-X CommunicationEurope and MENA Cooperation Advances in Information and Communication TechnologiesVehicular Communications and NetworksCapacity Analysis of Vehicular Communication Networks During the last 15 years, the interest in vehicular communication has grown, especially in the automotive industry. Due to the envisioned mass market, projects focusing on Car-to-X communication experience high public visibility. This book presents vehicular communication in a broader perspective that includes more than just its application to the automotive industry. It provides, researchers, engineers, decision makers and graduate students in wireless communications with an introduction to vehicular communication focussing on car-to-x and train-based systems. Emphasizes important perspectives of vehicular communication including market area, application areas, and standardization issues as well as selected topics featuring aspects of developing, prototyping, and testing vehicular communication systems. Supports the reader in understanding common characteristics and differences between the various application areas of vehicular communication. Offers both an overview of the application area and an in-depth discussion of key technologies in these areas. Written by a wide range of experts in the field.Universal vehicular communication promises many improvements in terms of acdent avoidance and mitigation, better utilization of roads and resources such as time and fuel, and new opportunities for infotainment applications. However, before widespread acceptance, vehicular communication must meet challenges comparable to the trouble and disbelief that accompanied the introduction of traf c lights back then. The rst traf c light was installed in 1868 in London to signal railway, but only later, in 1912, was invented the rst red-green electric traf c light. And roughly 50 years after the rst traf c light, in 1920, the rst four-way traf c signal comparable to our today’s traf c lights was introduced. The introduction of traf c signals was necessary after automobiles soon became prevalent once the rst car in history, actually a wooden motorcycle, was constructed in 1885. Soon, the scene became complicated, requiring the introduction of the “right-of-way” philosophy and later on the very rst traf c light. In the same way the traf c light was a necessary mean to regulate the beginning of the automotive life and to protect drivers, passengers, as well as pedestrians and other inhabitants of the road infrastructure, vehicular communication is necessary to accommodate the further growth of traf c volume and to signi cantly reduce the number of accidents.This is the second volume of proceedings including selected papers from the International Conference on IT Convergence and Security (ICITCS) 2017, presenting a snapshot of the latest issues encountered in the field. It explores how IT convergence and security issues are core to most current research, industrial and commercial activities and consists of contributions covering topics including machine learning & deep learning, communication and signal processing, computer vision and applications, future network technology, artificial intelligence and robotics. ICITCS 2017 is the latest in a series of highly successful",,2017.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10108cb0a7eaac31459fb7d424729fd1406c43bf,https://www.semanticscholar.org/paper/10108cb0a7eaac31459fb7d424729fd1406c43bf,Alternatively Deployed Artificial Lift System for Deepwater Subsea Operations,"
 In the Gulf of Mexico, the rapid pressure depletion and reservoir depth of the Lower Tertiary intervals lead to low oil recovery. A high-reliability, through-tubing subsea electrical submersible pump (ESP) system that takes an integrated approach to production optimization will enable producers to cost-effectively extract more hydrocarbons from the increasingly challenging reservoirs in today's subsea assets. The potential increase in production depends on the maximum drawdown pressure limitations of both well casing design and rock strength. ESPs in deepwater fields are also considered to be an enhancer rather than an enabler by extending the production plateau 5 to 8 years after initial well/field startup with natural flow and seabed boosting. Hence, a robust ESP system that can be installed and operated a few years after field startup without a workover for replacing the upper completions. A robust, reliable ESP would unlock additional value to deepwater operators by delaying CAPEX and eliminating ESP failures, such as degradation of components due to high-pressure/high-temperature (HP/HT) cycling, during the first few years of nonoperation.
 Designing ESPs for deepwater application is a multidisciplinary challenge and needs to be approached from a full system-reliability standpoint rather than improvements to the ESP hardware alone. Implementation of ESPs in deep water requires both upfront planning at a full-system level and high degree of flexibility for installation, deployment, and retrieval. Finally, because the impact of an unplanned ESP failure is significantly detrimental to project economics, efforts to improve robustness of the ESP hardware must be complemented with automation of ESP operation to reduce or eliminate operator-induced failures. Recent industry improvements in machine learning and predictive analytics need to be leveraged to implement condition-based monitoring of ESPs to better anticipate failures and plan for replacements and/or adjustments to extend the life of degraded units.
 A collaborative project was undertaken to develop the concept of an alternatively deployed through-tubing ESP (TTESP) system targeted for deepwater subsea operations. The goal was to reduce intervention costs, which, together with ESP run life, are the primary factors influencing the economics of subsea ESPs, including conventionally deployed through-tubing ESPs. The project scope encompassed the downhole hardware, from immediately below the subsea tree through the upper completion, as well as deployment and retrieval equipment and methodology.
 Economic analyses of subsea fields were conducted to identify the factors contributing to intervention costs so that alternatives could be developed. Multiple concepts were evaluated, and the proof-of-concept system was selected based on superior economic return compared with the baseline. During this proof-of-concept phase, significant testing of key technologies was conducted. The studies showed that conventional intervention vessels and methods will not reduce the intervention costs associated with TTESPs. Lighter vessels together with technologies and methods that minimize intervention time and frequency—and, consequently, reservoir damage and deferred production—are the answer. Eliminating the wait for an available offshore rig is also a key factor in improving overall production economics. The proposed alternatively deployed TTESP system and its associated deployment methodology could reduce the intervention time by half and eliminate reservoir damage. This unconventional deployment could be conducted with lighter service vessels, further reducing intervention costs.","Day 2 Tue, May 14, 2019",2019.0,10.2118/194400-MS,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
5caa35987b5df145a5bdc8dd4766dbeff952ebe2,https://www.semanticscholar.org/paper/5caa35987b5df145a5bdc8dd4766dbeff952ebe2,Research on Application and Development of Financial Big Data,"This paper discuss the basic thinking, methods and tools of software engineering, masters the financial business knowledge, the analysis and design theory and methods of financial information systems, and has the ability to analyze, design, implement and maintain financial information systems, and can be used in financial applications. IT companies such as development companies and financial information system providers engage in the analysis, design and implementation of software, or engage in the analysis, design, implementation, maintenance and management of financial information systems in the IT departments of various financial institutions such as banks, securities and insurance. A financial information-based composite software talent with a solid professional foundation, broad knowledge, and the ability to adapt to the future development of information technology. Specifically, it is reflected in four aspects: knowledge system, professional skills, project experience and comprehensive quality: Introduction On July 1, 2017, the General Office of the State Council issued the “Opinions on Strengthening the Service and Supervision of Market Subjects by Using Big Data”; on July 4, the State Council issued the “Guiding Opinions on Actively Promoting the “Internet+” Action” On September 5, the State Council issued the ""Outline for the Promotion of Big Data Development."" The intensive introduction of these heavy documents marks the official establishment of China's big data strategic deployment and top-level design. Benefiting from the rapid expansion of the big data market, the demand for related IT support has exploded. Among them, enterprises that provide big data infrastructure, big data software technology services, and industry big data content consulting services have brought unprecedented Customer group. IDC predicts that by 2020, the company's expenditure based on big data computing and analysis platform will exceed 500 billion US dollars, and the compound growth rate will reach 34.1% in the next 5 years; in the next 3 to 5 years, China needs 1.8 million data talents, but currently only About 300,000 people. At the same time, China's colleges and universities in cloud computing, data science and other majors are still in their infancy, and the talents cultivated each year are far from meeting the needs of the industry. Therefore, it is imperative to open a big data major and accelerate the cultivation of talents. The financial industry is the industry that relies most on data and is the easiest to realize data. In recent years, emerging financial institutions such as consumer loans and P2P are the products of the combination of big data technology and finance. At present, the demand for big data talents in finance is extremely strong in China. Only Internet finance is one year, and the growth rate is 3-5 times per year. It is generally believed that there will be a gap of 1 million talents in Internet finance, and the most lacking is big data risk control talents, including data mining and statistical modeling talents from primary to advanced. Core and Featured Courses Cloud Computing and Introduction to Big Data As an introductory course in the direction of this major, this course introduces students to the concepts, technologies and applications related to cloud computing and big data, and enables students to establish a preliminary understanding of the relevant knowledge, technology and development prospects of the profession, as a guide for the follow-up course. Distributed Computing Framework Foundation As the foundation and core technology course of this major, this course introduces students to the basic concepts, installation and configuration of the Hadoop distributed computing system, distributed programming model (Map/Reduce), distributed file system (HDFS), and related scheduling. , monitoring and maintenance tools enable students to build a basic understanding of distributed computing systems, master the primary distributed application design and implementation methods, and lay the theoretical and practical foundation for subsequent in-depth courses. Distributed Database Management and Development As a core technical course in this major, this course introduces students to the basic concepts of distributed databases, installation and configuration, management and maintenance, data access and development. The course focuses on NoSQL databases such as HBase, MongoDB, Redis, etc., and describes their use and development in a distributed environment. To enable students to establish a basic understanding of distributed databases, master the primary distributed database application system design and development methods, and lay the theoretical and practical foundation for the subsequent in-depth courses. Distributed Computing Framework Component Technology As a core advanced course in this major, this course introduces students to mainstream components on the Hadoop distributed computing platform, including Hive, Pig, Sqoop, Flume, Kafka, Zookeeper and more. Enable students to have a complete Hadoop ecosystem-based design and implementation of big data applications. Real-time Calculation and Memory Calculation As a core advanced course in this major, this course introduces students to high-performance distributed computing frameworks, including Storm and Spark, as a more powerful alternative to the Hadoop framework. Data Visualization Technology As an elective course in this major, this course introduces students to the basics of data visualization and the design and use of platforms and development tools, including Excel, Reporting Services, Chart.js, D3.js, Tableau, etc. Through this course, students will be able to present the results of big data processing in an efficient, flexible and friendly manner. Data Statistics and Analysis As a core advanced course in this major, this course introduces students to statistical analysis techniques based on Python and R. Including data file editing and finishing, basic statistical analysis, parameter estimation and hypothesis testing, non-parametric testing, analysis of variance, correlation analysis, regression analysis, cluster analysis, discriminant analysis, factor analysis, correspondence analysis, reliability analysis, survival Analysis, time series analysis, and the drawing of statistical graphs enable students to master the processing and analysis methods of typical industry business data. Knowledge System Mathematical basis: Including calculus, linear algebra, probability statistics, numerical analysis, etc. IT foundation: Including operating systems, networks, databases, software engineering, programming techniques, data structures and algorithms, etc. Knowledge base in the financial sector. Including international finance, marketing, insurance, securities investment, etc. Professional Skills Database system management and development: MySQL, MongoDB, Redis, HBase, etc. Big Data Application Development Language: Java as the core, supplemented by Python, Scala, R, etc. Construction, configuration, development and deployment of big data processing frameworks: Hadoop, Storm, Spark, etc. Use of data analysis and presentation tools: reporting tools, D3.js, etc. Project Experience Familiar with enterprise software project life cycle, development process, specification, etc. Understand and implement software quality requirements: performance, security, scalability, maintainability, reliability, etc. Understand the financial industry: industry background, business model, market characteristics, and how data and IT systems are used in the financial industry Comprehensive Quality Good professional basic qualities: document writing, presentation reporting, business communication, etc. Strong learning ability and study habits, has a certain degree of microinnovation, data awareness Course Settings Table The professional competence-course structure derivation process mainly includes two stages: “computation ability theme” and “capability-curriculum structure transformation”. The main process of the first phase of the ""computational power theme"" is as follows: Figure 1. The data analysis process of ""ability topic calculation. Hadoop Big Data Integrated Experiment System This experimental system is designed to provide students with a complete set of Hadoop and its environment, design, development, monitoring, maintenance tools, software and services. With this experimental system, the experimental and training environment requirements of the core technology courses of this major can be met. This experimental system is divided into two major components: A virtual lab environment for students to learn big data. The environment is carried out by means of the aforementioned virtualized desktop teaching system, and the network administrator configures the big data learning virtual machine in advance for the students to use. The real environment for research or large-scale case presentations. This environment is carried out through several servers. Main Function A. Basic platform: The basic platform for big data storage and processing, which can realize the storage and management of massive data, support common components of platforms such as Hive, Impala, Pig, Spark, and Yarn, and provide support for data analysis services on the platform. These common components increase the ease of use of platform data, making data manipulation and data analysis easier to use, saving labor and reducing labor time. B. Data integration: support the unified storage of massive structured data, semi-structured data, and unstructured data, deepen the expansion of enterprise intelligence and service capabilities, and improve the decision-making level of enterprises. We can use enterprise-level data ETL tools or open source ETL tools. For example, Flume, Sqoop, Kafka, etc., integrate externally structured, semi-structured and unstructured data into big data platforms. Through this pla","DEStech Transactions on Social Science, Education and Human Science",2019.0,10.12783/dtssehs/aems2018/28012,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
6273eecc2aa48d93804ee26ed745b91b59885e42,https://www.semanticscholar.org/paper/6273eecc2aa48d93804ee26ed745b91b59885e42,Bayesian Filtering Methods For Dynamic System Monitoring and Control,"of a dissertation at the University of Miami. Dissertation supervised by Professor Ramin Moghaddas. No. of pages in text. (175) Real-time system monitoring and control represent two of the most important issues that characterize modern industries in critical areas of civilian and military interest, including power grid, energy, healthcare, aerospace, and infrastructure. During the past decade, there has been a rapid development of robust dynamic system monitoring and control methods for fault diagnosis and failure prognosis. Among various monitoring and control policies, condition-based maintenance (CBM) has been studied by many researchers due to its ability to enable a large amount of monitoring data for real-time diagnostics and prognostics. A considerable amount of literature has been published on the subject, providing a large volume of dynamic system control methods. Previously published studies are limited by assumptions that can generally be distinguished into three main categories: i) predefined system failure thresholds, ii) simplified latent dynamics, and iii) unrealistic parametric forms that describe the evolution of system dynamics through time. This thesis provides an array of solution approaches that overcomes the aforementioned assumptions in a smart and effective way by introducing novel quantitative frameworks for real-time monitoring, control, and decision-making for dynamic systems. The proposed frameworks are categorized into two main phases of a comprehensive framework. The first phase contains two original Bayesian filtering methods for condition monitoring and control of systems with either linear or non-linear degradation dynamics. The former is designed only for systems with linear latent and observable dynamics and utilizes Kalman filtering for state-parameter inference. It considers a failure process that is purely stochastic and is based on logistic regression. This process is directly affected by the latent system dynamics, therefore avoiding the need for a priori failure thresholds. The latter takes into consideration multiple levels of system dynamics that evolve either linearly or non-linearly. A hybrid particle filter is developed for state-parameter inference, while an Extreme Learning Machine artificial neural network is utilized to relate sensor observations to latent system dynamics. Both frameworks are tested and validated on synthetic and real-world time-series datasets. The second phase of this thesis introduces an original method for optimal control and decision-making that employs Bayesian filtering-based deep reinforcement learning with fully stochastic environments. Sets of deep reinforcement learning agents were trained to develop control policies. Bayesian filtering methods from the first phase were utilized to provide environment states that use the estimates from latent system dynamics. This method is used in two different applications for maintenance cost minimization and estimating remaining useful life of a system under condition monitoring. Results obtained from applying the framework on simulated and realworld time-series data suggest that the proposed Bayesian filtering-based deep reinforcement learning algorithm can be trained even with limited data, which can be useful for real-time control and decision making for many dynamic systems. Dedicated to my fantastic parents, Gordios and Vasiliki, for providing me with every opportunity and encouragement in life",,2019.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
8f1644359a2679dc961392b28e3fae6df2ad48cf,https://www.semanticscholar.org/paper/8f1644359a2679dc961392b28e3fae6df2ad48cf,The Subsea Pipeline Inspection Paradigm - What Next ?,"
 Shell in the UK has a vast network of more than 200 pipelines & umbilicals covering some 3000+ kilometres. Historically, Shell has executed Side Scan Sonar Surveys along these pipelines using a Remotely Operated Towed Vehicle and subsequently followed up with ROV based surveys & inspections. However, in 2018, the respective Geomatics & Subsea Maintenance / Pipelines Departments decided to take advantage of new & emerging innovative technologies and compiled a minimal technical scope & tender document to tap into the latest that the market could offer. Consequently, Shell UK awarded DeepOcean (Norway) with a contract for their ""Fast Digital Imaging Service"" and embarked on a 45 day survey campaign. In 2019, the same subsea inspection project will be executed once again and the lessons learned ought to inspire and excite many different disciplines and communities, both internally within Shell and externally e.g OGA - Oil & Gas Authority & other valued stakeholders.
 The paper highlights the key technologies that were deployed and how the new deliverables & business insights take us down the road to Digitalisation including scope for future Machine Learning & Automation processes. Challenges arising from the acquisition and managing the associated data sets shall also be discussed. The speaker will spark dialogue at the end by asking the respective communities how robotics and artificial intelligence will change the industry landscape ?","Day 1 Tue, September 03, 2019",2019.0,10.2118/195755-MS,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
036cc3918942bf59f2d3ea6364e7c2a5f0ecbe94,https://www.semanticscholar.org/paper/036cc3918942bf59f2d3ea6364e7c2a5f0ecbe94,"Frontiers of WWW Research and Development - APWeb 2006, 8th Asia-Pacific Web Conference, Harbin, China, January 16-18, 2006, Proceedings",,APWeb,2006.0,10.1007/11610113,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
972b20f1a231cb991f43b5a8dc99b8892c6a3db9,https://www.semanticscholar.org/paper/972b20f1a231cb991f43b5a8dc99b8892c6a3db9,Stress Detection for Keystroke Dynamics,"Background. Stress can profoundly affect human behavior. Critical-infrastructure operators (e.g., at nuclear power plants) may make more errors when overstressed; malicious insiders may experience stress while engaging in rogue behavior; and chronic stress has deleterious effects on mental and physical health. If stress could be detected unobtrusively, without requiring special equipment, remedies to these situations could be undertaken. In this study a common computer keyboard and everyday typing are the primary instruments for detecting stress. Aim. The goal of this dissertation is to detect stress via keystroke dynamics – the analysis of a user’s typing rhythms – and to detect the changes to those rhythms concomitant with stress. Additionally, we pinpoint markers for stress (e.g., a 10% increase in typing speed), analogous to the antigens used as markers for blood type. We seek markers that are universal across all typists, as well as markers that apply only to groups or clusters of typists, or even only to individual typists. Data. Five types of data were collected from 116 subjects: (1) demographic data, which can reveal factors (e.g., gender) that influence subjects’ reactions to stress; (2) psychological data, which capture a subject’s general susceptibility to stress and anxiety, as well as his/her current stress state; (3) physiological data (e.g., heart-rate variability and blood pressure) that permit an objective and independent assessment of a subject’s stress level; (4) self-report data, consisting of subjective self-reports regarding the subject’s stress, anxiety, and workload levels; and (5) typing data from subjects, in both neutral and stressed states, measured in terms of keystroke timings – hold and latency times – and typographical errors. Differences in typing rhythms between neutral and stressed states were examined to seek specific markers for stress. Method. An ABA, single-subject design was used, in which subjects act as their own controls. Each subject provided 80 typing samples in each of three conditions: (A) baseline/neutral, (B) induced stress, and (A) post-stress return/recovery-to-baseline. Physiological measures were analyzed to ascertain the subject’s stress level when providing each sample. Typing data were analyzed, using a variety of statistical and machine learning techniques, to elucidate markers of stress. Clustering techniques (e.g., K-means) were also employed to detect groups of users whose responses to stress are similar. Results. Our stressor paradigm was effective for all 116 subjects, as confirmed through analysis of physiological and self-report data. We were able to identify markers for stress within each subject; i.e., we can discriminate between neutral and stressed typing when examining any subject individually. However, despite our best attempts, and the use of state-of-the-art machine learning techniques, we were not able to identify universal markers for stress, across subjects, nor were we able to identify clusters of subjects whose stress responses were similar. Subjects’ stress responses, in typing data, appear to be highly individualized. Consequently, effective deployment in a realworld environment may require an approach similar to that taken in personalized medicine.",,2018.0,10.1184/R1/6723227.V3,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
ff64088e36b4f7b6048ffd5519b9dead00f8023a,https://www.semanticscholar.org/paper/ff64088e36b4f7b6048ffd5519b9dead00f8023a,On the adoption and impact of predictive analytics for server incident reduction,"The Predictive Analytics for Server Incident Reduction (PASIR) solution developed at IBM has been broadly deployed to 130 IT environments since the beginning of 2014. The infrastructures of these IT environments, pertaining to various industries around the world, are serviced by IBM support groups. More specifically, incidents occurring on servers, including the descriptions of the problems, are reported into a ticket management system. These tickets are then resolved by the assigned support teams, which record in the system the resolution steps taken. PASIR, first classifies the incident tickets of an IT environment to identify high-impact incidents describing server unavailability and performance degradation issues by using ticket descriptions and resolutions. Second, the occurrence of these high-impact tickets is correlated with server properties and utilization measures to identify troubled server configurations and prescribe improvement actions through multivariate analysis. In this paper, we present the findings from deploying our two-step machine learning model in the field. In particular, we describe the PASIR methodology, from ticket classification to the recommendation of modernization actions. We also assess the process of manual ticket labeling and the impact of noisy input data on our automatic classifier, and we demonstrate the model effectiveness by comparing predictions on the impact of prescriptive actions with actual system improvements.",IBM J. Res. Dev.,2017.0,10.1147/JRD.2016.2631400,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
75fdffc32af1b97a4f19812fa6e14266228e8588,https://www.semanticscholar.org/paper/75fdffc32af1b97a4f19812fa6e14266228e8588,Modeling Stablization of Crane and Ship by Gyroscopic Control Using the Moving Frame Method,"Norwegian industries are constantly assessing new technologies and methods for more efficient and safer production in the aqua cultural, renewable energy, and oil and gas industries. These Norwegian offshore industries share a common challenge: to install new equipment and transport personnel in a safe and controllable way between ships, farms and platforms. This paper deploys the Moving Frame Method (MFM) to analyze the motion induced by a crane and controlled by a gyroscopic inertial device mounted on a ship. The crane is a simple two-link system that transfers produce and equipment to and from barges. An inertial flywheel — a gyroscope — is used to stabilize the barge during transfer. The MFM describes the dynamics of the system using modern mathematics. Lie group theory and Cartan’s moving frames are the foundation of this new approach to engineering dynamics. This, together with a restriction on the variation of the angular velocity used in Hamilton’s principle, enables an effective way of extracting the equations of motion. This project extends previous work. It accounts for the dual effect of both the crane and the stabilizing inertial device. Furthermore, this work allows for buoyancy and motor induced torques. Furthermore, this work displays the results in 3D on cell phones. The long-term results of this work leads to a robust 3D active compensation method for loading/unloading operations offshore. Finally, the interactivity between the crane and the stabilizing gyro anticipates the impending time of artificial intelligence when machines, equipped with on-board CPU’s and IP addresses, are empowered with learning modules to conduct their operations.","Volume 4B: Dynamics, Vibration, and Control",2018.0,10.1115/IMECE2018-86165,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
ba2e6bdc737a199db8b70bf2a393b54c5fdecd41,https://www.semanticscholar.org/paper/ba2e6bdc737a199db8b70bf2a393b54c5fdecd41,The road to enterprise Artificial Intelligence : a case studies driven exploration,"Increases in the volume of data and the availability of compute power have driven a number of advancements in the field of Artificial Intelligence (AI), and AI technologies and applications are getting a flood of publicity in the media. While four in five executives agree that AI is a strategic opportunity for their organization, only about one in five has incorporated AI in some offerings or processes, and only one in 20 has extensively incorporated AI in their offerings or processes. There is a gap between expectation and action, and we are still in the early days of enterprise AI adoption. This thesis explores the path enterprises need to take to close this gap and to build an enterprise AI capability, thereby realizing the full value of this disruptive technology. Through a literature review it proposes a seven component holistic framework that can guide enterprises through this journey. The framework is more ‘wide than deep’, and it is supplemented with five case studies that take deep dives into the real life journeys of enterprises from different industries. These stories provide a vivid illustration of best practices and challenges. The case studies cover Danske Bank fighting financial fraud with deep learning, Deutsche Telekom improving customer service with an intelligent digital assistant, General Electric deploying machine learning applications for monitoring workflows in the Industrial Internet of Things, General Mills automating insights for marketers, and Kaiser Permanente using state of the art Natural Language Processing techniques on unstructured triage notes to improve patient flow forecasting. Learnings from the case studies are synthesized into recommendations to aid practitioners on the road to enterprise Artificial Intelligence. Thesis Supervisor: Jeanne Ross Title: Principal Research Scientist, Center for Information Systems Research",,2018.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
d31a7209656b2e5529ddae3d629fcc5dbbcc57b1,https://www.semanticscholar.org/paper/d31a7209656b2e5529ddae3d629fcc5dbbcc57b1,Strategies for Adopting Additive Manufacturing Technology Into Business Models,"Strategies for Adopting Additive Manufacturing Technology Into Business Models by Robert Martens MS, University of Glamorgan, 2007 MBA, Keele University, 2006 Doctoral Study Submitted in Partial Fulfillment of the Requirements for the Degree of Doctor of Business Administration Walden University August 2018 Abstract Additive manufacturing (AM), also called 3-dimensional printing (3DP), emerged as a disruptive technology affecting multiple organizations’ business models and supply chains and endangering incumbents’ financial health, or even rendering them obsolete. The world market for products created by AM has increased more than 25% year over year. Using Christensen’s theory of disruptive innovation as a conceptual framework, theAdditive manufacturing (AM), also called 3-dimensional printing (3DP), emerged as a disruptive technology affecting multiple organizations’ business models and supply chains and endangering incumbents’ financial health, or even rendering them obsolete. The world market for products created by AM has increased more than 25% year over year. Using Christensen’s theory of disruptive innovation as a conceptual framework, the purpose of this multiple case study was to explore the successful strategies that 4 individual managers, 1 at each of 4 different light and high-tech manufacturing companies in the Netherlands, used to adopt AM technology into their business models. Participant firms originated from 3 provinces and included a value-added logistics service provider and 3 machine shops serving various industries, including the automotive and medical sectors. Data were collected through semistructured interviews, member checking, and analysis of company documents that provided information about the adoption of 3DP into business models. Using Yin’s 5-step data analysis approach, data were compiled, disassembled, reassembled, interpreted, and concluded until 3 major themes emerged: identify business opportunities for AM technology, experiment with AM technology, and embed AM technology. Because of the design freedom the use of AM enables, in combination with its environmental efficiency, the implications for positive social change include possibilities for increasing local employment, improving the environment, and enhancing healthcare for the prosperity of local and global citizens by providing potential solutions that managers could use to deploy AM technology. Strategies for Adopting Additive Manufacturing Technology into Business Models by Robert Martens MBA, University of Keele, 2006 MS, University of Glamorgan, 2007 Doctoral Study Submitted in Partial Fulfillment of the Requirements for the Degree of Doctor of Business Administration Walden University August 2018 Dedication I dedicate this work to my family, who believed in me during this quest. In particular, I wish to thank my father, Dominicus Martens, for telling me about the many journeys he made across the world, for giving me the appreciation for mechanical engineering and procurement, and for showing me technology and business go well together. To my mother, Cornelia, for gifting me with stamina, and an inquisitive and critical mind. I want to thank my wife, Lu Dongmei, for her constant encouragement during this research and belief in my abilities to achieve this goal. To my children Niek, Louis, Max, and Franc: thanks for your support along this journey; I hope I have shown you the importance of goal setting and dedication. Never stop learning. Acknowledgments I would like to acknowledge my family, friends, colleagues, classmates, and Walden faculty for their support during this doctoral study project. To my coach while in China, Lynda Aurora, who pushed me to pursue this dream. In particular, I wish to thank my chair, Dr. Susan Fan, for her guidance, Dr. Charles Needham, for his professionalism, and Dr. Lisa Kangas, for her eye to detail, towards the completion of quality research. To my Walden classmates with whom I shared many ups and downs: thank you for your time, support, motivation, and inspiration.",,2018.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
2ff29ea3641c45e90157a04bd47ed6d497ccac14,https://www.semanticscholar.org/paper/2ff29ea3641c45e90157a04bd47ed6d497ccac14,Implementation of Data Mining from Social Media for Improved Public Health Care,"To improve public health care outcomes with reduced cost, this research proposed a framework which focuses on the positive and negative symptoms of illnesses and the side effects of treatments. However, previous studies have been limited as they neither identified influential users nor discussed how to model forms of relationships that affect network dynamics and determine the accurate ranking of certain end user’s feedbacks. In this research, a two-step analysis framework is proposed as the system. In the first level, the system utilized exploratory analysis and clustered users and their useful feedbacks through self-organizing maps (SOM). In the second level, the system developed three lists of negative and positive feedbacks and treatment symptoms caused by implanting the SOM that considered accurate ranking by calculating the frequency of each term of interests. The feasibility of the proposed solution is confirmed as performance evaluations of the system in terms of computational costs. The results showed that these solutions are reasonable computational costs relative to memory and processor usage. Keywords—Data mining; social media; medical data; end user feedbacks; positive terms; negative terms; symptoms I. RESEARCH MOTIVATIONS AND BACKGROUND Data mining from social media recently gained the attention of many important businesses and industries. Data mining is empowered by the recent advances in big data analysis as well as the network modeling of social media forums and websites, which are integrated to achieve knowledge discovery solutions and to extract useful information from various fields [1]. The health care industry is one of the most important fields that can be significantly enhanced by modern data mining techniques that allow the discovery of certain trend patterns as a product of the social media feedbacks generated dynamically from the experiences and opinions of end users [2]. These techniques can be applied to drug feedbacks in social media which can help manufacturers continuously enhance their products at reducedcosts. Successful data mining from social media can result in numerous benefits for business owners and manufacturers [3]; however, a number of challenges should be addressed first to reach the acceptable level and wide deployment of this new technology [4]. In addition, classical data mining techniques and algorithms should be empowered with an intelligent pattern recognition tool to predict and visualize the common trends of the data in general. Moreover, it should weigh certain descriptions or feedbacks based on their frequency and eliminate some neutral words as a filtration technique in the preprocessing stage. Different online forums contain feedbacks that should be network-modelled for a more convenient analysis. All these challenges are the main inspirations that motivated this research. The main contributions of this work can be considered as developing a feasible text data-mining solution that can partition different users with certain ID by modeling their existence in different web forums. Furthermore, the accurate clustering of negative and positive feedbacks and the visualization of the overall positive or negative feedback trends in a reasonable computational cost solution is another benefit from this research. The basic concepts are explained in this section as a background of the research field and the proposed solutions. First, the processes of data collection and mining are considered challenging tasks given a large number of networks studied, and this requires a complex representation of the social network structure. The complexity of such structure is derived from network density and levels of the parents and nodes clustering social media contents. Network clustering involves complex, big, and parallel data processing to cover the analysis of each networks’ nodes representing certain user communities. A part of the network usually as small as the sample size is used for data collection. Future Technologies Conference (FTC) 2017 29-30 November 2017| Vancouver, Canada 235 | P a g e Traditionally, structuring and modelling social networks can extract useful information as topic trends and the trends of opinions and the linguistics properties play an effective role in this study. At the beginning, certain word filtration techniques are used to remove unwanted words, such as stop and stemming words [5]. The concept of the self-organizing map (SOM) is utilized in particular research fields; in the simplest terms, SOM is a predefined wordlist used to correlate with the large data from the social networks under the tests to extract positive and negative words [6]. Importantly, certain algorithms are used to determine the frequency of certain positive or negative words, so that weighing the word (in another meaning its effectiveness among other words) can be determined. Finally, simple statistical tools are used to identify a positive or negative trend and the most common words describing the symptoms of drug use. Similar solutions in the literature on disease surveillance for the case of Influenza-related community who share fluposting online is utilized through the technique text and structural data mining of web and social media (WSM) [7] [8]. In the critical analysis of the SOM and WSM techniques in the literature [9], [10], it can be concluded that SOM techniques have more advantages in terms of the capability of investigating the positive and negative feedbacks of treatments. This advantage can be achieved by mapping large dimensional information onto a low dimensional space. II. METHODOLOGIES OF THE PROPOSED SYSTEM A two-step framework is proposed as an investigatory analysis to evaluate the correlations between user posts and positive/negative words under a drug name. The correlation is obtained by using SOM. Using a network-based approach, the system enabled users and their posts to find the possible partition using complete linkages. The two processes involved with inter-social dynamic maps for reviewing SOM results are described below:  The correlation between user and judgment.  The partition between users and their posts. Regarded as an unsupervised technique, Self-OrganizingMap (SOM) is used to explore the survey dataset based on the artificial neural network. The representation of the SOM data is in multidimensional data such as two or three dimension. Based on the data compression of the vector quantization technique, the SOM process is used to reduce the dimensionality of sectors. The information is stored as a topological relationship within the training sets in a network. Therefore, large data sets are visualized with high dimensionality using SOM. The competitive learning approach of SOM has one neuron unconnected to the input and output layers for each training phase. Although the connection between the neurons is absent, communication exists between each phase through a neighborhood function. The proposed SOM approach is used to summarize and visualize the profiles of individual patients. This visualization process helps determine domain experts. The two perspectives involved in the process of accurately obtaining results are computational and scientific perspectives [11]. By using SOM in the computational perspective, feasibility is examined by extracting useful information from questionnaires. In the scientific perspective, the different types of patient diseases such as type-I diabetes are collected to understand the responses in the diabetes survey and suggest about domain experts to clinicians. By contrast, clinicians are required to take a survey about their patients. The mean, skewness, variance, and frequency are the traditional descriptive statistical methods, but it provides simplified conclusions. Thus, data are analyzed based on statistical machine learning tools with black box by clinicians. The SOM algorithm is used for mining correlations and clustering similar responses within the surveys. If the dimension is higher for clustered responses, then the data is visualized in a two-dimensional grid to reduce data complexity. Complexity is reduced by revealing more meaningful relationships and by understanding the dependencies among the survey responses. Previously, SOM is used to visually explore data areas such as health, lifestyle, nutrition, financial, gene expression, marine safety, and linguistics. Recently, SOM is utilized to explore questionnaire-based loneliness survey data. The present research also focuses on improving data interpretation by revealing possible associations between the tendency of item nonresponse and the background variables of participants. The flaw conclusion is obtained by item nonresponse which is related to the background variables of respondents, such as age and gender nonresponses. Considering the undetected non-causative relationships between independent and dependent variables, the nonresponse factors affected patient satisfaction. In the present study, item nonresponse does not refer to participants who fail to return the survey, but to the ones who choose not to respond to all questions. In the proposed approach, the issues involved in this research are included for data analysis. Large surveys have demonstrated that although respondents and non-respondents in patient satisfaction surveys may differ according to several demographic and clinical characteristics, the differences in satisfaction between them tend to be relatively small and non-respondents do not constitute a homogenous group. Many highly sophisticated statistical methods are used as a standard technique to handle the problem of missing responses. In the existing link method, the idea that missing data are not just a statistical nuisance but also contain valuable information as such is tested simply by including the number of item nonresponses per respondent as an explanatory variable in the models. The expected predictors of pati",,2018.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
7e4ad1b1f3a30599fcbcfbe754d53eaa3c99d7d3,https://www.semanticscholar.org/paper/7e4ad1b1f3a30599fcbcfbe754d53eaa3c99d7d3,Desarrollo de una aplicación para reconocimiento y muestra de información de obras de arte: RecoArt,"Hoy en dia, los telefonos moviles son utilizados en la mayoria de nuestras actividades. Desde pedir un taxi o realizar alguna compra en alguna tienda ubicada en el otro lado del mundo, nosotros podemos hacer todo esto de un modo simple con nuestros telefonos moviles. La tecnologia evoluciona, y con ella se mejora tambien las capacidades de los telefonos moviles. Hoy, un telefono movil tiene mas capacidades que un ordenador promedio en la decada pasada; esto permite que las nuevas aplicaciones moviles sean mas potentes y de mayor utilidad para el usuario. Cada ano, los fabricantes de telefonos mejoran sus terminales, anaden nuevas caracteristicas que permiten que los telefonos moviles amplien su capacidad. El estandar de la industria de hoy, por ejemplo, es que los moviles dispongan de doble camara. Este es un estandar reciente, que permite al telefono tener un procesamiento de imagenes mejor, por ende, una calidad de fotos superior inclusive en ambientes con poca luz. Pero no basta que los terminales mejoren su hardware, si estos no vienen acompanados por software que explote ese hardware. Ese software es el llamado “framework” el cual permite a los desarrolladores utilizar ese hardware avanzado para el desarrollo de aplicaciones potentes. El desarrollo de aplicaciones moviles, lejos de lo que fue en sus inicios (cuando se desarrollaba aplicaciones de forma artesanal) hoy en dia es una rama de la ingenieria de software especializada, por lo que debe respetar procedimientos de la ingenieria, pero al ser una rama especializada, se debe utilizar metodologias que vayan acorde a la realidad de esta rama. Las metodologias de desarrollo de software tradicional son incompatibles con el desarrollo de aplicaciones moviles, por lo que para este tipo de software es mejor trabajar con metodologias agiles. El presente proyecto tiene la finalidad de proponer el desarrollo de una aplicacion movil que permita el reconocimiento de obras de arte. Aunque el desarrollo de la aplicacion sea tanto para Android como para iOS, el prototipo de aplicacion sera desarrollado para iOS. Asi se puede explotar los frameworks avanzados que Apple dispone para el reconocimiento de imagenes, machine learning y realidad aumentada, herramientas necesarias para el desarrollo de la aplicacion. Se ha pensado en un mecanismo de uso de la aplicacion orientada a la simplicidad. El usuario solo debera enfocar el cuadro con su telefono movil y la aplicacion sera capaz de reconocer el cuadro y brindarle toda la informacion relacionada de la misma. Adicional, se ha propuesto la mecanica de reconocimiento por medio de fotos, donde el usuario tendra la posibilidad de tomar una foto o seleccionar una foto que tenga almacenada en su movil de la obra de arte a reconocer. Asi mismo, la construccion de esta aplicacion estara dirigida por la metodologia de desarrollo de software agil Mobile-D, la cual nos permite abarcar todo el proceso, desde la toma de requisitos, hasta la fase de pruebas y despliegue de la misma. El presente documento detalla el proceso completo de construccion de la aplicacion de reconocimiento de obras de arte, bautizada con el nombre de RecoArt. 
Abstract: 
Today, mobile phone is used for almost all human activities. To ask for a taxi or do shopping in a store from the other side of the world, we can do all this thing in a simple way with our mobile. Technology evolves and with it, mobile phones capabilities too. Nowadays, a mobile phone has more capabilities than an average computer in the past decade. With this, new apps are more powerful and useful for users. Every year, phone manufacturers improve their terminals, add new features that allows to expand mobile phones capabilities. For example, now mobiles have two posterior cameras as a new industry standard. With this, mobile phones have a better image processing, therefore photos with a better quality even in dark scenarios. But it isn’t enough, if mobiles don’t have software that exploits hardware capabilities. This software is named “framework”, and allows developers use this advanced hardware to develop powerful apps. 
Mobile app development today is a specialized part of software engineering, and some engineering rules should be respected. In this part of software engineering, we must use methodologies accord to mobile app development. Traditional methodologies are incompatible with mobile app development, for this reason, this development works better with Agile methodologies. The aim of this project is proposing the development of an app for painting recognition. Although the development of the application is for both Android and iOS devices, the app prototype was developed for iOS devices. With this prototype, we can exploit advanced frameworks for image recognition, machine learning and augmented reality, developed by apple and necessaries for this app development. The app design focusses on simplicity. User only focusses the mobile on a painting and automatically the app will recognize it and show all the information about it. In addition, the user also can use the recognizer mode by photo. With this mode, the user can take a photo of a painting or select a photo stored in mobile, and the app also will recognize it automatically. In the same way, the development app process will be driven by agile software methodology Mobile- D. With this methodology, we cover all phases of the development process, from requirements analysis to test and deploy phases. This document describes all construction process of painting recognition app, named RecoArt.",,2018.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
57b672bbd41fa4fae7b4307191978599513107f4,https://www.semanticscholar.org/paper/57b672bbd41fa4fae7b4307191978599513107f4,Data Engineering Project (Educating For The Future PhUSE WG),"With an expected 100% increase, over the next 3 years, of data from non-EDC sources (such as smartphones, wearables and custom apps) the traditional methods of managing data for clinical trials presents executives with a resourcing headache. As such, many companies are looking for lower cost strategies to sure up this shortfall in resourcing. However, citing case studies from other industries, there are new methodologies/technologies in data engineering which could enable automation of much of the “heavy-lifting” currently practiced in clinical data management and statistical programming. This paper discusses the Data Engineering Project within the PhUSE Computational Science (CS) Working Group, Educating For The Future, with a view to educate clinical data managers in data engineering principles so that they can be prepared, equipped and effective in dealing with the coming “data tsunami” heading to the shores of clinical research. INTRODUCTION Did you realise we are living in the age of the Fourth Industrial Revolution? Perhaps you have been busy downloading a myriad of “apps” designed to make your life easier or connecting on social media, uncovering relationships and associations you didn’t even know you had. Perhaps you have been shopping a global marketplace, comparing prices, quality and availability, all at your fingertips and in a minutes’ time. While this has been happening, the Fourth Industry Revolution has been evolving at exponential proportions ​. Just ask Siri! The term “Industrie 4.0”, was originated in Germany, as a government-led initiative, to transform manufacturing through advanced digital capability. Thus creating the concept of a “smart factory”, based on four key design principles ​: 1. Interconnection of machines, devices, sensor and people 2. Vast amounts of useful information (data) to drive decision making 3. Technical assistance to aid humans, for example to visualise data or to perform tasks that may be of safety concern for a human. 4. The use of cyber-physical systems to make decisions on their own and to perform tasks as autonomously as possible. Emerging from the premise of “Industrie 4.0” is the advent of the term “The Fourth Industrial Revolution” (also referred to as “4IR” or “I4.0”). This term originated in 2016 when described by Klaus Schwab (Founder and Executive Chairman of the World Economic Forum), as a “technological revolution that will fundamentally alter the way we live, work, and relate to one another”. Klaus goes on to describe it as a digital revolution with innovative uses of a combination of technologies that build upon the premise of the third revolution (i.e. electronics and information technology to automate production). As a result, emerging technologies have brought forth advancements in fields such as ​artificial intelligence, robotics, the Internet of Things, autonomous vehicles, 3D printing, nanotechnology, biotechnology, materials science, energy storage, and quantum computing. This rapid evolution will undoubtedly affect industries world-wide, already disrupting many industries, such as travel agencies, video rentals and bookstores​ . The pharmaceutical industry is also experiencing the impacts of I4.0. Digital and mobile technologies has brought on significant advancements in data acquisition and accessibility as it relates to health care and patient data. As reported in the Tufts-Veeva eClinical Landscape study in 2017, data coming from sources such as, smartphones, custom applications, and mobile health are expected to double in the next 3 years ​. Therefore requiring greater capabilities in handling large volumes of data, as well as data from coming in through various data streams and 1 PhUSE EU Connect 2018 formatting. As with other industries, data will become a critical asset to their business and the effective utilisation of this data can play a critical role in driving growth in the business and bringing novel therapies to the patients who need them. In this paper, we will focus on the works of the Data Engineering Project within the Educating For The Future Working Group. With the formation of the Working Group in early 2018, the team had taken on the mission to explore how data engineering techniques, successfully deployed in other industries, could be utilised in the pharmaceutical industry, with a goal to ​facilitate the education ​of the pharmaceutical industry on these techniques. We will share with you some introductory information about Data Engineering and Data Science and explore how embracing new data engineering techniques may affect the industry culture. You will learn about use cases of Data Engineering in other industries and how advances in digital capability have affected their business model. We will also share some of the many software packages and tools available to enable automation, commonly used in Data Engineering and Data Science. Finally we will reflect on the benefits that data standardisation has brought to the pharmaceutical industry and share our vision for disseminating information to facilitate your learning going forward. DATA ENGINEERING To start this learning journey, exploring the term “Data Engineering” opens the door to the vast opportunities and roles available today centered around data. In doing a simple search on the internet, ​“what is data engineering?​”, one will find many posts expressing their understanding of Data Engineering with some variation but also some similarity. However, what is clear is that Data Engineering encompasses the many considerations that need to be taken into account to optimally curate, transform, secure and disseminate data suitable for analysis. As technology and tools have become more advanced, building such a platform and infrastructure requires engineers and architects of both general and specific expertise. The Data Engineer combines knowledge in areas such as software development, infrastructure, data architecture, data warehousing, cloud technology and data cleaning in order to design, build and test solutions that define the pipelines of data throughout the enterprise, making the data accessible to the organisation.​ [5] [27] [31] Optimised Data Engineering appropriately balances the efficiency of an automated process against the cost of development and maintenance of that process, ensuring repetitive processes that require humans to write code, press keys, cut-and-paste and update documents are minimised or eliminated. DATA SCIENCE Often paired with the term “Data Engineering” is also the term “Data Science”. According to Kelle O’Neal and Charles Roe: “Data Science allows enterprises the ability to turn their data assets into a narrative. Data Science allows that narrative to be expanded across timelines, in different data spaces that trace from the past into the future, with much more involved questions and answers about an enterprise, different potential outcomes, and repercussions based on recommendations. Data Science employs a range of mathematical, business, and scientific techniques to solve complex problems about an organisation’s data assets.” ​ In contrast, the focus of the Data Engineer is on the process from data curation to dissemination and the focus of the Data Scientist is on the analytics of the data, thus extracting knowledge from the data. To achieve quality data capture, near-real-time accessibility and meaningful analytics, one cannot function without the other, and effective teamwork optimises the value of each role. As such, an analytics team would be composed of distinct roles/capabilities​ : ● Data Engineers (in areas such as database architecture, database development, machine learning architecture, ETL scripting , etc.) ● Data Scientists ● Business Analysts Data Engineering brings together the broad expertise, of these roles, to ensure the data are curated and accessible to the Data Scientist, and in our environment today, this process is becoming more and more complex. Therefore, 2 PhUSE EU Connect 2018 expertise in curating big-data and data of varying formats (structured and unstructured) is a critical core competency to optimise the potential impact of these digital assets (i.e. the data). The Data Scientist works deep in the data, utilizing various tools and techniques to discover patterns in the data that may drive decision making for the business. Optimising utilisation of the data to enable accurate conclusions can bear greater value to the organisation. As an example, per Tom Eunice’s post, “a fraud-detection algorithm may be very accurate when based on many months of historical data. However, months of historical data may not always be available. Designing a fraud-detection model that is still accurate using historical data from only a few days would be of more use and more practical to implement.” ​ The Business Analyst helps the Data Scientist understand the meaning of the data and the relevance of any discovered relationships. Initially, uncovering relationships in the data and upon further investigation, identifies meaningful patterns that may reveal information that otherwise may not have been known. ​ As you will see in the sections to follow, the full complement of the roles in an analytics team is what drives the business value. One discipline without the other (e.g. data engineering without data science) will result in missed opportunities. In the sections to follow, we often refer to Data Engineering, however, due to the close ties to Data Science, some examples elude to both Data Engineering and Data Science. USE CASES FROM OTHER INDUSTRIES In this section, we present three use cases from the transportation, retail, and agricultural industry. The use cases illustrate the importance and usage of Data Engineering. In each example the data collected, the consumer of the data, and the value of the organisation is reviewed. Similarities and potential applications to the pharmaceutical industry are discussed. UBER When",,2018.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
bd1a8094a14f68528d8910b7455f630e6d4cd36c,https://www.semanticscholar.org/paper/bd1a8094a14f68528d8910b7455f630e6d4cd36c,SIGLOG monthly 198,"Deadline: January 15th, 2018 Submission Deadline: January 22nd, 2018 Rebuttal: March 22 25th, 2018 Notification: April 2nd, 2018 Camera-Ready: May 2nd, 2018 FSCD Conference: July 9 12th, 2018 FLoC Conference: July 6 19th, 2018 * PROGRAM COMMITTEE CHAIR Helene Kirchner, Inria * CONFERENCE & WORKSHOP CHAIR: Paula Severi, Leicester U. 13TH WORKSHOP ON GAMES FOR LOGIC AND PROGRAMMING LANGUAGES (GaLoP 2018) Call for Abstracts Thessaloniki, Greece, 14-15 April, 2018 http://www.gamesemantics.org * GaLoP is an annual international workshop on game-semantic models for logics and programming languages and their applications. This is an informal workshop that welcomes work in progress, overviews of more extensive work, programmatic or position papers and tutorials. * GaLoP XII will be held in Thessaloniki, Greece, on 14-15 April 2018 as a satellite workshop of ETAPS (http://www.etaps.org/). * IMPORTANT DATES Submission: 22 January 2018 Notification: 12 February 2018 Workshop: 14-15 April 2018 * Invited talks Guy McCusker (Bath) Matteo Mio (Lyon) Ulrich Schopp (Munchen) * Programme Chairs Ugo Dal Lago (Bologna, co-chair) Gabriel Sandu (Helsinki, co-chair) 4th ACM CYBER-PHYSICAL SYSTEM SECURITY WORKSHOP (ACM CPSS’18) Incheon, Korea, June 4, 2018 (in conjunction with ACM AsiaCCS’18) http://jianying.5gbfree.com/cpss/CPSS2018/ Extended submission deadline: Jan 27, 2018 (23:59 GMT) * CONFERENCE OUTLINE Cyber-Physical Systems (CPS) consist of large-scale interconnected systems of heterogeneous components interacting with their physical environments. There are a multitude of CPS devices and applications being deployed to serve critical functions in our lives. This workshop will provide a platform for professionals from academia, government, and industry to discuss how to address the increasing security challenges facing CPS. Besides invited talks, we also seek novel submissions describing theoretical and practical security solutions to CPS. Papers that are pertinent to the security of ACM SIGLOG News 37 January 2018, Vol. 5, No. 1 embedded systems, IoT, SCADA, smart grid, and critical infrastructure networks are all welcome. * IMPORTANT DATES Extended submission deadline: Jan 27, 2018 (23:59 GMT) Notification: Mar 10, 2018 Camera-ready due: Mar 31, 2018 * Program Chairs: Dieter Gollmann (Hamburg University of Technology, Germany & NTU, Singapore) Jianying Zhou (SUTD, Singapore) * Further information CPSS Home: http://jianying.5gbfree.com/cpss/ Email: cpss2018@easychair.org CONTINUITY, COMPUTABILITY, CONSTRUCTIVITY: FROM LOGIC TO ALGORITHMS 2017 Second Call for Submissions (Postproceedings) Deadline for submission: 1 February 2018 * After the successful start of the new EU-MSCA-RISE project ""Computing with Infinite Data"" (CID) and the excellent Workshop CCC 2017 in Nancy (France) in June this year, we are planning to publish a collection of papers dedicated to the meeting and to the project as a Special Issue in the open-access journal LOGICAL METHODS IN COMPUTER SCIENCE. * The issue should reflect progress made in Computable Analysis and related areas, and is not restricted to work in the CID project or presented at the Workshop. * Submissions are welcome from all scientists on topics in the entire spectrum from logic to algorithms including, but not limited to: Exact real number computation, Correctness of algorithms on infinite data, Computable analysis, Complexity of real numbers, real-valued functions, etc. Effective descriptive set theory, Constructive topological foundations, Scott’s domain theory, Constructive analysis, Category-theoretic approaches to computation on infinite data, Weihrauch degrees, Randomness and computable measure theory, Other related areas. * EDITORS: Ulrich Berger (Swansea, UK) Pieter Collins (Maastricht, NL) Mathieu Hoyrup (Nancy, FR) Victor Selivanov (Novosibirsk, RUS) Dieter Spreen (Siegen, DE) Martin Ziegler (KAIST, KR) * DEADLINE FOR SUBMISSION: 1 February 2018 * If you intend to submit a paper for the special issue, please inform us by sending email to: spreen@math.uni-siegen.de by 1 January 2018 COMPUTABILITY IN EUROPE: SAILING ROUTES IN THE WORLD OF COMPUTATION (CiE 2018) Final Call for Papers ACM SIGLOG News 38 January 2018, Vol. 5, No. 1 Kiel, Germany July 30 August 3, 2018 http://cie2018.uni-kiel.de * CiE 2018 is the fourteenth conference organized by CiE (Computability in Europe), a European association of mathematicians, logicians, computer scientists, philosophers, physicists and others interested in new developments in computability and their underlying significance for the real world. * THE PROGRAMME COMMITTEE cordially invites all researchers (European and non-European) to submit their papers in all areas related to computability for presentation at the conference and inclusion in the proceedings at https://easychair.org/conferences/?conf=cie2018 Submission guidelines are available on the conference web-site. * The CONFERENCE PROCEEDINGS will be published by LNCS, Springer Verlag. * IMPORTANT DATES: Deadline for abstract submission: January 17, 2018 Deadline for article submission: February 1, 2018 Notification of acceptance: April 6, 2018 Early registration before: May 30, 2018 * TUTORIAL SPEAKERS: Pinar Heggernes (Bergen, Norway), Bakhadyr Khoussainov (Auckland, NZ) * INVITED SPEAKERS: Kousha Etessami (Edinburgh, UK) Johanna Franklin (Hempstead, US) Mai Gehrke (Nice, France) Alberto Marcone (Udine, Italy) Alexandra Silva (London, UK) Jeffrey O. Shallit (Waterloo, Canada) * SPECIAL SESSIONS: Approximation and Optimisation, Bioinformatics and Bio-inspired Computing, Computing with Imperfect Information, Continuous Computation, History and Philosophy of Computing, SAT-Solving. * WORKSHOP: Women in Computability. * Check the web-site for further details on the conference, such as organisation and grants. SPECIAL ISSUE OF IJAR ON ""DEFEASIBLE AND AMPLIATIVE REASONING"" Call for Papers * Classical reasoning is not flexible enough when directly applied to the formalization of certain nuances of decision making as done by humans. These involve different kinds of reasoning such as reasoning with uncertainty, exceptions, similarity, vagueness, incomplete or contradictory information and many others. * Everyday reasoning usually shows the two salient intertwined aspects below: Ampliative aspect: augmenting the underlying reasoning by allowing more conclusions. Defeasible aspect: curtailing the underlying reasoning by either disregarding or disallowing some conclusions that somehow ought not ACM SIGLOG News 39 January 2018, Vol. 5, No. 1 to be sanctioned. * This special issue aims at bringing together work on defeasible and ampliative reasoning from the perspective of artificial intelligence, cognitive sciences, philosophy and related disciplines in a multi-disciplinary way, thereby consolidating the mission of the DARe workshop series. * The submission url is: http://www.evise.com/evise/jrnl/IJA * When submitting your manuscript, please select ‘‘VSI:DARe special issue’’ as the article type. * If you have any enquiries, please feel free to contact us at dare.to.contact.us@gmail.com * IMPORTANT DATES Submission deadline: 15 February 2018 Notification: 1 November 2018 Publication date: 1 January 2019 * Guest editors Richard Booth, Cardiff University, UK Giovanni Casini, University of Luxembourg Szymon Klarman, Semantic Integration Ltd., UK Gilles Richard, Universite Paul Sabatier, France Ivan Varzinczak, CRIL, Univ. Artois & CNRS, France THE 17TH IEEE INTERNATIONAL CONFERENCE ON COGNITIVE INFORMATICS AND COGNITIVE COMPUTING (ICCI*CC’18) UC Berkeley, CA, USA July 15-18, 2018 http://www.ucalgary.ca/icci_cc/iccicc-18 * The IEEE ICCI*CC series is a flagship conference of its field sponsored by IEEE Computer, Computational Intelligence and SMC Societies. The theme of ICCI*CC’18 is on Cognitive Machine Learning, Brain-Inspired Systems and Cognitive Robotics. * You are welcome to submit a paper to IEEE ICCI*CC’18 or to organize a special session related to the theme of the conference. The Proceedings of ICCI*CC’18 will be published by IEEE CS Press (EI Indexed). A good rate of selected papers from the proceedings will be recommended to leading international journals and/or IEEE transactions with ISI/EI indexes. * IMPORTANT DATES Submission deadline: February 16, 2018. 25th WORKSHOP ON LOGIC, LANGUAGE, INFORMATION AND COMPUTATION (WoLLIC 2018) Call for Papers July 24th-27th, 2018, Bogota, Colombia http://wollic.org/wollic2018/ * WoLLIC is an annual international forum on inter-disciplinary research involving formal logic, computing and programming theory, and natural language and reasoning. Each meeting includes invited talks and tutorials as well as contributed papers. * Contributions are invited on all pertinent subjects, with particular ACM SIGLOG News 40 January 2018, Vol. 5, No. 1 interest in cross-disciplinary topics. Typical but not exclusive areas of interest are: foundations of computing and programming; novel computation models and paradigms; broad notions of proof and belief; proof mining, type theory, effective learnability; formal methods in software and hardware development; logical approach to natural language and reasoning; logics of programs, actions and resources; foundational aspects of information organization, search, flow, sharing, and protection; foundations of mathematics; philosophy of mathematics; philosophy of language; philosophical logic. * IMPORTANT DATES: Mar 11, 2018: Paper title and abstract deadline Mar 18, 2018: Full paper deadline Apr 15, 2018: Author notification Apr 22, 2018: Final version deadline (firm). * Further details: http://wollic.org/wollic2018/ WOMEN IN LOGIC WORKSHOP (WiL 2018) Call for Papers July 8, 2018, Oxford UK https://sites.google.com/site/womeninlogic2018/welcome/ * Affiliated with LICS (http://lics.siglog.org/lics18/) Held as part of FLoC (http://www.floc2018.org/) * We are holding the 2nd Women in Logic (WiL) workshop as a LICS 2018 associated w",SIGL,2018.0,10.1145/3183645.3183651,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
fbad4dbfff0d8b675af74a86e7e6bb9c389ae76b,https://www.semanticscholar.org/paper/fbad4dbfff0d8b675af74a86e7e6bb9c389ae76b,A Computing Perspective on Smart City,"DEVELOPING smart city is the key to the next generation urbanization process for improving the efficiency, reliability, and security of a traditional city. The concept of smart city includes various aspects such as environmental sustainability, social sustainability, regional competitiveness, natural resources management, cybersecurity, and quality of life improvement. With the massive deployment of networked smart devices/sensors, an unprecedentedly large amount of sensory data can be collected and processed by advanced computing paradigms, which are the enabling techniques for smart city. For example, given historical environmental, population, and economic information, salient modeling and analytics are needed to simulate the impact of potential city planning strategies, which will be critical for intelligent decision-making. Analytics are also indispensable for discovering the underlying structure from retrieved data in order to design the optimal policies for real time automatic control in the cyberphysical smart city system. Furthermore, uncertainties and security concerns in the data collected from heterogeneous resources aggravate the problem, which makes smart city planning, operation, monitoring, and control highly challenging. Green, sustainable, and secure computing in smart cities has recently become a very active area of research in academia and has attracted significant industry interest. Since the computing issues for smart cities are highly interdisciplinary and cover various topics, a special section of Smart City Computing in the IEEE Transactions on Computers becomes an ideal forum for presenting and discussing the latest research results. The goal of this special section is to present the outstanding research results dedicated to the topics of green, sustainable, and secure computing for smart cities. We have received 40 manuscript submissions in total and six papers have finally been accepted after several rounds of very constructive and deep reviews. The smart city brings convenience to users through providing personalized yet efficient services. However, it also introduces privacy and security issues. Two different methods are used to counteract these issues. The active method is to prevent the overcollection of private data (Dai et al.), while the passive method is to make private data more secure via certain encryption algorithms (Zhang et al.). Dai et al. study the current state of data overcollection and look at some of the most frequent cases of overcollected data. They present a mobile cloud framework, which is an active approach to eradicate data overcollection. Through putting all users’ data into a cloud, the security of the data can be greatly improved. Zhang et al. use the BGV encryption scheme to encrypt the private data and employ cloud servers to perform the high-order back-propagation algorithm on the encrypted data efficiently for deep computation model training. Furthermore, their proposed scheme approximates the sigmoid function as a polynomial function to support the secure computation of the activation function with the BGV encryption. Data infrastructures play an important role in smart city computing. Two interesting topics are included in this special section, which are wireless sensor network for data collection (Santos et al.) and spatial-temporal database for data storage (Ding et al.), respectively. Santos et al. propose a decentralized algorithm for detecting damage in structures using a WSAN. As key characteristics, beyond presenting a fully decentralized (in-network) and collaborative approach for detecting damage in structures, the algorithm makes use of cooperative information fusion for calculating a damage coefficient, which represents frequency and amplitude shifts simultaneously. Ding et al. propose the Parallel-Distributed Network-constrained Moving Objects Database (PD-NMOD), a general framework that manages big trajectory data in a scalable manner. The PD-NMOD provides an infrastructure that is able to support a wide variety of smart transportation applications, thus benefiting the smart city vision as a whole. Furthermore, two interesting applications are selected to instantiate the effect of smart city computing, which includes smart grid (Wei et al.) and smart railway transportation (Huang et al.). Concerning the application domain of the smart grid, Huang et al. propose a comprehensive framework to integrate the operations of smart buildings into the energy scheduling of bulk power systems through proactive building demand participation. This new scheme enables buildings to proactively express and communicate their energy consumption preferences to smart grid operators rather than passively receive and react to market signals and instructions such as time varying electricity prices. For smart railway transportation, Huang et al. propose an energy-efficient train control framework through integrating both offline and onboard optimization techniques. The offline processing builds a decision-tree based sketchy solution through a complete flow of sequence mining, optimization, and machine learning. The onboard system feeds the train parameters into the decision tree to derive an optimized control sequence. For this special section of the IEEE Transactions on Computers we have selected the above very interesting papers to represent some important advances in smart city computing. As a conclusion, it should be noted that the research L. Wang (corresponding author) is with the School of Computer Science, China University of Geosciences, Wuhan, P.R. China. E-mail: Lizhe.Wang@computer.org. S. Hu is with the Michigan Technological University. G. Betis is with the EIT ICT Labs, France. R. Ranjan is with the University of Newcastle, United Kingdom.",,2016.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
80040e993990f21db4741381c02165c3db06ace0,https://www.semanticscholar.org/paper/80040e993990f21db4741381c02165c3db06ace0,Modeling and Integrating Cognitive Agents Within the Emerging Cyber Domain,"One of the elements missing from virtual environments in the emerging cyber domain is an element of active opposition. For example, in a training simulation the instructor assigns the student a task or objective, and the student then practices within the environment (the “cyber range”) until they feel comfortable with the task or are able to demonstrate the requisite level of mastery. The environment may have static defenses, such as access control or firewalls, or a fixed set of intrusion methods to defend against, but it typically lacks any active opposition that might adapt defensive or offensive actions (e.g., monitor logs, blocked connections, exploit switching or information gathering). This is akin to training fighter pilots against adversaries who know how to use their weapons, but do not have any tactical or strategic goals beyond that. This is unfortunate for two reasons: 1) it trains cyber operators to behave as though opponents do not have a tangible existence or do not have higher-level goals, and 2) it ignores an opportunity to tailor the student’s learning experience through adjustable adversary behavior. Cognitive agents have the potential to transform the cyber operations training experience. The application of cognitive agents to the roles of cyber offense and defense would provide a more complete cyber ecology for training purposes and thus a more realistic training experience for the student. There are two key challenges to creating such cyber agents: 1) modeling the complex, and continually evolving, processes of cyber operations within a cognitive architecture, and 2) defining the tools and data standards to enable cognitive agents to interoperate with networks in a portable way. This paper discusses novel models of cyber offensive and defensive behavior based on observation and elaboration of human expertise, as well as an approach to the creation of software adapters that translate from task-level actions to network-level events to support agent-network interoperability. ABOUT THE AUTHORS Randolph M. Jones, PhD, is a senior artificial intelligence engineer at Soar Technology, and co-founded Soar Technology in 1998. Dr. Jones received his BS in mathematics and computer science from UCLA, and he received his M.S. and Ph.D. in information and computer science from the University of California, Irvine. Ryan O’Grady is the technical lead for Soar Technology’s emerging business area in cyberspace training and visualization, and a senior software engineer in the Intelligent Training business area. Mr. O’Grady received a BSE in Computer Science Engineering from the University of Michigan in 2004. Certifications: Security+, CPTE, OSCP Denise Nicholson, PhD, CMSP, is the Director of Soar Technology’s new Technology Area ""X"" leading an effort to explore, identify and pursue innovative applications of intelligent systems for critical and challenging problems, such as Cyber Security. Dr. Nicholson has a Ph.D. and M.S. in Optical Sciences from the University of Arizona, and a B.S. in Electrical Computer Engineering from Clarkson University. Robert Hoffman, PhD, is a Senior Research Scientist at the Florida Institute for Human and Machine Cognition (IHMC). He is senior editor of the Department on Human-Centered Computing of IEEE: Intelligent Systems. His latest book is Accelerated Expertise: Training to High Proficiency in A Complex World (2014, Taylor & Francis). Larry Bunch is a Senior Research Associate at IHMC. He received his BS in computer science from the University of West Florida and has published extensively concerning software agents, semantic policies and reasoning, and large-scale event visualizations. Jeffrey M. Bradshaw, PhD, is a Senior Research Scientist at IHMC. He co-edits the HCC Department of IEEE Intelligent Systems and has published widely in software agents, semantic technologies, digital policy management, and human-agent-robot teamwork (HART). Ami Bolton, PhD, is a Program Officer at the Office of Naval Research (ONR). Her programs focus on enhancing individual and team decision-making and combat effectiveness through advances that improve perception, cognition, and team coordination. Dr. Bolton received a M.S. in Human Factors from the Florida Institute of Technology, and Ph.D. in Applied Experimental & Human Factors Psychology from University of Central Florida. Interservice/Industry Training, Simulation, and Education Conference (I/ITSEC) 2015 2015 Paper No. #15232 Page 2 of 10 Modeling and Integrating Cognitive Agents Within the Emerging Cyber Domain Randolph M. Jones, Ryan O'Grady, Denise Nicholson, Robert Hoffman, Larry Bunch, Jeffrey Bradshaw, and Ami Bolton Soar Technology IHMC Office of Naval Research Ann Arbor, MI Pensacola, FL Arlington, VA rjones@soartech.com, ryan.ogrady@soartech.com, denise.nicholson@soartech.com, rhoffman@ihmc.us, lbunch@ihmc.us, jbradshaw@ihmc.us, amy.bolton@navy.mil Cyber warfare presents a persistent and evolving threat to military and civilian information systems. Both DoD (Parrish, 2013) and ODNI (Pellerin, 2013) rank cyber warfare as our top national security concern. In addition to threats to our defensive forces, cyber attacks pose an economic threat on the order of one trillion dollars (Ponemon, 2013). Although individual cyber-warfare tools operate at extremely fast speeds, aggressors increasingly pursue a “cyber kill-chain” (Hutchins et al., 2010) over days, weeks, or months. Would-be cyber aggressors are constantly changing their attack vectors to take advantage of security lapses by human resources and the latest vulnerabilities in information technology. These human-speed activities are guided by cognitive behavior that includes a variety of types of goals and expertise: script-kiddies, ideological activists, investigators, financial criminals, intelligence agents, or cyber warfighters (Lathrop et al., 2010). At the human, cognitive level, offense depends on and reacts to responses of defenders (Pfleeger & Caputo, 2012) and users (Bowen et al., 2012) that are also cognitively driven. Current cyber-warfare tools comprise suites of technical mechanisms that respond to the tools that aggressors and defenders use, but not to the individuals themselves. Human tactics are currently addressed through human-staffed wargames at cyber ranges (Merit, 2013; Pridmore, 2012). Human role-players are expensive, not repeatable, and not deployable as an automated system. There is an emerging need for cognitive-level synthetic cyber offense and defense, to ensure realistic cyber simulation and training. Building effective training systems for cyber warfare presents a suite of unique problems: Offensive and defensive activity is highly dynamic. The characteristics of target network environments are driven by the users of the system and their current activities, which are highly variable and unpredictable. User behavior often creates vulnerabilities that can be exploited. Cyber warfighters themselves are extremely adaptive and creative. In order to meet their objectives they will change tactics or tools based on opportunities detected in a computer network or responses initiated by adversaries or users. Current training environments do not adequately capture the dynamic and cognitive-level characteristics of cyber warfare. They are unable to capture the purposefulness, creativity, and adaptability of actual cyber warfighters. Studying previous offensive and defensive scenarios in a classroom environment is an effective means of understanding the building blocks of cyber warfare, but falls short of creating the skills needed to deal with a creative and time-sensitive event or a sophisticated but dynamic plan. Computerized unit tests can build fundamentals for dealing with individual components cyber warfare, but they do not help the trainee learn to recognize and make sense of the larger picture, nor do they capture the dynamic nature of networks and users. If cyber warfighters are to learn to respond to a cunning and adaptive opponent, they need to train against cunning and adaptive opponents. An effective cyber-warfare training system must be adaptable and deal with the changing nature of a networked environment. It must be able to model the dynamic nature of cyber aggressors, users, and defenders. It must create a virtual environment that replicates the environment that the trainee will ultimately operate in. An appropriate virtual environment also creates the opportunity for accurate post-event forensic analysis by providing access to databases, configurations, and system logs. This paper presents our efforts to address these issues through the development of cognitive agents for cyber offense and defense. The Soar cognitive architecture described in this paper is not to be confused with Soar Technology, the affiliation of some of the authors. Soar is not a commercial product, but is available under a General Public License from http://soar.eecs.umich.edu/ maintained by the University of Michigan. The Soar architecture provides the technological foundation for the cognitive agents described here. Interservice/Industry Training, Simulation, and Education Conference (I/ITSEC) 2015 2015 Paper No. #15232 Page 3 of 10 MODELING AND INTEGRATION CHALLENGES In order to build realistic cognitive agents, the agents must encode appropriate domain expertise, and they must interact with a realistic cyber environment (Jones & Laird, 1997). In addition to realism, cost effective cognitive agents also need to address these related issues: Reduce cost of realistic role playing in cyber-warfare simulation, system engineering, and analysis of cyber operations. Enable end-user updating of agent knowledge with minimal support from software engineers, both through coaching by instructor Subject-Matter Experts (SMEs) and through explicit addition of new knowledge about cyber tactics. Be readily adaptable to a wide range of network structures, devices, and protocols. In o",,2015.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
8bcb0086d9a4daf9da32cd61aa8958c4d7d2e389,https://www.semanticscholar.org/paper/8bcb0086d9a4daf9da32cd61aa8958c4d7d2e389,Designing Great Products and Building High Performing Teams with DFMA,"Hypertherm designs and manufactures plasma, laser, and water jet cutting equipment. Our products incorporate electro-mechanical assemblies, molded components, and machined parts – all of which present ample opportunities to apply both design for assembly and design for manufacture (DFMA) concepts. In this paper we'll discuss our experiences with forming cross-functional teams to apply DFMA as part of the new product development process as well as integrating it into our Lean manufacturing approach. We'll discuss a specific case study of how DFMA was applied to the recently launched Powermax45 XP plasma arc cutting system. Compared to a previous generation product, this design reduced parts count by 10%, fastener count by 20%, weight by 12%, assembly time by 15% and material cost by 6%. These material cost and operational improvements have a direct and lasting impact on the profitability of this business team. Overview of Hypertherm and the Light Industrial Business. In Lean organizations, the objective is to reduce waste to maximize delivery of value to customers. At Hypertherm DFMA complements Lean in our quest to design great products that deliver maximum value to customers while taking advantage of the collaborative experience to build high-performing teams. The case study we'll share in this paper revolves around the development and introduction of Powermax45 XP Light Industrial plasma cutting system which launched in 2016. This new product shares a lineage of more than 20 years with similar, prior systems designed by Hypertherm, including the MAX43, Powermax600, and Powermax45. These systems are designed to cut metal up to 5/8"" thickness while being very portable and affordable. Figure 1. History of the 3/8-5/8"" Light Industrial cutting systems Hypertherm, Inc., located in Hanover, New Hampshire, is a company dedicated to providing the best industrial cutting solutions in our industry (www.hypertherm.com). The company was founded in 1968 by Chairman of the Board Dick Couch and Dartmouth College Engineering Professor Emeritus Bob Dean when the pair discovered the potential to cut metal with speed, accuracy, and precision using a narrowly-focused plasma arc (Figure 2). Figure 2. The conductive electric arc used for cutting is a result of adding energy to a gas to dissociate its molecules and ionize its atoms. Our focus on technology remains strong with product offerings in several cutting-related markets: plasma arc, laser, and water jet cutting, as well as CNC motion control hardware/software and CAM software solutions (Figure 3). More than 10% of our 1430 Associates work in engineering roles and we have been awarded more than 100 patents. Of those 1430 Associates, approximately 1100 work in one of our eleven Upper Valley facilities located in and around the Hanover/Lebanon, NH area. Nearly all our manufacturing is done in the United States while the company has a global presence in 93 countries. In 2013 North American markets accounted for 45% of sales and exports to Europe, Asia, and South America accounted for the remaining 55%. Figure 3. Hypertherm plasma, laser, and waterjet cutting. Lean Enterprise Culture and Operational Excellence. A culture of continuous improvement has been central to Hypertherm's value system for more than 25 years. The pursuit of Operational Excellence (OpEx) through Lean principles (Duggan, 2011) aligns naturally with such a culture. Design for Manufacture and Assembly (DFMA) has become an essential element in our OpEx strategy, with an initial formal appearance in 2003 and subsequent re-emphasis in the last 3 years. It's well established that DFA when deployed and executed can address several of the seven Lean wastes (Shipulski, 2006) such as inventory, travel, motion, waiting, etc. by eliminating unnecessary parts and reducing parts count. Consequently, the space, time, and effort to maintain a steady stream of supply is reduced as well. Lean improvements to the value stream alone will not lead to Operational Excellence. Design improvements reducing parts count, executed through DFMA methodology, must take place as well. Our experience at Hypertherm is revealing that DFMA, when integrated into each product development cycle, can produce step change improvements in material and labor costs. In between product development cycles operations teams employ Lean principles to drive labor costs lower and reduce material costs as well through strong supply chain partnerships. Labor and material cost savings achieved over the life cycle of a product represent the collaborations of both the design and operations teams. As we'll discuss in this paper, the design community cannot be successful without the input of the operations team. Likewise, the operations community needs the design team to carefully consider design choices such that parts count, along with material and labor costs, are reduced without adverse impact on function. One of the key measures tracked at the business team level is labor cost per system produced. At Hypertherm, Lean and continuous improvement efforts, combined with DFMA, have contributed to 37% decrease in labor/system for the Powermax product family over a 3-year period. These products are manufactured in two mixed-model value streams that have been designed with Lean principles then refined through multiple continuous improvement cycles. The Powermax45 XP is assembled in a 5station flow cell that feeds a shared functional test/burn in area before being packaged for sale (Konstantakos, 2016). The Operations team responsible for this manufacturing value stream has invested in understanding the root causes of imbalance and variability in assembly operations leading to continuous improvement in product throughput. Hypertherm received its formal introduction Design for Manufacture and Assembly in 2003 as part of a major redesign effort with a large, automated cutting system (Shipulski, 2006). Since that project launch DFMA has largely taken place through ""organic"" efforts within the company. Because of the costcompetitive landscape that the 45XP would launch into we decided that a more organized and structured approach would be beneficial. This should not imply that was not already happening within the design team just that an infusion of innovative thinking would be needed to insure the success of the product. Role of Learning DFMA in New Product Development of Powermax45 XP. To accomplish the business objectives of reducing cost and assembly complexity we explored enhancing our long-standing (but somewhat dormant) relationship with Boothroyd-Dewhurst. In addition, organizational learning objectives were also particularly important for many of our younger engineers who may not have had prior exposure to DFMA concepts. During early stages of the product development process we began arrangements for a Value Engineering and DFMA workshop that would combine both learning with execution. The learning element was an important consideration: at Hypertherm we have tried to align our strategic improvement programs with a learning program methodology known as the 6 Disciplines (Jefferson and Wick, 2015). While we are still in the process of implementing several of these elements effectively, these serve as valuable guidelines for our learning organization: Define business outcomes (not just learning objectives). With DFMA the business outcomes are largely built-in: decrease parts count, reduce assembly complexity, cut assembly time, and drive down material costs. For our 45XP project we knew we had a 10% (material) cost reduction goal and that the strategy would necessarily rely heavily on DFMA. In addition, being relatively early in the product development timetable we felt that the team could incorporate a value engineering element into the workshop to promote a more functional approach to the design. Finally, less tangible but equally important goals included promoting collaboration, communication, relationship building, and peer-peer networking in our engineering community. We believe that these are important elements is building a high performing team. Define a complete learning experience (including transfer to work). A secondary, learning oriented goal was to have approximately 20 members of the engineering community trained or refreshed in DFMA concepts as well as software tools for application. With over 120 engineers on staff at Hypertherm we knew that this would be only one of several workshops that would eventually take place. We felt it was important to connect the workshop to a tangible product development project to avoid training for training's sake (i.e. just hoping that learning transfer would come at some later date). Everyone attending the workshop would contribute to the product design in some way. Deliver learning for application and make learning easy. Chris Tsai, Director of DFMA for Boothroyd Dewhurst, became our contact for designing the workshop. Chris shares the philosophy of implementation-based learning so it was relatively easy to adapt the value engineering and DFMA implementation workshop approach for the project. Drive learning transfer and avoid learning ""scrap"". On the Powermax45 XP project we were fortunate to have strong support for DFMA from the Engineering Team leader, Product Development Project leader, and design engineers themselves. After the workshop the application of DFMA principles was reinforced within the team on a regular basis. Deploy performance support. Leadership attended the workshop as well so they were wellequipped to be the first level of performance support. Other key support attributes that we need to improve on are availability of resources (people and subject matter experts; reference documents, tools, and templates), that are practical, clear, and concise. We believe that this will come in the form of key, bite-sized e-learning reference/brush-up modules, as well as a library of tools, templates, and refere",,2017.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
71af6878cf6528e2df24a856809106059484e259,https://www.semanticscholar.org/paper/71af6878cf6528e2df24a856809106059484e259,Developments in Artificial Intelligence – Opportunities and Challenges for Military Modeling and Simulation,"One of the principal themes the NATO Science and Technology Organization (STO) is fostering in 2017 is ""Military Decision Making using the tools of Big Data and Artificial Intelligence (AI)"". Simulation might play a significant role to play in these developments as it can act as a testbed for such concepts and support the military decision makers in future operations that are enhanced by AI. Simulation is already making a significant impact in the development of AI outside of the defence sector. Companies such as DeepMind and Nvidia are using computer games and simulations to “train” AI and autonomous systems, analogous to humans training in simulations. The rate of progress is high, driven by increases in computing power, availability of data and improved algorithms. AI can now “beat” humans at many computer and board games and is moving towards tackling more strategic games that have parallels with military C2. If such developments translate into the defence sphere then we could foresee humans and autonomous systems training in the same simulation systems, both separately and together, and the AI in the autonomous system being the same as that in the simulation. As autonomous systems proliferate across the nations, M&S technology and techniques might be used to improve the interoperability of autonomous systems. To maximise such synergies, it will be essential that NATO embraces all communities that have an interest in AI. Assessing the risks of potential adversary’s use of AI and commercial autonomous systems is also necessary. Despite recent advances, AI development still faces significant technological and ethical challenges and these must be monitored and addressed as necessary. 1.0 CONTEXT Artificial Intelligence or AI is a technology or concept that has developed over many decades and periodically becomes mainstream news. In the latter half of the 2010s this is still very much the case with regular forecasts of its impact on society, jobs and the world economy. Some of these predictions appear to be nearing reality and AI devices are even entering the home. AI also has the potential to influence and sometimes disrupt the ways that companies and organisations operate and AI-based technology revolutions are anticipated. AI also has enduring impact on media and culture and like much technology it can be considered to have beneficial and harmful uses and its impact has and will have political and ethical implications. Over the decades many AI predictions have come true to some degree but in some cases not at all or only partially. Good examples of this are to be found in transport where passenger aircraft have considerable levels of automation but society remains some way off from accepting pilotless passenger aircraft. For STO-MP-MSG-149 11 1 Developments in Artificial Intelligence – Opportunities and Challenges for Military Modeling and Simulation railways, some are now fully automated whilst others continue to put high reliance on the human. Cars can now park themselves and have high levels of automation but are yet to be fully autonomous in all environments and applications. There is no doubt however, that there is a trend towards greater use of autonomous systems and AI. This is being driven by ever greater processing power together with the ability for very large data sets (“big data”) to be captured and used to help build more capable AI. Such resources can also be accessed online in the cloud, driving down the cost of developing and distributing AI programs. The military have developed and deployed autonomous systems for a very long time, for example in the use of land and sea mines. In the 20th century proximity fuzes came into service that were semi-intelligent, sensing and exploding at the most appropriate time for the target. Analogue computers also assisted operators as part of fire control calculations and missiles and rockets in World War 2 become remotely piloted or fully autonomous. With the advent of digital computing, autonomy in military systems is commonplace, reducing the manpower requirement or in assisting the human, but there remains a significant ethical dimension in the use of fully autonomous systems. Developments in AI and autonomous systems outside of defence are of significant interest as they may provide answers how to better manage and interpret data within military command and control systems but also because they may enhance potential adversary’s capabilities. This was recognised in 2017 by the NATO Science and Technology Organization’s (STO) which made ""Military Decision Making using the tools of Big Data and Artificial Intelligence (AI)"" one of its principal themes. The modelling and simulation (M&S) community has strived itself to develop AI, reducing or eliminating the need for human input. Sometimes termed Semi-Automated Forces (SAF) or Computer-Generated Forces (CGF) this has benefits in analysis and training, reducing the number of role players and improving consistency. Simulation itself is now being used to “train” AI/autonomous systems, as such environments are repeatable and controllable and can generate highly tailorable data output. However, reproducing credible and realistic behaviours in simulation remains a significant challenge and the M&S community continues to strive to enhance its AI. The computer games industry also sees AI as a challenge as games can easily lose their entertainment value if their AI is poorly implemented. 2.0 WHAT IS ARTIFICIAL INTELLIGENCE? Artificial intelligence (AI) is a broad topic area as it depends on what nature of human intelligence are being replicated and that AI technology can take many different forms. The Oxford Dictionary definition is “The theory and development of computer systems able to perform tasks normally requiring human intelligence, such as visual perception, speech recognition, decision-making, and translation between languages.” The nature of the intelligence can range from “narrow” intelligence which is highly tailored or specialised through to artificial “general” intelligence which is flexible, adaptive and inventive, much like the human brain. There are many approaches to AI for example, decision tress, fuzzy logic and neural nets with some approaches becoming synonymous with AI. For example, machine learning is an approach that gives ""computers the ability to learn without being explicitly programmed” by learning from and making predictions from data. In broad terms AI is the ‘what’, machine learning is an approach to the ‘how’, and self-driving cars might be the ‘why’. Machine learning methods are based on learning data representations, as opposed to task-specific algorithms. Learning can be supervised, partially supervised or unsupervised. Neural nets or networks are computer systems modelled on the human brain and nervous system with an interconnected group of nodes, akin to the vast network of neurons in a brain. Deep or reinforcement learning, which is inspired by the way animals seem to learn, has taken the neural nets approach and added layers of nodes taking advantage of current day higher processing power and making significant advances in image recognition for example and is generally seen as at the current forefront of AI technology. Developments in Artificial Intelligence – Opportunities and Challenges for Military Modeling and Simulation 11 2 STO-MP-MSG-149 An autonomous system builds on the use of AI and extends it into the physical world, in for example a robot or vehicle, requiring an awareness of the world through sensors, a task(s) and minimal human intervention. Some argue that some AI algorithms are not really showing intelligence but are a predetermined and limited set of responses to a predetermined and limited set of inputs. Professor Isbell of Georgia Tech suggests that systems should have two features before they can be considered AI. Firstly, they must learn over time as their environment changes. Secondly, their challenge must be demanding too for humans to learn, so a machine programmed to automate repetitive work would not be considered an AI system. Another example would be the AI in many computer video games; it may appear to represent human behaviour but this is preprogrammed and there is little or no learning over time. 3.0 HISTORICAL CONTEXT An overview of the general history of AI including work on AI by the M&S community will be provided to give context to the progress currently being made and the possible trajectory of the field into the future.",,2017.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
d3a340e3d61ea3d80309909839479b757fe8a37c,https://www.semanticscholar.org/paper/d3a340e3d61ea3d80309909839479b757fe8a37c,DPWeka: Achieving Differential Privacy in WEKA A thesis submitted in partial fulfillment of the requirements for the degree of Master of Science in Computer Science by,"Organizations belonging to the government, commercial, and non-profit industries collect and store large amounts of sensitive data, which include medical, financial, and personal information. They use data mining methods to formulate business strategies that yield high longterm and short-term financial benefits. While analyzing such data, the private information of the individuals present in the data must be protected for moral and legal reasons. Current practices such as redacting sensitive attributes, releasing only the aggregate values, and query auditing do not provide sufficient protection against an adversary armed with auxiliary information. In the presence of additional background information, the privacy protection framework, differential privacy, provides mathematical guarantees against adversarial attacks. Existing platforms for differential privacy employ specific mechanisms for limited applications of data mining. Additionally, widely used data mining tools do not contain differentially private data mining algorithms. As a result, for analyzing sensitive data, the cognizance of differentially private methods is currently limited outside the research community. This thesis examines various mechanisms to realize differential privacy in practice and investigates methods to integrate them with a popular machine learning toolkit, WEKA. We present DPWeka, a package that provides differential privacy capabilities to WEKA, for practical data mining. DPWeka includes a suite of differential privacy preserving algorithms which support a variety of data mining tasks including attribute selection and regression analysis. It has provisions for users to control privacy and model parameters, such as privacy mechanism, privacy budget, and other algorithm specific variables. We evaluate private algorithms on realworld datasets, such as genetic data and census data, to demonstrate the practical applicability of DPWeka.",,2017.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
cedfd8e152345764404a0dcd3b9c682a1b55fffd,https://www.semanticscholar.org/paper/cedfd8e152345764404a0dcd3b9c682a1b55fffd,The Case for Compulsory Approximation,"Approximation is a fundamental concept in some application domains. In the next phase of research on approximate computing, the community should absorb lessons and constraints from these fields with compulsory approximation. This essay (1) surveys domains with compulsory approximation; and (2) advocates for research that builds new abstractions for old approximations. 1. Approximate Computing’s Adolescence Research on approximate computing has aged out of its infancy. We have an initial slate of approximate hardware designs, new compiler optimizations for approximation on current hardware, and language tools to control accuracy–efficiency trade-offs. But the field has taken only tentative steps toward broader impact: vexing but valid concerns, like the feasibility of quality guarantees, still hinder widespread adoption of approximate systems. As approximate computing begins to mature, the community should reflect on its strategy. Here are two reasonable visions for the next phase of approximation research: • Status quo. We keep developing general-purpose techniques for approximation and quality control. If we push the quality– efficiency trade-off far enough for enough benchmarks, industry will eventually adopt approximation. This direction involves only shallow engagement with individual application domains. • Technology transfer via case study. Researchers embed with specific application domains where we know approximation can be effective. We learn what it will take to “sell” our favorite techniques, from neural accelerators to stochastic logic circuits, to experts in those domains. These deployments will serve as case studies to inform and encourage broader adoption. This essay advocates for a third, less obvious strategy to complement these directions. Instead of developing new approximation strategies from whole cloth and then working to apply them, the community should go to where approximations are already widespread. In domains where approximation is a fact of life, not an optional luxury, there is no “selling” necessary: approximate computing is already deployed. We call these instances of approximate computing compulsory approximation. This essay makes the case for hunting approximation ideas in the wild. We should bring techniques from domains with compulsory approximation into the approximate-computing fold. The potential benefits are twofold: 1. The domains themselves stand to benefit from new abstractions for approximations they already use. Even established domainspecific approximation strategies can be ad hoc and difficult to control; the techniques and tools we have developed in the approximate-computing community can help make them more principled. 2. Approximate computing as a whole can benefit from expertise that is currently locked away within application domains. These domains are older and more mature than approximate computing as a buzzword, so they have long contended with problems that our community is only beginning to recognize. We enumerate examples of domains with compulsory approximation and suggest ways that our community can engage with them. 2. Compulsory Approximation Domains This section surveys examples of compulsory approximation. We identify domain-specific approximations that (1) have something to teach us in the approximate-computing community; and (2) have problems that we can help address by applying approximation ideas. 2.1. Machine Learning Machine learning’s charter is to approximate problems that we cannot solve exactly—or even precisely define. And while machine learning and AI have undergone a full-scale revolution over the last five years, they can still be difficult to trust. For example: deep-learning implementers use a technique called dropout, which randomly deletes neurons from networks during training [9]. Dropout appears to avoid overfitting, leading to a better training result—but it is hard to see exactly why it works or explain when it might go wrong. Similarly, Hogwild! is a technique for parallel stochastic gradient descent that ignores data inconsistency [6]; its sensitivity to the underlying machine’s memory model is unclear. Summing up these implementation problems and more, a Google paper recently called machine learning “the high-interest credit card of technical debt”: while ML can accomplish amazing feats quickly, the reasons for its success (and, eventually, failure) can be murky [7]. We can learn: Processes and policies for measuring quality and deciding when it is good enough to ship. We can offer: Tools for expressing and enforcing quality requirements in the language, especially when composing learning components into larger systems. 2.2. Numerical & Scientific Computing Floating-point numbers are the world’s oldest and most widespread deployment of approximate computing. In scientific computing, the practice of bounding floating-point error is just as mature. The floating-point error problem has spawned an entire field of study, numerical analysis, and many textbooks devoted to the topic [1]. Numerical computing represents an extraordinarily thorough case study in high-overhead, manual approaches to deriving strong accuracy properties. We can learn: Mathematical tools for deriving hard error bounds when the error model resembles floating-point rounding. The same techniques may not generalize to other approximation strategies— random bit flips or neural accelerators, for example—but they can still be valuable when errors are well behaved. We can offer: Tools to automate tedious accuracy analyses and transformations that currently require experts. Panchekha et al.’s Herbie tool for fixing numerical stability problems is an important first step [5]. 2.3. Real-Time Graphics In games and other real-time graphics applications, everything is a compromise. Fundamentally, GPU-accelerated rasterization is a faster but worse alternative to full ray tracing. Game engines will go to extremes to maintain a smooth frame rate by simplifying scenes wherever possible.1 Level-of-detail (LOD) techniques simplify objects for quick-and-dirty rendering when they are rendered in the background of a complex scene [2]. We can learn: Low-overhead techniques for tuning accuracy at run time in response to resource demands. The soft real-time constraints in game engines require them to adapt to changing conditions; the same capability should apply in other domains. We can offer: Tools for navigating the huge space of individual approximation decisions that game developers typically implement.",,2016.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
4f8c4025b2ce0c0eb837b7389bba31f6f1cae7c7,https://www.semanticscholar.org/paper/4f8c4025b2ce0c0eb837b7389bba31f6f1cae7c7,Implementing Operational Analytics using Big Data Technologies to Detect and Predict Sensor Anomalies,"Operational analytics when combined with Big Data technologies and predictive techniques have been shown to be valuable in detecting mission critical sensor anomalies that might be missed by conventional analytical techniques. Our approach helps analysts and leaders make informed and rapid decisions by analyzing large volumes of complex data in near real-time and presenting it in a manner that facilitates decision making. It provides cost savings by being able to alert and predict when sensor degradations pass a critical threshold and impact mission operations. Operational analytics, which uses Big Data tools and technologies, can process very large data sets containing a variety of data types to uncover hidden patterns, unknown correlations, and other relevant information. When combined with predictive techniques, it provides a mechanism to monitor and visualize these data sets and provide insight into degradations encountered in large sensor systems such as the space surveillance network. In this study, data from a notional sensor is simulated and we use big data technologies, predictive algorithms and operational analytics to process the data and predict sensor degradations. This study uses data products that would commonly be analyzed at a site. This study builds on a big data architecture that has previously been proven valuable in detecting anomalies. This paper outlines our methodology of implementing an operational analytic solution through data discovery, learning and training of data modeling and predictive techniques, and deployment. Through this methodology, we implement a functional architecture focused on exploring available big data sets and determine practical analytic, visualization, and predictive technologies. APPROACH This study developed an operational analytics implementation that uses Big Data technologies and machine learning algorithms to determine and predict sensor anomalies. A previous study [1] showed that Big Data Analytics can uncover anomalies that may be missed through conventional analyses. This study enhances that effort and shows a methodology to implement operational analytics that can be applied toward common solutions for data analysis. Our operational analytics implementation relies on continuous learning from historical data to analyze data in the stream of real-time operations. In the previous study, where data was identified that can be used to uncover anomalies, this implementation extends that approach and now identifies trends and correlations that reveal anomalies that can be missed by traditional analytic techniques with limited datasets. This study adopted a three-step methodology to implementing operational analytics – Discovery, Modeling and Operations as shown in Fig. 1. Copyright © 2016 Advanced Maui Optical and Space Surveillance Technologies Conference (AMOS) – www.amostech.com Fig. 1. Operational Implementation Approach Fig. 1 shows the three steps to implement operational analytics and the continuous feedback between learning and operational deployment. The following sections will elaborate on the methodology employed as applied to a realworld problem of analyzing large datasets such as would be encountered at an operational site.",,2016.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
2db4aecb770967dc75aea40138dcf916525e0a43,https://www.semanticscholar.org/paper/2db4aecb770967dc75aea40138dcf916525e0a43,To Share or not to Share: Access Control and Information Inference in Social Networks,"Online social networks (OSNs) have been the most successful online applications during the past decade. Leading players in the business, including Facebook, Twitter and Instagram, attract a huge number of users. Nowadays, OSNs have become a primary way for people to connect, communicate and share life moments. Although OSNs have brought a lot of convenience to our life, users' privacy, on the other hand, has become a major concern due to the large amount of personal data shared online. In this thesis, we study users' privacy in social networks from two aspects, namely access control and information inference.

Access control is a mechanism, provided by OSNs, for users themselves to regulate who can view their resources. Access control schemes in OSNs are relationship-based, i.e., a user can define access control policies to allow others who are in a certain relationship with him to access his resources. Current OSNs have deployed multiple access control schemes, however most of these schemes do not satisfy users' expectations, due to expressiveness and usability.

There are mainly two types of information that users share in OSNs, namely their activities and social relations. The information has provided an unprecedented chance for academia to understand human society and for industry to build appealing applications, such as personalized recommendation. However, the large quantity of data can also be used to infer a user's personal information, even though not shared by the user in OSNs.

This thesis concentrates on users' privacy in online social networks from two aspects, i.e., access control and information inference, it is organized into two parts.

The first part of this thesis addresses access control in social networks from three perspectives. First, we propose a formal framework based on a hybrid logic to model users' access control policies. This framework incorporates the notion of public information and provides users with a fine-grained way to control who can view their resources. Second, we design cryptographic protocols to enforce access control policies in OSNs. Under these protocols, a user can allow others to view his resources without leaking private information. Third, major OSN companies have deployed blacklist for users to enforce extra access control besides the normal access control policies. We formally model blacklist with the help of a hybrid logic and propose efficient algorithms to implement it in OSNs.

The second part of this thesis concentrates on the inference of users' information in OSNs, using machine learning techniques. The targets of our inference are users' activities, represented by mobility, and social relations. First, we propose a method which uses a user's social relations to predict his locations. This method adopts a user's social community information to construct the location predictor, and perform the inference with machine learning techniques. Second, we focus on inferring the friendship between two users based on the common locations they have been to. We propose a…",,2016.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
a17b7080c281af230ade8bd680ba0c43e6d70ef1,https://www.semanticscholar.org/paper/a17b7080c281af230ade8bd680ba0c43e6d70ef1,Modelling the instrumental value of software requirements,"Numerous studies have concluded that roughly half of all implemented software requirements are never or rarely used in practice, and that failure to realise expected benefits is a major cause of software project failure. This thesis presents an exploration of these concepts, claims, and causes. It evaluates the literature s proposed solutions to them, and then presents a unified framework that covers additional concerns not previously considered.

The value of a requirement is assessed often during the requirements engineering (RE) process, e.g., in requirement prioritisation, release planning, and trade-off analysis. In order to support these activities, and hence to support the decisions that lead to the aforementioned waste, this thesis proposes a framework built on the modelling languages of Goal Oriented Requirements Engineering (GORE), and on the principles of Value Based Software Engineering (VBSE). 

The framework guides the elicitation of a requirement s value using philosophy and business theory, and aims to quantitatively model chains of instrumental value that are expected to be generated for a system s stakeholders by a proposed software capability. The framework enriches the description of the individual links comprising these chains with descriptions of probabilistic degrees of causation, non-linear dose-response and utility functions, and credibility and confidence. A software tool to support the framework s implementation is presented, employing novel features such as automated visualisation, and information retrieval and machine learning (recommendation system) techniques. These software capabilities provide more than just usability improvements to the framework. For example, they enable visual comprehension of the implications of what-if? questions, and enable re-use of previous models in order to suggest modifications to a project s requirements set, and reduce uncertainty in its value propositions.

Two case studies in real-world industry contexts are presented, which explore the problem and the viability of the proposed framework for alleviating it. The thesis research questions are answered by various methods, including practitioner surveys, interviews, expert opinion, real-world examples and proofs of concept, as well as less-common methods such as natural language processing analysis of real requirements specifications (e.g., using TF-IDF to measure the proportion of software requirement traceability links that do not describe the requirement s value or problem-to-be-solved).

The thesis found that in general, there is a disconnect between the state of best practice as proposed by the literature, and current industry practice in requirements engineering. The surveyed practitioners supported the notion that the aforementioned value realisation problems do exist in current practice, that they would be treatable by better requirements engineering practice, and that this thesis proposed framework would be useful and usable in projects whose complexity warrants the overhead of requirements modelling (e.g., for projects with many stakeholders, competing desires, or having high costs of deploying incorrect increments of software functionality).",,2015.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
c8003f27c8965688c6c2499759e0350ab6f3c608,https://www.semanticscholar.org/paper/c8003f27c8965688c6c2499759e0350ab6f3c608,Ambient Assisted Living for the Motor Impaired,"The field of Ambient Assisted Living (AAL) has shown great potential in counteracting some of the effects of the worldwide population ageing phenomenon. Its main goal is to promote a safe, healthy, and functional living environment for the elderly and people with disabilities who wish to live independently in their home. To achieve this goal, AAL environments utilize Information and Communication Technologies (ICTs) and the emerging Ambient Intelligence (AmI) paradigm in order to provide sophisticated solutions that can support the needs of an elderly person or a person with disabilities, at home. This chapter will present examples of AAL environments found in research and academic literature and the solutions they offer to cater for the basic needs of motor-impaired people in order to support their independent living and quality of life. The challenges of using such technologies will also be discussed. INTRODUCTION The World Health Organization (WHO) World Report on Disability (World Health Organization, 2011), states that approximately one billion people worldwide experience a disabling condition. This is the first ever global estimate of persons with disabilities in the last 40 years. The term “disabled” according to WHO is used for people who are experiencing a limitation in their movement, activities, and senses due to a physical or mental condition. The report also states that almost everyone will be temporarily or permanently impaired at some point in his or her life, especially when at old age. At the same time, people with disabilities are more susceptible to poorer health outcomes and lower education achievements, which often lead to higher rates of poverty than people without disabilities. Disabled people are in need of rehabilitation services in order to maximize their functioning required to support independence. But in developing countries such access to rehabilitation services is often limited and in some cases nonexistent altogether. Even in high-income countries about 20%40% of people with disabilities have limited assistance for their everyday activities. In the US, for example, 70% of adults have to rely on family and friends for assistance with daily activities. The number of people experiencing motor impairments and other disabilities is only expected to rise in the near future, as the world population continues to age at an unprecedented rate. According to the United Nations World Population Ageing report (United Nations, 2000), worldwide population ageing is enduring and has a growing rate of 2.6%per year, considerably faster than the population as a whole, which is increasing at 1.2 % annually. Europe is currently holding the highest proportion of older persons, with a population of 60 or over currently constituting 24.5% of its total population. In the United States that number is 19.1% respectively. From the above, it can be argued that the increase of the ageing population will have major implications for all aspects of people’s everyday life particularly of socio-economic nature. The number of people that will need some form of institutionalized help is going to increase, adding on the burden of the existing health care systems. Governments around the world have taken serious notice of this reality and of the need to come up with strategies to adapt their social practices and processes in order to accommodate this dynamic population shift in the population. The need to find ways to make it easier for people with age and other related disabilities to live a longer, satisfying and independent life in their own homes is now more imperative than ever. Ambient Assisted Living (AAL) is a domain that has attracted a steadily growing attention in the scientific community because it involves emerging innovative technological solutions that can counteract some of the challenges described above. The main focus in AAL is on supporting persons with disabilities in their own environment and providing the means to increase the degree of independent living. Its aim is to provide integral solutions in the areas of home care, independent living, and institutionalized care homes that will improve the quality of life and lower the costs involved with health, home care and related social services. In order to achieve the above, AAL depends heavily on Information and Communication Technologies (ICTs) and the emerging Ambient Intelligence paradigm. This chapter provides an overview of how Ambient Assisted Living technologies can play a catalytic role in improving the living environment for people with motor impairments by providing solutions that can increase their level of independence. The chapter begins with an overview of the fields of Ambient Intelligence and Ambient Assisted Living, followed by a brief presentation of the latest research initiatives in Europe. It then discusses how AAL can provide solutions for the fulfillment of the four identified requirements for independent living: mobility, environment control, safety, health and emergency assistance, and social inclusion. Finally, the major challenges of AAL are discussed followed by the conclusion. AMBIENT ASSISTED LIVING (AAL) OVERVIEW AAL refers to the use of Information and Communication Technologies (ICT) in a person's living environment in an unobtrusive way enabling them to continue living a comfortable, independent, active life and staying socially connected well into old age. AAL’s main goal is to provide the technological platform to support individuals in living an autonomous life for as long as possible. The roots of AAL are in traditional Assistive Technologies for people with disabilities, ‘Design for All’ approaches to usability and accessibility, as well as in the emerging computing paradigm of Ambient Intelligence (Pieper, Antona, Cortes, 2011). Ambient Intelligence (AmI) is a term that refers to the vision of a world in which smart, intuitively operated devices support users in an unobtrusive way in their everyday life. AmI has enabled the introduction of ubiquitous information, computational, and communication technology in a seamless yet unobtrusive way creating smart everyday living environments (Encarnação & Kriste, 2005). In such smart environments, intelligent applications and devices become aware of the human goals and needs by operating collectively and sharing information and intelligence through a hidden network that connects them in a way that is natural and intuitive to the user (Aarts & Encarnação, 2008). AmI infrastructures have gained a great momentum in today’s world in many industries such as home automation, entertainment, automotive, and healthcare to name a few. The technologies involved have the capacity to transform everyday common objects from CD players to coffee machines into smart objects that support context awareness, personalization, anticipatory behavior, and adaptation, all of which enable a certain degree of autonomous decision-making. Lighting, sound, vision, home appliances, and other electronic devices, all come into play in an AmI environment and share the same purpose, to improve user experience by facilitating the user’s interaction with it (Aarts & Kriste, 2005). Aarts and Encarnação (2008) stated that the notion intelligence reflects that the digital surroundings in a smart environment exhibit certain forms of social interaction, in other words they are able to recognize the occupants, adapt themselves to their needs, learn from their behavior, and possibly act on their behalf. Based on the described notion of intelligence they have synthesized the following list of the most important features of Ambient Intelligence: • Integration through large-scale embedding electronics into the environment • Context-awareness through user, location, and situation identification – the system uses sensors to perceive a situation, the location where that situation is taking place, and the user involved • Personalization through interface and service adjustment – the system can change its behavior according to the needs of the user • Adaptation through learning the user’s behaviors • Anticipatory behavior through reasoning – the system acts on behalf of the user making decisions based on predictions and expectations about future actions AAL with the help of the Ambient Intelligence (AmI) paradigm and new ICT technologies can now provide smart sophisticated solutions that offer the potential to change dramatically the quality of life for a disabled person, often making the difference from living with personal assistance on a daily basis to living an autonomous life. One of AAL’s focal concerns is also to offer user-friendly interfaces that are adaptable to the needs and abilities of the user and user-centric methods of interaction for the individual with his or her immediate environment (Pieper et al, 2011). AAL RESEARCH INITIATIVES IN EUROPE In the recent years, policy initiatives have been launched in Europe on the field of Ambient Assisted Living (AAL) in order to create a favorable ground towards research, development, and deployment of ICT technologies with focus on addressing the challenges, but also the opportunities of ageing. The “Ageing Well in the Information Society” Action Plan was adopted in June 2007 by the European Commission with the goal to bring forward a package of measures that should lead to greater uptake of ICTs by Europe’s senior citizens and stimulate industry to produce technologies appropriate for them (Stephanidis, 2011). For that purpose, the European Commission launched a dedicated action in the 7 Framework Programme and partial funding of the Ambient Assisted Living Joint Research and Innovation Programme, involving most EU Member States (Stephanidis, 2011). By 2013, the EU and Member States, and the private sector will have invested more than €1 billion in research and innovation for ageing well: some €600m in the Ambient Assisted Living Joint Programme (AAL JP), and an ",,2013.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
5d85b10ed56bcbc556c54916ce520b53209837e8,https://www.semanticscholar.org/paper/5d85b10ed56bcbc556c54916ce520b53209837e8,Pattern-Oriented Application Frameworks for Domain Experts to Effectively Utilize Highly Parallel Manycore Microprocessors,"Manycore microprocessors are powerful computing engines that are architected to embrace the use of parallelism to extract computational throughput from the continued improvements in the semiconductor manufacturing process. Yet the performance of the software applications running on these microprocessors is highly sensitive to factors such as data layout, data placement, and synchronization. These factors are not usually part of an application domain experts daily concerns, as they look to utilize the powerful compute capabilities of manycore microprocessors for their applications, but failure to carefully address these concerns could mean an order of magnitude of loss in application execution latency and/or throughput. With the proliferation of manycore microprocessors from servers to laptops and portable devices, there is increasing demand for the productive development of computationally efficient business and consumer applications in a wide range of usage scenarios. The sensitivity of execution speed to software architecture and programming techniques can impede the adoption of the manycore microprocessors and slow the momentum of the semiconductor industry. 
This thesis discusses how we can empower application domain experts with pattern-oriented application frameworks, which can allow them to effectively utilize the capabilities of highly parallel manycore microprocessors and productively develop efficient parallel software applications. Our pattern-oriented application framework includes an application context for outlining application characteristics, a software architecture for describing the application concurrency exploited in the framework, a reference implementation as a sample design, and a set of extension points for flexible customization. 
We studied the process of accelerating applications in the fields of machine learning and computational finance, specifically looking at automatic speech recognition (ASR), financial market value-at-risk estimation (VaR), and financial potential future exposure (PFE). We present a pattern-oriented application framework for ASR, as well as efficient reference implementations of VaR and PFE. For the ASR framework, we demonstrate its construction and two separate deployments, one of which flexibly extends the ASR framework to enable lip-reading in high-noise recognition environments. The framework enabled a Matlab/Java programmer to effectively utilize a manycore microprocessor to achieve a 20x speedup in recognition throughput as compared to a sequential CPU implementation. 
Our pattern-oriented application framework provides an approach for crystallizing and transferring the often-tacit knowledge of effective parallel programming techniques while allowing for flexible adaptation to various application usage scenarios. We believe that the pattern-oriented application framework will be an essential tool for the effective utilization of manycore microprocessors for application domain experts.",,2010.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
7a3dc8e0dd0dda53871f19a7797078a38ac51ea1,https://www.semanticscholar.org/paper/7a3dc8e0dd0dda53871f19a7797078a38ac51ea1,"Cloud Computing, First International Conference, CloudCom 2009, Beijing, China, December 1-4, 2009. Proceedings",,CloudCom,2009.0,10.1007/978-3-642-10665-1,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
8eb202d8d9c5f39219f70aeabc52db6e597848fa,https://www.semanticscholar.org/paper/8eb202d8d9c5f39219f70aeabc52db6e597848fa,A Framework for Action Detection in Virtual Training Simulations Using Synthetic Training Data,"In virtual military training, tracking and evaluating trainee behavior throughout a simulation exercise helps address specific training needs, improve the realism of simulations, and customize the training experience. While it is straightforward to parse the event log of a simulation to identify atomic behaviors such as unit movements or attacks, it remains difficult to fuse these events into higher-level actions that better characterize trainees’ intentions and tactics. For example, if each unit is controlled by an individual trainee, how should the movement information from all units be aggregated to determine what formation the group is moving in? Similarly, how can all of the information from nearby terrain environments be combined with kinetic actions to determine whether the trainees are executing an ambush attack, or is simply engaging the enemy group? While an experienced human observer-controller can quickly assess the battle map to provide an appropriate interpretation for such events, it remains a challenging task for computers to automatically detect such high-level behaviors when performed by human trainees. In this work, we proposed a machine-learning (ML) framework for recognizing tactical events in virtual training environments. In our approach, unit movements, surrounding environments, and other atomic events are rasterized as a 2D image, allowing us to solve the action detection problem as image classification and video temporal segmentation tasks. In order to bootstrap ML models for these tasks, we utilize synthetic training data to procedurally generate a large amount of annotated data. We demonstrate the effectiveness of this framework in the context of a virtual military training prototype, detecting troop formations and other tactical events such as ambush and patrolling. ABOUT THE AUTHORS Andrew Feng is a Research Scientist at the Institute for Creative Technologies at the University of Southern California, working on the One World Terrain project. Previously, he was a research associate focusing on character animation and automatic 3D avatar generation. His research work involves applying machine learning techniques to solve computer graphics problems such as animation synthesis, mesh skinning, and mesh deformation. He received his Ph.D. and MS degree in computer science from the University of Illinois at Urbana-Champaign. Email: feng@ict.usc.edu Andrew S. Gordon is a Research Associate Professor of Computer Science and Director of Interactive Narrative Research at the Institute for Creative Technologies at the University of Southern California. His research advances technologies for automatically analyzing and generating narrative interpretations of time-series data. He received his Ph.D. in 1999 from Northwestern University. Email: ​gordon@ict.usc.edu 2020 Paper No. 20302 Page 1 of 12 Interservice/Industry Training, Simulation, and Education Conference (I/ITSEC) A Framework for Action Detection in Virtual Training Simulations Using Synthetic Training Data Andrew Feng, Andrew S. Gordon Institute for Creative Technologies University of Southern California Los Angeles, CA USA feng@ict.usc.edu, gordon@ict.usc.edu INTRODUCTION Among the most compelling use-cases for interactive virtual simulations is for virtual team training exercises, e.g., where teams of warfighters control their own virtual avatars in simulated battles, practicing tactical maneuvers and essential skills in increasingly challenging scenarios. While these computer-based exercises lack the physical stresses of live training, their great potential lies in their capacity for guided deliberate practice of skills, where the environments themselves are tailored to the abilities of the team and responsive to their successes and failures in their performance. In both live and virtual training, however, this responsiveness has generally required the participation of human facilitators, i.e., Observer Controllers that track the progress of trainees toward learning objectives, and actively shape the learning environment to further this progress. Automating these capabilities of human Observer Controllers would have several practical benefits for virtual training for operational units. Replacing training support staff with software reduces labor costs, at the very least. More importantly, such automation can help remove the reliance of operational units on outside contractors and home-station training facilities in conducting virtual training exercises. As this reliance is reduced, commanders are more able to lead their own training exercises, on their own schedules, wherever their point of need.. Much of the difficulty in automating the Observer Controller stems from the simulation environment's lack of understanding of what the human trainees are actually doing in the virtual space. Seeing a given formation and orientation of trainee avatars on a virtual ridge line, for example, it might be obvious to a human Observer Controller that the trainees were setting up for a deliberate ambush of an approaching enemy force, prompting him or her to modify the enemy's reaction to reinforce lessons related to the performance of this tactical maneuver. The software of the virtual simulation environment, however, would likely be oblivious to the impending ambush altogether, aware only of the avatars' positions in virtual space and the avatar controls provided by the trainees' user interfaces. Automating the human Observer Controller, in this case, requires an ability to recognize complex group behaviors based on the positions and individual actions of avatars in the group, within the battlefield context. Analogous perceptual recognition tasks have been successfully automated in other domains using contemporary machine learning technologies, such as deep neural networks. Here, recognizing the complex behaviors of teams of trainees in virtual environments can be seen as a type of time-series classification task. The input consists of positional and low-level simulation event data over time, and output assigns labels to durations that best characterize the class of behavior that is being executed, from a given vocabulary. Today, a high-accuracy classifier of this sort could easily be constructed using supervised learning methods, provided that enormous amounts of data were available, expertly annotated with high levels of inter-rater agreement. However, the required amounts of data (perhaps tens of thousands of examples) is far beyond what might be reasonably obtained from the deployed use of any existing virtual training software. Even assuming such large-scale datasets could be collected, its annotation by teams of experts would be especially costly and difficult, given the sensitive nature of data generated during military training exercises. If contemporary machine learning technologies are to be used to automate functions of human Observer Controllers, an alternative approach to the collection and annotation of training data is needed. In this paper, we investigate an alternate approach to the collection and annotation of datasets for behavior recognition using machine learning methods. Our approach involves the automatic generation of synthetic training data, collected by authoring behavior programs to be executed thousands of times by teams of fully-autonomous 2020 Paper No. 20302 Page 2 of 12 Interservice/Industry Training, Simulation, and Education Conference (I/ITSEC) agents within the target simulation environment. Multiple programs are authored, one for each of the classes of behavior that is to be recognized in teams of human trainees, which allows durations of generated data to be automatically annotated with the correct class label. We demonstrate this approach by generating behaviors corresponding to different troop maneuver formations, as would be executed by squads of Army soldiers (teams of 9 trainees). We evaluate the effectiveness of the resulting behavior classifier using gold-standard test data collected using volunteers in a testbed multiplayer simulation environment. Results indicate that high-accuracy recognition is possible using this approach, but requires the application of domain adaptation techniques to bridge the gap between synthetic data and human performance data. RELATED WORK Behavior Recognition in Virtual Simulations and Video Games The problem of recognizing team behaviors in virtual simulations using supervised machine learning methods has been investigated previously by Sukthankar and Sycara (2006). In their work, two-person tactical maneuvers are recognized in a multiplayer, game-based simulation environment using hand-authored behavior templates, which are used to classify durations of gameplay data using trained Hidden Markov Models. Our approach is analogous in several respects, using hand-authored behavior programs rather than templates, but eliminates the additional step of hand annotating the training data used in supervised learning. Additionally, we address the more challenging problem of recognizing behaviors in larger teams (nine-person infantry squads). Behavior classification has also been investigated in the analysis of gameplay data from multiplayer video games. Ahmad et al. (2019) demonstrate a successful approach to behavior classification that minimizes the hand annotation of training data. In their approach, referred to as Interactive Behavior Analysis, large amounts of multiplayer gameplay data are interactively analyzed through an iterative process of visualization, labeling, and clustering. Aimed at analysts who are interested in understanding player strategies and tactics, this descriptive approach allows for quick ​post hoc analyses with minimal labeling. Our approach contrasts with this previous work in that we aim to identify behaviors ​in situ​, using prescriptive definitions, for the purpose of providing real-time responses within the simulation environment. Methods for plan a",,,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
787b8f43c78406d85919892b19122965a5026c31,https://www.semanticscholar.org/paper/787b8f43c78406d85919892b19122965a5026c31,VISUALISING DISTRIBUTED SIMULATION DESIGN AND DEPLOYMENT,"The “Hello World” code provided as a sample application for the High Level Architecture (HLA) framework comprises over 2500 lines of code, of which less than 50 provide any real simulation logic. This coding and integration overhead represents a considerable development effort. This overhead, and the lack of a standardized design pattern for separating simulation logic from integration code, often results in high levels of code coupling. This in turn leads to poor code structure and design, poor reuse, and increased maintenance costs. Significant improvements can be made in the design, development and deployment of HLA based simulations through the application of a Component-Based Development (CBD) approach and the use of visual design metaphors. These improvements are realized through the use of CBD patterns, which provide a mechanism to clearly separate the simulation logic from the integration requirements of the federate. Using this model, simulation logic remains separated from the integration code. Generated integration logic can then be used to manage the physical integration with the RTI, and any other required services, such as data transformations. The simulation logic and the integration logic collectively form the HLA federate. This separation also allows the developer to visually model the relationships between federates, allowing the creation of a simulation without the need to consider integration requirements. This visual design approach can be used to encapsulate the simulation workflow, the asynchronous ‘publish and subscribe’ relationships between the components and the FOM, the synchronous inter-component relationships (method calls between components outside of the RTI), any data transformation required to insure interoperability, and the deployment of the simulation. This paper examines a number of visualizations that can be used to significantly aid in the development and deployment of distributed simulations. It will also examine how a simulation can be directly generated, deployed and executed from the visual model. AUTHORS’ BIOGRAPHIES Dr Russell Keith-Magee is a Software Engineer at Calytrix Technologies with a research background in both Physics and Computing. His doctoral thesis was on biologically motivated models of machine learning and development. He also holds a Bachelor of Science with Honours in Computer Science and a Bachelor of Science in Physics. Shawn Parr is the co-founder and Chief Technology Officer at Calytrix Technologies, an Australian based research and development company specializing in component-based solutions and HLA simulation. Shawn has been working in the IT industry for over 10 years. He holds a research-based Masters degree, and a Bachelor of Science. Alex Radeski is a Senior Software Engineer at Calytrix Technologies and has a keen interest in Design Patterns, Component Based Development, and real-time visual simulation. Alex has been working as a software engineer for over 7 years, and holds a Bachelor of Science in Computing. VISUALISING DISTRIBUTED SIMULATION DESIGN AND DEPLOYMENT Dr Russell Keith-Magee, Shawn Parr, Alex Radeski Calytrix Technologies Pty Ltd Perth, Western Australia",,,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
900dd0734b22fff6ff315c8d6af6a0e13624ce3a,https://www.semanticscholar.org/paper/900dd0734b22fff6ff315c8d6af6a0e13624ce3a,Proactive software rejuvenation solution for web enviroments on virtualized platforms,"The availability of the Information Technologies for everything, from everywhere, at all times is a growing requirement. We use information Technologies from common and social tasks to critical tasks like managing nuclear power plants or even the International Space Station (ISS). However, the availability of IT infrastructures is still a huge challenge nowadays. In a quick look around news, we can find reports of corporate outage, affecting millions of users and impacting on the revenue and image of the companies. 
It is well known that, currently, computer system outages are more often due to software faults, than hardware faults. Several studies have reported that one of the causes of unplanned software outages is the software aging phenomenon. This term refers to the accumulation of errors, usually causing resource contention, during long running application executions, like web applications, which normally cause applications/systems to hang or crash. Gradual performance degradation could also accompany software aging phenomena. The software aging phenomena are often related to memory bloating/ leaks, unterminated threads, data corruption, unreleased file-locks or overruns. We can find several examples of software aging in the industry. 
The work presented in this thesis aims to offer a proactive and predictive software rejuvenation solution for Internet Services against software aging caused by resource exhaustion. To this end, we first present a threshold based proactive rejuvenation to avoid the consequences of software aging. This first approach has some limitations, but the most important of them it is the need to know a priori the resource or resources involved in the crash and the critical condition values. Moreover, we need some expertise to fix the threshold value to trigger the rejuvenation action. Due to these limitations, we have evaluated the use of Machine Learning to overcome the weaknesses of our first approach to obtain a proactive and predictive solution. 
Finally, the current and increasing tendency to use virtualization technologies to improve the resource utilization has made traditional data centers turn into virtualized data centers or platforms. We have used a Mathematical Programming approach to virtual machine allocation and migration to optimize the resources, accepting as many services as possible on the platform while at the same time, guaranteeing the availability (via our software rejuvenation proposal) of the services deployed against the software aging phenomena. 
The thesis is supported by an exhaustive experimental evaluation that proves the effectiveness and feasibility of our proposals for current systems.",,2011.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
2c8d9135b0b21434c9d652bbe9185715126966e6,https://www.semanticscholar.org/paper/2c8d9135b0b21434c9d652bbe9185715126966e6,COMPUTATIONAL MODELS FOR SUSTAINABLE DEVELOPMENT,"Genetic erosion is a serious problem and computational models have been developed to prevent it. The computational modeling in this field not only includes (terrestrial) reserve design, but also decision modeling for related problems such as habitat restoration, marine reserve design, and nonreserve approaches to conservation management. Models have been formulated for evaluating tradeoffs between socioeconomic, biophysical, and spatial criteria in establishing marine reserves. The percolation theory and shortest path modeling have also been used. In this article we discuss the computational models that have been developed keeping in mind the sustainable developmentConservationists estimate that alarming rate at which biological species are disappearing will have an indelible impact on humanity. Targets which were set in 2002 to reduce the biodiversity loss by 2010 have not been met. The third global diversity outlook report said that loss of wildlife and habitats could not only exacerbate climate change through rising emissions but could also have a negative impact on food sources and industry. Computational Models in Aid Of Developing Policies For Sustainable Development The development of right policies for sustainable development is very important and involves complex decision making about the judicious use of natural resources and about striking a balance between societal, economic and environmental needs. Computational models have been used for policy formulation for example, the impact project (http://www.policy-impact.eu/home, impact: integrated methods for policy making using argument modeling and computer assisted text analysis) aims to make progress in the area of state-of the art of computational models of argumentation about policy issues, contribute to computational linguistics by developing methods for mining arguments in natural language texts, find ways and means to increase the quantity and quality of public participation in consultation processes, and invent user friendly tools (such as graphic interfaces to increase public participation). These tools can also be used for policy formulation on Biodiversity. This will lead to involvement of more people, and key stakeholders to integrate biodiversity considerations into their work. The IMPACT argumentation toolbox aimed to consist of, firstly, an argument reconstruction tool: the manual reconstruction of arguments from natural language texts was done which was supported by a library of argumentation as a constituent of argument reconstruction tool. This was done in order to enable future web logs to mark up the structure of arguments in articles in a way which allows arguments to be automatically aggregated, analyzed and visualized. The legal knowledge extension format formed a part of the basis of this tool. A policy modeling and analysis tool based on the computational models of argumentation about alternative courses of action depending on the goals and values of multiple stakeholders was also included. Prior research on knowledge representation languages for concepts (ontologies), defeasible generalizations (rules) and precedent cases including the legal knowledge interchange format (lkif) was utilized for this. The lkif was developed in the Estrella project (http://www.estrellaproject.org, Estrella:The European project for Standardized Transparent Representations in order to Extend Legal Accessibility IST-2004-027655) aimed to develop and validate an “open, standards-based platform allowing public administrations to both develop and deploy comprehensive legal knowledge management solutions”. Legal document and data management, in addition to knowledge based systems is supported by Estrella, to provide a holistic solution for improving the efficiency and quality of public administration which requires the application of complex legislation and other legal sources. Both the legal and legislative data and its analysis including possible implications on past, present and future scenarios will have to be incorporated to arrive at informed and efficient solutions. The public administration and other users are provided with a variety of competing development environments, inference engines and other tools to choose from. The main technical objectives of the Estrella project are to “develop a Legal Knowledge Interchange Format (LKIF), building upon emerging XML-based standards of the Semantic Web, including RDF and OWL, and Application Programmer Interfaces (APIs) for interacting with legal knowledge-based systems”. The policy modeling and analysis tool is proposed to include a graphical user interface for a dialogue with an inference engine to simulate and analyze the consequences of a proposed policy. The tool has been proposed to be rich in graphical interfaces to enable clear visualization of its reasoning. The comparative analysis of different policy proposals will also be facilitated by this tool. Monendra Grover et al./ Indian Journal of Computer Science and Engineering (IJCSE) ISSN : 0976-5166 Vol. 2 No. 1 55 A structured consultation tool, based on prior research on the PARMENIDES system was a part of the toolbox. The PARMENIDES system was developed by University of Liverpool (http://cgi.csc.liv.ac.uk/~parmenides/index.php). The Parmenides system is a system for deliberative democracy and allows the government and public to interact in a two way fashion. It enables the government to present policy proposals to the public and lets the public submit their views on the policy. Parmenides exploits argumentation schemes and argumentation frameworks to graphically analyze the opinions submitted by the users. The structured consultation tool is an intelligent, advanced, polling and survey tool, based on the computational models of argumentation. The models of argumentation schemes together with the model of the issues and the arguments put forward previously in the ongoing consultation are used to generate questions in the surveys. The tool substantively increases the signal to noise ratio in online discussions, without restricting the solid arguments which can be made, by helping users to apply a model of rational argument. The arguments can be more easily tracked, mapped and visualized, since there is no need to manually reconstruct arguments from natural language texts. An argument analysis, tracking and visualization tool, based on computational models of argument and argument mapping methods is also a constituent of IMPACT argumentation toolbox. There are three main features of this tool. The analysis features of this tool enables citizens to identify the applied argumentation schemes, to list implicit premises helpful for asking questions. The tracking features of this tool enables users to register their interest in particular issues and request as well as receive notification whenever new arguments have been put forward which affect these interests. The visualization feature of this tool provides a variety of graphical and interactive views onto argument graphs. This will enable citizens to appreciate and analyze the complexity of the policy issues in their entirety and contribute to the policy formulation. Besides the policy formulation tools several other issues in sustainability have been addressed by using computational models as detailed below. Computational Sustainability Computational Sustainability is a highly interdisciplinary field, with the vision that information and computing science have a potential to play an indispensable role in increasing the efficiency and effectiveness of management and allocation of our natural resources. Some of the examples of studying computational biodiversity include more efficient use of natural resources, more realistic models of maintaining and increasing biodiversity and more effective large scale computational equilibrium models of renewable energy. Computational sustainability has a unique societal relevance and effective environmental component. Computational thinking and approach is essential to provide effective and efficient solutions which include balancing environmental, economic and societal needs. Computational sustainability takes a holistic approach and encompasses problems in diverse disciplines such as ecology, natural resources, atmospheric science, materials science, renewable energy and biological and environment engineering. Computational sustainability addresses the sustainability issues by translating them into decision and optimization problem. The field of computational sustainability not only draws from computer science and mathematics, it has pushed the boundaries of these disciplines itself. This is in view of the fact that sustainability issues are of unique scale complexity and impact. The sustainability problems require integration of a wide variety of techniques from various areas with in applied mathematics and computer science such as data mining, machine learning, optimization, constraint processing and dynamical systems. The field of complex systems is also relevant to computational sustainability. The systems studied in the realm of computational sustainability consist of highly interconnected components or agents, often with conflicting interests. The Institute of computational sustainability is one of the leading institutes in the world for computational sustainability (http://www.cis.cornell.edu/ics/). The multi-disciplinary, multi-institutional ICS research team is based at Cornell University of and includes leading computer and environmental scientists at Oregon State University, Bowdoin College, Howard University, and The Conservation Fund (TCF). The computational sustainability has had a direct impact on the sustainability research for example. The ICA has developed methods and models to design economically viable conservation corridors for grizly bears and other species in U.S. and understand the impact of climate change in terms of aerosol interac",,2011.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
0fb307f8c94835678bfb14d452aaf9bab651c010,https://www.semanticscholar.org/paper/0fb307f8c94835678bfb14d452aaf9bab651c010,"Web-based management of simulation models - concepts, technologies and the users' needs","Simulation models are commonly developed by scientists to evaluate scenarios that show potential developments of the Earth system in the past, present and future. In order to provide adequate information systems that facilitate access to simulation models to a broad, heterogeneous user community, the deployment of such models on web servers provides a technical fundament. However, many aspects need to be taken into account for setting up operational, user-friendly web-based systems that include access and administration tools for simulation models. Data integration, data exchange, scenario management, and visualisation are among the most important functionalities to be accounted for, while usability needs to be aimed at by choosing an appropriate abstraction level and providing a careful interface design. Usually simulation models encapsulate complex algorithms, which have been developed by domain experts and implemented based on very diverse technologies. In order to provide the functionality of such models to users over the Internet, standardisations such as the Web Processing Service developed by the Open Geospatial Consortium (OGC) help to specify the technological framework, but do not provide concepts for guaranteeing the aforementioned functionalities and usability. An additional requirement from the administrator and developer perspective is to offer a minimum level of flexibility in information architectures in order to adapt and exchange single components such as a simulator or data base. In many cases, this flexibility stands in conflict with a rapid, use-case specific development. In this paper, different integration concepts for hydrological simulation models into web-based management systems are compared to each other. All concepts were developed to fulfil the requirements of heterogeneous user groups, ranging from scientists to re-insurance companies. Their implementation in prototypical realworld systems was performed in inter-disciplinary groups of experts in Hydrology and Information Technology. While the first three integration concepts focus specifically on functionality (legacy model encapsulation, integration of real-time data, scenario management) and usability (user interface, visualisation) for single simulation models or static process chains, the fourth use case outlines a way towards more generic service composition based on a workflow management system. A comparison of the potential and limitations of these architectures results in a discussion of aspects to be taken into account for making simulation models accessible and usable for science, industry and governmental agencies. From our experience of designing, setting up and running the developed systems we conclude that functionality and usability are in the main interest of the end users of such systems. Each user group has different requirements, depending on their expertise and objectives. However, a clear, easy-to-use user interface is far less error-prone and avoids semantic problems for lay users, while experts require complex control mechanisms to run, calibrate or even re-design their modelling infrastructure. Integrating third-party data sources is possible, but requires well-defined machine readable user interfaces. For system administration and sustainability, system architectures incorporating a higher flexibility and implementation effort in the setup phase are seen to pay off in the long term. It is very important that all relevant aspects have to be specified in the design phase of a web-based management system for simulation models. Depending on this specification, the target system focuses either on implementation speed or flexibility, which comes with the cost of a more complex service-based infrastructure. The demand for using and accessing simulation models has increased in number and complexity in recent years. With the availability of appropriate concepts and technologies from information technology, integrating such systems into a web environment is a worthwhile, yet challenging task for the modelling and Information Technology communities.",,2009.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
6210230b732ad653815a69a5c24f090f143ee899,https://www.semanticscholar.org/paper/6210230b732ad653815a69a5c24f090f143ee899,Pro RFID in BizTalk Server 2009,,,2009.0,10.1007/978-1-4302-1838-8,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
8efd1d3b981c6110560f328d3e50a4f456c46728,https://www.semanticscholar.org/paper/8efd1d3b981c6110560f328d3e50a4f456c46728,Modeling Human Behavior for Image Sequence Understanding and Generation,"The comprehension of animal behavior, especially human behavior, is one of the most ancient and studied problems since the beginning of civilization, The big list of factors that interact to determine a person action require the collaboration of different disciplines, such as psichology, biology, or sociology. In the last years the analysis of human behavior has received great attention also from the computer vision community, given the latest advances in the acquisition of human motion data from image sequences.

Despite the increasing availability of that data, there still exists a gap towards obtaining a conceptual representation of the obtained observations. Human behavior analysis is based on a qualitative interpretation of the results, and therefore the assignment of concepts to quantitative data is linked to a certain ambiguity. 

This Thesis tackles the problem of obtaining a proper representation of human behavior in the contexts of computer vision and animation. On the one hand, a good behavior model should permit the recognition and explanation the observed activity in image sequences. On the other hand, such a model must allow the generation of new synthetic instances, which model the behavior of virtual agents. 

First, we propose methods to automatically learn the models from observations. Given a set of quantitative results output by a vision system, a normal behavior model is learnt. This results provides a tool to determine the normality or abnormality of future observations. However, machine learning methods are unable to provide a richer description of the observations. We confront this problem by means of a new method that incorporates prior knowledge about the enviornment and about the expected behaviors. This framework, formed by the reasoning engine FMTL and the modeling tool SGT allows the generation of conceptual descriptions of activity in new image sequences. Finally, we demonstrate the suitability of the proposed framework to simulate behavior of virtual agents, which are introduced into real image sequences and interact with observed real agents, thereby easing the generation of augmented reality sequences.

The set of approaches presented in this Thesis have a growing set of potential applications. The analysis and description of behavior in image sequences has its principal application in the domain of smart video--surveillance, in order to detect suspicious or dangerours behaviors. Other applications include automatic sport commentaries, elderly monitoring, road traffic analysis, and the development of semantic video search engines. Alternatively, behavioral virtual agents allow to simulate accurate real situations, such as fires or crowds. Moreover, the inclusion of virtual agents into real image sequences has been widely deployed in the games and cinema industries.",,2009.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
55f8d4782b98a5f1bb9fbebfe1ef54cb6f4094fc,https://www.semanticscholar.org/paper/55f8d4782b98a5f1bb9fbebfe1ef54cb6f4094fc,Computer Controlled Instrumentation Projects By Sophomore Level Eet Students,"This paper presents student-initiated projects as part of an instrumentation and data acquisition course for sophomore-level electronics engineering technology students. Project objectives and associated assessment methodologies as well as general project management concepts are discussed. Two sample instrumentation projects reported in this paper are an automated street parking system and a computer-controlled bowling game system. Both projects focused on instrumentation system development integrating multiple sensors and actuators, data acquisition hardware, interface electronics, control logic implementation in LabVIEW software, and wood/metal work for prototype development. These end-of-semester course projects were carried out during the final four weeks of the semester after eleven weeks of lecture/laboratory session. Introduction The ability to conduct and design experiments is rated as one of the most desirable technical skills of engineering and engineering technology graduates. Specifically, the referenced survey indicates that employers want graduates with a working knowledge of data acquisition, analysis and interpretation; and an ability to formulate a range of alternative problem solutions. Additionally, potential employers of our EET graduates are in the automated manufacturing and testing sector of the industry providing additional motivation for an instrumentation and data acquisition course at the sophomore level of a four-year EET program. This course consists of two hours of lecture and three hours of laboratory per week. Students have had courses in electrical circuit analysis, electrical machines, and analog and digital electronics before taking this course. The first three weeks of the fifteen-week semester are devoted primarily to LabVIEW programming. During the next eight weeks, the concepts and integration of sensors and actuators, interface electronics, and data acquisition and instrument control hardware /software are covered. The final four weeks are dedicated to student-initiated laboratory design projects. This paper focuses on general approach to implementing end-of-semester course projects and associated assessment tools used to assess the project objectives. Technical details of two sample instrumentation projects, an automated street parking system and a computerized bowling game system, implemented during the spring-2007 semester are also presented. Course project objectives and the associated assessment method The learning and teaching objectives for the project experience are listed in the next page. A list of questions was prepared based on the stated objectives, and the survey was conducted at the end of second and fourth week of the four-week project experience as an indirect assessment tool. The results of the first survey was used to improve the project experience during the second half, and the results of the second survey is to be used to improve the next offering of the instrumentation project experience in spring-2008. Students are also assessed using direct assessment tools for teamwork, oral presentation, final report, successful operation and demonstration of the completed project, and design review meetings. Example rubrics used to assess teamwork and oral presentation are shown in Appendices A and B, respectively. Results of direct and indirect assessment instruments are archived for use as an input to the course P ge 13322.2 continuous improvement process and also as part of display materials for program accreditation visits. Project Learning Objectives Project Teaching Objectives ‚ Gain experience in interpreting technical specifications and selecting sensors and transducers for a given application ‚ Foster discovery, self-teaching, and encourage desire and ability for life-long learning ‚ Understand terminologies associated with instrumentation systems ‚ Provide experience in designing instrumentation system based on specifications ‚ Gain experience in developing computerized instrumentation systems for industrial processes using multiple sensors, interface electronics, data acquisition hardware, and GPIB and serial instruments ‚ Develop soft skills including teamwork, openended problem solving, formal report writing and oral presentation Project management Early in the semester students start developing potential project topics with appropriate feedback and guidance from the instructor leading to a required pre-proposal submission by the fifth week of the semester. Upon approval of the pre-proposal, students are required to submit a formal proposal for a specific project topic by the ninth week of the fifteen-week semester. Use of a minimum of four sensors/transducers and four actuators is required as part of any project. The required proposal is quite detailed as it includes project implementation ideas supported by major outcomes and specifications, I/O interface drawing, circuit schematics, parts list with vendor and price information, LabVIEW program flow chart, and project completion schedule including a Gantt chart. An example student-generated Gantt chart is shown in Appendix C, prepared using Vision Professional. For implementation of the project, students are in charge of selecting the necessary sensors and actuators and are required to use the well-equipped departmental shop for fabrication and metal/wood work. Each group of two students is allocated a nominal budget of $200 for purchasing project-specific parts not normally available in the laboratory. Project deliverables include pre-proposal, proposal, preliminary design review, critical design review, final report, and a formal presentation. Student presentations and final reports are archived for use as part of the display materials for future accreditation visits. Laboratory setup Each station is equipped with a PC, and GPIB/RS-232 interfaced instruments such as digital multimeter, triple output laboratory power supply, arbitrary function generator, and two-channel color digital oscilloscope. The instrumentation and data acquisition specific software and hardware are briefly described below. Software: LabVIEW 8.5 from National Instruments Data acquisition (DAQ) board: Model 6024E from National Instruments ‚ 16 single-ended or 8 differential analog input channels, 12 bit resolution, 200 kS/s ‚ 2 analog voltage output channels, 12 bit resolution, 10 kHz update rate ‚ 8 digital I/O channels with TTL/CMOS compatibility; and Timing I/O GPIB controller board: ‚ IEEE 488.2 compatible architecture (eight-bit parallel, byte-serial, asynchronous data transfer) ‚ Maximum data transfer rate of 1 MB/sec within the worst-case transmission line specifications Signal conditioning accessory: ‚ Model SC-2075 from National Instruments ‚ Desktop signal breakout board with built-in power supplies, connects directly to 6024E DAQ board P ge 13322.3 Sample Project: Automated street parking system The objective of the automated street parking system was to implement a prioritized parking system with prepayment and post payment options including a boot system for parking violators. For this street parking management system, three categories of cars are considered: resident, frequent, and visitor. A resident car can be parked for an unlimited amount of time without accruing any fines, a frequent car can be parked on a daily basis for a limited number of hours to be billed for parking fees on a biweekly basis, and a visitor car would need to pay upfront for parking. Additionally, activation of a boot system from under the street upon expiration of parking credit and/or other violations was an integral part of the system. A block diagram representation of the I/O interface for the street parking system is shown in Figure 1 and a pictorial view of the system is shown in Figure 2. This prototype system consisted of three parking spots along a street. A total of eight analog inputs were used in implementing the system: three inputs for detecting the type of car, three inputs for parking spot availability status, one input for spot selection for prepayment, and an additional input for coin collection system. The coin collection system was based on an inductive proximity sensor while the other seven input signals were based on simple voltage divider networks and/or photoresistors. An example voltage-divider based interface for prepayment spot selection is shown in Figure 3. Figure 1: A block diagram representation of the I/O interface for the automated street parking system. P ge 13322.4 Figure 2: A pictorial view of the automated street parking system. Figure 3: Implementation of the spot selection logic for prepayment. The parking system used a total of nine outputs: six digital outputs for various parking status indicators and three analog (but used as digital) outputs for driving the solenoids for the boot system. In case of malfunctioning of the car type detection system for a given spot, the status light will turn red and draw attention of the police via the end of street display lights. This end of street display and the boot system get activated in case of an unpaid visitor car in a spot. For an activated boot deployment system, only the police personnel can release the boot system. After the car is removed from the spot, the system resets itself for the next car to be parked. The control logic for this system was implemented in LabVIEW software. A typical front panel display for the automated street parking system is shown in Figure 4 and it includes status monitoring of the following subsystems: parking spot, boot activation, prepayment, coin collection, and prepaid parking timer. The corresponding block diagram for implementing the Figure 4: A typical front panel display of the automated street parking system. P ge 13322.5 parking spot logic functions are shown in Figure 5. The major LabVIEW function blocks used are case structure, sequence structure, for loop, subVIs, local variable, various array and string f",,2008.0,10.18260/1-2--3460,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
a5dffa86ee25138623b71f4d03a5ff5757fbdfdd,https://www.semanticscholar.org/paper/a5dffa86ee25138623b71f4d03a5ff5757fbdfdd,On National Projects,"Albania used the support from the Safe Online initiative to end violence against children online on multiple levels. To reach children, the agency trained children to become peer educators on online violence, increasing their knowledge on safe Internet navigation and helping them spread that information to their peers. UNICEF Albania conducted research on children’s use of the Internet and risk of online violence using the Global Kids Online methodology. The results from this study are being used to conduct national awareness activities. UNICEF Albania also worked at the national level by strengthening the national child helpline and hotline, which alert authorities about child sexual abuse material online. They also carried out an assessment of national legislation, policies and programmes currently targeting online child sexual exploitation and abuse in-line with the WePROTECT Global Alliance Model National Response. This will inform measures to improve the systematic response to online child sexual exploitation and abuse. UNICEF Albania also helped the government to develop a national strategy for cyber-security with a focus on children’s online safety. The agency also supported law enforcement authorities, strengthening their ability to investigate and prosecute cases of online child sexual abuse and exploitation. In addition, they worked with local municipalities and Internet service providers to promote “Albania Friendly Wi-Fi,” a safe certification standard for public Wi-Fi. These services will make Internet navigation safer for children, while also increasing national awareness on internet safety. They also used the support from the Safe Online initiative to engage the information, communications and technology sector on children’s Internet usage and risks in the digital space. to better detect, deter and prevent this type of violence. To do so, Justice and Care will explore the profiles of those who perpetrate and facilitate online sexual exploitation of children, interviewing convicted offenders, key informants, and others. This analysis will fill a gap in global research into online child exploitation of children and shed light on the “supply-side” of such violence in a country known to be an epicenter of live-streamed child sexual abuse. Ultimately, this research will seek to inform practical strategies and enhance industry, prevention and law enforcement response to the issue. The project also trained parents, teachers, social service providers and other adults to become advocates for cyber safety. At the industry level, the project also engaged Internet service providers and other technology companies in the fight against online child sexual exploitation and abuse. protect children from being victims of child sexual abuse. The chatbot will first be developed for pilot use in the United Kingdom with the potential for scaling up in other countries. The Safe Online initiative supported ChildFund Australia’s Swipe Safe program in Vietnam, which aimed to help young people navigate the Internet safely by educating them on potential risks, such as cyber scams, bullying or sexual abuse, and offering them strategies to protect themselves. ChildFund Australia designed, created and tested a training program to promote online safety – and ever since, the curriculum has been adapted by non-governmental organisations not just in Viet Nam, but in Laos and Myanmar as well. Swipe Safe mobilizes parents, youth, schools and the private sector to play an active role in children’s online safety. The program provided training for parents and Internet café owners and managers to identify and address risks that might happen to children, from online to offline and vice versa. It also supported schools to develop child-friendly policies and guidance on online safety. Swipe Safe is active in advocating to the national government with lessons learned to inform national policy and response and linking such legislation with the strengthening of existing structures. A key innovation of the program is that it engaged young volunteers in local communities with extensive knowledge on technology to train young people and others, as these trainers can more directly relate to their peers’ experiences and help keep the curriculum up to date. Disrupting Harm is a large-scale data collection and research project to better understand online child sexual exploitation and abuse across the world. This study is assessing the scale, nature and context of this issue in 14 countries across Southern and Eastern Africa and Southeast Asia. Supported by the Safe Online initiative, three grantee partners will work together to conduct the study, including ECPAT International, INTERPOL and the UNICEF Office of Research – Innocenti. ECPAT's role is to conduct a comprehensive analysis, allowing partners (and all others working in this arena) to better understanding the context of children's safety online. A new version of Disrupting Harm is currently bringing design, and it is expected to bring the large-scale research project to other regions. NTERPOL will bring the most advanced technology to investigators of online CSEA through its new DevOps Group Project. The initiative will facilitate research and development by an expert group of investigators, non-governmental organisations, academia, and information technology companies, and extend solutions to specialised officers worldwide via INTERPOL’s secure channels. Headquartered in France, this project has a global reach. Disrupting Harm is a large-scale data collection and research project to better understand online child sexual exploitation and abuse across the world. This study is assessing the scale, nature and context of this issue in 14 countries across Southern and Eastern Africa and Southeast Asia. Supported by the Safe Online initiative, three grantee partners will work together to conduct the study, including ECPAT International, INTERPOL and the UNICEF Office of Research – Innocenti. INTERPOL's role is to examine the threats facing children online and analyse data from law enforcement agencies across the world. A new version of Disrupting Harm is being designed and is expected to bring the research project to other regions. with INHOPE and other specific will explore and quantify the issues facing content moderators, as it relates to their exposure of traumatic child sexual abuse material. They will also identify coping strategies currently used by content moderators, and highlight what works – and what does not work – for individuals and organisations that do this work. Results of this study will be used to develop a pilot intervention to support and protect the mental health of content moderators. Lapsia Ry will develop and launch ReDirection, an evidence-based self-help programme working to prevent the consumption of CSAM on the Dark Web. By providing targeted support for these individuals, the project will also reveal new information about these searchers and their pathways to CSAM access and use. This programme builds on the Finnish government’s accredited New Direction rehabilitation programme for sex offenders. Headquartered in Finland, this project has a global reach. Through this project, Technological University Dublin will develop a deployable tool that reveals the patterns of adults perpetrating online child sexual abuse and the children who are affected by such violence. By using advanced artificial intelligence machine learning for text, the study will advance global understanding of trends in perpetrator behaviour (conduct, contact, content) – including grooming – and debunk strategies and tactics used to lure and coerce children into sexually exploitative acts. N-Light will be created in collaboration with two essential partner organisations, the Irish Society for the Prevention of Cruelty to Children (ISPCC) and Hotline.ie, the Irish national centre combatting illegal content online, specifically child sexual abuse material (CSAM) and activities relating to online child sexual exploitation (OCSE). Once finalized, N-Light will be tested by both partner organisations, with the intention to make it available to other hotlines in the INHOPE network and child agencies for their use, which would in turn lead to an enriched, more robust and representative data sample and analysis capacity. In addition, the data and insights will serve to better understand and conceptualise victim and perpetrator behavior, patterns and activity, ultimately informing the further development of evidence-based solutions that would have the potential of transformative impact in tackling this heinous crime against children. on the psychological processes through which people of online sexual and professional support. In addition, the group will explore the efficacy and impact of prevention interventions targeting people engaging with online abuse. Overall, the project will ask a fundamental – and often overlooked – question: who seeks help for child sexual exploitation and abuse, and can we get more people to do so before committing a crime? This project will expand the group’s existing model of psychological predicators of help-seeking for people at risk of offending and examine how to amplify the psychological factors that support such help-seeking behaviors. At the same time, the project will also look into the psychological barriers that prevent help-seeking and explore ways to weaken those barriers in the sphere. The World Health Organisation is using Safe Online investments to explore current systems of prevention and response to online child sexual exploitation and abuse. These findings will support governments and civil society organisations, giving them the tools and evidence, they need to implement effective, evidence-based programs to keep children safe online. After determining what works and does not work around preventing and responding to sexual and emotional online child abuse, the proj",,2003.0,10.6027/9789289335157-8-en,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
91903c0cae572554ff6828366d1461b083c582de,https://www.semanticscholar.org/paper/91903c0cae572554ff6828366d1461b083c582de,"Intelligent problem solving : methodologies and approaches : 13th International Conference on Industrial and Engineering Applications of Artificial Intelligence and Expert Systems, IEA/AIE 2000, New Orleans, Louisiana, USA, June 19-22, 2000 : proceedings","Keynote Presentation.- Multisensor Data Fusion.- Intelligent Agents I.- Implementing Multi-party Agent Conversations.- Agreement and Coalition Formation in Multiagent-Based Virtual Marketplaces.- A Framework for the Development of Cooperative Configuration Agents.- Java-Based Distributed Intelligent Agent Architecture for Building Safety-Critical Tele-Inspection Systems on the Internet.- Artificial Neural Network I.- The Use of AI Methods for Evaluating Condition Dependent Dynamic Models of Vehicle Brake Squeal.- Towards an Estimation Aid for Nuclear Power Plant Refuelling Operations.- Drilling Performance Prediction Using General Regression Neural Networks.- Identifying Significant Parameters for Hall-Heroult Process Using General Regression Neural Networks.- Data Mining I.- Mapping Object-Oriented Systems to Distributed Systems Using Data Mining Techniques.- Scaling the Data Mining Step in Knowledge Discovery Using Oceanographic Data.- Information Management and Process Improvement Using Data Mining Techniques.- Combinatorial Optimization.- Combinatorial Optimization A Comparative Analysis of Search Methods as Applied to Shearographic Fringe Modelling.- Vision Guided Bin Picking and Mounting in a Flexible Assembly Cell.- A Brokering Algorithm for Cost & QoS-Based Winner Determination in Combinatorial Auctions.- An Overview of a Synergetic Combination of Local Search with Evolutionary Learning to Solve Optimization Problems.- Expert Systems I.- Maintenance of KBS's by Domain Experts.- A Simulation-Based Procedure for Expert System Evaluation.- Gas Circulator Design Advisory System: A Web Based Decision Support System for the Nuclear Industry.- Expert Systems and Mathematical Optimization Approaches on Physical Layout Optimization Problems.- Diagnosis I.- Locating Bugs in Java Programs - First Results of the Java Diagnosis Experiments Project.- Application of a Real-Time Expert System for Fault Diagnosis.- Operative Diagnosis Algorithms for Single-Fault in Graph-Based Systems.- On a Model-Based Diagnosis for Synchronous Boolean Network.- DermatExpert: Dermatological Diagnosis through the Internet.- Best Papers.- Aerial Spray Deposition Management Using the Genetic Algorithm.- Dynamic Data Mining.- Information Systems I.- Knowledge-Intensive Gathering and Integration of Statistical Information on European Fisheries.- Using a Semantic Model and XML for Document Annotation.- Understanding Support of Group in Web Collaborative Learning, Based on Divergence among Different Answering Processes.- Fuzzy Logic and Its Applications.- Fuzzy Modeling Approach for Integrated Assessments Using Cultural Theory.- Fuzzy Knowledge-Based System for Performing Conflation in Geographical Information Systems.- Modeling of, and Reasoning with Recurrent Events with Imprecise Durations.- Linguistic Approximation and Semantic Adjustment in the Modeling Process.- A Fuzzy Inference Algorithm for Lithology Analysis in Formation Evaluation.- Intelligent Agents II.- Approximating the 0-1 Multiple Knapsack Problem with Agent Decomposition and Market Negotiation.- Design and Development of Autonomous Intelligence Smart Sensors.- ADDGEO: An Intelligent Agent to Assist Geologist Finding Petroleum in Offshore Lands.- SOMulANT: Organizing Information Using Multiple Agents.- Design.- Inventiveness as Belief Revision and a Heuristic Rule of Inventive Design.- A Decision Support Tool for the Conceptual Design of De-oiling Systems.- ProCon: Decision Support for Resource Management in a Global Production Network.- Intelligent Infrastructure That Support System's Changes.- Diagnosis II.- Using Description Logics for Case-Based Reasoning in Hybrid Diagnosis.- Printer Troubleshooting Using Bayesian Networks.- Using XML and Other Techniques to Enhance Supportability of Diagnostic Expert Systems.- Learning and Diagnosis in Manufacturing Processes through an Executable Bayesian Network.- Expert Systems II.- Solving Large Configuration Problems Efficiently by Clustering the ConBaCon Model.- XProM: A Collaborative Knowledge-Based Project Management Tool.- Building Logistics Networks Using Model-Based Reasoning Techniques.- A Supporting System for Colored Knitting Design.- Machine Learning and Its Applications.- Learning Middle-Game Patterns in Chess: A Case Study.- Meta-classifiers and Selective Superiority.- Logic and Its Applications.- The Formal Specification and Implementation of a Modest First Order Temporal Logic.- Determining Effective Military Decisive Points through Knowledge-Rich Case-Based Reasoning.- A Constraint-Based Approach to Simulate Faults in Telecommunication Networks.- A Least Common Subsumer Operation for an Expressive Description Logic.- Pattern Recognition.- Blob Analysis Using Watershed Transformation.- A Novel Fusion of Holistic and Analytical Paradigms for the Recognition of Handwritten Address Fields.- Pawian - A Parallel Image Recognition System.- An Automatic Configuration System for Handwriting Recognition Problems.- Detection of Circular Object with a High Speed Algorithm.- Artificial Neural Networks II.- Neural Network Based Compensation of Micromachined Accelerometers for Static and Low Frequency Applications.- Improving Peanut Maturity Prediction Using a Hybrid Artificial Neural Network and Fuzzy Inference System.- CIM - The Hybrid Symbolic/Connectionist Rule-Based Inference System.- A Neural Network Document Classifier with Linguistic Feature Selection.- Color Pattern Recognition on the Random Neural Network Model.- Integrating Neural Network and Symbolic Inference for Predictions in Food Extrusion Process.- Natural Language Processing.- Automatic Priority Assignment to E-mail Messages Based on Information Extraction and User's Action History.- Information Extraction for Validation of Software Documentation.- Object Orientation in Natural Language Processing.- Genetic Algorithm.- A Study of Order Based Genetic and Evolutionary Algorithms in Combinatorial Optimization Problems.- Nuclear Power Plant Preventive Maintenance Planning Using Genetic Algorithms.- Progress Report: Improving the Stock Price Forecasting Performance of the Bull Flag Heuristic with Genetic Algorithms and Neural Networks.- Advanced Reservoir Simulation Using Soft Computing.- Information Systems II.- Forest Ecosystem Management via the NED Intelligent Information System.- Friendly Information Retrieval through Adaptive Restructuring of Information Space.- A Smart Pointer Technique for Distributed Spatial Databases.- Distributed Problem Solving.- Deploying the Mobile-Agent Technology in Warehouse Management.- A Lightweight Capability Communication Mechanism.- Model-Based Control for Industrial Processes Using a Virtual Laboratory.- Autonomous Agents for Distributed Problem Solving in Condition Monitoring.- Modeling Issues for Rubber-Sheeting Process in an Object Oriented, Distributed and Parallel Environment.- Intelligent Agents III.- Reasoning and Belief Revision in an Agent for Emergent Process Management.- System Design and Control Framework for an Autonomous Mobile Robot Application on Predefined Ferromagnetic Surfaces.- Intelligent and Self-Adaptive Interface.- Agent Architecture: Using Java Exceptions in a Nonstandard Way and an Object Oriented Approach to Evolution of Intelligence.- Artificial Neural Networks III.- Neural Network Based Machinability Evaluation.- Performance of MGMDH Network on Structural Piecewise System Identification.- Black-Box Identification of the Electromagnetic Torque of Induction Motors: Polynomial and Neural Models.",,2000.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
2c598f5071937d27a517595d36b9def9bb3de629,https://www.semanticscholar.org/paper/2c598f5071937d27a517595d36b9def9bb3de629,Professional Visual C++ Activex Intranet Programming,"From the Publisher: 
Long Live the Information Superhighway! 
Wait a minute! I heard some loud heckling from the back row. 
""I thought this is a 'Microsoft ActiveX' technical programming book?!"" 
It is. 
The point is, there are two ways you can show a technology: either in dry, boring isolation, talking about the nuances without reference to the problems the technology is supposed to solve, or as we've attempted to do, applying the technology to a real-world situation. We've chosen to show you ActiveX programming by applying this technology to a large growth area, intranets. As you'll soon see, the ActiveX family of technologies is a very broad family covering the gamut of Internet-intranet, client-server and distributing computing solutions. It has to be this way, since ActiveX represents Microsoft's entire investment into the Internet-intranet (and object based distributed computing) race. Within this book, ActiveX will be used as a vehicle to explore many of the concepts and techniques involved in intranet construction. In many cases, the concepts and approaches explored are generally applicable to your practice whether it's Microsoft-, UNIX-, or even Netscape-centric. What's Covered in this Book Just before the technical reader, fluent with Visual C++, decides to return this book to the bookstore for a refund, I must say that this is by no means a 'lame theoretical treatise'. We turn on the power-throttle, and shine our high beams on the core ActiveX technologies once we've reached Chapter 2. In fact, a discussion of what intranets are about has been relegated to Appendix A, simply because you don't actually need to know about them to gain an understanding of ActiveX. From there we dive into the depths of the Microsoft's Component Object Model (COM) which is fundamental to all of Microsoft's ActiveX technology. Covering the basics, we'll be taking a view that reduces this complex topic to simple programming practices that we're fully familiar with. From there, we examine the concept of COM aggregation and show how it further enhances code reuse and provides a powerful mechanism for COM. In Chapter 3, we take our understanding of COM and put it into practice by writing an ActiveX control from scratch, using just raw C++. Here, we'll become intimate with the complete anatomy of a simple ActiveX control. When we move on to more powerful libraries and code generation wizards, this basic understanding will enable us to adapt and troubleshoot more effectively. In this chapter, we encounter many essential COM interfaces through actual hands-on programming; we'll also get acquainted with some indispensable COM programming tools such as the MIDL compiler and the Object Viewer utility. To handle some more complex problems without coding forever, we'll take a look at programming libraries to simplify the COM object programming task (ActiveX controls, to be precise). We'll explore how to code powerful, yet super efficient and tiny COM objects using the ActiveX Template Library (ATL) 2.1, and we will spend some time explaining many of the new COM interfaces, and show how ATL makes everything simple. Also in this chapter, we'll be learning about the threading models supported by COM objects and the different types of COM servers that can be created. Using ATL to create ActiveX controls is the focus of Chapter 4. Chapters 2, 3 and 4 give us enough background into understanding what ActiveX provides for the intranet development environment. We'll understand how ActiveX controls can be fundamental building blocks (actually software components) in both client- and server-based programming. We'll make excursions into the ActiveX controls (OCX ) specifications in Chapter 5, covering the differences between the OLE Control specification (for Visual controls) and the new OC96 specification (for ActiveX controls). We'll actually be designing an Events Calendar control. This control will display currently active events (for the month) from different company departments for easy and straightforward access. The distributed 'live update' nature of this control eliminates the need for consolidating events information in a centralized database. In Chapters 6 and 7 we put our design into code. Using Visual C++ 5.0 and MFC 4.21, we'll be building the actual Events Calendar intranet control. The control class and custom wizard provided by MFC greatly simplify much of the development. We'll also be building two additional 'back-end' ActiveX controls using ATL 2.1 to do data processing for the Visual Calendar control. Finally, we will test the controls and show that the Calendar control is a bona fide ActiveX control that can be hosted within containers such as Visual Basic 5.0, Internet Explorer 3.0, and FrontPage 97. In Chapter 8, we shift into the highest gear and attempt to put the Calendar control through its paces by using DCOM to run the front-end and back-end ActiveX controls across three separate machines. Along the way, we'll learn a lot about DCOM and how it enables true distributed computing. We'll also be examining the difficult problem of ActiveX control code installation and revision control, and see how the Internet Explorer 3.0 provides us with a ready-made solution to the problem. As part of the installation solution, we'll develop a small program to download controls from remote sites. After the intensive programming in Chapter 8, we shift our focus to a hot intranet issue in Chapter 9: security. We'll examine the topic by drawing a parallel to the Windows NT security model which is fundamental to all other security mechanisms built upon it. We'll learn about the various security, authentication and encryption APIs and COM interfaces available to intranet application developers. Special attention will be paid to DCOM related security issues and how arbitrary distributed objects may be prevented or allowed to execute on certain machines. We conclude our coverage in Chapter 10 by casting aside our overly enthusiastic attitude and examine some real and hard-to-tackle ActiveX and intranet deployment issues, suggesting potential solutions wherever they are available. We'll cover a lot of ground in the following pages. I hope your journey into the exciting world of ActiveX will be as pleasant, productive, and profitable for you as it has been for us. What You Need to Use This Book To use this book you need Visual C++ 5.0, and the latest version of Microsoft's best-selling C++ compiler. This version is 32-bit only, so you'll need to install it on Windows 95, Windows NT 3.51 or NT 4, which means a 486 CPU or better and a minimum 16Mb of memory. For Visual C++, you'll need quite a lot of hard disk space - a typical installation is 170 Mbytes. You can do a minimal installation which takes up around 40 Mbytes, but this will mean longer compile times as the CD-Rom will be utilized more often. Some of the later chapters require you to have access to a network and a second computer to test the code correctly. You'll also need to have DCOM for Windows 95 (information on obtaining this is given in Chapter 8) or Windows NT 4.0. |AUTHORBIO: Bitten by the microcomputer bug since 1978, Sing has grown up with the microprocessor age. His first personal computer was a $99 do-it-yourself Netronics COSMIC ELF computer with 256 bytes of memory, mail ordered from the back pages of Popular Electronics magazine. Currently, Sing is an active author, consultant, and entrepreneur. He has written for popular technical journals and is the creator of the ""Internet Global Phone"", one of the very first Internet phones available. His wide-ranging consulting expertise spans Internet and Intranet systems design, distributed architectures, digital convergence, embedded systems, real-time technologies, and cross platform software design. Recently, he has completed an assignment with Nortel Multimedia Labs working in Computer Telephony Integration, and Advanced Callcenter Management products. Sing is a founder of microWonders, an emerging company specializing in products to fulfill the ubiquitous ""computing anywhere"" vision. Other titles by this author: ATL Programmer's Resource Kit Professional COM Applications with ATL Professional IE4 Programming Visual C++ 4 Master Class|AUTHORBIO: Panos Economopoulos has been the architect, designer and leader for implementations of a number of complex and successful distributed computer systems. Currently, he is Manager of Research and Development at Telesis North. Here, he designed the OnAir series of mobile client-server products that provide efficient and robust remote access to BackOffice servers over a variety of satellite and other wireless networks. He has extensive experience as a consultant to the Industry and has developed, and taught, a variety of courses both at University undergrad level and for mature developers. He's also carried out advanced research at the University of Toronto - results of which have been published in several research journals. Other titles by this author ATL Programmer's Resource Kit, Professional COM Applications with ATL|",,1997.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
adf2bf68ef79c3b6318e7a10fcef165665910ce9,https://www.semanticscholar.org/paper/adf2bf68ef79c3b6318e7a10fcef165665910ce9,Managing Learning Curves In The Unknown: From ‘Learning By Doing’ To ‘Learning By Designing’,"Central in this paper is a puzzling innovation project involving the introduction of a new machine in an assembly plant in the aviation industry. The project drew our attention because it achieved remarkably high performance results despite being launched with a negative business case. The observed performance trend neither corresponds to uncertainty reduction nor results from a pure investment in the unknown: we demonstrate that this case is an anomaly with regards to investment decision-making logics and learning dynamics (Learning by Doing) which traditionally explain performance gains. We find that the dynamics at work were made possible by an original and rigorous managerial approach adopted to address the significant presence of unknown at project launch and during project deployment. Based on this case study, we identify three principles respectively aiming at guiding investment decisions, at (economically) managing projects and at organizing learning in the unknown. The first principle recommends to keep using the classical economic tools (e.g. business cases) which advise against the project, but in a ""diverted"" way, i.e. as a means to keep the losses under control in case of failure. The second one suggests to clearly set in the team's mission the objective to discover new performance variables. The third one proposes to deploy a learning strategy related to the newly-discovered variables that is based on the very objective to build profitability and turn the project into a profitable one.",,2018.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
f5172db961538ed9fc9cafecd0764821b6dd6714,https://www.semanticscholar.org/paper/f5172db961538ed9fc9cafecd0764821b6dd6714,Assessing Machine Learning Approaches to Address IoT Sensor Drift,"The proliferation of IoT sensors and their deployment in various industries and applications has brought about numerous analysis opportunities in this Big Data era. However, drift of those sensor measurements poses major challenges to automate data analysis and the ability to effectively train and deploy models on a continuous basis. In this paper we study and test several approaches from the literature with regard to their ability to cope with and adapt to sensor drift under realistic conditions. Most of these approaches are recent and thus are representative of the current state-of-the-art. The testing was performed on a publicly available gas sensor dataset exhibiting drift over time. The results show substantial drops in sensing performance due to sensor drift in spite of the approaches. We then discuss several issues identified with current approaches and outline directions for future research to tackle them.",ArXiv,2021.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
712aa27546a025c55ed16b76bc2a6bd8b0743469,https://www.semanticscholar.org/paper/712aa27546a025c55ed16b76bc2a6bd8b0743469,Location-Aware Application Deployment in Multi-Cloud,"The client-centric multi-cloud has become a popular cloud ecosystem because it allows enterprise users to share the workload across multiple cloud providers to achieve high-quality services with lower operation costs and higher application resilience. From the perspective of application providers, the location of cloud resources for application deployment significantly impacts the deployment cost and performance of applications, e.g., request response time. This gives rise to the problem of location-aware application deployment in multi-cloud to select suitable cloud resources from widely distributed multi-cloud data centers to balance the cost and performance. Existing research works did not pay full attention to the key impact of the location for application deployment. Therefore, it is urgent to study the problem both theoretically and in practice. In this thesis, innovative optimization methods and machine learning techniques are proposed for three common scenarios, namely composite application deployment, application replication and deployment, and elastic application deployment.
First, this thesis studies the composite application deployment problem with the goal to minimize the average response time of composite applications subject to a budget constraint. We propose a Hybrid Genetic Algorithm (GA)-based approach, i.e., H-GA, for solving the problem with an extremely large search space. H-GA features a newly designed and domain-tailored service clustering algorithm, repair algorithm, solution representation, population initialization, and genetic operators. Experiments show that H-GA can outperform significantly several state-of-the-art approaches, achieving up to about 8% performance improvement in terms of response time, and 100% budget satisfaction in the meantime.
Second, this thesis studies the application replication and deployment problem with the goal to minimize the total deployment cost of all application replicas subject to a stringent requirement on average response time. We propose two approaches under different optimization frameworks to solve the problem. With user requests dispatched to the closest application replicas, we develop an approach under a GA framework for Application Replication and Deployment (ARD), i.e., GA-ARD. GAARD features problem-specific solution representation, fitness measurement, and population initialization, which are effective to optimize the deployment of application replicas in multi-cloud. The experiments show that GA-ARD outperforms common application replication and placement strategies in the industry. With user requests flexibly dispatched among different application replicas, we develop another approach under a two-stage optimization framework, i.e., MCApp. MCApp can optimize both replica deployment and request dispatching by combining mixed-integer linear programming with domain-tailored large neighborhood search. Our experiments show that MCApp can achieve up to 25% reduction in total deployment cost compared with several recently developed approaches.
Third, this thesis studies the elastic application deployment problem to minimize the deployment cost over a time span such as a billing day while satisfying the constraint on average response time. The goal of adapting resources for application deployment in response to dynamic and distributed workloads motivates us to adopt deep reinforcement learning (DRL) techniques. The proposed approach, namely DeepScale, applies a deep Q-network (DQN) to capture the optimal scaling policy that can perform online resource scaling. DeepScale also includes a long short-term memory-based prediction model to allow the DQN to consider predicted future requests while making cost-effective scaling decisions. Besides, we design a penalty-based reward function and a safety-aware action executor to ensure that any scaling decisions made by DRL can satisfy the response time constraint. The experiments show that DeepScale can significantly reduce the deployment cost of applications compared with the state-of-the-art baselines, including Amazon auto-scaling service and recently proposed RL-based algorithms. In the meanwhile, DeepScale can effectively satisfy the constraint on the average response time.
In summary, this thesis studies three new problems for location-aware application deployment in multi-cloud. We propose four novel approaches under different optimization and machine learning frameworks, i.e., HGA, GA-ARD, MCApp, and DeepScale, for solving these problems. New constraint handling techniques are developed to satisfy the practical deployment requirements of enterprise applications.",,,10.26686/wgtn.20099645,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
4a1b1e54455ce6339f1ad5cf8b0ea2abed919cc2,https://www.semanticscholar.org/paper/4a1b1e54455ce6339f1ad5cf8b0ea2abed919cc2,Rotten-Fruit-Sorting Robotic Arm: (Design of Low Complexity CNN for Embedded System),": Industrial Automation has revolutionized the processing industry due to its high accuracy, the time it saves, and its ability to work without tiring. Being the most fundamental part of automation machines, robotic arms are being used as a fundamental component in many types of domestic as well as commercial automation units. In this paper, we proposed a low-complexity convolutional neural network (CNN) model and successfully deployed it on a locally generated robotic arm with the help of a Raspberry Pi 4 module. The designed robotic arm can detect, locate, and classify (based on fresh or rotten) between three species of Mangos (Ataulfo, Alphonso, and Keitt), on a conveyor belt. We generated a dataset of about 6000 images and trained a three-convolutional-layer-based CNN. Training and testing of the network were carried out with MatLab, and the weighted network was deployed to an embedded environment (Raspberry Pi 4 module) for real-time classiﬁcation. We reported a classiﬁcation accuracy of 98.08% in the detection of fresh mangos and 95.75% in the detection of rotten mangos. For the designed robotic art, the achieved angle accuracy was 93.94% with a minor error of only 2 ◦ . The proposed model can be deployed in many food- or object-sorting industries as an edge computing application of deep learning.","The 1st International Conference on Energy, Power and Environment",2022.0,10.3390/engproc2021012109,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
a93e75b9cdcff21f0cc804d33271b6fff38c7a22,https://www.semanticscholar.org/paper/a93e75b9cdcff21f0cc804d33271b6fff38c7a22,Digitizing the Pharma Neurons - A Technological Operation in Progress!,"BACKGROUND
Digitization and automation is the buzzword in clinical research and pharma companies are investigating heavily here. Right from drug discovery to personalized medicine, digital patients and patient engagement, there is great consideration of technology at each step.


METHODS
The published data and online information available is reviewed to give an overview of digitization in pharma, across the drug development cycle, industry collaborations and innovations. The regulatory guidelines, innovative collaborations across industry, academics and thought leadership are presented. Also included are some ideas, suggestions, way forwards while digitizing the pharma neurons, the regulatory stand, benefits and challenges.


RESULTS
The innovations range from discovering personalized medicine to conducting virtual clinical trials, and maximizing data collection from the real-world experience. To address the increasing demand for the real-world data and the needs of tech-savvy patients, the innovations are shaping up accordingly. Pharma companies are collaborating with academics and they are co-innovating the technology. E.g. Massachusetts Institute of Technology's program. This focuses on the modernization of clinical trials, strategic use of artificial intelligence and machine learning using real-world evidence, assess the risk-benefit ratio of deploying digital analytics in medicine, and proactively identifying the solutions.


CONCLUSIONS
With unfolding data on impact of science and technology amalgamation, we need a need shared mindset between data scientists and medical professionals to maximize the utility of enormous health and medical data. To tackle this efficiently, there is a need of cross-collaboration and education, and align with ethical and regulatory requirements. A perfect blend of industry, regulatory, and academia will ensure a successful digitization of pharma neurons.",Reviews on recent clinical trials,2020.0,10.2174/1574887115666200621183459,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
6eba9d9bd73349de9cd01e05f6b8e2ca0bf63a85,https://www.semanticscholar.org/paper/6eba9d9bd73349de9cd01e05f6b8e2ca0bf63a85,An Introduction to this Special Issue of AI Magazine,"Web. Indeed, the authors of each article included in this special issue have promised to create and maintain a URL pointing to a working prototype. Now, almost a year later, we have the fruit of this labor. A series of high quality articles describing a wide range of AI systems that are accessible on the Web right now (or so I hope). The articles describe a broad and diverse set of systems. The AI technologies used span the gamut from machine learning to natural language processing, from case-based reasoning to knowledge representation, and In AAAI-96 I gave an invited talk exhorting AI researchers to deploy their prototype systems on the World Wide Web. Deploying AI systems on the Web provides tangible evidence of the power and utility of AI techniques. Next time you encounter AI bashing, wouldn’t it be satisfying to counter with a handful of well-chosen URLs? Well, see the sidebar. At the conference, Jude Shavlik asked me to edit a special issue of AI Magazine describing AI systems that have the Web as their domain. I suggested that the authors make their prototype systems accessible on the more. Applications include Web page filtering, a grant finder, a FAQ finder, a home page finder, a shopping assistant, and more. Is the challenge of deploying systems on the Web a distraction from our long-term goal of understanding intelligence? I believe that the field benefits from a mixture of long-term and short-term goals and from both empirical and theoretical work. Work toward the goal of deploying systems on the Web is a valuable addition to the current mix for two reasons. Internally, the Web suggests novel research challenges and new constraints on existing techniques. Externally, intelligent Web “apps” will help to enhance the reputation of AI and to counter the popular misconception that “if it works, it ain’t AI.” Database and operating system research has spawned software industries with annual revenues in the billions. I look forward to the day when we have a comparable AI industry. Hopefully, Intelligence on the Web will be a step in that direction. An Introduction to This Special Issue of AI Magazine",,1997.0,10.1609/AIMAG.V18I2.1288,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
75b21f9ef074a7385ef3837f63fcb0206006eb10,https://www.semanticscholar.org/paper/75b21f9ef074a7385ef3837f63fcb0206006eb10,COGNITIVE RISKS,"ABSTRACT Decision science have begun to enter the lexicon of risk professionals as the concepts from Prospect Theory become popular in media outlets who increasingly warn about the risk of human biases. Decision-making under uncertainty, popularized by Dan Kahneman, Amos Tversky, Paul Slovic, Herbert Simon and other economists, is more than an examination of human biases. Prospect Theory is a reexamination of the theory of choice and the causes of violations of utility theory that has blossomed into a broad and diverse body of research in behavioral and cognitive science. This paper is an outline for a proposed draft of a cognitive risk framework that will be developed to incorporate behavioral and cognitive science into an enterprise risk framework for cybersecurity and enterprise risk governance. Herbert Simon coined the term “Bounded Rationality” in his seminal book of the same name. “Broadly stated, the task is to replace the global rationality of economic man with the kind of rational behavior that is compatible with the access to information and the computational capacities that are actually possessed by organisms, including man, in the kinds of environments in which such organisms exist” (Simon 1955a: 99). Before the development of modern of PCs and even more powerful machine learning algorithms, Simon foresaw the opportunity at the intersection of human decision-making and technology. Since Simon, other economists and researchers have broaden insights from a multidisciplinary offering of academic studies into applied behavioral science. Notwithstanding these advances, only a few scientists have developed decision science solutions at scale at the enterprise level. Machine learning and other forms of artificial intelligence will require new rules of engagement and governance controls to ensure that bias and ethical use standards have been put in place. Data, the newest commodity in all digital strategies, must be better organized and structured in organizations to allow for efficacious information workflows needed to power organizations to higher performance. And lastly, the role of humans working with and alongside machines as decision-support tools are in the early stage of deployment. The research for the book, Cognitive Risks, will examine the last frontier in risk management – the role of human actors in a business environment that is transitioning to digital products and services. A new level of awareness is needed in a digital environment that differs from the physical world. We know this because of the advent of misinformation that now permeates the Internet. Nation states and Dark Web criminals have weaponized trust in the Internet through misinformation campaigns in social media sites by using behavioral science, or more specifically, cognitive hacks to change our behavior when surfing the web. These attacks are low cost and very effective because most observers are not aware of cognitive risks. There are many variations of “cognitive hacks” and “cognitive risks” which will be explained in detail in the book. Dimitry Kiselev, director general of Russia’s state-controlled Rossiyua Segodnya media conglomerate, “Objectivity is a myth which is proposed and imposed on us.” Today, thanks to the Internet and social media, the manipulation of our perception of the world is taking place on previously unimaginable scales of time, space and intentionality. Cognitive hacks and cognitive risks are part of a new lexicon of risks we must learn. Cognitive risks are commonly referred to as heuristic behavior. Heuristics is any approach to problem solving or self-discovery that employs a practical method that is not guaranteed to be optimal, perfect, or rational, but is nevertheless sufficient for reaching an immediate, short-term goal or approximation. Where finding an optimal solution is impossible or impractical, heuristic methods can be used to speed up the process of finding a satisfactory solution. Heuristics can be mental shortcuts that ease the cognitive load of making a decision. Large swaths of the economy have already misjudged the potential, and the threats, of digital transformation. The questions explored in this paper and the subsequent book, Cognitive Risks, that will follow is why? Why do some leaders see opportunity when others only see problems? Why has the retail industry been blindsided by firms like Amazon, Google, Apple, and so many others? The research for the book will also include an exhaustive review of how applied behavioral science can be used to enhance organizational performance, risk management and cybersecurity in all organizations. Few, if any studies to date, have combined a multidisciplinary approach to enterprise risk management and organizational performance. This will be the first study that builds on a 2020 study of advancements in enterprise risk and board governance to provide a comprehensive analysis of methods and processes to apply behavioral science to address a range of risks facing organizations as they transition to a digital economy.",EDPACS,2021.0,10.1080/07366981.2020.1840020,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
0200cbc5c758a33a47a3993c18a9784001e0e72a,https://www.semanticscholar.org/paper/0200cbc5c758a33a47a3993c18a9784001e0e72a,A Modular Approach for Intrusion Detection System in Wireless Networks,"The increasing reliance upon wireless networks has put tremendous emphasis on wireless network security. Intrusion detection in wireless network has become an indispensable component of any useful wireless network security system, and has recently gained attention in both research and industry communities due to widespread use of wireless local area network (WLAN). Although some intrusion prevention systems have recently appeared in the market, their intrusion detection capabilities are limited. This paper focus on detecting intrusion or anomalous behavior of nodes in WLAN’s using modular technique. We explore the security vulnerabilities of 802.11, numerous intrusion detection techniques, and different network traffic metrics also called as features. Based on the study of metrics we propose a modular based intrusion detection approach. KeywordIntrusion detection system(IDS), Signature based Intrusion Detection (SID), Anomaly based Intrusion Detection (AID) I. INTRODCTION Wireless networks do not rely on a preexisting network infrastructure, and are characterized by wireless multi-hop communication. Unlike fixed wired networks, wireless networks have many operational limitations. For example, the wireless link is constrained by transmission range and bandwidth, and the mobile nodes may be constrained by battery life, CPU, and memory. Wireless networks are used in situations where a network must be deployed rapidly, without an existing infrastructure. Applications of wireless networks include the tactical battlefield, emergency search and rescue missions, as well as civilian ad-hoc situations, such as conferences and classrooms. Wireless networks are vulnerable to additional threats above those for a fixed wired network, due to the wireless communication link and the dynamic and cooperative nature of the routing infrastructure. The wireless link does not provide the same level of Protection for data transmission as a wired link, allowing adversaries within radio transmission range to make attacks against the data transmitted over the wireless link without gaining physical access to the link [1]. II. INTRUSION DETECTION SYSTEM (IDS) Intrusion Detection is the process of monitoring computers or networks for unauthorized entrance, activity, or file modification. IDS can also be used to monitor network traffic, thereby detecting if a system is being targeted by a network attack such as a denial of service attack [1]. There are two basic types of intrusion detection depending on the data collection mechanism Host-based and Network-based. Each has a distinct approach to monitoring and securing data, and each has distinct advantages and disadvantages. In short, hostbased IDSs examine data held on individual computers that serve as hosts, while network-based IDSs examine data exchanged between computers. In addition, IDS may be classified based on the detection technique as described below [2, 3, 5]. There are generally two types of approaches based on the detection technique taken toward network intrusion detection: Misuse Detection also referred to as Signature based Intrusion Detection (SID) and Anomaly based Intrusion Detection (AID). In misuse detection, each network traffic record is identified as either normal or one of many predefined intrusion types. A classifier is typically then trained to discriminate one category from another, based on network traffic data. On the other hand, anomaly detection amounts to training models for learning normal traffic behavior and then classifying, as intrusions, any network behavior that significantly deviates from the known normal network traffic patterns. Therefore, anomaly detection techniques rely on a norm profile and consider a deviation of the subject's behavior from its norm profile as a symptom of an intrusion. Signature recognition techniques utilize intrusion International Journal of Advances in Computer Networks and Security 58 signatures profiles of intrusion characteristics and consider the presence of an intrusion signature as evidence of an intrusion. Anomaly detection techniques use only data of normal activities in information systems for training and building a norm profile. Signature recognition techniques rely on data of both normal and intrusive activities for learning intrusion signatures either manually or automatically through data mining. However, signature recognition techniques have a limitation in that they cannot detect novel intrusions whose signatures are unknown. Anomaly detection techniques capture both known intrusions and unknown intrusions if intrusions demonstrate a significant deviation from a norm profile. Several types of anomaly detection techniques exist: string-based, specification-based, and statistical-based. These model errors are called the inaccuracy in the behavior models. For example, a part of intrusive behavior model falls into normal behavior space. In addition, the intrusive behavior model cannot cover all intrusive behavior space, and the normal behavior model cannot cover all normal behavior space either. This is referred to as the incompleteness in the behavior models. In summary, there are two quality factors in every behavior models, namely inaccuracy and incompleteness. To build a practical intrusion detection system, it is critical to know the precise influence of these two quality factors on its performance [4]. Model Generalization and Its Implications on Intrusion Detection Fig.1 (a) Behavior spaces. (b) Intrusive behavior model. (c) Normal behavior model. Conceptually, SID is only based on the knowledge of the intrusive behavior space, and AID is only based on the knowledge of the normal behavior space. Perfect detection of intrusions can be achieved only if we have a complete behavior model of any one of the two behavior spaces, because what is not bad is good and vice versa ideally. Most of past research only credited the overall efficiency of an intrusion detection technique to such model generalization, and there is hardly any evaluation of It is difficult to model such behavior spaces completely and correctly in reality. Figures 1(b) and 1(c) illustrate real behavior models for signature-based (i.e., intrusive behavior model) and for anomaly-based intrusion detection (i.e., normal behavior model). As the figures indicate, there exist model errors in the behavior models for SID techniques as well as AID ones. Intrusion Signature Intrusion signatures have been characterized as strings, event sequences, activity graphs, and intrusion scenarios (consisting of event sequences, their preconditions, and target compromised states). Finite state machines, colored Petri Nets, associate rules and production rules of expert systems have been used to represent and recognize intrusion signatures. Intrusion signatures are either manually encoded or automatically learned through data mining. However, signature recognition techniques have a limitation in that they cannot detect novel intrusions whose signatures are unknown. III. SECURITY THREATS IN WIRELESS NETWORKS A great deal has been published about previous research on specific security threats in wireless networks from different perspectives. In general, those threats fall into six basic categories [3, 7] as discussed below. 3.1 Interception and unauthorized monitoring of wireless Traffic takes advantages of the open nature of wireless network in several different approaches. • War Drivers [8] are attackers that drive around in a car with a specially configured laptop computer that has software such as Netstumbler [9] to identify wireless network characteristics, e.g. physical location, SSID and security mechanism. We can assume that it is still possible to identify a stumbler by looking for clients that constantly probe for networks but never join. It is also possible, however, to use a completely passive variant of Netstumbler. Use of Netstumbler alone may not be a danger that warrants any action from a wireless intrusion detection system. • Sniffing software, e.g. Kismet [10], enables a war driver or other intruder to eavesdrop on all traffic data of wireless networks. An attacker could capture all the management data that are in plaintext in frame header and use them for further attacks. Even the encrypted data can be accumulated to crack WEP encryption. Detection of passive sniffing is almost impossible. 3.2 Encryption attacks International Journal of Advances in Computer Networks and Security 59 Encryption attacks in 802.11 networks are an extension of interception attacks and have been well-known almost ever since the beginning of the deployment of in 802.11 LAN’s. Lots of early research has elaborated on the weakness of the algorithms used in WEP and its successor, WPA. 3.3 Jamming an access point This a type of Denial-OfService attacks. Current research has shown that, not only do the Denial-of-Service attacks developed for conventional networks still work in a wireless network, but 802.11 networks themselves are extremely vulnerable to Denial-Of-Service [11]: • Radio Frequency interference: An 802.11 network operates in the unlicensed 2.4 GHz and 5GHz bands and although the technology used was originally developed for the military to prevent jamming by an opponent, the current implementation is not immune to radio interference. A strong radio signal may still render an access points useless. To detect these kind of attacks it is necessary to monitor signal to noise ratios on a wireless network. Most network cards will allow you to collect information on signal strength and signal-to noise ratios. • Power saving attack: Another misuse of a management frame, a Traffic Indication Map (TIM) message, can be used by the attacker to fool the client to enter a sleep state which was designed for power saving. This attack can be detected by similar sensor architecture as used in the previous attack. • Virtual carrier-sense attack: An attacker could periodically clai",,2011.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
5a1a8eb1048a1eec7f3770578dad7babe5e3a2ad,https://www.semanticscholar.org/paper/5a1a8eb1048a1eec7f3770578dad7babe5e3a2ad,Proactive Maintenance Model Using Reinforcement Learning Algorithm in Rubber Industry,"This paper presents an investigation into the enhancement of availability of a curing machine deployed in the rubber industry, located in Tamilnadu in India. Machine maintenance is a major task in the rubber industry, due to the demand for product. Critical component identification in curing machines is necessary to prevent rapid failure followed by subsequent repairs that extend curing machine downtime. A reward in the Reinforcement Learning Algorithm (RLA) prevents frequent downtime by improving the availability of the curing machine at time when unscheduled long-term maintenance would interfere with operation, due to the occurrence of unspecified failure to a critical component. Over time, depreciation and degradation of components in a machine are unavoidable, as is shown in the present investigation through intelligent assessment of the lifespan of components. So far, no effective methodology has been implemented in a real-time maintenance environment. RLAs seem to be a more effective application when it is based on intelligent assessment, which encompasses the failure and repair rate used to calculate availability in an automated environment. Training of RLAs is performed to evaluate overall equipment efficiency (OEE) in terms of availability. The availability of a curing machine in the form of state probability is modeled in first-order differential-difference equations. RLAs maximize the rate of availability of the machine. Preventive maintenance (PM) rate for four modules of 16 curing machines is expressed in a transition diagram, using transition rate. Transition rate indicates the degree of PM and unplanned maintenance rates that defines the total availability of the four modules. OEE is expressed in terms of the availability of curing machines, which is related to performance and quality. The results obtained by RLA are promising regarding short-term and long-term efficiencies of OEE, which are 95.19% and 83.37%, respectively.",Processes,2022.0,10.3390/pr10020371,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
0b0647b68b332057635fd1cd9b9387a0d7e6fd89,https://www.semanticscholar.org/paper/0b0647b68b332057635fd1cd9b9387a0d7e6fd89,Real-Time Lime Quality Control through Process Automation,"In the steel industry Tata steel, India, most of the lime produced in the lime plant is used in the steel-making process at LD shops. The quality of steel produced at LD shops depends on the quality of lime used. Moreover, the lime also helps in the crucial dephosphorization process during steel-making. The calcined lime produced in the lime plant goes to the laboratory for testing its final quality (CaO%), which is very difficult to control. To predict, control and enhance the quality of lime during lime making process, five machine-learning-based models such as multivariate linear regression, support vector machine, decision tree, random forest and extreme gradient boosting have been developed using different algorithms. Python has been used as a tool to integrate the algorithms in the models. Each model has been trained on the past 14 months’ data of process parameters, collected from level 1 sensor devices, to predict the future quality of lime. To boost the model’s prediction performance, hyper-parameter tuning has been performed using grid-search algorithm. A comparative study has been done among all the models to select a final model with the least root mean square error in predicting and control future lime quality. After the comparison, results show that the model incorporating support vector machine algorithm has least value of root mean square error of 1.23 in predicting future lime quality. In addition to this, a self-learning approach has also been incorporated into support vector machine model to enhance its performance further in realtime. The result shows that the performance has been boosted from 85% strike-rate in +/-2 error range to 90% of strike-rate in +/-1 error range in real-time. Further, the above predictive model has been extended to build a control model which gives prescriptions as output to control the future quality of lime. For this purpose, a golden batch of good data has been fetched which has shown the best quality of lime (≥ 94% of CaO%). A good range of process parameters has been extracted in the form of upper control limit and lower control limit to tune the set-points and to give the prescriptions to the user. The integration of these two models (Predictive model and control model) helps in controlling the quality of lime 12 hours before its final production of lime in lime plant. Results show that both models (Predictive model and control model) have 90% of strike-rate within +/-1 of error in real-time. Finally, a human machine interface has been developed to facilitate the user to take action based on control model’s output. Eventually this work is deployed as a lime making process automation to predict and control the lime quality.",,2021.0,10.35940/IJESE.B2502.057221,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
e332d2091e697a87bd19eb3d2bb779e6204b0bef,https://www.semanticscholar.org/paper/e332d2091e697a87bd19eb3d2bb779e6204b0bef,SON Function Coordination in Campus Networks Using Machine Learning,"With the advent of 5G, network lifecycle operations such as service initial deployment, configuration changes, upgrades, optimization, and self-healing to name a few, should be fully automated processes to reduce capital expenditure (CAPEX) and operational expenditure (OPEX), and also to allow new players such as industry owners, to come into the scene as nontraditional network operators. To this end, self-organized networks functions (SF) have been proposed as a first attempt to provide self-adaptation capabilities to mobile networks on different fronts and to reduce the error-prone human intervention. Nevertheless, deploying multiple optimization functions in a network brings demanding challenges in terms of conflicting objectives in coordination. Automatically coordinating all those functions is paramount for industry owners in campus networks (CN) since they often do not have a deep expertise to carry out network optimization in an agile manner. Typically, each SF aim at individual goals modifying coupled network parameters, generally in dissonant directions with respect to other SF, jeopardizing the global stability of the system. This work presents an explicit formulation of the joint optimization problem when load balancing optimization (LBO) and coverage and capacity optimization (CCO) are instantiated in a CN.",2022 IEEE Wireless Communications and Networking Conference (WCNC),2022.0,10.1109/wcnc51071.2022.9771586,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
5a5c88eda3141cb068346c9c321b2ced0957f959,https://www.semanticscholar.org/paper/5a5c88eda3141cb068346c9c321b2ced0957f959,Scene setter presentation: Why does innovation and technology matter to the industry’s future?,"Technology is an enabler, not a metaphysical abstraction that will resolve the energy transition. Many of the technologies our industry is working on could help us reduce potential environmental impacts, advance decarbonisation, and improve efficiency. Some cost-saving examples of innovation include centralising and automating work planning and monitoring in well operations, oil and gas production, and refining through to using Robotic Process Automation to automate manual processes. Revenues can be enhanced by using machine learning and advanced analytics to optimise entire systems from the reservoir through to refineries, enabling intervention ahead of predicted equipment failure, optimising supply and demand functions through trading and shipping. Huge investment is also being placed in carbon reduction technology like satellite imaging and drones. We are finally seeing some momentum in CCUS and hydrogen. Digital technologies enable everything we do, particularly because they can be deployed relatively quickly and low cost. But we need to consider a few things when we consider technology. First, we cannot afford to wait for game-changing technologies to solve climate change. Second, the Paris goals can be met with technologies that exist today, albeit not at the price point we would ideally like. The challenge is pace of deployment, supported by customer demand and regulation. A technology-inclusive approach to policymaking can ensure they are able to use the broadest possible range of emission mitigation solutions possible. By picking winners early in the race and focusing public support on a small number of technologies, we are concentrating risk and increasing chances of failure. To view the video, click on the link to the right.",The APPEA Journal,2022.0,10.1071/aj21456,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
7da8a032f2104b92e94616e314630d6bf71930a9,https://www.semanticscholar.org/paper/7da8a032f2104b92e94616e314630d6bf71930a9,ADVANTAGE OF A GREEN AND SMART PORT OF THE FUTURE,"The digitalization is undeniably arriving to the port industry. However, modern digital technologies had not pervaded before in the sector because of, mainly, ports’ complexity and heterogeneity as wide ecosystems. When it comes to applying innovative digital tools in maritime ports, a series of additional common barriers are usually faced: (i) unavailability of clear reference in open-source based technologies, (ii) closed-environments and high pricing rates of private providers, (iii) restricting regulations inside and outside the ports preventing port authorities to deploy useful products and (iv) high heterogeneity of objectives, data or perspectives to carry out focused accountable actions. The PIXEL project has helped ports of all sizes to overcome those barriers during the last three years. Throughout a variety of domains of action – including clean energy, environmental performance, smart intermodal transport or machine learning-based maritime data analytics, PIXEL has addressed those hindrances, driving four European ports towards the Port of The Future stand. The different open applications developed form an ecosystem that may be adopted by external ports aiming at improving their digitalisation levels and their operational and environmental performance. For the 2021 edition, we are presenting the suite of tools deployed in the ports in the context of the action, their success stories and best practices, and how they can be leveraged by worldwide maritime transport entities in the future. It is our objective to provide a comprehensive review of their functioning, technical traits and particularities and how they are planned to be exploited by the Consortium. The PIXEL team truly believes that this will mean a milestone in the operational research field for the ports sector. Finally, we aim at offering a perspective on the usage of modern technologies in maritime ports based on the experience and lessons learned in the conduction of PIXEL project and the interaction with other initiatives in the period 2018–2021.",Urban and Maritime Transport XXVII,2021.0,10.2495/ut210171,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
caf1eb17b91b0f1fb450703c9e81ecd6931924ca,https://www.semanticscholar.org/paper/caf1eb17b91b0f1fb450703c9e81ecd6931924ca,Accelerating the Uptake of Advanced Digital Technologies for Health and Safety Management within Construction: Small and Medium Enterprises (SMEs),"Health and safety problems are essential for the construction industry, and such problems are more pronounced in small and medium enterprises (SMEs) due to the lack of financial resources and skilled personnel. Researchers have explored the feasibility and viability of addressing such constraints using artificial intelligence-enhanced, low-cost sensor systems. Our previous studies have investigated both conventional machine learning and deep neural network models for recognizing workers' postures from low-cost wearable sensors and assessing the ergonomics risks for injury prevention. In the next steps for this research, we are investigating adoption drivers and diffusion barriers for the scaled deployment of AI-enhanced sensor networks and other emerging digital technologies for construction health and safety in a real-work setting. Although the COVID-19 pandemic has brought unprecedented challenges, it has also sped up the digital technology adoption. The discussion in this paper is directed at building on this momentum to advance the use of emerging digital technologies at construction SMEs. The authors conducted a systematic review of literature on digital technologies at construction SMEs and how COVID-19 affected the digital transformation at SMEs. After an initial screening of a total of 170 articles, the key publications based on the research questions were selected for a more in-depth analysis. It emerged that although construction SMEs have embraced the use of several digital technologies during the current pandemic, there is still a large digital divide between these companies and larger companies. The research discussed in this paper contributes to efforts directed at addressing this problem through the design and deployment of SME-centric digital technologies for construction health and safety. © 2021 Computing in Civil Engineering 2021 - Selected Papers from the ASCE International Conference on Computing in Civil Engineering 2021. All rights reserved.",Computing in Civil Engineering 2021,2022.0,10.1061/9780784483893.103,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
68b728fb56797b32b74ffcd6e2bec9a97f1d63a5,https://www.semanticscholar.org/paper/68b728fb56797b32b74ffcd6e2bec9a97f1d63a5,A Framework for Auditing Multilevel Models using Explainability Methods,": Multilevel models (MLMs) are increasingly deployed in industry across different functions. Applications usually result in binary classification within groups or hierarchies based on a set of input features. For transparent and ethical applications of such models, sound audit frameworks need to be developed. In this paper, an audit framework for technical assessment of regression MLMs is proposed. The focus is on three aspects: model, discrimination, and transparency & explainability. These aspects are subsequently divided into sub-aspects. Contributors, such as inter MLM-group fairness, feature contribution order, and aggregated feature contribution, are identified for each of these sub-aspects. To measure the performance of the contributors, the framework proposes a shortlist of KPIs, among others, intergroup individual fairness ( Diff Ind_MLM ) across MLM-groups, probability unexplained (PUX) and percentage of incorrect feature signs (POIFS) . A traffic light risk assessment method is furthermore coupled to these KPIs. For assessing transparency & explainability, different explainability methods (SHAP and LIME) are used, which are compared with a model intrinsic method using quantitative methods and machine learning modelling. Using an open-source dataset, a model is trained and tested and the KPIs are computed. It is demonstrated that popular explainability methods, such as SHAP and LIME, underperform in accuracy when interpreting these models. They fail to predict the order of feature importance, the magnitudes, and occasionally even the nature of the feature contribution (negative versus positive contribution on the outcome). For other contributors, such as group fairness and their associated KPIs, similar analysis and calculations have been performed with the aim of adding profundity to the proposed audit framework. The framework is expected to assist regulatory bodies in performing conformity assessments of AI systems using multilevel binomial classification models at businesses. It will also benefit providers, users, and assessment bodies, as defined in the European Commission’s proposed Regulation on Artificial Intelligence, when deploying AI-systems such as MLMs, to be future-proof and aligned with the regulation.",ArXiv,2022.0,10.48550/arXiv.2207.01611,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
fdf487264a94fc014098523252c83ac28311cb14,https://www.semanticscholar.org/paper/fdf487264a94fc014098523252c83ac28311cb14,An efficient Serverless Architecture to support fine-grained coordination and state sharing for Stateful Applications,"In today’s cloud driven work culture, serverless infrastructure is widely adopted due to its pay-as-you-go beneﬁts. In serverless resource allocation is fully managed by the cloud provider and the developer can solely concentrate on application development. To take to beneﬁt of serverless infrastructure many organizations are migrating their work on serverless. A range of applications and domains including image, text, and speech processing have led to machine learning (ML) becoming a widely deployed technology across many IT industries. It is diﬃcult to implement ML algorithms workﬂows on a serverless platform. There are clearly deﬁned user interactions during various steps in ML workﬂows, such as data preprocessing, data training, and data ﬁne-tuning. The user may execute this frequently and will require low latency and the provision of resources automatically. A serverless platform must be designed eﬃciently to support a stateful application workﬂow. There is a need for distributed shared memory layer to manage synchronization and ﬁne-grained coordination between intermediate shared data. The proposed architecture provides an eﬃcient workﬂow for stateful application with low network latency and the ability to share the mutable object with ﬁne grained coordination and synchronization. I have deployed the architecture on Amazon Web Services lambda functions. I have also validated the architecture using micro-benchmarks like latency, throughput, and parallelism. Further to state the ﬁne-grained state management in ML application on serverless I have compared its performance with AWS Spark Clusters for k-means clustering algorithm using 30 GB of data generated using spark-pref. According to the results, it was found that the proposed architecture optimizes ML application’s performance on serverless in terms of time.",,,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
4661974c05d6d46c7708b5bf7440675229487cab,https://www.semanticscholar.org/paper/4661974c05d6d46c7708b5bf7440675229487cab,Intrusion Detection System for 5G with a Focus on DOS/DDOS Attacks,"The industry of telecommunications is being transformed towards 5G technology, because it has to deal with the emerging and existing use cases. Because, 5G wireless networks need rather large data rates and much higher coverage of the dense base station deployment with the bigger capacity, much better Quality of Service - QoS, and the need very low latency [1–3]. The provision of the needed services which are envisioned by 5G technologies need the new service models of deployment, networking architectures, processing technologies and storage to be defined. These technologies will cause the new problems for the cybersecurity of 5G systems and the security of their functionality. The developers and researchers working in this field make their best to secure 5G systems. The researchers showed that 5G systems have the security challenges. The researchers found the vulnerabilities in 5G systems which allow attackers to integrate malicious code into the system and make the different types of the illegitimate actions. MNmap, Battery drain attacks and MiTM can be successfully implemented on 5G. The paper makes the analysis of the existing cyber security problems in 5G technology. Based on the analysis, we suggest the novel Intrusion Detection System - IDS by means of the machine-learning algorithms. In the related papers the scientists offer to use NSL-KDD in order to train IDS. In our paper we offer to train IDS using the big datasets of DOS/DDOS attacks, besides of training using NSL-KDD. The research also offers the methodology of integration of the offered intrusion detection systems into an standard architecture of 5G. The paper also offers the pseudo code of the designed system.",2021 11th IEEE International Conference on Intelligent Data Acquisition and Advanced Computing Systems: Technology and Applications (IDAACS),2021.0,10.1109/IDAACS53288.2021.9661021,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
8d5b9403278c1b0939488285a0dddefe274213d8,https://www.semanticscholar.org/paper/8d5b9403278c1b0939488285a0dddefe274213d8,Improving automated visual fault inspection for semiconductor manufacturing using a hybrid multistage system of deep neural networks,,J. Intell. Manuf.,2022.0,10.1007/s10845-021-01906-9,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
32715b4400ac93d65cbb589a866abba79395ac73,https://www.semanticscholar.org/paper/32715b4400ac93d65cbb589a866abba79395ac73,New computational intelligence algorithms and their applications,"Several new computational intelligence algorithms and their applications are investigated in this thesis. First, a linear support vector machine decision tree (LSVM-DT) is developed by building a binary tree with a linear support vector machine in each tree node. It has built-in rare event detection mechanism, and allows efficient rule extraction. Secondly, an efficient recursive update algorithm when new data becomes available is derived for least squares support vector machines. This is very essential in online learning. Thirdly, a three-layered learning system is proposed. It consists of a random mapping stage and a learning stage with ordinary least squares, error-correcting least squares, or a linear support vector machine. Next, the three-layered system with ordinary least squares is further developed into a statistical, self-organizing learning system (SSOLS). It incorporates automatic determination of the enhancement nodes using validation, VC dimension, and efficient leave-one-out methods. The t-test pruning procedure and the gradient descent update are used to make the network more compact. The last past of this thesis investigates a real-world example of how computational intelligence algorithms can be applied to automate the decision making processes in manufacturing industries. Data are collected from a Six Sigma simulator that simulates an advanced TV production line. Several computational intelligence algorithms are then used to model this manufacturing process, and a global optimization technique is applied to obtain the optimum input settings that result in maximum overall % yield. Comparison with traditional methods such as Design of Experiments shows promise in deploying computational intelligence algorithms in manufacturing enterprises of the future.",,2003.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
e545b981c54b55e01bf52e0c82d8eb589841302d,https://www.semanticscholar.org/paper/e545b981c54b55e01bf52e0c82d8eb589841302d,Systematic trading : calibration advances through machine learning,"Systematic trading in finance uses computer models to define trade goals, risk controls and rules that can execute trade orders in a methodical way. This thesis investigates how performance in systematic trading can be crucially enhanced by both i) persistently reducing the bid-offer spread quoted by the trader through optimized and realistically backtested strategies and ii) improving the out-of-sample robustness of the strategy selected through the injection of theory into the typically data-driven calibration processes. While doing so it brings to the foreground sound scientific reasons that, for the first time to my knowledge, technically underpin popular academic observations about the recent nature of the financial markets. The thesis conducts consecutive experiments across strategies within the three important building blocks of systematic trading: a) execution, b) quoting and c) risk-reward allowing me to progressively generate more complex and accurate backtested scenarios as recently demanded in the literature (Cahan et al. (2010)). The three experiments conducted are: 1. Execution: an execution model based on support vector machines. The first experiment is deployed to improve the realism of the other two. It analyses a popular model of execution: the volume weighted average price (VWAP). The VWAP algorithm targets to split the size of an order along the trading session according to the expected intraday volume's profile since the activity in the markets typically resembles convex seasonality – with more activity around the open and the closing auctions than along the rest of the day. In doing so, the main challenge is to provide the model with a reasonable expected profile. After proving in my data sample that two simple static approaches to the profile overcome the PCA-ARMA from Bialkowski et al. (2008) (a popular two-fold model composed by a dynamic component around an unsupervised learning structure) a further combination of both through an index based on supervised learning is proposed. The Sample Sensitivity Index hence successfully allows estimating the expected volume's profile more accurately by selecting those ranges of time where the model shall be less sensitive to past data through the identification of patterns via support vector machines. Only once the intraday execution risk has been defined can the quoting policy of a mid-frequency (in general, up to a week) hedging strategy be accurately analysed. 2. Quoting: a quoting model built upon particle swarm optimization. The second experiment analyses for the first time to my knowledge how to achieve the disruptive 50% bid-offer spread discount observed in Menkveld (2013) without increasing the risk profile of a trading agent. The experiment depends crucially on a series of variables of which market impact and slippage are typically the most difficult to estimate. By adapting the market impact model in Almgren et al. (2005) to the VWAP developed in the previous experiment and by estimating its slippage through its errors' distribution a framework within which the bid-offer spread can be assessed is generated. First, a full-replication spread, (that set out following the strict definition of a product in order to hedge it completely) is calculated and fixed as a benchmark. Then, by allowing benefiting from a lower market impact at the cost of assuming deviation risk (tracking error and tail risk) a non-full-replication spread is calibrated through particle swarm optimization (PSO) as in Diez et al. (2012) and compared with the benchmark. Finally, it is shown that the latter can reach a discount of a 50% with respect to the benchmark if a certain number of trades is granted. This typically occurs on the most liquid securities. This result not only underpins Menkveld's observations but also points out that there is room for further reductions. When seeking additional performance, once the quoting policy has been defined, a further layer with a calibrated risk-reward policy shall be deployed. 3. Risk-Reward: a calibration model defined within a Q-learning framework. The third experiment analyses how the calibration process of a risk-reward policy can be enhanced to achieve a more robust out-of-sample performance – a cornerstone in quantitative trading. It successfully gives a response to the literature that recently focusses on the detrimental role of overfitting (Bailey et al. (2013a)). The experiment was motivated by the assumption that the techniques underpinned by financial theory shall show a better behaviour (a lower deviation between in-sample and out-of-sample performance) than the classical data-driven only processes. As such, both approaches are compared within a framework of active trading upon a novel indicator. The indicator, called the Expectations' Shift, is rooted on the expectations of the markets' evolution embedded in the dynamics of the prices. The crucial challenge of the experiment is the injection of theory within the calibration process. This is achieved through the usage of reinforcement learning (RL). RL is an area of ML inspired by behaviourist psychology concerned with how software agents take decisions in an specific environment incentivised by a policy of rewards. By analysing the Q-learning matrix that collects the set of state/actions learnt by the agent within the environment, defined by each combination of parameters considered within the calibration universe, the rationale that an autonomous agent would have learnt in terms of risk management can be generated. Finally, by then selecting the combination of parameters whose attached rationale is closest to that of the portfolio manager a data-driven solution that converges to the theory-driven solution can be found and this is shown to successfully outperform out-of-sample the classical approaches followed in Finance. The thesis contributes to science by addressing what techniques could underpin recent academic findings about the nature of the trading industry for which a scientific explanation was not yet given: • A novel agent-based approach that allows for a robust out-of-sampkle performance by crucially providing the trader with a way to inject financial insights into the generally data-driven only calibration processes. It this way benefits from surpassing the generic model limitations present in the literature (Bailey et al. (2013b), Schorfheid and Wolpin (2012), Van Belle and Kerr (2012) or Weiss and Kulikowski (1991)) by finding a point where theory-driven patterns (the trader's priors tend to enhance out-of-sample robustness) merge with data-driven ones (those that allow to exploit latent information). • The provision of a technique that, to the best of my knowledge, explains for the first time how to reduce the bid-offer spread quoted by a traditional trader without modifying her risk appetite. A reduction not previously addressed in the literature in spite of the fact that the increasing regulation against the assumption of risk by market makers (e.g. Dodd–Frank Wall Street Reform and Consumer Protection Act) does yet coincide with the aggressive discounts observed by Menkveld (2013). As a result, this thesis could further contribute to science by serving as a framework to conduct future analyses in the context of systematic trading. • The completion of a mid-frequency trading experiment with high frequency execution information. It is shown how the latter can have a significant effect on the former not only through the erosion of its performance but, more subtly, by changing its entire strategic design (both, optimal composition and parameterization). This tends to be highly disregarded by the financial literature. More importantly, the methodologies disclosed herein have been crucial to underpin the setup of a new unit in the industry, BBVA's Global Strategies & Data Science. This disruptive, global and cross-asset team gives an enhanced role to science by successfully becoming the main responsible for the risk management of the Bank's strategies both in electronic trading and electronic commerce. Other contributions include: the provision of a novel risk measure (flowVaR); the proposal of a novel trading indicator (Expectations’ Shift); and the definition of a novel index that allows to improve the estimation of the intraday volume’s profile (Sample Sensitivity Index).",,2015.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
1690f9b7c9a39357177b4532592ae46b4ea11434,https://www.semanticscholar.org/paper/1690f9b7c9a39357177b4532592ae46b4ea11434,Online Reinforcement Learning Control of an Electromagnetic Manipulator,"Machine Learning Control is a control paradigm that applies Artificial Intelligence methods to control problems. Within this domain, the field of Reinforcement Learning (RL) is particularly promising, since it provides a framework in which a control policy does not have to be programmed explicitly, but can be learned by an intelligent controller directly from real-world data, allowing to control systems that are either arduous or even impossible to model analytically. However, in spite of such considerable potential, the RL paradigm poses a number of challenges that effectively hinder its applications in the real-world and in industry. It is therefore critical that research in this field is advanced until RL-based controllers can be practically demonstrated to be real-world feasible and reliable. This thesis report presents the attempts made at applying control strategies based on Reinforcement Learning to solve a precise positioning task with a physical experimental setup. The setup at hand is a magnetic manipulator (magman) characterized by a high degree of nonlinearity. The controller uses the spatially continuous magnetic field generated by four actuators to displace a steel ball, constrained to move in one dimension, towards a reference position. Two different implementations of the Q-learning algorithm (Sutton, Barto, et al., 1998) were deployed. In spite of the good results obtained in a simplified simulated environment, both implementations failed on the experimental setup. The negative outcome of these experiments is mainly due to the fact that, since the task at hand is an accurate positioning task, the reward obtained by the learner while interacting with the environment is too sparse for it to be able to learn a stabilizing control policy. Other factors have presumably contributed to the controllers’ failure, such as the circumstance that the agent does not have access to the full system state information and a sub-optimal tuning of the algorithms’ hyper-parameters. Besides model-free RL, the Value Iteration model-based method was successfully applied both in simulations and with the experimental setup. The present findings suggest that, in order to solve the magman task with model-free RL, more sophisticated algorithms need to be deployed, such as for example an agent that can naturally deal with continuous state and action spaces, as the DDPG algorithm (Lillicrap et al., 2015), with exploration carried out in the parameter-space rather than in the control action space (Plappert et al., 2017), in addition to a more optimal exploitation of the information extracted from the environment, for example using Hindsight Experience Replay (Andrychowicz et al., 2017).",,2019.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
dace52c2aebc2657dda059072787ad2b53b5590a,https://www.semanticscholar.org/paper/dace52c2aebc2657dda059072787ad2b53b5590a,Detection of Coordinate Based Accident-Prone Areas on Road Surface using Machine Learning Methods,"The growth in the road networks in India and other developing countries have influenced the growth in transport industry and other industries, which depends on the road network for operations. The industries such as postal services or mover services have influenced the similar growths in these industries as well. However, the dependency of these industries is high on the road surface conditions and any deviation on the road surface conditions can also influence the performance of the services provided by the mentioned services. Nonetheless, the conditions of the road surface are one of the prime factors for road safety and number of evidences are found, which are discussed in subsequent sections of this work, that the bad road surface conditions are increasing the road accidents. Several parallel research attempts are deployed in order to find out, the regions where the road surface conditions are not proper, and the traffic density is higher. Nevertheless, outcomes of these parallel works are highly criticised due to the lack of accuracy in detection of the road surface defects, detection of accurate location of the defects and detection of the traffic density data from various sources. Thus, this work proposes a novel framework for detection of the road defect and further mapping to the spatial data coordinates resulting into the detection of the accident-prone zones or accident affinities of the roads. This work deploys a self-adjusting parametric coefficient-based regression model for detection of the risk factors of the road defects and in the other hand, extracts the traffic density of the road regions and further maps the accident affinities. This work outcomes into 97.69% accurate detection of the road accident affinity and demonstrates less complexity compared with the other parallel research outcomes.",,2020.0,10.47277/ijceit/12(3)1,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
514ce8be29824781ca0e5489c680502b0f31242f,https://www.semanticscholar.org/paper/514ce8be29824781ca0e5489c680502b0f31242f,Real Prediction Machines,"Predicting the future is no longer about the mystical reading of natural and celestial phenomena. Today it is all about data. 
The Real Prediction Machine (RPM) is a domestic product that uses big and small data, in combination with machine learning and predictive modelling to make predictions about specific future events. 
 
Contemporary use of digital networked technology, such as personal computers and smart phones, is effectively feeding a live global human behaviour laboratory with data scientists experimenting on an (often) unknowing pool of billions. The futures that emerge from this research are as yet mostly unknown, but there are hints – as this data accumulates it can be analysed, mined and used in algorithms; patterns or trends invisible to the human observer can be identified; and seemingly random events become predictable. At this time prediction algorithms are predominantly being exploited by big industries such as banking, insurance and commerce, or examined in massive research projects such as the EU funded FuturICT project. They are, however, making surreptitious steps into our lives through tailored internet browsing and predictive shopping with occasional Kafkaesque consequences. 
RPMs exploits the potential of this technology motivated not by the interests of industry and research but by the more emotive and personal needs/desires of people – this has the purpose of communicating the transformative potential of big data in domestic life, and asking if the future possibilities described by the project are desirable. 
 
The concept 
When things fail they rarely do so instantaneously but through a gradual process of deterioration. Based on this observation, predictive analytics, through the deployment of sensors in pertinent places and contexts, can determine the when things begin to fail. Such techniques are increasingly used in the mechanical and structural world - to predict for example when a vehicle or bridge might be in need of pre-emptive maintenance. 
 
The RPMs use similar techniques but in the context of human everyday life to predict anything from health related events such as a heart attack to more emotive forecasts such as a domestic argument. 
 
Once the event has been chosen the necessary and available data streams, from local sensors to RSS feeds, determined they are fed into the prediction algorithm - the output of which controls the visual display on the prediction machine. This informs the viewer if the chosen event is approaching, receding or impending. 
 
The Real Prediction Machines was commissioned by the Crafts Council for the exhibition Crafting Narrative. This explored how contemporary designers and makers use objects as mediums to tell stories. 
 
Project developed in collaboration with Subramanian Ramamoorthy and Alan Murray. Engineering by Nick Williamson.",,2015.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
309f1fa5345e8ab46addd5b3b87343d3a06c7430,https://www.semanticscholar.org/paper/309f1fa5345e8ab46addd5b3b87343d3a06c7430,Implementation of machine learning algorithm in embedded devices,"This paper describes the usage of neural networks in microcontrollers for deployment in embedded devices. The issue is focused on the design of a suitable neural network, its optimization and deployment in a 32-bit microcontroller with regards to the limiting factors of the chosen microcontroller. The introductory part of the article is a description of the used technology and hardware on which the solution will be implemented. Accelerometer motion recognition was chosen as a practical application. The proposed solution recognizes 6 basic movements, respectively movement in three axes. Tensorflow and Keras frameworks were used to design and implement a neural network. The created neural network model was after optimization implemented in the firmware of the STM32L4x microcontroller. The proposed solution implements automatic motion detection and its subsequent classification. The proposed principle can be applied to a group of sensors connected to the available interfaces of the microcontroller. Application with an accelerometer can be used to detect specific vibrations, application with MEMS microphones can be used to detect specific sound patterns that indicate a possible fault condition of the monitored device in industry.",2020 19th International Conference on Mechatronics - Mechatronika (ME),2020.0,10.1109/ME49197.2020.9286705,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
d37e9fab5a3e07af7f4a5d596d6e25b366cce464,https://www.semanticscholar.org/paper/d37e9fab5a3e07af7f4a5d596d6e25b366cce464,Introduction to the ACSAC’20 Special Issue,"The Annual Computer Security Applications Conference (ACSAC) brings together cutting-edge researchers, with a broad cross-section of security professionals drawn from academia, industry, and government, gathered to present and discuss the latest security results and topics. ACSAC’s core mission is to investigate practical solutions for computer and network security technology. The 36th Annual Computer Security Applications Conference was held virtually on December 7-11, 2020. As in the previ-ous year, ACSAC especially encouraged contributions in the area of Deployable and Impactful Security . Deployable and impactful security solutions aim to address key real-world challenges, which may include accuracy, run-time overhead, ground-truth labeling, human aspects, usability, and energy consumption. Having the deployability and impactfulness goals motivates one to focus on solving the most critical real-world challenges, which may otherwise be ignored by the fast-moving research community. In addition, ACSAC encourages authors of accepted papers to submit software and data artifacts and make them publicly available to the entire community. Releasing software and data artifacts represents an important step towards fa-cilitating the reproducibility of research results, and ultimately contributes to the real-world deployment of novel security solutions.Forthis special issue we invited authors of papers that appeared at ACSAC 2020 and that successfully passed an evaluation of their software and/or data artifacts to submit an extended version of their papers. This selection criteria ensured that the research has a high potential for being deployed in real-world environments and to be used to implement practical defense systems.Thisvolume contains three manuscripts on topics from three different areas: attacks on trusted execution environments, operations security, and usable security and privacy. In “Faulty Point Unit: ABI Poisoning Attacks on Trusted Execution Environments,” Alder et al. show that current Trusted Execution Environment (TEE) implementations do not properly sanitize Floating Point Unit (FPU) registers on enclave entry. This leads to potential security issues. For instance, the authors show that it is possible to change the prediction results of a machine learning classifier running in an enclave. The extended version of the ACSAC 2020",Digital Threats: Research and Practice,2022.0,10.1145/3534708,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
81615b07cabd5d2cf15a807a054e56fc322cd72c,https://www.semanticscholar.org/paper/81615b07cabd5d2cf15a807a054e56fc322cd72c,Investigation of Data Compression Methods for Intelligent Machine Condition Monitoring,"Condition monitoring (CM) deliveries significant benefits to the industry by minimising breakdown losses and enhancing the safety and high-performance operation of machinery. However, the use of data acquisition systems with multiple sensors and high sampling rates leads to massive data and causes considerably high cost for purchasing and deploying hardware for data transmission, storage and processing. Hence, data compression is crucial and important to reduce the data size and speed up the calculation for the development of intelligent machine CM systems. Although data compression has received high attention in many fields, few researchers have focused on their research in the field of machine CM. Therefore, this PhD research concentrates on investigating novel and high-performance data compression algorithms according to the characteristics of one-dimensional (1D) and two-dimensional (2D) signals to solve the bottleneck of the massive data transmission, and hence improve the performance of remote and real-time machine CM systems. 
 
 
The research is carried out according to a compound experimental and analytic route based on a wireless senor network. To demonstrate the effectiveness of data compression based techniques for CM, the prototype of an intelligent wireless sensing system is developed using cost-effective micro-electromechanical systems (MEMS) accelerometers and the Bluetooth low energy (BLE) communication module. Moreover, various waveform parameters with low cost computing in time and frequency domains are investigated and identified that RMS is the most effective parameter to give good indication for the leakage in a piping system, showing that data compression via statistics is effective and thus indicates that the performance of data compression for CM highly depends on applications. 
 
 
Subsequently, high-performance but high-complexity methods are proposed base on dimension reduction, sparse representation, feature extraction and advanced compressive sensing (CS) for fault diagnosis of rotating machinery with 1D or 2D signals, which have the potentials to be implemented on MEMS modules in a wireless sensor network (WSN) in future. Firstly, a compression scheme based on dimension reduction is proposed to extract the periodic characteristics of the 1D vibration signal. Recurrence plot (RP) of vibration phase space trajectory and its quantification indicators, as well as principal component analysis (PCA), are combined to realize feature extraction, compression and fault classification for a tapered roller bearing system. 
 
 
Secondly, a two-step compression method is performed on 1D vibration signals based on frequency shift, adaptive sparse representation and CS is explored to overcome the problem of the large quantity of data storage for ball bearing fault diagnosis. Simultaneously, this compression method has the capability to reconstruct envelope signals with noise elimination.Then, for 2D thermal images captured from a two-stage reciprocating compressor, the dense scale-invariant feature transform (SIFT) features indicating edge information are extracted and represented as a sparse matrix by sparse coding. The compressed features are used for the classification of six different types of faults with the support vector machine (SVM). 
 
 
Finally, the advanced CS technique is exploited on pre-processing the 2D thermal images of gearboxes to realise intelligent fault classification with high accuracy of more than 99.81% by a typical deep learning algorithm, namely convolutional neural network (CNN). The CNN calculation speed is dramatically accelerated with compressed images. All these proposed approaches are evaluated by simulations and experiments, which verifies that they can reliably detect the fault types or classify different fault types with very high accuracy. Besides, the proposed data compression based intelligent CM approaches provide theoretical bases for maintenance-free CM systems because data compression can save the transmission bandwidth and power consumption for remote and real-time machine CM systems.",,2020.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
7e85a1462d85730543ef206a923fae64d9236b4f,https://www.semanticscholar.org/paper/7e85a1462d85730543ef206a923fae64d9236b4f,Digital Twin: Universal User Interface for Online Management of the Manufacturing System,"
 Industry 4.0 concept enables connecting a multitude of equipment to computer simulations through IoT and virtual commissioning, but using conventional interfaces for each separate piece of equipment for control and maintenance of Digital Twins is not always an optimal solution. Industrial Digital Twins software toolkits usually consist of simulation or offline programming tools. It can even connect real machines and controllers and sensors to feed a simulation with actual production data and later analyze it. Moreover, Virtual Reality (VR) and Augmented Reality (AR) are used in different ways for monitoring and design purposes. However, there are many software tools for the simulation and re-programming of robots on the market already, but those are a limited number of software that combine all these features, and all of those send data only in one way, not allowing to re-program machines from the simulations. The related research aims to build a modular framework for designing and deploying Digital Twins of industrial equipment (i.e., robots, manufacturing lines), focusing on online connectivity for monitoring and control. A developed use-case solution enables one to operate the equipment in VR/AR/Personal Computer (PC) and mobile interfaces from any point globally while receiving real-time feedback and state information of the machinery equipment. Gamified multi-platform interfaces allow for more intuitive interactions with Digital Twins, providing a real-scale model of the real device, augmented by spatial UIs, actuated physical elements, and gesture tracking.
 The introduced solution can control and simulate any aspect of the production line without limitation of brand or type of the machine and being managed and self-learning independently by exploiting Machine Learning algorithms. Moreover, various interfaces such as PC, mobile, VR, and AR give an unlimited number of options for interactions with your manufacturing shop floor both offline and online. Furthermore, when it comes to manufacturing floor data monitoring, all gathered data is being used for statistical analysis, and in a later phase, predictive maintenance functions are enabled based on it.
 However, the research scope is broader; this particular research paper introduces a use-case interface on a mobile platform, monitoring and controlling the production unit of three various industrial- and three various mobile robots, partially supported by data monitoring sensors. The solution is developed using the game engine Unity3D, Robot Operation System (ROS), and MQTT for connectivity. Thus, developed is a universal modular Digital Twin all-in-one software platform for users and operators, enabling full control over the manufacturing system unit.",Volume 2B: Advanced Manufacturing,2021.0,10.1115/imece2021-69092,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
fabc67311339044d9f55e32d274dc44db610f1f2,https://www.semanticscholar.org/paper/fabc67311339044d9f55e32d274dc44db610f1f2,Real Time Customer Churn Scoring Model for the Telecommunications Industry,"There are two types of customers in the telecommunication industry; the pre-paid and the contract customers. In South Africa it is the pre-paid customers that keep telcos constantly worried because such customers do not have anything binding them to the company, they can leave and join a competitor at any time. To retain such customers, telcos need to customise suitable solutions especially for those customers that are agitating and can churn at any time. This needs customer churn prediction models that would take advantage of big data analytics and provide the telco industry with a real time solution. The purpose of this study was to develop a real time customer churn prediction model. The study used the CRISP-DM methodology and the three machine learning algorithms for implementation. Watson Studio software was used for the model prototype deployment. The study used the confusion matrix to unpack a number of performance measures. The results showed that all the models had some degree of misclassification, however the misclassification rate of the Logistic Regression was very minimal (2.2%) as differentiated from the Random Forest and the Decision Tree, which had misclassification rates of 20.8% and 21.7% respectively. The results further showed that both Random Forest and the Decision Tree had good accuracy rates of 78.3% and 79.2% respectively, although they were still not better than that of the Logistic Regression. Despite the two having good accuracy rates, they had the highest rates of misclassification of class events. The conclusion we drew from this was that, accuracy is not a dependable measure for determining model performance.",2020 2nd International Multidisciplinary Information Technology and Engineering Conference (IMITEC),2020.0,10.1109/IMITEC50163.2020.9334129,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
6ea25b3405e8cd27a0f0a5a8e0f2dcbc053d6280,https://www.semanticscholar.org/paper/6ea25b3405e8cd27a0f0a5a8e0f2dcbc053d6280,Quantitative Finance Research Newsletter Oxford-Man Institute OMIReNew Recovering Missing Firm Characteristics with Attention-based Machine Learning,"model reconstructs firm characteristics with high accuracy and comfortably outperforms competing approaches. Revisiting the vast literature on risk factors in financial research reveals the of missing observations The work develops an approach for solving time-consistent risk-sensitive stochastic optimisation problems using model-free reinforcement learning. It assumes that agents assess the risk of a sequence of random variables using dynamic convex risk measures. They employ a time-consistent dynamic programming principle to determine the value of a particular policy, and develop policy gradient update rules that aid in obtaining optimal policies. They further develop an actor-critic style algorithm using neural networks to optimise over policies. Finally, the work demonstrates the performance and flexibility of their approach by applying it to three optimisation problems: statistical arbitrage trading strategies, obstacle avoidance robot control, and financial hedging. We present a simple and effective methodology for the generation of lexicons (word lists) that may be used in natural language scoring applications. In particular, in the finance industry, word lists have become ubiquitous for sentiment scoring. These have been derived from dictionaries such as the Harvard Inquirer and require manual curation. Here, we present an automated approach to the curation of lexicons, which makes automatic preparation of any initial word list immediate, which can then be further curated. We show that our automated word lists deliver comparable performance to traditional lexicons on machine learning classification tasks. This new approach will enable finance academics and practitioners to create and deploy new word lists in addition to the few traditional ones in a facile manner. The work studies the long-term impact of climate change on economic activity across countries, using a stochastic growth model where productivity is affected by deviations of temperature and precipitation from their long-term moving average historical norms. Using a panel data set of 174 countries the 1960 2014, they find that the per-capita real output divergence to The demonstrates the effectiveness with numerical experiments which highlight both removal and the fidelity of the calibrated simulator. on the estimation of the equity joint estimation of the The work investigates the impact of order flow imbalance (OFI) on price movements in equity markets in a multiasset setting. First, authors show that taking into account multiple levels of the order book when defining order book imbalance leads to higher explanatory power for the contemporaneous price impact of OFI. Using a principal component analysis of OFI across order book levels, they define a notion of integrated OFI which shows superior explanatory power for market impact both in-sample and out-of-sample. Second, they examine the notion of cross-impact and show that, once the information from multiple levels is included in OFI, multi-asset models with cross-impact do not provide additional explanatory power for contemporaneous impact compared to a sparse model without cross-impact terms. However, they find evidence that cross-impact terms provide additional information for intraday forecasting of future returns",,2022.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
4fe7d1e2dcd288e8d28ea0086c9a0796a60e5d56,https://www.semanticscholar.org/paper/4fe7d1e2dcd288e8d28ea0086c9a0796a60e5d56,Node-Node Data Exchange in IoT Devices Using Twofish and DHE,"Article history: Received: 24 December, 2020 Accepted: 08 March, 2021 Online: 20 March, 2021 Internet of Things provides the support for devices, people and things to collaborate in collecting, analyzing and sharing sensitive information from one device onto the other through the internet. The internet of things is thriving largely due to access, connectivity, artificial intelligence and machine learning approaches that it supports. The stability and enhanced speed of the internet is also attributable to the huge adoption rate that IoT continues to enjoy from Governments, industry and academia in recent times. The increased incidences of cyber-attacks on connected systems in recent times, has inspired the heightened efforts from Governments, industry practitioners and the research world towards improving existing approaches and the engineering of new innovative schemes of securing devices, the software or the platforms for the deployment of IoT. Security solution for Internet of things includes the use of secure ciphers and key exchange algorithms that ensures the provisioning of a security layer for the: devices or hardware, communication channels, cloud, and the life cycle management constituting the Internet of things. The use of key exchange algorithms in resilient cryptographic solution that have less computational requirements without compromising the security efficiency in the encryption of messages for IoT continues to be the preferred approach in securing messages in a node-node exchange of data. This paper aims at providing a cryptographic solution that uses a key exchange cryptographic primitive and a strong cipher in encrypting messages for exchange between nodes in an IoT. Towards achieving this goal, the Diffie-Hellman key exchange (DHE) protocol was used to provide a secure key exchange between the communicating nodes, whiles the Twofish block cipher was used in the encryption and decryption of messages, assuring the security, privacy and integrity of messages in a node-node IoT data exchange. The cryptographic solution has a high throughput.",,2021.0,10.25046/AJ060271,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
ecb7fcc50b6aad362c44bc7ad85fcfadde24337d,https://www.semanticscholar.org/paper/ecb7fcc50b6aad362c44bc7ad85fcfadde24337d,Deep-Learning-Enabled Automatic Optical Inspection for Module-Level Defects in LCD,"Liquid crystal display (LCD) defects detection on module level is increasingly important for flat-panel displays (FPD) industry to increase the production capacity via machine vision technology. However, it is an overwhelmingly challenging issue due to various difficulties. This article discloses a practical automatic optical inspection (AOI) system consisting of hardware structure and software algorithm to detect module-level defects. The AOI system is the core component to build a distributed integrated inspection system with the help of the Internet of Things (IoT). Starting from the analysis of the challenges encountered in module-level defects inspection, a delicate photograph scheme is proposed to reveal different kinds of defects. In order to robustly work on the module-level defects detection with complex situations, a novel framework based on YOLOV3 detection unit is proposed in this article, including the preprocessing module, detection module, defects definition module, and interferences elimination module. To the best of our knowledge, this is the first work that designs a practical AOI system for module-level defects detection. In order to demonstrate the effectiveness of the proposed method, extensive experiments have been conducted on the manufacturing lines. The evaluation of the detection performance of the AOI system in comparison with a manual scheme indicates that the proposed system is practical for module-level defects detection. Currently, the proposed system has been deployed in a real-world LCD manufacturing line from a major player in the world.",IEEE Internet of Things Journal,2021.0,10.1109/JIOT.2021.3079440,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
9f3f6ab0c372325af61a43f114070343b36f7dde,https://www.semanticscholar.org/paper/9f3f6ab0c372325af61a43f114070343b36f7dde,Adaptive Autonomous Agent Responses to Targeted Malware Attacks,"We describe the application of machine learning and data mining techniques for defensive autonomous agent responses to targeted cyberattacks. This approach clusters and classifies captured enterprise malware, and fuses the inferred classes with other relevant threat data to detect targeted attacks indicative of malware delivery attempts by advanced persistent adversaries. Defensive autonomous agents, whose behaviors are learned through specialized process modeling algorithms, are then improved through our enhanced situational knowledge of targeted attacks. The autonomous agents are guided by high-level process models, with which human operators interact for orchestrating lower-level autonomous agents. Agent responses are focused on protecting critical cyber assets, leveraging knowledge of potential paths of exploitation through the network. We employ agent-based simulation for rapid testing and refinement of process orchestration and agent behaviors. 1.0 RESEARCH CHALLENGES Organizations undergo continual attacks from a range of threat actors with varying capabilities and intent. When defenders are better able to understand their adversaries, they are better able to respond. Indeed, it has been recognized that incident response would greatly benefit from improved capabilities for malware analysis [1]. Among the most serious threats are advanced adversaries who are targeting a specific organization. Attackers morph their malware to help evade detection by anti-malware systems, and target different individuals in the organization with different malware delivery methods. Since malware campaigns are tracked using hashes computed using byte code of malware, morphing of the malware also makes it difficult to recognize targeted campaigns. Security operations suffer from largely manual processes for correlating attack indicators, making it difficult to keep pace with adversary activities. There are numerous open problems in recognizing targeted malware attacks and mounting adaptive autonomous responses against them. While advanced methods exist for computing malware semantic similarity, malware is readily shared through cybercrime industry, so that features derived from other malware artifacts and context are needed for distinguishing among adversaries. Knowledge about defensive responses by human operators needs to be captured by autonomous agents, and continually updated as defensive processes improve; operators also need to interrogate and orchestrate autonomous agents when needed. Ideally, attack responses should ideally be guided by paths of potential adversary movement through the network, and focused on protecting critical cyber assets. Overall, this problem involves rich webs of interrelated data, which requires a flexible and manageable knowledge base to be maintained and shared among defensive agents. There are also scalability issues, since the space of malware is large and their correlations scale quadratically, as do other kinds of relationships such as potential adversary paths. Adaptive Autonomous Agent Responses to Targeted Attacks PAPER NBR x STO-MP-IST-148 2.0 APPROACH We propose an intelligent autonomous agent architecture for recognizing and responding to targeted malware attacks. Through unsupervised learning on malware features, this discover clusters of related malware indicative of targeted attacks, and then classifies those clusters according to known adversaries. For this, we can leverage our previous work in symbolic interpretation to extract generalized semantics from binaries, for fast similarity matching against large malware repositories [2]. These malware inferences can be fused with other threat information (delivery mechanisms, social engineering employed, known threat actor behaviors, etc.), for more accurate and fine-grained classification of adversary campaigns. This enhanced situational awareness can guide the responses of our defensive autonomous agents, e.g., applying similar responses for similar attacks. We propose to automate the learning of response behaviors through process mining [3]. This extracts hierarchical Markov models from cyber defender event logs, for learning patterns in defender operational processes (e.g., adversary hunters). We then map the discovered lower-level processes to autonomous agents, which communicate with high-level process models for organizational vetting and agent orchestration. The autonomous agents are informed by our knowledge of targeted attacks. Through Monte Carlo simulation and machine learning, the agent models are trained to adapt to different targeted attack situations. Through machine learning of optimal processes, we thus adapt the autonomous agents to best respond to targeted attacks. We define orchestration processes that capture the high-level flow of an organization’s security operations. An agent-based simulation framework then simulates attacker and defender agents, which generate simulation event logs for further iterations of process refinement. The agent responses are guided by an understanding of potentially exploitable paths through the enterprise network [4], as well as historical patterns of communication among mission-critical cyber assets [5]. For knowledge management and situational awareness within this complex web of interrelated information, we leverage our previous work in representing such interrelationships as a knowledge graph [6], with flexible schema-free design and ad hoc query-based analysis and visualization. Figure 2 shows our architecture for responding to targeted malware attacks via autonomous agents. Agents deployed on enterprise endpoint nodes and gateways monitor and detect malware attempts (phishing emails, malicious web sites, etc.). Detected malware are sent to a “malware jail” for quarantine and dynamic behavioral observation and analysis. Malware are unpacked and subjected to deep semantic analysis of binary code [7], which extracts generalized semantics for machine learning features. From these features (along with semantic features from trusted partners sharing malware intelligence), an intelligent response orchestrator compares malware instances with historical archives, combines malware similarity inferences (e.g., cluster matches) with other relevant information (e.g., mined rules for response processes, enterprise security posture, mission dependencies, and external threat intelligence) for deciding optimal responses. The response orchestrator then pushes response decisions to agents deployed on network hosts. Figure 1: Clusters of Related Malware [10].",,2017.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
45f3bc90e4c721a0a91a5083a6272552a10f4d78,https://www.semanticscholar.org/paper/45f3bc90e4c721a0a91a5083a6272552a10f4d78,How to Gain a Competitive Edge with the Digital Twin,"
 With the move towards IIoT and the Digital Age, companies in the process industry are looking to digitalize processes. One critical element in this development is the increased adoption of the Digital Twin (DT), which is being deployed in all phases of a product, plant, or process lifecycle from design to operations to maintenance.
 A DT is a virtual representation of a physical product or process used to understand and predict the physical counterpart's performance characteristics. They are used throughout the product lifecycle to simulate, predict, and optimize the product and production system before investing in physical prototypes.
 The concept is not new. For more than 30 years, product and process engineering teams have used 3D renderings and process simulation to validate manufacturability. What is new, however, is that several factors have now converged to bring the concept of the DT to the forefront as a disruptive trend in the process industry. This will be demonstrated using relevant use cases featuring organizations that have seen increases in efficiency improvements after employing digitalization and the DT
 By incorporating simulation, data analytics, and machine learning, the DT is able to demonstrate impacts of usage and training scenarios in a virtual setting. This enables identification of any potential issues in the design phase as opposed to the commissioning phase as is very common today. Process and sensor data from physical objects are collected and analyzed to determine real-time performance and operating changes over time. Feeding this data back into the product lifecycle, the DT is continuously updated to reflect changes to the physical counterpart. This creates a closed-loop of virtual feedback that makes products, production, and performance optimization possible at minimal cost.",,2020.0,10.4043/30801-ms,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
c3da4a75d102951d3566893e972b6fe549121a81,https://www.semanticscholar.org/paper/c3da4a75d102951d3566893e972b6fe549121a81,A Tutorial on Software Engineering Intelligence: Case Studies on Model-Driven Engineering,"The recent advances in Artificial Intelligence (AI) are dramatically impacting the way we are modelling software systems. A large number of computational intelligence based approaches and tools, combining computational search and machine learning, proved to be successful in automating and semiautomating several activities to support developers. However, the adoption of computational intelligence to address model driven engineering problems is still under-explored. In this tutorial, we will give an overview about computational intelligence, why model-driven engineering is a suitable paradigm for computational intelligence and how computational intelligence could benefit from the recent advances in model-driven engineering. Then, we will focus on some case studies that we published around the adaptation of a variety of computational intelligence techniques for model transformations, models evolution, model changes detection, co-evolution, model/metamodel refactoring, models merging, models quality and metamodels matching. To make the tutorial interactive, the participants will have the opportunity to practice our interactive intelligent MDE tools during the tutorial. Finally, we will conclude the tutorial with different suggestions to enhance the adoption of MDE intelligence research into industry, and the lessons that we learned along this journey and our vision about the future of MDE intelligence. The event will target a wide range of researchers and practitioners from both the model-driven engineering and computational intelligence communities and will reduce the gap between them. The participants will learn about the recent advances in computational intelligence and acquire the required skills to apply them for relevant MDE problems. Model-Driven Engineering, computational intelligence, model transformations, models refactoring, machine learning I. SHORT BIO OF THE PRESENTERS The three organizers of the tutorial have extensive experience and complementary expertise on both MDE and Computational Intelligence. Dr. Kessentini is the PC chair of the foundations track of MODELS2019, he will be the general chair of ASE2021 and he organized with Dr. Deb the search based software engineering symposium (SSBSE2016). Dr. Marouane Kessentini is a recipient of the prestigious 2018 President of Tunisia distinguished research award, the University of Michigan distinguished teaching award, the University of Michigan distinguished digital education award, the Collge of Engineering and Computer Science distinguished research award, 4 best paper awards, and his AI-based software refactoring invention, licensed and deployed by industrial partners, is selected as one of the Top 8 inventions at the University of Michigan for 2018, among over 500 inventions, by the UM Technology Transfer Office. Prior to joining UM in 2013, He received his Ph.D. from the University of Montreal in Canada in 2012. He received several grants from both industry and federal agencies and published over 110 papers in top journals and conferences. Dr. Kessentini has several collaborations with industry on the use of computational search, machine learning and evolutionary algorithms to address software engineering and services computing problems. He is the co-founder of IWoR and NasBASE, General Chair of SSBSE16 and ASE21, and PC chair of MODELS19, GECCO14-15. He served as invited speaker at SSBSE and WCCI, graduated 13 Ph.D. student as a chair and serving as associate editor in 7 journals and PC member of over 100 conferences. He organized tutorials on Search based Software Engineering at SSBSE2018, WCCI2016, ASE2016, etc. He has extensive publications on adopting computational intelligence in MDE [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13] and organized several workshops at MODELS (2013, 2014 and 2015) around this topic. Xin Yao is a Chair Professor of Computer Science at the Southern University of Science and Technology, Shenzhen, China, and a part-time professor at the University of Birmingham, UK. His major research interests include evolutionary computation, ensemble learning and search-based software engineering. His work won the 2001 IEEE Donald G. Fink Prize Paper Award, 2010, 2015 and 2017 IEEE Transactions on Evolutionary Computation Outstanding Paper Awards, 2010 BT Gordon Radley Award for Best Author of Innovation (Finalist), 2011 IEEE Transactions on Neural Networks Outstanding Paper Award, and many other best paper awards. He received the prestigious Royal Society Wolfson Research Merit Award in 2012 and the IEEE CIS Evolutionary Computation Pioneer Award in 2013. He has extensive publications on search based software engineering. Kalyanmoy Deb is Koenig Endowed Chair Professor at Department of Electrical and Computer Engineering in Michigan State University, USA. Prof. Deb’s research interests are in evolutionary optimization and their application in multicriterion optimization, modeling, and machine learning. He was awarded IEEE EC Pioneer award, Infosys Prize, TWAS Prize in Engineering Sciences, CajAstur Mamdani Prize, Distinguished Alumni Award from IIT Kharagpur, EdgeworthPareto award, Bhatnagar Prize in Engineering Sciences, and Bessel Research award from Germany. He is fellow of IEEE and ASME. He has published over 500 research papers with Google Scholar citation of over 122,000 with h-index 112. He is in the editorial board on 18 major international journals.",,2019.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
84e88786fdaa33adc4bf239339373094b26ce38d,https://www.semanticscholar.org/paper/84e88786fdaa33adc4bf239339373094b26ce38d,A deep learning approach for detecting drill bit failures from a small sound dataset,,Scientific reports,2022.0,10.1038/s41598-022-13237-7,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
02a0356138f9929964343933b4bfa84dc5d55070,https://www.semanticscholar.org/paper/02a0356138f9929964343933b4bfa84dc5d55070,A View from Industry: Securing IoT with Azure Sphere,"Every year, 9 billion new devices powered by single-chip computers-MCUs-are deployed; all with little to no cybersecurity and most with no network connectivity. These devices are in your home, in your office, and in every industrial or commercial setting on the planet. Azure Sphere offers to improve MCU computing by bringing cloud connectivity, intelligence, and high security to these devices. The Azure Sphere solution consists of three components: a new class of cross-over MCUs incorporating Microsoft silicon security technology, a new OS built around a custom Linux kernel, and a cloud-based security service that guards every Azure Sphere-based device on the planet for its deployed lifetime. In this talk, I will explain the market scenarios Azure Sphere addresses, dig into the silicon and software architecture that compose the Azure Sphere solution, review some of the project's history, and demo the Azure Sphere experience. Finally, I will look towards the future and discuss research problems out on the horizon. Specifically, I'll explore some scenarios around the convergence mobility, IoT and facial recognition via machine learning where strong security and privacy guarantees are especially important.",HotMobile,2019.0,10.1145/3301293.3302378,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
b0982a1895f2a792c8b0c2dd691533012ef3f1b5,https://www.semanticscholar.org/paper/b0982a1895f2a792c8b0c2dd691533012ef3f1b5,Interactive Learning of Temporal Features for Control,"The ongoing industry revolution is demanding more flexible products, including robots in household environments or medium scale factories. Such robots should be able to adapt to new conditions and environments, and to be programmed with ease. As an example, let us suppose that there are robot manipulators working in an industrial production line that need to perform a new task. If these robots were hard coded, it could take days to adapt them to the new settings, which would stop the production of the factory. Easily programmable robots by non-expert humans would speed up this process considerably. In this regard, we present a framework in which robots are capable to quickly learn new control policies and state representations, by using occasional corrective human feedback. To achieve this, we focus on interactively learning these policies from non-expert humans that act as teachers. We present a Neural Network (NN) architecture, along with an Interactive Imitation Learning (IIL) method, which efficiently learns spatiotemporal features and policies from raw high dimensional observations (raw pixels from an image), for tasks in environments not fully temporally observable. We denominate IIL as a branch of Imitation Learning (IL) where human teachers provide different kinds of feedback to the robots, like new demonstrations triggered by robot queries [1], corrections [2], preferences [3], reinforcements [4], etc. Most IL methods work under the assumption of learning from perfect demonstrations; therefore, they fail when teachers only have partial insights in the task execution. Non-expert teachers could be considered all the users who are neither Machine Learning (ML)/control experts, nor skilled to fully show the desired behavior of the policy. Interactive approaches like COACH [5], and some Interactive Reinforcement Learning (IRL) approaches [4], [6], are intended for non-expert teachers, but are not completely deployable for end-users. Sequential decision-making learning methods (IL, IIL, IRL, etc.) rely on good state representations, which make the shaping of the policy landscape simple, and provide good generalization properties. However, this requirement brings the need of experts on feature engineering to pre-process the states properly, before running the learning algorithms.",,2020.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
4f8c4025b2ce0c0eb837b7389bba31f6f1cae7c7,https://www.semanticscholar.org/paper/4f8c4025b2ce0c0eb837b7389bba31f6f1cae7c7,Implementing Operational Analytics using Big Data Technologies to Detect and Predict Sensor Anomalies,"Operational analytics when combined with Big Data technologies and predictive techniques have been shown to be valuable in detecting mission critical sensor anomalies that might be missed by conventional analytical techniques. Our approach helps analysts and leaders make informed and rapid decisions by analyzing large volumes of complex data in near real-time and presenting it in a manner that facilitates decision making. It provides cost savings by being able to alert and predict when sensor degradations pass a critical threshold and impact mission operations. Operational analytics, which uses Big Data tools and technologies, can process very large data sets containing a variety of data types to uncover hidden patterns, unknown correlations, and other relevant information. When combined with predictive techniques, it provides a mechanism to monitor and visualize these data sets and provide insight into degradations encountered in large sensor systems such as the space surveillance network. In this study, data from a notional sensor is simulated and we use big data technologies, predictive algorithms and operational analytics to process the data and predict sensor degradations. This study uses data products that would commonly be analyzed at a site. This study builds on a big data architecture that has previously been proven valuable in detecting anomalies. This paper outlines our methodology of implementing an operational analytic solution through data discovery, learning and training of data modeling and predictive techniques, and deployment. Through this methodology, we implement a functional architecture focused on exploring available big data sets and determine practical analytic, visualization, and predictive technologies. APPROACH This study developed an operational analytics implementation that uses Big Data technologies and machine learning algorithms to determine and predict sensor anomalies. A previous study [1] showed that Big Data Analytics can uncover anomalies that may be missed through conventional analyses. This study enhances that effort and shows a methodology to implement operational analytics that can be applied toward common solutions for data analysis. Our operational analytics implementation relies on continuous learning from historical data to analyze data in the stream of real-time operations. In the previous study, where data was identified that can be used to uncover anomalies, this implementation extends that approach and now identifies trends and correlations that reveal anomalies that can be missed by traditional analytic techniques with limited datasets. This study adopted a three-step methodology to implementing operational analytics – Discovery, Modeling and Operations as shown in Fig. 1. Copyright © 2016 Advanced Maui Optical and Space Surveillance Technologies Conference (AMOS) – www.amostech.com Fig. 1. Operational Implementation Approach Fig. 1 shows the three steps to implement operational analytics and the continuous feedback between learning and operational deployment. The following sections will elaborate on the methodology employed as applied to a realworld problem of analyzing large datasets such as would be encountered at an operational site.",,2016.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
480a5dcec5ac4d52f4378bde7a3c63510b178ef8,https://www.semanticscholar.org/paper/480a5dcec5ac4d52f4378bde7a3c63510b178ef8,"Upon Human's Unknown, Can AI Help to Address Uncertainty? A Practice on Package Substrate AOI Defect Detection","To recognize defects produced in manufacturing (MFG) process becomes a daily in-line and in-time challenge in today's semiconductor industry. This paper presents an AI-assisted defect inspection method for substrates, which are widely used in packaged IC products. With the help of Automated Optical Inspection (AOI) equipment and the adoption of Machine Learning (ML) and Deep Neural Network (DNN) technologies, manual efforts in substrate defect examination have significantly been reduced, and yet it is still insufficient for the high standard required by automatic defect classification task. One big challenge is that while MFG in-process conditions change, the defect variation also occurs. The defect variances may include some uncertain scenarios, such as compound of multiple known defects, or even out-of-domain (OOD), i.e., unknown defect. A question is then raised: can we build an AI model which has capability to address uncertainty just like human reaction when facing unsure situations? In this paper, we introduce a new ML technology AL-VAE which integrates Active Learning (AL) and Variational Autoencoder (VAE) techniques to enhance our field-deployed AOI defect detection flow. The AL can address the uncertainty and filter out the ambiguity, accelerate training convergence, and minimize the training dataset to reduce labeling efforts. The VAE aims at learning deep latent-variable distribution of image, which can reproduce the similar image's feature multiple times from latent space. Aggregating the reproduced samples can significantly increase the uncertainty information, especially for the unknown defects. To demonstrate the advantage of our ML method, the public MS COCO dataset and our MFG substrate-defect dataset are evaluated. The results indicate that our method selects around 50% samples from original training dataset, and the overall accuracy is still kept at the same level. In addition, the uncertainty information aggregating samples reproduced from VAE shows that our model can recognize the difference between trained classes and untrained scenarios. Our technology enhances the field-deployed AI-assisted defect inspection flow with human-like intelligence which can significantly narrows the gap between AI computing and human understanding.","2021 16th International Microsystems, Packaging, Assembly and Circuits Technology Conference (IMPACT)",2021.0,10.1109/IMPACT53160.2021.9696716,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
8b357b17f707935a706461c1db0d39e8b6801070,https://www.semanticscholar.org/paper/8b357b17f707935a706461c1db0d39e8b6801070,Learning Enhanced Diagnosis of Logic Circuit Failures,"As semiconductor manufacturing progresses to smaller process nodes, it is becoming increasingly difficult to climb the yield learning curve rapidly. The rate of yield learning dictates the growth and success of the semiconductor industry, and must be accelerated to fulfill competitive time-to-market, time-to-money and time-to-volume requirements. Software-based diagnosis plays a crucial role in yield learning. Diagnosis comprehends the test response of a failing circuit to determine the location, and sometimes, in addition, characterize the nature of a defect affecting the failing circuit. Besides identifying likely failure mechanisms and increasing the quality of chip testing, the feedback provided by diagnosis is used to select chips for physical failure analysis (PFA). PFA aims to visually examine a chip to characterize a defect, prevent similar defects in the future and, consequently, improve the design and manufacturing of a chip.However, PFA is often destructive, time- and cost-intensive, and not always successful. Diagnosis, on the other hand, is non-invasive and time- and cost-effective; moreover, it assists PFA and guides yield learning. The advantages of diagnosis, coupled with the diminishing performance of PFA with advancing technology, make it an encouraging facilitator for rapid yield learning.Therefore, the objective of diagnosis must be logic-level defect characterization to minimize (and ideally eliminate) the need for PFA, and accelerate yield learning. Logic characterization of a defect includes the derivation of its physical location and precise logic behavior. In this dissertation, a comprehensive diagnosis methodology is developed to actualize the aforementioned objective.The developed methodology comprises of three methods. LearnX/MD-LearnX is a physically-aware method that employs (a) the X-fault model to avert the elimination of a correct defect candidate and (b) machine learning to build a candidate-ranking model that learns the hidden correlations between the tester response and the defect candidates to pinpoint the correct candidate. PADLOC, which stands for Physically-Aware Defect LOcalization and Characterization, improves the physical location of a back-end defect (i.e., a defect that affects one or more interconnects and resides outside a standard cell) returned by LearnX/MD-LearnX by partitioning the defective net into physical subnets and identifying the subnets that influence defect excitation. In addition, PADLOC deduces the precise impact of a defect on the circuit functionality by examining its surrounding circuitry. NOIDA, which stands for NOise-resistant Intra-cell Diagnosis Approach, pinpoints the location of a defect within a failing standard cell implicated by LearnX/MD-LearnX. In contrast to prior work that typically constructs/employs a fault dictionary, NOIDA ascertains the location as well as the behavior of a front-end defect (i.e., a defect that resides inside a standard cell) by monitoring the logical activity of its intra-cell neighborhood. Additionally, NOIDA is resistant to circuit-level noise that may originate from potentially inaccurate transistor-level simulation. Results from numerous experiments reveal that our diagnosis methodology outperforms state-of-the-art commercial diagnosis. LearnX/MD-LearnX reports fewer defect candidates than commercial diagnosis for 69.4% silicon fail logs without losing accuracy. PADLOC implicates a smaller physical area for a defect for 47.2% silicon fail logs and attains at most 44X improvement. NOIDA reports an ideal diagnosis for 38.0% more front-end defects, when compared to leading-edge commercial diagnosis. In the presence of noise, NOIDA achieves an ideal diagnosis 7.6X more often.In summary, this dissertation endeavors to characterize a defect residing in a logic chip in terms of its precise physical location and logic behavior, which, consequently, most likely, enables rapid yield learning. The deployment of machine learning to pinpoint the correct candidate in LearnX/MD-LearnX, and the investigation of the neighborhood of a defect to determine its exact physical location and logic behavior in PADLOC and NOIDA are the novel components of this dissertation, and the reasons for its superiority over the state-of-the-art.",,2020.0,10.1184/R1/11962164.V1,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
9055642116f7b435c51b5f3e08a0752812c9acb6,https://www.semanticscholar.org/paper/9055642116f7b435c51b5f3e08a0752812c9acb6,Garbage Classification and Recognition Based on SqueezeNet,"In this study, an intelligent garbage classification and recognition system was deployed on the industry integrated computer with the I3-7100U processor and 2G memory. Considering the unit prediction time and prediction accuracy, SqueezeNet was selected as the classification network training model among ResNet, InceptionV3, and SqueezeNet. The pretraining SqueezeNet network model on the ImageNet-1000 dataset was used for transfer learning, and the model predication accuracy was improved by using image enhancement and Adam optimizer. The comparison between the comprehensive test set and the real garbage image showed that the model predication accuracy reached 87.7% after training, and the prediction time in the industrial integrated machine was less than 2 seconds, which met the needs of practical applications.",2020 3rd World Conference on Mechanical Engineering and Intelligent Manufacturing (WCMEIM),2020.0,10.1109/wcmeim52463.2020.00032,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
accddd1e2e3c72aa1d96abb78113c49f17e39bc4,https://www.semanticscholar.org/paper/accddd1e2e3c72aa1d96abb78113c49f17e39bc4,WHITE PAPER ACHIEVE COMPLETE AUTOMATION WITH ARTIFICIAL INTELLIGENCE AND MACHINE LEARNING,"As agile models become more prominent in software development, testing teams must shift from slow manual testing to automated validation models that accelerate the time to market. Currently, automation test suites are valid only for some releases, placing greater pressure on testing teams to revamp test suites, so they can keep pace with frequent change requests. To address these challenges, artificial intelligence and machine learning (AI/ML) are emerging as viable alternatives to traditional automation test suites. This paper examines the existing challenges of traditional testing automation. It also discusses five use-cases and solutions to explain how AI/ML can resolve these challenges while providing complete and intelligent automation with little or no human intervention, enabling testing teams to become truly agile. Industry reports reveal that many enterprise initiatives aiming to completely automate quality assurance (QA) fail due to various reasons resulting in low motivation to adopt automation. It is, in fact, the challenges involved in automating QA that have prevented its evolution into a complete automation model. Despite the challenges, automation continues to be a popular initiative in today’s digital world. Testing communities agree that a majority of validation processes are repetitive. While traditional automation typically checks whether things work as they are supposed to, the advent of new technologies like artificial intelligence and machine learning (AI/ML) can support the evolution of QA into a completely automated model that requires minimal or no human intervention. Pain points of Complete Automation Introduction Let us look at the most pertinent problems that lead to low automation statistics: • Frequent requirement changes – While most applications are fluid and revised constantly, the corresponding automation test suite is not. Keeping up with changing requirements manually impedes complete automation. Moreover, maintaining automated test suites becomes increasingly complicated over time, particularly if there are frequent changes in the application under test (AUT) • Mere scripting is not automation – Testing teams must evolve beyond traditional test automation that involves frequent manual script-writing. • Inability to utilize reusable assets – It is possible to identify reusable components only after a few iterations of test release cycles. However, modularizing these in a manner that can be reused everywhere is a grueling task. • Talent scarcity – Finding software development engineers in test (SDETs) with the right mix of technical skills and QA mindset is a significant challenge QA teams today are looking for alternatives to the current slow and manual process of creating test scripts using existing methodologies. It is evident that intelligent automation (automation leveraging AI/ML) is the need of the hour. External Document © 2020 Infosys Limited External Document © 2020 Infosys Limited A. Automation test suite creation How can enterprises achieve complete automation in testing? Use cases and solutions for intelligent test automation The key to achieving complete automation lies in using AI/ML as an automation lever instead of relegating it to scripting. Optimizing manual test cases using AI/ML is a good start. Helping the application selflearn and identify test suites with reusable Use case 1: Optimizing a manual test suite Testing teams typically have a large set of manual test cases for regression testing, which are written by many people over a period of time. Consequently, this leads to overlapping cases. This increases the burden on automation experts when creating the automation test suite. Moreover, as the test case suite grows larger, it becomes difficult to find unique test cases, leading to increased execution effort and cost. Solution 1: Use a clustering approach to reduce effort and duplication A clustering approach can be used to group similar manual test cases. This helps teams easily recognize identical test cases, thereby reducing the size of the regression suite without the risk of missed coverage. During automation, only the most optimized test cases are considered, with significant effort reduction and eliminating duplicates. Use case 2: Converting manual test cases into automated test scripts Test cases are recorded or written manually in different formats based on the software test lifecycle (STLC) model, which can be either agile or waterfall. Sometimes, testers can record audio test cases instead of typing those out. They also use browserbased recorders to capture screen actions while testing. Solution 2: Use natural language processing (NLP) In the above use case, the execution steps and scenarios are clearly defined, assets can be more advanced utilities for automated test suite creation. Leveraging AI in test suite automation falls into two main categories – ‘automation test suite creation using various inputs’ after which the tester interprets the test cases, designs an automation framework and writes automation scripts. This entire process consumes an enormous amount of time and effort. With natural language processing (NLP) and pattern identification, manual test cases can be transformed into ready-to-execute automation scripts and, furthermore, reusable business process components can be easily identified. This occurs in three simple steps: • Read – Using NLP to convert text into the automation suite. • Review – Reviewing the automation suite generated. • Reuse – Using partially supervised ML techniques and pattern discovery algorithms to identify and modularize reusable components that can be plugged in anywhere, anytime and for any relevant scenario. and ‘automation test suite repository maintenance’. The following section discusses various use cases for intelligent automation solutions under these two categories which address the challenges of end-to-end automation. All these steps can be implemented in a tool-agnostic manner until the penultimate stage. Testers can review the steps and add data and verification points that are learnt by the system. Eventually, these steps and verification points are used to generate automation scripts for the tool chosen by the test engineer (Selenium, UFT or Protractor) only at the final step. In this use case, AI/ML along with a toolagnostic framework helps automatically identify tool-agnostic automation steps and reusable business process components (BPCs). The automation test suite thus created is well-structured with easy maintainability, reusability and traceability for all components. The solution slashes the planning and scripting effort when compared to traditional mechanisms. External Document © 2020 Infosys Limited External Document © 2020 Infosys Limited Use case 3: Achieving intelligent DevOps Software projects following the DevOps model usually have a continuous integration (CI) pipeline. This is often enabled with auto-deploy options, test data and API logs with their respective requests-response data or application logs from the DevOps environment. While unit tests are available by default, building integration test cases requires additional effort. Solution 3: Use AI/ML in DevOps automation By leveraging AI/ML, DevOps systems gain analytics capabilities (becoming ‘intelligent DevOps’) in addition to transforming manual cases to automated scripts. Fig 1: Achieving complete automation on a DevOps model with AI/ML AI Components Optimized Test Cases Automated Unit Tests Virtual Service / Component Test Cases Functional Test Scenarios SMART DIAGNOSTIC ANALYTICS Recovery Techniques Test Reports / Defect Logs Application Logs Test Reports / Defect Logs Request -Response Data / Test Data Develop Test Deploy",,2020.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
4a215520c5c57ba29e8a55a71e1691c0521a5ab5,https://www.semanticscholar.org/paper/4a215520c5c57ba29e8a55a71e1691c0521a5ab5,Designing and deploying insurance recommender systems using machine learning,"Recommender systems have become extremely important to various types of industries where customer interaction and feedback is paramount to the success of the business. For companies that face changes that arise with ever‐growing markets, providing product recommendations to new and existing customers is a challenge. Our goal is to give our customers personalized recommendations based on what other similar people with similar portfolios have, in order to make sure they are adequately covered for their needs. Our system uses customer characteristics in addition to customer portfolio data. Since the number of possible recommendable products is relatively small, compared to other recommender domains, and missing data is relatively frequent, we chose to use Bayesian Networks for modeling our systems. We also present a deep‐learning‐based approach to provide recommendations to prospects (potential customers) where only external marketing data is available at the time of prediction.",Wiley Interdiscip. Rev. Data Min. Knowl. Discov.,2020.0,10.1002/widm.1363,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
d61e36f1c572fabecee46340bc348a57db8e6777,https://www.semanticscholar.org/paper/d61e36f1c572fabecee46340bc348a57db8e6777,Discussion on the Flexible Manufacturing and Operation Strategy Model for a Machine Tool Component Company in Taiwan,"Recently, many manufacturing industries have been facing challenges such as rising material costs, small-volume and large-variety products, shortened production cycles, increased labor costs and longer after-sales service times, which is a very tough challenge for most small and medium-sized component manufacturing suppliers. In addition to the current hot topics in the manufacturing industry - Smart Manufacturing (Industry 4.0) and lean production management, if small and medium-sized enterprises are not able to adjust the pace of manufacturing timely and find a suitable production model, they will soon be overwhelmed by the torrent of the era of speed and accuracy. In the face of the dramatic changes in the industry structure, the company can deploy the global expansion of overseas customers in advance, and adjust to apply and implement a flexible manufacturing model system through the introduction of the Industrial Internet of Things and flexible manufacturing production management. In order to meet the market needs, the manufacturing industry is gradually oriented towards customized production and the rapid development of new products. To meet such stringent requirements, flexible manufacturing becomes one of the necessary ways for enterprises to consider their development models. Therefore, the efficiency and reliability of work can be improved through the Industrial Internet of Things that facilitates machine-to-machine communication, cloud-based big data and learning and imitations of smart robots. This study is an in-depth study of a company that is currently in the process of digital transformation, collecting relevant information and reviewing the analysis to find a suitable smart manufacturing solution for the company and to explore the impact of the COVID-19 pandemic on the strategic development of the company. The findings can provide a significant reference for homotypic companies in the development of their business strategies.",International Business Research,2021.0,10.5539/ibr.v14n10p1,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
cdb66a2f6de4f002b4a871ade24e6e61d321e566,https://www.semanticscholar.org/paper/cdb66a2f6de4f002b4a871ade24e6e61d321e566,Detecting Drill Failure in the Small Short-sound Drill Dataset,"Monitoring the conditions of machines is vital in the manufacturing industry. Early detection of faulty components in machines for stopping and repairing the failed components can minimize the downtime of the machine. This article presents an approach to detect the failure occurring in drill machines based on drill sounds from Valmet AB. The drill dataset includes three classes: anomalous sounds, normal sounds, and irrelevant sounds, which are also labeled as “Broken”, “Normal”, and “Other”, respectively. Detecting drill failure effectively remains a challenge due to the following reasons. The waveform of drill sound is complex and short for detection. Additionally, in realistic soundscapes, there are sounds and noise in the context at the same time. Moreover, the balanced dataset is small to apply state-of-the-art deep learning techniques. To overcome these aforementioned difficulties, we augmented sounds to increase the number of sounds in the dataset. We then proposed a convolutional neural network (CNN) combined with a long short-term memory (LSTM) to extract features from log-Mel spectrograms and learn global highlevel feature representation for the classification of three classes. A leaky rectified linear unit (Leaky ReLU) was utilized as the activation function for our proposed CNN instead of the rectified linear unit (ReLU). Moreover, we deployed an attention mechanism at the frame level after the LSTM layer to learn long-term global feature representations. As a result, the proposed method reached an overall accuracy of 92.35% for the drill failure detection system.",ArXiv,2021.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
066894fb19cbee73495b0eded1f33fa3c61c1d96,https://www.semanticscholar.org/paper/066894fb19cbee73495b0eded1f33fa3c61c1d96,Mobile sensing and social computing,"With the rapid development of social networks and social environments, mobile sensing has increasingly emerged as one of the most important technologies to develop social computing solutions. Social computing is a general term for an area of computer science that is concerned with the intersection of social behavior and computational systems, providing a programmable combination of contributions from both humans and computers. A key factor for social computing is how social information is collected from the ubiquitous environments and can be widely used to provide social services in mobile environments. Mobile sensing is increasingly becoming part of everyday life, as smartphones are becoming the central personal computational device in people’s lives. Mobile sensing presents several challenges related to wireless sensor networks, machine learning, human–computer interaction, and mobile systems. Sensor-equipped mobile phones can be combined with wireless sensor networks installed in the environment to develop social machines in many sectors of our economy, including business, healthcare, social networks, environmental monitoring, and transportation. Some research efforts on social computing and mobile sensing have been in progress, including mobile sensing algorithms, applications and systems, and methods and techniques to develop virtual societies. This IJDSN Special Issue is an opportunity to bring multi-disciplinary experts, academics, and practitioners together to exchange their experience in the development and deployment of mobile sensing and social computing systems. This Special Issue brings together researchers and developers from industry and academy to report on the latest scientific and technical advances on the application of mobile sensing and social computing and to showcase the latest systems using these technologies. Filipe et al. compile and compare technologies and protocols published in the most recent researches, seeking Wireless Body Area Network (WBAN) issues for medical monitoring purposes to select the most useful solutions for this area of networking. The most important features under consideration in our analysis include wireless communication protocols, frequency bands, data bandwidth, transmission distance, encryption, authentication methods, power consumption, and mobility. WBAN supporting healthcare applications are in early development stage, but offer valuable contributions at monitoring, diagnostic, or therapeutic levels. They cover real-time medical information gathering obtained from different sensors with secure data communication and low power consumption. Filipe et al. demonstrate that some characteristics of surveyed protocols are very useful to medical appliances and patients in a WBAN domain. Marcelino et al. present a solution to overcome barriers between elderlies and their information and communication technology (ICT) usage in order to potentiate all the benefits provided from mobile sensing and social computing. They present a survey on guidelines, standards, and advices regarding usability and accessibility issues when developing solutions for elderly people made having in mind that senior population have singular requirements due to age-related changes and also frequently technological illiteracy. The authors have identified and applied the most important guidelines to their own solution. A prototype was made using responsive design in order to be adaptable to any type of devices. Zong and Wen propose a new approach to calculate the smartphone orientation by detecting the vehicle starting action and then establish the coordinate system relationship between vehicle and smartphone. Furthermore, they trained the classified model offline to match the acceleration characteristics with traveling speed. In the model training process we compared different classification algorithms. Due to enclosed areas and intensive energy consumption, GPS or WiFi sometime are invalid. In this paper, Zong and Wen propose a new approach to estimate the traveling speed after analyzing the acceleration characteristics in time domain and frequency domain. Shuyun et al. propose a method used to calculate the link importance degree index, and the index is used to evaluate the link’s information. Besides, a multiobjective optimization model is proposed, its aim is to minimize the total cruise time under detecting as many important links as possible and minimize the information value undetected by unmanned aerial vehicles (UAVs), and the fuzzy operator is introduced to the constraint conditions. Finally, a case study is used to",Int. J. Distributed Sens. Networks,2016.0,10.1177/1550147716665512,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
d0d453eb2a9b94ddd040b772c0af151dadeb593d,https://www.semanticscholar.org/paper/d0d453eb2a9b94ddd040b772c0af151dadeb593d,Research on Classification of Intrusion Detection in Internet of Things Network Layer Based on Machine Learning,"The emergence of the Internet of Things (IoT) is not only a global revolution in the information industry, but also brought tremendous changes to our lives. With the development of the technology and means of the IoT, information security issues have gradually emerged, and intrusion attacks have become one of the main problems of the IoT network security. The network layer of the IoT is the key connecting the platform and sensors or controllers of the IoT, and it is also the most standardized, the strongest and the most mature part of the whole physical network architecture. Its large-scale development has led to the network layer's security issues will receive more attention and face more challenges. This paper proposes an intrusion detection algorithm deployed on the network layer of the IoT, which uses the BPSO algorithm to extract features from the NSL-KDD dataset, and applies support vector machines (SVM) as the core model of the algorithm to detect and identify abnormal data, especially DoS attacks. Experimental results show that the model's detection rate of abnormal data and DoS attacks are significantly improved.",2021 IEEE International Conference on Intelligence and Safety for Robotics (ISR),2021.0,10.1109/ISR50024.2021.9419529,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
f10cc958252a7c18444ee2634f84b48623c5725a,https://www.semanticscholar.org/paper/f10cc958252a7c18444ee2634f84b48623c5725a,"Machine learning 2018 and Big Data 2018- Digital transformation and the convergence of new emerging digital technologies - Samir El Masri - Digitalization. Cloud, UAE","Digital transformation is a journey that stems from strong beliefs in the digital economy by senior management supported by a digital transformation strategy. The strategy is much more difficult to deploy than develop and it may only be achieved when the transformation is led by CEOs reinforced by mature capabilities. Unfortunately, most digital transformation initiatives have failed in the past and many more will fail in the future. Advanced change is the method of using advanced innovations to make modern — or adjust existing — trade forms, culture, and client encounters to meet changing business and showcase prerequisites. Additionally, advanced changes have reshaped how companies approach client benefits. Making call centers and in-store benefit work areas run more proficiently with advanced innovation is of course awesome. But genuine change comes after you see at all accessible advances and consider how adjusting your trade to them can donate clients distant better;a much better;a higher;a stronger;an improved"">a higher involvement. Social media wasn’t designed to require the put of call centers, but it’s ended up an extra channel (and opportunity) to offer way better client benefit. Adjusting your benefit offerings to grasp social media is another great case of a computerized change.The ancient show was to hold up for clients to come to discover you, whether in individual or by calling an 800 number. But the rise of social media has changed benefit much like it’s changed promoting, promoting, and indeed deals and client benefit. Dynamic companies grasp social media as a chance to expand their benefits offerings by assembly clients on their stages of choice. 
 
Machine learning includes computers finding how they can perform errands without being expressly modified to do so. For straightforward errands relegated to computers, it is conceivable to program calculations telling the machine how to execute all steps required to fathom the issue at hand; on the computer's portion, no learning is required. For more progressed assignments, it can be challenging for a human to physically make the required calculations. In hone, it can turn out to be more successful to assist the machine develop its claim calculation, instead of have human software engineers indicate each required step. The discipline of machine learning employs different approaches to assist computers learn to achieve errands where no completely palatable calculation is accessible. In cases where tremendous numbers of potential answers exist, one approach is to name a few of the right answers as substantial. This will at that point be utilized as preparing information for the computer to progress the algorithm(s) it employments to decide rectify answers. 
This reimagining of commerce within the advanced age is computerized transformation.A key component of computerized change is understanding the potential of your innovation. Once more, that doesn’t cruel inquiring “How much speedier can we do things the same way?” It implies inquiring “What is our innovation truly able of, and how can we adjust our trade and forms to create the foremost of our technology investments?” Before Netflix, individuals chose motion pictures to lease by attending to stores and combing through racks of tapes and circles in look of something that looked great. Presently, libraries of computerized substance are served up on individual gadgets, total with proposals and audits based on client inclinations.It rises above conventional parts like deals, showcasing, and client benefit. Instep, computerized change starts and closes with how you think almost, and lock in with, clients. As we move from paper to spreadsheets to keen applications for overseeing our trade, we have the chance to reimagine how we do commerce — how we lock in our clients — with computerized innovation on our side. For little businesses fair getting begun, there’s no ought to set up your trade forms and change them afterward. You'll be able to future-proof your association from the word go. Building a 21st-century commerce on stickies and manually written records fair isn’t feasible. Considering, arranging, and building carefully sets you up to be spry, adaptable, and prepared to develop. Not so long prior, businesses kept records on paper. Whether transcribed in records or written into reports, commerce information was analog. On the off chance that you needed to assemble or share data, you managed with physical reports — papers and folios, xeroxes, and faxes. Then computers went standard, and most businesses begun changing over all of those ink-on-paper records to advanced computer records. Typically called digitisation: the method of changing over data from analog to computerized.The method of using digitized data to form set up ways of working less difficult and more proficient is called digitalisation. Note the word established in that definition: Digitalisation isn’t approximately changing how you are doing trade, or making unused sorts of businesses. It’s around keeping on keeping on, but quicker and way better presently that your information is right away open and not caught in a record cabinet someplace in a dusty archive. Think of client benefit, whether in retail, field ops, or a call center. Digitalization changed benefit until the end of time by making client records effortlessly and quickly retrievable through the computer. The essential technique of client benefit didn’t alter, but the method of handling an inquiry, looking up the important information, and advertising a determination got to be much more productive when looking paper records was supplanted by entering many keystrokes on a computer screen or versatile gadget. Digital change is changing the way trade gets done and, in a few cases, making completely unused classes of businesses. With the computerized change, companies are taking a step back and returning to everything they do, from inside frameworks to clients intuitive both online and in individual. They’re inquiring huge questions like “Can we alter our forms in a way that will empower way better decision-making, game-changing efficiencies, or distant better;a much better; a higher;a stronger;an improved"">an improved client encounter with more personalization?” Now we’re immovably dug in within the computerized age, and businesses of all sorts are making intelligent, viable, and troublesome ways of leveraging innovation. Netflix could be an incredible case. It begun out as a mail arrange benefit and disturbed the brick-and-mortar video rental commerce. At that point, computerized advancements made wide-scale gushing video conceivable. Nowadays, Netflix takes on conventional broadcast and cable tv systems and generation studios all at once by advertising a developing library of on-demand 
 
These failures have been mainly due to organizations undertaking digital change instead of digital transformation in addition to the lack of capabilities and non-readiness of the company to manage this transformation. New digital emerging technologies remain the backbone and the enabler of any digital transformation activities. The digitization of operations, workforce, marketing, and new digital business models will be realized by the convergence of all new emerging digital technologies through new products/services, price, customer experience, and platform values. In this talk, data science, machine learning, analytics, big data, IOT and their interrelationships will be demonstrated. Examples of how digital initiatives could help the industry by improving efficiency, avoiding trips, reducing unplanned downtime and transforming from time-based to condition-based maintenance will also be illustrated.",,2018.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
f8b46a31d5f6c05ede392808bce0763e3b4dea59,https://www.semanticscholar.org/paper/f8b46a31d5f6c05ede392808bce0763e3b4dea59,"Data-Driven Construction Safety Information Sharing System Based on Linked Data, Ontologies, and Knowledge Graph Technologies","Accident, injury, and fatality rates remain disproportionately high in the construction industry. Information from past mishaps provides an opportunity to acquire insights, gather lessons learned, and systematically improve safety outcomes. Advances in data science and industry 4.0 present new unprecedented opportunities for the industry to leverage, share, and reuse safety information more efficiently. However, potential benefits of information sharing are missed due to accident data being inconsistently formatted, non-machine-readable, and inaccessible. Hence, learning opportunities and insights cannot be captured and disseminated to proactively prevent accidents. To address these issues, a novel information sharing system is proposed utilizing linked data, ontologies, and knowledge graph technologies. An ontological approach is developed to semantically model safety information and formalize knowledge pertaining to accident cases. A multi-algorithmic approach is developed for automatically processing and converting accident case data to a resource description framework (RDF), and the SPARQL protocol is deployed to enable query functionalities. Trials and test scenarios utilizing a dataset of 200 real accident cases confirm the effectiveness and efficiency of the system in improving information access, retrieval, and reusability. The proposed development facilitates a new “open” information sharing paradigm with major implications for industry 4.0 and data-driven applications in construction safety management.",International journal of environmental research and public health,2022.0,10.3390/ijerph19020794,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
33000cfad01184a29c7ff6fab57299c6f4d4fd9f,https://www.semanticscholar.org/paper/33000cfad01184a29c7ff6fab57299c6f4d4fd9f,Intelligent drying systems,"The era of Artificial Intelligence (AI) has become a reality. Computer technologies have reached a level where they are not only for information processing, but also as substitute for human being on routine tasks, which can be described as a set of programable operations. In such cases, computer is clearly superior to humans, because it is not affected by emotions and human errors amongst its multifarious advantages. Moreover, computers can do some tasks beyond the scope of human intelligence, e.g., goal-oriented control and optimization. Drying with its inherent transient nature, uncertainty and still poorly understood coupling of transport processes and materials science is an ideal playground for AI. High variability in thermo-physical properties of materials and relatively low energy efficiency of drying make this game even more exciting. Currently, each drying application requires much preliminary research due to unknown process-product interactions, very specific to each product and drying technology. We believe that future development of drying technologies will focus on the need in embedding AI into drying systems. So far, the progress is still very limited. Most published research deals with “black-box” computing, such as artificial neural network (ANN), fuzzy logic (FL) or evolutionary algorithms (EA). These black-box approaches clearly demonstrate ability to manage hidden uncertainty and non-linearity of process/product parameters, but they do not contribute to the improvement of our knowledge about drying process and optimal control strategies. Recent advances in computer vision and machine learning have opened up new avenues for future development of drying systems. Unfortunately, their applications in drying technologies are still very limited. One possible reason is that they still have not demonstrated their unique advantages in simplifying research and development of new knowledge. We also should realize that this shift in paradigm would require critical revision of the philosophy of drying R&D. Drying community needs to become aware of advances in the AI space and applications to realworld systems. So far, control strategies in drying are limited to the control of process parameters. Our knowledge of product quality transformation and critical control points in the process of drying are still not included in dryer control strategy. AI in the form of computer vision and soft sensors will fill this gap, delivering real-time information about the product quality attributes. It is similar to intelligence of experienced cook, who controls the process of food preparation by observing visible changes in food. Unfortunately, our drying technologies are still an art of blind processing. Hence, the concept of intelligent observer is the first step in the development of intelligent drying systems. Sharing of digital data through IoT could provide the basis for the development of unified methodology. The next goal is the development of digital twins of commonly used drying systems. Machine learning could be used as an excellent tool to validate drying models. Eventually it will make possible model predictive control (MPC) and multiobjective optimization (MOO). The first step in this direction is already made. We believe that in the coming decades AI will find important industrial applications in various drying systems, allowing cost-effective production of consistently high quality products. This will require both academics and industry to keep up with the rapidly developing field of AI and technology-push innovations in industrial drying. References below provide a valuable introduction and exposure to the status and future of AI in drying technologies. We",Drying Technology,2020.0,10.1080/07373937.2019.1650452,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
023dc669070168a27ea6c422a7e61b7fe673f4de,https://www.semanticscholar.org/paper/023dc669070168a27ea6c422a7e61b7fe673f4de,A DID for Everything,"The decentralized identifer (DID) is a new and open standard type of globally unique identifer that ofers a model for lifetime-scope portable digital identity that does not depend on any centralized authority and that can never be taken away by third-parties [14]. DIDs are supported by the W3C community [14] and the Decentralized Identity Foundation (DIF) [16]. They are the ""atomic units"" of a new layer of decentralized identity infrastructure. However, DIDs can be extended from identifers for people to any entity, thus identifying everything. We can use DIDs to help us identify and manage objects, machines, or agents through their digital twins; we can expand them to locations, to events, and even to pure data objects, which we refer to as decentralized autonomic data (DAD) items [1][3]. The paper will present novel use-cases for DIDs and DADs and propose a new cryptographic data structure that 01/17/19 A DID for Everything 1.0 1 is a self-contained blockchain of DADs. This enables the verifcation of the provenance of a given data fow. It builds on a prior paper [1] and an associated reading [2]. DIDs are only the base layer of decentralized identity infrastructure. The next higher layer (where most of the value is unlocked) is verifable claims. This is the technical term for a digitally signed electronic data structure that conforms to the interoperability standards being developed by the W3C Verifable Credentials Working Group [15]. When a DID and hence DADs of the resultant data are extended to machines and autonomic data, the provenance chain of the data fow can provide the basis for verifable claims and attestations about the data fow as well as the basis for a reputation. WHY THIS MATTERS Today, the Internet is probably best described as a network comprised of all interconnected entities, traditionally referring to human users and computers. When we add connected entities and devices in the so-called Internet of Things (IoT), the number of addressable elements is in the tens of billions, with an estimate of 75 bn connected IoT devices in 2025 [4]. Software services, such as algorithms and bots, further extend this universe of identifable entities. The resulting combinatorics of possible connections between any given set of entities is an impossibly large number. Yet in today's user journeys or business environments, agents (whether human, machine, or software) increasingly need to communicate, access or transact with a diverse group of these interconnected objects to achieve their goals in both the digital and physical worlds. This requires a straightforward and ubiquitous method to address, verify, and connect these elements together. Defnition of Entity: Something that has a distinct and independent existence either in the real or the digital world. Examples of an entity are: • Living Organism • Physical Object • Locations or Events • Machines and Devices in the Internet of Things (IoT) • Digital Asset, Data Set, or Agent Human or object identities are stored in multiple centralised or federated systems such as government, ERP, IoT, or manufacturing systems. From the standpoint of cryptographic trust verifcation, each of these centralised authorities serves as its own root of trust. An entity trailing along a value chain is interacting with multiple systems. Consequently, a new actor in any given value chain has no method to independently verify credentials of a human or attributes of either a physical object or data item (provenance, audit trail). This results in the existence of complex validation, quality inspection, and paper trail processes, and enormous hidden trust overhead costs are added to all value chains and services. 01/17/19 A DID for Everything 1.0 2 To be a truly global solution, easy to use and still safe from hacking and sovereign interference, such a scheme must include: • preservation of privacy • security from tampering • reliable trust verifcation • assurance of risk • independence from any vendor-defned naming API • one-to-one mappable onto each entity. Therefore, a universal addressing, trust-verifcation system and associated interoperable protocol must be utilised, empowering every form of entity. Why it Matters for People Today when entities want their identities to be confrmed they transfer information such as a birth certifcate, physical address, or social security number to multiple third parties, who start to validate the same data in diferent contexts for KYC and authentication processes. The parties to which they sent that information retains it, meaning the data is out there in silos, creating risks in terms of data loss, privacy breaches, and use of inconsistent data and forcing companies that might not want to be in that position to store that information. It also enables businesses to harvest people's personal data for commercial purposes, which does not necessarily refect the intentions of the individual people. This situation results in big problems for humans such as broken health care records. Patients will need a universally addressable healthcare record system that is controlled by the patient itself, that consistently stores all relevant verifed health care data, and that is able to share this data with doctors that need to connect with it. To enable the doctors or algorithms, they need a data-fow provenance to verify the integrity, quality, or reputation of a healthcare record to decide on treatments and give the patient confdence about the proposed treatments. Why it Matters for Businesses Defnition of a Digital Twin: A digital twin is a digital representation of either a real-world or digital entity. A digital twin exists over the life-cycle of an entity from planning, manufacturing, testing, birth, and operations to decommissioning and reuse. The more past and present data are related and analysed, the more knowledge can be deployed to drive signifcant improvements on an individual entity or system level. It is estimated that by 2022 the IoT powered by digital twins will save consumers and businesses worth $1 trillion a year in asset maintenance [5]. The notion of digital twinning for objects, machines, and agents is becoming relevant to an increasing number of human services and Industry 4.0 use cases. This is the result of the growth in digital services, connections, and 01/17/19 A DID for Everything 1.0 3 data streams from the Internet of Things (IoT) devices that increasingly drive integration with machine learning algorithms, resulting in graph-type data chains for processing the IoT data streams. Today, digital twins are captured in siloed, proprietary IoT solutions by individual corporates that do not own the physical object over the life-cycle and even do not interact with parties using the object further down a value chain. Decentralized solutions are liberating the digital twins from silos and establishing more valuable and interoperable verifable attributes about entities and the data chains they connect with. Why it Matters for Objects, Machines and Agents There is no widely adopted authentication or verifcation systems in place to provide the equivalent of KYC (know-your-customer) for non-human entities, that is: KYA (know-your-agent), KYB (know-your-bot), KYM (know-your-machine), or KYO (know-your-object). In a world when objects and machines are connected with datastreams and intelligent agents that perform transaction on behalf of the entity, the number of agent-to-agent transactions will outgrow the number of human transactions by many orders. An agent transacting with another party can independently verify the identifer of the this party, its attributes, and the provenance of the data sets that are involved in the transaction. Digital twins of 3D-printed objects for safety critical parts such as a turbine of an airplane provide an important example. For these parts, it is important to have an precise audit trail about the 3D printing process to prove that the object was manufactured in accordance to stringent specifcations. The digital twin stores design, manufacturing, post processing, and quality-assurance data about the 3D-printed object. These data are coming from multiple systems resulting in a variety of data chains. With DIDs and DADs the integrity of the data chains can be verifed. The verifcation of the datachains and the underlying data results in important proofs about the provenance of 3D printed object. Why it Matters for the World The diverse application of decentralized identifers (DIDs) will have substantial infuences in broader applications on a global scale. The seamless provenance of physical objects or data items through any value chain has major implications on the risk and value properties of the processed data. Within any dynamic process, participating entities have substantial interest in the authenticity and trustworthiness in any individual step. Data that is accumulated with an unforgeable audit trail that references decentralized identifying information (Person, Device or any other Entity) for any transformation step holds greater value then it would have without such properties. Managing the sustainability of the commons requires mechanisms to value natural capital and to account for the externalities that arise from human activities. This should attribute extractions from and contributions to the commons by organisations, organisms, machines and information. We need to identify these entities and must 01/17/19 A DID for Everything 1.0 4 identify both positive and negative impacts these entities are having on the commons. Knowing what these impacts are enables us to count what matters and to put a value on what counts. The promise of a overarching prevalence through the broad use of DIDs also provides the key component for achieving the vision of a circular economy: a regenerative system in which resource input and waste, emission, and energy leakage are minimized by slowing, closing, and narrowing energy an",,2019.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
47e045aec019d02f03cab06980c9d09176c6b0f5,https://www.semanticscholar.org/paper/47e045aec019d02f03cab06980c9d09176c6b0f5,Internet of Nanothings (IoNT) and Machine Learning (ML) – Innovations in drug discovery and Healthcare system,"The IoNT offers a medium to connect various nanodevices with the help of high speed networks. Using this technology nanodevices can be deployed along with other advanced technologies such as cloud computing, big data and ML. Many tedious tasks can be taken over by linked inanimate objects and better availability of information using IoNT-ML. This technology has shown great promise in improving efficiencies across numerous pharmaceutical and healthcare industries with high quality and vast datasets. It has the potential to foster innovation while simultaneously improving productivity and delivering better outcomes across the value chain. IoNT-ML can significantly improve the value proportion of pharmaceutical companies by driving innovation and the creation of new business models. This technology can be implemented in almost every aspect of the pharmaceutical industry, right from the drug discovery and development to manufacturing and marketing. This review discusses the principle of ML and its various applications in healthcare sector.",International Journal of Frontiers in Science and Technology Research,2022.0,10.53294/ijfstr.2022.2.1.0021,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
c59dc28f30409531bae0cd0b99969070ecd3ed01,https://www.semanticscholar.org/paper/c59dc28f30409531bae0cd0b99969070ecd3ed01,The Application of English Teaching Based on Cloud Network with Virtual Machine Technology,"The research and practice of virtual machine technology in English teaching is in its infancy, research involving the production and use of the teaching courseware, the use of existing educational software, and the use of computer networks classrooms, the campus network, the Internet and distance learning utility. These research and practice indicates the trend of the future development of foreign language teaching. This article focuses on the discusses of the popularity of cloud computing and virtual machine instruction, by introducing the technology of VMware virtualization to solve problems encountered in the process of teaching cloud computing, fully demonstrated the widely use of virtualization technology and its irreplaceable role . Introduction The development of information technology and computer applications has had a tremendous impact on foreign language teaching. Multimedia technology can be used to design a new process of teaching and interactive, personalized training methods , which tightly integrated English teachers’ teaching process and students learning process, and prompting the English teachers to generate new ideas in teaching, promoting the teaching process of fundamental change, prompting the students to change the traditional passive learning style [1,2]. This paper aims to present their views on the problems encountered in the promotion process and teaching process, through traditional and relatively mature virtualization technology to solve problems encountered in teaching ""cloud computing"" [3]. In fact, this is also a kind of cloud computing solutions, more specifically, an application of virtualization technology, a combination of virtualization technology and teaching application of cloud computing, a platform to promote cloud computing with VMware technology built. Advantage of virtual machine Convenient and safe use of a computer through the virtual machine to install more than one operating system to learn; portability of software test platform migration process; develop cross-platform system software for cross-platform testing. For example, mission-critical Windows and Linux-based [4,5] application development, virtual machines can take advantage of cross-platform development. The use of virtual machines in a computer at the same time enables multiple clients connected into a network, completely realistic simulated environment for testing or learning. Noting that virtualization is the logical representation of resource. Cloud computing introduction Although very young, cloud computing has become a broader application of technology, and various cloud emerging in the IT sector, some analysts believe that cloud computing represents a change in the way of enterprise computing. Expected that over the next five years, many giant manufacturers around the world, such as IBM, Dell, and Hewlett-Packard will transfer its own product line to cloud computing. With more and more enterprises turning to cloud computing, the traditional CPU chip chase higher performance, the pursuit of more large-scale supercomputers tirelessly to improve the performance of a single system industry development model will be slowly 2nd International Conference on Management Science and Industrial Engineering (MSIE 2013) © 2013. The authors Published by Atlantis Press 627 replacing. Cloud computing course, have great vitality and represent the future direction of development of the IT industry, on behalf of the people’s target in the IT industry, but this is also need to spread cloud computing in teaching which not only needs to implement a solid theoretical foundation for students, but also requires a combination of practice and more experimental, and a deep understanding of cloud computing. PM-LB algorithm for virtual machine deployment based on the performance of vector The study of deployment algorithm should fully considering the cloud computing’s multi-user and multi-service environment, the reason is that the system is based on a virtual machine hosted business whose dependence is different for different resources, mainly dependent on the performance of the virtual machine as a user preference for performance and making resource allocation is given adequate resources to reserve space, the side of the user is designed to obtain a better user experience, admittedly. When dealing with the virtual machine deployment, we need first to effectively monitor the performance of the virtual machines. For that the virtual machine hardware resources generally consist primarily of CPU performance, memory utilization, network connectivity and configuration state of the virtual machine on the host operating status, etc. Standardization of performance characteristics herein by reference Virtual Machine Manager 2008 technical report performance evaluation criteria for the physical servers , the four basic performance of the CPU, memory, substitution, and a hard disk , for example, per 10min to extract the average value of the condition of use , according to resource characteristics calculated under treatment:",,2013.0,10.2991/MSIE-13.2013.138,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
9656acc489a092e7d4c93c86ac45ccd411214da7,https://www.semanticscholar.org/paper/9656acc489a092e7d4c93c86ac45ccd411214da7,Machine Learning based data analytics for IoT enabled Industry Automation,"The main aims of this projects to the replacement of old communication that uses wired links with new communication that is wireless communication.The main reason to move to wireless communication is to improve the mobility, reduce the deployment cost, reduce cable damage and to improve the scalability.The current industrial revolution is the 4.0 industrial revolution which combines different technologies such as Internet of Things (IOT), robotics, virtual reality and artificial intelligence. The current industrial revolution is the 4.0 industrial revolution which combines different technologies such as Internet of Things (IOT), robotics, virtual reality and artificial intelligence.The current industrial revolution is the 4.0 industrial revolution which combines different technologies such as Internet of Things (IOT), robotics, virtual reality and artificial intelligence.The second aim of this project is to connect devices to IOT so as to improve theaccessibility of the industry from anywhere in the world. These services are known as Best Effort services.","International Journal of Scientific Research in Science, Engineering and Technology",2022.0,10.32628/ijsrset229240,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
3e646ac7ba2a3fc1027362331f93f66bd57a926d,https://www.semanticscholar.org/paper/3e646ac7ba2a3fc1027362331f93f66bd57a926d,Security and Privacy in the Internet of Things: Technical and Economic Perspectives,"For the last twenty years, the Internet extends from digital spheres into the physical world through applications such as smart homes, smart cities, and Industry 4.0. Although this technological revolution of the Internet of Things (IoT) brings many benefits to its users, such as increased energy efficiency, optimized and automated processes, and enhanced comfort, it also introduces new security and privacy concerns. In the first part of this thesis, we examine three novel IoT security and privacy threats from a technical perspective. As first threat, we investigate privacy risks arising from the collection of room climate measurements in smart heating applications. We assume that an attacker has access to temperature and relative humidity data, and trains machine learning classifiers to predict the presence of occupants as well as to discriminate between different types of activities. The results show the leakage of room climate data has serious privacy implications. As second threat, we examine how the expansion of wide-area IoT infrastructure facilitates new attack vectors in hardware security. In particular, we explore to which extent malicious product modifications in the supply chain allow attackers to take control over these devices after deployment. To this end, we design and build a malicious IoT implant that is inserted in arbitrary electronic products. In the evaluation, we leverage these implants for hardware-level attacks on safetyand security-critical products. As third threat, we analyze the security of ZigBee, a popular network standard for smart homes. We present novel attacks that make direct use of the standard’s features, showing that one of its commissioning procedures is insecure by design. In the evaluation of these vulnerabilities, we reveal that attackers are able to eavesdrop key material as well as take-over ZigBee products and networks from a distance of more than 100 meters. In the second part of this thesis, we investigate how IoT security can be improved. Based on an analysis of the root causes of ZigBee’s security vulnerabilities, we learn that economic considerations influenced the security design of this IoT technology. Consumers are currently not able to reward IoT security measures as an asymmetric information barrier prevents them from assessing the level of security that is provided by IoT products. As a result, manufacturers are not willing to invest into comprehensive security designs as consumers cannot distinguish them from insufficient security measures. To tackle the asymmetric information barrier, we propose so-called security update labels. Focusing on the delivering of security updates as an important aspect of enforcing IoT security, these labels transform the asymmetric information about the manufacturers’ willingness to provide future security updates into an attribute that can be considered during buying decisions. To assess the influence of security update labels on the consumers’ choice, we conducted a user study with more than 1,400 participants. The results reveal that the proposed labels are intuitively understood by consumers, considerably influence their buying decisions, and therefore have the potential to establish incentives for manufacturers to provide sustainable security support.",,2019.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
ce9d9a33c93e9a7cac2a1e77be2dce2984542361,https://www.semanticscholar.org/paper/ce9d9a33c93e9a7cac2a1e77be2dce2984542361,Prediction of Throughput of EXT WLANs through Machine Learning,"WLANs are expected to be one of the foundations of next-generation wireless communication systems. They're well-known for their unique capacity to deliver high data speeds in key places (hotspots).WLANs, with IEEE 802.11 as the industry's most widely accepted standard, are a cost-effective alternative for wireless internet connection that can cover most of today's communication needs in residential and business contexts. However, the scarcity of frequency spectrum in the ISM radio bands, growing throughput demands from new bandwidth-demanding applications, and the heterogeneity of current wireless network architecture all contribute to tremendous complexity [1]. In dense WLAN deployments, such concerns become more important, resulting in several partially overlapping scenarios and coexistence issues.",2022 International Conference for Advancement in Technology (ICONAT),2022.0,10.1109/ICONAT53423.2022.9726087,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
475b343d4f42c8048aa7c98f8a6c9230298c6afa,https://www.semanticscholar.org/paper/475b343d4f42c8048aa7c98f8a6c9230298c6afa,Internet of Things (IoT) Security and Forensics: Challenges and Opportunities,"Internet of Things (IoT) devices are increasingly found around, in, and on us (e.g., smart home and other consumer devices) in applications ranging from environmental monitoring to healthcare (e.g., healthcare or medical IoT devices) to surveillance to industry (e.g., industrial IoT IIoT), and battlefields / military (e.g., Internet of Battlefield / Military Things). Such devices are also generally capable of capturing a broad range of information, including digital artifacts that can be used for cyber threat intelligence and inform security mitigation strategy formulation. There are, however, a number of challenges associated with designing IoT cyber security and threat intelligence solutions. In addition to the technical challenges, there are also associated legal and policy challenges that need to be considered in the design and deployment of such solutions in practice. For example, how do we use machine/deep learning to facilitate detection of real-time attacks against IoT devices and systems, and how can we automatically identify and collect digital evidence in a forensically sound manner which can be subsequently used for cyber threat intelligence? In the event that the attackers use sophisticated tools to obfuscate their trails, can we design machine/deep learning techniques to unobfuscate and/or identify and exploit vulnerabilities to get access to digital evidence? What are the potential legal implications and challenges? Can we also design explainable AI techniques to facilitate the explanation and inclusion of such digital evidence and cyber threat intelligence in court proceedings or presentations to C-level or boards in organizations? Based on these discussed challenges, we will identify potential opportunities for stakeholders in academia (e.g., students and researchers), industry and government.",CPSIOTSEC@CCS,2021.0,10.1145/3462633.3484691,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
91033df18d9e46014932bc4dc52961a665dee5ef,https://www.semanticscholar.org/paper/91033df18d9e46014932bc4dc52961a665dee5ef,In-Vehicle Occupancy Detection And Classification Using Machine Learning,"Occupancy detection is a difficult problem. There are several mechanisms exists for occupancy detection in vehicles, particularly in Automobiles. Now, safety has become an important and necessary aspect of the automobile industry. Airbag became a basic and important safety measure in cars. Even though airbag is a vehicle safety device, it can kill children below 12 years due to its rapid action by the exerting lot of force. This project explains about detecting the number of passengers sitting in the car and then classifying each person whether he/she is a child or an adult by processing the image taken from the camera. So that the deployment of airbags can be avoided near children. Each time car speeds from 0 Kmph to 20 Kmph, occupancy of the car is determined and each one is classified again. We are using widely used technique Haar Cascades, for detection. First, we detect faces and then classify each occupant adult or child.","2020 11th International Conference on Computing, Communication and Networking Technologies (ICCCNT)",2020.0,10.1109/ICCCNT49239.2020.9225661,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
a0c0ad35b950d758467eee6dd639c4be13104031,https://www.semanticscholar.org/paper/a0c0ad35b950d758467eee6dd639c4be13104031,AUTOMATIC LOGGING-WHILE-DRILLING DIPOLE SONIC SHEAR PROCESSING ENABLED BY PHYSICS-DRIVEN MACHINE LEARNING,"Logging-while-drilling (LWD) dipole sonic tools have been introduced to the industry as a supplement to monopole and quadrupole measurement because they can provide shear slowness anisotropy, which is essential for formation characterization and well completion applications. Due to the presence of the collar, which acts as a strong waveguide, the recorded formation signal is significantly affected at low frequencies. Consequently, an automated interpretation of LWD dipole sonic data re-mains a challenge. The traditional dispersive semblance-based method requires accurate estimates of parameters such as borehole size and/or mud slowness to avoid bias in the dispersion model used in the processing. Recently, a frequency-slowness domain inversion scheme has been developed that can invert for both the formation shear slowness and mud slowness by minimizing the guidance-mismatch cost function. However, this method uses an isotropic dispersion model and requires selecting narrow-band dispersion data in the low-frequency range with good-quality, which can limit the range of applicability of the method and also requires user input through-out the process. We have previously developed a physics-driven machine learning-based method to enhance the interpretation of wireline dipole sonic data. However, the LWD scenario introduces additional complexity. This work extends the method to support the interpretation of LWD dipole sonic. An anisotropic root-finding mode-search algorithm is first used to generate extensive synthetic formation flexural dispersion curves that can match dispersion measurements in strong anisotropic formations in high-angle and horizontal wells, with a known tool model. Special care needs to be taken to pick the formation flexural mode from several co-existing modes arising from the strong coupling between tool and formation. After quality control and verification, this comprehensive synthetic dataset is used to train a neural network model. We then develop an inversion-based algorithm, taking advantage of this efficient neural network model and combining it with a clustering algorithm, to reliably label and ex-tract the formation flexural mode, processed from either the modified Prony’s method, or a broadband dispersion analysis algorithm. The extraction around the formation flexural kick-in frequency is used for developing a quality control method. The strongest collar arrival, on the other hand, can be confidently removed due to the fundamental difference in its dispersion characteristics from the formation flexural mode. This novel method can automatically and efficiently label the formation flexural mode and simultaneously invert it for formation shear slowness together with other relevant parameters such as mud slowness without user intervention. Since this method is built upon an anisotropic model, it can be applied to the full frequency range of the data spectrum without the traditional isotropic model assumption. Additionally, the regression analysis of the inverted mud slownesses can further provide physical constraint to reduce uncertainties in the inverted shear slowness. The algorithm has been tested on field data showing good performance. It makes edge deployment possible so that LWD telemetry can be optimized to transmit the processed data to the surface in real-time, which is essential to leverage the advantages of the conveyance method.",,2021.0,10.30632/SPWLA-2021-0059,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
88d2e6e788d714394db4bf0e68216eba5c7df89e,https://www.semanticscholar.org/paper/88d2e6e788d714394db4bf0e68216eba5c7df89e,Fe b 20 19 Efficient Learning for Deep Quantum Neural Networks,"Neural networks enjoy widespread success in both research and industry and, with the imminent advent of quantum technology, it is now a crucial challenge to design quantum neural networks for fully quantum learning tasks. Here we propose the use of quantum neurons as a building block for quantum feed-forward neural networks capable of universal quantum computation. We describe the efficient training of these networks using the fidelity as a cost function and provide both classical and efficient quantum implementations. Our method allows for fast optimisation with reduced memory requirements: the number of qudits required scales with only the width, allowing the optimisation of deep networks. We benchmark our proposal for the quantum task of learning an unknown unitary and find remarkable generalisation behaviour and a striking robustness to noisy training data. Machine learning (ML), particularly applied to deep neural networks via the backpropagation algorithm, has enabled a wide spectrum of revolutionary applications ranging from the social to the scientific [1, 2]. Triumphs include the now everyday deployment of handwriting and speech recognition through to applications at the frontier of scientific research [2–4]. Despite rapid theoretical and practical progress, ML training algorithms are computationally expensive and, now that Moore’s law is faltering, we must contemplate a future with a slower rate of advance [5]. However, new exciting possibilities are opening up due to the imminent advent of quantum computing devices that directly exploit the laws of quantum mechanics to evade the technological and thermodynamical limits of classical computation [5]. The exploitation of quantum computing devices to carry out quantum maching learning (QML) is in its initial exploratory stages [6]. One can exploit classical ML to improve quantum tasks (“QC” ML, see [7] for a discussion of this terminology) such as the simulation of manybody systems [8], adaptive quantum computation [9] or quantum metrology [10], or one can exploit quantum algorithms to speed up classical ML (“CQ” ML) [11–14], or, finally, one can exploit quantum computing devices to carry out learning tasks with quantum data (“QQ” ML) [15–17]. Particularly relevant to the present work is the",,2019.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
eb9faadf9205436973df7dfbb0c6ef3cbd081f1b,https://www.semanticscholar.org/paper/eb9faadf9205436973df7dfbb0c6ef3cbd081f1b,There is an elephant in the room: Towards a critique on the use of fairness in biometrics,"In 2019, the UK’s Immigration and Asylum Chamber of the Upper Tribunal dismissed an asylum appeal basing the decision on the output of a biometric system, alongside other discrepancies. The fingerprints of the asylum seeker were found in the EU’s asylum fingerprint database and registered in 2016, which contradicted the appellant’s account that he left Iraq in 2017. The Tribunal found this biometric evidence ‘unequivocal’ and denied the asylum claim. Nowadays, the proliferation of biometric systems in our societies is shaping public debates around its political, social and ethical implications. Yet whilst concerns towards the racialised use of this technology for migration control or law enforcement have been on the rise, investment in the biometrics industry and innovation is increasing considerably. Moreover, fairness has also been recently adopted by biometrics to analyse demographic bias of biometric systems. Thousands of studies have been recently published to mitigate bias and discrimination on facial recognition, fingerprints or finger veins, among others, even suggesting a lack of bias on these systems. However, algorithmic fairness cannot distribute justice in scenarios which are broken or intended purpose is to discriminate, such as biometrics deployed at the border. In this paper, we offer a critical reading of recent debates about biometric fairness and show its limitations drawing on research in fairness in machine learning and critical border studies. Building on previous fairness demonstrations, we prove that biometric fairness criteria are mathematically mutually exclusive. Then, the paper moves on illustrating empirically that a fair biometric system is not possible by reproducing experiments from previous works. Finally, we discuss the politics of fairness in biometrics systems by situating the debate around biometrics used at the border. We claim that bias and error rates have different impact on citizens and asylum seekers. Fairness has overshadowed the elephant in the room of biometrics, focusing on the demographic biases and ethical discourses of these algorithms rather than examine how these systems reproduce historical and political injustices in these contexts.",ArXiv,2021.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
7e0255fced7b8aadce8235452cc3f8fc73d373de,https://www.semanticscholar.org/paper/7e0255fced7b8aadce8235452cc3f8fc73d373de,Design and implementation of AR/VR services at the edge infrastructure using Network Function Virtualization,"Nowadays Augmented Reality (AR) and Virtual Reality (VR) are dominant IT research and industry related topics. They exploit on-device advanced technology, based on Machine Learning (ML) algorithms, to add useful information above the real world (AR) or to create a complete new virtualized world where the user takes an important role (VR). However, AR and VR, although they are equipped with common low-cost peripherals (IP cameras, LCD displays, speakers, microphones, etc.), they need special purpose core components to run ML power-demanding algorithms which dramatically increase the overall cost of the device. Moreover, those devices lack of portability and flexibility, since they are usually designed for a specific service, thus making impossible the re-use of the device for another task. The goal of this thesis is to present a solution that allows to provide AR/VR services requested by a client using legacy devices, without affecting implementation of the latter. We introduce Network Edge AR/VR (NEAR), a NFV framework for AR/VR services deployed transparently at the edge. The edge infrastructure, namely a network area provided with high-computing power and located near the customer requesting a service, is needed to meet the ultra-low latency requested by those services. Furthermore, a NFV framework is useful wherever we want to optionally provide flexibility and scalability to the requested services, such as content caching and client APIs to dynamically manage the running service. NEAR's philosophy is to offload computing capabilities from a special-purpose device (Server) to a general-purpose Edge Gateway acting as a Proxy server allowing a client to request the augmented/virtual service trough the proxy, in charge of performing the selected service. Transparency, a key feature of NEAR, avoids to affect design and implementation of the endpoints (server and client). In fact, NEAR does not need special equipment and/or specific components both at client-side and server-side in order to perform the chosen AR/VR task.",,2019.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
8c2b9c317190908351095d80dc45cf4027e229ff,https://www.semanticscholar.org/paper/8c2b9c317190908351095d80dc45cf4027e229ff,On the effectiveness of learning through the use of the web based Laboratories – the experience of using the solar e-lab,"This paper elaborates on the experience from the operation of the solar energy e-learning laboratory (Solar e-lab) in Cyprus, and analyses the results of the online evaluation submitted by the e-lab users. The aim of solar energy e-learning laboratory is to use web-based technology as a tool to make the laboratory facilities accessible to engineering students and technicians, including handicapped, located outside the e-lab premises anywhere in the world. In this way, the laboratory, its equipment and experimental facilities are available and can be shared by many people, thus reducing costs and widening educational experiences. Throughout its 5 years of operation, the solar e-lab has been accessed by users from over 400 locations from 75 countries spread all over the world. The total number of hits recorded on the site exceeded 1.2 million. Furthermore, a number of colleges and Universities are using the solar e-lab as part of their training programme. Judging from the online student evaluation reports sent to the solar e-lab administrator, it can be said that there is nearly excellent satisfaction by the users. The feedback provides useful information as to the students’ satisfaction with the e-lab interaction, its course content, and organisation. Introduction The role that web-based learning undertakes within the teaching and learning environments has gained widespread acceptance over the last number of years as remote engineering is becoming an important element in engineering education (Auer et al 2003, Benmohamed et al 2005, Agrawal and Srivastava 2007, Helander and Emani 2008). The development of web-based remote engineering experimentation laboratories can significantly enhance the students’ learning experience. A recent assessment study (Nickerson et al 2007) comparing versions of remote labs versus hands-on labs in a junior-level mechanical engineering course on machine dynamics and mechanisms, suggests that students learned lab content information equally well from both types of laboratories, and that they have a realistic understanding and appreciation of the practical advantages of remote laboratories. Using remote laboratories has the potential to significantly reduce obstacles related of cost, time-inefficient use of facilities, inadequate technical support and limited access. The growing importance of sharing engineering resources through web-based laboratories can be attributed to the growing complexity of engineering tasks, the highly specialized and expensive equipment as well as software tools and simulators. It can also be attributed to the necessary use and application of high tech equipment and the need of high qualified staff to control such equipment. There is evidence that the distance educated students are more motivated than on-site students and that there is significant relationship between the student attends class and the motivation to do well (Bisciglia et al 2005). As online education becomes an everyday part of engineering education in nowadays (Hutzel 2002, Lindsay and Good 2005, Aziz et al 2006, Machotka and Nedic 2006), online methods in engineering education will increase the breadth and scale of engineering education, thus extending the reach of institutions and the delivery of education to broader audiences (Lindsay and Good 2006). In contrast to “traditional” engineering, experts in remote engineering are deployed in a relatively broad range of activities that span to different sectors of industry. They typically work in locally distributed teams and coordinate their work amongst themselves. This requires not only competent handling of tools and methods for diagnosis, maintenance, monitoring and repair, but above all require the ability to communicate effectively with others (e.g. customers, users, installers) with the help of computer-aided means of communication. Skilled service technicians must solve the “mutual knowledge problem”, for example by integrating the know-how of others in order to accomplish their goals using appropriate tools (e.g. electronic conferencing or groupware applications). Special focus must be placed on accessing distributed information from suppliers, customers and manufacturers over the Internet. Because e-maintenance is primarily immaterial, the quality assessment made by customers is highly dependent on those employees who perform such services. For this reason, technicians and engineers must also be trained in customer orientation with an emphasis on communication training and customer-centred action. The solar e-lab features: architecture and organisation The solar energy e-learning laboratory is a good example of a web-based laboratory. It was developed within the MARVEL project of the Leonardo da Vinci programme (Müller and Ferreira 2003) and it focuses on experiential based learning arrangements allowing remote and distributed working with laboratories, workshops and real working-places to train students in remote engineering (Auer et al 2003). The solar e-lab comprises a pilot solar energy conversion plant which consists of two flat-plate solar collectors having a surface area of 3 m2 located on the roof of the laboratory, an insulated thermal storage tank located in the solar energy laboratory and other auxiliary equipment and accessories. It is also equipped with all necessary instrumentation, control and communication devices which are needed for remote access, control, and data collection and processing. A major goal of the solar e-lab is the usage of real worlds in virtual learning environments in order to support workprocess-oriented and distributed cooperative learning with real-life systems. Its aim is to use the Internet as a tool to make the laboratory facilities accessible to engineering students (especially handicapped) and technicians located outside the laboratory, including overseas. In this way, the solar energy e-learning lab and its equipment and experimental facility will be available and be shared by many people, thus increasing availability and reducing costs. The system architecture used in the solar energy e-learning lab is illustrated in fig. 1. The user can access the solar e-lab through a PC which acts as a web server. This server hosts the e-learning platform with all necessary extensions for PHP support as well as the database necessary for this platform. It also communicates with the machine hosting the application software (TestPoint). Whenever a user wishes to get into the system, the communication will be done through this server. That is, the user sends his/her request to the system, the web server communicates with the TestPoint web server and it collects the data and transfers them to the user. The actual running of the set-up is done via the TestPoint, which is an interface tool capable of acquiring data through various sensors, storing the data in a form that the user likes, and processing and handling the data in a meaningful manner. This particular software consists of two parts, the programming and the runtime parts. The programming is needed only to the system designer, while the runtime is necessary to run the particular experiment and is available to the interested user free of charge. Any collected data can be stored in popular programme formats (Word, Excel, etc.) allowing the user to print his own report formats and hand in a report of his choice. This tool is located on a dedicated server allowing faster data handling. Fig. 1. The solar e-lab system architecture A user may visit the laboratory website anytime from anyplace in the world. The only requirements are a computer connected to the internet and any of the standard web browsers. By typing the address of the solar energy e-learning laboratory (http://e-lab.hti.ac.cy ), the user can visit the initial page of the website. It is possible for visitors with little interest in solar energy to read and study on the subject with no requirements or registration or testing. So, not all of the pages require login. As a matter of fact, one can see most of the pages without the need of creating an account. Login, and thus creating an account, is only needed when the user decides to take the so called pre-lab test and conduct an online live experiment through the internet. A booking tool is available to control the access time to the system. In order to be able to make a booking, to have access to the system for conducting an experiment, a remote student has to attempt a pre-lab quiz and get a passing grade. In case the system is busy, because another user is online performing an experiment, the user is entitled and he/she can get into the e-lab as an observer. The system will open a new window and the remote user will be able to have a view of the system in operation and get the readings but he/she will not be allowed to intervene into the operation and control of the system. The “observer user” can, however, record the readings and use them for calculations if he/she wishes so. The learning experience The solar e-lab is designed for real time online live experiments in the field of solar engineering. The design of the learning scenarios comprises of a series of exercises of different degree of difficulty and complexity. The introductory work with all the notes, system explanations and glossary allow the student/interested party to get familiar with the system and the work to be followed. The subsequent exercises with the difficulties and unexpected problems of real life experimentation allow the student to realize the difficulties of the work (Kolb’s first and second steps). For each exercise, the student undergoes an online assessment and is allowed to proceed to a real experiment only if he/ she is successful to the pre-lab test. As a last step into the real world of experimentation the student may get access to the system and perform system control and data gathering. During this part of the work the student will get acquaint with the rem",,2009.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
0f753654a8fddca425d3fc8f0c2436cd1ffe8f6a,https://www.semanticscholar.org/paper/0f753654a8fddca425d3fc8f0c2436cd1ffe8f6a,Enhancing Hardware Malware Detectors’ Security through Voltage Over-scaling,"Computing systems are under continuous attacks by increasingly motivated and sophisticated adversaries. These attackers exploit vulnerabilities to compromise systems and deploy malware. Although significant effort continues to be directed at making systems more resilient to attacks, the number of exploitable vulnerabilities is overwhelming. While preventing compromise is difficult, signature based static analysis techniques can be easily bypassed using metamorphic/polymorphic malware or zero-day exploits since their signatures have not yet been encountered. On the other hand, dynamic detection techniques can detect unseen signatures since they monitor the behavior of the program. However, the complexity and difficulty of continuous dynamic monitoring have traditionally limited its use due to constrained resources. Against this backdrop, several research studies proposed using Hardware Malware Detectors (HMDs) to make the continuous dynamic monitoring resource-efficient through hardware support. Specifically, HMDs are machine learning classifiers that use low-level hardware features such as instructions traces, memory access patterns, etc. and classify malware as a computational anomaly. HMDs can offer a significant advantage to defend against malware attacks because they can be ‘always on’ with small-to-no impact on performance. It appears that the industry started to show interest in using HMDs too; SnapDragon processor from Qualcomm appears to be using hardware features to detect malware, but the technical details are not published [4]. As HMDs showed potential defense effectiveness, it is natural to expect that attackers attempt to find adaptive ways to evade detection. As a consequence, it was shown that attackers can adapt malware to continue to operate while avoiding detection by HMDs [3]. We address the challenge of defending HMDs against evasive malware by utilizing approximate computing (AC). In particular, we propose V-HMDs, which are HMDs that uses voltage over-scaling (VOS) for evasion resilience purpose; it induces stochastic computations in HMD’s model during inference, resulting in V-HMDs that are resilient to adversarial evasion attack.",,2021.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
e8db164ea96a61abcd282c65b4131908473a15c4,https://www.semanticscholar.org/paper/e8db164ea96a61abcd282c65b4131908473a15c4,Turning a Breakthrough Technology into a Scalable Process: Sealed Wellbore Pressure Monitoring,"
 A breakthrough patent-pending pressure diagnostic technique using offset sealed wellbores as monitoring sources was introduced at the 2020 Hydraulic Fracturing Technology Conference. This technique quantifies various hydraulic fracture parameters using only a surface gauge mounted on the sealed wellbore(s). The initial concept, operational processes, and analysis techniques were developed and deployed by Devon Energy. By scaling and automating the process, Sealed Wellbore Pressure Monitoring (SWPM) is now available to the industry as a repeatable workflow that greatly reduces analysis time and improves visualizations to aid data interpretations.
 The authors successfully automated the SWPM analysis procedure using a cloud-based software platform designed to ingest, process, and analyze high-frequency hydraulic fracturing data. The minimum data for the analysis consists of the standard frac treatment data combined with the high-resolution pressure gauge data for each sealed wellbore. The team developed machine learning algorithms to identify the key events required by a sealed wellbore pressure analysis: the start, end, and magnitude of each pressure response detected in the sealed wellbore(s) while actively fracturing offset wells. The result is a rapid, repeatable SWPM analysis that minimizes individual interpretation biases.
 The primary deliverables from SWPM analyses are the Volumes to First Response (VFR) on a per stage basis. In many projects, multiple pressure responses within a single stage have been observed, which provides valuable insight into fracture network complexity and cluster/stage efficiency. Various methods are used to visualize and statistically analyze the data.
 A scalable process facilitates creating a statistical database for comparing completion designs that can be segmented by play, formation, or other geological variations. Completion designs can then be optimized based upon the observed well responses. With enough observations and based on certain spacings, probabilities of when to expect fracture interactions could be assigned for different plays.",,2021.0,10.2118/204193-MS,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
f93c7edee65ee73923d5110c2d17cbce1d4e544d,https://www.semanticscholar.org/paper/f93c7edee65ee73923d5110c2d17cbce1d4e544d,Manufacturing Execution Systems Mes Optimal Design Planning And Deployment,"The thesis work introduces an assessment framework consisting of decisive criteria and related indicators which describe qualitatively the suitability of AI for MES functions based on three criteria with related indicators. In addition, the researcher displays furthermore how the developed assessment framework can be used in order to assess the MES functions regarding their AI “readiness”. In order to cope the findings through the thesis work an inductive research approach has been applied. Existing literature in the fields of intelligent manufacturing, Manufacturing-Execution-Systems, machine learning, deep learning, intelligent manufacturing, digital twin, and assessment methodologies have been extensively studied in order to base the theoretical developed framework on grounded theory. A case study was carried out in order to test the validity and reliability of the developed assessment framework for industry. The outcome of this thesis work was an assessment framework consisting of decisive criteria and related indicators when evaluating a MES function in respect to its AI suitability. Furthermore, an assessment checklist has been provided for the industry in order to be able to assess a MES function regards AI support in a quick and pragmatic manner. To generate a more generalizable assessment framework criteria and indicators have to be adapted, likewise testing the outcome of analogue and digital assessment methodologies will provide material for future studies. Artificial Intelligence arises in the manufacturing field very rapidly. Implementing Artificial Intelligence (AI) solutions and algorithms in the manufacturing environment is a wellknown research field in academia. On the other hand, Manufacturing-Execution-System (MES) providers do not have a theoretical and pragmatic framework regarding the evaluation of MES functions in respect to their suitability for Artificial Intelligence. In order to be able to pre-evaluate whether a MES function shall be AI supported an intense literature research has been conducted. Academia shows few investigations regarding this field of research. Recent studies have been concerning about possible applications for MES functions in combination with AI. However, there is a lack of research in terms of preevaluating a MES function before embedding the function with AI",,2017.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
92d6eee37aa37a7f51143c5a44879626ff8f3a16,https://www.semanticscholar.org/paper/92d6eee37aa37a7f51143c5a44879626ff8f3a16,Call for Papers,"Driven by the sixth generation (6G) communication system, the 5G communication system's large-connection, high-bandwidth and low-latency scenarios are developing in the direction of all day long information service under the integration of satellite networks. With the continuous evolution of storage, computing, networking and communication infrastructures with their extensive deployments, 6G network will promote the seamless connection of Cyber-Physical world and human society, that is, the deep integration of physical space, information space and human society. However, with the development of artificial intelligence (AI) chips, intelligent big data driven networking systems are becoming more and more extensive. A massive number of evergrowing smart devices impose new challenges to big data sensing, transmitting, processing and management. And the bottlenecks of conventional networking and communication systems from different scales, ranging from wireless local area networks to macro-cellular networks, have been exposed, such as increased complexity with large scale networks, dynamic topology in mobile networking, resource constraints in computing and networking, heterogeneous architectures, impracticality of centralized, weak survivability, and unattended resolution of potential failures. Thus, there is a need to fundamentally address all the above-mentioned issues in big data networking domains. We need to investigate novel contributions in the field of machine learning and big data analytics applied to networking. Based on AI, edge computing and Internet of things, we need to realize the deep integration of big data, intelligent algorithms and networking, including frameworks capable of collecting and analyzing both online and offline massive datasets and scalable analytic techniques. As one of the technical symposia of the 2021 International Wireless Communication and Mobile Computing Conference (IWCMC 2021), Big Data Networking Symposium seeks cross disciplinary innovative research ideas and applications results from academia and industry for big data networking issues including novel theory, algorithms, protocols, architectures and applications. Accepted papers will be published in the IEEE IWCMC 2021 proceedings and will be submitted to the IEEE digital library (IEEE Xplore).",,2014.0,10.1080/00208825.2014.11043934,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
505195a1db461b083ee30141923b1610ec2cb8ee,https://www.semanticscholar.org/paper/505195a1db461b083ee30141923b1610ec2cb8ee,Artificial intelligence and the dreaded 's.,"Numerous industries are being disrupted by growth in new technologies, especially information technologies, and healthcare is no exception. Advances in robotics, wireless sensor networks, 5D printing, and cloud technologies are reshaping countless industries. I am intrigued by the increasing importance of automation, machine learning, and artifi cial intelligence (AI) in healthcare. Let us explore three questions together: • Where are common applications of AI and automation in healthcare? • What implications for physician assistants (PAs) arise from increased automation and AI in caring for patients? • Did AI bring back the ’s that causes any self-respecting PA to cringe? I nearly panicked recently when I caught sight of the following two headlines from online articles about new healthcare technologies, which might lead a person to think the PAs of the future are not people at all. At the very least, I was ready to e-mail the AAPA communications team to combat those pesky apostrophes. The articles actually detailed advances in automation and AI within healthcare. Bright.MD raises another $8M for “virtual physician’s assistant” SmartExam (www.mobihealthnews.com/content/ brightmd-raises-another-8m-virtual-physicians-assistantsmartexam) Healthcare Chatbots: The Physician’s Assistant of the Future? (http://blog.kantarhealth.com/blog/brian-mondry/ 2016/11/28/healthcare-chatbots-the-physician’s-assistantof-the-future) Next, let us sort out AI and automation. According to Merriam Webster, artifi cial intelligence is the capability of a machine to imitate intelligent human behavior. Automation, on the other hand, is the automatically controlled operation of an apparatus, process, or system by mechanical or electronic devices that take the place of human labor. COMMON APPLICATIONS A widely adopted automation in healthcare is appointment reminder software that automatically reminds patients of their upcoming scheduled appointments, with options to customize the message and/or time it is delivered for patient preference. Similarly, missed appointment notifi cation systems can alert a PA to a potentially worrisome pattern of missed appointments for a patient identifi ed as high-risk. Robotics, commonly deployed in areas such as pharmacy and surgery, are automations proven to increase effi ciency and safety. According to CB Insights, about 86% of healthcare provider organizations, life science companies, and healthcare technology vendors are using AI technology. The most common applications seem to fall into one of ten categories: managing medical records and other data; doing repetitive jobs such as analyzing tests, interpreting radiologic studies, and data entry; helping design treatment plans; digital consultation (such as the Babylon app); virtual nurses (such as the Molly app), medication management (such as the AiCure app); drug development; precision medicine; health monitoring; and healthcare system analysis.1 Numerous tech giants are investing heavily in AI applications for healthcare as well, such as Microsoft’s Healthcare NExT initiative and Google’s Deepmind Health.",JAAPA : official journal of the American Academy of Physician Assistants,2018.0,10.1097/01.JAA.0000530302.23280.25,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
4e4310aa507659c88b0a58b15bc49f87747a6fc6,https://www.semanticscholar.org/paper/4e4310aa507659c88b0a58b15bc49f87747a6fc6,LOCAT: Low-Overhead Online Configuration Auto-Tuning of Spark SQL Applications,"Spark SQL has been widely deployed in industry but it is challenging to tune its performance. Recent studies try to employ machine learning (ML) to solve this problem, but suffer from two drawbacks. First, it takes a long time (high overhead) to collect training samples. Second, the optimal configuration for one input data size of the same application might not be optimal for others. To address these issues, we propose a novel Bayesian Optimization (BO) based approach named LOCAT to automatically tune the configurations of Spark SQL applications online. LOCAT innovates three techniques. The first technique, named QCSA, eliminates the configuration-insensitive queries by Query Configuration Sensitivity Analysis (QCSA) when collecting training samples. The second technique, dubbed DAGP, is a Datasize-Aware Gaussian Process (DAGP) which models the performance of an application as a distribution of functions of configuration parameters as well as input data size. The third technique, called IICP, Identifies Important Configuration Parameters (IICP) with respect to performance and only tunes the important ones. As such, LOCAT can tune the configurations of a Spark SQL application with low overhead and adapt to different input data sizes We employ Spark SQL applications from benchmark suites TPC-DS, TPC-H, and HiBench running on two significantly different clusters, a four-node ARM cluster and an eight-node x86 cluster, to evaluate LOCAT. The experimental results on the ARM cluster show that LOCAT accelerates the optimization procedures of the state-of-the-art approaches by at least 4.1× and up to 9.7×; moreover, LOCAT improves the application performance by at least 1.9× and up to 2.4×. On the x86 cluster, LOCAT shows similar results to those on the ARM cluster.",SIGMOD Conference,2022.0,10.1145/3514221.3526157,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
80b2d2596fbc7bbc945116295b2f229daf74cb44,https://www.semanticscholar.org/paper/80b2d2596fbc7bbc945116295b2f229daf74cb44,An in-hive soft sensor based on phase space features for Varroa infestation level estimation and treatment need detection,"Abstract. Bees are recognized as an indispensable link in the human food chain and general ecological system.
Numerous threats, from pesticides to parasites, endanger bees, enlarge the burden on hive keepers, and frequently lead to hive collapse.
The Varroa destructor mite is a key threat to bee keeping, and the monitoring of hive infestation levels is
of major concern for effective treatment. Continuous and unobtrusive monitoring of hive infestation levels along with other vital bee hive parameters is coveted, although there is currently no explicit sensor for this task. This problem is strikingly similar to issues such as
condition monitoring or Industry 4.0 tasks, and sensors and machine learning bear the promise of viable solutions (e.g., creating a soft sensor for the task).
In the context of our IndusBee4.0 project, following a bottom-up approach, a modular in-hive gas sensing system, denoted as BeE-Nose, based on common
metal-oxide gas sensors (in particular, the Sensirion SGP30 and the Bosch Sensortec BME680) was deployed for a substantial part of the 2020
bee season in a single colony for a single measurement campaign. The ground truth of the Varroa population size was determined by repeated conventional method application.
This paper is focused on application-specific invariant feature computation for daily hive activity characterization.
The results of both gas sensors for Varroa infestation level estimation (VILE) and automated treatment need detection (ATND), as a thresholded or two-class interpretation of VILE, in the order of up to 95 % are presented.
Future work strives to employ a richer sensor palette and evaluation approaches for several hives over a bee season.
",Journal of Sensors and Sensor Systems,2022.0,10.5194/jsss-11-29-2022,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
d8cf928867771c4321968ca83fa8c9314607eb55,https://www.semanticscholar.org/paper/d8cf928867771c4321968ca83fa8c9314607eb55,EdgeTune: Inference-Aware Multi-Parameter Tuning,"Deep Neural Networks (DNNs) have demonstrated impressive performance on many machine-learning tasks such as image recognition and language modeling, and are becoming prevalent even on mobile platforms. Despite so, designing neural architectures still remains a manual, time-consuming process that requires profound domain knowledge. Recently, Parameter Tuning Servers have gathered the attention o industry and academia. Those systems allow users from all domains to automatically achieve the desired model accuracy for their applications. However, although the entire process of tuning and training models is performed solely to be deployed for inference, state-of-the-art approaches typically ignore system-oriented and inference-related objectives such as runtime, memory usage, and power consumption. This is a challenging problem: besides adding one more dimension to an already complex problem, the information about edge devices available to the user is rarely known or complete. To accommodate all these objectives together, it is crucial for tuning system to take a holistic approach to parameter tuning and consider all levels of parameters simultaneously into account. We present EdgeTune, a novel inference-aware parameter tuning server. It considers the tuning of parameters in all levels backed by an optimization function capturing multiple objectives. Our approach relies on inference estimated metrics collected from our emulation server running asynchronously from the main tuning process. The latter can then leverage the inference performance while still tuning the model. We propose a novel one-fold tuning algorithm that employs the principle of multi-fidelity and simultaneously explores multiple tuning budgets, which the prior art can only handle as suboptimal case of single type of budget. EdgeTune outputs inference recommendations to the user while improving tuning time and energy by at least 18\% and 53\% when compared to the baseline.",Middleware,2022.0,10.1145/3528535.3533273,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
2db4aecb770967dc75aea40138dcf916525e0a43,https://www.semanticscholar.org/paper/2db4aecb770967dc75aea40138dcf916525e0a43,To Share or not to Share: Access Control and Information Inference in Social Networks,"Online social networks (OSNs) have been the most successful online applications during the past decade. Leading players in the business, including Facebook, Twitter and Instagram, attract a huge number of users. Nowadays, OSNs have become a primary way for people to connect, communicate and share life moments. Although OSNs have brought a lot of convenience to our life, users' privacy, on the other hand, has become a major concern due to the large amount of personal data shared online. In this thesis, we study users' privacy in social networks from two aspects, namely access control and information inference.

Access control is a mechanism, provided by OSNs, for users themselves to regulate who can view their resources. Access control schemes in OSNs are relationship-based, i.e., a user can define access control policies to allow others who are in a certain relationship with him to access his resources. Current OSNs have deployed multiple access control schemes, however most of these schemes do not satisfy users' expectations, due to expressiveness and usability.

There are mainly two types of information that users share in OSNs, namely their activities and social relations. The information has provided an unprecedented chance for academia to understand human society and for industry to build appealing applications, such as personalized recommendation. However, the large quantity of data can also be used to infer a user's personal information, even though not shared by the user in OSNs.

This thesis concentrates on users' privacy in online social networks from two aspects, i.e., access control and information inference, it is organized into two parts.

The first part of this thesis addresses access control in social networks from three perspectives. First, we propose a formal framework based on a hybrid logic to model users' access control policies. This framework incorporates the notion of public information and provides users with a fine-grained way to control who can view their resources. Second, we design cryptographic protocols to enforce access control policies in OSNs. Under these protocols, a user can allow others to view his resources without leaking private information. Third, major OSN companies have deployed blacklist for users to enforce extra access control besides the normal access control policies. We formally model blacklist with the help of a hybrid logic and propose efficient algorithms to implement it in OSNs.

The second part of this thesis concentrates on the inference of users' information in OSNs, using machine learning techniques. The targets of our inference are users' activities, represented by mobility, and social relations. First, we propose a method which uses a user's social relations to predict his locations. This method adopts a user's social community information to construct the location predictor, and perform the inference with machine learning techniques. Second, we focus on inferring the friendship between two users based on the common locations they have been to. We propose a…",,2016.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
0ea9e984d07fddabb2f4b35dfab9c46de7ad2a24,https://www.semanticscholar.org/paper/0ea9e984d07fddabb2f4b35dfab9c46de7ad2a24,AN INVESTIGATION TO IMPROVE THE PERFORMANCE OF PORTABLE INTELLIGENT SYSTEM IN A DISTRIBUTED NETWORK,"A low cost adaptive control system called Portable Intelligent System (PIS) was designed to improve the machining accuracy by reducing the dimensional variations in a component being machined in a CNC milling machine. This system can be mounted as an auxiliary device on an existing CNC machine. The PIS consists of portable modular fixture (PMF), laser detection system (LDS) and adaptive control system (ADS). The workpiece which need to be machined is mounted on the PMF. The LDS is used to collect real-time machining data. The ADS uses real-time machining data and determine the dimensional variations from engineering specification (called as delta). Based on the delta value, ADS makes the decision to reduce the dimensional variations via PMF. The decision making algorithm used in the system is feed forward back propagation neural network. The initial design of PIS uses default ANN parameters to monitor and control the dimensional variations found in the component. This paper aims to further investigate and improve the performance of the ADS to six sigma quality level. In addition, an attempt was made to implement the PIS solution in a distributed manufacturing network. From the study, it was concluded that optimum ANN parameters helped to improve the performance of PIS, thus able to improve the machining capability closer to six sigma quality level. In addition, a framework was developed to deploy the PIS solution in a distributed network. In other words, this type of technology can be implemented in one part of the world and the machining can be controlled in the other part of the work at fraction of cost. KEYWORDS— Adaptive control system; low cost automation; precision machining; CNC milling machine; six sigma; artificial neural network; portable intelligence system; distributed network. INTRODUCTION Customers demand high quality products at low cost. A high quality product implies tight tolerances in the manufacturing processes. The tight tolerances define the dimensional variations found in the machined surface. For instance, a component needs to maintain consistent thickness throughout the machined surface within a specified tolerance specification. With the current technology, CNC machine tools are able to machine the components within the tolerance specifications but the machined surfaces are not even (uneven surface). Nithyanandam et al. [1] demonstrated such uneven surfaces found in components machined using all CNC machine tools. This unevenness is called as dimensional variations or error. Figure 1 shows such uneven surface found on a sample component being machined using CNC milling machine. Fig. 1. Dimensional Variations in a sample component. A product consists of several components. Manufacturers accept a component as good quality when the dimensional variations fall within the tolerance specification, even though the surface of the component is not even (as shown in Fig 1). Taguchi et al. [2] stated that any dimensional deviations from the specified mean value of a component’s dimension is cost to quality. When each component is having such dimensional variations, then the final product could face cumulative dimensional variations or cumulative error. This could lead to malfunction of a product over the period of its usage. Therefore, each and every component needs to be machined with tight tolerance and closer to the targeted mean or targeted value. To machine the components with tight tolerance, a low cost adaptive control system called portable intelligent system (PIS) was developed. This system consists of portable modular fixture (PMF), laser detection system (LDS) and adaptive control system (ADS). The ADS consists of artificial neural network (ANN), micro-controller and business intelligent (BI) unit. In ANN, feed forward back propagation neural network algorithm was used with default parameters. Figure 2 shows the working principle of the PIS solution. The step-by-step development of this system was elaborately discussed in the initial study [3]. Figure 3 shows the experimental outcome of that study comparison of machining conditions between conventional machining and PIS machining. Fig. 2. Working principle of portable intelligent system. Fig. 3. Dimensional variations in Traditional machining vs. Existing PIS machining. Even thought PIS helped to machine the components at tight tolerance but not to the six sigma quality level. Therefore, an investigation was carried out to improve the performance of PIS. For this, various ANN parameters such as number of hidden layers, number of neurons in the hidden layers, type of transfer function, ratio between training and test datasets are investigated in detailed. Then, an attempt Ganesh Kumar et al., International Journal of Advanced Engineering Technology E-ISSN 0976-3945 Int J Adv Engg Tech/Vol. VII/Issue II/April-June,2016/375-380 was made to implement this solution in a distributed network. The details of these findings are well documented in this paper. LITERATURE REVIEW Factor of Safety (FOS) is a major factor to be considered in designing and manufacturing components for automobiles and aerospace industries. The ultimate aircraft structural FOS is defined as the ratio between design ultimate load to design limit (or actual applied) load on the structure and it is usually equal to 1.5 for U.S. military and commercial aircraft [4]. From the design perspective, proper fit of components in assembly is a natural way of improving FOS. This implies designing components to tight tolerances. Practically achieving this tight tolerances involves the use of precision machining to reduce the dimensional variations. However, it is difficult to machine the components to tight tolerance using traditional machining processes. Artificial Intelligence offers a wide range of possibilities for the improvement of manufacturing processes. Many researchers [5 14] have done extensive research in the field of Machine Learning (ML) and Artificial Neural Network (ANN). In manufacturing applications, usually they are used for the prediction and control of machining parameters in manufacturing processes. Machine Learning is the use of various algorithms and programs to mimic the process of learning. A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P, if its performance at tasks in T, as measured by P, improves with experience E [15]. Among these algorithms, Artificial Neural Network (ANN) is widely used in manufacturing applications. MatLab [19] suggested using back propagation neural network algorithm for prediction in manufacturing applications. Peng et al. [16] used an ANN based Machine Learning approach to model and predict emission of NOx in coal power plants. The function of artificial neural network (ANN) is to imitate human brain for the implementation of the functions such as association, self-organization and generalization. It can approximate any functions more efficiently and thus it is suitable for modeling any non-linear process [15]. Garcia et al. [6] used an ANN based approach to predict and monitor surface roughness in Taper Turning operation in CNC Lathes. Kanta et al. [7] used ANN approach coupled with Genetic Algorithm to optimize machining parameters in a CNC Milling operation for minimizing the surface roughness on work piece. Kannan et al. [8] optimized the machining parameters of EDM process using ANN approach. Sangwan et al. [9] used an integrated Neural Net and Genetic Algorithm for the prediction and control of optimal machining parameters to reduce surface roughness. Ugarsen et al. [10] used ANN to predict machining performance of EDM process. Neukart et al. [11] studied the application of ANN for Machine learning in machine tools. Beatrice et al. [12] used ANN to predict and control surface roughness of AISI H13 Steel by optimizing cutting fluid application. Gowda et al. [13] used the ANN approach to predict and control circularity, cylindricity and surface roughness in drilling Al-Si3N4 metal matrix composites. Neto et al. [14] studied the optimum ANN model to predict hole diameter in drilling process. EXPERIMENTAL ARRANGEMENTS A preliminary study [3] was carried out in Makino CNC milling machine with Aluminum 6061 as the work piece and ZCC carbide tip end mill cutter as the tool. The ANN algorithm was modeled using MatLab with default values (ANN parameters). Figure 4 shows the experimental setup of the PIS system. The process capability of the existing PIS was determined close to 4.4 sigma. Therefore, an investigation was required to optimize the system to six sigma quality level. A. Laser detection system The two laser sensors (Keyence LK-H027) are mounted on the spindle head of the CNC milling machine, which helps to measure the machining condition in real-time. The workpiece is mounted on the portable intelligent system and the machining operation starts. The laser sensors collect the workpiece thickness via Keyence 5001 controller and stored into a centralized database. Figure 5 shows the working principle of the laser detection system. The alignment of these sensors depends on the accuracy of the machining. Figure 6 shows the alignment of the laser detection system setup. Fig. 4. Experimental Setup with existing PIS. Fig. 5. Working principal of laser detection system. Fig. 6. Alignment of Laser heads. B. Artifical neural network algorithm A typical artificial neural network is represented as shown in Figure 6. The ANN algorithm used in this study was feed forward back propagation learning algorithm, which is shown in the Equation (1) ∑ (1)",,2016.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
ffea4feb73ced4ce7b2e36b587b9aac84d829b51,https://www.semanticscholar.org/paper/ffea4feb73ced4ce7b2e36b587b9aac84d829b51,The Age of Thrivability: Vital Perspectives and Practices for a Better World,"Solving Complex Industrial Problems Without Statistics. 2016. Ralph R. Pawlak. Boca Raton, FL: CRC Press. 143 pages. This book fills a unique gap. For those in our community who are not strong in mathematical and analytical methods (but who wish to contribute more fully to the “analyze” stage of improvement projects), Pawlak provides practical examples to demonstrate the value of qualitative reasoning. For those who want to add a layer of common-sense thinking to continuous improvement efforts that are already data driven, he provides the background and the basis for making sure the analytical and computational parts of projects are well-framed. Despite the title, this book does not suggest industrial problems can (or should) be solved without robust mathematical methods and statistics. Rather, Pawlak spends most of the book describing and explaining the qualitative processes that should accompany all problem-solving efforts in industry. Grounded in 14 short case studies, the techniques he shares are simple and immediately applicable: identifying what characteristics distinguish good output from bad, generating “clues,” visual observation, articulating differences, and staging assembly and operations trials, among others. Even without a complete and rigorous analysis, these “clues” may provide enough information for readers to identify and deploy an appropriate corrective action. The examples cover a range of cases, from the new product development process, to assembly, to design, to machining, to troubleshooting the causes for defects. In summary, this is the only book I have ever encountered that comprehensively explains the thought process of the “fuzzy front end” in Lean Six Sigma problem solving. As a result, it provides excellent case studies that can be used as a first step toward understanding problem solving in common industrial scenarios. Despite a sometimes repetitive feel, the conversational tone of this book makes the material particularly accessible to beginners who might benefit the most from that repetition. “It does not matter that some problems that you encounter will require knowledge beyond your capacity,” Pawlak advises. In fact, the author could have gone further: the primary weakness of this book is that each of the case studies could easily have been two or three times as long and still maintained reader interest. Overall, the “crime scene” approach he recommends helps to frame the process of discovery and learning in a straightforward, accessible way, and provides a unifying thread through all the cases.",,2016.0,10.1080/10686967.2016.11918491,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
d82ddf66dac305abdb8e44ad4e567fe619e517ff,https://www.semanticscholar.org/paper/d82ddf66dac305abdb8e44ad4e567fe619e517ff,DETECTING POTENTIAL VEHICLE CONCERNS USING NATURAL LANGUAGE PROCESSING APPLIED TO AUTOMOTIVE BIG DATA,"A large volume of unstructured data exists in the automotive industry and needs to be analyzed to detect potential vehicle concerns. Much of this data is textual in nature since customer complaints are made through call center interactions and warranty repairs. Current approaches to detect potential vehicle concerns in text data include various keyword search methods. In this paper, we apply Natural Language Processing (NLP) and shallow machine learning methods on text data to create classifiers to detect the potential vehicle concern of airbag non-deployment. For this potential vehicle concern, we show the performance of multinomial Naïve Bayes (NB), Support Vector Machine (SVM) and Gradient Boosted Trees (GBT) classifiers against keyword search methods. We present challenges of classification model development related to the nature of automotive data and limited training data. Our findings provide insights on robust text classification approaches that can improve identification of potential vehicle concerns. INTRODUCTION Automotive corporations and the U.S. federal government [1] are driving improvements in product safety through the collection and analysis of both structured and unstructured (text) data. Despite their efforts, a common problem facing large corporations today is how to extract meaningful insights about product safety from large volumes of unstructured, noisy data that they have accumulated in many disparate systems. These data systems present clear opportunities for analyzing actionable information regarding product complaints and potential defects, but are commonly referred to as “dark data” because they are not easily analyzed due to their unstructured nature [2]. Consider the text data of vehicle warranty claims, call center transactions, and product complaints on social media; these sources all contain valuable information that may describe potential vehicle concerns, but are not represented in a relational structure that can be easily queried. In addition, large corporations, such as General Motors (GM), spend resources maintaining these data systems and encounter challenges efficiently extracting actionable information from them because these systems were not originally created for safety event detection. At the same time, the U.S. federal government has created several incident reporting and complaint collection systems for a variety of industries: the Food and Drug Administration’s (FDA) Adverse Event Reporting System (AERS) [3] for the pharmaceutical industry, the Federal Aviation Administration’s (FAA) Aviation Safety Reporting System (ASRS) [4] for the aviation industry, and the National Highway Traffic Safety Administration’s (NHTSA) Vehicle Owner Questionnaire (VOQ) [5] and Transportation Recall Enhancement, Accountability and Documentation (TREAD) [6] for the automotive industry. The effort by the U.S. federal government in creating these systems is due to the public interest in ensuring that products created by these industries are safe for consumers. Yet, the fundamental problem still exists; all of these systems contain large volumes of dark data because they all have varying degrees of unstructured data in the form of text. Ultimately, private industry and the U.S. federal government have a vested interest in developing techniques for the transformation of unstructured data into structured data to facilitate detection and monitoring of potential vehicle concerns within the automotive industry. For both private industry and the government there is a need to produce statistics that provide an overview of how certain types of product failures are reduced in response to their actions",,2019.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
f3dd7d67ac4d7d6843f326ac510d49926a7af934,https://www.semanticscholar.org/paper/f3dd7d67ac4d7d6843f326ac510d49926a7af934,Social Network Mining for Analysis of Social Phenomena,"In the last decade, a large amount of human interaction data has become available that either comes from online social networks or was captured using wearable devices. Traditional social science does not provide tools for conducting data-driven research using such data. Computational social science aims to close this gap by developing data mining and machine learning approaches for utilizing this data. Toward this end, in this thesis we explore different ways of studying social phenomena by applying data mining techniques to social network data. We use two different types of data: The first type is face-to-face interaction networks data obtained using novel wearable radio-frequency identification (RFID) technology, which represents high-resolution social networks. These networks allow researchers to study human interaction and social phenomena, such as community formation, on a micro level. The second type of data is the data available from online social networks and media. These data enable the study of social phenomena on a large scale, such as the study of changes in human mobility during disasters. It is important to consider both data types, as only in this way may we present a complete picture of the process of mining social network for analysis of social phenomena. We structure this work following the cross-industry standard process for data mining (CRISP-DM) framework. First, in CRISP-DM’s data understanding phase, we aim to analyze face-to-face interaction networks and especially the process of community formation in those networks. We also validate whether online social networks can be used as a proxy for offline activities. The CRISP-DM framework’s subsequent data preparation, modeling and evaluation phases are presented in three different types of studies. In this manner, we demonstrate the scope of what is possible in using social network mining to help us understand social phenomena and activities. The first study, conducted in collaboration with sociologists, shows how analysis of face-to-face interactions may enhance sociological studies. In the second study, which was carried out together with the United Nations Pulse Lab Jakarta, we show how forest fires are discussed in social media and how changes in the mobility of populations can be observed using online social media data. In the third study, we propose an improvement to the k-nearest neighbors classification algorithm for data with non-uniform geospatial distribution, a characteristic often observed in data from online social networks. These three studies show different possibilities when studying social phenomena and developing applications based on social network data. Finally, when discussing the deployment phase of the CRISP-DM framework, we present the Ubicon platform, which we used to collect data and to deploy some of our results. Altogether, this thesis shows different approaches for exploring social phenomena using data mining and machine learning techniques applied to social network data. The presented approaches have different potential use cases, from practical applications to more theoretical sociological work. The insights presented in this thesis are relevant for researchers from different disciplines, including computer science and sociology, interested in studying and developing methods to work with social network data.",,2019.0,10.17170/KOBRA-20190815628,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
a812368fe1d4a186322bf72a6d07e1cf60067234,https://www.semanticscholar.org/paper/a812368fe1d4a186322bf72a6d07e1cf60067234,Gaussian processes for modeling of facial expressions,"Automated analysis of facial expressions has been gaining significant attention over the past years. This stems from the fact that it constitutes the primal step toward developing some of the next-generation computer technologies that can make an impact in many domains, ranging from medical imaging and health assessment to marketing and education. No matter the target application, the need to deploy systems under demanding, realworld conditions that can generalize well across the population is urgent. Hence, careful consideration of numerous factors has to be taken prior to designing such a system. The work presented in this thesis focuses on tackling two important problems in automated analysis of facial expressions: (i) view-invariant facial expression analysis; (ii) modeling of the structural patterns in the face, in terms of well coordinated facial muscle movements. Driven by the necessity for efficient and accurate inference mechanisms we explore machine learning techniques based on the probabilistic framework of Gaussian processes (GPs). Our ultimate goal is to design powerful models that can efficiently handle imagery with spontaneously displayed facial expressions, and explain in detail the complex configurations behind the human face in real-world situations. To effectively decouple the head pose and expression in the presence of large outof-plane head rotations we introduce a manifold learning approach based on multi-view learning strategies. Contrary to the majority of existing methods that typically treat the numerous poses as individual problems, in this model we first learn a discriminative manifold shared by multiple views of a facial expression. Subsequently, we perform facial expression classification in the expression manifold. Hence, the pose normalization problem is solved by aligning the facial expressions from different poses in a common latent space. We demonstrate that the recovered manifold can efficiently generalize to various poses and expressions even from a small amount of training data, while also being largely robust to corrupted image features due to illumination variations. State-of-the-art performance is achieved in the task of facial expression classification of basic emotions. The methods that we propose for learning the structure in the configuration of the muscle movements represent some of the first attempts in the field of analysis and intensity estimation of facial expressions. In these models, we extend our multi-view approach to exploit relationships not only in the input features but also in the multi-output labels. The structure of the outputs is imposed into the recovered manifold either from heuristically defined hard constraints, or in an auto-encoded manner, where the structure is learned automatically from the input data. The resulting models are proven to be robust to data with imbalanced expression categories, due to our proposed Bayesian learning of the target manifold. We also propose a novel regression approach based on product of GP experts where we take into account people’s individual expressiveness in order to adapt the learned models on each subject. We demonstrate the superior performance of our proposed models on the task of facial expression recognition and intensity estimation.",,2016.0,10.25560/44106,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
497c20a6d7ab043d2a07a50e19e27424c92fb003,https://www.semanticscholar.org/paper/497c20a6d7ab043d2a07a50e19e27424c92fb003,The PCOR Partnership Initiative: The State of the Region,"The PCOR Initiative, a 5-year project beginning in early 2020, is one of four newly selected regional initiative awards funded by the U.S. Department of Energy (DOE). The PCOR Initiative will accelerate the deployment of carbon capture, utilization, and storage (CCUS) in the central and northwestern region of North America, comprising ten U.S. states and four Canadian provinces. With extensive fossil fuel resources, large-scale anthropogenic CO2 sources, and geologic storage options, this region provides all of the essential elements necessary for infrastructure development and widespread CCUS deployment. The project builds on 16 years of applied research undertaken by the Plains CO2 Reduction (PCOR) Partnership throughout the northern Great Plains, including the successful assessment and monitoring of over 6 million metric tons of associated CO2 storage, incidental to enhanced oil recovery (EOR) operations at the Bell Creek oil field in southeastrn Montana. 
 
Managed by the Energy & Environmental Research Center (EERC) in Grand Forks, North Dakota, the PCOR Initiative is supported by the North Dakota Industrial Commission, through the Oil and Gas and Lignite Research Programs, and by research partners at the University of Alaska at Fairbanks and the University of Wyoming. Many of the 120+ previous PCOR Partnership member organizations are supporting this new initiative, with additional members also expected. Our team also includes the partners associated with Project Tundra, which constitutes the largest planned capture of CO2 in the world from a power generation facility. With many partners actively engaged in commercial projects, the PCOR Initiative draws together a powerful team to advance and accelerate CCUS deployment in the region and satisfy the objectives put forth by DOE. 
 
The PCOR Initiative will identify and address onshore regional storage and transport challenges facing commercial deployment of CCUS within its region. To achieve this, the PCOR Initiative will 1) address key technical challenges by advancing critical knowledge and capabilities; 2) facilitate data collection, sharing, analysis, and collaboration; 
3) evaluate regional infrastructure challenges and needs; and 4) promote regional technology transfer. 
 
The PCOR Initiative will engage federal and state regulators in the region to support the continued development and knowledge transfer of regulatory policies to accelerate the deployment of CCUS. The regulatory efforts will focus on understanding the federal and state permitting processes and timing and address major regulatory topics, including pore space ownership, practical approaches to defining the area of review, and management of the long-term liability associated with a closed storage site. The PCOR Initiative will facilitate dialogue regarding the status of CCUS projects and regulatory challenges, with an emphasis on knowledge transfer between states with active CCUS projects and Class VI primacy and states with less CCUS development and no CCUS regulations established. North Dakota is the only state currently with Class VI primacy, and Wyoming has a pending Class VI primacy application. These states can provide valuable insight to other states by sharing their learnings from the Class VI primacy application process and Class VI regulatory program implementation at the state level. 
 
The expected outcomes of the PCOR Initiative include: 
• Solving technical challenges through collaboration, and addressing issues including stacked storage; storage optimization; effective monitoring, verification, and accounting strategies; and risk assessment resulting in increased regulator and industry confidence. 
• Advancing knowledge transfer and data sharing to develop key technologies, including National Risk Assessment Partnership tools and machine-learning applications, through collaboration with the DOE National Laboratory Network complex. 
• Accelerating CCUS through the evaluation and promotion of infrastructure requirements, building stakeholder support through techno- and socioeconomic assessments. 
• Promoting regional technology transfer to accelerate infrastructure development and deployment of CCUS, and building regulatory confidence and supporting business case developments that support the CCUS goals of the DOE Fossil Energy Program.",SSRN Electronic Journal,2021.0,10.2139/ssrn.3812017,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
9c1e79d8633399ac07dca5a87d12f2785c244336,https://www.semanticscholar.org/paper/9c1e79d8633399ac07dca5a87d12f2785c244336,ADAPTIVE PID-LIKE CONTROL USING BROAD LEARNING SYSTEM FOR NONLINEAR DYNAMIC SYSTEMS,"This paper presents a new learning control structure using broad learning system (BLS) for adaptive PID-like control of unknown digital nonlinear dynamic systems with time delays. The proposed control method, abbreviated as BLS-APIDLC, is novel in combining BLS and model predictive control to develop a new PID-like control law for high-performance setpoint tracking control and disturbance rejection. Comparative simulations on two renowned nonlinear digital time-delay systems are well used to show the effectiveness and superiority of the proposed method by comparing to four existing methods. INTRODUCTION In the past decades, conventional proportional-integralderivative (PID) controllers have gained widespread use in numerous control applications due to its simplicity of design and efficiency in the industrial applications (Astrom and Hagglund, 1995; Silva et al., 2005; O’Dwyer, 2009; Vilanova and Visioli, 2012). Although the PID controller has only three parameters to be tuned, it is surprisingly difficult to find the right tuning for them without systematic procedures. As such, the tuning of the PID gains is always a challenge in the state of the art of PID controller design. In other words, the main problem with a PID controller is the fact that the parameters of the PID controller must be adjusted properly to meet desired performance. This problem becomes more important when considering issues that include stability and control performance. Recently, the area of adaptive PID control and its related control approaches have still been developed by researchers (Oliveira and Lemos, 2000; Pan et al., 2007; Fahmy et al., 2014; Yang et al., 2015). Adaptive PID controller designs with controller parameters updated online by the neural network models were also presented. Pan et al. (2007) developed a two-layer supervised control method for tuning PID controller parameters based on model parameters estimated by the lazy learning technique. Fahmy et al. (2014) proposed an adaptive PID controller using the recursive least square algorithm which updates the PID gains automatically online to force the actual system to behave like a desired reference model. Oliveira and Lemos (2000) proposed a comparison of some fuzzy-modelbased adaptive-predictive control strategies. Yang et al. (2015) presented an adaptive predictive control strategy based on Laguerre functions in the chopper cascade control system, and examined by experiments. More recently, machine learning algorithms have made significant progress, especially deep learning technologies that have been made in wide areas (Tsai et al., 2014; Rosa and Yu, 2016; Ghazia et al., 2017; Andò et al., 2018; Y. Kang et al., 2019). By successively adjusting the weights between neurons over many input-output pairs, the function computed by the network is refined over time so that it provides more accurate predictions. The lately presented broad learning system (BLS) is an emerging way for efficient and effective modeling of complex systems (Chen and Liu, 2018; Jin and Chen, 2018; Chen et al., 2019). Chen and Liu (2018) developed a very fast and efficient BLS based on the random vector functional-link neural networks (RVFLNN) (Pao et al., 1994) to offer an alternative way for deep learning and structure. The designed model can be expanded in wide fashion when new feature nodes and enhancement nodes are needed. Moreover, the corresponding incremental learning algorithm is also designed. The BLS offers an alternative to deep learning because it has a fast and broad expansion without the need for retraining through incremental learning. The input signals are passed to the mapped feature layer and then Paper submitted 01/12/20; revised 03/25/20; accepted 05/03/20. Author for correspondence: Ching-Chih Tsai (e-mail: cctsai@nchu.edu.tw). 1 Department of Electrical Engineering, National Chung Hsing University, Taichung, Taiwan. 2 Department of Electronic Engineering, Hsiuping University of Science and Technology, Taichung, Taiwan. 358 Journal of Marine Science and Technology, Vol. 28, No. 5 (2020) passed to the enhancement layer via a nonlinear transformation. Although NNs possess good function approximation capabilities for nonlinear dynamic systems, the training process is time-consuming. On the other hand, the BLS system has been shown to preserve good function approximation capabilities and has been illustrated the feasibility and benefits of BLS-based control techniques for identification and control of nonlinear dynamic systems (Chen and Liu, 2018; Jin and Chen, 2018; Vong et al., 2020; Xu et al., 2018; and Feng and Chen, 2018a; Chen et al., 2019). Conventional PID controllers have been regarded as the simplest and the most deployed controller in industry. To extend the robustness and adaptability of the conventional PID controller, by integrating the simplicity and effectiveness of the conventional PID controller and the learning and automatic adjustment capabilities of the intelligent control strategy based on the PID-like controller for the nonlinear dynamic system have been proposed (Wang et al., 1997; Tsai et al., 2005; Cong and Liang, 2009; Fu and Chai, 2011). For example, Wang et al. (1997) presented an adaptive PID-like controller using a Modified Neural Network (MNN) for learning the characteristics of a dynamic system. Tsai et al. (2005) proposed an adaptive PID-Like fuzzy-neural controller and applied to the nonlinear model reference control system. Fu and Chai (2011) presented a robust self-tuning PID-like controller by combining a pole assignment selftuning PID controller with a filter. Cong and Liang (2009) proposed a PID-like neural network nonlinear adaptive controller for motion control systems by using a mix locally recurrent neural network. The gradient descent method is used for online adjustment and the initial PID parameters are needed which can operate the closed-loop stably. Kumar et al. (2014) proposed a hybrid neural network-based PID like adaptive controller for precise position control of a permanent-magnet synchronous motor (PMSM) servo drive. So far, many adaptive PID control for industrial applications have been proposed (Tung, 2012; Tsai et al., 2017; Tsai et al., 2019). Feng and Chen (2018b) presented a PID-like control method using BLS; however, this kind of PID-like control method was limited to a class of nonlinear dynamic systems without time-delays. Inspired by the above-mentioned surveys, the objective of this paper is to propose a BLS-based adaptive predictive PIDlike control, called BLS-APIDLC, of a class of unknown nonlinear discrete-time time-delay systems not only for disturbance rejection but also for precise tracking and guaranteed stability. The presented contents of the paper are written in two principal contributions. One is the theoretical derivation and proof of a more general adaptive PID-like control approach using BLS for unknown nonlinear time-delay dynamic systems by comparing to the result (Feng and Chen, 2018b), and the other is comparative investigation of the proposed BLS-APIDLC in comparison with four existing adaptive PID control methods. Fig. 1. Topological structure of the used BLS model. The rest of this paper is organized as follows. The basic ideas of the BLS and BLS identifier design are described in Sections II and III, respectively. Section IV is devoted to proposing a BLS-APIDLC control law, investigating its closed-loop stability and iterative control algorithm. Section V uses computer simulations to explore the effectiveness and superiority of the proposed BLS-APIDLC method for two illustrious nonlinear time-delay systems. Section VI is finished with the conclusions and future work of the paper. II. BROAD LEARNING SYSTEM (BLS)",,2020.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
abe44d51eaa69e752a06e7d2113c59af6f13ab4e,https://www.semanticscholar.org/paper/abe44d51eaa69e752a06e7d2113c59af6f13ab4e,MACHINE LEARNING APPROACH FOR SENSING HARMFUL GAS DETECTION,"Industrial Air pollution that acts on our everyday activities and general well being. This thing constitutes a warning via the system and therefore the general well being on the earth. The terrible need to be compelled to detect the quality of ambient air is incredibly obtrusive, due to the rise in factory-made pursuit over the latest years. Folks have to be compelled to grasp the extent on which their pursuit has an effect on quality of ambient air. This paper came up with the Associate in Industrial Air pollution observance approach. This approach is being expanded to exploit the Arduino microcontroller. This Industrial pollution observance approach is sketched to analyze quality of ambient air in period. The clean air is estimated by the designed approach to be precise. The consequence is shown on the created apparatus exhibit port along with accessed cloud on any sensible portable accessory. This paper offers the improvement of pollution tracking systems with deployment of intelligent sensors. The proposed approach finds application in industry and additionally in monitoring of causes like a fan, motors.",,2021.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
4867a63e5edd7ee10f28067d6dd5b07d69321de7,https://www.semanticscholar.org/paper/4867a63e5edd7ee10f28067d6dd5b07d69321de7,Thesis Title: A Framework for Improving the Performance of Signature-based Network Intrusion Detection Systems,"Network Intrusion detection systems (NIDSs) have been widely deployed in different network environments (e.g., banks, schools) to defend against a variety of network attacks (e.g., Trojans, worms). Generally, a network intrusion detection system can be classified into two categories: signature-based NIDS and anomaly-based NIDS. In realworld applications, the signature-based NIDS is more prevalent than the anomaly-based detection as the false alarm rate of the former is much lower than the latter. However, we identify three major issues that can greatly affect the performance of a signature-based NIDS. Expensive signature matching. The traditional signature matching in a signature-based NIDS is too expensive that the computing burden is at least linear to the size of an incoming string. Therefore, the operational burden of a signature-based NIDS could be significantly increased in a large-scale network environment. Overhead network packets. In a large-scale network environment, a signature-based NIDS usually has to drop lots of network packets since the number of incoming packets exceeds its maximum processing capability. Massive false alarms. Although the false alarm rate of a signature-based NIDS is much smaller than that of an anomaly-based NIDS. The number of false alarms generated by a signature-based NIDS can still increase the difficulty in analyzing true alarms and adversely affect the analysis results. To mitigate the above issues, in this thesis, we propose several approaches in improving the performance of a signature-based NIDS such as Snort in the following three aspects: Signature matching improvement.We design an exclusive signature matching scheme to help perform a more efficient signature matching with the purpose of enhancing the performance of signature matching in a heavy traffic environment. Network packet filtration and reduction. To mitigate this issue, we advocate the method of constructing a packet filter such as blacklist-based packet filter, list-based packet filter and trust-based packet filter to help filter out target network packets for a signature-based NIDS such as Snort in terms of IP reputation. This packet filter can be deployed in front of a signature-based NIDS and reduce its workload in an intensive traffic network. False alarm reduction. To resolve this issue, we design several false alarm filters such as machine-learning based false alarm filters, alarm filters using knowledge-based alert verification and context-based alarm filters to help reduce false alarms (or non-critical alarms) that are generated by a signature-based NIDS. A Framework. In addition, we further propose a framework by combining the above work to overall improve the performance of a signature-based NIDS such as Snort. As a case study of the framework, we implement an enhanced filter mechanism (shortly EFM) that consists of three major components: a context-aware blacklist-based packet filter, an exclusive signature matching component and a KNN-based false alarm filter. In particular, the component of context-aware blacklist-based packet filter is responsible for filtering out network packets in terms of IP reputation. The exclusive signature matching component is implemented in the context-aware blacklist-based packet filter and aims to speed up the signature matching. At last, the component of KNN-based false alarm filter is responsible for filtering out false alarms which are produced by the context-aware blacklist-based packet filter and the NIDS. In the evaluation, the experimental results demonstrate that our framework is promising and by deploying with the EFM, the performance of a signature-based NIDS such as Snort can be improved in the aspects of network packet filtration, signature matching improvement and false alarm reduction.",,2013.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
d15db8c76050e50bcac5cb4cb0b45deb85574606,https://www.semanticscholar.org/paper/d15db8c76050e50bcac5cb4cb0b45deb85574606,Automated Scalability of Cloud Services and Jobs,"Many scientific and commercial applications require access to computation, data or networking resources based on dynamically changing requirements. Users and providers both require these applications or services to dynamically adjust to fluctuations in demand and serve end-users at required quality of service (performance, reliability, security, etc.) and at optimized cost. This may require resources of these applications or services to automatically scale up or down. 
The European funded COLA (Cloud Orchestration at the Level of Application) project aims to design and develop a generic framework that supports automated scalability of a large variety of applications. Learning from previous similar efforts and with the aim of reusing existing open source technologies wherever possible, COLA elaborated a modular architecture called MiCADO (Microservices-based Cloud Application-level Dynamic Orchestrator) [1] that provides optimized deployment and run-time orchestration for cloud applications. 
 
MiCADO is built from well-defined building blocks implemented as microservices. This modular design supports various implementations where components can be replaced relatively easily with alternative technologies. The generic, technology independent architecture diagram of MiCADO is represented in Figure 1. Building blocks, both on the MiCADO Master and also on the MiCADO Worker Nodes are implemented as microservices. The current implementation uses widely applied technologies, such as Docker Swarm as Container Orchestrator [2], Occopus as Cloud Orchestrator [3], and Prometheus [4] as the Monitoring System. 
 
The user facing interface of MiCADO is a TOSCA (Topology and Orchestration Specification for Cloud Applications, an OASIS standard) [5] based description of the desired topology and its associated scalability and security policies. This interface can then be embedded to existing GUIs, custom web interfaces or science gateways. 
 
The first prototype implementations of MiCADO show promising results on various application types. The two main targeted application categories are cloud-based services where scalability is achieved by scaling up or down the number of containers and virtual machines based on load, performance and cost, and the execution of a large number of (typically parameter sweep style) jobs where a certain number of these jobs need to be executed by a set deadline. 
Direct involvement of industry partners assures that the results of COLA are prototyped on real application scenarios. Three near production quality demonstrators and twenty further proof of concept case studies are being implemented using MiCADO and demonstrating its applicability in case of both service and job type scalability. Some of the applications prototyped are directly related to services utilized in science gateways, such as the Data Avenue service of WS-PGRADE [6].",,2018.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
5b320308332d871f2bf7c02ab3b9f879edd8c7fc,https://www.semanticscholar.org/paper/5b320308332d871f2bf7c02ab3b9f879edd8c7fc,Improving Ambulance Dispatching with Machine Learning and Simulation,". As an industry where performance improvements can save lives, but resources are often scarce, emergency medical services (EMS) providers continuously look for ways to deploy available resources more eﬃciently. In this paper, we report a case study executed at a Dutch EMS region to improve ambulance dispatching. We ﬁrst capture the way in which dispatch human agents currently make decisions on which ambulance to dispatch to a request. We build a decision tree based on historical data to learn human agents’ dispatch decisions. Then, insights from the ﬁtted decision tree are used to enrich the commonly assumed closest-idle dispatch policy. Subsequently, we use the captured dispatch policy as input to a discrete event simulation to investigate two enhancements to current practices and evaluate their performance relative to the current policy. Our results show that complementing the current dispatch policy with redispatching and reevaluation policies yields an improvement of the on-time performance of highly urgent ambulance requests of 0.77 percentage points. The performance gain is signiﬁcant, which is equivalent to adding additional seven weekly ambulance shifts.",,2021.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
095aeceba84ba188a2b3bb6e086d3755f11e824a,https://www.semanticscholar.org/paper/095aeceba84ba188a2b3bb6e086d3755f11e824a,"Using Augmented Intelligence to Automate Subsea Inspection Data Acquisition, Processing, Analysis, Reporting and Access","
 Augmented Intelligence (AI2) involves fusing Analyst Intuition with Artificial Intelligence to deliver an optimised combination of human-machine decision support.
 AI2 is being incorporated by i-Tech Services / Leidos into the physical inspection of offshore Oil, Gas, and Renewables assets, delivering valuable data driven insights that contribute to greater efficiency, enhanced condition monitoring, improved asset integrity and asset life extension.
 The deployment of vehicular and diver assets to obtain such inspection data, with associated support vessels, remains a major cost challenge for Operators.
 We believe the industry needs to approach this challenge from two key directions. Firstly, through the application of autonomous systems for data acquisition and delivery, reducing vessel reliance, and secondly through automating the acquisition and processing of data and maximising the insight provided by the data.
 This paper will examine the use of Augmented Intelligence to optimise the Subsea Inspection data workflow as a key use case, to demonstrate the principles.
 The historic paradigm consists of a fragmented evolving approach, with insufficient consideration and design across all the sensors, processing analytical engines and data visualisation. The approach being adopted is to closely link all aspects of the data workflow, within the context of delivering the data and beyond in terms of harvesting additional insight and value.
 To achieve the optimum workflow a number of developmental initiatives are being knitted into a modular platform, each element providing standalone value but the sum of the parts generates the most significant value and cost reduction.
 The elements being combined are automatic data quality control at acquisition source and through the full workflow, automated processing, machine vision for object recognition and reporting and machine learning to optimise the system intelligence. All of these are designed to augment the expertise of the analyst / user, detecting change to learnt parameters, by using real time data and critically by referencing large historical data sets and as-built data.
 The outputs from a system holistic approach will be improved data acquisition with more efficient high quality right first time data reporting. In addition layers of analytics, with smart, intuitive data access and retrieval will optimise delivery of key information within large data sets, together with maximising value and insight.","Day 2 Tue, May 07, 2019",2019.0,10.4043/29335-MS,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
6ea59f72f3239e524b7fb5d09eaf2fc9638cb018,https://www.semanticscholar.org/paper/6ea59f72f3239e524b7fb5d09eaf2fc9638cb018,Ai Technology Achieving General Purpose Ai That Can,"Artificial IntelligenceArtificial Intelligence in Cyber Security: Impact and ImplicationsTechnological Progress, Artificial Intelligence, and Inclusive GrowthEnergy Research AbstractsCan Artificial Intelligence ImproveHandbook of Pharmaceutical Granulation TechnologyArtificial Intelligence for BusinessArtificial Intelligence in EducationThe Democratization of Artificial IntelligenceAdvances in Artificial Intelligence, Software and Systems EngineeringFinancing Our FutureProceedings of the Future Technologies Conference (FTC) 2021, Volume 1Artificial IntelligenceNew Technologies in Dermatological Science and PracticeArtificial IntelligenceGame Theory and Machine Learning for Cyber SecurityRegulatory Aspects of Artificial Intelligence on BlockchainConcise Encyclopedia of Software EngineeringEuropean Artificial Intelligence (AI) Leadership, the Path for an Integrated VisionAI In The Age Of Cyber-DisorderReadings in Artificial Intelligence and DatabasesHuman decisionsConstitution 3.0Artificial Intelligence and IoT-Based Technologies for Sustainable Farming and Smart AgricultureArtificial Intelligence and Deep Learning for Decision MakersThe Regional Economics of Technological TransformationsArchitects of IntelligenceArtificial IntelligenceArtificial Intelligence and Integrated Intelligent Information SystemsECIAIR 2019 European Conference on the Impact of Artificial Intelligence and Robotics Intelligence UnboundHow to Achieve Inclusive GrowthArtificial Intelligence for Business OptimizationArtificial Intelligence in SocietyThe Myth of Artificial IntelligenceThe Digital Innovation RaceAI-First HealthcareProject Management Best Practices: Achieving Global ExcellenceConnected WorldBiotechnology: Concepts, Methodologies, Tools, and Applications The interaction of database and AI technologies is crucial to such applications as data mining, active databases, and knowledge-based expert systems. This volume collects the primary readings on the interactions, actual and potential, between these two fields. The editors have chosen articles to balance significant early research and the best and most comprehensive articles from the 1980s. An in-depth introduction discusses basic research motivations, giving a survey of the history, concepts, and terminology of the interaction. Major themes, approaches and results, open issues and future directions are all discussed, including the results of a major survey conducted by the editors of current work in industry and research labs. Thirteen sections follow, each with a short introduction. Topics examined include semantic data models with emphasis on conceptual modeling techniques for databases and information systems and the integration of data model concepts in high-level data languages, definition and maintenance of integrity constraints in databases and knowledge bases, natural language front ends, object-oriented database management systems, implementation issues such as concurrency control and error recovery, and representation of time and knowledge incompleteness from the viewpoints of databases, logic programming, and AI.The artificial intelligence (AI) landscape has evolved significantly from 1950 when Alan Turing first posed the question of whether machines can think. Today, AI is transforming societies and economies. It promises to generate productivity gains, improve well-being and help address global challenges, such as climate change, resource scarcity and health crises.As technology continues to saturate modern society, agriculture has started to adopt digital computing and data-driven innovations. This emergence of “smart” farming has led to various advancements in the field, including autonomous equipment and the collection of climate, livestock, and plant data. As connectivity and data management continue to revolutionize the farming industry, empirical research is a necessity for understanding these technological developments. Artificial Intelligence and IoT-Based Technologies for Sustainable Farming and Smart Agriculture provides emerging research exploring the theoretical and practical aspects of critical technological solutions within the farming industry. Featuring coverage on a broad range of topics such as crop monitoring, precision livestock farming, and agronomic data processing, this book is ideally designed for farmers, agriculturalists, product managers, farm holders, manufacturers, equipment suppliers, industrialists, governmental professionals, researchers, academicians, and students seeking current research on technological applications within agriculture and farming.This book constitutes the refereed proceedings of the Second International Conference, SLAAI-ICAI 2018, held in Moratuwa, Sri Lanka, in December 2018. The 32 revised full papers presented were carefully reviewed and selected from numerous submissions. The papers are organized in the following topical sections: ?intelligence systems; neural networks; game theory; ontology engineering; natural language processing; agent based system; signal and image processing.After a long time of neglect, Artificial Intelligence is once again at the center of most of our political, economic, and socio-cultural debates. Recent advances in the field of Artifical Neural Networks have led to a renaissance of dystopian and utopian speculations on an AI-rendered future. Algorithmic technologies are deployed for identifying potential terrorists through vast surveillance networks, for producing sentencing guidelines and recidivism risk profiles in criminal justice systems, for demographic and psychographic targeting of bodies for advertising or propaganda, and more generally for automating the analysis of language, text, and images. Against this background, the aim of this book is to discuss the heterogenous conditions, implications, and effects of modern AI and Internet technologies in terms of their political dimension: What does it mean to critically investigate efforts of net politics in the age of machine learning algorithms?Intelligence Unbound explores the prospects, promises, and potential dangers of machine intelligence and uploaded minds in a collection of stateof-the-art essays from internationally recognized philosophers, AI researchers, science fiction authors, and theorists. Compelling and intellectually sophisticated exploration of the latest thinking on Artificial Intelligence and machine minds Features contributions from an international cast of philosophers, Artificial Intelligence researchers, science fiction authors, and more Offers current, diverse perspectives on machine intelligence and uploaded minds, emerging topics of tremendous interest Illuminates the nature and ethics of tomorrow’s machine minds—and of the convergence of humans and machines—to consider the pros and cons of a variety of intriguing possibilities Considers classic philosophical puzzles as well as the latest topics debated by scholars Covers a wide range of viewpoints and arguments regarding the prospects of uploading and machine intelligence, including proponents and skeptics, pros and consCompanies that don't use AI to their advantage will soon be left behind. Artificial intelligence and machine learning will drive a massive reshaping of the economy and society. What should you and your company be doing right now to ensure that your business is poised for success? These articles by AI experts and consultants will help you understand today's essential thinking on what AI is capable of now, how to adopt it in your organization, and how the technology is likely to evolve in the near future. Artificial Intelligence: The Insights You Need from Harvard Business Review will help you spearhead important conversations, get going on the right AI initiatives for your company, and capitalize on the opportunity of the machine intelligence revolution. Catch up on current topics and deepen your",,2022.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
441ddc05abb939e25b21065414d2fe9660d53635,https://www.semanticscholar.org/paper/441ddc05abb939e25b21065414d2fe9660d53635,RPA for Telcos: The Next Wave of BPM Evolution,"As various disruptive digital technologies continue to upend the traditional business models and trigger unprecedented changes in customer behavior, the telecommunication industry must fundamentally reimagine its operations. A swift modernization of existing business processes and cost-prohibitive systems is vital for wireless carriers to capitalize on the emerging opportunities to grow revenues in the digital era.<br><br>Over the years, several inefficiencies have crept into processes across the sector. For instance, the typical operations landscape of many communications services providers (CSPs) continues to be weighed down by disparate systems, siloed processes, and highly labor-intensive activities that hinder business agility. This, in turn, adversely impacts telcos’ ability to effectively respond to the rapidly evolving customer demands, and diminishes their ability to compete effectively with new-age cross-industry players. Unless the industry streamlines its operations first, any economies of scale and transformation benefits that may result from digitization initiatives will at best be suboptimal. Specifically, telcos need to evaluate their existing IT infrastructure and application systems in view of the dynamic business priorities. Advanced technologies such as automation, cognitive computing, and artificial intelligence (AI) will help wireless carriers effectively transform their core operations, and realize business outcomes faster for higher return on investment (ROI). From an operational standpoint, this would translate into telcos’ manual, time-consuming, and expensive processes being replaced with automated software solutions that are more reliable and agile. To be fair to CSPs, they embraced process automation more than a decade ago. Many telcos have since launched several business process management (BPM) projects to eliminate manual intervention in different functions spanning their operations support systems (OSS) and business support systems (BSS). Be it for optimizing the workflows underpinning network inventory and configuration and service provisioning, or automating processes concerning revenue and order management, operators have aggressively adopted BPM. <br><br>Where does then the new wave of process automation, enabled by the likes of AI and machine learning, fit in? Does this technology stack–known as robotic process automation (RPA)–require a standalone deployment, or can it be integrated with the BPM implementation in a non-intrusive way? And, what tangible business benefits can telcos accrue from these investments?",,2017.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
b2b8efc1ef1cd6248119e3503c43a352ec9dc973,https://www.semanticscholar.org/paper/b2b8efc1ef1cd6248119e3503c43a352ec9dc973,Real-Time Event-Driven Learning in Highly Volatile Systems: A Case for Embedded Machine Learning for SCADA Systems,"Extracting key system parameters and their impact on state transition is a necessity for knowledge and data engineering. In Decision Support Systems, the quest for yet more efficient and faster methods of sensitivity analysis (SA) and feature extraction in complex and volatile systems persists. A new improved event tracking methodology, the fastTracker, for real-time SA in large scale complex systems is proposed in this paper. The main feature of fastTracker is its high-frequency analytics using meager computational cost. It is suitable for data processing and prioritization in embedded systems, Internet of Things (IoT), distributed computing (e.g. Edge computing) applications. The presented algorithm’s underpinning rationale is event driven; its objective is to correctly and succinctly quantify the sensitivity of observable changes in the system (output) with respect to the input variables. To demonstrate the performance of the proposed fastTracker methodology, fastTracker was deployed in the Supervisory control and data acquisition (SCADA) system from real cement industry. fastTracker has been verified by system experts in real industrial application. Its performance was compared with other real-time event-based SA techniques. The comparison revealed savings of 98.8% in processing time per sensitivity index and 20% in memory usage when compared with EventTracker, its closest rival. The proposed methodology is more accurate and 80.9% faster than an entropy-based method. Its application is recommended for reinforced learning and/or formulating system key performance indicators from raw data.",IEEE Access,2022.0,10.1109/access.2022.3173376,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
d0f37dfa4d0ad91a0fc5957fa7b1c7a96015a791,https://www.semanticscholar.org/paper/d0f37dfa4d0ad91a0fc5957fa7b1c7a96015a791,Advances of Straddle Packer Microfrac Stress Measurements for Reservoir Development – The Pursuit of Subsurface Tectonic Strain Behavior,"
 The objective of this work is to highlight wireline straddle-packer microfrac testing is an underutilized technology today by the oil and gas industry despite these tests have evolved significantly in the last 10 years. This work also summaries the technological improvements and latest advances of microfrac service deployment in addition to share the future of in-situ reservoir stress monitoring from fiber-optic Distributive Strain Sensing (DSS).
 Over 500 microfrac tests and more than 30 decades of stress testing data are compiled and analyzed from science and data-collection pilot wells drilled around the world. The number of pressure tests collected by the industry is estimated by Baker Hughes’ database and competitor’s market share to compare the substantial difference between the number of reservoir pressure points and microfrac stress test collected every year for the last decade. Machine learning algorithms predict tectonic strain values to match microfrac formation breakdown and fracture closure using basic rock elastic properties to calculate the static stiffness of the formations where the stress tests are obtained.
 The microfrac success rate has increased from 20% to 85% in the last decade thanks to upgraded straddle packer tool capabilities and improved operational practices. The formation breakdown pressure data consistently indicates higher level of uncertainty than reservoir pore pressure. However, the industry collects several orders of magnitude more pore pressure points than microfrac stress tests every year. Possibly, this is the consequence of using basic effective in-situ stress ratio models by geomechanics practitioners that requires few calibration points from leak-off tests or borehole breakout modelling. This practice could treat microfracs as a nice-to-have calibration data rather than an essential subsurface tectonic stress information. A significant increase in microfrac testing is observed during the US shale gas revolution in order to calibrate stress profile models where basic effective stress ratio models failed to predict a lithology-dependent stress contrast between pay and non-pay intervals. The data shows the importance of using microfrac tests to calibrate subsurface tectonic strain values and predict accurate hydraulic fracture containment.
 The predicted tectonic strain data from microfrac testing shows values between 0.05 to 1.2 mStrain which can also be detected with current fiber optic technology using two centimeter grading and capable of detecting two micrometers of deformation. This new distributed strain sensing technology can be implemented to detect changes of stress and strain as the reservoir is developed by producer and injector wells. This technology may be the future of stress monitoring at the reservoir scale.",,2020.0,10.2523/iptc-20257-ms,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
900a66badbb50fbf52af7517ce56a2c7d67f9829,https://www.semanticscholar.org/paper/900a66badbb50fbf52af7517ce56a2c7d67f9829,DESIGN AND ANALYSIS OF A PAVEMENT MARKER DETECTION SYSTEM,"OF THESIS DESIGN AND ANALYSIS OF A PAVEMENT MARKER DETECTION SYSTEM Personal injuries and property damage due to the failure of snow-plowable pavement markers which detach from pavement surfaces has led to the development of new all-plastic pavement markers which are located entirely below the planar surface of the pavement. The new all-plastic design pushes existing solutions used to avoid striping over highway reflectors into obsolescence since current solutions operate using electromagnets to sense the metal housings of snow-plowable pavement markers. A replacement solution is currently sought by the highway maintenance industry and three different marker detection methods were developed and tested on real-world highways with both new and aging pavement markers to find that optimal solution. With the developed technologies accruing 106,038 observed data points, it is clear that the ideal solution to marker detection and avoidance is the deployment of a machine vision system operating on a deep learning trained model optimized for the detection of differing types of pavement markers on various pavement surfaces. The machine vision system can be further improved in several areas, the most important of which is the optimization model’s processing speeds such that the system could operate at highway speeds while providing real-time analysis of the integrity of installed pavement markers.",,2020.0,10.13023/ETD.2020.362,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
27507038d7b2f709c7fe5af83eee6460f3d70d6f,https://www.semanticscholar.org/paper/27507038d7b2f709c7fe5af83eee6460f3d70d6f,A Framework of Best Practices for Delivering Successful Artificial Intelligence Projects. A Case Study Demonstration,"
 A practical framework that outlines the critical steps of a successful process that uses data, machine learning (Ml), and artificial intelligence (AI) is presented in this study. A practical case study is included to demonstrate the process. The use of artificial intelligent and machine learning has not only enhanced but also sped up problem-solving approaches in many domains, including the oil and gas industry. Moreover, these technologies are revolutionizing all key aspects of engineering including; framing approaches, techniques, and outcomes. The proposed framework includes key components to ensure integrity, quality, and accuracy of data and governance centered on principles such as responsibility, equitability, and reliability. As a result, the industry documentation shows that technology coupled with process advances can improve productivity by 20%.
 A clear work-break-down structure (WBS) to create value using an engineering framework has measurable outcomes. The AI and ML technologies enable the use of large amounts of information, combining static & dynamic data, observations, historical events, and behaviors. The Job Task Analysis (JTA) model is a proven framework to manage processes, people, and platforms. JTA is a modern data-focused approach that prioritizes in order: problem framing, analytics framing, data, methodology, model building, deployment, and lifecycle management. The case study exemplifies how the JTA model optimizes an oilfield production plant, similar to a manufacturing facility. A data-driven approach was employed to analyze and evaluate the production fluid impact during facility-planned or un-planned system disruptions. The workflows include data analytics tools such as ML&AI for pattern recognition and clustering for prompt event mitigation and optimization.
 The paper demonstrates how an integrated framework leads to significant business value. The study integrates surface and subsurface information to characterize and understand the production impact due to planned and unplanned plant events. The findings led to designing a relief system to divert the back pressure during plant shutdown. The study led to cost avoidance of a new plant, saving millions of dollars, environment impact, and safety considerations, in addition to unnecessary operating costs and maintenance. Moreover, tens of millions of dollars value per year by avoiding production loss of plant upsets or shutdown was created. The study cost nothing to perform, about two months of not focused time by a team of five engineers and data scientists. The work provided critical steps in ""creating a trusting"" model and ""explainability’. The methodology was implemented using existing available data and tools; it was the process and engineering knowledge that led to the successful outcome. Having a systematic WBS has become vital in data analytics projects that use AI and ML technologies. An effective governance system creates 25% productivity improvement and 70% capital improvement. Poor requirements can consume 40%+ of development budget. The process, models, and tools should be used on engineering projects where data and physics are present.
 The proposed framework demonstrates the business impact and value creation generated by integrating models, data, AI, and ML technologies for modeling and optimization. It reflects the collective knowledge and perspectives of diverse professionals from IBM, Lockheed Martin, and Chevron, who joined forces to document a standard framework for achieving success in data analytics/AI projects.","Day 2 Wed, September 22, 2021",2021.0,10.2118/206014-ms,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
1ab1617ec95700f6dd5b313f4175d44dcf63903d,https://www.semanticscholar.org/paper/1ab1617ec95700f6dd5b313f4175d44dcf63903d,COLOR-BASED OPTICAL CAMERA COMMUNICATION USING DEEP LEARNING,"When a network is not available or allowed (e.g., air gap), all benefits of digital communication and the Internet disappear. To enable substantial data transmission, techniques are described herein for using Machine Learning to embed data into color images and to read it with camera-equipped devices. In addition to data transmission, this enables new applications such as embedding of content on packaging (instructions or runnable code and configurations directly on devices), physical documents with selective encryption (role-based permission to read parts of a document), protection from digital tracking, prevention of tampering on physical documents, and more. DETAILED DESCRIPTION Passing information to a device without cabled or radio-based wireless connectivity is an ongoing problem. One approach is to use a different bands of the radio spectrum, such as visible light. This is the approach taken in the fields of Visible Light Communication (VLC) and Optical Camera Communication (OCC). VLC has emerged as a promising technology for wireless communication owing to advantages such as bandwidth, license, coexistence, and security. However, VLC requires an active source (i.e., emitting light) to modulate the signals. On the other hand, OCC can also operate on passive sources, that is, data printed on pictures, such as Quick-Response (QR) codes. A QR code® is a type of two-dimensional matrix barcode first designed for the automotive industry in Japan. A barcode is a machine-readable optical label that contains information, typically about the item to which it is attached. A QR code uses four standardized encoding modes (numeric, alphanumeric, byte/binary, and Kanji) to efficiently store data and consists of black squares arranged in a square grid on a white background, which can be read by an imaging device such as a camera and processed using Reed-Solomon error correction until the image can be appropriately interpreted. The required data is then extracted from patterns that are present in both horizontal and vertical 2 Rohrseitz: COLOR-BASED OPTICAL CAMERA COMMUNICATION USING DEEP LEARNING Published by Technical Disclosure Commons, 2018 2 5660 components of the image. QR codes became popular outside the automotive industry due to their fast readability and greater storage capacity compared to standard Universal Product Code (UPC) barcodes. Applications include product tracking, item identification, time tracking, document management, and general marketing. The amount of data a standard QR code can carry is limited by the number of its pixels. To increase the payload, the number of pixels needs to be increased: either by making the QR code bigger or by making the pixels smaller. Both options are constrained by the measurement devices: to read more data, either the field of view or the pixel resolution must be larger. Another approach to increase the payload is to increase the amount of data per pixel. This can be accomplished by using more than the two black and white states, for instance greyscale and color. The more shades that can be robustly measured, the larger the payload. Accurately measuring color requires robustness to variations due to incoming light. Without compensation, the intensity and the color of the light source modify what the photoreceptors (“pixels”) of a camera can measure. This problem of “color constancy” remains largely unsolved by technical systems. Even though biological vision achieves color constancy, its precise mechanisms are not yet fully understood. Functionally, it is clear that there is an evolutionary advantage in maintaining colors constant in forests speckled by light or at different times of day. To mitigate for this effect, current techniques use a reduced color palette, for instance four colors. Given the ubiquity of QR codes, the application potential of using more colors to increase bandwidth in optical wireless transmission information is vast. For one, when standard networking technologies are unavailable or not allowed (e.g., air gaps), the enormous societal and business benefits of digital communication and the Internet disappear. Accurately measuring color would enable networking using, for instance, QR codes, pictures, or electronic displays as data emitters and cameras or other measurement devices as receivers. Such a communication system also carries important technical benefits in terms of latency and availability. The techniques described herein use a deep neural network trained specifically to measure color in the context of data transmission. Although some current techniques relevant to OCC make use of neural networks, their role is not to measure color. Empirical 3 Defensive Publications Series, Art. 1391 [2018] https://www.tdcommons.org/dpubs_series/1391 3 5660 results indicate very high accuracy (98%) in a task that had not yet been solved by a technical system. The deep neural network is capable of accurately measuring the color of a region of the field of view by including in the processing the adjacent areas (in contrast, cameras are designed to convey only the measured color of one specific region, the “pixel”). A Machine Learning process prior to deployment determines which locations and calculations the neural network needs to perform. To create the deep neural network capable of accurately measuring color, a workflow of three high-level steps is carried out: (1) creation of a relevant dataset, (2) training of the deep neural network, and (3) deployment on a camera-carrying device. The first step involves acquiring a most complete and diverse dataset. To be robust against the main problem of accurately measuring color namely changes in illumination a deep neural network is trained on a dataset that contains multiple pictures of the colors to be recognized, each under different light intensities and colors. The dataset is stored with metadata that describe the characteristics to be learned. In the second step, an appropriate deep learning architecture is designed and parametrized. It is typical of Machine Learning to have many variants of architecture and parameters that lead to a similarly effective systems. The dataset may also be different and still be effective. It is the practitioner’s role to find the right combination to achieve the intended goals. The third step involves transferring the learned model to a deployment system for inference (for instance, a smartphone or an industrial measurement system), whose camera characteristics must match those of the camera used for acquiring the dataset. For experimental verification, a dataset was created, the neural network trained, and a measurement application implemented on a mobile device. To capture the dataset, an array of colored elements were placed under a camera, surrounded by a set of programmable color light bulbs. An application was run which at fixed time intervals (two seconds) automatically changes the values of hue, brightness, and saturation by predefined amounts, while capturing pictures with the camera. To capture the metadata, the standard procedure was followed of storing each picture file in a folder corresponding to the incoming light characteristics. 4 Rohrseitz: COLOR-BASED OPTICAL CAMERA COMMUNICATION USING DEEP LEARNING Published by Technical Disclosure Commons, 2018 4 5660 To train the neural network on the dataset, the following characteristics were used:  Type: convolutional neural network  Activation function: Rectified Linear Unit (ReLU); If input <= 0, then output = 0, otherwise if input >0, then output = input)  Optimizer: Adam (ADAptive with Momentum https://arxiv.org/abs/1412.6980)  Loss function: categorical crossentropy (quantifies the difference between two probability distributions – http://deeplearning.net/software/theano/library/tensor/nnet/nnet.html#theano.tenso r.nnet.nnet.categorical_crossentropy)",,2018.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
f5792d3c5e53f406b1aee6aae4505792b23bfcc0,https://www.semanticscholar.org/paper/f5792d3c5e53f406b1aee6aae4505792b23bfcc0,Dynamic Placement of Rapidly Deployable Mobile Sensor Robots Using Machine Learning and Expected Value of Information,"
 Although the Industrial Internet of Things has increased the number of sensors permanently installed in industrial plants, there will be gaps in coverage due to broken sensors or sparse density in very large plants, such as in the petrochemical industry. Modern emergency response operations are beginning to use Small Unmanned Aerial Systems (sUAS) that have the ability to drop sensor robots to precise locations. sUAS can provide longer-term persistent monitoring that aerial drones are unable to provide. Despite the relatively low cost of these assets, the choice of which robotic sensing systems to deploy to which part of an industrial process in a complex plant environment during emergency response remains challenging.
 This paper describes a framework for optimizing the deployment of emergency sensors as a preliminary step towards realizing the responsiveness of robots in disaster circumstances. AI techniques (Long short-term memory, 1-dimensional convolutional neural network, logistic regression, and random forest) identify regions where sensors would be most valued without requiring humans to enter the potentially dangerous area. In the case study described, the cost function for optimization considers costs of false-positive and false-negative errors. Decisions on mitigation include implementing repairs or shutting down the plant. The Expected Value of Information (EVI) is used to identify the most valuable type and location of physical sensors to be deployed to increase the decision-analytic value of a sensor network. This method is applied to a case study using the Tennessee Eastman process data set of a chemical plant, and we discuss implications of our findings for operation, distribution, and decision-making of sensors in plant emergency and resilience scenarios.","Volume 13: Safety Engineering, Risk, and Reliability Analysis; Research Posters",2021.0,10.1115/imece2021-70759,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
13b2e03de3e7e305a96d495b7367a31f461da1de,https://www.semanticscholar.org/paper/13b2e03de3e7e305a96d495b7367a31f461da1de,A Support Vector Machine-based Prediction Model for Electrochemical A Support Vector Machine-based Prediction Model for Electrochemical Machining Process Machining Process,"Abstract Manufacturing of quality products is one of the core measures to address competitiveness in industries. Hence, it is always necessary to accomplish quality prediction at early stages of a manufacturing process to attain high quality products at the minimum possible cost. To achieve this goal, the past researchers have developed and investigated the applications of different intelligent techniques for their effective deployment at various stages of manufacturing processes. In this paper, support vector machine (SVM), a supervised learning system based on a novel artificial intelligence paradigm, is employed for prediction of three responses, like material removal rate, surface roughness and radial overcut during an electrochemical machining (ECM) operation. Gaussian radial basis kernel function is adopted in this algorithm to provide higher prediction accuracy. Regression analyses are also carried out to validate the effectiveness of these prediction models. The SVM-based results show good agreement between the experimental and predicted response values as compared to linear and quadratic models. Among the four ECM process parameters, i.e. applied voltage, tool feed rate, electrolyte concentration and percentage of reinforcement of B 4 C particles in the metal matrix, tool feed rate is identified having the maximum influence on the considered responses.",,,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
c9b43fcd195729f73e547287f73b44d1f55d9046,https://www.semanticscholar.org/paper/c9b43fcd195729f73e547287f73b44d1f55d9046,Automated game testing using computer vision methods,"Video game development is a growing industry nowadays with high revenues. However, even if there are many resources invested in the software development process, many games still contain bugs or performance issues that affect the user experience. This paper presents ideas on how computer vision methods can be used to automate the process of game testing. The goal is to replace the parts of the testing process that require human users (testers) with machines as much as possible, in order to reduce costs and perform more tests in less time by scaling with hardware resources. The focus is on solving existing real-world problems that have emerged from several discussions with industry partners. We base our methods on previous work in this area using intelligent agents playing video games and deep learning methods that interpret feedback from their actions based on visual output. The paper proposes several methods and a set of open-source tools, independent of the operating system or deployment platform, to evaluate the efficiency of the presented methods.",2021 36th IEEE/ACM International Conference on Automated Software Engineering Workshops (ASEW),2021.0,10.1109/ASEW52652.2021.00024,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
61efff04bf68fce3384b7daa0379338297552310,https://www.semanticscholar.org/paper/61efff04bf68fce3384b7daa0379338297552310,Closed Loop Paging Optimization for Efficient Mobility Management,"The 4G/5G networks deploy conventional Tracking Area Update and multi-step paging procedures for mobility management. The paging procedure consumes significant amount of licensed spectrum resources. The signaling overhead is going to worsen further with the increasing use of small cells and higher user mobility speed. To address this challenge, the telecommunication industry is embracing closed loop approaches to predict user mobility patterns. Though the mobility pattern prediction is a known problem, most of the existing solutions apply it for enhancing the handover management and use academic dataset. Furthermore, limited research has been done on idle-state users. In this paper, we propose a Closed Loop Paging (CLOP) optimization solution using semi-supervised learning model to reduce paging overhead. We harness the real network dataset to analyze the location trail of users in the network to predict a subset of Base Stations for paging to locate idle-state users. Our empirical results demonstrate that Linear Support Vector Machine (L-SVM) classification method excels when compared to other supervised learning models. The L-SVM Classifier saves nearly 43% of the paging overhead with a marginal increase in connection establishment delay by around 7.3%.",2021 IEEE 18th Annual Consumer Communications & Networking Conference (CCNC),2021.0,10.1109/CCNC49032.2021.9369589,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
952ec800bb69063afb0abba7c1f446d21a9475d1,https://www.semanticscholar.org/paper/952ec800bb69063afb0abba7c1f446d21a9475d1,Modular approach to data preprocessing in ALOHA and application to a smart industry use case,"Applications in the smart industry domain, such as interaction with collaborative robots using vocal commands or machine vision systems often requires the deployment of deep learning algorithms on heterogeneous low power computing platforms. The availability of software tools and frameworks to automatize different design steps can support the effective implementation of DL algorithms on embedded systems, reducing related effort and costs. One very important aspect for the acceptance of the framework, is its extensibility, i.e. the capability to accommodate different datasets and define customized preprocessing, without requiring advanced skills. The paper addresses a modular approach, integrated into the ALOHA 1 tool flow, to support the data preprocessing and transformation pipeline. This is realized through customizable plugins and allows the easy extension of the tool flow to encompass new use cases. To demonstrate the effectiveness of the approach, we present some experimental results related to a keyword spotting use case and we outline possible extensions to different use cases. Keywords— Deep Learning, flows and tools, computer-aided design, edge computing.",ArXiv,2021.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
10108cb0a7eaac31459fb7d424729fd1406c43bf,https://www.semanticscholar.org/paper/10108cb0a7eaac31459fb7d424729fd1406c43bf,Alternatively Deployed Artificial Lift System for Deepwater Subsea Operations,"
 In the Gulf of Mexico, the rapid pressure depletion and reservoir depth of the Lower Tertiary intervals lead to low oil recovery. A high-reliability, through-tubing subsea electrical submersible pump (ESP) system that takes an integrated approach to production optimization will enable producers to cost-effectively extract more hydrocarbons from the increasingly challenging reservoirs in today's subsea assets. The potential increase in production depends on the maximum drawdown pressure limitations of both well casing design and rock strength. ESPs in deepwater fields are also considered to be an enhancer rather than an enabler by extending the production plateau 5 to 8 years after initial well/field startup with natural flow and seabed boosting. Hence, a robust ESP system that can be installed and operated a few years after field startup without a workover for replacing the upper completions. A robust, reliable ESP would unlock additional value to deepwater operators by delaying CAPEX and eliminating ESP failures, such as degradation of components due to high-pressure/high-temperature (HP/HT) cycling, during the first few years of nonoperation.
 Designing ESPs for deepwater application is a multidisciplinary challenge and needs to be approached from a full system-reliability standpoint rather than improvements to the ESP hardware alone. Implementation of ESPs in deep water requires both upfront planning at a full-system level and high degree of flexibility for installation, deployment, and retrieval. Finally, because the impact of an unplanned ESP failure is significantly detrimental to project economics, efforts to improve robustness of the ESP hardware must be complemented with automation of ESP operation to reduce or eliminate operator-induced failures. Recent industry improvements in machine learning and predictive analytics need to be leveraged to implement condition-based monitoring of ESPs to better anticipate failures and plan for replacements and/or adjustments to extend the life of degraded units.
 A collaborative project was undertaken to develop the concept of an alternatively deployed through-tubing ESP (TTESP) system targeted for deepwater subsea operations. The goal was to reduce intervention costs, which, together with ESP run life, are the primary factors influencing the economics of subsea ESPs, including conventionally deployed through-tubing ESPs. The project scope encompassed the downhole hardware, from immediately below the subsea tree through the upper completion, as well as deployment and retrieval equipment and methodology.
 Economic analyses of subsea fields were conducted to identify the factors contributing to intervention costs so that alternatives could be developed. Multiple concepts were evaluated, and the proof-of-concept system was selected based on superior economic return compared with the baseline. During this proof-of-concept phase, significant testing of key technologies was conducted. The studies showed that conventional intervention vessels and methods will not reduce the intervention costs associated with TTESPs. Lighter vessels together with technologies and methods that minimize intervention time and frequency—and, consequently, reservoir damage and deferred production—are the answer. Eliminating the wait for an available offshore rig is also a key factor in improving overall production economics. The proposed alternatively deployed TTESP system and its associated deployment methodology could reduce the intervention time by half and eliminate reservoir damage. This unconventional deployment could be conducted with lighter service vessels, further reducing intervention costs.","Day 2 Tue, May 14, 2019",2019.0,10.2118/194400-MS,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
1def29b65d295562bcc4e6e21d02a962531a5a8b,https://www.semanticscholar.org/paper/1def29b65d295562bcc4e6e21d02a962531a5a8b,Case-study Led Investigation of Explainable AI (XAI) to Support Deployment of Prognostics in the industry,"Civil nuclear generation plant must maximise it’s operational uptime in order to maintain it’s viability. With aging plant and heavily regulated operating constraints, monitoring is commonplace, but identifying health indicators to pre-empt disruptive faults is challenging owing to the volumes of data involved. Machine learning (ML) models are increasingly deployed in prognostics and health management (PHM) systems in various industrial applications, however, many of these are black box models that provide good performance but little or no insight into how predictions are reached. In nuclear generation, there is significant regulatory oversight and therefore a necessity to explain decisions based on outputs from predictive models. These explanations can then enable stakeholders to trust these outputs, satisfy regulatory bodies and subsequently make more effective operational decisions. How ML model outputs convey explanations to stakeholders is important, so these explanations must be in human (and technical domain related) understandable terms. Consequently, stakeholders can rapidly interpret, then trust predictions better, and will be able to act on them more effectively. The main contributions of this paper are: 1. introduce XAI into the PHM of industrial assets and provide a novel set of algorithms that translate the explanations produced by SHAP to text-based human-interpretable explanations; and 2. consider the context of these explanations as intended for application to prognostics of critical assets in industrial applications. The use of XAI will not only help in understanding how these ML models work, but also describe the most important features contributing to predicted degradation of the nuclear generation asset.",PHM Society European Conference,2022.0,10.36001/phme.2022.v7i1.3336,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
63f096677b797ea6b73174ee01df58d56ed41d3d,https://www.semanticscholar.org/paper/63f096677b797ea6b73174ee01df58d56ed41d3d,The Agile Deployment Using Machine Learning in Healthcare Service,,"Proceedings of the International Conference on Paradigms of Computing, Communication and Data Sciences",2020.0,10.1007/978-981-15-7533-4_70,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
954e6ed1d7185bc16bf1d4a8f196eafc95c5998e,https://www.semanticscholar.org/paper/954e6ed1d7185bc16bf1d4a8f196eafc95c5998e,Achieving Operational Efficiencies from a Centralized Alarm Management System,"
 Alarm Management Systems (""AMS"") have been adopted in the oil & gas industry where several benefits were realized. Such as improved panel operator effectiveness, maintaining higher levels of plant uptime and integrity, reducing the number of abnormal situation. Which ultimately leads to higher asset productivity.
 Several OPCO have multiple operational assets/sites that are geographically diverse. Where each asset might have a different Integrated Control System (""ICS"") due to the time and availability of technology at the time of commissioning. Requiring diverse locally implemented AMS.
 A unified CAMS thus reduces time and effort to develop, deploy, and maintain alarm systems and is an essential toolkit for enhanced safe operation of the plant.
 Some sites have multiple plants wuth common pocess control section. The process control enginners needs to visit individual plants access DCS alalrms. By carryinhour corporate alarm management, engibbers at their office PCs have the access to the DCS alarms.
 Implementing CAMS requires the presence of a robust data presence infrastructure in place. Notably a centralized plant information management system, where several real time data points with regards to alarms and operator inputs can be captured.
 A CAMS unifies the approach of how alarm management is conducted in the company. Where a CAMS system generates a set of standard and custom templates that highlight the performance of each operating asset/shift/panel operator. Providing insights into the performance of each asset, efficiency of each operational shift and response of the panel operators. That when addressed, will lead to an overall performance and production of the operational asset.
 With this alarm management data, it can be further enhanced through data analytics to identify areas where operational efficiencies can be achieved. Additionally, the CAMS reduces the times and effort to deploy an alarm management system for any future operating asset expansions.
 CAMS coupled with real time data and Machine learning algorithms, past behaviours of the plant can be correlated, which can then be utilised for future predictions on alarms. This would further enhance our data driven decision-making, and would reduce the dependence on personal driven decisions.
 It can be concluded, that the CAMS is worthy investment for operating companies that have geographical/ICS diverse operational assets.","Day 3 Wed, November 17, 2021",2021.0,10.2118/208034-ms,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
954e6ed1d7185bc16bf1d4a8f196eafc95c5998e,https://www.semanticscholar.org/paper/954e6ed1d7185bc16bf1d4a8f196eafc95c5998e,Achieving Operational Efficiencies from a Centralized Alarm Management System,"
 Alarm Management Systems (""AMS"") have been adopted in the oil & gas industry where several benefits were realized. Such as improved panel operator effectiveness, maintaining higher levels of plant uptime and integrity, reducing the number of abnormal situation. Which ultimately leads to higher asset productivity.
 Several OPCO have multiple operational assets/sites that are geographically diverse. Where each asset might have a different Integrated Control System (""ICS"") due to the time and availability of technology at the time of commissioning. Requiring diverse locally implemented AMS.
 A unified CAMS thus reduces time and effort to develop, deploy, and maintain alarm systems and is an essential toolkit for enhanced safe operation of the plant.
 Some sites have multiple plants wuth common pocess control section. The process control enginners needs to visit individual plants access DCS alalrms. By carryinhour corporate alarm management, engibbers at their office PCs have the access to the DCS alarms.
 Implementing CAMS requires the presence of a robust data presence infrastructure in place. Notably a centralized plant information management system, where several real time data points with regards to alarms and operator inputs can be captured.
 A CAMS unifies the approach of how alarm management is conducted in the company. Where a CAMS system generates a set of standard and custom templates that highlight the performance of each operating asset/shift/panel operator. Providing insights into the performance of each asset, efficiency of each operational shift and response of the panel operators. That when addressed, will lead to an overall performance and production of the operational asset.
 With this alarm management data, it can be further enhanced through data analytics to identify areas where operational efficiencies can be achieved. Additionally, the CAMS reduces the times and effort to deploy an alarm management system for any future operating asset expansions.
 CAMS coupled with real time data and Machine learning algorithms, past behaviours of the plant can be correlated, which can then be utilised for future predictions on alarms. This would further enhance our data driven decision-making, and would reduce the dependence on personal driven decisions.
 It can be concluded, that the CAMS is worthy investment for operating companies that have geographical/ICS diverse operational assets.","Day 3 Wed, November 17, 2021",2021.0,10.2118/208034-ms,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
4a27d3b79988a808df552f207cf9bbce4b3055a1,https://www.semanticscholar.org/paper/4a27d3b79988a808df552f207cf9bbce4b3055a1,Institutionalizing Technologies in South African Universities: The Imperatives in the Fourth Industrial Revolution Era,"This study investigated the essentials for institutionalising technologies for teaching and learning across the three types of universities in South Africa. This was intending to determine the skills required for teaching and learning in the 4IR era. Survey design using structured interviews was adopted for the study. The population consisted of all the Executive Deans/Deans of Faculties in twenty-six (26) South African Universities. A total of twenty-two (22) Universities were purposefully selected to allow fair representation to make the findings generalizable. Seventy (70) Executive Deans/Deans participated in the online data collection via Microsoft Teams and Zoom. Findings revealed that the universities in the Republic deployed fifty-seven different technologies to facilitate their teaching and learning activities, and teaching platforms had been supplemented with new technologies such as WhatsApp, Zoom, and MS Teams. Although resistance had been experienced in the past and most faculties had also not been provided with the institutional guidelines for embedding 4IR in their activities, the teaching staff is competent to moderately competent in the use of existing technologies for teaching because most faculties had provided training in an on-going basis. Improvements in the standard and quality of teaching and learning were observed, and the roles of the industry partners and the community of practice had been highly beneficial. The study concludes that while Universities are settled to adopt blended learning as the strategy to implement the convergence between human and machines in the era of the fourth industrial revolution, there is a need to have a national policy in place that deals with funding (special grant) to plug the gap on the digital divide.",,2021.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
d1c468ca203f770e511d568f6e38b4306c4941ca,https://www.semanticscholar.org/paper/d1c468ca203f770e511d568f6e38b4306c4941ca,Natural Disaster Prediction by Using Image Based Deep Learning and Machine Learning,,Lecture Notes in Networks and Systems,2021.0,10.1007/978-3-030-84760-9_6,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
3855da29f4f6a2abba13ebc7ab4da1c611a37ad4,https://www.semanticscholar.org/paper/3855da29f4f6a2abba13ebc7ab4da1c611a37ad4,Security And Privacy Issues In A Knowledge Management System Epub Download,"Summarizes the current state and upcoming trends within the area of fog computing Written by some of the leading experts in the field, Fog Computing: Theory and Practice focuses on the technological aspects of employing fog computing in various application domains, such as smart healthcare, industrial process control and improvement, smart cities, and virtual learning environments. In addition, the Machine-toMachine (M2M) communication methods for fog computing environments are covered in depth. Presented in two parts—Fog Computing Systems and Architectures, and Fog Computing Techniques and Application—this book covers such important topics as energy efficiency and Quality of Service (QoS) issues, reliability and fault tolerance, load balancing, and scheduling in fog computing systems. It also devotes special attention to emerging trends and the industry needs associated with utilizing the mobile edge computing, Internet of Things (IoT), resource and pricing estimation, and virtualization in the fog environments. Includes chapters on deep learning, mobile edge computing, smart grid, and intelligent transportation systems beyond the theoretical and foundational concepts Explores real-time traffic surveillance from video streams and interoperability of fog computing architectures Presents the latest research on data quality in the IoT, privacy, security, and trust issues in fog computing Fog Computing: Theory and Practice provides a platform for researchers, practitioners, and graduate students from computer science, computer engineering, and various other disciplines to gain a deep understanding of fog computing. How the enabling technologies in 5G as an integral or as a part can seamlessly fuel the IoT revolution is still very challenging. This book presents the state-of-the-art solutions to the theoretical and practical challenges stemming from the integration of 5G enabling technologies into IoTs in support of a smart 5G-enabled IoT paradigm, in terms of network design, operation, management, optimization, privacy and security, and applications. In particular, the technical focus covers a comprehensive understanding of 5G-enabled IoT architectures, converged access networks, privacy and security, and emerging applications of 5G-eabled IoT. This book constitutes the refereed proceedings of the International ECML/PKDD Workshop on Privacy and Security Issues in Data Mining and Machine Learning, PSDML 2010, held in Barcelona, Spain, in September 2010. The 11 revised full papers presented were carefully reviewed and selected from 21 submissions. The papers range from data privacy to security applications, focusing on detecting malicious behavior in computer systems. This timely book provides broad coverage of security and privacy issues in the macro and micro perspective. In macroperspective, the system and algorithm fundamentals of next-generation wireless networks are discussed. In micro-perspective, this book focuses on the key secure and privacy techniques in different emerging networks from the interconnection view of human and cyber-physical world. This book includes 7 chapters from prominent international researchers working in this subject area. This book serves as a useful reference for researchers, graduate students, and practitioners seeking solutions to wireless security and privacy related issues Recent advances in wireless communication technologies have enabled the large-scale deployment of next-generation wireless networks, and many other wireless applications are emerging. The next generation of mobile networks continues to transform the way people communicate and access information. As a matter of fact, next-generation emerging networks are exploiting their numerous applications in both military and civil fields. For most applications, it is important to guarantee high security of the deployed network in order to defend against attacks from adversaries, as well as the privacy intrusion. The key target in the development of next-generation wireless networks is to promote the integration of the",,2022.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
1d81480f1ea5263974bfa900a841860be3d8184b,https://www.semanticscholar.org/paper/1d81480f1ea5263974bfa900a841860be3d8184b,Big Data Statistics Analysis of Computer Intelligence and Machine Learning Technology,"In the recent technological world, Computer technology and internet technology plays a vital role. These technologies are deployed in many industries. In this research, we are going to study the correlation between the complexities of financial reporting and the value of economic statistics in the context of Computer intelligence and information technology. Financial reporting is referred to as an accounting practice that involves processes like documentation & communication of financial activities. The performance over time of the companies is derived from the financial report. Financial reports provide vital information about the company’s financial status and it also helps the company in decision making. Balance sheets, income statements, statements on shareholder equity, cash flow statements, are some of the types of financial reports. It facilitates tracking the cash flow, analyzing shareholder equity, evaluating the assets and liabilities, checking the profitability etc. Economic statistics are referred to us as the branch of applied statistics that deals with the collection and processing of economic data. It also compiles and analyzes the economic data. Financial reports check the money flow for businesses. Economic Statistics checks the larger trends that decide the money & resources flow. Even though, both help in making financial decisions there is some complexity between the two correlated fields. With the help of computer intelligence and internet technology, the complexity is reduced considerably.",2022 6th International Conference on Intelligent Computing and Control Systems (ICICCS),2022.0,10.1109/ICICCS53718.2022.9788198,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
974435ad374206b6107f47f63db707bfda7701f3,https://www.semanticscholar.org/paper/974435ad374206b6107f47f63db707bfda7701f3,INCEpTION - Corpus-based Data Science from Scratch,"In recent years, corpus-based data science has seen rapid adoption both in science and in industry. Developing corpusbased models for text mining from scratch has penetrated a huge number of application fields. This renders common approached to corpus annotation unscalable. Instead machineassisted annotation with a human-in-the loop is becoming crucial for the adoption of NLP by data scientists. INCEpTION [1] is a web-based annotation platform for machine-assisted annotation which provides such a tool. The platform targets users in any domain or application scenario that are in need of text that is annotated with specific categories and relations or linked to knowledge bases. It uses machine learning to provide annotation suggestions including activelearning driven guidance, thus improving annotator efficiency and quality. The modular architecture allows using different external annotation services to provide such suggestions. It supports entity disambiguation and linking, cross-document coreference, as well as fact linking using custom domainspecific RDF-based internal knowledge bases or using local or remote external knowledge bases through SPARQL. Annotation interoperability is ensured through the use of UIMA [2] as well as through the support of various annotation formats including CLARIN TCF [3]. At the level of the annotation scheme, the platform is compatible with the DKPro Core [4] type system facilitating interoperability with many of the NLP tools integrated within DKPro Core. INCEpTION is a multi-user platform. Users assume different roles (e.g. admin, project creator, normal user) on the platform as well as in individual projects (manager, annotator, adjudicator). User authorization can be delegated to an external mechanism users to be authenticated against infrastructure identity providers. This is essential for the deployment of the platform at the level of local or national infrastructures where it is used by users from many different organizations. Being a web-based tool these geographically distributed users can also conveniently collaborate on annotation projects within the platform. Further connectivity with other services is possible through a remote access API compatible with the OpenMinTeD AERO protocol1 that permits the automated setup and management of annotation projects. This allows projects to embed the annotation tool into a larger annotation campaign management",,2018.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
25ee6b6cffcc3ff756bc53c30c3400d44d0fff1a,https://www.semanticscholar.org/paper/25ee6b6cffcc3ff756bc53c30c3400d44d0fff1a,Industry First AI-Powered Fully Automated Safety Observation System Deployed to Global Offshore Fleet,"
 As the oil and gas industry is facing tumultuous challenges, adoption of cutting-edge digital technologies has been accelerated to deliver safer, more efficient operations with less impact on the environment.
 While advanced AI and other digital technologies have been rapidly evolving in many fields in the industry, the HSE sector is playing catch-up. With the increasing complexity of risks and safety management processes, the effective application of data-driven technologies has become significantly harder, particularly for international organizations with varying levels of digital readiness across diverse global operations. Leaders are more cautious to implement solutions that are not fit-for purpose, due to concerns over inconsistencies in rolling out the program across international markets and the impact this may have on ongoing operations.
 This paper describes how the effective application of Artificial intelligence (AI) and Machine Learning (ML) technologies have been used to engineer a solution that fully digitizes and automates the end-to-end offshore behavior-based safety program across a global offshore fleet; optimizing a critical safety process used by many leading oil & gas organization to drive positive workplace safety culture. The complex safety program has been transformed into clear, efficient and automated workflow, with real-time analytics and live transparent dashboards which detail critical safety indicators in real time, aiding decision-making and improving operational performance.
 The novel behavior-based safety digital solution, referred to as 3C observation tool within Noble drilling, has been built to be fully aligned with the organization's safety management system requirements and procedures, using modern and agile tools and applications for fully scalability and easy deployment. It has been critical in sharpening the offshore safety observation program across global operations, resulting in a boost of the workforce engagement by 30%, and subsequently increasing safety awareness skill set attainment; improving overall offshore safety culture, all while reducing operating costs by up to 70% and cutting carbon footprint through the elimination of 15,000 manhours and half a million paper cards each year, when compared to previously used methods and workflows","Day 4 Fri, September 10, 2021",2021.0,10.2118/205465-ms,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
2b9fd83059cfe5d41e2386734bac6f1f1d1546c6,https://www.semanticscholar.org/paper/2b9fd83059cfe5d41e2386734bac6f1f1d1546c6,Predicting Arrival Times of Container Vessels : A Machine Learning Application,"A Dutch Logistic Service Provider (LSP) currently applies a reactive attitude towards arrival time information that is solely based on the carrier’s sailing schedule. However, this sailing schedule historically appears to be unreliable: 20% of the orders that the LSP executed last 2.5 years, did not arrive on time. Since LSPs remain dependent on carriers from the container shipping industry, a platform capable of delivering and processing accurate information is essential for increasing efficiency, visibility and customer service. Not being able to exactly know when an order will arrive, negatively affects the businesses of both the LSP and the customer in terms of decreased efficiency and increased costs. We therefore propose a more proactive attitude towards arrival times by means of a predictive model based on historical order data. We applied the Random Forest technique to this end. The model is able to predict the deviation in the arrival time that is provided by the carrier in their sailing schedule in advance of actual shipment. Deployment of the actual prediction algorithm is expected to lead to improved business processes in terms of increased efficiency and decreased costs for both the LSP and the customer.",,2019.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
3b1a406ff586f58e676320aea89043a62d0ec1c2,https://www.semanticscholar.org/paper/3b1a406ff586f58e676320aea89043a62d0ec1c2,Guest Editorial on “Knowledge fusion intelligent optimization for complex systems”,,Complex & Intelligent Systems,2021.0,10.1007/S40747-021-00394-X,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
9c0a10217286b64a2f5a64724d636f78d712adc4,https://www.semanticscholar.org/paper/9c0a10217286b64a2f5a64724d636f78d712adc4,Predictive Asset Analytics: The Future of Maintenance,"
 Major Overhauls (MOH) of major Rotating Equipment is an essential activity to ensure equipment and overall plant's productivity and reliability requirements are met. This submission summarizes Maintenance cost reduction and MOH extension benefits on an integrally geared centrifugal Instrument Air (IA) compressor through a first of its kind Predictive Maintenance (PdM) solution project in ADNOC.
 Appropriate planning for Major Overhauls (MOH) in accordance with OEM, company standards and international best practices are crucial steps. Digitalization continues to transform the industry, with enhancements to maintenance practices a fundamental aspect. Centralized Predictive Analytics & Diagnostics (CPAD) project is a first of its kind in ADNOC as it ventures into on one of the largest predictive maintenance projects in the oil & gas industry. CPAD enables Predictive Maintenance (PdM) through Advanced Pattern Recognition (APR) and Machine Learning (ML) technologies to effectively monitor & assess equipment performance and overall healthiness. Equipment performance is continuously assessed through the developed asset management predictive analytics tool. Through this tool, models associated with the equipment were evaluated to detect performance deviation from historical normal operating behavior. Any deviation from the historical norm would be flagged to indicate condition degradation and/or performance drop. Moreover, the software is configured to alert for subtle changes in the system behavior that are often an early warning sign of failure. This allows for early troubleshooting, planning and appropriate intervention by maintenance teams.
 Using the predictive analytics software solution, an MOH interval extension was implemented for an integrally geared centrifugal IA compressor installed at an ADNOC Gas Processing site. The compressor was due for MOH at its traditional fixed maintenance interval of 40,000 running hours in Nov 2019. Through this approach, the actual performance and condition of the compressor was assessed. Its process and equipment parameters (i.e. casing vibrations, bearing vibrations, bearing temperatures and lube oil supply temperature/pressure, etc.) were reviewed, which did not flag any abnormality. The compressor's performance had not deviated from the historical norm; indicating that the equipment was in a healthy condition and had no signs of performance degradation. With this insight, a 15 months extension of the MOH was achieved. Furthermore, a 30% maintenance cost reduction throughout the compressor's life cycle is projected while ensuring equipment's reliability and integrity are upheld. A total of 7 days maintenance down time including work force and materials planning for the MOH activities was deferred. The equipment remained in operation until its rescheduled date for MOH.
 Through the deployment of predictive analytics solutions, informed decisions can be made by maintenance professionals to challenge traditional maintenance practices, increase Mean Time Between Overhauls (MBTO), realize the full potential of a plant's process & utilities machinery and optimize operational costs of plant assets.","Day 3 Wed, November 17, 2021",2021.0,10.2118/207616-ms,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
8dc6411e55cab57dbad552f524e17e44d2172bcd,https://www.semanticscholar.org/paper/8dc6411e55cab57dbad552f524e17e44d2172bcd,"Technology Driven Disruption in Global Retailing: Implications for Strategy, Execution and Economic Policy","Global retailing is expected to be a 28 Trillion USD industry by 2020 and currently represents 31% of global GDP. (Berkshire Hathaway 2016) Currently, across a global landscape the retail sector is experiencing the effects of technological disruption, perhaps the fourth major retail disruption in the last 150 years. Some analysts have stated that retail may change more in the next five years than in the last fifty. 
 
Projected impacts will ripple across not only the retail industry but across consumers, suppliers and impact the workforce of nations. In the USA some estimates have up to 25% of the workforce in retail or supporting firms. Today many of those jobs are in jeopardy as firms deploy new technology such as robots, AI, machine learning, digital, VR and new business models using these technologies. Globally these changes will affect both developed nations as well as the developing economies in fundamental ways. Along with disrupted jobs new opportunities are expected to be created that require skill sets that many workers are not currently prepared. Currently there are differing opinions by experts regarding the net, net employment impact of these new technologies. 
 
Research Purpose: This research will review the academic and practitioner literature on current and future projected technological trends in retail and discuss implications for retail strategy and execution, and potential governmental policy implications. In addition to examining the literature we will conduct exploratory interviews with retail leaders, leading suppliers, retail trade associations and technology service providers about their thoughts on the speed and impact of technology on retail evolution/revolution. We will focus our work mostly in the food/CPG and apparel industries given past research and access opportunities with industry leaders. The primary purpose of this research is to review the state of technologies impact on this key global industry and to summarize implications for private and public policy strategies. We will be looking in particular where there are areas of agreement/disagreement in the literature and among experts. 
 
Research Methodology: This research will conduct secondary analysis of the business literature on retail technology and competitive trends and supplement using an exploratory qualitative interviews of key informants from industry. Thus this research is largely exploratory. 
 
Analysis will use existing models of technology and business model diffusion, together a discussion of how the current disrupted environment is different and/or similar to past technology disruptions. The expected impacts of retail disruption will also be examined using Christiansen’s model of industry disruption and impacts on industry value-chain configuration. 
 
Finding/Results: The findings of this research will be presented in the form of a summary of retail trends and anticipated impacts across the industry value chain and on employment. Areas of agreement / disagreement in the literature and among experts will be highlighted. In particular this research will describe the key technology trends affecting retailers, overview the expected impacts of those trends and then discuss implications for retail strategies, such as Omni-channel and business model. In addition policy issues regarding employment impact and future skill level training will be discussed. Examples depicting trends and impacts will be described in the discussion. 
 
Conclusions: In summary this research reviews technology trends in retailing and then discusses implications of these trends for retail practitioners and for public policy makers. Managerial and competitive issues such as strategic flexibility/agility, scale, retail branding, productivity and implications of evolving GMROI on retail investment. Limitations of the research and exploratory methodology will be reviewed and opportunities for future research will be presented in the context of the literature and managerial decision making.",,2019.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
1460f1c20b4314cf5fa9a3d12bcf878614723f4c,https://www.semanticscholar.org/paper/1460f1c20b4314cf5fa9a3d12bcf878614723f4c,Technology Focus: Sand Management (October 2021),"Our industry is under pressure to produce cleaner energy. That is the mantra, more so than a few years ago. A recent report from the International Energy Agency suggested that all greenfield developments in the oil and gas sector should be stopped forthwith if we are to achieve the net-zero target by 2050. That essentially means that we squeeze what we can from the not-so-easy and mature reservoirs, many of which have sand-control problems. Perhaps that is the reason most operators are working ever harder to manage and produce such assets, a trend reflected in the number of papers written. More importantly, a large proportion of papers this year were on sand consolidation and through-tubing exclusion methods, which primarily target mature producing reservoirs.
 A few technology trends are becoming apparent. There is a move to gravel pack longer and longer horizontal sections. It is now possible to pack more than 7,000 ft with zonal isolation. Through-tubing sand-control remediation continues to evolve. Sand consolidation is moving toward nanoparticles, with a promise of better regained permeability. Further strides have been made in developing filters to achieve behind-screen compliance for better sand retention.
 Industry has been enchanted by what data analytics and machine learning can potentially offer, and perhaps rightly so. Several papers this year apply these tools to sand management. For those interested, I would recommend paper SPE 200949 and OTC 31234 as further reading. Unfortunately, from a sand-control perspective, I do not yet see a compelling narrative. One interesting statistic that I stole from a LinkedIn post is that the rising 3-year trend of papers in OnePetro on this subject has fallen dramatically between 2020 and 2021. I have not independently verified these figures, but it does tell a story. Is the excitement waning?
 Recommended additional reading at OnePetro: www.onepetro.org.
 SPE 203238 - Sanding Propensity Prediction Technology and Methodology Comparison by Surej Kumar Subbiah, Universiti Teknologi Malaysia and Schlumberger, et al.
 SPE 201768 - Using Artificial Intelligence for Determining Threshold Sand Rates From Acoustic Monitors by Srinivas Swaroop Kolla, The University of Tulsa, et al.
 OTC 30386 - Pioneering Slickline Deployed Through Tubing Gravel Pack in Malaysia: Successful Case Study and Lessons Learned by Ertiawati Mappanyompa, Petronas, et al.",Journal of Petroleum Technology,2021.0,10.2118/1021-0067-jpt,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
8eb202d8d9c5f39219f70aeabc52db6e597848fa,https://www.semanticscholar.org/paper/8eb202d8d9c5f39219f70aeabc52db6e597848fa,A Framework for Action Detection in Virtual Training Simulations Using Synthetic Training Data,"In virtual military training, tracking and evaluating trainee behavior throughout a simulation exercise helps address specific training needs, improve the realism of simulations, and customize the training experience. While it is straightforward to parse the event log of a simulation to identify atomic behaviors such as unit movements or attacks, it remains difficult to fuse these events into higher-level actions that better characterize trainees’ intentions and tactics. For example, if each unit is controlled by an individual trainee, how should the movement information from all units be aggregated to determine what formation the group is moving in? Similarly, how can all of the information from nearby terrain environments be combined with kinetic actions to determine whether the trainees are executing an ambush attack, or is simply engaging the enemy group? While an experienced human observer-controller can quickly assess the battle map to provide an appropriate interpretation for such events, it remains a challenging task for computers to automatically detect such high-level behaviors when performed by human trainees. In this work, we proposed a machine-learning (ML) framework for recognizing tactical events in virtual training environments. In our approach, unit movements, surrounding environments, and other atomic events are rasterized as a 2D image, allowing us to solve the action detection problem as image classification and video temporal segmentation tasks. In order to bootstrap ML models for these tasks, we utilize synthetic training data to procedurally generate a large amount of annotated data. We demonstrate the effectiveness of this framework in the context of a virtual military training prototype, detecting troop formations and other tactical events such as ambush and patrolling. ABOUT THE AUTHORS Andrew Feng is a Research Scientist at the Institute for Creative Technologies at the University of Southern California, working on the One World Terrain project. Previously, he was a research associate focusing on character animation and automatic 3D avatar generation. His research work involves applying machine learning techniques to solve computer graphics problems such as animation synthesis, mesh skinning, and mesh deformation. He received his Ph.D. and MS degree in computer science from the University of Illinois at Urbana-Champaign. Email: feng@ict.usc.edu Andrew S. Gordon is a Research Associate Professor of Computer Science and Director of Interactive Narrative Research at the Institute for Creative Technologies at the University of Southern California. His research advances technologies for automatically analyzing and generating narrative interpretations of time-series data. He received his Ph.D. in 1999 from Northwestern University. Email: ​gordon@ict.usc.edu 2020 Paper No. 20302 Page 1 of 12 Interservice/Industry Training, Simulation, and Education Conference (I/ITSEC) A Framework for Action Detection in Virtual Training Simulations Using Synthetic Training Data Andrew Feng, Andrew S. Gordon Institute for Creative Technologies University of Southern California Los Angeles, CA USA feng@ict.usc.edu, gordon@ict.usc.edu INTRODUCTION Among the most compelling use-cases for interactive virtual simulations is for virtual team training exercises, e.g., where teams of warfighters control their own virtual avatars in simulated battles, practicing tactical maneuvers and essential skills in increasingly challenging scenarios. While these computer-based exercises lack the physical stresses of live training, their great potential lies in their capacity for guided deliberate practice of skills, where the environments themselves are tailored to the abilities of the team and responsive to their successes and failures in their performance. In both live and virtual training, however, this responsiveness has generally required the participation of human facilitators, i.e., Observer Controllers that track the progress of trainees toward learning objectives, and actively shape the learning environment to further this progress. Automating these capabilities of human Observer Controllers would have several practical benefits for virtual training for operational units. Replacing training support staff with software reduces labor costs, at the very least. More importantly, such automation can help remove the reliance of operational units on outside contractors and home-station training facilities in conducting virtual training exercises. As this reliance is reduced, commanders are more able to lead their own training exercises, on their own schedules, wherever their point of need.. Much of the difficulty in automating the Observer Controller stems from the simulation environment's lack of understanding of what the human trainees are actually doing in the virtual space. Seeing a given formation and orientation of trainee avatars on a virtual ridge line, for example, it might be obvious to a human Observer Controller that the trainees were setting up for a deliberate ambush of an approaching enemy force, prompting him or her to modify the enemy's reaction to reinforce lessons related to the performance of this tactical maneuver. The software of the virtual simulation environment, however, would likely be oblivious to the impending ambush altogether, aware only of the avatars' positions in virtual space and the avatar controls provided by the trainees' user interfaces. Automating the human Observer Controller, in this case, requires an ability to recognize complex group behaviors based on the positions and individual actions of avatars in the group, within the battlefield context. Analogous perceptual recognition tasks have been successfully automated in other domains using contemporary machine learning technologies, such as deep neural networks. Here, recognizing the complex behaviors of teams of trainees in virtual environments can be seen as a type of time-series classification task. The input consists of positional and low-level simulation event data over time, and output assigns labels to durations that best characterize the class of behavior that is being executed, from a given vocabulary. Today, a high-accuracy classifier of this sort could easily be constructed using supervised learning methods, provided that enormous amounts of data were available, expertly annotated with high levels of inter-rater agreement. However, the required amounts of data (perhaps tens of thousands of examples) is far beyond what might be reasonably obtained from the deployed use of any existing virtual training software. Even assuming such large-scale datasets could be collected, its annotation by teams of experts would be especially costly and difficult, given the sensitive nature of data generated during military training exercises. If contemporary machine learning technologies are to be used to automate functions of human Observer Controllers, an alternative approach to the collection and annotation of training data is needed. In this paper, we investigate an alternate approach to the collection and annotation of datasets for behavior recognition using machine learning methods. Our approach involves the automatic generation of synthetic training data, collected by authoring behavior programs to be executed thousands of times by teams of fully-autonomous 2020 Paper No. 20302 Page 2 of 12 Interservice/Industry Training, Simulation, and Education Conference (I/ITSEC) agents within the target simulation environment. Multiple programs are authored, one for each of the classes of behavior that is to be recognized in teams of human trainees, which allows durations of generated data to be automatically annotated with the correct class label. We demonstrate this approach by generating behaviors corresponding to different troop maneuver formations, as would be executed by squads of Army soldiers (teams of 9 trainees). We evaluate the effectiveness of the resulting behavior classifier using gold-standard test data collected using volunteers in a testbed multiplayer simulation environment. Results indicate that high-accuracy recognition is possible using this approach, but requires the application of domain adaptation techniques to bridge the gap between synthetic data and human performance data. RELATED WORK Behavior Recognition in Virtual Simulations and Video Games The problem of recognizing team behaviors in virtual simulations using supervised machine learning methods has been investigated previously by Sukthankar and Sycara (2006). In their work, two-person tactical maneuvers are recognized in a multiplayer, game-based simulation environment using hand-authored behavior templates, which are used to classify durations of gameplay data using trained Hidden Markov Models. Our approach is analogous in several respects, using hand-authored behavior programs rather than templates, but eliminates the additional step of hand annotating the training data used in supervised learning. Additionally, we address the more challenging problem of recognizing behaviors in larger teams (nine-person infantry squads). Behavior classification has also been investigated in the analysis of gameplay data from multiplayer video games. Ahmad et al. (2019) demonstrate a successful approach to behavior classification that minimizes the hand annotation of training data. In their approach, referred to as Interactive Behavior Analysis, large amounts of multiplayer gameplay data are interactively analyzed through an iterative process of visualization, labeling, and clustering. Aimed at analysts who are interested in understanding player strategies and tactics, this descriptive approach allows for quick ​post hoc analyses with minimal labeling. Our approach contrasts with this previous work in that we aim to identify behaviors ​in situ​, using prescriptive definitions, for the purpose of providing real-time responses within the simulation environment. Methods for plan a",,,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
d85e0d876fa2a02241b665087ccd7084f3843cf1,https://www.semanticscholar.org/paper/d85e0d876fa2a02241b665087ccd7084f3843cf1,Evaluation of the extent of integration of humanoids in tackling social-economic challenges confronting the Africa continent,"The sustenance and growth of any nation, depends critically and essentially on her level of development. A country seen as developed when is able to provide qualitative life for her citizenry.1 The African continent amidst her huge human, material and natural resources has been battling with the problems of development even after many of the countries in the region has gained her independence. Economic Researchers have overtime attributed this challenge to factors like: Inadequate executive capacity responsible for the formulation and implementation of developmental policies, Lack of good governance, High level of corruption and indiscipline and mono-economic base of most countries in the region. However, European and Asian countries experience enviable growth and development patterns attributable to : the development of agricultural sector, improved mass education of her citizenry, development of indigenous industries, export oriented strategy, disciplined leadership style, existence of efficient bureaucracy human resources development, encouragement of a dynamic private sector working in cooperation with the government towards a society-wide vision of development, institutional capacity building and attention to the problems of governance, consistency and policy stability.2–4 It is said that human and technology go hand in hand. There have been rapid advances in technology in most western world and this has led to a surge of both government and public interest in automation and robotics.5–7 Robotics, which is an out spring from Mechatronics, has overtime evolved due to advances in Control Technologies and Computer Science. The rapid progress in innovative Sensors and Actuators technologies and breakthroughs in Data Science specifically in the area of Artificial Intelligence (AI) and Machine Learning (ML) have resulted in the development of Intelligent Robots.8 It is believed that the future would be a place where man and robots cohabit for the realization of man’s all round needs. In recent times, various kinds of practical assistive robots have been developed to suits mans need in different fields. For instance in the medical and welfare fields, exoskeleton power assist robots, assistive manipulators, robotic beds, and intelligent wheelchairs have been developed to cater for critical challenges in these areas by providing direct and physical assist either disabled people or their helpers.9 Several robots have been developed for the Industries, Educational sector, Aviation and Transport sectors, Commercial sectors etc. The quest to build a human like robot led to the development of Humanoids. This kind of robots which is seen as the most advanced creature of Man is envisioned to be the most ideal assistants and companion of human beings. This set of robots can interact with man and carry out functions independently. Humanoids are believed to be more efficient and effective than man in some areas as they only a stable source of power to function. As such they can be deployed to undertaking physically and mentally challenging routines.10 Humans in different situations need companion at different point in time. Cases abound where man is not able to get companion or counsel from other humans immediately when sort. This could be due to the fact that they are far away off/out of communication or they are no more. Thus, humanoids could fit in to meet this need as they communicate in a socially intelligent manner, recognizes and can learn human behaviors and psychology.11–13 Many western countries have embraces the use of Automation and Robotics in ameliorating her development challenges as: Robots have increased their productivity and competitiveness, increased productivity has lead to increased demand thus creating new job opportunities. Automation also has led overall to an increase in labor demand and positive impact on wages, and Robots complement and augment labor. Currently, the level of integration of these intelligent robots in both domestic and public environments of most third world countries like Africa has been very insignificant. This to an extent could be attributed to the Government polices; Religious and Cultural believe of the people. Most of the robots in the regions are mostly industrial .Very few available robots in the region are humanoids. Many have not been really integrated into public place and private homes. This is due to the fact that majority are of them are not built to adapt properly with the culture, religion and psychology of the people in the region. As such people find it difficult to accept these robots as companions and helpers in either homes, schools, hospitals, shopping malls, airports and other public places. If Africa is to fully embrace humanoids, there is an urgent need to build humanoids that are adaptive to the cultural, religious and psychological inclination of people in this region. To be able realize this; there is great need for more advanced research cognitive science/reasoning, adaptive learning and human",ICRA 2018,2018.0,10.15406/IRATJ.2018.04.00103,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
598e392add0580da30ae50559e8361df27e34d0d,https://www.semanticscholar.org/paper/598e392add0580da30ae50559e8361df27e34d0d,Emotional System for Military Target Identification Prof,"The thought of man-made systems and machines having emotions sounds like science fiction, however, few decades ago the idea of machines with intelligence seemed also like fiction, but today we are developing intelligent machines and using them successfully in different applications. Would we accept the idea of machines that could “feel”? What role would emotions play in machine learning and decision making? How can we model artificial emotions within intelligent systems? Can a machine’s decision capability be improved if it had emotions? These are questions one may ask when hearing that machines may have emotions, albeit artificial emotions. In this paper, we discuss these questions, and review briefly some of the latest works on modeling emotions within intelligent systems; including our own model that is based on an emotional neural network (EmNN). The EmNN has emotional neurons, weights, and two embedded emotional responses; anxiety and confidence. These emotional parameters are updated during task learning, and used during decision making. The paper will also present an application of the EmNN to military target identification, in addition to discussing the potential of using the emotional system to improve information exploitation. 1.0 INTRODUCTION In our daily lives, the amount of information we receive, perceive and then react to is tremendous. Much of our reaction to input information is formed into decisions that we make. What makes us act in a certain way, or decide for, or against an act has its roots back in our previous experiences. Whether we choose anchovies or pineapple on our pizza is a decision we make based on a previous experience with these flavors that leads us to decide what to have. Such a simple process of deciding on our favorite topping is as important and complicated as the more critical decisions we need to make throughout our lives. All decisions involve a learning process, resulting in association, classification and then decision making. During the learning process information which can take different forms, is exchanged between our natural sensors and the main processor; the brain. This sounds very technical and parallel to describing a computing system. However, computing systems lack one aspect of a human processing system; emotions. Over the past decades machines and systems have been developed and deployed to aid us in decision making and taking action; spanning application areas from simple electronic toys, industry and manufacturing, intelligence and security, to more complicated tasks in medicine, military applications, and space navigation. As the creators (designers) of these machines, we aim to assure that the action or the decision taken by the machine is correct and complies with our way of making a decision. In simplistic terms, we tend to create machines that would make the decisions on our behalf, and as information age progresses, and powerful high-tech systems grow even faster, our expectations of the machines are increasing. Most of the systems that we develop, and use to make decisions on our behalf, do not go through the learning process and the experiences that we possess. With the exception of some artificial intelligent systems that could interact with their input stimuli, and adapt their output or decision accordingly, systems Emotional System for Military Target Identification 18 2 RTO-MP-IST-087 UNCLASSIFIED/UNLIMITED UNCLASSIFIED/UNLIMITED and machines rely entirely on a set of commands that we provide to govern their actions, and this has been fine until our demands started requiring more complicated decisions by machines. Therefore, more and more intelligent systems are being developed, based in particular on neural networks which form the brain of a machine. These systems imitate our learning process and decision making by repeatedly exposing a neural network to examples of input information and its corresponding output, response, action or decision; this process models the previous experience process in humans. The neural network-based systems have been popularly used, and have shown success in various application areas, where association, classification and decision making can be obtained based on accumulated memory of past experiences. Despite the success of such intelligent systems, there has been a major and vital difference between a human decision-maker and a machine decision-maker, namely emotionwe have it, and machines do not. The idea of machines with affection or feelings is controversial, and some works expressed doubt about this idea [1,2], however, the concept of machines with emotions has lately attracted the attention of many researchers, and is currently gaining momentum with novel architectures emerging to artificially model emotions in one way or another. Recent definitions of emotion have either emphasized the external stimuli that trigger emotion, or the internal responses involved in the emotional state, when in fact emotion includes both of those things and much more [3]. The effective role of emotions on cognitive processing, learning and decision making in animals and humans has been emphasized by several researchers [4-8]. Emotions play an important role in human decision-making process, and thus they should be embedded within the reasoning process when we try to model human reactions [9]. Although computers and machines do not have physiologies like humans, information signals and regulatory signals travel within them; there will be functions in an intelligent complex adaptive system, that have to respond to unpredictable, complex information that play the role that emotions play in people [1]. Such computers will have the same emotional functionality, but not the same emotional mechanisms as human emotions. We may think of machine emotions as machine intelligence; we do not expect machines to “feel” the way we feel, but we could simulate machine emotions just as we do machine intelligence [9]. There have been examples of research works that attempted to incorporate emotions in machines in one way or another [9-20]. It was concluded from these works that if emotions such as anxiety, fear, and stress are included in systems that aim to simulate the human behaviour in certain circumstances, the system will be more user-friendly and its responses will be more similar to human behaviour. Other recent research works suggested the use of emotional components within neural models and control systems. For example, Abu Maria and Abu Zitar [21] proposed and implemented a regular and an emotional agent architecture which is supposed to resemble some of the human behaviours. They noticed that artificial emotions can be used in different ways to influence decision-making. Gobbini and Haxby [22] proposed a model for distributed neural systems that participate in the recognition of familiar faces, highlighting that this spatially distributed process involves not only visual areas but also areas that primarily have cognitive and social functions such as person knowledge and emotional responses. Coutinho and Cangelosi [7] suggested the use of modelling techniques to tack into the emotion/cognition paradigm, and presented two possible frameworks that could account for their investigation, one of which explored the emergence of emotion mechanisms. Most of these previous attempts on incorporating emotions in to machine learning have shown successful results, and provided a positive trend to developing machines with emotions, albeit simulated. Lately, we proposed an emotional neural network (EmNN) which was based on the novel emotional back propagation (EmBP) learning algorithm [23], and used it to solve a facial recognition problem. In other works [24,25], we explored the potential of using emotional neural networks in different tasks, such as Emotional System for Military Target Identification RTO-MP-IST-087 18 3 UNCLASSIFIED/UNLIMITED UNCLASSIFIED/UNLIMITED more complicated face recognition tasks and in blood cell identification. The difference between an emotional system and the more traditional approaches; including intelligent systems, is the simulated artificial emotions of the system. These additions have several advantages over traditional approaches. The embedded artificial emotions narrow the gap between humans and systems; thus instead of “human and machine interaction” we have “human and human-like machine interaction”. The closer and more coherent communication of information between humans and emotional systems has the advantage of faster communication, since both systems (human and machine) have emotions. This is not the case with traditional systems, where often a human operator perceives information and makes decisions which could differ due to his/her emotions. In this paper, we present the emotional neural network (EmNN) and describe its emotional parameters. The EmNN will also be applied to identify potential military targets, such as navy ships, helicopters, jetfighters, tanks and other assorted military vehicles. One of the aims of this work is to mimic the way a human would recognize these targets, by: firstly, using different images of targets with random orientations, angles, and backgrounds, secondly, avoiding complicated image pre-processing phases, and using only global image pattern averaging to simulate a human’s “glance” or quick look at a target image, and finally, using the EmNN to perform the target identification, by repeatedly exposing random target images to the network during its training phase; this process simulates the human “getting familiar” with the objects, without the need to look into edges, local features, angles or colors of a potential target. The paper is organized as follows: Section 2 presents the EmNN and describes the differences between conventional neural networks and emotional neural networks. Section 3 presents the application of the EmNN to military target identification, describin",,2009.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
70a6876d2f5b001be38608182de55e7d5a75cbfd,https://www.semanticscholar.org/paper/70a6876d2f5b001be38608182de55e7d5a75cbfd,The Human-Side of Service Engineering: Advancing Technology's Impact on Service Innovation,,AHFE,2021.0,10.1007/978-3-030-80840-2_1,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
2ff58257d18a2cfc43417df163c3222e0e2943f2,https://www.semanticscholar.org/paper/2ff58257d18a2cfc43417df163c3222e0e2943f2,IMS2020 Microwave Week Workshops,": This workshop showcases intelligent mixed-signal, RF/mm-wave, and microwave photonics systems that exploit machine learning and AI techniques in three focused application areas: advanced wireless communication, sensing, and computation. With an emphasis on wireless communication, the workshop explores machine-learning and AI techniques for RF signal conditioning, dynamic wireless-spectrum collaboration, wireless PA linearization, and massive MIMO– mm-wave phased-array beamforming. With a focus on sensing and imaging applications, the workshop presents machine-learning–based radar signal-processing techniques for autonomous navigation and their implementation in integrated frequency-modulated continuous-wave radar systems. Regarding computation, the workshop combines mixed-signal, RF/mm-wave, and microwave-photonics circuit techniques to accelerate energy-efficient, multidimensional signal processing for machine-learning and AI algorithms. In addition, the workshop discusses several applications of photonic deep-learning hardware accelerators in wireless communication, such as RF fingerprinting. Abstract: Advances in mm-wave CMOS technology have resulted in fully integrated mm-wave radar sensors that offer a cost-effective and robust solution to automotive safety, provide accurate industrial sensing, and enable gesture recognition. This workshop features technical experts from academia and industry who present the state of the art in mm-wave CMOS technology, such as all-digital architectures, higher carrier frequencies, advanced signal processing, and machine learning. These technologies promise to improve the achievable accuracy and push performance levels further. The speakers will also share their view of the Abstract: Coherent detection, where the optical carrier’s phase information provides higher signal-to-noise ratios, has ever-increasing momentum. Today, coherent communication dominates long-haul networks operating with data rates beyond 400 Gb/s per wavelength. Thanks to advancements in digital signal processing that leverage ultralow-power implementations in deep submicron technologies (for instance, 7 nm), the cost and power of coherent transponders are becoming competitive for short-reach networks (inter- and intra-data centers). Reducing the cost and enhancing the overall performance of such networks is only achievable through highly integrated solutions that encompass complex digital signal-processing algorithms, state-of-the-art transimpedance amplifiers and modulator drivers, and integrated silicon photonics. Codesign and co-optimization become the key factors in further scaling the power and performance of coherent transponders. They will be the focus of this workshop and addressed by several speakers from different backgrounds. Abstract: Quantum computing has spurred intense research to develop electronics to control quantum devices operating at cryogenic temperatures. Several applications beyond quantum computing require cryogenic electronics to be compatible with very low ambient temperatures or outperform their room-temperature counterparts. This workshop presents an overview of cryogenic electronics, from applications to device operation, focusing on ICs. Applications that must operate at cryogenic temperatures, such as quantum computing (first talk) and particle physics (second talk), will be presented to highlight requirements, current limitations, and future perspectives. The operation of silicon-germanium (SiGe) (third talk) and CMOS (fourth talk) at cryogenic temperatures will be discussed, and four examples of ICs that employ SiGe, bulk CMOS, and fully depleted silicon-on-insulator (SOI) CMOS and target low-noise Abstract: Low-noise amplifiers (LNAs), PAs, switches, and phase shifters can be integrated into one silicon RF front-end (RFFE) IC for mm-wave 5G, and multichannel integration is likely; however, the cost, robustness, and manufacturability advantages of the all-silicon RFFE IC approach are not clear compared to hybrid III-V/ silicon alternatives. mm-Wave 5G broadband PA power efficiency is considerably lower than for 4G equipment. Gallium-nitride (GaN)/ gallium-arsenide (GaAs) III-V–based PAs have a higher output power than silicon-based PAs and good efficiency, but the cost of hybrid-integration approaches rapidly increases with complexity, as will be covered in this workshop. mm-Wave PA linearity versus power-added efficiency at power backoff is always a design tradeoff, and novel RF linearization techniques are required to improve 5G mm-wave PAs. All-silicon solutions with superstrates for antennas will be investigated, and we will discuss the PA antenna and package codesign for 5G MIMO PAs. Design: Basics Abstract: This workshop by phased-array experts in academia and industry provides an in-depth learning experience and walks attendees through different aspects of mm-wave phased-array transceiver design. It covers the following topics: 1) silicon-based mm-wave phased-array basics; 2) phase- and gain-control circuits; 3) package, antenna, and module codesign and calibration; 4) phased-array measurements on-chip and over the air; 5) phased-array applications in commercial and defense systems; and 6) current 5G New Radio phased-array systems and limitations and an outlook Abstract: 5G communication in sub-6-GHz frequencies offers enhanced data rates, capacity, and flexibility but faces challenges, such as energy efficiency, linearity, integration, and scalability. To increase battery life, optimizing PA efficiency is of the utmost importance. This workshop investigates digitally intensive transmit architectures and predistortion techniques that enhance the efficiency of the transmitters and PAs used in these next-generation wireless systems. Experts from industry and academia will share their latest research on linearization techniques to build highly efficient linear PAs in various technologies, employing topologies such as Doherty, out-phasing, and polar. Circuit topologies and digital signal-processing algorithms for the predistortion of these PAs will also be covered. Abstract: 5G will be key to meeting an order-of-magnitude increase in the data-traffic demand on mobile networks. 5G massive MIMO technology will deliver high data rates to many users, helping to increase capacity. It will support real-time multimedia services and reduce energy consumption by targeting signals to individual users through digital beamforming. Element-level digital beamforming that supports emerging multibeam communications and directional sensing at the mm-wave frequency range will expand the use of mm-wave phased arrays and make them broadly applicable across U.S. Department of Defense systems. This workshop presents state-of-the-art radio circuits and systems exploiting MIMO and digital beamforming at sub-6-GHz and mm-wave bands for civilian 5G New Radio and defense applications. Abstract: Indoor positioning and localization will be critical to the next-generation Internet of Things (IoT). The technology obtains the location of a device or user in an indoor environment, which is a key function enabling various IoT applications, such as smart buildings, distance-bounded security, smart industrial applications, and so forth. In this workshop, several popular smartphone-based wireless technologies for locating people and objects will be discussed. Bluetooth Low Energy (BLE), ultrawideband (UWB), and WiFi are three popular standards-compliant localization approaches. BLE is the most widely adopted smartphone-based wireless protocol, so BLE-based localization has an advantage in densely deployed infrastructure. UWB is an emerging wireless-localization technology that is used in advanced smartphones (for example, the iPhone 11). The new UWB protocol, IEEE 802.15.4z, can provide centimeter-level accuracy thanks to its wide spectrum. Finally, WiFi, as a wireless technology deployed in most buildings, will play an important role in accurate positioning with the upcoming IEEE 802.11az protocol. Abstract: This vertically oriented workshop provides technical know-how, from the satellite to the device, by bringing together commercial and defense leaders in space hardware. It reviews satellite orbits and the demands on antenna systems and also provides a detailed overview of CubeSats and the drive for small-form-factor, high-reliability electronics, followed by a comprehensive examination of the market and challenges for satellite-communication terminals. The workshop covers RFICs for space in CMOS and III-V technology, including a special discussion of advanced, very-low-power CMOS for deep-space sensors. A technical review of radiation types and their effects on CMOS and the techniques to successfully design in space using a radiation-hard library or next-generation radiation-hard process on advanced bulk CMOS is offered. This is a great place for new and experienced engineers to learn about the adventure of space. Abstract: PAs do not fulfill all the requirements of linearity, energy efficiency, and bandwidth necessary for New Radio and mm-wave 5G operation and future communications, particularly in user equipment. New techniques are required to design ultrahigh-linearity PAs and yield improved linearization, efficiency enhancements, and bandwidth-extension methods to dramatically improve performance. All aspects of New Radio and mm-wave PA design become more challenging when equipment is placed in arrays with non-negligible element-to-element coupling. This workshop explores PA designs in the mm-wave spectrum as well as linearization techniques (digital predistortion, outphasing, envelope tracking, and so forth) and efficiency enhancements (load and supply modulation and so on) in user equipment and base stations. Abstract: In emerging 5G cellular communication and other mm-wave systems, the generation, distribution, and synchronization of local-oscillator (LO) signals remain a challenge. This ",IEEE Microwave Magazine,2020.0,10.1109/mmm.2020.2971397,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
f1f7e6bcdba65392e79661a8b952d0b622746ba3,https://www.semanticscholar.org/paper/f1f7e6bcdba65392e79661a8b952d0b622746ba3,Revenue assurance in electrodistribution using smart meters data,"The increasing complexity of services also encourages the complexity and heterogeneity of the systems that providers use for measuring and billing these services. The complexity may result in the occurrence of errors and consequently in a revenue leakage. For the telecom industry that strives to adapt their services not only to the needs of customers but also to new technological opportunities the biggest complexity issue is billing the services. They were the first to recognize the need to develop a systematic way to grapple the problem of errors that cause the revenue leakage. The sector prepared a comprehensive framework of procedures called ""revenue assurance"". With the liberalization of the electricity market, the services of electricity distribution became as complicated as the telecommunications services. This further significantly enhanced the deployment of smart grids, advanced metering infrastructures and smart meters that, with the abundance of data, give opportunities for new services. This master thesis presents the approach we took to grapple with the revenue assurance. We used general procedures and took into consideration the peculiarities of the electricity distribution domain as well. Particular attention was given to data quality issue. 
In the practical part of the thesis, we focused on acquiring knowledge from the data that would benefit us in detecting the revenue leakage, from the smart meters’ data collected by advanced metering infrastructure in particular. In the data warehouse, the data was combined with the billing system data and the geographic information system data. While building the data warehouse, we encountered some problems with data quality. After we had pointed out the problems, we indicated how to eliminate them and how to establish a mechanism for monitoring any possible recurrences of errors. 
The thesis focused on collecting information on the characteristics of consumers. Once we acquired this knowledge, we used it to look for any thefts of electricity. We made a comparison of machine learning methods for the classification of daily load curves of consumers into typical groups. Based on the analysis of the results obtained, we selected the best method, i.e. the expectation maximization method. At the same time, we determined the best number of clusters with the typical dynamics of daily consumption. Once all measuring points with the 15-minute consumption data were classified, we were determining the characteristics of a consumer that coincide the most with the dynamics of his electricity consumption. Again, we tested several machine learning methods and established that decision trees are the most appropriate tool for this task. With established behavior, we estimated daily consumption for all other measuring points. Thus prepared data were used to develop an analytical structure that proves to be an excellent base for discovering the revenue leakage. We automated the entire process of filling the warehouse, finding and applying knowledge and, last but not least, processing the analytical structure. We demonstrated the usefulness of this practice with a few examples of actual reports.",,2016.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
ca1217621296f42d425c9f363ccb1e20540f54aa,https://www.semanticscholar.org/paper/ca1217621296f42d425c9f363ccb1e20540f54aa,Edge Analytics as Key Enabler for the Success of Oil & Gas 4.0,"
 
 
 Corporations & plant operators are focused on improving profitability by every possible $ (Dollar). With no control over the feed stock and finished material pricing, which are mostly governed by regulations, operators must look with in their Operation & Maintenance (O&M) for possible areas of optimization. In modern days, CIOs are greatly empowered to drive Digital Transformation strategy and contribute in improved profitability. Many of such initiatives fail to deliver the desired outcomes in timely manner. This paper discusses the essential elements of EDGE analytics embedding the fundamental aspects of plant O&M in such Digital Transformation initiatives, and thus enabling faster value realization of the investment.
 
 
 
 Every industry can be looked in to three different tiers as Assets, Process & systems and Business. Each of them has different outcomes to contribute in overall profitability. Moreover, technology, methodology, people and their collaborative adoption play a big role in achieving those outcomes.
 Asset performance is evaluated using KPIs like availability, reliability, cost & Safety. Process involves interaction of people & process fluids with those assets. However, the fundaments remain same as the source of information, based on which the O&M decisions are taken.
 Transducers & measurements around plant assets are the single biggest source of information for such decision making. Often, this information is not best utilized in terms of generating proactive insights. Information from these measurements can be converted into simple analytics. E.g. Proximity type vibration transducer measures vibration amplitude which is used for equipment safe operation and shutdown when exceed the defined threshold. Same transducer also provides the gap voltage and spectral data. Analytics definition using proximity probe gap voltage will identify rotor position within the bearing and then comparing it across the machine train along with orbit preload can give significant information about rotor dynamics. Such analytics available at EDGE are often overlooked in enterprise level predictive analytics deployed by Artificial Intelligence/ Machine Learning. One of the reasons for it is cyber security concern for the data exchange across IT-OT network layers.
 This paper talks about implementing comprehensive strategy utilizing the information from EDGE devices for EDGE analytics, various challenges and possible way forward in complying to cyber security requirements for data exchange from OT-IT systems and thus enhancing value of big data in Data Lakes.
 
 
 
 EDGE analytics adds immense value for assets in achieving best of performance KPIs. With increased adoption of cyber security practices in plant operations, there are many IT approved architectures exist today to share these analytics insight to L4 - business network. With focus on industry domain experience around systems of systems and progressive thinking, EDGE analytics will play inevitable role in the overall success of O&G 4.0 in coming years.
 
 
 
 ""Data is the new oil"". Corporates in energy sectors are driving forth industrial revolution with this belief and investing heavily in big data-based projects. Asset condition monitoring systems at EDGE in OT domain remains in silos. High resolution static & dynamic data in various operating modes collected in these Condition monitoring systems hasn't been historically much interfaced with such big data projects. Use cases of such interfaces with static & dynamic data, benefits, related workflow, data exchange architecture is relatively newer in the industry. Comprehensive discussion of these aspects in this paper will enable industries to realize faster outcomes.
",,2020.0,10.2118/203269-ms,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
13364ed14f828438d4b08136c92b2081799c971a,https://www.semanticscholar.org/paper/13364ed14f828438d4b08136c92b2081799c971a,Some educational implications of the microprocessor revolution,"The development of the microprocessor has enabled a whole new generation of sophisticated electronic devices to be produced. Microprocessors are cheap to make, versatile in their range of application, and can be used to produce powerful computers and sophisticated pieces of control apparatus, which are being widely deployed in industry and commerce. They are also cheap enough to be used in a variety of household devices, from washing machines to videogames. This thesis examines some of the effects which the microprocessor might have on people's lives in the future, and the educational implications of their proliferation. From the educational point of view there are, perhaps, two broad areas of interest; firstly, direct effects arising from the introduction of microprocessors into educational establishments and, secondly, the indirect effects arising from changes in society. Before attempting to predict future changes in the educational system it is useful to review some current areas of interest and, therefore, some aspects of educational theory which may be relevant are examined. One widely accepted view of the microprocessor revolution is that it will lead to massive unemployment, and the validity of this view is examined. Some of the predictions of job losses which may ensue are also considered, together with some of the possibilities for occupying increased leisure time which may result. The concept of ""education for leisure"" is examined vis-a-vis possible changes in employment patterns. The direct role of microprocessors in schools is considered, together with future implications for teaching and learning. Possible consequences on the curriculum are discussed.",,1984.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
350c4282de22497acbab4bf900577751f83bd135,https://www.semanticscholar.org/paper/350c4282de22497acbab4bf900577751f83bd135,Diseño y construcción de un sistema de control de temperatura para la actualización de una máquina de recuperación de disolvente en residuos de serigrafía en la industria del plástico,"Waste generated in the screen printing industry plastics have led companies to use different mechanisms to treat and comply with regulations of the Ministry of Environment; the solvent it is recovered as raw material and the waste is sent to other companies for further treatment. 
Mr. Wilson Teran has built a machine for the treatment of solvent recovery, but it has not implemented a system that optimizes the processing time and is limited by the time of delivery. 
This project have finality the design and implementation for upgrade of three-phase system control and protection system for this has been integrated the variables of temperature and processing time; these are in a control card designed in various stages as: signal synchronization of zero crossing, conditioning of temperature signal, design power source, software for controlling the process variables. 
The upgrade of the control system is designed for the user to have a better control and monitoring of variables also it has the necessary assurances that comply with the rules governing in Ecuador, allowing reduce processing time and increased production capacity. 
Performance testing of all deployed elements is performed, allowing this project has basic and fundamental aspects of learning related to Instrumentation, Automatic Control, microcontrollers, etc., of the race of Electronic Engineering.",,2016.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
406289d08cbefcc4a10d2e0630b5e18778aec20f,https://www.semanticscholar.org/paper/406289d08cbefcc4a10d2e0630b5e18778aec20f,An SDN/NFV based Intelligent Fog Architecture for DDoS Defense in Cyber Physical Systems,"The Internet of Things (IoT) has garnered considerable interest in recent years as its technical capabilities have facilitated the creation of a diverse variety of commercial revenue models and services (e.g., medical cyber-physical systems, smart agriculture, Industry 4.0, smart cities, etc.). However, the security gaps in intelligent systems continue to be a significant source of concern. Malevolent attackers employ attacks like distributed denial of service (DDoS) to weaken system availability. To mitigate attack traffic, adopting a modern Software-defined Networked backbone and Network Function Virtualization (NFV) paradigm has proven to be a potential approach. This paper offers a comprehensive examination of the cutting-edge defense solutions driven by contemporary SDN/NFV technologies, highlighting the key limitations and future scope. The paper also presents the multi-dimensional view of the defense architecture by examining the Internet of Things (IoT) scenarios, defense strategies, the role of Fog to overcome research gaps, and leveraging SDN/NFV architecture for an intelligent defense solution. Furthermore, the paper examines the profound network backbone and offers the intelligent Machine and Deep learning-based distributed DDoS defense solution deployed in the Fog layer. The article concludes with an evaluation of open issues and research prospects for vulnerability apprehension in Smart systems. Keywords—SDN, Software Defined Network, NFV, Network Functions Virtualization, Cloud, IoT, Fog Computing, Distributed Denial of Service, DDoS",2021 10th International Conference on System Modeling & Advancement in Research Trends (SMART),2021.0,10.1109/SMART52563.2021.9676241,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
e857721185318f846d64adcf5e067b6dc6c5893e,https://www.semanticscholar.org/paper/e857721185318f846d64adcf5e067b6dc6c5893e,CellDefectNet: A Machine-designed Attention Condenser Network for Electroluminescence-based Photovoltaic Cell Defect Inspection,"—Photovoltaic cells are electronic devices that convert light energy to electricity, forming the backbone of solar energy harvesting systems. An essential step in the manufacturing process for photovoltaic cells is visual quality inspection using electroluminescence imaging to identify defects such as cracks, ﬁnger interruptions, and broken cells. A big challenge faced by industry in photovoltaic cell visual inspection is the fact that it is currently done manually by human inspectors, which is extremely time consuming, laborious, and prone to human error. While deep learning approaches holds great potential to automating this inspection, the hardware resource-constrained manufac- turing scenario makes it challenging for deploying complex deep neural network architectures. In this work, we introduce CellDefectNet, a highly efﬁcient attention condenser network designed via machine-driven design exploration speciﬁcally for electroluminesence-based photovoltaic cell defect detection on the edge. We demonstrate the efﬁcacy of CellDefectNet on a benchmark dataset comprising of a diversity of photovoltaic cells captured using electroluminescence imagery, achieving an accuracy of ∼ 86.3% while possessing just 410K parameters ( ∼ 13 × lower than EfﬁcientNet-B0, respectively) and ∼ 115M FLOPs ( ∼ 12 × lower than EfﬁcientNet-B0) and ∼ 13 × faster on an ARM Cortex A-72 embedded processor when compared to EfﬁcientNet-B0.",ArXiv,2022.0,10.48550/arXiv.2204.11766,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
7eb7e238c73258d034e911f7bed7f2a9cd3c5a43,https://www.semanticscholar.org/paper/7eb7e238c73258d034e911f7bed7f2a9cd3c5a43,Automated Geosteering While Drilling Using Machine Learning. Case Studies,"
 Today's oil & gas industry faces a number of different challenges. Drilling activities are ramping up due to an increase in hydrocarbon demand combined with a reduction of easy-to-recover reserves. Horizontal drilling is growing and has become an integral part of field development. The geology is becoming more and more complex requiring drilling through dense layers targeting thin-layered reservoirs with lateral changes and anisotropy. In recent years, companies have been looking at the ways of optimizing drilling costs by increasing efficiency and process automation. This has been a driver for many companies to stay profitable and efficient in the market.
 One of the areas of interest for process automation has been a geosteering. Geosteering is the real-time adjustment well trajectory while drilling to maximize effective footage in the target zone. In this paper, innovative new approaches to automation of the geosteering process will be discussed. This approach has been successfully tested and deployed in several leading O&G companies.
 The main objective of automated geosteering is to optimize horizontal well placement while freeing up time operational geologists had spent doing routine work in order to focus on complex and more intense tasks as well as the reduction of operational errors related to human factors. This paper will provide details on several automated geosteering algorithms. They have been tested successfully on large numbers of wells. The results of automated geosteering were as close as 90% to the manual interpretations done by geologists. When the results diverged, the geologists often ""agreed"" with the interpretation proposed by the algorithm.","Day 2 Tue, October 27, 2020",2020.0,10.2118/202046-ms,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
805233aaaf5174eaf6ea7a157738d17169682037,https://www.semanticscholar.org/paper/805233aaaf5174eaf6ea7a157738d17169682037,Machine Learning-Based Soft Sensors for Vacuum Distillation Unit,"Introduction Product quality assessment in the petroleum processing industry, such as crude distillation, can be difficult and time-consuming, e.g. due to a manual collection of liquid samples from the plant and the subsequent chemical laboratory analysis of the samples. The product quality is an important property that informs whether the products of the process are within the regulated specifications, such as ASTM Petroleum Standards. In particular, the delays caused by sample processing (collection, laboratory measurements, results analysis, reporting, etc.) can lead to detrimental economic effects. One of the strategies to deal with this problem is so-called soft sensors. Soft sensors are a collection of models that can be used to predict and forecast some infrequently measured properties (such as laboratory measurements of petroleum products) based on more frequent measurements of quantities like temperature, pressure and flow rate provided by physical sensors [1]. Soft sensors short-cut the pathway to obtain relevant information about the product quality, often providing relevant measurements as frequently as every minute. One of the applications of soft sensors is for the real-time optimization of a chemical process by a targeted adaptation of operating parameters. Models used for soft sensors can have various forms, however, among the most common are those based on artificial neural networks (ANNs) [2]. While soft sensors can deal with some of the issues in the refinery processes, their development and deployment can pose other challenges that are addressed in this paper. Firstly, it is important to enhance the quality of both sets of data (laboratory measurements and physical sensors) in a so-called data pre-processing stage (as described in Methodology section) [3]. Secondly, once the data sets are preprocessed, different models need to be tested against prediction error and the model’s interpretability. In this work, we present a framework for soft sensor development from raw data to ready-to-use models.",ArXiv,2021.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
09c8f025a19693d5ebd97794eb2c480746cccaf7,https://www.semanticscholar.org/paper/09c8f025a19693d5ebd97794eb2c480746cccaf7,Mems Based Motor Fault Detection in Windmill Using Neural Networks,"Today wind turbine technology is one of the fastest growing power generation technologies operating in large numbers at harsh and difficult environment sites and it is difficult to monitor each and every windmill separately. There are times when faults occur in motors of windmills are not detected in earlier stage and we come to know about damage when motor gets fully damaged. Here we using wireless monitoring based on MEMS accelerometer sensor which senses the vibrations occurring in the motor and based on the severity of vibrations, sensor sends the data to the controlling unit to take further action. Neural network based work is included to get the accurate and precise vibratory signals to detect fault at a very early stage to avoid full damage to the motor. KeywordsAccelerometer, MEMS, Neural network. I. I.INTRODUCTION Condition monitoring and fault diagnosis of induction motors are of great importance in production lines. It can significantly reduce the cost of maintenance and the risk of unexpected failures by allowing the early detection of potentially catastrophic faults. In condition based maintenance, one does not schedule maintenance or machine replacement based on previous records or statistical estimates of machine failure. Rather one relies on the information provided by condition monitoring systems assessing the machine's condition. Thus the key for the success of condition based maintenance is having an accurate means of condition assessment and fault diagnosis. On-line condition monitoring uses measurements taken while a machine is in operating condition. There are around 1.2 billion of electric motors used in the United States, which consume about 57% of the generated electric power. Over 70% of the electrical energy used by manufacturing and 90% in process industries are consumed by motor driven systems. Among these motor systems, squirrel-cage induction motors (SCIM) have a dominant percentage because they are robust, easily installed, controlled, and adaptable for many industrial applications. SCIM find applications in pumps, fans, air compressors, and machine Tools, mixers, and conveyor belts, as well as many other industrial applications. Moreover, induction motors may be supplied directly from a constant frequency sinusoidal power supply or by an a.c. variable frequency drive. Thus condition based maintenance is essential for an induction motor. It is estimated that about 38% of the induction motor failures are caused by stator winding faults, 40% by bearing failures, 10% by rotor faults, and 12% by miscellaneous faults. Bearing faults and stator winding faults contribute a major portion to the induction motor failures. Though rotor faults appear less significant than bearing faults, most of the bearing failures are caused by shaft misalignment, rotor eccentricity, and other rotor related faults. Besides, rotor faults can also result in excess heat, decreased efficiency, reduces insulation life, and iron core damage. So detection of mechanical and electrical faults are equally important in any electrical motor. II. II.EXISTING SYSTEM The existing system is based on acoustic emission sensor (Hall Effect sensor). 1-Acoustic emission (AE) Acoustic emission (AE) is the sound waves produced when a material undergoes stress (internal change), as a result of an external force. AE is a phenomenon occurring in for instance mechanical loading generating sources of elastic waves. This occurrence is the result of a small surface displacement of a material produced due to stress waves generated when the energy in a material or on its surface is released rapidly. The wave generated by the source is of practical interest in methods used to stimulate and capture AE in a controlled fashion, for study and/or use in inspection, quality control, system feedback, process monitoring and others. AE is commonly defined as transient elastic waves within a material, caused by the release of localized stress energy. Hence, an event source is the phenomenon which releases elastic energy into the material, which then propagates as an elastic wave. Acoustic emissions can be detected in frequency ranges under 1 kHz, and have Mems Based Motor Fault Detection In Windmill Using Neural Networks www.ijres.org 54 | Page been reported at frequencies up to 100 MHz, but most released energy within 1 kHz to 1 MHz. Rapid stress releasing events generate a spectrum of stress waves starting at 0 Hz, and typically falling off at several MHz . Fig:1 AE sensor working Acoustic emission (AE) sensors have been used to characterize wear in machine tools, and monitor bearing and gear problems in centrifugal pumps. First developed as a NonDestructive Testing (NDT) technique to detect cracks in civil structures, these sensors detect acoustic emissions generated by the release of vibration waves in a crystalline lattice due to plastic deformation or crack growth. Measurements are made using piezoelectric transducers with high natural frequencies, 100 kHz to 1 MHz, to capture the ultrasonic AE emissions. An AE sensor is useful as it has the ability to detect subsurface cracks in gear teeth or bearings before appearing on the surface causing further damage. More recently MEMS acoustic sensors have been developed, and one design includes multiple transducers on a single substrate, which each detect acoustic emission energy at different frequencies. This helps distinguish spurious acoustic emissions arising from impact and friction, from those arising from plastic deformation. When compared to typical piezoelectric sensors, the MEMS devices have lower sensitivities and fail to detect some acoustic emissions. In addition, the acoustic emission signal suffers severe attenuation as is crosses various interfaces, such as a gearbox or bearing casings. In one experiment consisting of a pinion gear and an associated bearing, a 44dB attenuation was seen between an AE sensor placed directly on the pinion to one placed on the bearing casing,and in some cases, intermediate loss of the signal was observed. III. PROPOSED SYSTEM Wireless communicators were deployed on the turbines to provide a stable and trouble free communication network. The units were installed on the turbines. All the above problems were brushed away by the wireless solution. The maintenance team could now concentrate on maximizing power generation rather than waste time on maintaining the network. In the proposed method, vibration signals are obtained using piezoelectric sensor and motor current signature analysis is performed using Hall Effect sensor. The features of the signal are analyzed using wavelet packet transform. Besides other signal processing techniques, wavelet packet transform is preferred because it has certain advantages. Traditional signal processing techniques like Fourier transform can perform only on stationary signals. Since it is not well suited for non-stationary signals Short time Fourier transform (STFT) is used. STFT uses a constant window function as a base to obtain the frequency spectrum coefficients. The size of the window function cannot be changed which led to the need for wavelet transform. Wavelet transform uses a varying size window function as its base. In wavelet transform low frequency signals are decomposed repeatedly to obtain low frequency information. In wavelet transform the information about high frequency signals are limited. In the proposed method, wavelet packet transform decomposes both low frequency and high frequency information. It can analyze both stationary and nonstationary signals. There are many classifier models to effectively classify the faulty data from the healthy one. They are:  Analytical model-based methods,  Artificial Intelligence-based methods. Analytical model based methods are efficient monitoring systems for providing warning and predicting certain faults in their early stages. Artificial Intelligence based methods are of two categories: Knowledge based models and Data based models. When considering fault diagnostics of induction motor it is difficult to develop an analytical model that describes the performance of a motor under all its operation points. It is difficult for a human expert to distinguish faults from the healthy operation. Though analytical based methods and knowledge based methods are effective classification methods, their performance in induction motors is not good. Moreover conventional methods cannot be applied effectively for vibration signal diagnosis due to their lack of adaptability and the random nature of vibration signal. In such a situation, data based models are used to classify faults in induction motors. Some of the popular data based models are neural networks, fuzzy systems and Support vector machine. Neural networks and fuzzy logic are widely used in the field of fault diagnostics. Fuzzy logic provides a systematic framework to process vague and qualitative knowledge. Using fuzzy logic it is Mems Based Motor Fault Detection In Windmill Using Neural Networks www.ijres.org 55 | Page possible to classify a fault in terms of its degree of severity. Artificial neural network are modeled with artificial neurons. Each artificial neuron accepts several inputs, applies preset weights to each input and generates a nonlinear output based on the result. The neurons are connected in layers between the inputs and outputs. Support Vector Machine, a novel machine learning technique is used in this paper. It is based on statistical learning theory, and is introduced during the early 90‟s. SVM is opted in this paper since it is shown to have better generalization properties than traditional classifiers. Efficiency of SVM does not depend on the number of features of classified entities. Property is very useful in fault diagnostics, because the number of features to be chosen to be the base of fault classification is thus not limited. Industrial Motor‟s condition monitoring systems collect data from ",,2014.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
53c2de154f13c605ff2a9374c93a6a3e0604f670,https://www.semanticscholar.org/paper/53c2de154f13c605ff2a9374c93a6a3e0604f670,Machine Learning Approach for Predictive Maintenance of the Electrical Submersible Pumps (ESPs),"Electrical submersible pumps (ESPs) are considered the second-most widely used artificial lift method in the petroleum industry. As with any pumping artificial lift method, ESPs exhibit failures. The maintenance of ESPs expends a lot of resources, and manpower and is usually triggered and accompanied by the reactive process monitoring of multivariate sensor data. This paper presents a methodology to deploy the principal component analysis and extreme gradient boosting trees (XGBoosting) in predictive maintenance in order to analyze real-time sensor data to predict failures in ESPs. The system contributes to an efficiency increase by reducing the time required to dismantle the pumping system, inspect it, and perform failure analysis. This objective is achieved by applying the principal component analysis as an unsupervised technique; then, its output is pipelined with an XGBoosting model for further prediction of the system status. In comparison to traditional approaches that have been utilized for the diagnosis of ESPs, the proposed model is able to identify deeper functional relationships and longer-term trends inferred from historical data. The novel workflow with the predictive model can provide signals 7 days before the actual failure event, with an F1-score more than 0.71 on the test set. Increasing production efficiencies through the proactive identification of failure events and the avoidance of deferment losses can be accomplished by means of the real-time alarming system presented in this work.",ACS omega,2022.0,10.1021/acsomega.1c05881,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
80040e993990f21db4741381c02165c3db06ace0,https://www.semanticscholar.org/paper/80040e993990f21db4741381c02165c3db06ace0,Modeling and Integrating Cognitive Agents Within the Emerging Cyber Domain,"One of the elements missing from virtual environments in the emerging cyber domain is an element of active opposition. For example, in a training simulation the instructor assigns the student a task or objective, and the student then practices within the environment (the “cyber range”) until they feel comfortable with the task or are able to demonstrate the requisite level of mastery. The environment may have static defenses, such as access control or firewalls, or a fixed set of intrusion methods to defend against, but it typically lacks any active opposition that might adapt defensive or offensive actions (e.g., monitor logs, blocked connections, exploit switching or information gathering). This is akin to training fighter pilots against adversaries who know how to use their weapons, but do not have any tactical or strategic goals beyond that. This is unfortunate for two reasons: 1) it trains cyber operators to behave as though opponents do not have a tangible existence or do not have higher-level goals, and 2) it ignores an opportunity to tailor the student’s learning experience through adjustable adversary behavior. Cognitive agents have the potential to transform the cyber operations training experience. The application of cognitive agents to the roles of cyber offense and defense would provide a more complete cyber ecology for training purposes and thus a more realistic training experience for the student. There are two key challenges to creating such cyber agents: 1) modeling the complex, and continually evolving, processes of cyber operations within a cognitive architecture, and 2) defining the tools and data standards to enable cognitive agents to interoperate with networks in a portable way. This paper discusses novel models of cyber offensive and defensive behavior based on observation and elaboration of human expertise, as well as an approach to the creation of software adapters that translate from task-level actions to network-level events to support agent-network interoperability. ABOUT THE AUTHORS Randolph M. Jones, PhD, is a senior artificial intelligence engineer at Soar Technology, and co-founded Soar Technology in 1998. Dr. Jones received his BS in mathematics and computer science from UCLA, and he received his M.S. and Ph.D. in information and computer science from the University of California, Irvine. Ryan O’Grady is the technical lead for Soar Technology’s emerging business area in cyberspace training and visualization, and a senior software engineer in the Intelligent Training business area. Mr. O’Grady received a BSE in Computer Science Engineering from the University of Michigan in 2004. Certifications: Security+, CPTE, OSCP Denise Nicholson, PhD, CMSP, is the Director of Soar Technology’s new Technology Area ""X"" leading an effort to explore, identify and pursue innovative applications of intelligent systems for critical and challenging problems, such as Cyber Security. Dr. Nicholson has a Ph.D. and M.S. in Optical Sciences from the University of Arizona, and a B.S. in Electrical Computer Engineering from Clarkson University. Robert Hoffman, PhD, is a Senior Research Scientist at the Florida Institute for Human and Machine Cognition (IHMC). He is senior editor of the Department on Human-Centered Computing of IEEE: Intelligent Systems. His latest book is Accelerated Expertise: Training to High Proficiency in A Complex World (2014, Taylor & Francis). Larry Bunch is a Senior Research Associate at IHMC. He received his BS in computer science from the University of West Florida and has published extensively concerning software agents, semantic policies and reasoning, and large-scale event visualizations. Jeffrey M. Bradshaw, PhD, is a Senior Research Scientist at IHMC. He co-edits the HCC Department of IEEE Intelligent Systems and has published widely in software agents, semantic technologies, digital policy management, and human-agent-robot teamwork (HART). Ami Bolton, PhD, is a Program Officer at the Office of Naval Research (ONR). Her programs focus on enhancing individual and team decision-making and combat effectiveness through advances that improve perception, cognition, and team coordination. Dr. Bolton received a M.S. in Human Factors from the Florida Institute of Technology, and Ph.D. in Applied Experimental & Human Factors Psychology from University of Central Florida. Interservice/Industry Training, Simulation, and Education Conference (I/ITSEC) 2015 2015 Paper No. #15232 Page 2 of 10 Modeling and Integrating Cognitive Agents Within the Emerging Cyber Domain Randolph M. Jones, Ryan O'Grady, Denise Nicholson, Robert Hoffman, Larry Bunch, Jeffrey Bradshaw, and Ami Bolton Soar Technology IHMC Office of Naval Research Ann Arbor, MI Pensacola, FL Arlington, VA rjones@soartech.com, ryan.ogrady@soartech.com, denise.nicholson@soartech.com, rhoffman@ihmc.us, lbunch@ihmc.us, jbradshaw@ihmc.us, amy.bolton@navy.mil Cyber warfare presents a persistent and evolving threat to military and civilian information systems. Both DoD (Parrish, 2013) and ODNI (Pellerin, 2013) rank cyber warfare as our top national security concern. In addition to threats to our defensive forces, cyber attacks pose an economic threat on the order of one trillion dollars (Ponemon, 2013). Although individual cyber-warfare tools operate at extremely fast speeds, aggressors increasingly pursue a “cyber kill-chain” (Hutchins et al., 2010) over days, weeks, or months. Would-be cyber aggressors are constantly changing their attack vectors to take advantage of security lapses by human resources and the latest vulnerabilities in information technology. These human-speed activities are guided by cognitive behavior that includes a variety of types of goals and expertise: script-kiddies, ideological activists, investigators, financial criminals, intelligence agents, or cyber warfighters (Lathrop et al., 2010). At the human, cognitive level, offense depends on and reacts to responses of defenders (Pfleeger & Caputo, 2012) and users (Bowen et al., 2012) that are also cognitively driven. Current cyber-warfare tools comprise suites of technical mechanisms that respond to the tools that aggressors and defenders use, but not to the individuals themselves. Human tactics are currently addressed through human-staffed wargames at cyber ranges (Merit, 2013; Pridmore, 2012). Human role-players are expensive, not repeatable, and not deployable as an automated system. There is an emerging need for cognitive-level synthetic cyber offense and defense, to ensure realistic cyber simulation and training. Building effective training systems for cyber warfare presents a suite of unique problems: Offensive and defensive activity is highly dynamic. The characteristics of target network environments are driven by the users of the system and their current activities, which are highly variable and unpredictable. User behavior often creates vulnerabilities that can be exploited. Cyber warfighters themselves are extremely adaptive and creative. In order to meet their objectives they will change tactics or tools based on opportunities detected in a computer network or responses initiated by adversaries or users. Current training environments do not adequately capture the dynamic and cognitive-level characteristics of cyber warfare. They are unable to capture the purposefulness, creativity, and adaptability of actual cyber warfighters. Studying previous offensive and defensive scenarios in a classroom environment is an effective means of understanding the building blocks of cyber warfare, but falls short of creating the skills needed to deal with a creative and time-sensitive event or a sophisticated but dynamic plan. Computerized unit tests can build fundamentals for dealing with individual components cyber warfare, but they do not help the trainee learn to recognize and make sense of the larger picture, nor do they capture the dynamic nature of networks and users. If cyber warfighters are to learn to respond to a cunning and adaptive opponent, they need to train against cunning and adaptive opponents. An effective cyber-warfare training system must be adaptable and deal with the changing nature of a networked environment. It must be able to model the dynamic nature of cyber aggressors, users, and defenders. It must create a virtual environment that replicates the environment that the trainee will ultimately operate in. An appropriate virtual environment also creates the opportunity for accurate post-event forensic analysis by providing access to databases, configurations, and system logs. This paper presents our efforts to address these issues through the development of cognitive agents for cyber offense and defense. The Soar cognitive architecture described in this paper is not to be confused with Soar Technology, the affiliation of some of the authors. Soar is not a commercial product, but is available under a General Public License from http://soar.eecs.umich.edu/ maintained by the University of Michigan. The Soar architecture provides the technological foundation for the cognitive agents described here. Interservice/Industry Training, Simulation, and Education Conference (I/ITSEC) 2015 2015 Paper No. #15232 Page 3 of 10 MODELING AND INTEGRATION CHALLENGES In order to build realistic cognitive agents, the agents must encode appropriate domain expertise, and they must interact with a realistic cyber environment (Jones & Laird, 1997). In addition to realism, cost effective cognitive agents also need to address these related issues: Reduce cost of realistic role playing in cyber-warfare simulation, system engineering, and analysis of cyber operations. Enable end-user updating of agent knowledge with minimal support from software engineers, both through coaching by instructor Subject-Matter Experts (SMEs) and through explicit addition of new knowledge about cyber tactics. Be readily adaptable to a wide range of network structures, devices, and protocols. In o",,2015.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
31b37b6f3ed31f4307000f4b0d8172f1bb9215d3,https://www.semanticscholar.org/paper/31b37b6f3ed31f4307000f4b0d8172f1bb9215d3,Online Monitoring of Inner Deposits in Crude Oil Pipelines,"
 The formation of deposits is a very common issue in oil and gas pipeline transportation systems. Such sediments, mainly wax and paraffine for crude oil, or hydrates and water for gas, progressively reduce the free cross-sectional area of the pipe, leading in some cases to the complete occlusion of the conduit. The overall result is a decrease in the transportation performance, with negative economic, environmental, and safety consequences. To prevent this issue, the amount of inner deposits must be continuously and accurately monitored, such that the corresponding cleaning procedures can be performed when necessary. Currently, the former operation is still dictated by best-practice rules pertaining to preventive or reactive approaches, yet the demand from the industry is for predictive solutions that can be deployed online for real-time monitoring applications. The paper moves toward this direction by presenting a machine learning methodology that leverages pressure measurements to perform online monitoring of the inner deposits in crude oil trunklines. The key point is that the attenuation of pressure transients within the fluid is dependent on the free cross-sectional area of the pipe. Pressure signals, collected from two or more distinct locations along a pipeline, can therefore be exploited to estimate and track in real time the presence and thickness of the deposits. Several statistical indicators, derived from the attenuation of such pressure transients between adjacent acquisition points, are fed to a data-driven regression algorithm that automatically outputs a numeric indicator representing the amount of inner pipe debris. The procedure is applied to the pressure measurements collected for one and a half years on discrete points at a relative distance of 40 and 60 km along an oil pipeline in Italy (100 km length, 16-in. inner diameter pipes). The availability of historical data prepipe and postpipe cleaning campaigns further enriches the proposed data-driven approach. Experimental results demonstrate that the proposed predictive monitoring strategy is capable of tracking the conditions of the entire conduit and of individual pipeline sections, thus determining which portion of the line is subject to the highest occlusion levels. In addition, our methodology allows for real-time acquisition and processing of data, thus enabling the opportunity for online monitoring. Prediction accuracy is assessed by evaluating the typical metrics used in the statistical analysis of regression problems.",SPE Production &amp; Operations,2022.0,10.2118/209825-pa,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
3dac1931e00a8a4a3ee8e7061e99152eb39eb8ed,https://www.semanticscholar.org/paper/3dac1931e00a8a4a3ee8e7061e99152eb39eb8ed,"NICTA ATP lab: Eveleigh, Sydney, Australia","N ICTA (National ICT Austra-lia) is Australia's Information and Communications Technology Research Centre of Excellence. NICTA develops technologies that generate economic, social, and environmental benefits for Australia. NICTA collaborates with industry on joint projects, creates new companies, and provides new talent to the ICT sector through a NICTA-enhanced Ph.D. program. With six laboratories around Aus-tralia and more than 700 people, NICTA is the largest organization in Australia dedicated to ICT research. NICTA's research focuses on the following areas: computer vision, control and signal processing, machine learning, networks, optimization, and software systems. I am a Ph.D. student in the School of Engineering and Information Technologies at the University of Syd-ney. As well as a graduate researcher at NICTA Australian Technology Park (ATP) Laboratory in the Networks Research Group, which aims to improve overall user experience in accessing information and services by looking into ways to leverage the existing and researching into new infrastructure. NICTA provides many resources and other great opportunities that enhance research. My research focuses on large-scale networking systems. In particular we are developing Moana, a new Internet service abstraction. The Internet was originally designed for efficient data communication between end devices, but this is an increasingly ill-fitting model for today's information cen-tric services. We therefore argue for a network service model that instead focuses on interlinking information providers and consumers through a global information network. Moana is a global information storage and dissemination network, which simplifies developing and deploying of information rich applications. Moana loosely couples applications through a global information space that uses a Pub/ Sub communication model to communicate by publishing and subscribing to information. We are aiming to close the gap between the information model of an application and the supporting network service. We hope Moana will inspire the development of new types of information-centric applications by simplification of storage and dissemination and retrieval of information on an Internet-scale. NICTA has strong collaborations with leading research institutions around the world; we frequently get visiting researchers and intern students from all over Australia and the globe. This is a great way to extend our professional network and collaborate with other research institutions. In addition, students are offered a range of short courses for free. These courses are lectured by some of the best LABZ NICTA is all about growth, collectively as a lab and individually as a researcher.",XRDS,2014.0,10.1145/2590779,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
fd4df295b1dad849490b78efe9a1671436df6d09,https://www.semanticscholar.org/paper/fd4df295b1dad849490b78efe9a1671436df6d09,Blockchain outlook for deployment of IoT in distribution networks and smart homes,"Nowadays, unlike depleting fossil fuel resources, the integration of different types of renewable energy, as distributed generation sources, into power systems is accelerated and the technological development in this area is evolving at a frantic pace. Thus, inappropriate use of them will be irrecoverably detrimental. The power industry will reach a turning point in the pervasiveness of these infinite energy sources by three factors. Climate changes due to greenhouse gas accumulation in the atmosphere; increased demand for energy consumption all over the world, especially after the genesis of Bitcoin and base cryptocurrencies; and establishing a comprehensive perspective for the future of renewable energy. The increase in the pervasiveness of renewable energy sources in small-scale brings up new challenges for the power system operators to manage an abundant number of small-scale generation sources, called microsources. The current structure of banking systems is unable to handle such massive and high-frequency transactions. Thus the incorporation of cryptocurrencies is inevitable. In addition, by utilization of IoT-enabled devices, a large body of data will be produced must be securely transferred, stored, processed, and managed in order to boost the observability, controllability, and the level of autonomy of the smart power systems. Then the appropriate controlling measures must be performed through control signals in order to serve the loads in a stable, uninterruptible, reliable, and secure way. The data acquires from IoT devices must be analyzed using artificial intelligence methods such as big data techniques, data mining, machine learning, etc. with a scant delay or almost real-time. These measures are the controversial issues of modern power systems, which are yet a matter of debate. This study delves into the aforementioned challenges and opportunities, and the corresponding solutions for the incorporation of IoT and blockchain in power systems, particularly in the distribution level and residential section, are addressed. In the last section, the role of IoT in smart buildings and smart homes, especially for energy hubs schemes and the management of residential electric vehicle supply equipment is concisely discussed.",International Journal of Electrical and Computer Engineering (IJECE),2020.0,10.11591/ijece.v10i3.pp2787-2796,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
6f9a8003fbe29f7586f94d1a9d0d675760a1e9a3,https://www.semanticscholar.org/paper/6f9a8003fbe29f7586f94d1a9d0d675760a1e9a3,Low-cost passive wireless water leak sensor for the automotive industry,"The evolution of wireless technologies has shifted the expectations on how sensors anddata collection is performed. By reducing the complexity to deploy and connect sensors, anincrease in the variety and quantity of data collected is leading to the concept of ubiquitoussensing.However, there are measurements that still rely on manual work. In particular, the auto-motive industry still performs manually the rain isolation quality tests. These tests requirenowadays for an employee to manually place a handheld moisture sensor device in con-tact with the area to analyse, taking long time, low scalability and prone to human errors.Additionally, workers are exposed to these humid conditions which is hazardous for theirhealth.In order to tackle this problem, low-cost wireless identification tags are proposed in thiswork to be used instead. These tags, known as Radio Frequency Identification (RFID),are bundled in pairs, one waterproofed and one exposed to work as moisture detectionsensor. By interacting with them with a RFID telecommunications system, low level datais collected to perform predictions about their status. This way, it is possible to measuresignal differences when subjected to different quantities of water exposure, both in staticand in-movement scenarios, which lead to predictive methods of water leaks.In this work, different methodologies and setups will be evaluated on their performanceas predictors of wet RFID tags. The project involves researching the state of the art onthis topic, empirical experimentation of the different proposed methods, development of anautomated software platform and potential contribution to the academia with the resultsfound, by means of paper publication in conferences and/or journals. In particular, twodifferent experimental setups were tested, a single antenna setup and dual antenna setup.These setups were implemented in a controlled scenario, where 48 different experimen-tal iterations were performed. Later, the setup was implemented on a in-vehicle scenarioto emulate the real industrial operating scenario. In this scenario up to 70 different ex-perimental iterations where performed. When analysing the data samples, four differentmethods are proposed and implemented. Finally, the project also includes the evaluationthrough machine learning algorithms of the best method which achieved higher than80%accuracy test on predicting water leakages in real life scenario such as automotive “raintests”, through the developed sensors and system.With this research, the viability of this methodology for wireless detection of leaks hasbeen shown. Next steps are to explore different Tags manufacturers or complex scenariosinside production lines.",,2020.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
75fccf9eda7cf9c961efd3e9d42ba73ed37ab2e9,https://www.semanticscholar.org/paper/75fccf9eda7cf9c961efd3e9d42ba73ed37ab2e9,Tutorial: Edge Computing for Mobile Internet of Things,"Internet of things (IoT) has emerged as the enabling technology for smart applications in different domains, such as transportation, health-care, industry, smart homes and buildings, and education (e.g., [1-5]). IoT applications rely on the deployment of resourceconstrained devices that collect data from the environment it is immersed and control events of interest through actuators. One of the daunting challenges in many IoT applications is the need for the real-time processing of a large amount of produced data. Such processing is often impractical to be performed at the IoT devices, due to their resource-constrained nature and the incurred energy cost. In this regard, IoT data is often offloaded to be processed on distant powerful cloud servers, which return to IoT devices as the result of the heavy computations. This approach is well-suited for computation-intensive tasks in IoT applications. However, the process of task offloading to cloud servers incurs additional delays for the IoT application, in addition to the network overhead. Therefore, edge computing has been proposed to provide computation, communication, and storage resources closer to IoT devices. The general idea is to place resources in the proximity of IoT devices that will demand them. Thus, the latency involved in the IoT application is reduced since computation-intensive tasks are processed on edge devices rather than on distant cloud servers. One of the critical challenges in edge-aided IoT applications is that edge devices have limited resource capabilities when compared to cloud servers. In this regard, edge devices' resources must be managed and allocated in an efficient way, aimed at providing resources to IoT applications with guaranteed quality of service (QoS). This tutorial will motivate and explore the challenges, design principles, and goals of edge computing for IoT applications. It presents the building blocks for the design of optimization models for IoT task offloading to edge nodes. By doing so, it discusses the communication challenges between IoT and edge devices and highlights the different mathematical formulations commonly used in the literature to model IoT to edge communication. Furthermore, this tutorial discusses optimization-based and machine learning (ML)-based solutions for tackling the task offloading decision problem. Besides, this tutorial presents recent advancements in resource management solutions aimed at efficient resource allocation at edge devices. Finally, this tutorial shall conclude with a discussion of research opportunities and challenges in the edge-assisted Internet of things.",DIVANet,2021.0,10.1145/3479243.3494705,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
97b1ea3015d5ccb48885981a31021fb330e58274,https://www.semanticscholar.org/paper/97b1ea3015d5ccb48885981a31021fb330e58274,Top-Down Model Development Using Data Generated from a Complex Numerical Reservoir Simulation with Water Injection,"Top-Down Model Development Using Data Generated from a Complex Numerical Reservoir Simulation with Water Injection Yvon Martinez Numerical simulation and data-driven modeling are two current approaches in engineering reservoir modeling. Numerical reservoir simulation attempts to match past production history by modifying reservoir properties of the model. After multiple computationally intensive trial and error efforts, accurate history matches are identified. These history matches are used by project management for production forecasting purposes. Data-driven reservoir modeling utilizes measured data and is, therefore, free of assumptions that are often included in numerical reservoir simulations. Artificial intelligence and machine learning algorithms are technologies implemented in the development of a data-driven reservoir model with efforts to learn fluid flow through porous media from the datasets provided to the system. Training, calibration, and validation datasets ensure the success during the teaching process. Models, such as oil, gas, and water production, reservoir pressure and water saturation, are trained, calibrated, verified to ensure the success in the teaching process. After appropriate hyperparameter tuning, well-trained models are tested on blind datasets. This leads to the model being deployed on new datasets to again test the model’s performance during forecasting. The accuracy is based upon the model achieving a similar result to what numerical simulation found on the same dataset. The objective of this thesis is to use a 22-year dataset from a reservoir model generated by a numerical simulator that undergoes many complexities to approach the reality that takes place in the industry through the use of operational constraints, workover events requiring shut-in, random bottom hole pressure (“BHP”) trend, etc. Should the Top Down Model (“TDM”) be able to accurately learn the relationships between input attributes and outputs from the data, a similar procedure can then be applied to real field data in the future. The TDM’s capabilities will attempt to be proved when the blind validation’s in-time results match the data from the numerical simulation reservoir model. This will be done by excluding the data from the training, calibration, and validation datasets used to create the TDM. This thesis’s results should demonstrate that datadriven modeling is capable of history matching and forecasting the complexities of a reservoir.",,2020.0,10.33915/etd.7571,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
e06002981364c6655d7f70ac888eb53b4da0f731,https://www.semanticscholar.org/paper/e06002981364c6655d7f70ac888eb53b4da0f731,Dimensionality Reduction Methods of Electronic Nose Data for Bacteria Discrimination,"Electronic nose (EN) technology has emerged in the last decade as a viable means for analyzing and classifying samples based on odour. An EN instrument consists of an array of gas sensors that respond to a sample’s odorant molecules. A wide range of sensor materials now exist, including conducting polymers, metal oxides, and quartz crystal microbalance [1]. The array is constructed so that a wide range of compounds will invoke a response from the entire array, with individual sensor elements responding differently for samples from each odour category. The array response forms a unique “smellprint” that can be used to discern samples. Among the important advantages of EN systems are reduced sample preparation effort, decreased processing time, and simplicity of operation. As a result, they have been deployed for tasks such as process monitoring and quality control in several industries including pharmaceutical, food, and packaging [2]. The ability to process biological samples with EN has garnered interest of late – potential uses include food safety (detection of bacterial contaminants) and clinical diagnosis (pathogen identification). In both of these applications, EN testing will allow more expeditious results compared to traditional laboratory techniques (e.g. plating/culturing), thus allowing effective timely responses (e.g. issuing recalls and warnings on tainted food, or early patient treatment before a disease progresses). A growing body of research elicits optimism in this regard – the detection of food contaminants and pathogens responsible for several medical conditions (e.g. urinary tract infection, bacterial vaginosis) appear to be viable with commercially available EN systems [2,3]. In this paper, we used two EN modalities to investigate the detection and discrimination behaviour when processing bacteria samples of varying concentration provided by the Canadian Food Inspection Agency (CFIA), Ottawa, ON. In the food safety application, high sensitivity is of obvious importance. E. coli O157:H7, for instance, can cause infection at a dose of only 10 cells/mL [4]. Testing for the presence and/or type of bacteria in a candidate sample should ideally yield accurate results over a wide range of concentrations. In EN analysis, feature vectors for each sample are often highly dimensional, causing an exponential increase in volume of the feature space with each added dimension. This makes the number of degrees of freedom large in such cases, in turn causing analysis problems. This curse of dimensionality is a significant impediment in machine learning systems [5]. In this work, we employ uncorrelated LDA (ULDA) [6] feature reduction to mitigate this problem in the context of detecting and identifying bacteria of different concentrations, and compare its performance with principal component analysis (PCA). Several pitfalls that must be avoided for effective feature reduction are also discussed.",,2017.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
b77587379550f88dbb9d21b850ee69dad9014358,https://www.semanticscholar.org/paper/b77587379550f88dbb9d21b850ee69dad9014358,Learning-Based Reference-Free Speech Quality Assessment for Normal Hearing and Hearing Impaired Applications,"Accurate speech quality measures are highly attractive and beneficial in the design, finetuning, and benchmarking of speech processing algorithms, devices, and communication systems. Switching from narrowband telecommunication to wideband telephony is a change within the telecommunication industry which provides users with better speech quality experience but introduces a number of challenges in speech processing. Noise is the most common distortion on audio signals and as a result there have been a lot of studies on developing high performance noise reduction algorithms. Assistive hearing devices are designed to decrease communication difficulties for people with loss of hearing. As the algorithms within these devices become more advanced, it becomes increasingly crucial to develop accurate and robust quality metrics to assess their performance. Objective speech quality measurements are more attractive compared to subjective assessments as they are cost-effective and subjective variability is eliminated. Although there has been extensive research on objective speech quality evaluation for narrowband speech, those methods are unsuitable for wideband telephony. In the case of hearing-impaired applications, objective quality assessment is challenging as it has to be capable of distinguishing between desired modifications which make signals audible and undesired artifacts. In this thesis a model is proposed that allows extracting two sets of features from the distorted signal only. This approach which is called reference-free (nonintrusive) assessment is attractive as it does not need access to the reference signal. Although this benefit makes nonintrusive assessments suitable for real-time applications, more features need to be extracted and smartly combined to provide comparable accuracy as intrusive metrics. Two feature vectors are proposed to extract information from distorted signals and their performance is examined in three studies. In the first study, both feature vectors are trained on various portions of a noise reduction database for normal hearing applications. In the second study, the same investigation is performed on two sets of databases acquired through several hearing aids. Third study examined the generalizability of the proposed metrics on benchmarking four wireless remote microphones in a variety of environmental conditions. Machine learning techniques are deployed for training the models in the three studies. The studies show that one of the feature sets is robust when trained on different portions of the data from different databases and it also provides good quality prediction accuracy for both normal hearing and hearing-impaired applications.",,2018.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
14581d947f9856f442af4f707a704cd2cb2a34dc,https://www.semanticscholar.org/paper/14581d947f9856f442af4f707a704cd2cb2a34dc,Supercomputing-supported COVID-19 CT image comprehensive analysis assistant system,"Objective: The Coronavirus Disease 2019 (COVID-19) has become a global pandemic, causing millions of people to be infected worldwide Imaging analysis based on computed tomography (CT) data is an important means of clinical diagnosis A supercomputing-supported method is proposed for the construction of a new comprehensive CT analysis auxiliary system dealing with pneumonia Method: The system consists of four parts: input processing module, preprocessing module, imaging analysis subsystem, and artificial intelligence(AI) analysis subsystem Among the four parts, the imaging analysis subsystem detects the pneumonia features, distinguishes typical new coronary pneumonia by analyzing the typical imaging features, such as lung consolidation, ground-glass opacities, and crazy-paving pattern, and then comes to a conclusion of pneumonia The AI analysis subsystem uses a deep learning model to classify typical viral pneumonia and COVID-19, which enhances the screening ability of pneumonia Convolutional neural network is widely used as an effective algorithm for medical image analysis, particularly in image classification It is also widely utilized in the CT image screening and has achieved good results, which has attracted the attention of domestic and foreign scholars and industry The seasonable result derived from deep learning relies largely on the number and quality of training samples Given the lack of training samples, the system selects transfer learning as the technical direction for model construction Considering the quick response to the epidemic, the quality of easy maintenance and dynamic system updating is required Thus, after comparing and analyzing the performance and classification effect indexes of many common image classification models, we build a transfer learning neural network model on the basis of inception The entire neural network can be roughly divided into two parts: the first part uses a pre-trained inception network;the role of which is to convert image data into a one-dimensional feature vector The second part uses a fully connected network to improve classification prediction The imaging analysis method analyzes the image features of COVID-19, extracts the pneumonia feature areas, and carries out semantic analysis to achieve the delineation of the pneumonia target area Simultaneously, the typical imaging characteristics of COVID-19 (such as ground glass shadow, infiltration shadow, and lung consolidation) are targeted With regard to the pneumonia target area, a multi-level dynamic threshold segmentation is first used to determine the minimum lung tissue area (rectangular region of interest (ROI)) The extraction of the lung tissue area is designed as a normal workflow For each ROI, pixel statistics, threshold segmentation, regional dissolution and expansion, and abnormal proofreading are used to obtain the pneumonia target area Aiming at the relationship between the sizes of the pneumonia target area, a logical filter is established to detect the segmented distribution features and spatial relationship with the outer contour of the lung Then, based on the characteristic relationship of typical new coronary pneumonia, the typical characteristics of new coronary pneumonia are outlined The entire comprehensive analysis platform is built on the basis of the Tianhe artificial intelligence innovation-integrated platform The Tianhe artificial intelligence innovation-integrated platform is based on the hardware fusion supporting the environment of Tianhe supercomputing, cloud computing, and big data, upon which realizes the existing mainstream deep learning framework It is highly encapsulated with the processing model algorithm, forming a visual interactive template development environment covering multiple links such as data loading, model construction, training, verification, and solidified deployment As a service on this supporting platform, CT image comprehensive analysis AI auxiliary system has access to the computing resources, data resources, and external service capabiliti s of the platform and finally achieves the rapid integration and dynamic update of the system during the pandemic Result: After its release, the system has continuously and steadily provided new COVID-19 auxiliary diagnostic services and scientific research support for more than 30 hospitals and more than 100 scientific research institutions at home and abroad, providing important support for combating the epidemic Conclusion: The supercomputing-supported new coronary pneumonia CT image comprehensive analysis auxiliary system construction method proposed in this paper has achieved important application on diagnosis and research It is an effective way to achieve rapid deployment services and provide efficient support for emergencies The system applies artificial intelligence technology using CT imaging to screen for COVID-19 By applying artificial intelligence to the screening of COVID-19 with pneumonia and giving reference opinions for auxiliary diagnosis, the marking and area statistics of the inflammatory regions are improved The system achieves the combination of artificial intelligence traditional machine vision and deep learning technology to distinguish COVID-19 by using CT images The combined route of viral pneumonitis feature extraction based on traditional machine vision and the COVID-19 image classification based on artificial intelligence technology has achieved a comprehensive analysis of medical image features and COVID-19 screening The fast implementation mode of the fusion platform scenario is based on computing power and data support Relying on the Tianhe artificial intelligence innovation-integrated service platform, the platform supports intelligent frontier innovation on the basis of computing power and data, implements an open model of simultaneous research and application, and has a multi-industry training resource model library and large-scale distributed training sources With regard to rapid deployment and other service capabilities, this comprehensive analysis system is also the first public COVID-19 AI-assisted diagnostic system deployed online Analysis based directly on digital imaging and communications in medicine(DICOM) data and video data will effectively improve the analysis efficiency, but it will involve data ethics and security-related issues;however, it is the developing direction that needs to be resolved in the future © 2020, Editorial and Publishing Board of Journal of Image and Graphics All right reserved",,2020.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
6eea25b62ed1c6691d71e978206c96fe1de4a922,https://www.semanticscholar.org/paper/6eea25b62ed1c6691d71e978206c96fe1de4a922,Editorial: Advanced Industrial Networks with IoT and Big Data,,Mob. Networks Appl.,2019.0,10.1007/S11036-019-01228-4,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
9cc247ed02bbd6eb3dfa04637e105958ec07d79d,https://www.semanticscholar.org/paper/9cc247ed02bbd6eb3dfa04637e105958ec07d79d,Design considerations introducing analytics as a “dual use” in complex industrial embedded systems,"Embedded systems are today often self-sufficient with limited and predefined communication. However, this traditional view of embedded systems is changing through advancements in technologies such as, communication, cloud technologies, and advanced analytics including machine learning. These advancements have increased the benefits of building Systems of Systems (SoS) that can provide a functionality with unique capabilities that none of the included subsystems can accomplish separately. By this gain of functionality the embedded system is evolving towards a “dual use” purpose11In this paper we define dual usage as a control system having two purposes. In other contexts such as politics, diplomacy and export control, the term “dual-use” refers to technology that can be used for both peaceful and military aims, e.g., nuclear power technology., The use is dual in the sense that the system still needs to handle its original task, e.g., control and protect of an asset, and it must provide information for creating the SoS. Larger installations, e.g., industry plants, power systems and generation, have in most cases a long expected life-cycle, some up to 30–40 years without significant updates, compared to analytical functions that evolve and change much faster, i.e., requiring new types of data sets from the subsystems, not know at its first deployment. This difference in development cycles calls for new solutions supporting updates related to new requirements inherent in analytical functions. In this paper, within the context of “dual usage” of systems and subsystems, we analyze the impact on an embedded system, new or legacy, when it is required to provide analytic data with high quality. We compare a reference system, implementing all functions in one CPU core, to three other alternative solutions: a) a multi-core system where we are using a separate core for analytics, b) using a separate analytics CPU and c) analytics functionality located in a separate subsystem. Our conclusion is that the choice of analytics information collection method should to be based on intended usage, along with resulting complexity and cost of updates compared to hardware cost.",2021 26th IEEE International Conference on Emerging Technologies and Factory Automation (ETFA ),2021.0,10.1109/ETFA45728.2021.9613273,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
892e91a8a45bf5cc6b1ff03ebea279748f96bec7,https://www.semanticscholar.org/paper/892e91a8a45bf5cc6b1ff03ebea279748f96bec7,A new spatial spherical pattern model into interactive cartography pattern: multi-dimensional data via geostrategic cluster,,Multim. Tools Appl.,2022.0,10.1007/s11042-021-11339-4,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
0da04ad7ee5792897b455e6131478c9f2339ecaf,https://www.semanticscholar.org/paper/0da04ad7ee5792897b455e6131478c9f2339ecaf,Program verification,"I N 1 969, TON Y HOARE published a classical Communications’ article, “An Axiomatic Basis for Computer Programming.” Hoare’s article culminated a sequence of works by Turing, McCarthy, Wirth, Floyd, and Manna, whose essence is an association of a proposition with each point in the program control flow, where the proposition is asserted to hold whenever that point is reach. Hoare added two important elements to that approach. First, he described a formal logic, now called Hoare Logic, for reasoning about programs. Second, he offered a compelling vision for the program-verification project: “When the correctness of a program, its compiler, and the hardware of the computer have all been established with mathematical certainty, it will be possible to place great reliance on the results of the program, and predict their properties with a confidence limited only by the reliability of the electronics.” Hoare’s vision came under a scathing attack a decade later in an influential 1979 Communications’ article, “Social Processes and Proofs of Theorems and Programs,” by De Millo, Lipton, and Perlis. They argued that mathematical proofs are accepted through a social process. Program-correctness proofs will not be subject to a similar social process, due to their length and narrowness, so they will not be socially accepted. They concluded that “this makes the formal verification process difficult to justify and manage.” Hoare himself retracted, to some extent, his 1969 vision in 1995, writing “It has turned out that the world just does not suffer significantly from the kind of problems that our research was originally intended to solve.” In a parallel development, Amir Pnueli introduced the temporal logic of programs in 1977. Clarke and Emerson, and independently, Queille and Sifakis, then built on Pnueli’s work and developed, in the early 1980s, model checking, an algorithmic technique for checking properties of finite-state programs. That led to Pnueli receiving the ACM A.M. Turing Award in 1966, and Clarke-Emerson-Sifakis receiving the award in 2007. By the mid-1990s, several model checkers had been built and adopted for industrial usage by semiconductor and design-automation companies. Industrial temporal logics, such as PSL and SVA, based on Pnueli’s work, became industry standards in the early 2000s. The success of model checking in the semiconductor industry, where postproduction error correction is very difficult, points to an important insight that was missing in the early literature on program verification. Program verification is an expensive activity. Navigating the cost-benefit trade-off of program verification is ultimately a business decision. Model checking offered a different price point than full program verification: on one hand, model checking offers less— property checking and not full program verification, on the other hand, model checking costs less, due to higher level of automation. This cost-benefit trade-off suggests that how much verification should be done is context dependent. An operationsystem microkernel is probably a more appropriate target for a major verification effort than a dating app. Such an undertaking was initiated by an Australian team, who verified the seL4 microkernel using the Isabelle proof-assistant tool. Isabelle is based on a small logical core to increase the trustworthiness of proofs. This approach acknowledges that De Millo et al. were right—proofs do require a social process to be accepted—but in Isabelle (and similar tools) this social process can be confined to the small logic core. In fact, with the help of proof assistants, formal verification today is even bringing a new standard for rigor in mathematics. The emergence of cloud computing as the major context for much of today’s computing shifts the cost-benefit tradeoff of verification, due to its large scale. Because different users of the same cloud platform share hardware resources, security and privacy are of paramount interest. The Automated-Reasoning Group at Amazon Web Service (AWS) has been focusing on the development and use of formal-verification tools at AWS to increase the security assurance of its cloud infrastructure and to help customers secure themselves. At the same time, as the Spectre and Meltdown attacks have demonstrated, the large gap between the logical model (ISA) and the underlying microarchitecture of the X86 microprocessor not only provides side channels to attackers but also erects a major barrier to full verification. In 1969, Hoare wrote about mathematical certainty, great reliance, and confidence. In retrospect, the hope for “mathematical certainty” was idealized, and not fully realistic, I believe. Verification can give us great reliance and confidence, but at a cost that must be justified by the benefits. The deployment of autonomous systems with machine learning-based components brings new urgency and excitement to this important research area. Follow me on Facebook and Twitter.",Commun. ACM,2021.0,10.1145/3469113,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
f7fe5aee32641a18e1bb906d818af6e4a4bdcfb6,https://www.semanticscholar.org/paper/f7fe5aee32641a18e1bb906d818af6e4a4bdcfb6,Toward Scalable Artificial Intelligence in Finance,"Innovation in Artificial Intelligence (AI) continues to produce a wealth of techniques, mostly coming from the inductive form of AI also known as Machine Learning (ML). The majority of ML algorithms is industry-neutral and business process agnostic. ML innovation is propelled by publicly available research, which gets harvested into Open Source for wide distribution through software and Cloud vendors.Ongoing AI technology work creates an immense source of assets for data-driven modeling, delivered as software libraries. However, the application of these assets for data monetization in finance does not happen with nearly comparable success or speed. The latter challenge is commonly known as the ""scalability problem of AI"". As new techniques continue to grow vigorously, the investment from large finance institutions to cost-effectively produce applications for a variety of lines-of-business (LoBs) and business processes will increase. The availability of ML capabilities on Public Cloud is a way for enterprises to increase productivity by benefiting from the best AI assets available from providers and startups. But data is constrained in terms of location, access and use in most finance competences by either laws or internal Governance, Risk and Compliance (GRC) rules. Legal limitations include, and go beyond, Privacy Acts, impacting non-retail processes where AI techniques must be explained in layperson language to decision-makers and regulators before field deployment. The latter is not yet achieved satisfactorily. Lastly, a large percentage of AI projects fail, in part due to unsuitable ML modeling for analytics and forecasting problems in finance. The variety and complexity of human behavior present in most finance processes calls for understanding AI at a level of cognitive depth that has no precedent in other industries. It is imperative that AI be approached so that finance competence and functional specificity are embedded a-priori into ML techniques and not as use-case afterthoughts. For acceleration of AI assessments, it is critical that ML techniques available in software implement models readily aligned to finance problems.This paper presents an approach to building an Architecture for Artificial Intelligence (AI) in Finance by focusing on analytics and forecasting for business-to-business operations. This AI Architecture hinges on three axes and their interplay: Design Dimensions, Modeling Building-Blocks and Work-Practice. The goal is to support finance practitioners navigate the plethora of AI options more effectively and accelerate data monetization. While ML techniques in data analytics and forecasting apply to many scenarios, this paper focuses on selected competences in Banking, Financial Markets and Chief Finance Officer (CFO) operations.The architecture and method introduced in this paper is a first step toward a service practice. We harvest from our work carried out in banks, asset management firms and CFO lines-of-business as well as R&D experiences in new finance technologies for over one decade. As with any other architecture and deployment methodology, this work requires further harvesting, more information technology tools and sharing experiences across practitioners. It is hoped that finance organizations could adopt these new capabilities in their own Centers of Excellence or other internal organizations leading data-driven transformation and monetization across the firm.",2021 IEEE International Conference on Services Computing (SCC),2021.0,10.1109/scc53864.2021.00067,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
9ecf4af7a29186d4b09243469ea61c4126921df7,https://www.semanticscholar.org/paper/9ecf4af7a29186d4b09243469ea61c4126921df7,Multi-Source Spatial Entity Extraction and Linkage,"Web data sources contain large amounts of geo-social data, consisting of users, friendship/follower networks, check-ins, reviews, locations, etc., which are of great interest to academia and industry. There are publicly available datasets of samples of this web data, but they are very old (over ten years), not large, and not rich in attributes. Alternatively, one could use the public APIs to access and download web data. Unfortunately, this process is challenging due to the APIs limitations (e.g. the amount of data retrieved in a request, the number of requests performed within a timeframe, etc.). Thus, there is a need for algorithms that, given the limitations, are able to retrieve a good quality dataset from web sources, a need that the current state-of-the-art does not address. This thesis aims to provide algorithms and tools that can produce larger, recent, duplicate-free, and rich-in-attributes spatial entity data. To obtain larger and recent data from web data sources, we propose multi-source seed-driven (MSSD) algorithms that use the public free APIs to extract geosocial data. The MSSD algorithms aim to maximize the amount of data extracted while minimizing the number of requests and respecting each source's API limitations. The rationale behind the seeddriven algorithms is to perform some API requests for having an initial dataset and then use the points of the richest source as seed in the API requests for the rest of the sources. We propose different techniques for choosing the points and the radius of the search. We opt for a multi-source solution given that multiple sources provide independent information and diverse attributes as opposed to using only one source. Moreover, we experimentally demonstrate that using a single source algorithm sometimes converges to a dead end. The MSSD algorithms extract overall 14.3 times more data than the initial querying, and the optimized version MSSD* retrieves 90% of the data with less than 16% of the requests of the non-optimized version. When obtaining multi-source data, the same spatial entity might exist in different sources and sometimes even within the same source. These ""duplicates"" are not easy to detect since they have different attributes, they are expressed in different forms, and they might even contain contradicting attribute values. The problem of finding which pairs of spatial entities refer to a real physical entity is referred to as spatial entity linkage. We address this problem with several algorithms, which all share the same spatial blocking technique, and they use skylines to rank the compared pairs. The spatial blocking technique (QuadFlex) that we propose is a quadtree-inspired algorithm that groups the spatial entities based on the distance between them and the area's density. Moreover, it allows the assignment of spatial entities in more than one child to not miss any relevant comparisons. The spatial entities that fall into the same child are compared pairwise. To decide which pairs belong to the same physical entity, we propose novel skyline-based (SkyEx*) algorithms, which use preference functions to assign skylines to the pairs. The threshold-based SkyEx, SkyEx-F and SkyEx-FES require a threshold that is the number of skylines to separate the positive from the negative class, and they are able to achieve an F-measure of 0.72 on the whole dataset and 0.85 on a manually labeled sample. We introduce a fully unsupervised algorithm, SkyExD, which does not need a threshold and instead sets the cut-off based on the distance of the skylines. We demonstrate experimentally that SkyEx-D can reach a near-optimal F-measure (less than 0.01 loss). Additionally, we offer skyex, an R-package that implements the threshold-based and unsupervised skyline-based algorithms, supports the whole entity linkage pipeline with other stateof-the-art methods for entity blocking and comparisons, and provides a powerful Analysis and Visualization module to aid the explainability of the results. Besides the unsupervised algorithms, we propose a trained skyline-based algorithm, SkyEx-T, which is able to learn the preference function and the cut-off in tiny training sets (0.05%-1% of the dataset) and still achieve machine-learning-level accuracy. Moreover, the SkyEx-T model is fully explainable and readable, in contrast to the commonly-used black-box machine learning techniques. Furthermore, SkyEx-T has no weights nor layered architecture; consequently, it shows high robustness in deployment, while for the machine learning, some re-configuration and re-tuning of parameters might be needed when the new data arrives. Finally, we demonstrate that SkyEx-T cutoff closely approximates the optimal cut-off, even though it was learned on a tiny training set. With our algorithms in the spatial entity linkage, we ensure a duplicate-free dataset and rich-in-attribute spatial entities. Overall, this thesis contributes with effective and efficient algorithms for the initial and fundamental step of every geo-social research study: having recent, good-quality, rich-in-attributes datasets. We propose the MSSD-* algorithms that make the data extraction process more effective (14.3 times more data than the initial querying) while managing the requests carefully. We further improve the quality of the retrieved data by detecting pairs that refer to the same entity with high precision and recall while having an explainable and robust model (SkyEx-* algorithms). In the future, in the context of data extraction, we aim to work on hybrid algorithms that combine location-based with user-based and keyword-based API requests and use supervised techniques to learn the parameters of APIs. In the context of spatial entity linkage, we plan to work on hybrid blocking techniques that combine spatial attributes with textual and semantic ones, multi-class classification for the skyline-based algorithms, and crowdsourcing techniques for improving the labeling of the pairs. Members of the assessment committee are Associate Professor Manfred Jaeger, Aalborg University, Denmark, Associate Professor Maria Luisa Damiani, University of Milano, Italy, and Professor Konstantinos Stefanidis, University of Tampere, Finland. Professor Torben Bach Pedersen and Professor Esteban Zimányi are Suela Isaj’s supervisors. The moderator is Associate Professor Christian Thomsen.",,2021.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
d412e1c4e3f35f2d6006f1c661b24fc6370262bb,https://www.semanticscholar.org/paper/d412e1c4e3f35f2d6006f1c661b24fc6370262bb,Identification and Classification of Poultry Eggs: A Case Study Identification and Classification of Poultry Eggs: A Case Study Utilizing Computer Vision and Machine Learning Utilizing Computer Vision and Machine Learning,". We developed a method to identify, count, and classify chickens and eggs inside nesting boxes of a chicken coop. Utilizing an Internet of Things (IoT) AWS DeepLens Camera for data capture and inferences, we trained and deployed a custom Single Shot multibox Detector (SSD) model for object detection and classiﬁcation. This allows us to monitor a complex environment with multiple chickens and eggs moving and ap-pearing simultaneously within the video frames. The models can label video frames with classiﬁcations for eight breeds of chickens and/or four colors of eggs, with 98% accuracy on chickens or eggs alone and 82.5% accuracy while detecting both types of objects. With the ability to directly infer and store classiﬁcations on the camera, this setup can work in a low/no internet bandwidth setting in addition to an internet connected environment. Having these classiﬁcations beneﬁts farmers by providing the necessary base data required for accurately measuring the individual egg production of every chicken in the ﬂock. Additionally, this data supports comparative analysis between individual ﬂocks and industry benchmarks.",,,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
63263cefaffd056a826f1d9734256c91780b7923,https://www.semanticscholar.org/paper/63263cefaffd056a826f1d9734256c91780b7923,"Transitioning from Legacy Air Traffic Management to Airspace Management through Secure, Cloud-Native Automation Solutions","Advancements in Cloud-native services, Machine-Learning (ML), Artificial Intelligence (AI), and Rapid Application Development (RAD) using the Agile methodology has led countless industries to achieving desirable levels of automation while reducing cost and improving quality software deployments, timely / iterative delivery, and accountability. Coupling this framework with the principle of security as a shared responsibility further enhances the efficacy of an integrated Development, Security, and Operations (DevSecOps) Team within organizations to deliver secured digital solutions. Air Navigation Service Providers (ANSPs) around the world are currently exploring and embracing the digital evolution shifting from monolithic, legacy automation platforms to an application framework of microservices to allow for flexible operations as capabilities and airspace operations evolve. Specific to the US, the ATM automation system of today is comprised of both safety and non-safety critical systems, with mission-essential, efficiency-critical, and mission-support services that are predominately maintained and evolved through multi-year, one contractor-led programs. Although the system has proven resilient, it has not proven to be agile and flexible to allow for advances in capabilities on-board aircraft or in the data integration and sharing with other NAS automation systems. This creates significant overhead in development, sustainability, and operations of the current automation system, and leaves modernization efforts—in terms of new capabilities—in constant investment decision planning cycles, costing agencies not just money, but more time to innovate. To advance aviation into a new generation of interoperability leveraging collaborative frameworks and application specific capabilities, ANSPs must adapt to innovative methods to collect, process, and deliver critical and essential aeronautical, weather, and flight information to air traffic control operators and ultimately to airspace users. Doing so can not only lead to sustaining NAS automation systems while reducing the costs to develop and operate these systems, but it also provides an opportunity to present strategies on how to dramatically reduce the time and integration efforts needed to deploy new capabilities. Leveraging cloud-native technologies and services is a way to realize this automation evolution vision for ANSPs.This paper examines the migration from today’s systems to secure, cloud-native platforms to prove that Mission Services and Mission Applications can be rapidly available / deployable to operators who provide separation and flow management services, using a cyber-secured cloud-native environment. Aeronautical data typically used for tactical decision making is now seen as crucial to the decision-making process in Air Traffic Management (ATM). Integrating global and localized datasets into a digital aviation data platform enhances the capabilities of the solutions and opens the possibilities of leveraging big data analytics and microservices to compute trajectory predictions (TP), demand capacity balancing (DCB), arrival and departure sequencing, airspace delay, among others, in real-time to achieve operator-driven mission objectives. Technology has reached a state of maturity, especially in cloud and hybrid cloud solutions, to support safety of life operations, like ATM. This paper identifies approaches that are being considered for that migration to support the integration of new airspace entrants, the use of application services to provide a dynamic, evolutionary ATM platform, and addresses some of the safety and security strategies that must be considered for this evolution.",2021 IEEE/AIAA 40th Digital Avionics Systems Conference (DASC),2021.0,10.1109/dasc52595.2021.9594313,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
9384b351456209a871095c0a11bf3999e7b1b1c6,https://www.semanticscholar.org/paper/9384b351456209a871095c0a11bf3999e7b1b1c6,Deploying Artificial Intelligence for Component-Scale Multi-physical Field Simulation of Metal Additive Manufacturing,,,2020.0,10.1007/978-3-030-54334-1_19,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
8246bcab39de6760227a02e306a0823d86bcb1ad,https://www.semanticscholar.org/paper/8246bcab39de6760227a02e306a0823d86bcb1ad,Introduction to the Creation and Appropriation of Knowledge Systems Minitrack,"The objective of this minitrack is to contribute to the body of knowledge that helps scholars and practitioners increase their collective understanding of (1) how knowledge systems are planned, designed, constructed, implemented, used, evaluated, supported, upgraded, and evolved; (2) how knowledge systems impact the context in which they are embedded; and (3) the human behaviors reflected within and induced through both (1) and (2). By knowledge system, we mean a system in which human participants and/or machines perform work (processes and activities) related to the creation, retention, transfer and/or application of knowledge using information, technology, and other resources to produce informational products and/or services for internal or external customers. Such systems may include, but are not limited to, knowledge management systems, decision systems, social media, expert systems, machine learning systems, and other AI systems as well as any other IT-enabled knowledge management processes. It is the 6th year of the minitrack. We received eleven papers this year and after a rigorous review process, we accepted five papers for publication in the proceedings and presentation at the conference. The first paper, co-authored by Nico Wunderlich and Roman Beck, looks at how the concept of a digital business strategy leads to increased organizational innovativeness and firm performance. The research results reveal that IT knowledge of business employees has a higher positive impact on organizational innovativeness in organizations giving the digital business strategy high importance, whereas the top management team IT knowledge plays a greater role when digital business strategy is given low priority. The second paper, co-authored by Hans-Georg Fill and Felix Haerer, looks at how blockchain technologies can be applied to the domain of knowledge management. To validate the technical feasibility of the approach a first technical implementation is described and applied to a fictitious use case. The third paper, co-authored by Dimitris Karagiannis and Robert Andrei Buchmann, proposes a deployment approach for hybrid knowledge bases using agile diagrammatic means. The proposal is based on the RDF-semantics variant of OWL and leads to a particular type of hybrid knowledge bases hosted by the GraphDB system. The approach aims for complementarity and integration, providing means of creating semantic networks that are amenable to ontology-based reasoning. The fourth paper, co-authored by Kevin Lumbard, Ammar Abid, Christine Toh, and Matt Germonprez, focuses on understanding the types of information utilized in industry with the aim of enabling the development and sharing of methodologies that support the design of competitive products. As such, this paper proposes a framework of information archetypes utilized by designers in industry. The research results reveal two archetypes of information utilized by decision-makers within these companies during the development of new products and services. The fifth paper, co-authored by Olivia Hornung and Stefan Smolnik, presents a systematic review of existing evidence on the role of emotions in KM research. The review shows that despite KM’s long tradition, there is only limited evidence as to how emotions are related to KM, most of which mention emotions as motivation for KM. As a study’s result, the authors identify four research opportunities to further examine certain aspects of the role of emotions in KM. We wish to thank all of the authors who submitted work for consideration in this minitrack. We also thank the dedicated reviewers for the time and effort they invested in reviewing the papers. We believe that the accepted papers contribute to furthering our understanding on the creation and appropriation of knowledge systems. We look forward to discuss these further during our sessions in January 2018. Proceedings of the 51 Hawaii International Conference on System Sciences | 2018",,2017.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
aa7c857101ca206bfb40fb276b6fac505f81679e,https://www.semanticscholar.org/paper/aa7c857101ca206bfb40fb276b6fac505f81679e,AI-based algorithms and experimental evaluation for beyond 5 G,"The fifth-generation technology standard for cellular networks (5G) aims to provide high throughput, reduced latency, massive connection and a shift from service-orientation to user-orientation in requirements and innovations. To achieve the target goals, efficient resource allocation and management are required. In this regard, the concept of Network Function Virtualization (NFV) has been introduced, as well as the one of Software Defined Network (SDN). NFV is a state-of-the-art approach which replaces whole classes of network’s nodes functions, which were generally deployed on dedicated hardware, with pure software implementations. The virtualization of radio access networks (vRAN) comes under the NFV paradigm. The goal of vRANs is to centralize the virtualized radio access point (vRAP) processing stack, and to efficiently map the users’ requirements into radio and computational resources allocation. This progress is required by mobile operators in order to support the rising traffic demand and ensure the varied quality-of-service (QoS) requested at affordable cost. But currently, due to the high complexity of the dependencies between computing and radio resources, there still are not sufficiently efficient solutions to satisfy the telecommunications industry. In this work it is investigated and presented the design of a dynamic resource controller for vRANs based on machine learning techniques. Such a design allows to continuously adapt the resource allocation to the actual demand across vRAPs. In our configuration, the first acting entity is an encoder which accomplishes the task of translating the high-dimensional context information into a simpler representation. The context information is summed up in a vector per RAP containing samples of the traffic load to be managed and of the average and variation of the signal-to-noise ratio experienced by the users. It has been chosen to implement an independent deep autoencoder with nonlinear activation functions and sparsity constraint per context feature, in order to get the temporal correlation between its consecutive samples. The encoder is then followed by the proper resource manager, which has the duty of understanding the relationship between radio solutions and the corresponding computational load. The resource manager deployment is based on deep reinforcement learning techniques and it aims at the maximization of a reward signal which is proportional to the fulfilment of QoS requirements, traffic reliability and computational resources savings. The resource controller is composed by a CPU and a Radio policy. The first one is responsible for the distribution of the computational resources among all the RAPs. The last one is instead responsible for the choice of a bound for the modulation and coding schemes (MCS) to be adopted by each RAP, which strictly depends on the available computing capability. ii For what concerns the CPU policy, it has been designed a deep deterministic policy gradient algorithm based on an actor-critic model-free solution. While the actor neural networks is tasked with the choice of the CPU allocations, the critic provides the estimates for the received rewards. Such a model efficiently manages to cope with real valued actions and high-dimensional state spaces. The Radio policy instead relies on an iterative procedure in which a deep binary classifier judges in descending order different values for the upper bound to the eligible MCSs. The latter choice is fundamental in order to avoid incurring heavy deterioration in terms of system performance caused by CPU outages. The proposed design of a dynamic resource controller is a very promising one that potentially meets the stringent needs of telephone operators for a virtual access network that is able to meet the demands of the heterogeneous spectrum of users.",,2020.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
fd787b65bd7fdf7fc28e051acdc3d9310b3173dc,https://www.semanticscholar.org/paper/fd787b65bd7fdf7fc28e051acdc3d9310b3173dc,ANALYSIS AND FORECAST OF COMMODITY PRICE USING MACHINE LEARNING AND DEPLOYMENT IN WEB APPLICATION,"Prediction of financial market accurately is certainly significant. Commodity price fluctuations affects the global economic activity. Earning in export industry rely mainly on primary commodity and the movements of commodity prices has significant impact on overall economic progress for all countries. The method of forecasting plays a vital role in predicting adverse movements in case of commodity price prediction. In today’s world, the growth in deep learning outperforms in several demonstration in fields of financial market analysis. In this paper, we present a productionized commodity price analysis and prediction using LSTM model.  The LSTM model gathers data in a sequential order periodically of the commodity value.  Datasets used in our model are collected in real time from Yahoo Finance via API to reduce local storage.  The critical task is to transform a machine learning model into user accessible real time environment. [2]  Manual conversion of a machine learning model source code into a software is a time-consuming and error-prone task.  Investors and traders can buy and sell commodities directly in the spot market.",,2021.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
c54d8b94e3290ae5e02ffcfdc407e6d519ceec2f,https://www.semanticscholar.org/paper/c54d8b94e3290ae5e02ffcfdc407e6d519ceec2f,Medical Healthcare System with Hybrid Block based Predictive models for Quality preserving in Medical Images using Machine Learning Techniques,"Cloud technology is a business strategy that aims to provide the necessary material to customers depending on their needs. Individuals and cloud businesses alike have embraced the cloud storage service, which has become the most widely used service. The industries outsource their data to cloud storage space to relieve themselves of a load of dealing with redundant data contents. This must be protected to prevent the theft of personal belongings, and privacy must be improved as well. Different research projects have been suggested to ensure the safe management of the information included within the data content. The security of current research projects, on the other hand, still needs improvement. As a result, this method has been suggested to address the security concerns associated with cloud computing. The primary goal of this study effort is to offer a safe environment for cloud users while also increasing the profit of cloud resource providers by managing and securely delivering data contents to the cloud users. The bulk of sectors, including business, finance, the military, and the healthcare industry, do not store data in cloud-based storage systems. This technique is used to attract these kinds of customers. Increasing public acceptance Medical researchers are drawn to cloud computing because it allows them to store their study material in a centralized location and distribute and access it in a more flexible manner. They were collected from numerous individuals who were being evaluated for medical care at the time. Scalable and Enhanced Key Aggregate Cryptosystem is a protected data protection method that provides highly effective security in the health care industry. This approach handles disagreements in the outflow of sensitive information and guarantees the data security deployment of a Cloud-based Intelligent Health Monitoring system for the parties involved in the dispute. Using the suggested method, the encrypted data format of medical and health-care prescriptions is recorded as it passes through the hands of patients and healthcare institutions. To increase the level of security, the double encryption method is used. During the encryption process, the Ciphertext ID is referred to as a class. The keyholder is a master secret key that aids in the retrieval of the secret keys of different kinds of monsters and creatures. The extracted key is transmitted and kept as a single aggregate for the benefit of the patient or client to facilitate decryption. Between the use of a key aggregation cryptosystem and double encryption method, the Cloud-based Intelligent Health Monitoring systems may establish a secure link with Healthcare Organizations and patients. As a result, when compared to prior methods, the results demonstrate that the study methodology achieves high levels of security in terms of confidentiality and integrity, as well as great scalability.",2022 International Conference on Advanced Computing Technologies and Applications (ICACTA),2022.0,10.1109/ICACTA54488.2022.9753355,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
83338fac01dcee281ad4f41d05559a71a5d582bf,https://www.semanticscholar.org/paper/83338fac01dcee281ad4f41d05559a71a5d582bf,2019-01242-Temporary scientific engineer / Software development and machine learning for network security,"Scientific Context: In last years, Internet-of-Things became a reality with numerous protocols, platforms and devices [8] being developed and used to support the growing deployment of smart* services: smart-home, transport, -health, -city... and even the rather usual rigid systems with industry 4.0. Providing new services have required first the development of new functionalities with as underlining goals to have more powerand computeefficient devices which can embed various sensors. Obviously, IoT also supposes a full infrastructure to guarantee the efficiency of communications and processing of information. The embedded devices are thus completed by access points, routers, servers, etc. At the higher levels services are developed and provided to the users. This ecosystem is very rich and cannot be controlled by a unique entity, e.g. services are o en developed by third parties, manufacturer of embed devices are different to those providing connectivity... As a result, such a complex system is naturally a source of potential threats and real cases recently demonstrates that IoT can be affected by naïve weaknesses [1,6]. At Inria, we even demonstrated how simple and cheap can it be take over the control of a Z-Wave home installation in a silent manner [2]. Therefore, security is paramount of importance. In last decade, many IoT architectures have been proposed, such as the reference model IoT-A [3], including security modules. However, as highlighted before, security cannot be guaranteed without failure or by-design and this is all the more true with evolving ecosystems such as IoT, with now the emerging trend of using fog-based architecture rather than well-established cloud models. To enhance security, one option is to redesign an IoT architecture with stronger security but this will face the same problems as before, since some security issues can appear afterwards. Maintaining the architecture with new security elements would be therefore required but a remaining problems is the numerous number protocols or platforms that already exist. Nowadays, the only viable solution is so to provide new security mechanisms that could be composed on demand and deployed in any IoT deployment by the operators, the integrators or the vendors rather than developing protocolor architecture-centric security solutions. [1] Manos Antonakakis et. al , Understanding the Mirai Botnet, USENIX Security, 2017 [2] L. Rouch et. Al, A Universal Controller to Take Over a Z-Wave Network, Black Hat Europe, 2017 [3] Alessandro Bassi, Martin Bauer, Martin Fiedler, Thorsten Kramp, Rob van Kranenburg, Sebastian Lange, Stefan Meissner (eds), “Enabling Things to Talk”, Designing IoT solutions with the IoT Architectural Reference Model, Springer, 2013 [4] J. François et. al, PTF: Passive Temporal Fingerprinting, IFIP/IEEE International Symposium on Integrated Network Management (IM), 2011 [5] BF Van Dongen et. al, The prom framework: A new era in process mining tool support, ICATPN 2005 [6] C. Kolias, G. Kambourakis, A. Stavrou and J. Voas, ""DDoS in the IoT: Mirai and Other Botnets,"" in Computer, vol. 50, no. 7, pp. 80-84, 2017. [7] Markus Miettinen, Samuel Marchal, Ibbad Hafeez, N. Asokan, Ahmad-Reza Sadeghi, Sasu Tarkoma: IoT SENTINEL: Automated Device-Type Identification for Security Enforcement in IoT. ICDCS 2017: [8] A. Al-Fuqaha, M. Guizani, M. Mohammadi, M. Aledhari and M. Ayyash, ""Internet of Things: A Survey on Enabling Technologies, Protocols, and Applications,"" in IEEE Communications Surveys & Tutorials , vol. 17, no. 4, pp. 2347-2376, Fourthquarter 2015.",,2019.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
7e8eb541a9b6b68ecc8ab99162aeae118a07f197,https://www.semanticscholar.org/paper/7e8eb541a9b6b68ecc8ab99162aeae118a07f197,Step Change Transformation of Legacy Rigs to Autonomous Drilling Rigs,"
 During the past decade, drilling automation systems have been an attractive target for a lot of operating and drilling companies. Despite progress in automation in various industries, like mining and downstream, the drilling industry has lagged far behind in the real application of autonomous technologies implementation. This can be attributed to harsh environment, high level of uncertainty in input data, and that majority of stock is legacy drilling rigs, resulting in capital intensive implementations. In the past years there have been several attempts to create fully automated rigs, that includes surface automation and drilling automation. Such solutions are very attractive, because they allow people to move out of hazardous zones and, at the same time, improve performance. However, the main deficiency of such an approach is the very high capital investment required for development of highly bespoke rigs (Slagmulder 2016). And in the current business environment, with high volatility in oil and gas prices, plus the huge negative effect of the Covid-19 crisis on the world's economic situation, it would be hard to imagine that there are a lot of companies willing to make such a risky investment. In addition to this, due to the lack of demand, the market is full of relatively new, high-performance rigs.
 Taking all these into account, the obvious question is whether it makes sense to invest money and time into the development of drilling automation. The answer should be yes, for three substantial reasons:Automation improves personal safety, by moving people out of danger zones;Automation improves process safety, by transferring execution from person to machine, which reduces the risk of human error;Automation improves efficiency by bringing consistency to drilling and through the use of self-learning algorithms, which allow machines to drill each successive well better than the previous.
 This paper will not look into surface automation, such as pipe-handling, chemical and mud handling on site. The paper is focused on the subsurface, namely on the drilling automation process, the challenges that need to be overcome to deploy a vendor agnostic system on a majority of existing rigs.
 A vendor agnostic system is a modification of an operator's autonomous drilling system (Rassenfoss 2011), designed to use existing rigs, BHAs, and have minimum footprint on the rigs for operational use. A vendor agnostic system will increase adoption of automated technologies and further drive improvements in operational and business performance","Day 1 Mon, November 15, 2021",2021.0,10.2118/207551-ms,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
0b0bb8652877edd9d363213bcc98e8e3605ea669,https://www.semanticscholar.org/paper/0b0bb8652877edd9d363213bcc98e8e3605ea669,Special issue XSEDE16 & PEARC17 – Practice and experience in advanced research computing,"The conference on Practice and Experience in Advanced Research Computing (PEARC) provides a forum for discussing challenges, opportunities, and solutions among the broad range of participants in the research computing community. This community-driven effort builds on successes of the past and aims to grow and be more inclusive by involving additional local, regional, national, and international cyberinfrastructure and research computing partners spanning academia, government, and industry. The PEARC conference series is working to integrate and meet the collective interests of our growing community. PEARC originated from the XSEDE conference series, which showcased the discoveries, innovations, challenges, and achievements of those who use and support NSF Extreme Science and Engineering Discovery Environment (XSEDE) resources and services, as well as other digital resources and services throughout the world. The following papers represent the best papers from the transitionary conferences of this research community: XSEDE16, the final XSEDE conference, and PEARC17, the inaugural PEARC conference. These papers capture both important research contributions to the advanced computing community and best practices for advanced computing systems and remote user interfaces. These three papers cover a diverse set of topics: the impact of science gateways, innovative hardware impacting cyberinfrastructure, and improving computational frameworks with workflows, machine learning, and visualizations. The paper by Hu et al1 discusses how geospatial data have exploded to massive volume and diversity and subsequently cause serious usability issues for researchers in various scientific areas. This paper describes TopoLens, a cyberGIS community data service framework to facilitate geospatial big data access, processing, and sharing based on a hybrid supercomputer architecture. TopoLens delivers community data services developed for easy and efficient access to high-resolution topographic data. It supports on-demand data and map services, powered by hybrid cyberinfrastructure with cloud and HPC support, to efficiently produce datasets that are customized based on a user's request. The usability of TopoLens has been acknowledged in the topographic user community evaluation. The paper by Hancock et al2 dives into Jetstream, the NSF's first distributed production cloud resource. Jetstream offers a unique capability within the XSEDE-supported US national cyberinfrastructure, delivering interactive virtual machines (VMs) via the atmosphere interface. As a multi-region deployment that operates as an integrated system, Jetstream is proving effective in supporting modes and disciplines of research traditionally underrepresented on larger XSEDE-supported clusters and supercomputers. Jetstream has been used to perform research and education in biology, biochemistry, atmospheric science, earth science, and computer science. Lastly, the paper written by Li and Song3 discusses the challenges of large-scale simulations generating huge amounts of data with potentially critical information that is saved in intermediate files and is not instantly visible until advanced data analytics techniques are applied after reading all simulation. In this paper, the authors build a new computational framework to couple scientific simulations with multi-step machine learning processes and in situ data visualizations. This computational framework is built upon different software components and provides plug-in data analysis and visualization functions over complex scientific workflows. With this advanced framework, users can monitor and get real-time notifications of special patterns or anomalies from ongoing extreme-scale turbulent flow simulations. These three papers demonstrate the wide range of topics addressed by the PEARC community, which spans academic researchers, resource providers, industry partners, and other affiliates. The work done by this community and the insights shared at the annual PEARC conference are intended for broad application for those active in advanced computing research and practice. If you are already part of the PEARC community, we look forward to seeing you soon; if you have not yet attended a PEARC conference, we hope these papers will provide motivation and justification to invest your time and travel to join us. The editors would like to acknowledge the hard work of the program committees, both for XSEDE16 and for PEARC17, whose combined efforts resulted in this special issue. Sincerely, Paul Navrátil Maytal Dahan Technical Program Chair Technical Program Chair XSEDE16 PEARC17",Concurr. Comput. Pract. Exp.,2019.0,10.1002/CPE.5325,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
83d1f9167791ad6395e038be598550c8da63a2f1,https://www.semanticscholar.org/paper/83d1f9167791ad6395e038be598550c8da63a2f1,"Introduction to Business Intelligence, Analytics and Cognitive: Case Studies and Applications (COGS) Minitrack","The purpose of this minitrack is to introduce case studies of applications of business intelligence, data analytics and cognitive-enabled smart services across industries and societies. Business intelligence and data analytics have continued to make substantial inroads in the operational, managerial and strategic corporate decision-making processes. Recently, the emergence of cognitive computing systems that augment the creativity and productivity of people, and which are trained using artificial intelligence and machine learning algorithms to predict, infer and, up to some extent augment cognitive capabilities, has also extended the range of business intelligence and data analytics solutions on the market. We will consider results of recent research with focus on the design, analysis, implementation, adoption, and evaluation of real-life cases that provide us with opportunities to design, develop, and deploy these capabilities as micro-services that solve customer needs, including those with startup potential. Opening presentation “Meeting Analytics: Creative Activity Support Based on Knowledge Discovery from Discussions” investigates a mechanism to promote innovation by supporting discussions. Discussion mining is used to collect various data on meetings (statements and their relationships, presentation materials such as slides, audio and video, and participants’ evaluations on statements). These data are then used to extract important statements to be considered especially after the meetings has been held and actions have been undertaken. Author presents high-probability statements that should lead to innovations during meetings and facilitate creative discussions. He also proposes a creative activity support system that should help users to discover and execute essential tasks. In the paper “Customization of IBM Intu’s Voice by Connecting Text-to-Speech Service and Voice Conversion Network” authors investigate applications of popular IBM Intu service, which interacts with users by voice and text. They propose a voice customization service by which users can directly customize the voice of Intu. The method for voice customization is based on IBM Watson’s textto-speech service and voice conversion model. Users can train the voice conversion model by offering only 100 or more speech samples uttered in the preferred voice (target). Then the output speech of Intu (source) is converted into the speech uttered in the target voice. The last paper “The Challenges of Business Analytics: Successes and Failures” is a case study devoted to implementation of business data analytics programs in organizations; both successfully and unsuccessfully. It discusses in details the implications for selected organization. Authors analyze benefits and shortcoming of BI implementations across industries using several well-known industry cases. Based on the lesson learned, they offer ideas on how to implement a successful business analytics program. We hope you will learn new ideas in this exciting field of study and enjoy all presentations at the minitrack We thank the authors for submitting excellent results of their work to make this minitrack successful. Proceedings of the 51 Hawaii International Conference on System Sciences | 2018",HICSS,2017.0,10.24251/HICSS.2017.125,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
d31a7209656b2e5529ddae3d629fcc5dbbcc57b1,https://www.semanticscholar.org/paper/d31a7209656b2e5529ddae3d629fcc5dbbcc57b1,Strategies for Adopting Additive Manufacturing Technology Into Business Models,"Strategies for Adopting Additive Manufacturing Technology Into Business Models by Robert Martens MS, University of Glamorgan, 2007 MBA, Keele University, 2006 Doctoral Study Submitted in Partial Fulfillment of the Requirements for the Degree of Doctor of Business Administration Walden University August 2018 Abstract Additive manufacturing (AM), also called 3-dimensional printing (3DP), emerged as a disruptive technology affecting multiple organizations’ business models and supply chains and endangering incumbents’ financial health, or even rendering them obsolete. The world market for products created by AM has increased more than 25% year over year. Using Christensen’s theory of disruptive innovation as a conceptual framework, theAdditive manufacturing (AM), also called 3-dimensional printing (3DP), emerged as a disruptive technology affecting multiple organizations’ business models and supply chains and endangering incumbents’ financial health, or even rendering them obsolete. The world market for products created by AM has increased more than 25% year over year. Using Christensen’s theory of disruptive innovation as a conceptual framework, the purpose of this multiple case study was to explore the successful strategies that 4 individual managers, 1 at each of 4 different light and high-tech manufacturing companies in the Netherlands, used to adopt AM technology into their business models. Participant firms originated from 3 provinces and included a value-added logistics service provider and 3 machine shops serving various industries, including the automotive and medical sectors. Data were collected through semistructured interviews, member checking, and analysis of company documents that provided information about the adoption of 3DP into business models. Using Yin’s 5-step data analysis approach, data were compiled, disassembled, reassembled, interpreted, and concluded until 3 major themes emerged: identify business opportunities for AM technology, experiment with AM technology, and embed AM technology. Because of the design freedom the use of AM enables, in combination with its environmental efficiency, the implications for positive social change include possibilities for increasing local employment, improving the environment, and enhancing healthcare for the prosperity of local and global citizens by providing potential solutions that managers could use to deploy AM technology. Strategies for Adopting Additive Manufacturing Technology into Business Models by Robert Martens MBA, University of Keele, 2006 MS, University of Glamorgan, 2007 Doctoral Study Submitted in Partial Fulfillment of the Requirements for the Degree of Doctor of Business Administration Walden University August 2018 Dedication I dedicate this work to my family, who believed in me during this quest. In particular, I wish to thank my father, Dominicus Martens, for telling me about the many journeys he made across the world, for giving me the appreciation for mechanical engineering and procurement, and for showing me technology and business go well together. To my mother, Cornelia, for gifting me with stamina, and an inquisitive and critical mind. I want to thank my wife, Lu Dongmei, for her constant encouragement during this research and belief in my abilities to achieve this goal. To my children Niek, Louis, Max, and Franc: thanks for your support along this journey; I hope I have shown you the importance of goal setting and dedication. Never stop learning. Acknowledgments I would like to acknowledge my family, friends, colleagues, classmates, and Walden faculty for their support during this doctoral study project. To my coach while in China, Lynda Aurora, who pushed me to pursue this dream. In particular, I wish to thank my chair, Dr. Susan Fan, for her guidance, Dr. Charles Needham, for his professionalism, and Dr. Lisa Kangas, for her eye to detail, towards the completion of quality research. To my Walden classmates with whom I shared many ups and downs: thank you for your time, support, motivation, and inspiration.",,2018.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
e9db360184bd05e81e22e4863c552047ffca4754,https://www.semanticscholar.org/paper/e9db360184bd05e81e22e4863c552047ffca4754,Multi Uav Cooperative Surveillance With Spatio Temporal Ebooks File,"Covering the design, development, operation and mission profiles of unmanned aircraft systems, this single, comprehensive volume forms a complete, stand-alone reference on the topic. The volume integrates with the online Wiley Encyclopedia of Aerospace Engineering, providing many new and updated articles for existing subscribers to that work. Time-Critical Cooperative Control of Autonomous Air Vehicles presents, in an easy-to-read style, the latest research conducted in the industry, while also introducing a set of novel ideas that illuminate a new approach to problem-solving. The book is virtually self-contained, giving the reader a complete, integrated presentation of the different concepts, mathematical tools, and control solutions needed to tackle and solve a number of problems concerning time-critical cooperative control of UAVs. By including case studies of fixed-wing and multirotor UAVs, the book effectively broadens the scope of application of the methodologies developed. This theoretical presentation is complemented with the results of flight tests with real UAVs, and is an ideal reference for researchers and practitioners from academia, research labs, commercial companies, government workers, and those in the international aerospace industry. Addresses important topics related to time-critical cooperative control of UAVs Describes solutions to the problems rooted in solid dynamical systems theory Applies the solutions developed to fixed-wing and multirotor UAVs Includes the results of field tests with both classes of UAVs Across the globe, the past several years have seen a tremendous increase in the role of cooperative autonomous systems. The field of cooperative control and optimization has established itself as a part of many different scientific disciplines. The contents of this hugely important volume, which adds much to the debate on the subject, are culled from papers presented at the Seventh Annual International Conference on Cooperative Control and Optimization, held in Gainesville, Florida, in January 2007. First used in military applications, unmanned aerial vehicles are becoming an integral aspect of modern society and are expanding into the commercial, scientific, recreational, agricultural, and surveillance sectors. With the increasing use of these drones by government officials, business professionals, and civilians, more research is needed to understand their complexity both in design and function. Unmanned Aerial Vehicles: Breakthroughs in Research and Practice is a critical source of academic knowledge on the design, construction, and maintenance of drones, as well as their applications across all aspects of society. Highlighting a range of pertinent topics such as intelligent systems, artificial intelligence, and situation awareness, this publication is an ideal reference source for military consultants, military personnel, business professionals, operation managers, surveillance companies, agriculturalists, policymakers, government officials, law enforcement, IT professionals, academicians, researchers, and graduate-level students. Discover what lies beyond the bleeding-edge of autonomous airborne networks with this authoritative new resource Autonomous Airborne Wireless Networks delivers an insightful exploration of recent advances in the theory and practice of using airborne wireless networks to provide emergency communications, coverage and capacity expansion, information dissemination, and more. The distinguished engineers and editors have selected resources that cover the fundamentals of airborne networks, including channel models, recent regulation developments, self-organized networking, AI-enabled flying networks, and notable applications in a variety of industries. The book evaluates advances in the cutting-edge of unmanned aerial vehicle wireless network technology while offering readers new ideas on how airborne wireless networks can support various applications expected of future networks. The rapidly developing field is examined from a fresh perspective, one not just concerned with ideas of control, trajectory optimization, and navigation. Autonomous Airborne Wireless Networks considers several potential use cases for the technology and demonstrates how it can be integrated with concepts from selforganized network technology and artificial intelligence to deliver results in those cases. Readers will also enjoy: A thorough discussion of distributed drone base station positioning for emergency cellular networks using reinforcement learning (AI-enabled trajectory optimization) An exploration of unmanned aerial vehicle-to-wearables (UAV2W) indoor radio propagation channel measurements and modelling An up-to-date treatment of energy minimization in UAV trajectory design for delay tolerant emergency communication Examinations of cache-enabled UAVs, 3D MIMO for airborne networks, and airborne networks for Internet of Things communications Perfect for telecom engineers and industry professionals working on identifying practical and efficient concepts tailored to overcome challenges facing unmanned aerial vehicles providing wireless communications, Autonomous Airborne Wireless Networks also has a place on the bookshelves of stakeholders, regulators, and research agencies working on the latest developments in UAV communications. ""The ability to fly multiple unmanned aerial vehicles (UAVs) in collaboration has the potential to expand the scope of feasible UAV missions and could become the backbone of future UAV missions. However, despite having garnered significant research interest, there is no indication that systems supporting collaborative operation of multiple UAVs are close to achieving field deployment. The challenge of successfully deploying a quality system is inherently complex, and systems engineering offers an approach to handle the complexities. Effective application of systems engineering requires both knowledge breadth and depth. This thesis presents the results of a consolidation of information intended to support the conduct of systems engineering activities; and describes an experiment to ascertain the sensitivities of some key operational parameters, e.g., acquisition, pointing, and tracking. The experiment was conducted using Automatic Dependent Surveillance Broadcast (ADS-B) and visual tracking equipment employing state-of-the-art technology to understand the operating challenges and requirements of using this equipment to provide situational awareness for a UAV pilot""--Abstract. This book compiles some of the latest research in cooperation between robots and sensor networks. Structured in twelve chapters, this book addresses fundamental, theoretical, implementation and experimentation issues. The chapters are organized into four parts namely multi-robots systems, data fusion and localization, security and dependability, and mobility. This two volume set constitutes the refereed post-conference proceedings of the Second International Conference on Machine Learning and Intelligent Communications, MLICOM 2017, held in Weihai, China, in August 2017. The 143 revised full papers were carefully selected from 225 submissions. The papers are organized thematically in machine learning, intelligent positioning and navigation, intelligent multimedia processing and security, intelligent wireless mobile network and security, cognitive radio and intelligent networking, intelligent internet of things, intelligent satellite communications and networking, intelligent remote sensing, visual computing and three-dimensional modeling, green communication and intelligent networking, intelligent ad-hoc and sensor networks, intelligent resource allocation in wireless and cloud networks, intelligent signal processing in wireless and optical communications, intelligent radar signal processing, intelligent cooperative communications and networking. A comprehensive review of the state of the art in the control of multi-agent systems theory and applications The superiority of multi-agent systems over single agents for the control of unmanned air, water and ground vehicles has been clearly demonstrated in a wide range of application areas. Their large-scale spatial distribution, robustness, high scalability and low cost enable multi-agent systems to achieve tasks that could not successfully be performed by even the most sophisticated single agent systems. Cooperative Control of Multi-Agent Systems: Theory and Applications provides a wide-ranging review of the latest developments in the cooperative control of multi-agent systems theory and applications. The applications described are mainly in the areas of unmanned aerial vehicles (UAVs) and unmanned ground vehicles (UGVs). Throughout, the authors link basic",,2021.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
8a3ab92b1c64c932ac7d460e34222d6738539f19,https://www.semanticscholar.org/paper/8a3ab92b1c64c932ac7d460e34222d6738539f19,Program at a Glance,": In October, 2018, Verizon delivered the world's first 5G commercial use case in the form of fixed home broadband services in the U.S. This 5G FWA (Fixed Wireless Access) service dramatically reduced time and cost required to install optical cables, thus enabling wireless broadband speeds up to 1Gbps in many households. 2019 is another year of firsts for 5G, with the commercialization of NR-based 5G mobile service. In April of 2019, Korea's three mobile carriers, SK Telecom, KT, and LG U+, successfully launched 5G network services in Korea. That same month, Verizon also launched its 5G services in Minneapolis and Chicago. Other operators in the US are planning - or in some cases have already started - 5G services, while China and Japan are also set to launch their 5G commercial services soon. The successful execution of 5G commercialization demands prerequisites such as chips, devices, radio equipments (RAN, Core, S/W tools), S/W virtualization, AI, analytics, and containerization. Based on these, new 5G services will be excavated, from which users will reap the full extent of 5G's benefits. Thus far, Samsung has secured 5G network equipments (RAN, Core, S/W Tool) as well as obtained automation solutions Abstract: Concrete examples of what to expect when 5G comes to life will be presented. We will be looking at how 5G can support new consumer experiences, enhance healthcare, and transform industries like manufacturing and transport as well as how we work as an ecosystem to realize our 5G vision. Abstract: The Fourth Industrial Revolution is fundamentally changing all areas, and the entire industrial structure is expected to be completely changed in the not-to-distant future. AI, Big Data and High-Performance Computing will provide intelligent information services that mimic human capabilities. By implementing AI infrastructure for self-aware, judgment, evolution and collaboration, we will build a human-centered super-intelligent information society such as hyper-smart networking and ultra-realistic experiences. In this keynote presentation, Dr. Myung-Jun Kim provides the various AI researches carried out in ETRI and the future of AI expected by ETRI. Abstract: We are witnessing an exciting time for future wireless networks with the emergence of 5G. In contrast to 3G and 4G, which were mainly a continuation of their predecessors, 5G will represent a revolutionary leap and will have a huge impact on the transformation of wireless communications industries as well as vertical industries. In this talk, we will describe some of the important technologies and innovations ranging from air technologies and network design to services that are needed to meet the demands of the next-generation wireless networks and guarantee broadband ubiquitous communications of all things, including human-to-machine and machine-to-machine, for a connected living. Of particular interest is the use of machine learning, a powerful artificial intelligence approach, for supporting intelligent networks and optimizing resource allocation problems in wireless networks. Abstract: Disruption is happening in Manufacturing. Industrie 4.0 and IIOT programs promise to deliver better business outcome but resulted in early success pilots and projects but failed to scale to full deployment and many wrapped up within a year. Your IIOTl projects needs a Digital ready foundation. Cisco Secure, Intelligent and connected platform is the Digital Ready Foundation for Industrie 4.0 and your IIOT program. It addresses things/machine, Edge/Fog, Data, Security and Automation and management. It also create that ready foundation to customer to build future Digital capabilities and harvest real impact business outcome. lead for Manufacturing Industries, Digital Transformation Office at Cisco. He leads Cisco’s boardroom relevance in the Manufacturing industries to help our customers accelerate their digital transformation journeys. We do this by aligning Cisco’s unified technology platform & digital capabilities to our customer's business imperatives. Abstract: This research talk describes our recent work on “AI Connected Healthcare Platform” for accelerating innovations in Healthcare eco-system. This platform provides the capability to develop, deploy AI based Healthcare analytics and engage to provide real-time insights to patients. It allows healthcare providers to monitor, alert and engage patients through healthcare sensors. At the core, the platform consists of (i) a highly scalable monitor and provide alerts and insights to patients. Several use cases for the eco-system enablement are described to illustrate other important aspects of our platform. Abstract: Silicon Valley is the major breeding ground of a variety of technologies essential to 4th Industrial Revolution (4IR). One of the distinguished platforms on which 4IR technologies converge is autonomous vehicle (AV). Autonomous or Automated Driving (AD) is characterized by the convergence of key ICT technologies including AI, 5G, IoT, and Compute. Also, AD as part of mobility ecosystem is playing a key role in Mobility as a Service (MaaS) and the corresponding monetization business. All of the major players in Mobility as well as venture and legal entities have been establishing their presence in Silicon Valley. In this talk, the history and current trend of AV technology (sensors, computing, software stack, mapping, connectivity, etc.) in Silicon Valley are presented, along with the survey of the relevant SV companies, both the established and the startups. Also, current problems and obstacles to rapid advancement of AV to higher levels are discussed and suggested as future R&D directions. Abstract: With 3GPP R15 Frozen on June 15, 2018, 5G has achieved an important milestone. Major countries have announced 5G Republic of Korea has meaningfully launched of World’s First Commercial 5G network. In this presentation, I will introduce key technologies and services, which SK Telecom has been working on. Aligning to that, the 5G services such as immersive media (Virtual Reality)/AR (Augmented Reality), AI based media, smart factory and smart office will be addressed. In key member board member on Next Generation Media co., which is joint venture(JV) company between SK Telecom and Sinclair, the number one broadcasting company in US.He various ICT R&D projects SKT and deployed new technologies in the market, low latency live streaming, personalized recommendation, HEVC, and immersive AR/VR Optics, etc. Abstract: To support the expanded connectivity needs for the next decade, 5G is taking on a much bigger role than previous generations. Our vision for 5G is a unifying connectivity fabric and a platform for future innovations that will expand the value of mobile networks to connect new industries/devices, utilize new spectrum bands/types, and enable new deployments. mmWave is a set of higher spectrum bands that have so far been limited to fixed point-to-point link such as for wireless backhaul and home broadband delivery, but a new frontier has come to mobilize mmWave for the smartphone and make it available for the masses thanks to cutting edge R&D and extensive research/tests. 5G will serve as the foundational technology that makes it possible for everything and everyone to communicate and interact seamlessly, across connected cars and industrial IoT. This presentation will show how mmWave is enabling next gen mobile experiences, how we are solving the challenges, and also what new industry and services 5G will enable or improve. His roles include developing sound relationship with national and regional government officials and enhance Qualcomm’s corporate reputation. He advocates agreed corporate positions in coordination with internal/external stakeholders, introducing Qualcomm’s technology policies in the areas of mobile, automotive, IoT, and Smart X, and also spectrum policies. He has overall 20+ years of experience in the public policy and government relations areas. Business Applications: Research and Industry Proliferation of chatbots and their advances have been astounding. Almost every messaging platform has APIs to support chatbots. Businesses also have tremendous interests in chatbots for cost saving and better user experiences. However, despite the excitement and the prospect of chatbots, often in reality, their performance falls short of our expectations. This talk will (i) revisit the recent history of AI-enabled chatbots, (ii) present the current state of chatbot applications in the industry, and (iii) discuss the future direction of chatbots in the near term. State of the art technologies to improve chatbot performance and enable future advancement will be also briefly discussed. Abstract: Due to the ever increasing interest in mobile applications and services, the concept of mobile edge computing (MEC) has emerged. Due to the proximity between mobile user (MU) and MEC server located at base station (BS), i.e., the edge of radio access network (RAN), MEC can realize low-latency mobile application. For MEC, MU and MEC server need to exchange tasks using limited radio resources. Furthermore, when multiple MUs possess tasks, MEC server has to handle multiple tasks. Thus, the radio and computing resources need to be allocated to MUs by taking into account the wireless channel condition and the computing power of MUs and MEC server. In this research, a radio resource and computing resources allocation scheme is proposed to minimize the total processing completion time of all the tasks. Each task is assumed to be divided into local task and offload task. Local task is computed by each MU whilst offload task is computed by a MEC server. We first formulate the optimization problem to minimize the total processing completion time of all tasks. For the formulated optimization problem, we propose a two-step radio and computing resources allocation scheme which iteratively performs bisection search method a","2021 19th International Symposium on Modeling and Optimization in Mobile, Ad hoc, and Wireless Networks (WiOpt)",2013.0,10.1111/ajco.12153,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
2524e8ea6f100cfe50c794c9d599fecce443ff82,https://www.semanticscholar.org/paper/2524e8ea6f100cfe50c794c9d599fecce443ff82,P 01 – Deep learning based object detection and classification,"With the rapid advancement of deep learning and high performance computing technologies, data scientists can now use GPUs, GPU clusters or purpose-built hardware systems for deep learning to develop and deploy machine learning based applications such as massive image classification and video analytics. The aim of this project is to develop a deep learning based image classifier using the latest high performance platform for visual computing. The work will include assessing the existing deep learning software packages, deploy a selected package using the platform, and implement the image classification application using the deep learning package. The practical outcome from this work will be increased capabilities for automated fishery monitoring to support sustainable fisheries. The project has close links to computer science, mathematics and engineering, and can help bridge the gap between your courses and developing a real scientific application. The suitable candidate will have opportunities to have access to world class facilities and work alongside CSIRO senior scientists while you are enjoying generous personal development and learning opportunities. Skills required: • Programming language C or C++ preferred. • Image processing/image analysis or computer vision course or experience is preferred, but not compulsory. • Quick learner. • Good communication skills. Developmental outcomes for student: • Software that can be used by others. • Exposure to high-impact research, scientific expertise in multiple disciplines, and scientific infrastructure; • Improved image processing and computer vision skills; • Improved skills in scientific communication (e.g. writing scientific reports and oral presentations). Projects – Trustworthy Systems • Formal methods o Theorem proving o Protocols o Expressiveness o Blockchain • Systems o Kernels Middleware o Security • Programming Languages Formal Methods / Theorem Proving Project P02 Automating Formal Proofs Gerwin Klein, Daniel Matichuck Abstract: Isabelle is an interactive theorem prover which combines automated and human reasoning through the use of proof methods (or tactics). These methods allow users to make highlevel decisions on how to progress in a proof without worrying about the formal details. Powerful proof methods are required for large proof undertakings, such as the L4.verified project at Data61. Eisbach is an in-development prototype of a high-level language for writing proof methods in Isabelle, which is historically done in Standard ML. It is influenced by Coq’s Ltac, but distinguishes itself by leveraging Isabelle’s existing automation and backtracking infrastructure. The aim of this project is to investigate useful applications for Eisbach, with several possible domains to explore: An existing suite of Data61-developed separation logic tactics could be reimplemented and extended, or a similar investigation can be done against the verificationcondition generator used in the L4.verified proof; Or a novel proof method can be developed solve, for example, word arithmetic proofs that appear frequently in the verification of C programs. Novelty: This will be one of the first larger-scale uses of the new Eisbach proof language and the resulting feedback will contribute to its further development. Outcome: Automated proof tactics for previously manual proofs in one or more of the specified application domains, ideally reducing proof size and time for those applications. Reference Material Links: Trustworthy Systems Research Group (TS): http://trustworthy.systems seL4: http://ertos.nicta.com.au/research/sel4/ Isabelle: http://mirror.cse.unsw.edu.au/pub/isabelle/ P03 Formal Verification of multi-threaded embedded application software June Andronick, Corey Lewis Abstract: eChronos [0] is a small embedded OS for micro-controllers. It is commercially used in medical devices and is embedded in high-assurance autonomous flying vehicles (quadcopters) demonstrated in HACMS, a large DARPA-funded project, in collaboration with industry and university partners from the US. In Data61's eChronos verification project, we aim at proving strong guarantees about eChronos correct behavior, by means of formal (mathematical) proofs, machine-checked in the Isabelle/HOL theorem prover. The long-term goal is to provide developpers of embedded applications using eChronos with a formal and verified specification of the OS API functions used for synchronisation (semaphores, mutexes, etc). The challenge lies in providing the right abstraction level for the formal API and provide a usable framework for application code verification. This project will investigate eChronos-based application verification, via small case-studies, which could be derived or inspired from application code from the HACMS project, namely the SMACCMPilot open-source autopilot software from Galois [3]. [0] /projects/TS/echronos/ [1] ""Controlled owicki-gries concurrency: reasoning about the preemptible eChronos embedded operating system"". MARS'15. /publications/nictaabstracts/Andronick_LM_15.abstract.pml [2] ""Proof of OS scheduling behavior in the presence of interrupt-induced concurrency"". To appear in ITP'16. [3]http://smaccmpilot.org/ Novelty: Formal verification of real-world embedded application on a verified embedded OS API. Outcome:The expected outcome of the project is the experimental verification of a case-study application running on eChronos, exhibiting the required formalised API from eChronos. P04 Improving automation in concurrent software verification June Andronick, Corey Lewis Abstract: eChronos [0] is a small embedded OS for micro-controllers. It is commercially used in medical devices and is embedded in high-assurance autonomous flying vehicles (quadcopters) demonstrated in HACMS, a large DARPA-funded project, in collaboration with industry and university partners from the US. In Data61's eChronos verification project, we aim at proving strong guarantees about eChronos correct behavior, by means of formal (mathematical) proofs, machine-checked in the Isabelle/HOL theorem prover. The challenge lies in the concurrency due to eChronos running with interrupts enabled, including during scheduling operations, to ensure low latency. We have successfully proved, at a model level, the correctness of eChronos scheduling behavior in presence of interrupt-induced concurrency [1,2]. This project would look at increasing the automation and scalability of the framework. Opportunities for such improvements include (but are not limited to) increased reuse of already proven facts, investigation of more modular (but less fine-grain) approaches, more use of Isabelle automation, increased compositionality of the proof process, etc. [0] /projects/TS/echronos/ [1] ""Controlled owicki-gries concurrency: reasoning about the preemptible eChronos embedded operating system"". MARS'15. /publications/nictaabstracts/Andronick_LM_15.abstract.pml [2] ""Proof of OS scheduling behavior in the presence of interrupt-induced concurrency"". To appear in ITP'16. Novelty: Your work will contribute to the general feasibility and scalability of practical concurrent software verification. Outcome: Your work will directly impact the efficiency of the framework and proofs developed for the verification of eChronos. P05 Implement and Verify a CakeML Compiler Optimisation",,2016.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
6273eecc2aa48d93804ee26ed745b91b59885e42,https://www.semanticscholar.org/paper/6273eecc2aa48d93804ee26ed745b91b59885e42,Bayesian Filtering Methods For Dynamic System Monitoring and Control,"of a dissertation at the University of Miami. Dissertation supervised by Professor Ramin Moghaddas. No. of pages in text. (175) Real-time system monitoring and control represent two of the most important issues that characterize modern industries in critical areas of civilian and military interest, including power grid, energy, healthcare, aerospace, and infrastructure. During the past decade, there has been a rapid development of robust dynamic system monitoring and control methods for fault diagnosis and failure prognosis. Among various monitoring and control policies, condition-based maintenance (CBM) has been studied by many researchers due to its ability to enable a large amount of monitoring data for real-time diagnostics and prognostics. A considerable amount of literature has been published on the subject, providing a large volume of dynamic system control methods. Previously published studies are limited by assumptions that can generally be distinguished into three main categories: i) predefined system failure thresholds, ii) simplified latent dynamics, and iii) unrealistic parametric forms that describe the evolution of system dynamics through time. This thesis provides an array of solution approaches that overcomes the aforementioned assumptions in a smart and effective way by introducing novel quantitative frameworks for real-time monitoring, control, and decision-making for dynamic systems. The proposed frameworks are categorized into two main phases of a comprehensive framework. The first phase contains two original Bayesian filtering methods for condition monitoring and control of systems with either linear or non-linear degradation dynamics. The former is designed only for systems with linear latent and observable dynamics and utilizes Kalman filtering for state-parameter inference. It considers a failure process that is purely stochastic and is based on logistic regression. This process is directly affected by the latent system dynamics, therefore avoiding the need for a priori failure thresholds. The latter takes into consideration multiple levels of system dynamics that evolve either linearly or non-linearly. A hybrid particle filter is developed for state-parameter inference, while an Extreme Learning Machine artificial neural network is utilized to relate sensor observations to latent system dynamics. Both frameworks are tested and validated on synthetic and real-world time-series datasets. The second phase of this thesis introduces an original method for optimal control and decision-making that employs Bayesian filtering-based deep reinforcement learning with fully stochastic environments. Sets of deep reinforcement learning agents were trained to develop control policies. Bayesian filtering methods from the first phase were utilized to provide environment states that use the estimates from latent system dynamics. This method is used in two different applications for maintenance cost minimization and estimating remaining useful life of a system under condition monitoring. Results obtained from applying the framework on simulated and realworld time-series data suggest that the proposed Bayesian filtering-based deep reinforcement learning algorithm can be trained even with limited data, which can be useful for real-time control and decision making for many dynamic systems. Dedicated to my fantastic parents, Gordios and Vasiliki, for providing me with every opportunity and encouragement in life",,2019.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
395b153b5f3d4445e4e2920f5cb7e6531a970a62,https://www.semanticscholar.org/paper/395b153b5f3d4445e4e2920f5cb7e6531a970a62,Read PDF Home Automation Using Internet Of Things modernh.com,"hat bereits begonnen. Ihr Merkmal ist die ungeheuer schnelle und systematische Verschmelzung von Technologien, die die Grenzen zwischen der physischen, der digitalen und der biologischen Welt immer stärker durchbrechen. Wie kein anderer ist Klaus Schwab, der Vorsitzende des Weltwirtschaftsforums, in der Lage aufzuzeigen, welche politischen, wirtschaftlichen, sozialen und kulturellen Herausforderungen diese Revolution für uns alle mit sich bringt.This work presents the design and model implementation of a novel home automation system applying the Internet of Things (IoT) technology. It seeks simplified design protocols for developing a robust home automation system to deal with the problems of complexity, multiple incompatible standards and the resulting expenses in the existing systems. The embedded system featuresthe ubiquitous low-cost 32-bit ESP8266 System-on-chip (SoC) module interfaced to some sensors and actuators for interaction inthe home. Flexibility in the remote access, operation and management is achieved through HTML5 based intuitive mobile and web GUI applications. Web Application Messaging Protocol (WAMP) is deployed to ensure that individual applications and systems seamlessly communicate with a relatively high level of security using robust web service security protocol. This system offers a cost-effective and efficient solution, excluded, which are present mostly in other solutions, because the costs of a dedicated public IP address and a high-end computer are excluded, which are present mostly in other solutions.This book focuses on the emerging advances in distributed communication systems, big data, intelligent computing and Internet of Things, presenting state-of-the-art research in frameworks, algorithms, methodologies, techniques and applications associated with data engineering and wireless distributed communication technologies. In addition, it discusses potential topics like performance analysis, wireless communication networks, data security and privacy, human computer interaction, 5G Networks, and smart automated systems, which will provide insights for the evolving data communication technologies. In a nutshell, this proceedings book compiles novel and high-quality research that offers innovative solutions for communications in IoT networks.This edited book presents point of view and the work being undertaken by active researchers in the domain of IOT and its applications with societal impact. The book is useful to other researchers for the understanding of the research domain and different points of views expressed by the experts in their contributed chapters. The contributions are from both industry and academia; hence, it provides a rich source of both theoretical and practical work going on in the research domain of IOT.Advances in Computing, Communication, Automation and Biomedical Technology aims to bring together leading academic, scientists, researchers, industry representatives, postdoctoral fellows and research scholars around the world to share their knowledge and research expertise, to advances in the areas of Computing, Communication, Electrical, Civil, Mechanical and Biomedical Systems as well as to create a prospective collaboration and networking on various areas. It also provides a premier interdisciplinary platform for researchers, practitioners, and educators to present and discuss the most recent innovations, trends, and concerns as well as practical challenges encountered, and solutions adopted in the fields of innovation.This books objective is to explore the concepts and applications related to Internet of Things with the vision to identify and address existing challenges. Additionally, the book provides future research directions in this domain, and explores the different applications of IoT and its associated technologies. Studies investigate applications for crowd sensing and sourcing, as well as smart applications to healthcare solutions, agriculture and intelligent disaster management. This book will appeal to students, practitioners, industry professionals and researchers working in the field of IoT and its integration with other technologies to develop comprehensive solutions to real-life problemsThis book presents selected research papers on current developments in the fields of soft computing and signal processing from the Third International Conference on Soft Computing and Signal Processing (ICSCSP 2020). The book covers topics such as soft sets, rough sets, fuzzy logic, neural networks, genetic algorithms and machine learning and discusses various aspects of these topics, e.g., technological considerations, product implementation and application issues.Over 60 recipes will help you build smart IoT solutions and surprise yourself with captivating IoT projects you thought only existed in Bond moviesAbout This Book- This book offers key solutions and advice to address the hiccups faced when working on Arduino- based IoT projects in the real world- Take your existing skills and capabilities to the next level by building challenging IoT applications with ease.- Be the tech disruptor you always wanted to be with key recipes that help you solve Arduino IoT related problems smarter and faster.- Put IoT to work through recipes on building Arduino-based devices that take control of your home, health, and life!Who computational techniques with traditional computing methods has inspired researchers and academics alike to focus on developing innovative computational techniques. In the near future, computational techniques may provide vital solutions by effectively using evolving technologies such as computer vision, natural language processing, deep learning, machine learning, scientific computing, and computational vision. A vast number of intelligent computational algorithms are emerging, along with increasing computational power, which has significantly expanded the potential for developing intelligent applications. These proceedings of the International Conference on Inventive Computation Technologies [ICICT 2019] cover innovative computing applications in the areas of data mining, big data processing, information management, and security.There is no doubt that there has been much excitement regarding the pioneering contributions of artificial intelligence (AI), the internet of things (IoT), and blockchain technologies and tools in visualizing and realizing smarter as well as sophisticated systems and services. However, researchers are being bombarded with various machine and deep learning algorithms, which are categorized as a part and parcel of the enigmatic AI discipline. The knowledge discovered gets disseminated to actuators and other concerned systems in order to empower them to intelligently plan and insightfully execute appropriate tasks with clarity and confidence. The IoT processes in conjunction with the AI algorithms and blockchain technology are bound to lay out a stimulating foundation for producing and sustaining smarter systems for society. Advancing Smarter and More Secure Industrial Applications Using AI, IoT, and Blockchain Technology articulates and accentuates various AI algorithms, fresh innovations in the IoT, and blockchain spaces. The domain of transforming raw data to information and to relevant knowledge is gaining prominence with the availability of data ingestion, processing, mining, analytics algorithms, platforms, frameworks, and other accelerators. Covering topics such as blockchain applications, Industry 4.0, and cryptography, this book serves as a comprehensive guide for AI researchers, faculty members, IT professionals, academicians, students, researchers, and industry professionals.“With futuristic homes on the rise, learn to control and automate the living space with intriguing IoT projects.” About This Book Build exciting (six) end-to-end home automation projects with Raspberry Pi 3, Seamlessly communicate and control your existing devices and build your own home automation system, Automate tasks in your home through projects that are reliable and fun Who This Book Is For This book is for all those who a new interconnectivity your world. Style and approach End to end home automation projects with 3.Throughout human history, technological advancements human our advancements, been discover over large this human intervention. These advancements may become essential may book The book includes high-quality research work by academicians and industrial field computing and communication, full-length papers, research-in-progress papers and case studies related to all the areas of data mining, machine learning, Internet of things (IoT) and information security.This book presents chapters from diverse range of authors on different aspects of how Blockchain and IoT are converging and the impacts of these developments. The book provides an extensive cross-sectional and multi-disciplinary look into this trend and how it affects artificial intelligence, cyber-physical systems, and robotics with a look at applications in aerospace, agriculture, automotive, critical infrastructures, healthcare, manufacturing, retail, smart transport systems, smart cities, and smart healthcare. Cases include the impact of Blockchain for IoT Security; decentralized access control systems in IoT; Blockchain architecture for scalable access management in IoT; smart and sustainable IoT applications incorporating Blockchain, and more. The book from and practitioners from diverse perspectives. how Blockchain and IoT are converging and the impacts of these developments on technology and its application; Discusses IoT and Blockchain from cross-sectional and multi-disciplinary perspectives; Includes contributions from researchers, academics, and professionals from around the world.With the recent growth of big data and the internet of things (IoT), individuals can now upload, retrieve, store, and collect massive amounts of information to help drive decisions a",,2022.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
a5dffa86ee25138623b71f4d03a5ff5757fbdfdd,https://www.semanticscholar.org/paper/a5dffa86ee25138623b71f4d03a5ff5757fbdfdd,On National Projects,"Albania used the support from the Safe Online initiative to end violence against children online on multiple levels. To reach children, the agency trained children to become peer educators on online violence, increasing their knowledge on safe Internet navigation and helping them spread that information to their peers. UNICEF Albania conducted research on children’s use of the Internet and risk of online violence using the Global Kids Online methodology. The results from this study are being used to conduct national awareness activities. UNICEF Albania also worked at the national level by strengthening the national child helpline and hotline, which alert authorities about child sexual abuse material online. They also carried out an assessment of national legislation, policies and programmes currently targeting online child sexual exploitation and abuse in-line with the WePROTECT Global Alliance Model National Response. This will inform measures to improve the systematic response to online child sexual exploitation and abuse. UNICEF Albania also helped the government to develop a national strategy for cyber-security with a focus on children’s online safety. The agency also supported law enforcement authorities, strengthening their ability to investigate and prosecute cases of online child sexual abuse and exploitation. In addition, they worked with local municipalities and Internet service providers to promote “Albania Friendly Wi-Fi,” a safe certification standard for public Wi-Fi. These services will make Internet navigation safer for children, while also increasing national awareness on internet safety. They also used the support from the Safe Online initiative to engage the information, communications and technology sector on children’s Internet usage and risks in the digital space. to better detect, deter and prevent this type of violence. To do so, Justice and Care will explore the profiles of those who perpetrate and facilitate online sexual exploitation of children, interviewing convicted offenders, key informants, and others. This analysis will fill a gap in global research into online child exploitation of children and shed light on the “supply-side” of such violence in a country known to be an epicenter of live-streamed child sexual abuse. Ultimately, this research will seek to inform practical strategies and enhance industry, prevention and law enforcement response to the issue. The project also trained parents, teachers, social service providers and other adults to become advocates for cyber safety. At the industry level, the project also engaged Internet service providers and other technology companies in the fight against online child sexual exploitation and abuse. protect children from being victims of child sexual abuse. The chatbot will first be developed for pilot use in the United Kingdom with the potential for scaling up in other countries. The Safe Online initiative supported ChildFund Australia’s Swipe Safe program in Vietnam, which aimed to help young people navigate the Internet safely by educating them on potential risks, such as cyber scams, bullying or sexual abuse, and offering them strategies to protect themselves. ChildFund Australia designed, created and tested a training program to promote online safety – and ever since, the curriculum has been adapted by non-governmental organisations not just in Viet Nam, but in Laos and Myanmar as well. Swipe Safe mobilizes parents, youth, schools and the private sector to play an active role in children’s online safety. The program provided training for parents and Internet café owners and managers to identify and address risks that might happen to children, from online to offline and vice versa. It also supported schools to develop child-friendly policies and guidance on online safety. Swipe Safe is active in advocating to the national government with lessons learned to inform national policy and response and linking such legislation with the strengthening of existing structures. A key innovation of the program is that it engaged young volunteers in local communities with extensive knowledge on technology to train young people and others, as these trainers can more directly relate to their peers’ experiences and help keep the curriculum up to date. Disrupting Harm is a large-scale data collection and research project to better understand online child sexual exploitation and abuse across the world. This study is assessing the scale, nature and context of this issue in 14 countries across Southern and Eastern Africa and Southeast Asia. Supported by the Safe Online initiative, three grantee partners will work together to conduct the study, including ECPAT International, INTERPOL and the UNICEF Office of Research – Innocenti. ECPAT's role is to conduct a comprehensive analysis, allowing partners (and all others working in this arena) to better understanding the context of children's safety online. A new version of Disrupting Harm is currently bringing design, and it is expected to bring the large-scale research project to other regions. NTERPOL will bring the most advanced technology to investigators of online CSEA through its new DevOps Group Project. The initiative will facilitate research and development by an expert group of investigators, non-governmental organisations, academia, and information technology companies, and extend solutions to specialised officers worldwide via INTERPOL’s secure channels. Headquartered in France, this project has a global reach. Disrupting Harm is a large-scale data collection and research project to better understand online child sexual exploitation and abuse across the world. This study is assessing the scale, nature and context of this issue in 14 countries across Southern and Eastern Africa and Southeast Asia. Supported by the Safe Online initiative, three grantee partners will work together to conduct the study, including ECPAT International, INTERPOL and the UNICEF Office of Research – Innocenti. INTERPOL's role is to examine the threats facing children online and analyse data from law enforcement agencies across the world. A new version of Disrupting Harm is being designed and is expected to bring the research project to other regions. with INHOPE and other specific will explore and quantify the issues facing content moderators, as it relates to their exposure of traumatic child sexual abuse material. They will also identify coping strategies currently used by content moderators, and highlight what works – and what does not work – for individuals and organisations that do this work. Results of this study will be used to develop a pilot intervention to support and protect the mental health of content moderators. Lapsia Ry will develop and launch ReDirection, an evidence-based self-help programme working to prevent the consumption of CSAM on the Dark Web. By providing targeted support for these individuals, the project will also reveal new information about these searchers and their pathways to CSAM access and use. This programme builds on the Finnish government’s accredited New Direction rehabilitation programme for sex offenders. Headquartered in Finland, this project has a global reach. Through this project, Technological University Dublin will develop a deployable tool that reveals the patterns of adults perpetrating online child sexual abuse and the children who are affected by such violence. By using advanced artificial intelligence machine learning for text, the study will advance global understanding of trends in perpetrator behaviour (conduct, contact, content) – including grooming – and debunk strategies and tactics used to lure and coerce children into sexually exploitative acts. N-Light will be created in collaboration with two essential partner organisations, the Irish Society for the Prevention of Cruelty to Children (ISPCC) and Hotline.ie, the Irish national centre combatting illegal content online, specifically child sexual abuse material (CSAM) and activities relating to online child sexual exploitation (OCSE). Once finalized, N-Light will be tested by both partner organisations, with the intention to make it available to other hotlines in the INHOPE network and child agencies for their use, which would in turn lead to an enriched, more robust and representative data sample and analysis capacity. In addition, the data and insights will serve to better understand and conceptualise victim and perpetrator behavior, patterns and activity, ultimately informing the further development of evidence-based solutions that would have the potential of transformative impact in tackling this heinous crime against children. on the psychological processes through which people of online sexual and professional support. In addition, the group will explore the efficacy and impact of prevention interventions targeting people engaging with online abuse. Overall, the project will ask a fundamental – and often overlooked – question: who seeks help for child sexual exploitation and abuse, and can we get more people to do so before committing a crime? This project will expand the group’s existing model of psychological predicators of help-seeking for people at risk of offending and examine how to amplify the psychological factors that support such help-seeking behaviors. At the same time, the project will also look into the psychological barriers that prevent help-seeking and explore ways to weaken those barriers in the sphere. The World Health Organisation is using Safe Online investments to explore current systems of prevention and response to online child sexual exploitation and abuse. These findings will support governments and civil society organisations, giving them the tools and evidence, they need to implement effective, evidence-based programs to keep children safe online. After determining what works and does not work around preventing and responding to sexual and emotional online child abuse, the proj",,2003.0,10.6027/9789289335157-8-en,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
8bf7d1b20ca61ab9de848b92235f9a895915a2e3,https://www.semanticscholar.org/paper/8bf7d1b20ca61ab9de848b92235f9a895915a2e3,Editorial: New Frontiers for Artificial Intelligence in Surgical Decision Making and its Organizational Impacts,"Artificial Intelligence (AI) as a new technology in the Industry 4.0 scenario(1) is gaining importance in a variety of industrial and service sectors, including healthcare (2). Several algorithms were approved for clinical use worldwide, involving many clinical specialities. As of today, AIand Machine and Deep Learning-based applications in oncology and surgery have the primary aim of supporting clinical decision-making (3). The use of AI-based applications may be beneficial for medical doctors to guide their clinical decisions, helping them in early and more accurate diagnoses and appraising unforeseen issues. Such decision aids look particularly valuable when physicians are called to decide fast, in complex and uncertain situations and with time constraints. The current literature underlines how AI-based applications may contribute to better clinical outcomes. Still, such applications also imply the need for hospitals and institutions to understand the organizational implications, for example, in terms of investments, costs, the actual degree of efficiency and effectiveness, the relationship among the various stakeholders involved, and the educational and training plans. The purpose of the research topic call “New Frontiers for Artificial Intelligence in Surgical Decision Making and its Organizational Impacts “ was to collect the recent developments and undergoing studies in AI in surgery and surgical oncology. More in detail, the aim was to gather contributions on the advancement, deployment, use, and implementation of AI-based applications in surgical practice, understanding their potential contribution to clinical decision making. Moreover, the idea was to assess the potential impacts of such a technology on surgeons, other clinicians, patients, medical institutions, developers, and policy-makers, with an eye open to the organizational and educational consequences and opportunities. The special issue gathered six articles: five original research pieces and one mini-review. In the field of neuro-oncology, the paper by Tariciotti et al. (4) reported a pilot study concerning a Deep Model algorithm trained and internally validated to reliably differentiate ambiguous cases of primary central nervous system lymphomas, glioblastoma, and brain metastasis using T1-weighted",Frontiers in Surgery,2022.0,10.3389/fsurg.2022.933673,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
0a29101ca20edb01f403787844f455b317d34d62,https://www.semanticscholar.org/paper/0a29101ca20edb01f403787844f455b317d34d62,Taking ROCKET on an Efficiency Mission: Multivariate Time Series Classification with LightWaveS,"—Nowadays, with the rising number of sensors in sec- tors such as healthcare and industry, the problem of multivariate time series classiﬁcation (MTSC) is getting increasingly relevant and is a prime target for machine and deep learning approaches. Their expanding adoption in real-world environments is causing a shift in focus from the pursuit of ever-higher prediction accuracy with complex models towards practical, deployable solutions that balance accuracy and parameters such as prediction speed. An MTSC model that has attracted attention recently is ROCKET, based on random convolutional kernels, both because of its very fast training process and its state-of-the-art accuracy. However, the large number of features it utilizes may be detrimental to inference time. Examining its theoretical background and limitations enables us to address potential drawbacks and present LightWaveS: a framework for accurate MTSC, which is fast both during training and inference. Speciﬁcally, utilizing wavelet scattering transformation and distributed feature selection, we manage to create a solution that employs just 2.5% of the ROCKET features, while achieving accuracy comparable to recent MTSC models. LightWaveS also scales well across multiple compute nodes and with the number of input channels during training. In addition, it can signiﬁcantly reduce the input size and provide insight to an MTSC problem by keeping only the most useful channels. We present three versions of our algorithm and their results on distributed training time and scalability, accuracy, and inference speedup. We show that we achieve speedup ranging from 9x to 53x compared to ROCKET during inference on an edge device, on datasets with comparable accuracy.",ArXiv,2022.0,10.48550/arXiv.2204.01379,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
7991b7b0317c0d29bc209f18acc18b5933ac71be,https://www.semanticscholar.org/paper/7991b7b0317c0d29bc209f18acc18b5933ac71be,"Part B 1 ] 1 ( to be evaluated in Step 1 ) Ubiquitous , spatiotemporal , multimodal action recognition Act Now","Action and activity recognition lie at the core of a panoply of scenarios in human machine interaction, ranging from gaming, mobile computing and video retrieval to health monitoring, surveillance, robotics and biometrics. The problem, however, is made really challenging by the inherent variability of motions carrying the same meaning, the unavoidable over-fitting due to limited training sets, and the presence of numerous nuisance factors such as locality, viewpoint, illumination, and occlusions that make real-world deployment extremely difficult. The most successful recent approaches, which mainly classify bags of local features, have reached their limits: only understanding the spatial and temporal structure of human activities can help us to successfully locate and recognise them in a robust and reliable way. We propose here to develop novel frameworks for the integration of spatiotemporal action structure in both generative and discriminative models, pushing for a breakthrough ripe with enormous exploitation potential. A new class of hierarchical part-based discriminative models originally developed for object recognition are reinvented for action localisation and recognition, as a fundamental way of coping with complex activities formed by series of simple actions and addressing the issues with locality and multiple actors. New manifold learning techniques for generative graphical models are developed to tackle the presence of nuisance factors and improve their generalisation power. Finally, novel classes of graphical models able to handle whole convex sets of probabilities are formulated in order to address the issue of overfitting due to the limited size of the training sets. As companies are heavily investing on virtual mice, smart TVs, phones and cars, and range sensors are changing clinical practice and the entertainment industry, the timeliness and potential impact of this project could not be understated.",,,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
faa59797f53cd706359f1b9dde2d54197186e40c,https://www.semanticscholar.org/paper/faa59797f53cd706359f1b9dde2d54197186e40c,IMECE 2007-43720 Sensor Fusion for Machine Condition Monitoring,"Machinery maintenance accounts for a large proportion of plant operating costs. Compared with the conventional scheduled maintenance strategy which is to stop the machine at pre-determined intervals, modern condition-based maintenance strategy stops the machine only before there is evidence of impending failure. With the development of cheaper sensors, more and more sensors are designed for machine condition monitoring. It is now possible to use multi-modal sensor input to monitor machine condition in a collaborative and distributed manner. In this paper, three categories of methods for condition monitoring are reviewed – 1. knowledge based, 2. model based 3. data based methods. Knowledge-based systems are derivations from expert systems that use rules and inference engines to determine failures and their causes. Data-driven methods use machine fault data, typically derived during experiments to train a monitoring system. Pattern recognition algorithms then attempt to classify actual sensor data using the results of the training phase. However it is often impractical to obtain data for every type of fault. Model-based techniques on the other hand use mathematical models to predict machine performance. We propose to combine the model based and data based method for machine condition monitoring. The data is used to train the model and derive its parameters. The various fault modes are then identified and simulated. The output is then input to the classification schemes that can be then used to identify and classify real-time data. We apply the technique to condition monitoring of electrical motors. INTRODUCTION Machinery maintenance accounts for a large proportion of plant operating costs. It has been clearly demonstrated that the use of appropriate condition monitoring and maintenance management techniques can give industries significant improvements in efficiency and directly enhance profitability [1]. Condition monitoring or CBM (Condition Based Maintenance) is an effective form of predictive maintenance (PdM) where, as the name stated, people monitor the condition of specific areas of plant and equipment. CBM involves the observation of a system over time using periodically sampled dynamic response measurements from an array of sensors, the extraction of fault-sensitive features from these measurements, and the statistical analysis of these features to determine the current state of the system [2]. It is also referred to fault detection, fault isolation and identification. The use of CBM allows maintenance to be scheduled, or other actions to be taken to avoid the consequence of failure, before the failure occurs. It is typically much more cost effective than allowing the machinery to fail [1]. Over the past sixty years major improvements have occurred in the technology, practice and systems used for equipment condition measurement. Since 1939, vibration measurements have been used to judge the condition of machinery [3]. However, wireless technology [4] has only recently been developed and deployed for vibration-based condition monitoring. Besides sensor and signal processing technology, there have been significant developments in the architectures and methodologies to perform condition monitoring. Current existing techniques can be classified into three categories: knowledge based, model based, and data based methods. These methods are reviewed in the next section, followed by a review of popular frameworks of sensor fusion. A new combined method is proposed for machine condition monitoring. REVIEW OF CURRENT METHODS Knowledge based methods Knowledge based methods mainly perform automated reasoning to carry out situation assessment, consequence prediction and analysis [5]. One of the first machinery expert systems (Amethyst) was introduced by IRD in the mid 1980s. Also in the mid 1980s, Predict DLI developed an expert system for use with the US Navy aircraft carrier Condition Based Maintenance program for which they provided data and analysis [3]. This knowledge-based method is still popular for making expertise available to decision makers and technicians who need answers quickly. The most common form of expert systems is a program made up of a set of rules that analyze information (usually supplied by the user of the system) about a specific class of problems and recommend a course of action to implement corrections [6]. A typical representation of knowledge would be presented by rules that have the form: If (evidence exists for X) then do Y, where Y may involve performing a computation or updating a database. The rules, however, need to programmed into the system based upon opinions of human experts and can therefore be prone to subjectivity. Model based methods The basic principle of a model-based fault detection scheme is to generate residuals that are defined as the differences between the measured and the predicted variables. Ideally, these residuals are only affected by system faults and are not affected by any changes in the operating conditions, such as power quality changes or load variations [7, 8]. So the key point of this method is to find a variable which can be well modeled and measured to generate the residual signal which is not always available in some systems. Data based methods Many modern approaches to fault diagnosis and prognosis are based on the idea of pattern recognition (PR). In the broadest sense, a PR algorithm is simply one that assigns to a sample of measured data a class label, usually from a finite set. The appropriate class labels would encode damage type, location etc. In order to carry out the higher levels of identification using PR, it will almost certainly be necessary to construct examples of data corresponding to each class [9]. Classical Bayesian classifier, Linear Discriminant Analysis (LDA), Artificial Neural Network (ANN) and Support Vector Machine (SVM) are the major algorithms for pattern classification [10]. A feature is some characteristic of measurements (such as averages, dominant frequencies etc.) that provides information to discriminate between various classes of input data. In fact, an ideal feature extraction can make the job of classifier trivial If the extracted features are good then simple methods would do a good job in classification. Good features are however usually domain dependent. Different applications will have different features of interest. SENSOR FUSION ARCHITECTURE A complex machine consists of many components which could be the potential fault sources. When a single sensor is not able to identify all the faults in a machine, multiple sensors are needed to fulfill this task. Multi-sensor based condition monitoring system collects data from different sensors. Data fusion is necessary to make good use of all the sensors’ data. Bedworth and O’Brien [11] described a popular framework called the Omnibus model. Figure 1 gives the general layout of this framework, which consists of four main modules and can be executed along the clockwise loop. These modules are used to address the various tasks in sensor fusion and its functional objectives. Fig. 1 Omnibus data fusion model In pattern recognition module, there are two kinds of learning method which are supervised learning and unsupervised learning. Supervised learning is a type of learning algorithm in which the diagnostic is trained by showing it the desired label for each data set [10]. Unsupervised learning doesn’t require the labeled faults data, but it can only be used for detection which is called anomaly detection methods. If supervised learning is required, there will be serious demands associated with it; data from every conceivable damage situation should be available. The two possible sources of such data are computation or modeling, and experiment. In order to accumulate enough training data, it would be necessary to make copies of the system of interest and damage it in all the ways that might occur naturally; in reality, this is simply a waste of money. Modeling requires understanding of physics of the system. The more physics we know about the system, the less faults data we need for the training process. PROPOSED METHOD As the purpose of condition monitoring, not only the diagnosis of the machine faults but also some prognosis of the machine life are expected. Although the prognosis task of life time prediction is difficult at present due to the lack of the understanding of complex system and the lack of access to the faults data, the method used now should have the potential to achieve the prognosis task in the future. Supervised learning methods have the potential to predict the time to failure. The difficulty of this kind of methods is always the access to the faults data. Modeling of the machine measurable signal under normal condition and faults condition is vital to achieve this method. With the development of the physics which describes the complexity of the machine system, more and more measurable signal can be modeled mathematically. Figure 2 shows an ideal system diagram for machine online condition monitoring which uses the measurement of the system operation under normal condition to first identify the system parameters. The model with its parameters thus identified is used to simulate the system under faulty conditions. The simulated data can then be used to extract features and train classifiers. When the system is deployed and operational, sensor data from the machinery is used to extract features that are then classified by the classifiers obtained from the simulated results. The concept is applied to fault monitoring of 3-phase induction motors. Motor Current Signature Analysis (MCSA) is a commonly used technique for fault monitoring of large induction motors. It is a noninvasive, on-line monitoring technique for the diagnosis of problems in large induction motors. Specific harmonic components are located to detect different faults such as broken rotor bars",,2007.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
67c37b0e7ee1f23e1d6979cb3eb34d80fdd35fe0,https://www.semanticscholar.org/paper/67c37b0e7ee1f23e1d6979cb3eb34d80fdd35fe0,"Artificial-Intelligence-Based, Automated Decline Curve Analysis for Reservoir Performance Management: A Giant Sandstone Reservoir Case Study","
 Decline curve analysis (DCA) is one of the most widely used forms of data analysis that evaluates well behavior and forecasts future well and field production and reserves. Usually, this practice is done manually, making analysis of assets with a large number of wells cumbersome and time-consuming. Moreover, results are subject to alternate interpretations, mostly as a function of experience and objectives of the evaluator.
 In this work, despite the common practice of the industry, i.e. manual DCA, we developed and deployed cutting-edge technologies that intelligently apply DCA methods to any number of wells in an unbiased, systematic, intelligent, and automated fashion. The tool reads production data, and multidisciplinary well information (e.g., drilling and completion data, geological data, artificial lift information, etc.). Then it performs cluster analysis using unsupervised machine learning and pattern recognition to partition the dataset into internally homogeneous and externally distinct groups. This cluster analysis is later used for type-curve generation for wells with short production history. For wells with long enough history, the tool first detects production events through a fully automated event detection algorithm without any human interference. Since production events are highly correlated with real-time events, it also cross-validates with the operating conditions. Next, the last event is selected, and a decline curve is fitted using advanced nonlinear optimization and minimization algorithms. This leads to a reliable and unbiased prediction. For each cluster, a type curve is computed that truly captures the underlying production behavior of the wells that belong to the same group or cluster, and then is applied to the wells with short production history within that cluster. To capture the probabilistic nature of such analysis and quantify the inherent uncertainty, we extended the method to a probabilistic DCA using quantile regression.
 We successfully deployed this technology/tool to a giant Middle Eastern reservoir, with more than 2,000 wells and 70 years of production. Our predicted aggregated field decline rate is in good agreement with the client's reservoir simulation results run under the ""do-nothing"" scenario. While performing traditional DCA for such a field would require several weeks and significant resources, our automated solution integrates all real-life events/information and provides a comprehensive analysis in field, cluster and well level. In addition, our results are ""unbiased,"" as it is not subject to human errors or evaluator's interpretations.
 Our robust and intelligent DCA allows for exhaustive evaluation of production trends and opportunities in fields across time, production zones, well types, and any combinations of the above. The results demonstrate the effectiveness of the automated DCA to rapidly execute decline curve analysis for a large number of wells. The accuracy is improved significantly through automatic event detection, cross-validation of events, curve fitting optimization, quantile regression, and cluster-based type-curving.","Day 2 Tue, November 12, 2019",2019.0,10.2118/197142-ms,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
d89562ee18bce330f3cf72d793de5a921c08c5c4,https://www.semanticscholar.org/paper/d89562ee18bce330f3cf72d793de5a921c08c5c4,RETHINKING CALIBRATION FOR PROCESS SPECTROMETERS II,"Optical spectroscopy is a favored technology to measure chemistry and is ubiquitous in the hydrocarbon processing industry. In a previous paper, we focused on a generic, machine-learning approach that addressed the primary bottlenecks of mustering data, automating analyzer calibration, and tracking data and model performance over time. The gain in efficiency has been considerable, and the fact that the approach does not disturb any of the legacy (i.e., no changes or alterations to any analyzer or software in place) made deployment simple. We also standardized a procedure for doing calibrations that adheres to best practices, archives all data and models, provides ease of access, and delivers the models in any format. What remains is to assess the speed of processing and the quality of the models. To that end, a series of calibration experts were tasked with model optimization, restricting the work to selecting the proper samples to include in the computation and setting the number of factors in PLS. The amount of time and the quality of the models were then compared. The automated system performed the work in minutes rather than hours and the quality of the predictions at least matched the best experts and performed significantly better than the average expert. The conclusion is that there is a large amount of recoverable giveaway that can be avoided through automation of this process and the consistency it brings to the PLS model construction. RETHINKING SPECTROSCOPIC CALIBRATIONS II © 2020 ISA – The Instrumentation, Systems, and Automation Society. All rights reserved. 2 INTRODUCTION There is a lot of mundane work tied to the assembly of spectra and laboratory reference values to enable quality calibration work. There is also insufficient guidance when it comes to the model construction task. How much time should be spent on this task? How to best assess whether a spectrum-reference pair is an outlier or not? How many cycles of regression-sample elimination make sense? Where do we switch over from improving the model by adding PLS factors to overfitting and incorporating destabilizing noise? Why is this discussion important? Optical spectroscopy is one of the few instrumental technologies that allow us to assess the chemistry inside our manufacturing processes; the other being chromatography [1]. Spectroscopy measures the concentration of chemical functional groups, allowing us to infer a great number of properties with both accuracy and precision. Use of spectroscopy shifts the quality assessment from daily to minute-by-minute. In a petroleum refinery, where blending shifts can occur hourly, lab results are simply too infrequent and not timely. So, we adopt optical spectroscopy (NIR, IR or Raman) to solve the timeliness problem, but the quality of the reported values is completely controlled by the quality of the calibration. When we consider the costs of octane or RVP giveaway, the approach we take to optimize calibration models takes on a huge significance. There are automated optimization solutions that make sense to deploy as discussed in our earlier paper [2]. Are the costs of the solution justified? In this paper, we focus on two questions: one whether an automated calibration system can perform as well as true experts in the field, and the second to provide a mechanism for companies to objectively evaluate the quality of the models they produce in-house. To the second end, we propose a cost-free test to judge a company’s ability to generate optimal calibration performance. This paper builds on papers presented in the 2015, 2018, and 2019 ISA-AD Symposia [3, 1, 2]. The results reported in this paper supports our earlier contention that consistency in the approach to spectroscopic calibration is critical. For background on the multivariate calibration process, there are myriad articles and books. For the uninitiated or those interested in the organization of industrial projects, we recommend the text by Beebe et al. [4]; for more advanced practitioners interested in theory, Martens and Næs [5] is an excellent place to start.",,2020.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
396d0a8dc9ffd7f4530e1e0c900ac416024f3857,https://www.semanticscholar.org/paper/396d0a8dc9ffd7f4530e1e0c900ac416024f3857,Study of possible applications of AI Techniques to SEAT Manufacturing IT,"In order to survive in such an extreme competitive market as the car manufacturing industry is, 
automobile companies have traditionally invest a considerable amount of resources and effort in 
using innovative solutions to improve the efficiency of the manufacturing process in their plants. 
SEAT, being one of the innovation leading companies in the sector, aims to fully embrace the 
paradigms of the new industrial revolution known as “Industry 4.0”, based in the interconnection 
of machines, processes, systems and people. 
Artificial Intelligence techniques have a strong presence into the Industry 4.0 foundations, 
especially by means of exploiting the huge amount of digitalized data that is produced each 
second in a modern plant. But it is not only in data analysis where AI takes part in this new 
revolution but also in fields which are already mature enough to be deployed in industrial 
solutions like Computer Vision or Multi-Agent Systems. 
This work studies the evolution of the SEAT manufacturing process of its core plant in Martorell 
(Barcelona) together with the specific challenges that it faces nowadays and identifies a series of 
improvement opportunities for solving those challenges using Artificial Intelligence techniques. 
This identification is possible because it combined years-long working experience in the 
automobile industry with the learning acquired during the course of the Master in Artificial 
Intelligence of the Facultat d’Informatica de Barcelona. The work aims to analyze in general the 
implementation of Artificial Intelligence in a company, pointing out the real uses that the 
technology might have and it is written from a business strategic point of view. 
For each identified improvement opportunity the work conducts an analysis about the current 
situation, the most suitable AI technique to apply and the expected outcome of an improvement 
project. Such project would focus on implementing the selected technology for solving the 
specific problem or improving the described process. Both business and technological aspects 
are analyzed in detail for each improvement opportunity, setting the base for the execution of a 
full project in the future. 
Since the resources in a company are finite, both in terms of budget and personal, the next step 
is to carry out a prioritization of all the identified opportunities in order to start implementing 
the most affordable and interesting ones. For this purpose, the role of the business clients is 
essential to categorize the possible improvements. This is where the transference of the 
knowledge between IT and the manufacturing area occurs and the technology converts into a 
tool, a mean to achieve a goal and not a mean to itself. 
As a result of the prioritization, two projects are analyzed in detail for its future implementation. 
The first one consists on the application of process mining techniques for the optimization of the 
quality final revision workflow of the manufactured cars. The second one is a root cause analysis 
of the car painting failures that appear during the second part of the manufacturing process. 
Because of the bigger magnitude of the project, the data modeling part is to be implemented 
with external resources from the Volkswagen Group. Therefore the objective of the work is to 
reflect the project management part that I carry out as project leader at SEAT, including a 
detailed planning and exhaustive requirements specification, as well as the design of how the 
solution will be implemented and how the goals of the project will be accomplished. The work 
end by standing out the learning made during the whole process of the study and outlines the 
future work directions in order to transform SEAT manufacturing in one of the industry leaders of 
the upcoming industrial revolution brought by Artificial Intelligence.",,2016.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
84e9fc77d419b09924e0ee644028ad360574bb59,https://www.semanticscholar.org/paper/84e9fc77d419b09924e0ee644028ad360574bb59,Editorial: Data-Driven Solutions for Smart Grids,"This research topic deals with innovative data-driven solutions for smart grids. The integration of communication systems and smart metering in power system has led, in recent years, to capability to measure quantities and communicate measurements with a precision, a sampling rate and a bandwidth that were unimaginable just a few years ago. The potential for this amount of information is huge but the power system industry and community have not yet fully exploited such a capability. In this complex data-rich, but information-limited domain, the data streaming generated by the pervasive grid sensors do not always provide smart grids operators with the necessary information to react to external disturbances in a timely manner. Even if fast computing algorithms are utilized to convert data into information, smart grid operators face the challenge of not having the full picture of the information context and, therefore, the obtained information cannot be deployed with a high degree of confidence. To address this complex issue, the most promising research directions are oriented toward the conceptualization of improved information processing paradigms and smart decision support systems aimed at enhancing standard operating procedures, based on pre-defined grid conditions and static operating thresholds, with a set of interactive information services, which could promptly provide the right information at the right moment to the right decision maker. To effectively support the deployment of these services in modern smart grids it will be incumbent upon the scientific community to develop advanced techniques and algorithms for reliable power system data acquisition and processing, which should support semantics and content-based data extraction and integration from heterogeneous sensor networks. This research topic contains four articles. The paperOptimal Balancing ofWind Parks with Virtual Power Plants by Omelčenko andManokhin addresses data-driven solutions in the context of optimization of virtual power plants. This work proposes the use of machine learning to process available data measurements. The goal is to balance the power production and at the same time maximize the revenue of a portfolio of power plants with different technologies (biogas, wind, batteries, etc.) considering uncertainty in both price and power production. The paper Supporting Regulatory Measures in the Context of Big Data Applications for Smart Grids by Mladin discusses the policy and regulatory aspects. This paper focuses in particular on big data applications to the ongoing “energy transition” process built on higher renewable energy integration and digitalization, and discusses how this can help regulatory measures through societal acceptance and involvement. The paper Data Consistency for Data-Driven Smart Energy Assessment by Chicco addresses the issue of data consistency and discusses data-versus model-based approaches. The latter is an emerging topic and a potential paradigm shift in power system analysis that have been traditionally based on modelEdited and reviewed by: Huan Liu, Arizona State University, United States",Frontiers in Big Data,2021.0,10.3389/fdata.2021.815686,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
2f470cfddc970e8d134aacfa42600f0c399eed48,https://www.semanticscholar.org/paper/2f470cfddc970e8d134aacfa42600f0c399eed48,Evaluating Intrusion Detection Systems for Energy Diversion Attacks,"The widespread deployment of smart meters and ICT technologies is enabling continuous collection of high resolution data about consumption behavior and health of grid infrastructure. This has also spurred innovations in technological solutions using analytics/machine learning methods that aim to improve efficiency of grid operations, implement targeted demand management programs, and reduce distribution losses. One one hand, the technological innovations can potentially lead large-scale adoption of analytics driven tools for predictive maintenance and anomaly detection systems in electricity industry. On the other hand, private profit-maximizing firms (distribution utilities) need accurate assessment of the value of these tools to justify investment in collection and processing of significant amount of data and buy/implement analytics tools that exploit this data to provide actionable information (e.g. prediction of component failures, alerts regarding fraudulent customer behavior, etc.) In this thesis, the focus on the value assessment of intrusion/fraud detection systems, and study the tradeoff faced by distribution utilities in terms of gain from fraud investigations (and deterrence of fraudulent customer) versus cost of investigation and false alarms triggered due to probabilistic nature of IDS. Our main contribution is a Bayesian inspection game framework, which models the interactions between a profit-maximizing distribution utility and a population of strategic customers. In our framework, a fraction of customers are fraudulent they consume same average quantity of electricity but report less by strategically manipulating their consumption data. We consider two sources of information incompleteness: first, the distribution utility does not know the identity of fraudulent customers but only knows the fraction of these consumers, and second, the distribution utility does not know the actual theft level but only knows its distribution. We first consider situation in which only the first source of information incompleteness is present, i.e., the distribution utility has complete information about the actual theft level. We present two simultaneous game models, which have same assumption 3 about customer preferences and fraud, but differ in the way in which the distribution utility operates the IDS. In the first model, the distribution utility probabilistically chooses to use IDS with a default (fixed) configuration. In the second model, the distribution utility can configure/tune the IDS to achieve an optimal operating point (i.e. combination of detection probability and false alarm rate). Throughout, we assume that the theft level is greater than cost of attack. Our results show that for, the game with default IDS configuration, the distribution utility does not use the IDS in equilibrium if the fraction of fraudulent customers is less than a critical fraction. Also the distribution utility realizes a positive “value of IDS” only if one or both have the following conditions hold: (a) the ratio of detection probability and false alarm probability is greater than a critical ratio, (b) the fraction of fraudulent customers is greater than the critical fraction. For the tunable IDS game, we show that the distribution utility always uses an optimal configuration with non-zero false alarm probability. Furthermore, the distribution utility does not tune the false alarm probability when the fraction of fraudulent customers is greater than a critical fraction. In contrast to the game with fixed IDS, in the game of tunable IDS, the distribution utility realizes a positive value from IDS, and the value increases in fraction of fraudulent customers. Next, we consider the situation in which both sources of information incompleteness are present. Specifically, we present a sequential game in which the distribution utility first chooses the optimal configuration of the IDS based on its knowledge of theft level distribution (Stage 1), and then optimally uses the configured IDS in a simultaneous interaction with the customers (Stage 2). This sequential game naturally enables estimation of the “value of information” about theft level, which represents the additional monetary benefit the distribution utility can obtain if the exact value of average theft level is available in choosing optimal IDS configuration in Stage 1. Our results suggest that the optimal configuration under lack of full information on theft level lies between the optimal configurations corresponding to the high and low theft levels. Interestingly enough, our analysis also suggests that for certain technical (yet realistic) conditions on the ROC curve that characterizes achievable detection probability and false alarm probability configurations, the value of information about certain combination of theft levels can attain negligibly small values. Thesis Supervisor: Saurabh Amin Title: Robert N. Noyce Career Development Assistant Professor",,2016.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
c94355572a20f4443889f9b93233ac2facbbef78,https://www.semanticscholar.org/paper/c94355572a20f4443889f9b93233ac2facbbef78,A Machine Learning Framework for Profitability Profiling and Dynamic Price Prediction for the New York City Taxi Trips,"The New York City Taxi & Limousine Commission’s (NYC TLC) Yellow cabs are facing increased competition from app-based car services such as Ola, Uber, Didi, Lyft and Grab which is rapidly eating away its revenue and market share. Research work: In response to this, the study proposes to do profitability profiling of the taxi trips to focus on various key aspects that generate more revenue in future, visualization to assess the departure and arrival counts of the trips in various locations based on time of the day to maintain demand and supply equilibrium and also build a dynamic price prediction model to balance both margins as well as conversion rates. Methodology/Techniques used: The NYC TLC yellow taxi trip data is analysed through a cross-industry standard process for data mining (CRISP-DM) methodology. Firstly, the taxi trips are grouped into two profitability segments according to the fare amount, trip duration and trip distance by applying K means clustering. Secondly, spatiotemporal data analysis is carried to assess the demand for taxi trips at various locations at various times of the day. Thirdly, multiple linear regression, decision tree, and random forest models are adopted for dynamic price prediction. The findings of the study are as follows, high profitable segments are characterized by airport pickup and drop trips, Count of trip arrivals to airports are more compared to departures from airports at any time of the day, and further analysis revealed that drivers making only a few numbers of airport trips can earn more revenue compared to making more number of trips in local destinations. Compared to multiple linear regression and decision tree, the random forest regression model is considered to be most reliable for dynamic pricing prediction with an accuracy of 91%. Application of research work: The practical implication of the study is the deployment of a dynamic pricing model that can increase the revenue of the NYC TLC cabs along with balancing margin and conversion rates.",International Journal of Innovative Technology and Exploring Engineering,2020.0,10.35940/ijitee.e2669.039520,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
286efa99665ed3b1092652a168c8dab87fccc323,https://www.semanticscholar.org/paper/286efa99665ed3b1092652a168c8dab87fccc323,"Digital Initiatives, Infrastructures and Data Ecosystems in the Maritime Sector","
 Efficiency, performance and monitoring of vessels becomes of paramount importance around the globe. Assets security, vessels efficiency, new directives and legislation with regard to emissions quality and many others, urge the global maritime industry to take the right initiatives and make the appropriate investments to develop data ecosystems, that over time, if used intelligently, coherently and consistently, will allow owners and managers to reap tangible benefits such as, among others, significant cost savings, better vessel management and longer vessel life span. As of today, most shipowners and related stakeholders face huge challenges when it comes to data collection, processing, streaming, sharing and storage. Relevant data, if any, is isolated in distinct silos, in spurious and inconsistent formats with little or non-existent interconnectivity between such silos or storage mechanisms. In effect, to face the new challenging landscape, a fresh mindset and an open-minded approach is required. The paper uses data and relevant building blocks, related to vessel performance, assets tracking, route planning, engine monitoring, fuel consumptions, emissions quality, vessels tracking, performance alarms and notifications; that is a wide variety of data modules and reporting tools, that eventually serve pure reporting, real-time monitoring and visualization objectives; but also some additional, more powerful modules being used for analytics and strategic decision making. Such modules can leverage on historical data being captured over prolonged time periods, in the various interrelated data sources and by the relevant data collectors and, if deployed effectively, to construct supervised, unsupervised or even semi-supervised machine learning models. Eventually, such models will enable the various stakeholders in this domain, to achieve successful assignments related to predictions, regressions, classification and clustering. In effect, apart from pure vessel geolocation tracking capabilities, the above modules and tools will allow any shipowner to log-in and see how, a specific vessel under consideration does, in terms of performance and efficiency, in specific weather, geological and regional conditions. In addition to that, more advanced modules, for instance, might warn ship owners about the benefits of a potential hull maintenance or cleaning, give insights on engine efficiency and recommend actions or even provide indications or predictions of future likely delays in reaching at the port of destination. Among other things, this data collection and storage, in such a digitalization platform (will) allows the ongoing building-up of insights, knowledge and technical expertise associated to (optimized) vessels performance and all related functionalities as stated elsewhere. As the variety, veracity, volume and quality of the collected data, across the board, will be gradually enriched, enhanced and improved over time, allowing relevant stakeholders to gain real benefits, such as potentially reduced costs triggered by better and proactive vessels management, from such initiatives that might look and sound meaningless at the very beginning. The Paper builds upon the empirical evidence and relevant data associated to Tototheo Maritime’s, Digital Control Room and its associated Maritime Digitalization Platform that do provide, not only a state-of-the-art platform that facilitates visualization and snap-shot reporting functionalities but also modules upon which machine
",Conference Proceedings of ICMET Oman,2019.0,10.24868/icmet.oman.2019.017,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
90d3d4c720e61ead0c199afb97ad919c1ac2d2c2,https://www.semanticscholar.org/paper/90d3d4c720e61ead0c199afb97ad919c1ac2d2c2,Industry Dives into UK’s New Oil and Gas Data Hub,"Publication by the UK Oil and Gas Authority (OGA) of 130 terabytes of the country’s oil and gas data has captured the imaginations of the upstream industry both domestically and abroad.
 Launched 20 February, OGA’s National Data Repository (NDR) started off with 100 industry users and has since grown to 1,270 along with another 1,760 public users. Data consumers have signed up from 94 countries, with around half of active users from outside the UK. In April, users downloaded some 97,540 well data files, 1,913 2D seismic files, and 559 3D seismic files.
 In the right hands, the dataset is intended to provide insight into the evolution and future potential of exploration and production (E&P) on the UK Continental Shelf (UKCS) through interpretation of public well, geophysical, field, and infrastructure data—spanning from the birth of the North Sea as a petroleum province to the present.
 At least one operator is already taking the opportunity to use the database to search for prospective acreage. The NDR “adds momentum to Talon Petroleum’s push into the North Sea,” said Matthew Worner, director at West Perth, Australia-based Talon, as he took to Twitter following the repository’s unveiling. The company’s purchase this year of “specialist UK explorer” EnCounter Oil has it “well positioned to use the data from this release to identify exploration targets in future licensing rounds,” he said.
 Worner’s comments are an encouraging sign for the OGA, whose purpose for the release is to help renew interest in exploration and development and stimulate innovation on the UKCS, said Nic Granger, director of corporate at the OGA. “In terms of who we’re targeting, we’re looking to encourage inward investment into the UKCS,” she told JPT.
 Important, Granger noted, is that the data are “being opened up for the first time to organizations that aren’t operators,” namely those involved in the supply chain and technology innovators experimenting with machine learning and artificial intelligence.
 The OGA is further promoting collaboration on technology development with the May launch of its UKCS technology portal. It allows operators to share lessons learned on technology deployment and facilitates engagement with the supply chain and tech firms to address development opportunities.
 Data-Sharing Emphasis
 Information sharing—namely data sharing—is one of the OGA’s top priorities. Since it was established in 2015, the OGA has made available seismic data, digital well logs, geological mapping, and various kinds of reports from the field.
 The regulator has a portfolio of prospects that goes back decades thanks to early data-collection efforts by the British Geological Survey, said Nick Richardson, head of exploration and new ventures at the OGA, during a recent exploration-focused OGA podcast. “We’re going to try and make as much of that information as openly accessible as possible so that companies can look at it and derive new insights,” he said.",Journal of Petroleum Technology,2019.0,10.2118/0719-0031-JPT,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
e6c9199db9d2e8546547451ebcdeeb9ea1e8f461,https://www.semanticscholar.org/paper/e6c9199db9d2e8546547451ebcdeeb9ea1e8f461,Special issue on smart cities and its applications,"Smart cities are integrated with information and communication technologies (ICTs) to improve both the quality of public services and the welfare of citizens in the world including South Korea [1]. The smart city migrated into the digital cyber-enabled world with the integration of ICTs, including various city services such as energy, surveillance, transportation, commerce, and so forth [2]. The cyber-enabled world can be better prepared to interact with its citizens and create multiple business opportunities for smart cities in the context of a virtual environment [3]. To better support growing urbanization, the cyberenabled world can increase operational efficiency and share information about city decisions, processes, and infrastructure by realizing vertical markets. A vertical market represents an urban economy by selling products or services in which a specific industry or group of companies use similar means for development. For example, insurance, real estate, banking, home sharing, and heavy industry could refer to vertical markets. Vertical markets reduce costs and resource consumption from an economic viewpoint while combining the new generation of information technology, such as the Internet of Things (IoT) and cloud computing [4]. In fact, smart-city construction can bring transformation to the global economic development model and advance the market structure. The evolution and rapid deployment of vertical markets generates massive amounts of data at an unprecedented rate. Unfortunately, most data are wasted without exploiting potential knowledge because no mechanism to extract useful information exists. Additionally, the highly dynamic nature of urban life requires flexible and adaptable approaches that can cope with the dynamics of data to perform analytics and learn from real-time data. With emerging cyber-intelligence technologies, including big data, machine learning (ML), and artificial intelligence (AI), vertical markets move us closer to developing new smart-city applications that deliver complete adaptive data management solutions [5]. Smart-city applications can learn environment change conditions from a wide set of data, gain experience in context supported by big data analytics, and perform autonomic actions without human intervention. Here, we highlight several challenges, as well as promising future research directions, for incorporating ML and high-level intelligence into smart cities and their applications. When we think about a smart city and its applications, we consider how city resources are managed through cyber-intelligence technologies. Development of smart-city applications must address concerns for optimal provisioning and efficient utilization of city resources. In addition, providing smart-city applications for transportation, healthcare, convenience, agriculture, and government are the main premises of a smart city. Electronics and Telecommunications Research Institute (ETRI) Journal is a peer-reviewed open access journal launched in 1993 and published bimonthly by ETRI, Republic of Korea, aiming to promote worldwide academic exchange in the field of information, telecommunications, and electronics. The objective of this special issue is to focus on all aspects and future research directions in the rapidly progressing subject of smart cities and their applications. In particular, this special issue is dedicated to highlighting recent outstanding results for adopting cyber-intelligence and cognitive computing technologies in smart cities and their applications. We received a total of 15 manuscripts worldwide for this special issue and, after a rigorous review process, only 6 manuscripts have been published in this special issue. A short review regarding commitments for this special issue follow. Smart cities use a variety of IoT technologies and databases to improve efficiency and efficacy of city services. One of the main research areas in smart cities is how to combine and utilize data collected from various smart agents, such as surveillance cameras. The first paper [6] “Multimodal layer surveillance map based on anomaly detection using multi-agents for smart city security” by Shin and others deals with anomaly detection for both human behaviors and vehicle maneuvers, which lead to higher security in city life. This paper utilized a surveillance map database to train and test a model to detect anomalies. The proposed model showed good DOI: 10.4218/etr2.12467",ETRI Journal,2022.0,10.4218/etr2.12467,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
c62e33d4b9896f0f90aa8af743254baf5f3dd8aa,https://www.semanticscholar.org/paper/c62e33d4b9896f0f90aa8af743254baf5f3dd8aa,Towards Explainable Social Agent Authoring tools: A case study on FAtiMA-Toolkit,"Disclaimer: At the moment this article is currently awaiting submission review. Since this process usually takes a lot of time, feel free to use it this version as a reference until it is published. Thank you for reading, if you have any feedback please send us a message. Enjoy. The deployment of Socially Intelligent Agents (SIAs) in learning environments has proven to have several advantages in different areas of application. Social Agent Authoring Tools allow scenario designers to create tailored experiences with high control over SIAs behaviour, however, on the flip side, this comes at a cost as the complexity of the scenarios and its authoring can become overbearing. In this paper we introduce the concept of Explainable Social Agent Authoring Tools with the goal of analysing if authoring tools for social agents are understandable and interpretable. To this end we examine whether an authoring tool, FAtiMA-Toolkit, is understandable and its authoring steps interpretable, from the point-of-view of the author. We conducted two user studies to quantitatively assess the Interpretability, Comprehensibility and Transparency of FAtiMA-toolkit from the perspective of a scenario designer. One of the key findings is the fact that FAtiMA-Toolkit’s conceptual model is, in general, understandable, however the emotional-based concepts were not as easily understood and used by the authors. Although there are some positive aspects regarding the explainability of FAtiMA-Toolkit, there is still progress to be made to achieve a fully explainable social agent authoring tool. We provide a set of key concepts and possible solutions that can guide developers to build such tools. SectionionIntroduction Socially Intelligent Agents (SIAs) have an ever increasing range of applications from conversational interfaces on websites to tutors or teammates in educational environments [1, 2], where they are equipped with tools to conduct human-like interactions. Amongst the most promising applications of SIAs are serious games and social skills training environments. In these virtual environments SIAs behaviours can range from reactive wandering in the background of a scenario to complex social interactions that provide social support or assist the player in some skill training [3]. These autonomous agents sense the environment and act intelligently and independently from the user, allowing them to train and adapt specific verbal and nonverbal behaviors in socially challenging situations [4]. ar X iv :2 20 6. 03 36 0v 1 [ cs .M A ] 7 J un 2 02 2 A PREPRINT JUNE 8, 2022 Agent-based frameworks allow to simulate agent’s cognitive and affective processes [5, 6, 7, 8], in such environments, and can produce intelligent and emotional behaviour, in an unbounded number of situations. Yet, it is up to the author of a scenario – typically instructors, therapists, or researchers – to manually describe how individual traits, goals, beliefs and actions interact, and guarantee character adaptability and consistency as events unfold. This includes defining a plot, writing rules of behaviour, creating dialogues, keeping track of the possible outcomes, among other things. While this can be manageable in narrow domains of application, for a serious game or social skills training content designer, using an intelligent agent framework can quickly become an overbearing task. As a response to those difficulties, data-driven approaches to automatic content and agent creation have become very attractive and are pursued widely in academic research and industry [9]. Yet, these approaches require large datasets tailored to the domain1 and offer little control to the scenario designer. The main advantage of agent-based authoring tools is that it allows a scenario designer to have high control over content creation and target specific learning needs. Although previous research survey a set of challenges associated with the complexity and accessibility of the these tools [10, 11], we argue that tools scaffolded by understandable meta-models and metaphors will empower scenario designers and make them trust that they can easily create complex SIA interactions. Additionally, the way the author and the tool communicate should help the designer understand what comes next in the authoring workflow, and help them achieve their authoring goals. We refer to tools governed by these principles as Explainable Social Agent Authoring Tools. Explainability of social agent authoring tools not only is the extent to which a strong conceptual model can be easily understandable by authors, but also the extent to which a set of mechanisms can help the author, through interaction, explain the cause and effect of their actions. [12] explored this view of explanation-as-interaction making a parallel with Intelligent Tutoring System (ITS) design. Their view supports that XAI2 methods should drop the assumption that providing an explanation consists is summarizing a complex process in a few lines of text or simply showing a graphic illustrating the reasoning process of an algorithm3. They advocate that the tools should promote understanding by interaction, by applying similar methods to those applied in ITS research: highlight important concepts, integration of fragmentary knowledge, reflect on previous experience, etc. We argue that understanding how an AI framework works requires encoding knowledge explicitly in a framework of knowledge (meta-model) and that will allow to create context-aware authoring experiences that promote communication between the system and the user4. That implies design for explainability [13]. The contributions of this paper are fourfold. First, we introduce the concept of Explainable Social Agent Authoring Tools and we frame it within explainable AI (XAI) literature. Second, we examine whether an authoring tool, based on theoretical concepts is understandable and its authoring steps interpretable. Although there is an established assumption that theory-based architectures are understandable, we investigate if this is true from the point-of-the-view of the user, which has not been done before. We present data from two user studies, where we investigate whether FAtiMA-toolkit, which is grounded on Dennett’s Intentional Stance [14], is understandable. Third, data shows that while some artefacts are understandable other underlying concepts, namely emotions, that are not as easily understood. Furthermore, users choose between design long interactions or create scenario ramifications, due to the resulting complexity. Finally, we draw from the data and present a set of suggestions for leveraging machine learning approaches to drive explanation-asinteraction in agent-based authoring tools. It is our stance, that an interactive hybrid approach to authoring is necessary to make it a smooth process that users recognize as trustworthy. SectionionApproaches to Create Socially Intelligent Agents The design rational behind Socially Intelligent Agents (SIAs) represent decades of work across different fields such as Social Sciences, Cognitive Science and Human Computer Interaction [15]. Currently, there are two main approaches used to create SIAs: theory-driven and data-driven, both come with advantages and disadvantages. The former is a top-down approach that consists in developing computational models grounded on theoretical principles from the social sciences literature. The latter is bottom-up approach where the behaviour of the agents is generated/created from a large collection of examples of humans reacting and acting in different contexts. A less active line of research is the use of ontologies to support the decision-making of SIAs. Yet, they have gained a new momentum with newly datadriven models capable of making commonsense inferences about entities and events [16]. In this section, we describe characteristics of the tools under these two umbrella term and we provide a critical analysis these two approaches reflect transparency, complexity and interpretability the core concepts of an understandable tool (refer to Section 0.4 for more details). Furthermore, we also analyse the systems in terms of control and auditability as these concepts are central to social agents authoring tools. As a result, they struggle in generating content for open-ended worlds Explainable Artificial Intelligence These methods are usually referred as post-hoc explainability techniques in the literature in the form of authoring assistant, for instance.",ArXiv,2022.0,10.48550/arXiv.2206.03360,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
a17b7080c281af230ade8bd680ba0c43e6d70ef1,https://www.semanticscholar.org/paper/a17b7080c281af230ade8bd680ba0c43e6d70ef1,Modelling the instrumental value of software requirements,"Numerous studies have concluded that roughly half of all implemented software requirements are never or rarely used in practice, and that failure to realise expected benefits is a major cause of software project failure. This thesis presents an exploration of these concepts, claims, and causes. It evaluates the literature s proposed solutions to them, and then presents a unified framework that covers additional concerns not previously considered.

The value of a requirement is assessed often during the requirements engineering (RE) process, e.g., in requirement prioritisation, release planning, and trade-off analysis. In order to support these activities, and hence to support the decisions that lead to the aforementioned waste, this thesis proposes a framework built on the modelling languages of Goal Oriented Requirements Engineering (GORE), and on the principles of Value Based Software Engineering (VBSE). 

The framework guides the elicitation of a requirement s value using philosophy and business theory, and aims to quantitatively model chains of instrumental value that are expected to be generated for a system s stakeholders by a proposed software capability. The framework enriches the description of the individual links comprising these chains with descriptions of probabilistic degrees of causation, non-linear dose-response and utility functions, and credibility and confidence. A software tool to support the framework s implementation is presented, employing novel features such as automated visualisation, and information retrieval and machine learning (recommendation system) techniques. These software capabilities provide more than just usability improvements to the framework. For example, they enable visual comprehension of the implications of what-if? questions, and enable re-use of previous models in order to suggest modifications to a project s requirements set, and reduce uncertainty in its value propositions.

Two case studies in real-world industry contexts are presented, which explore the problem and the viability of the proposed framework for alleviating it. The thesis research questions are answered by various methods, including practitioner surveys, interviews, expert opinion, real-world examples and proofs of concept, as well as less-common methods such as natural language processing analysis of real requirements specifications (e.g., using TF-IDF to measure the proportion of software requirement traceability links that do not describe the requirement s value or problem-to-be-solved).

The thesis found that in general, there is a disconnect between the state of best practice as proposed by the literature, and current industry practice in requirements engineering. The surveyed practitioners supported the notion that the aforementioned value realisation problems do exist in current practice, that they would be treatable by better requirements engineering practice, and that this thesis proposed framework would be useful and usable in projects whose complexity warrants the overhead of requirements modelling (e.g., for projects with many stakeholders, competing desires, or having high costs of deploying incorrect increments of software functionality).",,2015.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
8f549b350c34235b90b244ef1a9fffa6e00c3128,https://www.semanticscholar.org/paper/8f549b350c34235b90b244ef1a9fffa6e00c3128,Team ADOPNET Advanced Technologies for Operated Networks,"networks as signalling related to mobility management represents a more and more important part of the total tra(cid:30)c. It is also useful for the analysis of the performance of complex link layer protocols in radio networks. our solution dynamically adapts the selected path across time avoiding overexploited e(cid:30)cient links as well as low throughput link usage. This adaptation is performed considering each link state and the amount of channel information available. This improves the throughput and delay with only small marginal overhead cost. Our proposal applies to all wireless multihop networks, with increased bene(cid:28)t for extending cell coverage. We demonstrate through our simulation study that our solution raises the system capacity by more than 50 percents in several scenarii as well as reduces packet delays compared to state-of-the-art protocols such as Ad-hoc On-demand Distance Vector (AODV), Optimized Link State Routing (OLSR) and Link State Opportunistic Routing (LSOR). Abstract: Arti(cid:28)cial Intelligence and Machine learning have been successfully applied to various domains. This success suggests that these techniques could be successfully applied in the context of wireless networks to improve the overall performance and e(cid:30)ciency. AI4Green is built around the need to build compre-hensive, sophisticated and energy-e(cid:30)cient algorithms and solutions at both radio access and core networks, but also on data centres and storage while keeping in mind the emergence of new architectures and the development of smart grids. Abstract: The participants in the ""Beyond 5G"" program will work together for three years to design technical solutions for the development of sovereign and secure next-generation 5G networks, while developing innovative uses for the industry of the future. The project goes far beyond a simple technical improvement by paving the way for a wide range of industrial uses based on new cognitive, predictive and contextual capabilities in order to provide an unprecedented experience. The project teams will also focus on post-5G developments, which will be driven by the introduction of disruptive technologies with severe constraints in terms of digital security. Abstract: The main objective of SPIDER is to develop an Autonomous cyber defense against Internet threats for 5G private networks. In order to reach this aim, 4 main drivers will be considered: The Abstract: The Maya project deals with the optimisation of meshed wireless networks when there is no infrastructure (natural disaster, desert zone). The approach is to use learning methods when possible. Abstract: The WEC-UP project proposes to build optimized, cooperative and coordinated Networks and Edge Clouds for verticals. The architecture enables the network to evolve into an E2E Cloud Native infrastructure that integrates 5G NR RAN, 5G Core network and EDGE computing. The project The goal is to study propose Abstract: The objective of the thesis is to analyze the performance of unicast and multicast/broadcast modes and to study how to combine them in a really hybrid mode in order to maximize the quality of service while limiting the radio resource that is used. Abstract: Even with dense base station deployments, public transport users often have a low quality for mobile services. Due to the insulation of the vehicle, passengers experience little to no connectivity on their end devices and low data bit rate. The objective of the project is to propose a mobile relay architecture for LTE and to study how it can be adapted and optimized for 5G.",,2020.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
981d1f317673c1ae3c0942e73207c85909e6490c,https://www.semanticscholar.org/paper/981d1f317673c1ae3c0942e73207c85909e6490c,Bringing RPA to the next level using AI,"Introduction: Artificial Perspicacity (AI) is transmuting the digital landscape in every field it reaches. The Robotic Process Automation (RPA) revolution goes hand in hand with the advances that Artificial Astuteness is making to transform industries ecumenical. Ultimately, Artificial Perspicacity augments RPA and its implements to surpass prospects. With this already established, it’s valid to verbally express that the world is gearing up for the robotic revolution. Albeit we are already experiencing many of its applications, there’s still plenty of potentials to tap into. What is RPA precisely? RPA or Robotic Process Automation is software technology, as dictated by business logic and structured inputs, aimed to program applications or robots to perform rudimental tasks, just like humans would, in an automated setting. RPA bots can mimic virtually any human action, emulating and integrating actions with digital systems to execute a business process. Robotic Process Automation (RPA) enables organizations to engender virtual workforces that increase process efficiency, truncate errors, and cut operational costs. However, even when enterprises implement RPA, many are challenged with scaling it across the organization and identifying all the potential business processes that can and should be automated for maximum ROI. This presentation will discuss how companies overcome these challenges utilizing AI powered process revelation. Automation has been transmuting the nature of work for over a decennium. It has superseded labour intensive tasks, undertaken perpetual ones, escalated the haste of engenderment and engendered incipient streams of work. Most organizations are already on RPA journey, which has resulted in productivity amendment, cost savings, process time amelioration with perpetual and rule-predicated processes. The commencement of the peregrination involves POCs, pilots, and initial automations. The next frontier is about scaling the  
deployment and adoption of cognitive technologies such as AI, analytics, machine learning that emulate human comportment. The transition involves transformation from running RPA on a few processes to scaling up RPA across the enterprise. This session would deal with strategies & challenges in enterprise wide adoption of Automation - crossing over from RPA to RPA+Cognitive , identifying the right operating model & establishing governance, Leadership & aptitude development , orchestrating stakeholders and board level endorsement. According to Investopedia, Robotic Process Automation is the “software that can be facilely programmed to do rudimentary tasks across applications just as human workers do.” Consequentiality of Robotic Process Automation According to insights developed by McKinsey&Company, RPA offers the potential ROI of 30-200% in the first year of avail alone. This staggering figure is met with the verbal expression made by Leslie Willcocks that “RPA takes the robot out of the human.” Companies and employees are taking notice, which is why everyone is so agog to invest in robotics and its emerging technologies. In essence, RPA is consequential because it is transforming the way businesses operate by availing automate perpetual tasks that are a component of a quotidian routine with a higher degree of efficiency than if performed by a human. Akin to cognitive automation, chatbots, and artificial perspicacity, RPA performs significantly more expeditious and more cost-efficaciously than human resources. Many fear that RPA implements and technologies can be perilous as they take jobs out of human hands. But you shouldn’t authentically worry as there is more to gain than lose when it comes to RPA. To put it into further context, implementing Robotic Process Automation into your workplace can avail with tasks such as monitoring customer activity to discover opportunities to upsell, monitoring client comportment to identify areas of opportunity, truncating cycle times in perpetual tasks to gain competitive advantage, capturing and analyzing bulks of information to provide more expeditious replication times, and more. In a wide range of industries that span healthcare, indemnification, finance, and more, there are many cumbersomely hefty-hitters that have already adopted RPA implements into their processes, including Wal-Mart, Ernst & Puerile, Walgreens, American Express, and more. These early-adopters have benefitted from minimizing staffing costs and human errors with the implementation of RPA technology.",,2020.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
6066ff4dd81f26710b654c4a28a3c246d724aba5,https://www.semanticscholar.org/paper/6066ff4dd81f26710b654c4a28a3c246d724aba5,"The Complex Challenge of Repairing the Gantry Steelwork on the First Generation Magnox Storage Pond at Sellafield: Legacy Waste Storage, First Generation Magnox Storage Pond","This paper puts into context the challenges that were faced when repairing the Gantry Steelwork of the First Generation Magnox Storage Pond (FGMSP). The First Generation Magnox Fuel Storage Pond (FGMSP) provided fuel storage and decanning capability from the early 1960’s until 1986. A significant programme of work has been underway since the completion of operational activities to support the programmes strategic intent of retrieving and storing all legacy wastes, and remediating the structure of the plant to support decommissioning activities. A key enabler to the retrievals programme is the Pond Skip Handler Machine (SHM), removed from service in 2002 following the discovery of significant signs of corrosion and distress, an inevitable consequence of being located in a coastal, salt laden environment. The SHM provides sole capability to access and retrieve the inventory of over 1000 fuel skips. It is also fundamental to future operations and the deployment of desludging equipment to recover significant bulk sludge’s from the pond floor. Failure of the SHM steelwork gantry at worst case could potentially result in the Skip Handler Machine being derailed. This has some potential to damage to the pond structure and at worst case may result in local radiological and environmental consequences. This paper will examine the challenges faced by the team as they successfully defined, planned and executed remedial work to a specific aspect of the civil structure, the SHM gantry rail system, using a purpose built refurbishment platform; the Gantry Refurbishment System. The paper will examine how an “innovative” approach was adopted to resolve the related issues of: • Refurbishing an aged structure to meet extended future operational demands. • The application of pragmatic engineering solutions against current codes and standards including seismic performance. • Provision of safe access for the workforce to undertake the refurbishment work against significant radiological and conventional safety constraints. • The use of off site test facilities to prove work methods. • Engagement of the multiple workforces including supply chain. • Development of challenging safety cases and management control arrangements to undertake the work. The paper will detail the arrangements established to engage all stakeholder groups aligned to a common goal, programme and end position, together with the arrangements put in place for managing the activities of delivery teams, operators and emergency response teams engaged in task execution over the five year period. Finally, the paper will also share the learning from the projects completion, so that the improvement opportunities flowing from this approach can be shared across the industry for the future benefit of all.Copyright © 2011 by ASME",,2011.0,10.1115/ICEM2011-59133,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
28943e4f8380fc989b2fd6067c9b366f4cdd48cc,https://www.semanticscholar.org/paper/28943e4f8380fc989b2fd6067c9b366f4cdd48cc,Key Aggregation Cryptosystem and Double Encryption Method for Cloud-Based Intelligent Machine Learning Techniques-Based Health Monitoring Systems,"Cloud technology is a business strategy that aims to provide the necessary material to customers depending on their needs. Individuals and cloud businesses alike have embraced the cloud storage service, which has become the most widely used service. The industries outsource their data to cloud storage space to relieve themselves of the load of dealing with redundant data contents. This must be protected to prevent the theft of personal belongings, and privacy must be improved as well. Different research projects have been suggested to ensure the safe management of the information included within the data content. The security of current research projects, on the contrary, still needs improvement. As a result, this method has been suggested to address the security concerns associated with cloud computing. The primary goal of this study effort is to offer a safe environment for cloud users while also increasing the profit of cloud resource providers by managing and securely delivering data contents to the cloud users. The bulk of sectors, including business, finance, military, and healthcare industry, do not store data in cloud-based storage systems. This technique is used to attract these kinds of customers. Increasing public acceptance, medical researchers are drawn to cloud computing because it allows them to store their study material in a centralized location and distribute and access it in a more flexible manner. They were collected from numerous individuals who were being evaluated for medical care at the time. Scalable and enhanced key aggregate cryptosystem is a protected data protection method that provides highly effective security in the healthcare industry. When parties interested in a dispute disagree on the outflow of sensitive information, this technique manages the disputes and ensures the data security deployment of a cloud-based intelligent health monitoring system for the parties involved. The encrypted data structure of medical and healthcare prescriptions is recorded as they move through the hands of patients and healthcare facilities, according to the technique recommended. The double encryption approach is used in order to raise the overall degree of security. An encryption class is created by referring to the Ciphertext ID during the encryption procedure. The keyholder is a master secret key that facilitates in the recovery of the secret keys of various monsters and creatures by acting as a conduit between them. It is transferred and stored as a single aggregate for the benefit of the patient or customer in order to make decryption more convenient and efficient. A safe connection between cloud-based intelligent health monitoring systems and healthcare organizations and their patients may be established via the use of a key aggregation cryptosystem and a double encryption approach, according to the researchers. Because of this, when compared to earlier techniques, the findings reveal that the research methodology provides high levels of security in terms of confidentiality and integrity, in addition to excellent scalability.",Computational intelligence and neuroscience,2022.0,10.1155/2022/3767912,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
231080b60ab52521b52308df165a307723956511,https://www.semanticscholar.org/paper/231080b60ab52521b52308df165a307723956511,Review evolution of cellular manufacturing system’s approaches: Human resource planning method,"32 Fig. 1. A Flow Diagram of worker assignment in a Cellular Manufacturing System 1.1 Optimum Number of Workers Perhaps, finding the optimal number of workers is the main idea of investigating HRM in CMS. To determine optimal number of operators and part assignment, Park and Lee (1995) developed a 2-stage model while in first stage, a Taguchi method was used to determine system performance which was then used as objective function of assigning model. The idea of maximizing saving costs between operation and outsourcing costs was investigated by Heady (1997). But their model did not investigate operator level, training, hiring and firing costs. Norman et al. (2002) proposed a model to assign workers in manufacturing cells in order to maximize the system profit. Ertay and Ruan (2005) developed the idea of determining number of operators for maximizing number of outputs. For this purpose, using weighted input data, a data envelopment analysis (DEA) was applied. But in the proposed model, the same skill for all operators and machines was considered. 1.2 Promoting and Assigning Skilled Workers Since in real industries, operator’s skill are not same, so their outputs will not be the same. The idea of considering operator levels was investigated by Suer and Cedeño (1996). For this purpose, a mixed integer programming method was used to generate alternative operator levels and then another integer programming is employed to find the optimal operator assignments to the cells. Askin and Huang (1997) used integer programming for assigning workers to cells in order to determine a training program for employees. Aryanezhad et al. (2009) considered 3 skill levels for workers, which can be promoted through the planning horizon by training. Then a multi-period scheduling model was developed for simultaneous cell forming and worker assignning. Jannes et al. (2005) focused on assiginings workers to team works with the aims of minimizing training and assigning costs as well as maximizing labor flexibility. In the same year, Fitzpatrick and Askin (2005) argued that elemens of a good team formation is not limited to personnal skills and characteristics but technological and human interactions. Hence, by using pre-determined skill level measures, they tried to select workers and assign them to appropriate teams in cells to maximize team performance. Cesaní and Steudel (2005) focused on some factors on deployment of labors. Then, they focused on work sharing, work balancing and leveling the operator assignments (in presence of bottleneck operations). To prevent overloading and over-assigning of operators, Satoglu and Suresh (2009) used goal programming in a mathematical model where the objectives were minimizing over assignment of workers, cross training, hiring and firing costs. 1.3 Cross-trained workers Note that cross-trained workers are refered to those workers that are trained to perofrm more than one task. Determinining best sets of crosstraining workers can improve system performance with more flexibility. Bartholdi and Eisenstein (1996) found that by using large work cells with multiple workstations and workers, a stable partition and assignment of work will spontaneously emerge that cause balance production lines and maximize the production rate. Kleiner et al. (1998) assumed a typical skilled workers, which can perform multi tasks with multifunctional machines, in a a computer based system. Other attributes of the proposed model were included cell lead time, part travel distance, A. Delgoshaei et al. / Journal of Project Management 4 (2019) 33 process yield, operator classification and labor efficiency. In continue, Gel et al. (2000) showed that cross-trained workers can achieve higher performance than normal workers. As a different point of view, Askin and Huang (2001) studied the performance of greedy, beam search, and simulated annealing for a multi-objective optimization model for the formation of worker teams and a cross-training plan for cellular manufacturing. Olorunniwo and Udo (2002) showed that top management role and employee cross-trained have significant impact on the successful implementation of CMS. Kher (2000b) focused on training schemes that obtained by using cross-trained workers under learning, relearning, and attrition conditions. The idea of distributing skilled workers within teams and the degree of workforce belongs to Molleman and Slomp (1999) where they indicated the mentioned items have significant impact on system performance. Their findings showed that a uniform distribution of workforce skill resulted better system performance and consequently each worker should master the same number of tasks. Later, Slomp and Molleman (2000) compared four cross-training policies based on the workload of the bottleneck worker in both static and dynamic circumstances. The results confirmed that better team performance can be expected by using higher levels of cross-training workers. Jensen (2000) involved with staffing level and shop layouts in departmental, strictly and hybrid cell layouts. By changing number of employees in each department and considering 3 levels of workload balance and 2 labor transferring rules, they evaluated flow time, mean of tardiness and square mean of job tardiness. Li et al. (2012) focused on minimizing average salary while maximizing average of satisfaction. For this purpose they developed a multi-objective mixed integer programming to determine number of cross-trained labors and also tasks that must be assigned to the labors in flexible assembly cell layout. Another contribution of their research was considering worker’s satisfaction and task redundancy levels. 1.4 Dual Resource Problems Dual constraint resource problems refers the problems where scheduling parts on machines and workers simultaneously. Kher (2000a) has investigated training schemes obtained by cross-trained workers under learning, relearning, and attrition conditions. Kher et al. (1999) further conclude that the effectiveness of cross-training depends significantly on the existing forgetting rate of the workers. In addition, they remarked on the significant relationship between batch size and worker flexibility cross-training include variability, labor interaction, resources utilization and transition efficiency. Molleman & Slomp (1999)indicate that the distribution of skill within teams and the degree of workforce multi-functionality have a significant impact on system performance. Their findings indicate that a uniform workforce skill distribution resulted in better system performance. In other words, each worker should master the same number of tasks. Xu et al. (2011) provided a novel research in dual resource systems. Hamedi et al. (2012) developed a model where parts, machines and workers are grouped and assigned to the generated virtual cells simultaneously. In continue, the developed model is solved through a multi-objective Tabu Search algorithm to find near optimum solutions. 1.5 Uncertain Market Demands The idea considering dynamic part demands in HRM-CMS which can cause system imbalance is less developed. To solve this problem, Mahdavi et al. (2010) developed an a multi-mode planning model for assigning workers to cells in a reconfigurable CMS. In the proposed model, hiring, firing and also salary costs were considered as a part of total system costs. Another contribution of their model was considering available time for workers. As described in pervious section, Mahdavi et al. (2012) focused on inter-cellular movements of workers and parts while processing on specific machine. Min and Shin (1993) considered the skilled human resource as a part of cell forming process. Their objective was finding machine operators with similar expertise and skills to produce similar part families. Black and Schroer (1993) investigated a case where multi-functional operators can walk within cells to complete operations. They reported that using portable work stations can increase the output rate. Morris and Tersine (1994) examined the impact of labor and equipment in a dual constraint resource planning to compare the process layouts and cell layouts. Hyer et al. (1999) carried out a filed study considering 8 human factors in cell systems to find the importance of different human factors may influence the CMS. As a result they concluded that communication and tem work ranked as the most important factors in 34 utilizing the cell systems.Cesaní and Steudel (2005) developed a 2 phase frame work for worker assignment in CMS based on human resource factors. In the first phase, they performed an empirical investigation to find important factors that affect the labor flexibility. In second phase they used these factors to find optimum worker assignment in cells. The contribution of their research is finding balance between the operators’ workload, the level and type of machine sharing to increase the performance of cell based systems. Chakravorty and Hales (2004) provided a case study to survey the impact of worker assignment on system performance in a manufacturer and supplier of residential and light commercial building products. Afterward, Chakravorty and Hales (2008) reported that during early stage of working after forming cells, both technical failures and human resource errors are existed. However, after spending a period although the technical problems may reduce but the human resource problems are still exists which must be managed to reduce the harms. Yu et al. (2014) focused on minimizing total labor hour while maximizing throughput time of products in a line-cell conversion problem. They found that implementing the proposed method can increase the workforce motivation. Jannes Slomp et al. (2005) proposed a new method which considered labor grouping as well as machine part grouping during the cell forming process. The contribution of their research is focusing on balanced loads for workers, minimization of inter-ce",Journal of Project Management,2019.0,10.5267/j.jpm.2018.7.001,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
8d18aa4b23b4f9124b9eb46427eeb9929a63fa05,https://www.semanticscholar.org/paper/8d18aa4b23b4f9124b9eb46427eeb9929a63fa05,IEEE Access Special Section Editorial: Mobile Multimedia for Healthcare,"With the availability of easy access to the Internet, and the proliferation of various mobile devices, people can get access to mobile multimedia ubiquitously in order to have a desktop experience. Mobile multimedia refers to different types of multimedia content (e.g., text, images, audios, and videos), which are accessed via heterogeneous mobile devices, such as mobile phones, portable devices, and smartphones. Mobile multimedia services and applications have tremendous potential in the healthcare sector. Currently, as the healthcare sector is increasingly dependent on mobile multimedia services and applications, many challenges remain unsolved for the successful deployment of mobile multimedia services in order to have a balance between power-limited mobile devices and resource hungry medical multimedia content. This Special Section aims to theme innovative research achievements in the field of related techniques, applications, services, and systems for mobile multimedia healthcare. The articles in this Special Section bring together academic and industrial researchers to identify and discuss technical challenges and recent results related to mobile multimedia for healthcare. As an application area, mobile multimedia is one of the most active and successful fields of use in the domain of the healthcare industry. The Special Section focuses on particular aspects of mobile multimedia for healthcare, such as comprehensive data sensing, assistive sensory media, and sharing mobile-healthcare big data. After a rigorous review process, we accepted the following articles to form the Special Section. In the article titled, ‘‘DEEP-SEE FACE: A mobile face recognition system dedicated to visually impaired people,’’ by Mocanu et al., the authors introduce a novel assistive device designed to improve cognition, interaction, and communication of visually impaired (VI) people in social encounters. The proposed approach jointly exploits computer vision algorithms and deep convolutional neural networks in order to detect, track, and recognize, in real time, various people’s existence in video streams. The major contribution of the article concerns a global, fixed-size face representation that takes into account various video frames while remaining independent of the length of the image sequence. For this purpose, the authors introduce an effective weight adaptation scheme that can determine the relevance assigned to each face instance, depending on the frame degree of motion/ camera blur, scale variation, and compression artifacts. The experimental results carried out on a large-scale data set validate the proposed methodology with an average accuracy and recognition rates superior to 92%.When tested in real-life indoor/outdoor scenarios, the proposed framework proves to be effective and easy to use, allowing the VI people to access visual information during social events. In the article, ‘‘Voice disorder identification by using machine learning techniques,’’ by Verde et al., the authors introduce an exhaustive comparison between the most used machine learning algorithms existing in the literature. The aim of this study is to identify the techniques capable of discriminating between pathological and healthy voices with more accuracy. This is fundamental to realizing a valid and precise mobile health system and a promising solution for people who desire the detection, monitoring, and treatment of their health conditions anywhere and at any time. All analyses are performed on a dataset of voices selected from the freely available Saarbrucken Voice Database. They show that the best accuracy in voice disease detection is achieved by the support vector machine algorithm or the decision tree one, depending on the features evaluated by using opportune feature selection methods. The article ‘‘Performance analysis of personal cloud storage services for mobile multimedia health record management,’’ by Akter et al., focuses on performance analysis of personal cloud storage services for mobile multimedia personal health record (PHR) management. In particular, the authors investigated using qualitative and quantitative analyses of the strengths and weaknesses of personal cloud storage services for PHR management scenarios. The qualitative analysis includes chunking, bundling, deduplication, delta-encoding, and data compression features. The quantitative analysis includes control data overhead, application data exchanged, and impact of data size on number of packets, as well as transmission rate, synchronization initialization time, and protocol overhead. Experimental results on various benchmark cloud storage datasets showed satisfactory outcomes. Wearable technology and personal health devices (PHDs) play a critical role in shaping the future of healthcare. The International Organization for Standardization (ISO) realized the role of personal health systems and proposed the 11 073 family of standards. To this end, the article ‘‘ISO/IEEE 11 073 personal health device (X73-PHD) standards",IEEE Access,2020.0,10.1109/access.2020.3017119,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
fa240ca00c5ce39af54062b5385577dfea9bcc8d,https://www.semanticscholar.org/paper/fa240ca00c5ce39af54062b5385577dfea9bcc8d,Service science – the trend and the future core,"With the increasing emphasis on the service sector in global business, the rise of the new scholarly domain of service science became inevitable. This innovative research domain appears to be a promising discipline in the modern business environment. Although, the term service science was issued by IBM a mere 6 years ago, it rapidly took off as the Council on Competitiveness published the Palmisano Report in that same year, forecasting that services would become the motor of economic development in the years to come. Service science applies a wealth of scientific methods from various disciplines, including management science, cognitive science, social science, computer science, economics, and engineering. As the global service-based economy and its research fields expand, the services, the vehicle of future business growth, will and should be the focus of business-related fields. This special issue comprises papers from the INFORMS 2010 Service Science Conference, organized by the National Taiwan University of Science and Technology, and held on 7–10 July 2010, in Taipei, Taiwan. From 78 submissions, 19 papers having high evaluation scores were selected, and the authors were invited to submit an extended version of the conference paper for further review. After review and revision, seven papers were accepted for publication in this special issue. These papers describe a cross-section of practical application experiences and state-of-the-art research work using service science methodologies across a variety of industrial domains in several countries. Accordingly, outstanding and insightful results are reported in these papers. Recently, commerce-related subjects have stimulated the broader use of computer science along with other state-of-the-art technologies. Any method for analysis derived from the principles gains superiority over others. Accordingly, Mehravaran and Logendran proposed mathematical programming models for the bicriteria unrelated parallel machine scheduling problem with sequence-dependent setup times in a supply chain. Moreover, the authors also built a search algorithm to find optimal and near optimal solutions for the problem, which has been shown to be NP-hard. Their work indicates that their algorithm is capable of producing high quality solutions within a reasonable computation time. Owing to the dramatic expansion of computer science and Internet-based technologies, the management of renewable resources such as human resources may be estimated by IT systems as part of an IT infrastructure library. Consequently, the twin objectives of better fulfilling customer needs and maximizing the efficiency of resources can be achieved. Grabarnik and Shwartz’s paper describes an IT system-based approach for scheduling of requests for services in groups, which minimizes the duration of requests that use partially overlapping sets of resources. This article validates its empirical results by comparing them to existing open source schedulers, illustrating that their approach is superior. The preliminary integrated approach used by Grabarnik and Shwartz has thus created new insights for the discipline. With the proliferation of cutting-edge devices such as computers and laptops, e-learning, also known as online learning, has drawn global interest. Now a major social trend, the rapid evolution in e-learning has led to the use of portable communication devices, such as cell phones, in a new learning style, called mobile learning (m-learning). In their paper concerning m-learning, Crescente and Lee collaborate to examine this up-to-date learning style. The authors establish a theoretical framework for m-learning, and suggest that this field has great promise as an academic discipline. Their research in this rising discipline has laid an innovative foundation for future researchers in associated fields. Another excellent case of applying methods in the service sciences to better examine the service-oriented issues of the local cultural industry (LCI) in Taiwan is the work of Chen. His study in this volume not only emphasizes service quality improvement for LCIs, but also adopts an integrated method of the Malcolm Baldrige National Quality Award, the quality function deployment, along with the balanced scorecard, to meet the needs of creative, systematic, and customeroriented solutions. Since globalization and localization have flourished worldwide in recent years, distinct cultural and historical resources of certain regions have been exploited by a variety of manufacturing and service industries. The tourist industry, often an LCI, is a key industry in many nations. In Chen’s study, I-lan County was used as a reference for LCI in Taiwan. They concluded that",,2011.0,10.1080/10170669.2011.548922,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
3f2bbbe358044faf5b6482308f7678e952b9882a,https://www.semanticscholar.org/paper/3f2bbbe358044faf5b6482308f7678e952b9882a,Federated Learning-Based Explainable Anomaly Detection for Industrial Control Systems,"We are now witnessing the rapid growth of advanced technologies and their application, leading to Smart Manufacturing (SM). The Internet of Things (IoT) is one of the main technologies used to enable smart factories, which is connecting all industrial assets, including machines and control systems, with the information systems and the business processes. Industrial Control Systems of smart IoT-based factories are one of the top industries attacked by numerous threats, especially unknown and novel attacks. As a result, with the distributed structure of plenty of IoT front-end sensing devices in SM, an effectively distributed anomaly detection (AD) architecture for IoT-based ICSs should: achieve high detection performance, train and learn new data patterns in a fast time scale, and have lightweight to be deployed on resource-constrained edge devices. To date, most solutions for anomaly detection have not fulfilled all of these requirements. In addition, the interpretability of why an instance is predicted to be abnormal is hardly concerned. In this paper, we propose the so- called FedeX architecture to address those challenges. The experiments show that FedeX outperforms 14 other existing anomaly detection solutions on all detection metrics with the liquid storage data set. And with Recall of 1 and F1-score of 0.9857, it also outperforms those solutions on the SWAT data set. FedeX is also proven to be fast in terms of training time of about 7.5 minutes and lightweight in terms of hardware requirement with memory consumption of 14%, allowing us to deploy anomaly detection tasks on top of edge computing infrastructure and in real-time. Besides, FedeX is considered as one of the frameworks at the forefront of interpreting the predicted anomalies by using XAI, which enables experts to make quick decisions and trust the model more.",IEEE Access,2022.0,10.1109/access.2022.3173288,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
e31589522b066d9096e67a2c21ef593039041795,https://www.semanticscholar.org/paper/e31589522b066d9096e67a2c21ef593039041795,A Battery Digital Twin framework for Predictive Maintenance and State of Health Estimation of Electric Vehicles,"To maximize the performance of Electric Vehicle (EV) battery, the requirements of the battery management system (BMS) are getting higher and higher, especially in terms of safety, predictive maintenance, and battery life. The on-board BMS cannot store or process large amounts of data during the operation of a vehicle, with poor real-time capability and data utilization rate. To effectively manage the battery, it is vital to build an Off-board digital twin that can mimic the actual battery with more intelligence. The paper proposes the digital twin framework of Li-Ion battery packs for a fleet of vehicles. This work presents the digital model of the battery, data driven models, contextual information, operational data and the cloud-based deployment. To extend the lifetime of the battery and bring more security into the system, an anomaly detection technique is proposed by using a machine intelligence approach that captures the temporal and spatial relationship between various battery parameters. Also, a learning-based prediction technique is proposed to estimate the health status of battery. The paper outlines the design methodology followed, challenges faced, drawbacks and further opportunities involved in developing the framework for creating a battery digital twin. The performance of the system is analysed with NASA prognostic data set and from the vehicle’s plant model with various drive cycles. Introduction A digital twin is a virtual representation of a physical asset. The digital twin technology helps the global industries to develop a digital DNA of their assets; ranging from as small as smartphone, to as large as a city and as complicated as a human, spanning across industries and processes. This cutting-edge technology helps the companies to optimize performance, maintenance and achieve better results. It came into prominence only upon the advent of other technologies like cloud, artificial intelligence, machine learning etc. VDI-Berichte Nr. 2384, 2021 579 https://doi.org/10.51202/9783181023846-579 Generiert durch IP '54.191.176.224', am 07.11.2021, 18:57:40. Das Erstellen und Weitergeben von Kopien dieses PDFs ist nicht zulässig. The electric vehicles (EV) are the future, and the automakers are investing more on to their EV sector nowadays. The key performance component of an EV is its battery. The electric battery is a merger of chemical, thermal, and mechanical processes. The lifetime of these devices depends greatly on the materials used, the system design and the operating conditions. This complexity has therefore made real-world control of battery systems challenging. Timely preparation for future eventualities is a cornerstone for managing batteries in an EV. The battery management system (BMS) of an EV receives considerable amount of data, and processing them is computationally intensive and requires more memory and processing power. Such processing would be difficult to be contained within the on-board systems, and all this data has to be computed elsewhere. With the advent of the Internet of Things and wireless communication in automotive, the information can be stored in the cloud, offering relentless computational power. However, sending the battery data separates it from the physical battery. The battery is still on-board, while the data is offboard. To effectively manage the battery, it is vital to build an off-board digital system that can resemble the actual battery with more intelligence. Due to aging, the parts of a vehicle are invariably at the risk. Not knowing the risk earlier can keep your car out of the road from sometime, the sooner the better. Electric vehicles reduced the output emissions, but the safety side of the batteries are certainly a costlier affair than the internal combustion engine, considering the multiple facets to include such as the chemicals used, the thermal state, mechanical parts, etc. The frequency and cost of regular maintenance service can be reduced if able to diagnose the issues in advance using digital techniques. Such a system identifies the risks and hinders it from becoming an issue, which not only reduces the downtime, but also the repair costs. If the system is able to warn early, component damage can be eliminated to an extent. The common type of batteries used in EV; the Lithium-ion cells are classified as class 9 hazardous materials. The safety risks involved due to thermal hazards alone from the battery perspective opens up a whole new world of risks; let alone the maintenance due to mechanical failure. Predicting a potential risk and subsequent drive to the mechanic ensures your vehicle in mint condition. Hence, this paper proposes the digital twin framework of Li-Ion battery packs for a fleet of vehicles for predictive maintenance. Our contributions in a nutshell:  Developed a digital twin framework of EV batteries for operating and diagnosing a fleet of vehicles VDI-Berichte Nr. 2384, 2021 580 https://doi.org/10.51202/9783181023846-579 Generiert durch IP '54.191.176.224', am 07.11.2021, 18:57:40. Das Erstellen und Weitergeben von Kopien dieses PDFs ist nicht zulässig.  Developed the State of health (SOH) prediction system of battery for the fleet of vehicles using battery digital twin  Developed an intelligent anomaly detection system to analyse the unusual nature of battery behaviour  Developed the digital twin framework and conducted performance evaluation using NASA dataset and data set generated using plant model. Related Works Even though the battery management system of EVs is well discussed in the literature, digital twin-based battery diagnosis is in the primitive stage only. Tanizawa et al. [6] propose a cloud-connected battery management system that continuously connects the batteries to the cloud, manages their state of charge and monitors changes in its characteristics. Koko Friansa et al. [2] proposes a battery monitoring system to monitor the operational and performance of batteries in a small microgrid system. Taesic Kim et al. [5] proposes a cloud-based battery condition monitoring and fault diagnosis platform for largescale lithium-ion battery energy storage systems. Weihan Li et al. [4] proposes a cloud battery management system with online state-of-charge and state-of-health estimates. Billy Wu et al. [1] discusses their perspectives on battery modelling, data-driven approaches and how these elements can be combined in a framework for creating a battery digital twin. Shichun Yang et al. [3] proposes a framework utilizing a cloud architecture for a cloud-based battery management system based on Cyber Hierarchy and Interactional Network (CHAIN) to leverage the use of algorithms that can be used to realize the state-of-X-estimation, thermal management, and other functions of traditional BMS system. The aforementioned works do not delve upon even the scantiest possibilities of irregularity in the battery data. They all assume that the data from the physical asset is in its pristine form. This paper focus on building a digital twin framework and an anomaly detection system by using a machine intelligence approach that captures the temporal and spatial relationship between various battery parameters. The paper also proposes to estimate the State of the health of the EV battery using Long Short-Term Memory (LSTM), a machine learning approach. Proposed Digital Twin Architecture The digital twin architecture of battery system consists of mainly two parts. First part is the physical system of Electric vehicle battery with real-world data and other is the twin of the system implemented on cloud infrastructure. The vehicle will be added to the digital twin VDI-Berichte Nr. 2384, 2021 581 https://doi.org/10.51202/9783181023846-579 Generiert durch IP '54.191.176.224', am 07.11.2021, 18:57:40. Das Erstellen und Weitergeben von Kopien dieses PDFs ist nicht zulässig. eco-system when the driver completes the initial registration process with vehicle information. Successful registration will create an instance of the digital twin in cloud, specific to the vehicle. This way, it is possible to manage multiple vehicles at the same time by creating instances of the digital twin.",ELIV 2021,2021.0,10.51202/9783181023846-579,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
7f99a822d07eb0b809e4b1b9b456925eb9098a1b,https://www.semanticscholar.org/paper/7f99a822d07eb0b809e4b1b9b456925eb9098a1b,The best of both worlds: Highlighting the synergies of combining manual and automatic knowledge organization methods to improve information search and discovery in oil and gas enterprises,"Research suggests organizations across all sectors waste a significant amount of time looking for information and often fail to leverage the information they have. In response, many organizations have deployed some form of enterprise search to improve the ‘findability’ of information. Debates persist as to whether thesauri and manual indexing or automated machine learning techniques should be used to enhance discovery of information. In addition, the extent to which a Knowledge Organization System (KOS) enhances discoveries or indeed blinds us to new ones remains a moot point. The oil and gas industry is used as a case study using a representative organization. Drawing on prior research, a theoretical model is presented which aims to overcome the shortcomings of each approach. This synergistic model could help re-conceptualize the ‘manual’ versus ‘automatic’ debate in many enterprises, accommodating a broader range of information needs. This may enable enterprises to develop more effective information and knowledge management strategies and ease the tension between what is often perceived as mutually exclusive competing approaches. Certain aspects of the theoretical model may be transferable to other industries which is an area for further research. Introduction Business justification Oil and gas exploration seeks to identify and model hydrocarbon resources through geoscientific methods. Exploration wells can cost over $100Million in deep water (Blackman 2012) and typically have a 30% chance of success (Oil and Gas UK 2011). It is therefore critical that all relevant information is included. A review of surveys across all business sectors indicates 24% of a business professional’s time is spent looking for information (Chui et al. 2012, Doane 2010, Outsell 2005, Feldman et al. 2005, Lowe et al. 2004, Adkins 2003, Delphi 2002, Feldman and Sherman 2001). Much lower figures (9-14%) have been reported from observational studies in organizations (Robinson 2010, Majid et al. 2000) and much higher figures (40%) reported in the oil and gas industry (Hills 2014, Chum et al. 2011). A review of surveys indicates that 48% of organizations felt search was unsatisfactory (Norling & Boye 2013, Mindmeter 2011, Doane 2010, Microsoft & Accenture 2010, Feldman 2009, AIIM 2008, Feldman et al. 2005, Tonstad & Bjorge 2003). Executives indicate missed opportunities caused by failing to leverage information effectively in the oil and gas enterprise could represent as much as 22% of annual revenue (Oracle 2012). Acknowledging this significant opportunity cost, Rasmus (2013) proposes the Serendipity Economy, where discovery of information can produce major leaps in value that cannot be predicted. Exploiting and using information to make better decisions and improve performance are the goals for Knowledge Management (KM). Causal factors for enterprise search performance are numerous, including information silos, search expertise, governance and technology issues (White 2012, DeLone and McLean 2002). Data from search logs (Dale 2013, Romero 2013) and from practitioners (Andersen 2012, White 2012), indicate issues exist with enterprise search. One issue is the vocabulary problem where two people will not choose the same name for the same concept 80% of the time (Furnas et al. 1987), causing a mismatch between the search terms used and the information sought. This leads to challenges for enterprise search in finding precise information and recalling all relevant information. Another issue is the minimal use of faceted search and categories which rarely stimulate serendipitous encounters (Cleverley and Burnett 2015a). Despite major investments, dissatisfaction with enterprise search is widespread (White 2014, Norling and Boye 2013). The role and concomitant benefits of thesauri and manual indexing as well as automated machine learning techniques in information discovery is a source of ongoing debate. While this topic is well developed within the literature, it is far from being addressed conclusively. Collins and Porras (1997, pg. 10) describe the decision making process of visionary companies in terms of “the tyranny of the OR, the genius of the AND” when coping with contradictory forces. Is this a philosophy to apply to enterprise information (knowledge organization) with respect to manual and automated methods? Furthermore, Knowledge Organization Systems (KOS) themselves may act to reveal, or conversely obscure information discoveries. Given these issues, there is a need to assess how manual and automated Knowledge Organization (KO) techniques might support information search and discovery. This research therefore reconsiders these issues within the context of the oil and gas industry, with the explicit intention of developing a synergistic model which encompasses the main benefits of each approach into a ‘best of both worlds’ scenario. The following research questions were identified to fulfill the aim of the research, the rationale for their inclusion is presented in the literature review: Q1. To what extent can a thesaurus be enhanced through automated techniques? Q2. What is the value of auto-categorizing content that is already manually classified? Q3. To what extent can manual and automated KOS techniques be combined in a search user interface to stimulate serendipity? The next section reviews the literature with a focus on oil and gas, followed by the methodology. The results are presented with discussion to help the reader better understand the findings and limitations. The paper concludes with the presentation of a theoretical model, areas for further research and implications for theory and practice. Literature Review This section presents a critical review of the academic and practitioners literatures relevant to the research, guided by a conceptual model (Figure 1). It provides a background to the areas under research from both academic and practitioner standpoints, identifies how the literature has informed the research questions, presents gaps in the existing literature and emphasizes how this research addresses those gaps, and highlights areas of input into the theoretical model. Figure 1 – Conceptual model to guide the reader through the literature review. Knowledge Organization (KO) Knowledge Organization (KO) expresses and imposes a particular structure of knowledge (a ‘view of reality’) behind collections of information (Ohly 2012). This reality is socially constructed: what is reality for one group may not be for another (Berger and Luckman 1966). Hjorland (2008, pg. 86) offers a holistic definition of KO, encompassing the broader social division of mental labour, to the narrower intellectual activities, “..such as document description, indexing and classification performed in libraries, databases, archives etc. These activities are done by librarians, archivists, subject specialists as well as by computer algorithms”. Hjorland continues, “Library and Information Science (LIS) is the central discipline of KO in this narrow sense (although seriously challenged by, among other fields, computer science)”. This alludes to the tension that exists between Library and Computer Science. Recent evidence from organizations (Quaadgras and Beath 2011) contradicts the definition made by Hjorland that KO is the preserve of information specialists. Corporate library or information center functions have traditionally focused on the centralized manual indexing of information using KOS, with indexes under their stewardship (Heye 2003). The growth in digital information creation has led to the breakup of these gatekeeping services and the centralized manual indexing model to a more federated model of KO by the masses. Zeeman et al. (2011) found government libraries plan to deploy, “high-end thesaurus and ontology tools.. to work with structured and unstructured data for decision-making research”. This provides evidence of how some corporate librarian skills and services are changing. Classification and categorization can be achieved manually (by creator or mediator) or automatically through supervised/semi-supervised machine learning. The use of the terms classification and categorization have been (and continue to be) used interchangeably by practitioners and can cause conceptual misunderstandings. Simplifying, classification organizes information to mutually exclusive non overlapping classes, whilst categorization is more flexible, recognizing similarities across entities enabling information to be organized into one or more categories (Jacob 2004). Applying this to a ‘typical’ oil and gas document, classification may involve assigning an item to a single Document Type it is a ‘Well Proposal’. Whilst categorization may include assigning the document to be about oil and gas well ‘33/4b5’ and ‘light tight oil’. Classification and categorization typically need an existing set of classes/categories like a taxonomy or authority list, whilst ‘tagging’ is also used to refer to the process of adding terms which may include those from outside controlled vocabularies to emphasize prominence (Hedden 2013). Knowledge Organization Systems (KOS) Hodge (2000, pg.1) defines Knowledge Organization Systems (KOS) as including, “classification and categorization schemes that organize materials at a general level, subject headings... and authority files that control variant versions of key information such as geographic names and personal names. Knowledge organization systems also include highly structured vocabularies, such as thesauri, and less traditional schemes, such as semantic networks and ontologies.” This definition is adopted for the research study, including automatically generated associative thesauri that involve no manual input. Zeng (2008) arranges KOS types in order of increasing sophistication, by both structure and use cases (eliminating ambiguity, controlling synonyms, establishing relationships and presenting properties). A corporate ",,2015.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
7a75884e7b41df3d2136f199de7e5c8565e4eec7,https://www.semanticscholar.org/paper/7a75884e7b41df3d2136f199de7e5c8565e4eec7,The ZSM Story: The Power to Transform,"The ZSM Story: The Power to Transform Nurit Sprecher (Nokia), iNduStry SpecificatioN Group (iSG) ZSM Vice chair The pivotal deployment of 5G and network slicing has triggered the need for a radical change in the way networks and services are managed and orchestrated. Full end-to-end automation of network and service management has become an urgent necessity for delivering services with agility and speed and ensuring the economic sustainability of the very diverse set of services offered by Digital Service Providers. The ultimate automation target is to enable largely autonomous networks which will be driven by high-level policies and rules; these networks will be capable of self-configuration, self-monitoring, self-healing and self-optimization without further human intervention. All this requires a new horizontal and vertical end-to-end architecture framework designed for closed-loop automation and optimized for data-driven machine learning and artificial intelligence algorithms. The ETSI ZSM (Zero-touch network and Service Management) group was formed in December 2017 with the goal to accelerate the definition of the required end-to-end architecture and solutions. A major millstone was reached during the summer of 2019 with the publication of ZSM Requirements (GS ZSM 001), ZSM Reference Architecture (GS ZSM 002) and ZSM Terminology GS ZSM 007). In GS ZSM 001, the ZSM group examined many business-oriented scenarios and the related automation challenges faced by operators and vertical industries. Subsequently, the team specified the architectural, functional and operational requirements for end-to-end network and service automation. The ZSM architectural framework specified in GS ZSM 002 was designed to satisfy these requirements. The resulting cutting-edge architecture is modular, flexible, scalable, extensible and service-based. The specified architecture supports open interfaces as well as model-driven service and resource abstraction. The ZSM management functions with their respective management service capabilities support advanced, data-driven automation based on closed-loop and integration of AI/ML techniques. Work done in organizations such as 3GPP SA5, O-RAN (Open RAN) Alliance, BBF (Broadband Forum), ETSI NFV (Network Functions Virtualisation), IETF (Internet Engineering Task Force), TM Forum, MEF (Metro Ethernet Forum), ONAP (Open Networking Automation Platform), OSM (Open Source MANO), etc. fits nicely into the ZSM architecture and can support the orchestration and automation of end-to-end services. The ZSM architecture provides a common foundation which allows a diverse ecosystem of open-source groups to produce interoperable solutions. The published specifications are publicly available at https:// www.etsi.org/committee/1431-zsm In addition, the ETSI ZSM group has just approved the ZSM landscape report (GR ZSM 004) that surveys activities relating to ZSM that are taking place in other organizations. This work helps us to identify and analyze the existing specifications and solutions (both ETSI and external ones) and to consider whether they can be leveraged in ways that will avoid duplication and maximize synergies. The report on means for automation (GR ZSM 005) has reached the stable state. The report describes different, existing and proven mechanisms or approaches aimed at achieving automation and zero-touch network management while analyzing their implications on the design and specification of the ZSM framework architecture and their utilization as a basis for future ZSM compliant solutions. A comprehensive review of areas with the highest impact for automation was developed, highlighting the vital role of several key means, such as intentbased modelling and orchestration, network governance, transfer-learning to help utilizing knowledge acquired for one task to solve the related ones, coordination among closed-control loops, etc. The ETSI ZSM group is currently working on the specification of solutions and management interfaces for the orchestration and automation of the emerging end-to-end network slicing technology (GS ZSM 003) as well as of the end-to-end, cross-domain service orchestration and automation (GS ZSM 008). Recently, the group started work on specifying solutions to enable closed-loop automation. Specifically, the ISG works on generic enablers for closed-loop automation (GS ZSM 0091), solutions to closed-loop automation use cases (GS ZSM 009-2), and investigation of advanced topics for next generation for next generation closed-loop operations (GR ZSM 0093). In addition, the group has agreed to study security aspects related to the ZSM framework and solutions and identify potential security threats and mitigation options that should be considered by the ZSM specifications to ensure that the automated processes are secured and deliver the intended business outcomes. Security is essential in the establishment of confidence in the automation process. All the draft specifications are available via the ZSM open area (https://docbox.etsi.org/ISG/ZSM/Open/Drafts). The schedule of each of the ZSM work items can be found at the ZSM Work Programme portal (https://portal.etsi.org/ tb.aspx?tbid=862&SubTB=862,863#/50611-work-programme). The ZSM technology addresses a clear market need and has a major global impact, helping to realize the ambitious vision of 5G. In September 2019 the ETSI Director General has granted a two-year extension to the ETSI ZSM group. The ZSM group intends to maintain the overall mindshare and architectural integrity in ZSM (beyond ETSI) and strengthen the collaboration with related organizations to leverage synergies and achieve alignment. The group plans to work constructively with open source communities (e.g. ONAP) to ensure alignment that will allow integrating open-source software components in ZSMbased solutions. We have just embarked on an exciting journey toward the automation transformation that will help operators meet user expectations for service agility and create new business opportunities. End-to-end automation is a “big deal” and represents the industry’s coming years journey. The use of AI/ML will evolve incrementally. Findings from real deployments and operational experience need to be fed into the specification work. The ETSI ZSM group is open for both ETSI members and non-ETSI members. The different players in the value chain are welcome to join the ISG effort, contribute to the development of the specifications and demonstrate Proofs of Concepts (PoCs).",,2020.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
fea46a73703538ddcc5d7e80e787ee189b6a76e5,https://www.semanticscholar.org/paper/fea46a73703538ddcc5d7e80e787ee189b6a76e5,The Composite Materials Manufacturaing HUB - Crowd Sourcing as the Norm,"The Composites Manufacturing HUB puts composites manufacturing simulations in the hands of those who need them to invent new and innovative ways to capture the extraordinary benefits of these high performance products at an acceptable manufactured cost. The HUB provides the user simple browser access to powerful tools that simulate the actual steps and outcome conditions of a complex manufacturing process without the need to download and maintain software in the conventional manner. Learning use of the manufacturing simulation tools will also be accomplished on the HUB in order to allow for continuous learning and growth of the human talent required in composites manufacturing. Need for manufacturing simulation ! ! Simulation in the design of composite structure has developed during the past four decades to a level of sophistication that allows for the successful design of complex integrated structural geometries consisting of multiaxial composite laminates of curvilinear geometry, sandwich construction, adhesive and mechanical joints, as well as, monocot constructions that possess significantly less sub-assemblies over their metallic counterparts. The Boeing 787 Dreamliner is one example of the success that this simulation capability has achieved to date (1). Here the forward fuselage shown in Figure 1 (40-ft. in length and 20-ft. in diameter) is designed and constructed as a single assembly. Simulation of the complex geometry and performance characteristics of this composite structure were enabled through geometric modeling, multi-axial laminate analysis of the material architecture and structural analysis of the forward fuselage structure. Sophisticated computer simulation codes now offer simulation tool sets that address these design issues. Simulation of the manufacturing of composite structure is not at the same level of development as that of design simulation. VISTAGY Inc. (Waltham, Mass.) recently announced the results of its composites engineering benchmarking survey entitled, ""How do your Composite Design Processes Compare to Industry Best Practices?""(2) The results of the study revealed that only 56 percent of the composite design companies surveyed considered themselves knowledgeable in composites manufacturing practices and were able to apply that knowledge during design. This suggests that 44 percent of companies need to enhance their knowledge of the manufacturing process if they are to improve their competitiveness. The process for developing new manufacturing simulation tools remains in its infancy. Unlike design simulation software, the manufacturing of polymer composite materials and structures involves multi-physics phenomena such as the curing reactions of thermoset polymers, melting and solidification of thermoplastic polymers, flow and impregnation of viscous polymers in fibrous preforms and tows, consolidation of fiber preforms, conduction and convective heat transfer, geometric conformation of fiber preforms to curvilinear surfaces, residual deformations due to anisotropy in thermal expansion and tooling-composite thermal interactions. These phenomena span the disciplines of polymer science, rheology, reaction kinetics, fluid mechanics of nonNewtonian liquids, heat and mass transfer, mathematical topology, anisotropic thermoelasticity and viscoelasticity. While multi-physics analysis tools have recently been introduced, their use in composites manufacturing simulation is still quite early. There are commercial tools offer a broad range of physical modeling capabilities to model flow, turbulence, heat transfer, and reactions for industrial applications. There is a strong economic driving force from the automotive industry to accelerate the development of manufacturing design tools and to discover lower cost manufacturing techniques. More recently, specialized simulation tools have been developed to address specific aspects of composites manufacturing. There is a commercial suite of software tools that supports multi-axial laminate definition and generation of flat patterns for sharing design data with the manufacturing floor. This tool creates ply geometry by defining transitions with sequence, drop-off and stagger profiles that automatically populate the CAD model. It can determine variable offset surfaces and solids, including mock-up surfaces for interference checking, mating surfaces that model where two parts join together and tooling surfaces for manufacturing. The tool provides manufacturing details such as splices, darts and tape courses and can develop data such as flat patterns and data to drive automated cutting machines, laser projection systems, fiber placement machines and tape laying machines. Another type of simulation tool, uses finite element (FE) software to simulate large deformations of highly anisotropic materials in the sheet forming process. There is also a commercial tool that simulates the curing and thermal deformations of thermoset polymer composites. Its foundation is a coupled thermochemical-stress-flow model with a dynamic autoclave controller simulation. It is, in essence, a virtual autoclave, equipped with capabilities enabling one to consider the following process parameters: heat transfer/autoclave characteristics, resin cure kinetics, multidirectional laminates/fabrics, honeycomb panels, thermal expansion/resin cure shrinkage and tool-part interaction. These examples illustrate the growing competencies in composites manufacturing simulation, but to provide the most value for the composites industry it is essential that these simulation tools be linked in a manner that provides for the modeling of the complete manufacturing process. Only then can the true economic benefits of composites simulation be realized. Further, access to the current suite of simulation tools is limited to individuals who have access to large scale computing and to organizations who have purchased expensive licenses for the simulation tools. Entrepreneurs who will significantly accelerate the innovation and development of this powerful set of tools, as well as, the composites manufacturing field, are at a severe disadvantage, because the overhead of just one set of commercially available simulation tools is substantial. Composite Materials Manufacturing HUB characteristics and functionality The Composites Manufacturing HUB is a cloudbased cooperative platform that hosts composites manufacturing simulation tools that may be accessed with a web browser from the Internet. The National Science Foundation provided the funding to develop the original HUB concept. There are currently 20 types of HUB organizations using the platform and software. The most successful HUB involves the subject of nanoparticles. To date that HUB boasts 10,000 users worldwide. It has over 350,000 simulations with over 210 engineering tools to simulate important nano phenomena important in nanoelectronics, materials science, thermal science, physics and chemistry. Over 2,500 content items such as tutorials seminars and full classes drive the overall community to over 175,000 users annually. The user community connects students at all levels, research professionals, faculty and industrial users. Tools range from molecular modeling and simulation to photonics. The Composites Manufacturing HUB has adopted the same platform functionality, which allows users to access tools on a server via web browser. Tools hosted on the Composites Manufacturing HUB can range from simple tools that require only small amounts to computational cycles and those that require the power super computing systems. The HUB provides access to the appropriate level of computing power for each tool and user problem. Further, the platform hosts learning tools that teach the underlying principles upon which the tool is based and demonstrate the correct use and limitations of the tool. Examples of tools include simple engineering mechanics formulations and models of heat and mass transfer essential to simulate composites manufacturing processes. Molecular modeling simulation and uncertainty quantification will inform all the manufacturing process simulations to provide guidance from first principles and to account for process variability. The HUB will also provide a forum for evaluation of tool performance by the user community though hosted discussions and rating systems. The HUB community can post “Wish lists” on the HUB for discussion. Tool developers are rewarded for both tool use levels and the development of new tools through funding developed by the HUB. Tools developed and placed on the HUB are subjected to a financial analysis to determine their worth to the HUB and the developer is rewarded accordingly. Specific composites manufacturing processes is the focus of the Composites Manufacturing HUB. While the choice of manufacturing processes is initially limited by the tools currently available, the number of process simulations will be expanded by new tool development during the program. Indeed, it is likely that the available tools on the HUB will be continuously changing as tools are invented, developed and matured. Over time mature tools will likely be migrated to commercial support enterprises. As such, the HUB embraces technology readiness levels (TRL) of TRL 2 through TRL 6 and fosters rapid deployment of manufacturing processes poised for commercialization. The HUB simultaneously embraces technology readiness levels of TRL 2 through TRL 6. At the TRL 6 level, existing simulation software is provided to the user community with the goals of education the user community in tool use and establishing gaps in functionality required for complex composites manufacturing process simulations. The TRL 2 level work is the research necessary to address the scientific foundation of the simulation tools identified to fill the missing gaps. In this way, the Composites Manufacturing HUB provides a “food chain” for development of the comprehensiv",,2011.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
50398d9875266cb2269efa099281119b08e31e79,https://www.semanticscholar.org/paper/50398d9875266cb2269efa099281119b08e31e79,HiTech Insights-Volume 1: Blockchain,"Given the disruptive power of blockchain, a growing number of high tech companies are deploying proofs of concept across different enterprise scenarios. If your enterprise is looking to cash in on this technology, it is important to first determine the merits of each use case and understand the key challenges and solutions available. As the blockchain revolution gains momentum, we discuss the possibilities for the high tech industry through six use cases. Riding the Blockchain Wave for High Tech Deployment of Blockchain: Non-financial Applications are Gaining Traction 1 According to the World Economic Forum, 10% of the world’s GDP might be stored on blockchain by 2027. Although the digital currency bitcoin was developed to disrupt the financial sector, its underlying blockchain technology is now being adopted for a number of non-financial scenarios. The distributed ledger technology ensures accountability, immutability, auditability, and integrity of data, making it ideal for developing such use cases. In the high tech industry, blockchain is relevant for many applications, like supply chain, loyalty and reward programs, digital rights management, audit trail compliance, digital identity, and HR background checks. However, enterprise blockchain is still maturing, and there have been no largescale production level deployments so far. To succeed with enterprise blockchain, organizations require a holistic, well thought out approach. The path to selecting the right architectures and technologies is likely to involve a series of trials, errors, and innovations. Assessing the Applicability of Enterprise Blockchain Before embarking on blockchain initiatives, organizations must first establish whether their use cases merit the use of blockchain. But what criteria should high tech companies use to decide this? Organizations can use blockchain in cases where trust is difficult to establish between parties in a transaction value chain. Transaction value chains involving multiple parties across different regions and varied systems of reconciliation, or those that require immutability and transparency into each change of state are ripe for blockchain disruption. Blockchain is also ideal if organizations need to significantly reduce the settlement time between involved parties, as it helps eliminate intermediate checks. To meet enterprise requirements, blockchain solutions must address high-level concerns related to architectural and implementation challenges, and ensure scalability, security, and interoperability. Some of the key factors to be considered for a successful implementation include: 1) Only private or permissioned blockchain may work in an enterprise setting 2) Open source technologies might be preferable 3) Blockchain solutions should ideally: a. Have no native currency such as bitcoin or ether b. Support public key infrastructure-based identity management and selective disclosures for audit c. Ensure privacy and confidentiality through digital certificates and encryption d. Ensure smart contracts are pluggable to enable easy deployment on other blockchain platforms e. Deploy an efficient and secure, role-based consensus algorithm that is adaptable to evolving business rules TM Blockchain in the Business 4.0 World Compelling Use Cases for the High Tech Industry Let’s look at some blockchain use cases across high tech industry segments such as computer platforms, technology services providers, electronics, and professional services providers. Supply chain: Blockchain when coupled with IoT can dramatically transform today’s supply chain programs. It introduces provenance to the supply chain while increasing the levels of transparency and visibility. OEMs can use blockchain to detect spare parts counterfeiting, as well as eliminate or reduce reconciliation step. It can also allow customers to trace products from source to outlet for environmental or fair-trade concerns. Audit trails: If enterprises standardize certain aspects of accounting on a private blockchain, auditors can automatically verify the important data behind the financial transaction. A consensus algorithm can streamline the manual and time-consuming processes of financial statement reconciliation and attestation. Blockchain opens up the possibility of real-time auditing to save effort and time, helping to move away from the traditional sampling and interviewing process used for data verification. This can help build trust with shareholders, regulators, and customers. Customer loyalty and employee rewards: With blockchain, businesses can reimagine the technology underpinning loyalty programs to increase efficiency and engagement. Loyalty providers can control how and with whom customers use rewards, while providing customers a frictionless experience in accessing and managing benefits across program partners and vendors. Similarly, blockchain can also be leveraged to build an effective employee reward platform. Digital rights management (DRM): Blockchain can help content creators establish information such as who the creator is, while sharing it immutably. It establishes ownership and makes the rights to a particular piece of content clear to everyone in the network. With a blockchainbased DRM solution, users will be required to establish their digital identity before they can open a piece of digital content on any device. For digital assets, the revenue can be directly credited to owners, based on asset usage policy. KYC and AML: Professional services firms together with financial institutions can use digital ledgers to ensure that the clients they work with are not supporting money-laundering activities. However, it is also essential that in designing such a solution, the participating entities have access to only the data that they require. Human resources: To authenticate employment and education records, recruiters, educational institutes, and government agencies can use blockchain solutions. This will in turn increase hiring efficiency and result in the recruitment of candidates with genuine credentials. TM Blockchain in the Business 4.0 World Overcoming the Challenges in Blockchain Implementation Since blockchain was primarily developed for crypto currencies, its adaptation for other industries is emerging slowly. Following are some of the issues that will need to be addressed for successful deployments: a. Scalability: Bitcoin processes seven transactions per second as its protocol restricts the block size to 1MB. However, the storage and computing power requirements for a centralized blockchain with full nodes operationalized in a network can be very high. Organizations can therefore use pruning and efficient consensus protocols or they can limit the data stored on the blockchain to just the essential data sets. For private blockchains, scalability might not be a major challenge as only a limited set of nodes run the transactions. b. Smart contracts: In case of an erroneous text entry, the negative impact can be more damaging than a traditional contract as smart contracts only interpret the literal meaning in the code, not the underlying intent. Enterprises entering into smart contracts should therefore ensure that the code is optimized at the input stage. They also need to be looked at from a regulatory and legal standpoint. c. ERP systems integration: An enterprise scenario will require good integration with ERP, CRM, and other systems. Standard, two-way data exchange between blockchain and existing enterprise systems will help expedite industry adoption. d. Proven reference architectures: Reference architectures need to be created for blockchain integration with the latest technologies across the business and front-end layers, since the underlying technology is still bare metal. As integrations are built with Big Data, IoT, and machine learning systems, reference architectures will mature. Domain specific designs, leveraging Ethereum or Hyperledger stacks, are expected to be adopted (for instance R3 Corda for financial services). e. Lack of pre-defined governance: In the absence of established rules or legal frameworks today, resolution of blockchain centric disputes is a challenge. With regulators working closely with the industry, universally accepted regulations are expected to be introduced for blockchain soon. f. Absence of tools and knowledge repositories: There are limited tools and knowledge repositories for blockchain implementation. Given its high potential and the growing number of investments in blockchain technology, faster evolution is expected with respect to integrated development environments. There are more than 70 platforms and another dozen expected shortly. 2 Smart contract languages such as Go and Solidity are quickly evolving. g. Privacy: Existing blockchains are designed to be anonymous. However, in business, controls and permissions are required, and organizations cannot deal with anonymous partners. Zero Knowledge Proof (ZKP), cryptographic, and segmentation based encryptions will have to be used to enhance privacy. TM Blockchain in the Business 4.0 World h. Lack of standards and interoperability: Blockchain lacks globally approved standards and the path to ISO standardization is long. Open process standards on the other hand, might lack credibility. Interoperability is another issue. Leaders of various blockchain and distributed ledger projects have only recently started working collectively on what could become an interoperable network of services. TM Blockchain in the Business 4.0 World Join the Blockchain Revolution Blockchain technologies offer a radical departure from the current transaction and recordkeeping mechanisms and can serve as a foundation of disruptive digital businesses for both established enterprises 3 and startups. However, it will take some time for the technology to fully mature for largescale enterprise level deployments. Within the next year, we expect blockchain technology to transce",,2018.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
50a35f82d440047222d962356818dac5f648ab20,https://www.semanticscholar.org/paper/50a35f82d440047222d962356818dac5f648ab20,Arti fi cial intelligence in semiconductor industry – Materials to applications,"This issue of “Artificial Intelligence in semiconductor industry – Materials to applications” contains a collection of seven extended papers on how the semiconductor industry benefits from artificial intelligence (AI) adoption and also discusses about the methodologies with which the semiconductor companies may position themselves to capitalize on the growing AI industry. The main theme of this special issue is to develop a smart and intelligent semiconductor industry, which refers to the development of a set of innovative practices and end-to-end solutions to satisfy the emerging storage, memory and specialized computing requirements of semiconductor industries. Specifically, this special issue has solicited the research works that discusses about the applications of machine learning (ML), deep learning and other AI in all the areas of semiconductor demand-forecasting and optimization related to its hardware manufacturing, automation of chip design and verification, operational efficiency in semiconductor fabrication, methods to shorten processing time, automated detection of defects in semiconductor circuits and other such innovative methodologies to fasten piloting, real-time inference and scalable deployment. There are many recent successful applications of AI and ML to solve both the hardware challenges in semiconductor domain. For instance, ML methods can be implemented to avoid time-consuming iterations, expedite yield ramp-up and lower the expenses that are required to sustain yield by reducing faults and out-of-tolerance process steps. They may incorporate the ability to automate the timeconsuming procedures of physical-layout design and verification. The seven papers in this special issue cover the novel algorithms to enhance the chip design. Wherein, the first paper titled “Efficient VLSI architecture for FIR filter design using modified differential evolution ant colony optimization algorithm” discusses about the efficient finite impulse response (FIR) filter architecture in combination with the differential evolution ant colony algorithm with a primary aim to satisfy the economic power utilization and also the specifications in the frequency domain and gain a quick convergence speed. The second paper titled “Optimized DA-reconfigurable FIR filters for software defined radio channelizer applications” describes about the different methods used for reconfigurable finite impulse response (RFIR) filter design. This research work has integrated both RFIR and SDR as a single system and implemented on Artix-7 development board of XC7A100tCSG324 to exploit the advantages in area-delay, power speed products and energy efficiency. Furthermore, the proposed DA-based RFIR filter is validated using Chipscope Pro software tool on Artix-7 FPGA in Xilinx ISE design suite and compared constraint parameters with the existing state-ofart results. The third paper titled “Grid Connected Operation and Performance of Hybrid DG having PV and PEMFC” integrates the distributed generation (DG) and the utility grid to provide reliable and secured power. This research work is more focused on the adequacy and security in the grid-integrated hybrid DG having photovoltaic and proton exchange membrane fuel cell. Furthermore, the simulation of this grid-connected hybrid DG is performed by using MATLAB/Simulink environment. The fourth paper titled “High speed Data Encryption Technique with Optimized Memory Based RSA algorithm for Communications” has proposed the novel RSA algorithm with lookup table, which is an extension to the Chinese remainder algorithm that works better for image and video in terms of time complexity. Moreover, the proposed method shows better performance compared to other standard methods with a minimal processing time. The fifth paper titled “A 10-bit 200 MS/s pipelined ADC with parallel sampling and switched op-amp sharing technique” describes a 10-bit 200-MS/s pipeline analog-to-digital (A/D) converter for optimizing the power in the architectural level by incorporating extensively parallelism and op-amp sharing circuit in the S/H circuit and the component ADCs. The converter achieves better results when compared with the other existing switching converters. The sixth paper titled “Orthogonal Mode Dual Band MIMO Antenna System for 5 G Smartphone Applications Using Characteristic Mode Analysis” develops a novel multi-antenna design approach to obtain efficient and uncorrelated antennas for 5 G mobile applications. With proper excitations for different characteristic modes that remain orthogonal to each other, well-matched MIMO antennas with low coupling and correlation have been achieved. The seventh paper titled “Design and implementation of artificial magnetic conductor surface as decoupling structure in microstrip antenna arrays” describes the technical perspective and recent works on AMC for antenna applications. Also, this research work has proposed novel theoretical aspect, simulation design procedure and the measurement setup used to characterize the AMC unit cell. From the results, it is evident that the employment of AMC as a decoupling structure has solved many issues while overcoming the major limitations in the conventional antenna design. We thank the Emerald publications and the editors for facilitating the publication of extended special issue in Circuit World Journal. We hope that the readers will gain state-of-the-art research knowledge from this special issue. The current issue and full text archive of this journal is available on Emerald Insight at:https://www.emerald.com/insight/0305-6120.htm",,2021.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
91903c0cae572554ff6828366d1461b083c582de,https://www.semanticscholar.org/paper/91903c0cae572554ff6828366d1461b083c582de,"Intelligent problem solving : methodologies and approaches : 13th International Conference on Industrial and Engineering Applications of Artificial Intelligence and Expert Systems, IEA/AIE 2000, New Orleans, Louisiana, USA, June 19-22, 2000 : proceedings","Keynote Presentation.- Multisensor Data Fusion.- Intelligent Agents I.- Implementing Multi-party Agent Conversations.- Agreement and Coalition Formation in Multiagent-Based Virtual Marketplaces.- A Framework for the Development of Cooperative Configuration Agents.- Java-Based Distributed Intelligent Agent Architecture for Building Safety-Critical Tele-Inspection Systems on the Internet.- Artificial Neural Network I.- The Use of AI Methods for Evaluating Condition Dependent Dynamic Models of Vehicle Brake Squeal.- Towards an Estimation Aid for Nuclear Power Plant Refuelling Operations.- Drilling Performance Prediction Using General Regression Neural Networks.- Identifying Significant Parameters for Hall-Heroult Process Using General Regression Neural Networks.- Data Mining I.- Mapping Object-Oriented Systems to Distributed Systems Using Data Mining Techniques.- Scaling the Data Mining Step in Knowledge Discovery Using Oceanographic Data.- Information Management and Process Improvement Using Data Mining Techniques.- Combinatorial Optimization.- Combinatorial Optimization A Comparative Analysis of Search Methods as Applied to Shearographic Fringe Modelling.- Vision Guided Bin Picking and Mounting in a Flexible Assembly Cell.- A Brokering Algorithm for Cost & QoS-Based Winner Determination in Combinatorial Auctions.- An Overview of a Synergetic Combination of Local Search with Evolutionary Learning to Solve Optimization Problems.- Expert Systems I.- Maintenance of KBS's by Domain Experts.- A Simulation-Based Procedure for Expert System Evaluation.- Gas Circulator Design Advisory System: A Web Based Decision Support System for the Nuclear Industry.- Expert Systems and Mathematical Optimization Approaches on Physical Layout Optimization Problems.- Diagnosis I.- Locating Bugs in Java Programs - First Results of the Java Diagnosis Experiments Project.- Application of a Real-Time Expert System for Fault Diagnosis.- Operative Diagnosis Algorithms for Single-Fault in Graph-Based Systems.- On a Model-Based Diagnosis for Synchronous Boolean Network.- DermatExpert: Dermatological Diagnosis through the Internet.- Best Papers.- Aerial Spray Deposition Management Using the Genetic Algorithm.- Dynamic Data Mining.- Information Systems I.- Knowledge-Intensive Gathering and Integration of Statistical Information on European Fisheries.- Using a Semantic Model and XML for Document Annotation.- Understanding Support of Group in Web Collaborative Learning, Based on Divergence among Different Answering Processes.- Fuzzy Logic and Its Applications.- Fuzzy Modeling Approach for Integrated Assessments Using Cultural Theory.- Fuzzy Knowledge-Based System for Performing Conflation in Geographical Information Systems.- Modeling of, and Reasoning with Recurrent Events with Imprecise Durations.- Linguistic Approximation and Semantic Adjustment in the Modeling Process.- A Fuzzy Inference Algorithm for Lithology Analysis in Formation Evaluation.- Intelligent Agents II.- Approximating the 0-1 Multiple Knapsack Problem with Agent Decomposition and Market Negotiation.- Design and Development of Autonomous Intelligence Smart Sensors.- ADDGEO: An Intelligent Agent to Assist Geologist Finding Petroleum in Offshore Lands.- SOMulANT: Organizing Information Using Multiple Agents.- Design.- Inventiveness as Belief Revision and a Heuristic Rule of Inventive Design.- A Decision Support Tool for the Conceptual Design of De-oiling Systems.- ProCon: Decision Support for Resource Management in a Global Production Network.- Intelligent Infrastructure That Support System's Changes.- Diagnosis II.- Using Description Logics for Case-Based Reasoning in Hybrid Diagnosis.- Printer Troubleshooting Using Bayesian Networks.- Using XML and Other Techniques to Enhance Supportability of Diagnostic Expert Systems.- Learning and Diagnosis in Manufacturing Processes through an Executable Bayesian Network.- Expert Systems II.- Solving Large Configuration Problems Efficiently by Clustering the ConBaCon Model.- XProM: A Collaborative Knowledge-Based Project Management Tool.- Building Logistics Networks Using Model-Based Reasoning Techniques.- A Supporting System for Colored Knitting Design.- Machine Learning and Its Applications.- Learning Middle-Game Patterns in Chess: A Case Study.- Meta-classifiers and Selective Superiority.- Logic and Its Applications.- The Formal Specification and Implementation of a Modest First Order Temporal Logic.- Determining Effective Military Decisive Points through Knowledge-Rich Case-Based Reasoning.- A Constraint-Based Approach to Simulate Faults in Telecommunication Networks.- A Least Common Subsumer Operation for an Expressive Description Logic.- Pattern Recognition.- Blob Analysis Using Watershed Transformation.- A Novel Fusion of Holistic and Analytical Paradigms for the Recognition of Handwritten Address Fields.- Pawian - A Parallel Image Recognition System.- An Automatic Configuration System for Handwriting Recognition Problems.- Detection of Circular Object with a High Speed Algorithm.- Artificial Neural Networks II.- Neural Network Based Compensation of Micromachined Accelerometers for Static and Low Frequency Applications.- Improving Peanut Maturity Prediction Using a Hybrid Artificial Neural Network and Fuzzy Inference System.- CIM - The Hybrid Symbolic/Connectionist Rule-Based Inference System.- A Neural Network Document Classifier with Linguistic Feature Selection.- Color Pattern Recognition on the Random Neural Network Model.- Integrating Neural Network and Symbolic Inference for Predictions in Food Extrusion Process.- Natural Language Processing.- Automatic Priority Assignment to E-mail Messages Based on Information Extraction and User's Action History.- Information Extraction for Validation of Software Documentation.- Object Orientation in Natural Language Processing.- Genetic Algorithm.- A Study of Order Based Genetic and Evolutionary Algorithms in Combinatorial Optimization Problems.- Nuclear Power Plant Preventive Maintenance Planning Using Genetic Algorithms.- Progress Report: Improving the Stock Price Forecasting Performance of the Bull Flag Heuristic with Genetic Algorithms and Neural Networks.- Advanced Reservoir Simulation Using Soft Computing.- Information Systems II.- Forest Ecosystem Management via the NED Intelligent Information System.- Friendly Information Retrieval through Adaptive Restructuring of Information Space.- A Smart Pointer Technique for Distributed Spatial Databases.- Distributed Problem Solving.- Deploying the Mobile-Agent Technology in Warehouse Management.- A Lightweight Capability Communication Mechanism.- Model-Based Control for Industrial Processes Using a Virtual Laboratory.- Autonomous Agents for Distributed Problem Solving in Condition Monitoring.- Modeling Issues for Rubber-Sheeting Process in an Object Oriented, Distributed and Parallel Environment.- Intelligent Agents III.- Reasoning and Belief Revision in an Agent for Emergent Process Management.- System Design and Control Framework for an Autonomous Mobile Robot Application on Predefined Ferromagnetic Surfaces.- Intelligent and Self-Adaptive Interface.- Agent Architecture: Using Java Exceptions in a Nonstandard Way and an Object Oriented Approach to Evolution of Intelligence.- Artificial Neural Networks III.- Neural Network Based Machinability Evaluation.- Performance of MGMDH Network on Structural Piecewise System Identification.- Black-Box Identification of the Electromagnetic Torque of Induction Motors: Polynomial and Neural Models.",,2000.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
76e78bc520bdc550e762a5d82bd6aca84bd8c5bb,https://www.semanticscholar.org/paper/76e78bc520bdc550e762a5d82bd6aca84bd8c5bb,Data Driven Methods For Fault Detection And Diagnosis In Chemical Processes Advances In Industrial Control Epdf File,"Guaranteeing a high system performance over a wide operating range is an important issue surrounding the design of automatic control systems with successively increasing complexity. As a key technology in the search for a solution, advanced fault detection and identification (FDI) is receiving considerable attention. This book introduces basic model-based FDI schemes, advanced analysis and design algorithms, and mathematical and control-theoretic tools. This second edition of Model-Based Fault Diagnosis Techniques contains: • new material on fault isolation and identification and alarm management; • extended and revised treatment of systematic threshold determination for systems with both deterministic unknown inputs and stochastic noises; • addition of the continuously-stirred tank heater as a representative process-industrial benchmark; and • enhanced discussion of residual evaluation which now deals with stochastic processes. Model-based Fault Diagnosis Techniques will interest academic researchers working in fault identification and diagnosis and as a text it is suitable for graduate students in a formal university-based course or as a self-study aid for practising engineers working with automatic control or mechatronic systems from backgrounds as diverse as chemical process and power engineering. With pressure increasing to utilise wastes and residues effectively and sustainably, the production of biogas represents one of the most important routes towards reaching national and international renewable energy targets. The biogas handbook: Science, production and applications provides a comprehensive and systematic guide to the development and deployment of biogas supply chains and technology. Following a concise overview of biogas as an energy option, part one explores biomass resources and fundamental science and engineering of biogas production, including feedstock characterisation, storage and pre-treatment, and yield optimisation. Plant design, engineering, process optimisation and digestate utilisation are the focus of part two. Topics considered include the engineering and process control of biogas plants, methane emissions in biogas production, and biogas digestate quality, utilisation and land application. Finally, part three discusses international experience and best practice in biogas utilisation. Biogas cleaning and upgrading to biomethane, biomethane use as transport fuel and the generation of heat and power from biogas for stationery applications are all discussed. The book concludes with a review of market development and biomethane certification schemes. With its distinguished editors and international team of expert contributors, The biogas handbook: Science, production and applications is a practical reference to biogas technology for process engineers, manufacturers, industrial chemists and biochemists, scientists, researchers and academics working in this field. Provides a concise overview of biogas as an energy option Explores biomass resources for production Examines plant design and engineering and process optimisation This book provides a complete picture of several decision support tools for predictive maintenance. These include embedding early anomaly/fault detection, diagnosis and reasoning, remaining useful life prediction (fault prognostics), quality prediction and self-reaction, as well as optimization, control and self-healing techniques. It shows recent applications of these techniques within various types of industrial (production/utilities/equipment/plants/smart devices, etc.) systems addressing several challenges in Industry 4.0 and different tasks dealing with Big Data Streams, Internet of Things, specific infrastructures and tools, high system dynamics and non-stationary environments . Applications discussed include production and manufacturing systems, renewable energy production and management, maritime systems, power plants and turbines, conditioning systems, compressor valves, induction motors, flight simulators, railway infrastructures, mobile robots, cyber security and Internet of Things. The contributors go beyond state of the art by placing a specific focus on dynamic systems, where it is of utmost importance to update system and maintenance models on the fly to maintain their predictive power. In many industrial applications early detection and diagnosis of abnormal behavior of the plant is of great importance. During the last decades, the complexity of process plants has been drastically increased, which imposes great challenges in development of model-based monitoring approaches and it sometimes becomes unrealistic for modern largescale processes. The main objective of Adel Haghani Abandan Sari is to study efficient fault diagnosis techniques for complex industrial systems using process historical data and considering the nonlinear behavior of the process. To this end, different methods are presented to solve the fault diagnosis problem based on the overall behavior of the process and its dynamics. Moreover, a novel technique is proposed for fault isolation and determination of the root-cause of the faults in the system, based on the fault impacts on the process measurements. Reliability Analysis and Asset Management of Engineering Systems explains methods that can be used to evaluate reliability and availability of complex systems, including simulation-based methods. The increasing digitization of mechanical processes driven by Industry 4.0 increases the interaction between machines and monitoring and control systems, leading to increases in system complexity. For those systems the reliability and availability analyses are increasingly challenging, as the interaction between machines has become more complex, and the analysis of the flexibility of the production systems to respond to machinery failure may require advanced simulation techniques. This book fills a gap on how to deal with such complex systems by linking the concepts of systems reliability and asset management, and then making these solutions more accessible to industry by explaining the availability analysis of complex systems based on simulation methods that emphasise Petri nets. Explains how to use a monitoring database to perform important tasks including an update of complex systems reliability Shows how to diagnose probable machinery-based causes of system performance degradation by using a monitoring database and reliability estimates in an integrated way Describes practical techniques for the application of AI and machine learning methods to fault detection and diagnosis problems",,2021.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
8d2436089b6b614d5e4ecef1dd7d7dd6e8707fd5,https://www.semanticscholar.org/paper/8d2436089b6b614d5e4ecef1dd7d7dd6e8707fd5,WOPTIMOP - A Cloud Based Intelligent Method Of Automatically Creating And Delivering Workload Optimized Platform Contents,"UNIX/Linux being the most preferred enterprise operating platform, provides state-of-art compiler and developer tool chain for almost all programming languages. And so far, has greatly helped enterprise application developers to optimize their applications for their specific workloads, through various set of options and tunables. However, the solution is still partial, as only the application is optimized for the workload. The Operating System and other components of the platform, though allows tuning and customization, is common for all. Hence each and every application and workload being characteristically different, cannot benefit much from such operating platforms. Here we propose a unique innovative solution that provides an Operating Platform completely optimized for a specific enterprise application and workload. The solution is in the form of a cloud based Content Delivery Network (CDN) which internally uses machine learning algorithms to intelligently exercise Profile Based Optimization (PBO)[2] and Runtime Architecture Diagnostics (RTADIAG)[1], and then deliver workload optimized operating platform. Problem Statement Performance and security are two of the most critical needs of any enterprise application or service. It is imperative for UNIX/Linux operating platform, being the most preferred in enterprise industry, to provide performance and security to the maximum possible extent. However, performance being a characteristic of each application, cannot be one for all. In other words, each application, for each different workload, demands customized performance tuning. Research and development in the field of compilers and developer tool chain, have solved this customization problem partially, through Profile Based Optimization (PBO)[2] technique. This technique improves application performance for the workload it handles. However, the operating system and other components of the platform as a whole, is not optimized for the application and the workload. The Operating Platform is one and common for all enterprise deployments. For instance, Operating Platforms like HP-UX or Linux are available as one release for all; they are not available in customized/tailor-made forms for different consumers in enterprise market. The invention proposed here solves this problem and provides an intelligent solution for enterprise consumers to get a customized version of the entire Operating Platform that provides performance and security optimized for their application and specific workloads. Our solution The invention described here primarily consists of a specialized Content Delivery Network (CDN) through which subscribed enterprise customers can get the platform specific software optimized for their workload. The CDN internally comprised of Machine Learning (ML) based Profile Guided Optimization (PGO/PBO) [4]. Following three key aspects triggered the invention described: 1. Any enterprise solution or service provider would expect the best price-performance ratio from the hardware where they run their workload 2 Hegde et al.: WOPTIMOP A Cloud Based Intelligent Method Of Automatically Crea Published by Technical Disclosure Commons, 2018 2 2. The workload and its characteristics differ from one enterprise to another. Often, the difference is very significant. 3. Today, almost all enterprise server (hardware or software) makers, provide Operating System and software components, which is common to all -one release across the world. Options to customize and tune are indeed there, but these are again common to all and in reality, results in very minimal gain. The invention here describes a CDN system to provide software components that are extremely tailormade and optimized to specific workload characteristics of each different enterprise. As mentioned above, the ML and PGO engines are the core components involved. As described in the Diagram-1, WOPTIMOP system is designed and constructed as follows: • A secure portal that provides web based tools is given to enterprise customers who would like to subscribe to the WOPTIMOP service. • The request is then received by the WOPTIMOP service provider • The MLIS server, then sends an initial platform with all software components which are instrumented for profiling. • The customer then uses the instrumented platform to run their workload. At the end of the exercise, this produces all data required by the WOPTIMOP system and the data is uploaded back to the WOPTIMOP system from the customer's server. This data does not contain any information or data related to customer. This contains data purely specific to platform provider. This makes it easy for the customer to safely upload the data back to WOPTIMOP system. • The MLIS server, then uses the data produced by instrumented platform, and generates the final platform content that is Profile Guided Optimized. In addition to the PGO, the MLIS server also detects various characteristics of the workload and uses right set of compiler-n-toolchain [2] options to build the optimized platform and software components. This platform is now tailormade to the characteristics of the customer's workload. The platform in its entirety is profile guided optimized. That means, the shared libraries, the executables from various software packages, the kernel [3] itself, all are PGO'ed. • The MLIS server then sends a report of all that has been done to the Labs/Developers of the RnD division of WOPTIMOP system, for review. The MLIS server provides options to the RnD engineers to modify decisions taken by the MLIS server, in which case, the MLIS server rebuilds the platform with RnD inputs too. • Once the final optimized platform is ready, an e-Mail is sent to the customer to notify them about the availability of the ""Workload OPTIMized Operating Platform"". • The customer then downloads the platform and installs them on their production server after testing. This provides the best price-performance ratio for the hardware associated with the platform for the specific workload. • The MLIS server also maintains data on what went wrong and what went good in various cases. The MLIS server continually passes this information through the ML engine to improve itself over a period of time. Some insights into internal algorithm of WOPTIMOP to provide deeper construction details: • As an example, when MLIS detects that the workload is memory intensive, the MLIS uses compiler options to optimize memory accesses, reduce cache penalties, etc. Similarly, if MLIS 3 Defensive Publications Series, Art. 1181 [2018] https://www.tdcommons.org/dpubs_series/1181 3 detects if workload is CPU intensive, it uses compiler options that optimizes loops, uses hardware specific instructions, etc. These are some simple examples of what MLIS does. • The MLIS also uses the invention of RTADIAG [1] to get runtime diagnostics and improve hardto-detect aspects. • Since different customers may have different supported versions of the platform, the MLIS server maintains indexes into code sources corresponding to each released version, so that it can build the platform precisely. • The ML engine uses algorithms like linear regression, logistic regression, support vector machines (SVM), etc. Diagram-1 Development / Workload Optimization Builds and Tests / Live and Production State MLIS Machine Learning Based Intelligent Server – produces PGO software intelligently Product Developers / RnD Labs Engineers ‘WOPTIMOP’ Subscription Customer’s Enterprise Server WOPTIMOP Content Delivery Network/Cloud Portal Provides Secure Web Based Tools 4 Hegde et al.: WOPTIMOP A Cloud Based Intelligent Method Of Automatically Crea Published by Technical Disclosure Commons, 2018 4 Prior Solutions All prior solutions and attempts to customize the operating platform for security and performance optimization of workload, was made through a few operating system kernel flag/tunable and/or configuration settings. Advantages In none of the prior solutions, the code of the operating system or components were optimized specially for a specific workload. All operating platforms were/are available as one global release. There is a significant advantage from invention described here as it enables enterprise application developers to: 1. get a custom operating platform optimized for their specific workload 2. get benefits of machine learning based continuous improvement of the operating platform 3. get security end-to-end and performance optimized for the workload References [1] RTADIAG Runtime Architecture Based Diagnostics [ http://www.tdcommons.org/dpubs_series/491/ ] [12 HP-UX PGO [ https://support.hpe.com/hpsc/doc/public/display?docId=emr_na-c02936806 ] [3] PGO Kernel Performance Improvement [ http://www.usenix.net/legacy/publications/library/proceedings/sf94/full_papers/partridge.pdf ] [4] Profile Guided Optimization [ https://en.wikipedia.org/wiki/Profile-guided_optimization ] Disclosed by: Suprateeka R Hegde – Hewlett Packard Enterprise Sujoy Saraswati – Hewlett Packard Enterprise Amrita H S – Hewlett Packard Enterprise 5 Defensive Publications Series, Art. 1181 [2018] https://www.tdcommons.org/dpubs_series/1181",,2018.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
57b672bbd41fa4fae7b4307191978599513107f4,https://www.semanticscholar.org/paper/57b672bbd41fa4fae7b4307191978599513107f4,Data Engineering Project (Educating For The Future PhUSE WG),"With an expected 100% increase, over the next 3 years, of data from non-EDC sources (such as smartphones, wearables and custom apps) the traditional methods of managing data for clinical trials presents executives with a resourcing headache. As such, many companies are looking for lower cost strategies to sure up this shortfall in resourcing. However, citing case studies from other industries, there are new methodologies/technologies in data engineering which could enable automation of much of the “heavy-lifting” currently practiced in clinical data management and statistical programming. This paper discusses the Data Engineering Project within the PhUSE Computational Science (CS) Working Group, Educating For The Future, with a view to educate clinical data managers in data engineering principles so that they can be prepared, equipped and effective in dealing with the coming “data tsunami” heading to the shores of clinical research. INTRODUCTION Did you realise we are living in the age of the Fourth Industrial Revolution? Perhaps you have been busy downloading a myriad of “apps” designed to make your life easier or connecting on social media, uncovering relationships and associations you didn’t even know you had. Perhaps you have been shopping a global marketplace, comparing prices, quality and availability, all at your fingertips and in a minutes’ time. While this has been happening, the Fourth Industry Revolution has been evolving at exponential proportions ​. Just ask Siri! The term “Industrie 4.0”, was originated in Germany, as a government-led initiative, to transform manufacturing through advanced digital capability. Thus creating the concept of a “smart factory”, based on four key design principles ​: 1. Interconnection of machines, devices, sensor and people 2. Vast amounts of useful information (data) to drive decision making 3. Technical assistance to aid humans, for example to visualise data or to perform tasks that may be of safety concern for a human. 4. The use of cyber-physical systems to make decisions on their own and to perform tasks as autonomously as possible. Emerging from the premise of “Industrie 4.0” is the advent of the term “The Fourth Industrial Revolution” (also referred to as “4IR” or “I4.0”). This term originated in 2016 when described by Klaus Schwab (Founder and Executive Chairman of the World Economic Forum), as a “technological revolution that will fundamentally alter the way we live, work, and relate to one another”. Klaus goes on to describe it as a digital revolution with innovative uses of a combination of technologies that build upon the premise of the third revolution (i.e. electronics and information technology to automate production). As a result, emerging technologies have brought forth advancements in fields such as ​artificial intelligence, robotics, the Internet of Things, autonomous vehicles, 3D printing, nanotechnology, biotechnology, materials science, energy storage, and quantum computing. This rapid evolution will undoubtedly affect industries world-wide, already disrupting many industries, such as travel agencies, video rentals and bookstores​ . The pharmaceutical industry is also experiencing the impacts of I4.0. Digital and mobile technologies has brought on significant advancements in data acquisition and accessibility as it relates to health care and patient data. As reported in the Tufts-Veeva eClinical Landscape study in 2017, data coming from sources such as, smartphones, custom applications, and mobile health are expected to double in the next 3 years ​. Therefore requiring greater capabilities in handling large volumes of data, as well as data from coming in through various data streams and 1 PhUSE EU Connect 2018 formatting. As with other industries, data will become a critical asset to their business and the effective utilisation of this data can play a critical role in driving growth in the business and bringing novel therapies to the patients who need them. In this paper, we will focus on the works of the Data Engineering Project within the Educating For The Future Working Group. With the formation of the Working Group in early 2018, the team had taken on the mission to explore how data engineering techniques, successfully deployed in other industries, could be utilised in the pharmaceutical industry, with a goal to ​facilitate the education ​of the pharmaceutical industry on these techniques. We will share with you some introductory information about Data Engineering and Data Science and explore how embracing new data engineering techniques may affect the industry culture. You will learn about use cases of Data Engineering in other industries and how advances in digital capability have affected their business model. We will also share some of the many software packages and tools available to enable automation, commonly used in Data Engineering and Data Science. Finally we will reflect on the benefits that data standardisation has brought to the pharmaceutical industry and share our vision for disseminating information to facilitate your learning going forward. DATA ENGINEERING To start this learning journey, exploring the term “Data Engineering” opens the door to the vast opportunities and roles available today centered around data. In doing a simple search on the internet, ​“what is data engineering?​”, one will find many posts expressing their understanding of Data Engineering with some variation but also some similarity. However, what is clear is that Data Engineering encompasses the many considerations that need to be taken into account to optimally curate, transform, secure and disseminate data suitable for analysis. As technology and tools have become more advanced, building such a platform and infrastructure requires engineers and architects of both general and specific expertise. The Data Engineer combines knowledge in areas such as software development, infrastructure, data architecture, data warehousing, cloud technology and data cleaning in order to design, build and test solutions that define the pipelines of data throughout the enterprise, making the data accessible to the organisation.​ [5] [27] [31] Optimised Data Engineering appropriately balances the efficiency of an automated process against the cost of development and maintenance of that process, ensuring repetitive processes that require humans to write code, press keys, cut-and-paste and update documents are minimised or eliminated. DATA SCIENCE Often paired with the term “Data Engineering” is also the term “Data Science”. According to Kelle O’Neal and Charles Roe: “Data Science allows enterprises the ability to turn their data assets into a narrative. Data Science allows that narrative to be expanded across timelines, in different data spaces that trace from the past into the future, with much more involved questions and answers about an enterprise, different potential outcomes, and repercussions based on recommendations. Data Science employs a range of mathematical, business, and scientific techniques to solve complex problems about an organisation’s data assets.” ​ In contrast, the focus of the Data Engineer is on the process from data curation to dissemination and the focus of the Data Scientist is on the analytics of the data, thus extracting knowledge from the data. To achieve quality data capture, near-real-time accessibility and meaningful analytics, one cannot function without the other, and effective teamwork optimises the value of each role. As such, an analytics team would be composed of distinct roles/capabilities​ : ● Data Engineers (in areas such as database architecture, database development, machine learning architecture, ETL scripting , etc.) ● Data Scientists ● Business Analysts Data Engineering brings together the broad expertise, of these roles, to ensure the data are curated and accessible to the Data Scientist, and in our environment today, this process is becoming more and more complex. Therefore, 2 PhUSE EU Connect 2018 expertise in curating big-data and data of varying formats (structured and unstructured) is a critical core competency to optimise the potential impact of these digital assets (i.e. the data). The Data Scientist works deep in the data, utilizing various tools and techniques to discover patterns in the data that may drive decision making for the business. Optimising utilisation of the data to enable accurate conclusions can bear greater value to the organisation. As an example, per Tom Eunice’s post, “a fraud-detection algorithm may be very accurate when based on many months of historical data. However, months of historical data may not always be available. Designing a fraud-detection model that is still accurate using historical data from only a few days would be of more use and more practical to implement.” ​ The Business Analyst helps the Data Scientist understand the meaning of the data and the relevance of any discovered relationships. Initially, uncovering relationships in the data and upon further investigation, identifies meaningful patterns that may reveal information that otherwise may not have been known. ​ As you will see in the sections to follow, the full complement of the roles in an analytics team is what drives the business value. One discipline without the other (e.g. data engineering without data science) will result in missed opportunities. In the sections to follow, we often refer to Data Engineering, however, due to the close ties to Data Science, some examples elude to both Data Engineering and Data Science. USE CASES FROM OTHER INDUSTRIES In this section, we present three use cases from the transportation, retail, and agricultural industry. The use cases illustrate the importance and usage of Data Engineering. In each example the data collected, the consumer of the data, and the value of the organisation is reviewed. Similarities and potential applications to the pharmaceutical industry are discussed. UBER When",,2018.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
93c8039993af1efd9d1f320575755084555bf7ab,https://www.semanticscholar.org/paper/93c8039993af1efd9d1f320575755084555bf7ab,Security and privacy solutions in edge envisioned connected things environment,"The evolution of the Internet of Things and thereafter Internet of Systems and now the latest entry, Internet of Skills has transformed the traditional technological backdrop into a data-driven periphery of connected things. The connected things (devices, sensors, machines, actuators, and so on) deployed in the smart cities generate a large amount of data that is transmitted to geo-located sites for further processing or storage. For instance, a huge amount of content is generated via social networking platforms which create an array of a connected society. Even more, the smart meters deployed at each household generate multi-thousand fold data at regular intervals of time which is processed at a central control center. This data is analyzed using sophisticated machine learning algorithms or big data technologies to identify the hidden correlations, patterns, and associations which in turn improves decision making in the smart ecosystems (comprising Industry 4.0, Health 4.0, Intelligent Transportation, and many more). The data generated from these connected things is transmitted at a high data rate and thereafter processed at remote cloud data centers. For the past some time, cloud data centers have completely driven the market of providing big data solutions. But, nowadays, the need for processing data closer to the location of the user has shifted the processing domain closer to the data sets or end-user, that is, at the edge of the network using edge computing technology. This transition has reshaped the data-driven connected ecosystem to Edge Envisioned Connected Things Environment. In traditional systems, a long delay is incurred when an edge device sends the data to the network and thereafter to a data center for processing at the cloud. However, in an edge envisioned environment, the data generated by connected things is sent to the local gateway, that is, edge nodes rather than transmitting it to the cloud for further processing and analysis. In this way, processing the data closer to the physical world from where the data itself originates can provide manifold benefits like ultra-low latency, higher bandwidth, and quick response time. Edge is nothing but a local compute which provides faster data analytics and also reduces the network traffic. In Edge Envisioned Connected Things Environment, most of the devices use web-based management interfaces where a user can install the updates, accomplish routine tasks, and adjust settings. Although theoretically, it is suggested to have unique and strong passwords practically, on average, the end-users employ weak passwords which are quite easy to crack by the attackers. This means that most of the edge-enabled things are protected with poor passwords that can make them easy targets for attackers to access their confidential data or use them as a potential botnet. The devices connected in such an environment are considered to be innocuous and often the data they collect and transmit is not encrypted or authenticated. Although the data from such devices seem harmless it can still be of value for a potential attacker. In another case, allowing an attacker to access your device without any authentication can be harmful as security is acknowledged at a lower priority in the Internet of Things environment and so. Even more, deploying such devices at locations with minimal oversights and assigning them key roles in organizations without taking proposer security precautions can lead to security implications. The connected devices are designed to send potentially sensitive data to the outside world or the locations outside the organization’s management. For this purpose, they often use mobile-friendly interfaces (due to the popularity of mobile computing), but most such devices have no security enforcement. However, these services need continuous identification of associated risk and security which requires proactive efforts by users and corporate security services. This includes the network log analysis to identify anomalous traffic patterns that are generated by anonymous devices within the organization’s network periphery, and thereafter the design and development of alternative security measures or plans for such unidentified devices or things are essential. An attacker can hack thermostats, cameras, baby monitors, smart meters, temperature controllers, and even assault rifles and thereafter create a panic situation or a security nightmare. As there are so many new nodes or things which are connected to the network and Internet as an open channel will also provide malicious and unanticipated actors who can acknowledge the connected environment with diverse attack vectors and possibly try to create an insecure environment or pour out sensitive data (which can be a patient’s health data). Some investigations suggested that everyday devices like, closed-circuit cameras, smart home equipment were hijacked and controlled by the attackers using malware and used against the servers of popular services providers. As we expect that such connected devices would be envisioned in",Trans. Emerg. Telecommun. Technol.,2020.0,10.1002/ett.4208,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
0138f0e45a79d004f5f7c1ff5a7a98c88dbfec80,https://www.semanticscholar.org/paper/0138f0e45a79d004f5f7c1ff5a7a98c88dbfec80,Artificial intelligence and autonomy in space: Balancing risks and benefits for deterrence and escalation control,"An overarching principle accepted by space-faring nations and industry alike is to maintain freedom of operations in a safe and secure environment, commensurate with national and commercial interests. Deterrence concepts and escalation control play key roles in realizing this principle in the increasingly congested, competitive and contested space environment. AI and autonomous machine learning are being pursued as critical enablers in commercial and military programs for space traffic management, routine space operations, space domain awareness (SDA), and space control. AI systems hold the potential to strengthen deterrence by improving both the speed and ability to assess threats and inform decision makers in times of crisis. However, issues that have arisen in terrestrial AI applications will be also present in these applications, with implications for space deterrence and escalation scenarios. Key among these are performance, explainability, and vulnerability. To date there are few if any international standards or regulations to guide best practices for choosing AI methods for space operations and developing a shared understanding of the risks and benefits to strategic stability. This paper explores trade-offs between explainability, performance, and vulnerability in AI methods applied to space control and SDA scenarios, and illustrates how choices on these trade-offs may affect deterrence signaling and escalation control in space. Copyright © 2020 Advanced Maui Optical and Space Surveillance Technologies Conference (AMOS) – www.amostech.com Introduction and Key Research Question Freedom of operations in a safe and secure environment in space, commensurate with national and commercial interests, is a fundamental principle of international policy as well as most space-faring nations. Preventing damage to space assets is tantamount to achieving a safe and secure space environment [6]. However, to date, there is less ability to respond to threats in space than to conventional threats, and countries with developed space assets are perceived to rely heavily on these assets for everything from civil uses to military use. As a result, deterrence concepts and escalation control play key roles in ensuring unimpeded use of space in the increasingly congested, competitive and contested space environment. Artificial Intelligence (AI) and autonomous machine learning are being pursued as critical enablers in commercial and military programs for space traffic management (STM), routine space operations, space domain awareness (SDA), and space control [5][9]. AI is generally defined as methods capable of rational and autonomous reasoning, action or decision making, or adaptation to complex environment, and to previously unseen circumstances [12]. AI systems hold the potential to strengthen deterrence by improving both the speed and ability to assess threats and inform decision makers in times of crisis. However, issues that have arisen in terrestrial AI applications will be also present in these applications, with implications for space deterrence and escalation scenarios. Key among these are performance, explainability, and vulnerability – all of which can vary depending on the AI algorithms, training data, and platforms. For example, deterrence signaling might be misconstrued and responses deemed escalatory if one nation does not fully understand the intentions and strategic goals of the other nation. AI systems could negatively affect signaling by compressing the timescale for making and communicating decisions, or incorrectly classifying observed behaviors. A result could be unintentional conflict escalation. To date there are few if any international standards and/or regulations to guide best practices for choosing AI methods for space operations and developing a shared understanding of the risks and benefits to strategic stability. This paper explores how AI deployed on critical space systems – and the design choices made about the characteristics of the AI methods may impact deterrence signaling and escalation control. Our key research question is, how might tradeoffs between explainability, performance, and vulnerability in AI methods applied to space control and SDA scenarios affect deterrence signaling and escalation control in space? The purpose is to stimulate dialogue on best practices for choosing AI methods for space operations and developing a shared understanding of the risks and benefits to strategic stability. AI in Space Operations: Exemplars and Use Cases Current and potential applications of AI in space operations are many. AI will be essential for managing megaconstellations (tens of thousands) of commercial telecommunications satellites in low Earth orbit (LEO); guiding functions such as scheduling and tasking; collision avoidance; and space debris mitigation. AI is also being explored for classification of observations from LEO constellations proposed to serve national security applications such as persistent overhead coverage and missile defense. Advancements in AI, in combination with increased availability of low-cost and secure cloud storage, have simultaneously led to improvements in SDA capabilities while decreasing costs. As databases grow with an increased number of objects to track and characterize, companies and countries will employ AI to make timely, cost-effective assessments for SDA, while reducing the role of the human-in-the-loop. For this research, we examine two exemplars of AI applications in space, with three different use cases for each (Table 1). The first exemplar involves scenarios for threat detection and response, in three different use cases that correspond to different phases of deterrence and escalation. In the scenarios for Exemplar 1, a national security satellite faces a potential threat that has yet to be realized. The primary functions of AI under these scenarios are to characterize and assess the threat, recommend (and potentially direct) options for defensive measures, and communicate a credible deterrence signal. The second exemplar involves scenarios to ensure resiliency of a meg-constellation. In these scenarios, a mega-constellation of satellites experiences failures of varying degrees. Here, primary functions of AI are to assess damage, attribute the cause, and recommend options for reconstitution and potential retaliation. These exemplars and scenarios are summarized in Table 1, along with functional roles of AI. The scenarios are viewed from the perspective of Country B. Copyright © 2020 Advanced Maui Optical and Space Surveillance Technologies Conference (AMOS) – www.amostech.com Table 1 Exemplars of AI in Space and Use Cases",,2020.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
58dc374e2669cd54638605c336334c9aff818095,https://www.semanticscholar.org/paper/58dc374e2669cd54638605c336334c9aff818095,Selected Papers from the Eleventh ITU Kaleidoscope Academic Conference,"This special section contains two updated papers, originally presented at the eleventh International Telecommunication Union’s (ITU) Kaleidoscope academic conference. The title of the conference was “ICT for Health: Networks, Standards and Innovation,” and took place in the United States, specifi cally in Atlanta, Georgia from 4-6 December 2019. The host was the Georgia Tech Research Institute with the collaboration of the World Health Organization (WHO). There were nearly 100 participants: 70 physically at the venue coming from 16 countries and over 30 on the web. The proceedings are available on the ITU website at https://www.itu.int/pub/T-PROC-KALEI-2018, and from the IEEE Xplore Digital Library. Pictorial highlights from the conference are available at https://www.fl ickr.com/photos/ itupictures/with/49237161532/. The topic of the next conference is “Industry-Driven Digital Transformation.” It was originally scheduled for 7-9 September 2020 in Hanoi, Viet Nam, in conjunction with the ITU Digital World 2020. However, due to the Covid-19 pandemic, it is now an online conference from 7–11 December 2020 running four hours per day to accommodate the various time zones. To access the conference, check its main page at https://www.itu.int/en/ITU-T/academia/kaleidoscope/2020. The ITU Kaleidoscope series of academic conferences started in 2008 to provide an interdisciplinary forum for the discussion of Information and Communication Technologies (ICTs) relevant to future telecommunication standards. Participants typically include researchers, academics, students, engineers, policymakers, regulators as well as futurists. The fi rst article in this issue deals with Machine Learning (ML) and Artifi cial Intelligence (AI) in medicine. The main challenge is that, because of the wide variety of patients and clinical conditions, ML/AI models must produce results that practitioners can rely on even when the algorithms process previously unseen data. Another diffi culty is that these algorithms are black boxes because their exact workings are unknown. Some bioethicists have suggested that applying trust to AI is a corruption of language that can corrupt thought because it is “a category error, mistakenly assuming that AI belongs to a category of things that can be trusted” [1]. This is why international cooperation is indispensable because it allows substantial synergies in the selection of the training and test data sets as well as the validation of the software, from both engineering and clinical viewpoints. This invited article from the Fraunhofer Heinrich Hertz Institute and the Technische Universität Berlin, titled “Toward Global Validation Standards for Health AI,” covers these aspects. The authors, Markus A. Wenzel and Thomas Wiegand, present an overview of the work being carried out under the joint auspices of the ITU-T and WHO to address the use of machine learning and artifi cial intelligence in healthcare, and highlight what has been achieved in terms of guidelines. On the regulatory side, they mention the contributions of the National Health Service in the UK and the International Medical Device Regulators Forum. On the standardization side, they list activities by a variety of organizations such as the U.S. National Institute of Standards and Technology (NIST), the Chinese Electronics Standards Institute, the European Union High-Level Expert Group on AI, the German Deutsches Institut für Normung (DIN), the IEEE, and the International Organization for Standardization (ISO). The second article, “Converged Internet of Lights Network for Telecommunication, Positioning, Illumination and Medical Therapy,” is a joint contribution from several Chinese and British universities and research institutions. The authors are Jian Song, Xiaofei Wang, Jintao Wang, Hongming Zhang, Changyong Pan, Yue Zhang, and John Cosmas. They focus on the spectrum of the visible light from 380nm to 850nm, which is nearly one thousand times broader than the Radio Frequency (RF) spectrum. This is because Light Emitting Diodes (LEDs) can be deployed to modulate visible light for Visible Light Communication (VLC). Accordingly, lighting systems can be designed to combine information services using a network of LEDs integrated with sensors. This would constitute what the authors call the Internet of Lights (IoL). IoL, however, can have both positive and negative impact on human beings (as well as other animals), because of its eff ect on the circadian rhythms and hence body functions. On the positive side, it can be used as a non-intrusive intervention therapy to alleviate degenerative neurological diseases such as SeLecTeD Papers froM The ELeVenTh ITU KaLeiDoscope AcaDeMic Conference",IEEE Commun. Stand. Mag.,2020.0,10.1109/MCOMSTD.2020.9204601,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
cc79c0fa6e39ea58910fe54bb9b966d333d76cb3,https://www.semanticscholar.org/paper/cc79c0fa6e39ea58910fe54bb9b966d333d76cb3,Data Engineering Project (Educating for the Future PhUSE Working Group),"With an expected 100% increase, over the next 3 years, of data from non-EDC sources (such as smartphones, wearables and custom apps) the traditional methods of managing data for clinical trials presents executives with a resourcing headache. As such, many companies are looking for lower cost strategies to mitigate this resourcing challenge. However, citing case studies from other industries, there are new methodologies/technologies in data engineering which could enable automation of much of the “heavy-lifting” currently practiced in clinical data management and statistical programming. This paper discusses the Data Engineering Project within the PhUSE Computational Science (CS) Working Group, Educating for the Future, with a view to educate clinical data managers in data engineering principles so that they can be prepared, equipped and effective in dealing with the coming “data tsunami” heading to the shores of clinical research. INTRODUCTION Did you realise we are living in the age of the Fourth Industrial Revolution? Perhaps you have been busy downloading a myriad of “apps” designed to make your life easier or connecting on social media, uncovering relationships and associations you didn’t even know you had. Perhaps you have been shopping a global marketplace, comparing prices, quality and availability, all at your fingertips and in a minutes’ time. While this has been happening, the Fourth Industry Revolution has been evolving at exponential proportions . Just ask Siri! The term “Industrie 4.0”, was originated in Germany, as a government-led initiative, to transform manufacturing through advanced digital capability. Thus creating the concept of a “smart factory”, based on four key design principles : 1. Interconnection of machines, devices, sensors and people 2. Vast amounts of useful information (data) to drive decision making 3. Technical assistance to aid humans, for example to visualise data or to perform tasks that may be a safety risk for a human. 4. The use of cyber-physical systems to make decisions on their own and to perform tasks as autonomously as possible. Emerging from the premise of “Industrie 4.0” is the advent of the term “The Fourth Industrial Revolution” (also referred to as “4IR” or “I4.0”). This term originated in 2016 when described by Klaus Schwab (Founder and Executive Chairman of the World Economic Forum), as a “technological revolution that will fundamentally alter the way we live, work, and relate to one another”. Klaus goes on to describe it as a digital revolution with innovative uses of a combination of technologies that build upon the premise of the third revolution (i.e. electronics and information technology to automate production). As a result, emerging technologies have brought forth advancements in fields such as artificial intelligence, robotics, the Internet of Things, autonomous vehicles, 3D printing, nanotechnology, biotechnology, materials science, energy storage, and quantum computing. This rapid evolution will undoubtedly affect industries world-wide, already disrupting many industries, such as travel agencies, video rentals and bookstores . The pharmaceutical industry is also experiencing the impacts of I4.0. Digital and mobile technologies have brought on significant advancements in data acquisition and accessibility as it relates to health care and patient data. A recent study, conducted in 2017, by Tufts University in collaboration with Veeva Systems, shows that close to 100% 2 of companies surveyed, estimate utilization of various electronic data sources in clinical studies. As shown in Figure 1. below, companies utilizing e-sources such as, smartphones, custom applications, and mobile health, will more than double in the next 3 years . Therefore, requiring greater capabilities in handling large volumes of data, data from multiple sources and data of varying formats. As with other industries, data will become a critical asset and the effective utilisation of this data can play a key role in driving growth in the business while bringing novel therapies to the patients who need them. source: Tufts – Veeva 2017 EClinical Landscape Study. Tufts University, 2018, pp. 11–13, Tufts – Veeva 2017 EClinical Landscape Study. In this paper, we will focus on the works of the Data Engineering Project within the Educating for the Future Working Group. With the formation of the Working Group in early 2018, the team had taken on the mission to explore how data engineering techniques, successfully deployed in other industries, could be utilised in the pharmaceutical industry, with a goal to facilitate the education of the pharmaceutical industry on these techniques. We will share with you some introductory information about Data Engineering and Data Science and explore how embracing new data engineering techniques may affect the industry culture. You will learn about use cases of Data Engineering in other industries and how advances in digital capability have affected their business model. We will also share some of the many software packages and tools available to enable automation, commonly used in Data Engineering and Data Science. Finally we will reflect on the benefits that data standardisation has brought to the pharmaceutical industry and share our vision for disseminating information to facilitate your learning going forward. DATA ENGINEERING To start this learning journey, exploring the term “Data Engineering” opens the door to the vast opportunities and roles available today with the overarching goal to optimise the use of data in day to day business operations. In doing a simple search on the internet, “what is data engineering?”, one will find many posts expressing their understanding of Data Engineering, with some variation. However, what is clear is that Data Engineering encompasses the many considerations that need to be taken into account to optimally curate, transform, secure and disseminate data suitable for analysis. As technology and tools have become more advanced, building such a platform and infrastructure requires engineers and architects of both general and specific expertise. The Data Engineer combines knowledge in areas such as software development, infrastructure, data architecture, data warehousing, cloud technology and data cleaning in order to design, build and 3 test solutions that define the pipelines of data throughout the enterprise, making the data accessible to the organisation. [5] [27] [31] Optimised Data Engineering appropriately balances the efficiency of an automated process against the cost of development and maintenance of that process, ensuring repetitive processes that require humans to write code, press keys, cut-and-paste and update documents are minimised or eliminated. DATA SCIENCE Often paired with the term “Data Engineering” is also the term “Data Science”. According to Kelle O’Neal and Charles Roe: “Data Science allows enterprises the ability to turn their data assets into a narrative. Data Science allows that narrative to be expanded across timelines, in different data spaces that trace from the past into the future, with much more involved questions and answers about an enterprise, different potential outcomes, and repercussions based on recommendations. Data Science employs a range of mathematical, business, and scientific techniques to solve complex problems about an organisation’s data assets.” [26] In contrast, the focus of the Data Engineer is on the process from data curation to dissemination and the focus of the Data Scientist is on the analytics of the data, thus extracting knowledge from the data. To achieve quality data capture, near-real-time accessibility and meaningful analytics, one cannot function without the other, and effective teamwork optimises the value of each role. As such, an analytics team would be composed of distinct roles/capabilities : ● Data Engineers (in areas such as database architecture, database development, machine learning architecture, ETL scripting, etc.) ● Data Scientists ● Business Analysts Data Engineering brings together the broad expertise, of these roles, to ensure the data are curated and accessible to the Data Scientist, and in our environment today, this process is becoming more and more complex. Therefore, expertise in curating big-data and data of varying formats (structured and unstructured) is a critical core competency to optimise the potential impact of these digital assets (i.e. the data). The Data Scientist works deep in the data, utilizing various tools and techniques to discover patterns in the data that may drive decision making for the business. Optimising utilisation of the data to enable accurate conclusions can bear greater value to the organisation. As an example, per Tom Eunice’s post, “a fraud-detection algorithm may be very accurate when based on many months of historical data. However, months of historical data may not always be available. Designing a fraud-detection model that is still accurate using historical data from only a few days would be of more use and more practical to implement.” [17] The Business Analyst helps the Data Scientist understand the meaning of the data and the relevance of any discovered relationships. Initially, uncovering relationships in the data and upon further investigation, identifies meaningful patterns that may reveal information that otherwise may not have been known. [17] As you will see in the sections to follow, the full complement of the roles in an analytics team is what drives the business value. One discipline without the other (e.g. data engineering without data science) will result in missed opportunities. In the sections to follow, we often refer to Data Engineering, however, due to the close ties to Data Science, some examples elude to both Data Engineering and Data Science. USE CASES FROM OTHER INDUSTRIES In this section, we present three use cases from the transportation, retail, and agricultural industries. T",,2019.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
7e4ad1b1f3a30599fcbcfbe754d53eaa3c99d7d3,https://www.semanticscholar.org/paper/7e4ad1b1f3a30599fcbcfbe754d53eaa3c99d7d3,Desarrollo de una aplicación para reconocimiento y muestra de información de obras de arte: RecoArt,"Hoy en dia, los telefonos moviles son utilizados en la mayoria de nuestras actividades. Desde pedir un taxi o realizar alguna compra en alguna tienda ubicada en el otro lado del mundo, nosotros podemos hacer todo esto de un modo simple con nuestros telefonos moviles. La tecnologia evoluciona, y con ella se mejora tambien las capacidades de los telefonos moviles. Hoy, un telefono movil tiene mas capacidades que un ordenador promedio en la decada pasada; esto permite que las nuevas aplicaciones moviles sean mas potentes y de mayor utilidad para el usuario. Cada ano, los fabricantes de telefonos mejoran sus terminales, anaden nuevas caracteristicas que permiten que los telefonos moviles amplien su capacidad. El estandar de la industria de hoy, por ejemplo, es que los moviles dispongan de doble camara. Este es un estandar reciente, que permite al telefono tener un procesamiento de imagenes mejor, por ende, una calidad de fotos superior inclusive en ambientes con poca luz. Pero no basta que los terminales mejoren su hardware, si estos no vienen acompanados por software que explote ese hardware. Ese software es el llamado “framework” el cual permite a los desarrolladores utilizar ese hardware avanzado para el desarrollo de aplicaciones potentes. El desarrollo de aplicaciones moviles, lejos de lo que fue en sus inicios (cuando se desarrollaba aplicaciones de forma artesanal) hoy en dia es una rama de la ingenieria de software especializada, por lo que debe respetar procedimientos de la ingenieria, pero al ser una rama especializada, se debe utilizar metodologias que vayan acorde a la realidad de esta rama. Las metodologias de desarrollo de software tradicional son incompatibles con el desarrollo de aplicaciones moviles, por lo que para este tipo de software es mejor trabajar con metodologias agiles. El presente proyecto tiene la finalidad de proponer el desarrollo de una aplicacion movil que permita el reconocimiento de obras de arte. Aunque el desarrollo de la aplicacion sea tanto para Android como para iOS, el prototipo de aplicacion sera desarrollado para iOS. Asi se puede explotar los frameworks avanzados que Apple dispone para el reconocimiento de imagenes, machine learning y realidad aumentada, herramientas necesarias para el desarrollo de la aplicacion. Se ha pensado en un mecanismo de uso de la aplicacion orientada a la simplicidad. El usuario solo debera enfocar el cuadro con su telefono movil y la aplicacion sera capaz de reconocer el cuadro y brindarle toda la informacion relacionada de la misma. Adicional, se ha propuesto la mecanica de reconocimiento por medio de fotos, donde el usuario tendra la posibilidad de tomar una foto o seleccionar una foto que tenga almacenada en su movil de la obra de arte a reconocer. Asi mismo, la construccion de esta aplicacion estara dirigida por la metodologia de desarrollo de software agil Mobile-D, la cual nos permite abarcar todo el proceso, desde la toma de requisitos, hasta la fase de pruebas y despliegue de la misma. El presente documento detalla el proceso completo de construccion de la aplicacion de reconocimiento de obras de arte, bautizada con el nombre de RecoArt. 
Abstract: 
Today, mobile phone is used for almost all human activities. To ask for a taxi or do shopping in a store from the other side of the world, we can do all this thing in a simple way with our mobile. Technology evolves and with it, mobile phones capabilities too. Nowadays, a mobile phone has more capabilities than an average computer in the past decade. With this, new apps are more powerful and useful for users. Every year, phone manufacturers improve their terminals, add new features that allows to expand mobile phones capabilities. For example, now mobiles have two posterior cameras as a new industry standard. With this, mobile phones have a better image processing, therefore photos with a better quality even in dark scenarios. But it isn’t enough, if mobiles don’t have software that exploits hardware capabilities. This software is named “framework”, and allows developers use this advanced hardware to develop powerful apps. 
Mobile app development today is a specialized part of software engineering, and some engineering rules should be respected. In this part of software engineering, we must use methodologies accord to mobile app development. Traditional methodologies are incompatible with mobile app development, for this reason, this development works better with Agile methodologies. The aim of this project is proposing the development of an app for painting recognition. Although the development of the application is for both Android and iOS devices, the app prototype was developed for iOS devices. With this prototype, we can exploit advanced frameworks for image recognition, machine learning and augmented reality, developed by apple and necessaries for this app development. The app design focusses on simplicity. User only focusses the mobile on a painting and automatically the app will recognize it and show all the information about it. In addition, the user also can use the recognizer mode by photo. With this mode, the user can take a photo of a painting or select a photo stored in mobile, and the app also will recognize it automatically. In the same way, the development app process will be driven by agile software methodology Mobile- D. With this methodology, we cover all phases of the development process, from requirements analysis to test and deploy phases. This document describes all construction process of painting recognition app, named RecoArt.",,2018.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
e5f7951406318a3bbb76f2ae5358469e49d3a6f7,https://www.semanticscholar.org/paper/e5f7951406318a3bbb76f2ae5358469e49d3a6f7,"The Industry 4.0 Talent Pipeline: A Generational Overview of the Professional Competencies, Motivational Factors, and Behavioral Styles of the Workforce","This research seeks to identify emerging trends, pinpoint challenges and gain data-driven insights into the forces shaping the technical talent pipeline of Industry 4.0 in the United States, specifically Southeast Michigan which has one of the largest concentrations of engineers and technicians in the country. The rapid advance of digital technology has revolutionized engineering and industry. It is dramatically shaping the technical talent landscape. Simultaneously, major cultural changes are being forced by generational transition and leadership succession. To prosper in the Industry 4.0 ecosystem, individuals and organizations will be required to develop 21st century skill sets. The talent pipeline is failing to provide sufficient quantities of workers and calls for stepping up Industry 4.0 reskilling have become ever more urgent. Five themes have emerged: 1. The Workforce Must Embrace Frequent and Constant Change. 2. Teams Must be Flexible, Adaptive and Collaborative. 3. Companies Must Create Cultures of Inclusion and Transparency 4. Workers Must Become Life-long Learners & Dynamic Thinkers 5. The Need for Accelerated Education Workforce Development Reform An empirical investigation, focused on Southeastern Michigan was conducted with the support of local industry, educational institutions and government agencies. Three key segments of upcoming generations, currently advancing in the talent pipeline, were investigated: 1) future technical leaders, 2) future engineers and 3) future tradespeople. Based on responses to a series of questions using the TTI TriMetrix® DNA assessment suite a data-driven, validated assessment instrument, this research presents an overview of the development of 25 professional competencies that contribute to superior performance. Individual motives and behavioral styles are also explored. These findings provide some valuable insights and direction into what educators, industry and policy makers should address to upgrade the technical talent pipeline in the age of Industry 4.0 in order to protect and ensure the Unites States global leadership position. Overview of the Industry 4.0 Talent Landscape The rapid advance of digital technology is revolutionizing engineering and industry. The term “Industry 4.0” is now commonly used in reference to this revolution. Industry 4.0 is a result of the convergence of digital, biological and physical technologies. Industry 4.0 dynamics are dramatically shaping the technical talent landscape. Simultaneously, cultural changes are being driven by generational transition. As the Baby Boomer generation (1946-64) exits the workforce, the Millennial generation (1981-96) and the Generation Z’ers (1997-2012) rise up as the majority of the technical workforce. The combination of technological and cultural change has made the transformation to Industry 4.0 difficult to manage, especially for legacy companies and small and medium sized enterprises. Socio-cultural transformation has become a challenge confronting industry. Knowledge and experience are exiting the workforce in mass through the retirement of the Baby Boomer generation. A shortage of qualified talent has emerged, in part due to the smaller numbers of Generation X’ers (1965-80), who now make up 51% of the management and leadership positions. With an average of 20 years of workplace experience, Generation X’ers are assuming the top executive roles [1]. Socio-cultural transformation is complicated due to each generation’s radically different views on work-life balance. Millennials make up the largest segment of the working population. Not far behind, the Gen Zers are now entering the workforce in entry level positions. Both generations have their own unique attributes. For example, a common attribute of Gen Z is that they tend to be very inclusive in nature and willing to rally around causes. The technologies of Industry 4.0 coupled with generational transition are driving the need to step up workforce development efforts. Industry 4.0 skills have become critical components of labor markets [2]. Developing and retaining an Industry 4.0-ready workforce demands that industry and educators go beyond traditional reskilling and upskilling initiatives. Organizations must focus on career strategies, talent mobility and re-engineering ecosystems to drive organizational reinvention and new business models. Objectives of this Research This research seeks to build on the work of Pistrui and Kleinke et al. [3] [4] and Petrick, and McCreary [5] to identify emerging trends, pinpoint challenges and gain data-driven insights into the forces shaping the technical talent pipeline and Industry 4.0 in the United States, and in particular Southeast Michigan. Research objectives include: 1 Building on previous and ongoing research findings to provide a deeper and more comprehensive understanding of the talent pipeline in United States. 2 Evaluating three key segments of the talent pipeline: 1) next-generation leaders, 2) undergraduate engineering students and 3) skilled trade apprentices. 3 Defining the fundamental DNA of the talent pipeline in terms of the professional competencies, motivational factors and behavioral styles of the three groups. 4 Producing data-driven insights that industry and education collaborators can use to modify and strengthen the talent pipeline in United States. 5 Developing new knowledge and strategies to attract, develop and retain top Industry 4.0 talent in the United States. Emerging Workforce Themes and the Talent Horizon To prosper in the Industry 4.0 ecosystem, individuals and organizations will be required to develop 21st century skill sets. The talent pipeline is failing to provide sufficient quantities of workers and calls for stepping up Industry 4.0 reskilling have become ever more urgent [2]. The factories of the Industry 4.0 digital age are very different from the legacy operations of the automation-age of Industry 3.0. In 2015 alone, nearly 100,000 robots were deployed in automotive factories and a further 65,000 were installed in electronics factories as automation continues to reshape the size of the labor pool [6]. With the advent of Industry 4.0, each robot installation is also a digital installation as the robots are equipped with technologies such as machine learning, cloud computing and big data. The work of the labor pool must evolve. A recent study conducted by the Ralph C. Wilson, Jr. Foundation reports that in Southeast Michigan 30% of the middle skilled labor pool will be displaced by automation by 2030 [7]. There will also continue to be a shortage of workers with the skills industry is seeking. Many people will need to transition from “traditional careers” where they have trained to do specific tasks, (i.e. mechanical engineer) to “multitrack careers” where they will have multiple simultaneous jobs such as engineer, data analyst, and network administrator [4] [8]. To successfully navigate the Industry 4.0 environment (and beyond), organizations will need to integrate four (and soon 5) different generations into their workforce. This will be no easy task given the generational differences coupled with the shortage of qualified talent. Table 1 presents an overview of the emerging themes shaping the workforce environment. Four central themes are having dramatic impact on the Industry 4.0 work environment: Table 1 Emerging Workforce Themes 1. The Workforce Must Embrace Frequent and Constant Change Industry 4.0 is expected to significantly increase the pace of change. Companies need to be aware of the implications of disruption to their workforce [2] [3] [8]. 2. Teams Must be Flexible, Adaptive and Collaborative Team agility, an entrepreneurial mindset and the ability to persist through failure are fundamental to creating and sustaining networks of interrelated teams [4] [5] [23]. 3. Companies Must Create Cultures of Inclusion and Transparency This requires breaking down traditional hierarchies, implementing agile methodologies and embracing the changing nature of work tasks [11] [8] [9]. 4. Workers Must Become Life-long Learners & Dynamic Thinkers Dynamic thinking requires empathy, collaboration, experimentalism and a focus on solving problems and creating value for other humans [2] [3] [8]. Sources: Schwab, 2016; Pistrui and Kleinke, 2019; Arena, 2018; Pistrui and Kleinke, 2018; Brachman, 2018. Insights into 21st Century Industry 4.0 Skills, Mindsets and Cultures of Performance Research conducted by Pistrui et al. (2018; 2019) provides insights into three categories and specific types of skills that industry and educators view as critical for the 21st Century [3] [4]. Their research identified three categories of skills that are important to develop and employ in an Industry 4.0 environment including discerning skills, people skills and purposeful skills (see Table 2). These findings parallel the work of Petrick and McCreary (2019), Bawany (2019), Schwab (2019), and Arena (2018) who all identified similar skill sets, common trends and empirical findings [5] [9] [10] [11]. Table 2 21st Century Industry 4.0 Categories and Skill Types [3] [4] [5] [9] [10] [11] Sources: Pistrui and Kleinke, 2018, 2019 Petrick and McCreary (2019), Bawany (2019), Schwab (2019), Arena (2018). The need to develop discerning skills is a result of the disruption and uncertainties associated with Industry 4.0. This applies to both companies and educational institutions. People need to be able to identify patterns and make new connections in ways never imagined before. Moving forward, they must envision and create new products, efficient services and better user experiences. The research of Petrick and McCreary (2019) included 404 manufacturing companies directly involved in creating and implementing smart technologies. The findings were similar to the work of Pistrui et al., reporting that creativity and innovation, the ability to be forward looking, and having an improvement mindset are all re",,2020.0,10.18260/1-2--35341,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
af3b3032f2d41afa6d8a75c9958a119ed38197dc,https://www.semanticscholar.org/paper/af3b3032f2d41afa6d8a75c9958a119ed38197dc,"Regulatory Compliance? Elementary: With Overwhelming Data and Complex Regulatory Requirements, Banks Are Turning to Advanced Cognitive Computing to Help Their Compliance Teams","Jeopardy! viewers in 2011 witnessed a first in game show history: a computer not only competed in the show, but wiped the floor with its opponents. Watson, a cognitive computer system, left Ken Jennings and Brad Rutter--previously Jeopardy!'s top champions--in its dust over a three-round matchup. IBM built Watson specifically to compete in a trivia show, but the potential applications proved highly commercial. Watson is programmed to understand human language questions; comb through massive amounts of data; and provide intelligible, quick and correct answers. Unlike humans, Watson remembers everything. With every question and answer, Watson gets a little bit smarter. Through machine learning--by which computers gain capabilities without being explicitly programmed for them--Watson can become a technical expert in a variety of fields. In 2013, for example, Watson was deployed for lung cancer treatment decisions at Memorial Sloan-Kettering Cancer Center in New York City, incorporating symptoms and crunching data to recommend treatment approaches with specified levels of confidence. Crucially, Watson did not supplant doctors' decision-making. Instead, Watson became like a very knowledgeable colleague in the exam room, providing insights and making connections the health care providers might not have recognized otherwise. Last year, IBM acquired Promontory Financial Group, a D.C.-based consulting firm led by former regulators and industry veterans, to bring Watson's expertise to bear on financial regulatory compliance challenges. Promontory founder Gene Ludwig, a former comptroller of the currency, was excited by the opportunity to bring Promontory's in-house expertise to banks of all sizes in addition to the larger institutions that often hire the firm. ""For community and regional banks, the promise is that over time they will have access to our thoughts in an efficient and effective manner, and for the largest institutions, they have a new colleague called Watson,"" he says. Promontory is in the process of teaching Watson about regulations and guidance, increasing Watson's ability to understand the compliance questions bankers will ask it. 'Augmented intelligence' Banks using Watson can use its capability to help their compliance teams connect regulatory requirements to obligations to controls, then manage those controls through an interactive dashboard. For example, a compliance officer might review a document for relevant obligations. Instead of remembering or manually looking up each relevant regulation, Watson can scan the document and highlight specific areas where regulation or guidance is likely to apply. The banker can then review those recommendations and, much more quickly than before, incorporate them into the compliance program. ""It's augmented intelligence, not artificial intelligence,"" says Alistair Rennie, general manager for Watson Financial Services. ""The more we can look at in context, the better we can get answers to be, and the more we can work through expert guided discussions about what the right answer, decision or choice is, the better everything gets. …",,2017.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
6e52345184e660e82b2954c8195f8015ffe4a4e9,https://www.semanticscholar.org/paper/6e52345184e660e82b2954c8195f8015ffe4a4e9,"Proceedings of the seventh ACM International Workshop on Mobility in the Evolving Internet Architecture, MobiArch 2012, Istanbul, Turkey, August 22, 2012","Welcome to the seventh ACM International Workshop on Mobility in the Evolving Internet Architecture (MobiArch'12) in Istanbul, Turkey! We are delighted to see the workshop has attracted high quality submissions from international researchers in both academia and industry. 
 
This workshop provides the opportunity to participate in the exploration of the state of the art research results on mobile Internet computing and in particular, pricing and mobile cloud research. 
 
As the number of mobile users proliferates, there is an ever increasing appetite for innovative mobile services. Service providers are often under tremendous pressure of deploying new services to users quickly and cost effectively. This requires us to critically rethink the mobile services architecture that will lay the foundation for a futuristic mobile Internet and a growing revenue base. The aim of this proposed workshop is to have an open forum discussing cutting edge research contributions with a focus on economic and incentive issues in mobile networks and technologies pertaining to mobile cloud. Mobile operators cannot compete solely on higher bandwidths supported by the underlying technology (e.g. HSPA+, LTE). Service differentiation using innovations along other dimensions like pricing, incentives, QoS, etc. is turning out to be essential to compete. As mobile technology has matured tremendously in the past five years, we are moving into the domain of sophisticated mobile applications that encompass the regular World Wide Web, mobile centric web and cloud based services. However, new challenges have to be met: 
How do we create service differentiation with mobile economic incentives in mind? 
What are some of the new ""incentive-aware"" mobile network architectures? 
Are there new theories and models for understanding network pricing and congestion control? 
 
 
 
To meet with these challenges, researchers from a wide range of academic fields, including theory and algorithms, data mining and machine learning, computer systems and networks, statistical physics and complex systems, economics and managerial science, etc., are all actively studying various aspects concerning mobile networks. 
 
This workshop is intended to present such an opportunity and serve as a forum to bring together people from various fields to exchange their latest research results and to discuss new ideas and directions to properly understand these networks. 
 
Highlights of the workshop include a keynote speech delivered by Lili Qiu (Associate Professor at University of Texas, Austin), world-renowned researchers in the fields of mobile computing systems including mobile pricing, incentives and mobile cloud technology and services. 
 
The technical sessions consist of presentations and discussions of the 5 accepted full papers and 2 short papers on a wide variety of issues of mobile networking and architecture research from different angles. These papers were selected from 12 submissions. The selected papers were chosen by a technical program committee (TPC) of 19 experts in various fields related to mobile pricing and cloud technology. The selection process started shortly after the submission deadline. Each paper was reviewed by at least three independent reviewers, and evaluated based on scientific novelty, technical quality, relevance to the topics, and contribution to the field.",MobiArch@MobiCom,2012.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
739ca20817a8f07c4f3ec3fb5ebcbe6d50b55017,https://www.semanticscholar.org/paper/739ca20817a8f07c4f3ec3fb5ebcbe6d50b55017,Post-Processing of 3D Printed Metallic Micro-parts,"The fourth industrial revolution is transforming current manufacturing approaches. One of the main objectives of this Industry 4.0 is to mass produce highly customized products. Hence, this requires new types of production methods to manufacture parts with a large variety of dimensions and geometry. The present research project aims to contribute to this objective in collaboration with an industrial partner. They are a solution provider in the high-precision metal additive manufacturing (AM) field, combining AM, nanotechnology and machine learning providing unique freedom of design and material choice for high-precision parts by their developed 3D printer combining deposition of nanoparticle monolayers with laser sintering. However, most of their generated micro metal parts require post-processing steps before entering industrial applications. As additional process steps appear to be undesired, deploying post-processing could have an additional benefit of reducing the overall manufacturing time by balancing layer height and the polishing process to reduce the surface roughness of the generated metal micro-parts. Among other methods, electrochemical polishing (EP) seems a promising candidate for post-processing such parts. Nevertheless, EP is still unexplored in combination with this novel high-precision 3D printing method. This project aims to find a trade-off between layer thickness, printing speed and polishing. An optimum should be achieved to get smooth and flat surfaces to maintain high quality parts at reasonable fabrication time. Secondary, the project aims to acquire more knowledge of the printing technology and its interaction with post-processes on various alloys. Handling of these microparts, such as manual operations to attach electrodes for EP, is extremely challenging because of their small size (sub-mm) and relative brittleness. EP is the reverse of electroplating; hence, it dissolves metal on the surface of the part resulting in a smooth surface. This method does not apply forces on the brittle parts, requires very little handling and is able the reach very low surface roughness (submicron) on most geometries increasing fatigue resistance of the parts. However, EP of small micro-parts encounters some challenges. It requires the polishing medium (electrolyte) to be in contact with the part surface and a mechanical contact is required to close the electrical circuit (current flowing to the part). As generating a contact with typical metal parts is trivial, this is an issue for the printed micro-parts as contact points are relatively large compared to the parts to be polished. Second, EP produces gases that can interfere with medium contact of the part. This study presents a novel prototype for an EP setup to overcome such issues for polishing metallic micro-parts. It consists of a rotating drum designed to evacuate gases and vary while being mostly constant the contact point by tumbling the parts in a conductive drum. This ensures uniform and smooth surface roughness reduction on parts of all sizes and shapes. A preliminary study of the prototype impact shows that the printing time could be reduced by four times. Further, ongoing tests of EP on different alloys such as SS316, Ti and CuSn will be presented.",,2020.0,10.32393/csme.2020.1261,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
09a4ab6a61eaa69d1ce496a4f291d565329fc8d5,https://www.semanticscholar.org/paper/09a4ab6a61eaa69d1ce496a4f291d565329fc8d5,eResearch Collaboration Projects-supporting CSIRO's digital science and research,"Background CSIRO is Australia’s largest research agency and is a recognised leader in a diverse set of
science domains: Agricultural Sciences, Environment/Ecology, Plant and Animal Sciences,
Geosciences, Chemistry and Materials Science. CSIRO also manages research infrastructure
like the Australia Telescope National Facility (ATNF), the Marine Research Vessel RV
Investigator and the Pawsey Supercomputing Centre. For many years in Australia, and also worldwide [2], research and science have undergone
transformational changes with the introduction of new instruments and advanced facilities
with matching increases in storage and computing capabilities. Individual researchers were
taking a bespoke approach to matching these technologies and capabilities to the way that
research and science were carried out. Wider adoption of new practices required social
change (in the practice of science and research) and these changes remained fragmented
and tailored to specific sciences or even projects. Organisations, by and large, varied
enormously in their support of these new practices.As far back as 2007 [1], CSIRO eResearch practitioners advocated that science and research
practices within CSIRO adapt to deal with these challenges. Much like the rest of the world,
practices matured over the years: in CSIRO’s health and biosecurity, oceanographic and
atmospheric research, radio astronomy, agriculture and food as well as geological and
other earth sciences. However, a significant shift occured in 2018, with a formal recognition by the CSIRO Board
of the need to support the new “digital” science and research at an organisational level.
CSIRO developed strategic digital transformation initiatives, including CSIRO’s Managed
Data Ecosystem (MDE), Missions and the Digital Academy [4].The aim of the MDE is to connect current and new platforms in a seamless way and improve
interoperability between datasets so users will be able to easily find and work on multiple
datasets. It will provide a set of tools and approaches enabling CSIRO and partners to
improve our collaboration, mining and analysis of data. CSIRO Missions are major scientific and collaborative research programs aimed at making
significant breakthroughs in one of six major challenges facing Australia. They include the
resilient and valuable environments, food security and quality, health and well-being, future
industries, sustainable energy and resources, and regional security. CSIRO's Digital Academy is focused on investing in the digital capability of our staff and
involves a rethink in planning for a digitally driven research environment. It provides a
learning opportunity for our staff, helping define the digital talent, skills and new ways of
working. The Academy will help attract and retain new digital talent within the Australian
innovation system, develop new digital skills and mindsets in Australian’s scientists and
facilitate digital talent accessibility and collaboration across Australia’s innovation system.Existing Support for “Digital” Science through “eResearch” initiativesCSIRO Scientific Computing Services group has been providing a dedicated eResearch service
since 2011 [3] This service is delivered through ""eResearch Collaboration Projects” (eRCPs)
which now delivers specialist capabilities that includes Machine Learning, Data Analytics,
Scientific Visualisation, Workflow Management and Science Data Handling into research and
science projects. The eRCP process is run as a competitive grant process and continues to be very successful. In the latest cycle, forty Scientific Computing Services specialists successfully completed and
delivered over sixty eRCPs outcomes from a total of eighty submissions. The underlying
capabilities are delivered by members from each of teams in the Scientific Computing
Services group: Technical Solutions; Data Analytics and Visualisation; Research Software
Engineering; and Modelling and Dataflow. The eRCP process also provides a mechanism to
promote and introduce new tools and frameworks for consumption to CSIRO’s research
community eg Jupyter and R/Shiny. In the latest cycle, forty Scientific Computing Services specialists successfully completed and
delivered over sixty eRCPs outcomes from a total of eighty submissions. The underlying
capabilities are delivered by members from each of teams in the Scientific Computing
Services group: Technical Solutions; Data Analytics and Visualisation; Research Software
Engineering; and Modelling and Dataflow. The eRCP process also provides a mechanism to
promote and introduce new tools and frameworks for consumption to CSIRO’s research
community eg Jupyter and R/Shiny. Specialists from the Scientific Computing program are then assigned to work on one or more
approved eRCPs. Over the six-month cycle, the resource allocation is around 0.2 FTE, with
each staff member allocated 3 eRCP projects per cycle. Importantly, eRCPs are provided to
CSIRO researchers and scientists at no additional charge.The eRCP has been enormously successful over the years, with demand outstripping
capability to allocate staff to the projects. The program has demonstrated a range of useful
outcomes including – including for example - an augmented reality tool for analysing
bushfire plumes over Tasmania; a dashboard to interrogate cotton crop physiological
measurements and an online platform to monitor algal blooms for multiple water bodies.Scientific Computing specialists also provide dedicated support to CSIRO researchers, based
around the same set of core capabilities, via an entirely separate funding models known as
“pan deployments” as well as secondments. In both cases, CSIRO projects fund the specialists’
time at larger allocations, often extending over 12 months or more. In a sense, this acts like a
contractor service for Business Units, providing them with highly specialised skills but without
the need to recruit new staff of their own.Future PlansCSIRO Scientific Computing will respond to the major initiatives – MDE, Digital Academy and
Missions as follows:MDE - Redirect Scientific Computing expertise currently working on eRCPs and pan
deployments to MDE related activities. In the first instance, these specialists
will apply their skills and domain knowledge to one of several nominated
pilots, helping design and build foundational components of the MDE. - Over time, it is anticipated that those same specialists will contribute to the
ongoing development and enhancement of additional MDE components in
line with its progressive organisational rollout. Digital Academy - Develop/adapt training content as appropriate for the Digital Academy. For
example, making use of existing Software Carpentry material for HPC usage,
but customising appropriate aspects for our own computing environment.- Delivering training content to CSIRO staff. This has already proven very
successful in the machine learning area – with hundreds of staff attending
sessions - and will no doubt continue to grow over time.Missions - Scientific Computing will continue to provide CSIRO researchers with the
eResearch support they need in response to the significant scientific
challenges tackling Missions. REFERENCES 1. J. A. Taylor, J. Zic, and J. Morrissey, “Building CSIRO e-Research Capabilities,” in eResearch Australasia 2008.2. T. Hey, S. Tansley, and K. Tolle, “The Fourth Paradigm: Data-Intensive Scientific
Discovery,” Data-Intensive Sci. Discov. Microsoft Res., 2009.3. S. Moskwa, “The Accelerated Computing Initiative,” in eResearch Australasia, 2012.4. CSIRO Chief Executive's Report 2018-19: https://www.csiro.au/en/About/Ourimpact/Reporting-our-impact/Annual-reports/18-19-annual-report/part-1/chiefexecutive-reportABOUT THE AUTHOR(S) Dr John Zic is the Executive Manager of CSIRO’s Science Computing Services Mr Justin Baker is Leader of the Scientific Computing Data Analytics and Visualisation
Team.",,2020.0,10.6084/M9.FIGSHARE.11929647.V1,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
13ae2d9c19db5655400da53a7cb9453a30228861,https://www.semanticscholar.org/paper/13ae2d9c19db5655400da53a7cb9453a30228861,Editorial: Advance of simulations and techniques for communication networks and information systems,,Wirel. Networks,2021.0,10.1007/S11276-021-02601-6,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
41f95df93e0d113b2e8a5aa94a73156755ddd649,https://www.semanticscholar.org/paper/41f95df93e0d113b2e8a5aa94a73156755ddd649,"Generalized Net Model of an Automated System for Monitoring, Analysing and Managing Events Related to Information Security","A R T I C L E I N F O : RECEIVED: 02 JUL 2019 REVISED: 08 SEP 2019 ONLINE: 22 SEP 2019 K E Y W O R D S : security information and event management, information security, security tools, security services Creative Commons BY-NC 4.0 Introduction Every day cyber attackers break into networks disguising as employees and delete their tracks as they go. With time against you and with inadequate tools it can take an average of eight months to filter through such massive volumes of data in order to detect and contain the attack. The IBM QRadar Security Intelligence Platform is designed to automatically identify and analyse threads earlier in the attack cycle providing the necessary time to respond. Ivelina Vardeva, ISIJ 43, no. 2 (2019): 257-263 258 Methods Is computer security a problem? Today we are completely dependent on computer networks and information. Business, industry, utilities, and strategic sites bind their processes to computer networks and the Internet. Technology alone cannot solve the problem; they are the only tool we manage. People creating technology and managing information systems and computer networks are not mature, human errors create prerequisites for security breaches. The use of security information and event management (SIEM) systems increases the level of information security in already existing architectures that provide the ability to manipulate the flow of information and manage incidents and events in real-life mode of these systems. In order to man-age realtime security incidents, it is necessary to make a decision before the situation becomes critical. To perform such control and analysis, automated forecasting mechanisms are used based on the accumulated data for the normal operating state of these systems. The automation of real-time decision making is based on mechanisms that determine the state of information security. To enable security analysts to perform investigations, SIEM correlates information such as these examples: Point in time, Offending users, Origins, Targets, Vulnerabilities, Asset information, Known threats. Overview of key SIEM capabilities The key SIEM capabilities include: • Ability to process security-relevant data from a wide variety of sources, such as: o Firewalls o User directories o Proxies o Applications o Routers; • Collection, normalization, correlation, and secure storage of raw events, vulnerabilities, network flows, assets, and threat intelligence data; • Layer 7 payload capture up to a configurable number of bytes from un-encrypted traffic; • Comprehensive search capabilities; • Monitor network and host behaviour changes that could indicate an attack or policy breach such as these examples; • Off hours or excessive usage of an application or network activity patterns inconsistent with historical user profiles; • Prioritization of suspected attacks and policy breaches; • Notification by email, SNMP, and others; GN Model of an Automated System for Managing Information Security Events 259 • Provision of a variety of generic reporting templates. Based on these key capabilities of SIEM, intelligent automated security solutions are taken. They also include automation, dashboard, visualizations, workflows, reporting capabilities. Security intelligence platforms incorporate: • use cases – advanced threat detection, insider threat detection, risk and vulnerability management, critical data and GDPR, incident response, cloud security, compliance; • analytics engine – security analytics, real time detection and user driven analytics (machine learning, powerful search, behavioural analytics, artificial intelligence, threat hunting); • unlimited logging – date store (endpoint network, applications identity vulnerabilities, configuration assets 3th party data stores); • deployment model, that can be on the premise, as a service, cloud, or hybrid. All SIEM tools are an important part of the data security: they aggregate data from multiple systems (described above) and analyse that data to catch abnormal/unconventional behaviour or potential cyberattacks. What the SIEM processes involve is shown in Figure 1.",Information & Security: An International Journal,2019.0,10.11610/isij.4319,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
ae2d551263b943b149414d1bb96e24113a93257c,https://www.semanticscholar.org/paper/ae2d551263b943b149414d1bb96e24113a93257c,Generative Adversarial Networks for Generation of Synthetic High Entropy Alloys,"High-performance materials are a key tool for several reasons. On the one hand, their use brings obvious progress in the performance of the pieces where they are used in fields such as aeronautics, construction, or biotechnology. On the other hand, highperformance materials also allow more efficient use of energy in industrial processes where the use of such energy becomes intensive with its consequences in terms of environmental and economic sustainability. For these reasons, the emergence of high-performance materials such as high entropy alloys (HEAs) has captured the attention of industry and researchers within the last years. However, the development of these materials requires a large amount of time and money invested in the design, synthesizability evaluation, construction, and characterization of such compounds. The use of artificial intelligence for the design of materials, even in its current infancy status, provides a valuable tool to accelerate the initial phases of materials design and HEAs, where the high number of combinations brings a perfect scenario for the deployment of Machine Learning techniques. In this work, a Generative based approach is used, namely Generative Adversarial Networks (GANs) to generate synthetic HEAs for highly intensive industrial processes. The architecture model of a GAN involves two neural networks. The first one is a generator model for generating chemical compositions of candidate alloys to form the HEAs. The second one is a discriminator model for classifying the generated samples coming from the generator in real or fake compositions. The discriminator learns from a specific data structure that contains data from real samples to classify the generated samples. A GAN extension that conditionally generates the synthetic outputs by the addition of extra inputs was used. This so-called conditional tabular generative adversarial network (CTGAN) was developed to be used with tabular datasets as input. Such data is normally composed of a mix of continuous and discrete columns, making some deep neural network models fail in performing a properly modeling for this kind of inputs. In the present approach, the generated realistic synthetic data was based on the conventional parametric design parameters used for HEAs, i.e., atomic size difference δ, mean atomic radius a, average melting temperature Tm, mixing enthalpy ∆Hmix, mixing entropy ∆Smix, electronegativity χ, valence electron concentration (V EC), mean bulk modulus K, and the standard deviation for most of them. As conditioned input data, the chemical composition of the alloys and their phase has been considered. The phase was classified in four classes, namely amorphous, intermetallic, solid solution, and solid solution + intermetallic, which can be used as an indicator for their applicability. The CTGAN provides as output candidates of HEAs, the expected parameters mentioned above, and corresponding phase. The generated data is compared with the calculated data and a verification of novel generated compositions is done in open materials databases available in the literature. Finally, a specific data structure for the CTGAN training and results of the performance of this approach is provided, which was developed in the framework of the European project ACHIEF for the discovery of novel materials to be used in industrial processes. C. E. Precker, A. Gregores-Coto, and S. Muı́ños-Landı́n are with the AIMEN Technology Centre, Smart Systems and Smart Manufacturing–Artificial Intelligence and Data Analytics Laboratory, PI. Cataboi, 36418 Pontevedra, Spain. e-mail: christian.precker@aimen.es. Manuscript received August 18, 2021; revised August 24, 2021. Keywords—Artificial Intelligence, High Entropy Alloys, Generative Adversarial Networks, Intensive Energy Processes.",,2021.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
c5d2a523a209e4cb8b91c732e4019f1d2cfc3ba8,https://www.semanticscholar.org/paper/c5d2a523a209e4cb8b91c732e4019f1d2cfc3ba8,ACM Transactions on Internet of Things,"The Internet of Things (IoT) demands synergy among several research domains and incorporates a broad range of multidisciplinary topics, including low-power wireless networking, embedded systems, data streaming architectures, data analytics and machine learning, cloud and edge computing, service computing and middleware, and security and privacy, as well as social computing. ACM Transactions on Internet of Things (TIOT) publishes novel research contributions and experience reports broadly related to these topics and their interrelations in the context of IoT, with a focus on system designs, end-to-end architectures, and enabling technologies, covering in principle the entire spectrum from hardware devices up to the application layer. Along with this large breadth of scope, another defining element of TIOT is that the results and insights reported in it must be corroborated by a strong experimental component. This is expected to offer evidence of the proposed techniques in realistic scenarios (e.g., based on field deployments or user studies) or public datasets, with the intent to facilitate adoption and exploitation in the real world of the novel ideas published in TIOT. In the same light, experience reports about the use or adaptation of known systems and techniques in real-world applications are equally welcome, as these studies elicit valuable insights for researchers and practitioners alike. This first, inaugural issue bears witness to the aforementioned breadth of topics and emphasis on experimental validation, as it begins with articles proposing novel system-level techniques concerned with wearable computing and light-based positioning, continues with contributions concerned with security at the edge and IoT services in the cloud, and then ends with the definition of ontologies for IoT applications. Many other interesting papers have already been accepted and will appear in the upcoming issues. All of these high-quality contributions have been selected from an outstanding number of submissions from all over the world. We are very excited to see that the research field of IoT is increasingly gaining momentum. In this respect, we are fortunate to have an outstanding editorial board helping us with the process of reviewing and selecting from these many and diverse submissions. The associate editors on the board reflect the scientific mission and values of TIOT and comprise top-notch researchers from academia and industry, with a balanced mix of seniority, gender, and geography. We sincerely thank all of them for accepting to help us in the delicate task of bringing the first issues of TIOT to reality. Indeed, ACM TIOT is the result of the work of many people, some of whom we want to publicly thank in this inaugural editorial. We are very grateful to Steve Welch and the ACM Publications Board for kickstarting the process by contacting us and planting the seed of a new transaction on IoT in our heads. Lothar Thiele and Tarek Abdelzaher drafted the journal proposal alongside us, offering insights that were key in defining the current scope of TIOT; we are honored to have both",ACM Trans. Internet Things,2020.0,10.1145/3379599,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
7e5b2d6ecd340766f6eba8298eeb1493b69653d7,https://www.semanticscholar.org/paper/7e5b2d6ecd340766f6eba8298eeb1493b69653d7,Adaptive AR/MR Output Security on a Physical Device using Reinforcement Learning,"Augmented Reality (AR) and Mixed Reality (MR) are becoming increasingly ubiquitous. Research has shown that AR/MR will be a $100 industry by 2020 [7] and companies are chasing this market by making their own solution to AR/MR. Some solutions, such as Apple’s ARKit [18] and Google’s ARCore [19], use smartphones to create an AR/MR experience through the smartphone camera and display. However, these experiences are not fully immersive since users must hold up their smartphone in front of them. Microsoft’s HoloLense [17] and Magic Leap’s Magic Leap One [16] Head Mounted Display (HMD) AR/MR devices enable a much more immersive AR/MR experience since users do not have to hold a device in front of them and objects (i.e. “holograms”) are project right in front of them to augment their field of view. Although these devices are limited in their current field of view, their immersiveness is promising for the future of AR/MR. Although current immersive AR/MR technologies are focused on the use of proprietary devices, HoloKit [20] enables immersive AR/MR experiences with the use of a smartphone and simple cardboard mounting device (similar to Google Cardboard [21] which is for Virtual Reality experiences). In addition, researchers have suggested that AR car windshields may be a good use case for AR/MR technologies [23] through applications such as left-turn driving aids [24]. While the increasing proliferation of AR/MR devices will undoubtedly enable many new applications, issues of privacy and security cannot be ignored. Much of the previous work on AR/MR privacy has focused on the inputs to AR/MR devices (i.e. input security) [14,15]. Noting this gap in AR/MR security research, Lebek et al. suggested in a position paper that there should also be a focus on output security to secure the output of AR/MR devices [4]. Visual AR/MR output security is concerned with two issues pertaining to the user’s visual field: • Regulating visual content displayed to the user to reduce distraction and obstruction of the real-world context. • Preventing holograms from obscuring other holograms with a higher priority. To understand why these two issues are a concern for AR/MR security, take the case of an AR windshield in a car. Suppose the AR display in the car had a hologram to display the current speed and a hologram with the name of the song which is currently playing. It would be dangerous if either of the holograms obstructed an important real-world object such as a stop sign (the first security concern). In addition, it would also be dangerous if the name of the song hologram obstructed the current speed hologram (the second security concern). While one could leave these output security concerns to the application developer, it is much safer to have the OS guarantee the security of AR/MR device outputs. In this vein, previous work has investigated what an OS level AR/MR device output security module may look like by allowing developers to write policies [3]. However, as noted by Surin et al., these hand-coded policies are cumbersome and impractical for real-world use [1]. For example, specifying a policy to move holograms that are obstructing an important real-world object while not moving too far from its original location and not obscuring other important holograms at the same time is a very difficult task. Surin et al. proposed the use of reinforcement learning (RL) to solve this problem to automatically generate policies and demonstrated its effectiveness [1]. Although previous work has demonstrated the importance of output security and demonstrated its feasibility in simulation [1,3], they have not deployed output security policies on a physical AR/MR device to ascertain its viability in the realworld. In particular, whether a RL based output security model can be deployed without degrading performance was left an open question [1]. To fill this gap, this work investigates whether RL models can be deployed on a physical AR/MR device without a noticeable degradation in performance and test the deployment of an output security policy trained using RL. It is important to note that while this work focuses on visual output security, there are concerns that other AR/MR output, such as audio and haptic output, may need to be regulated as well [1,3]. In addition, one may also want to limit distracting and uncomfortable AR/MR output such as blinking holograms much like web browsers have evolved to block popups and the blink tag [3].",,2019.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
db2b4ea57fde0b16ecd5973447bd502ac3ae99e0,https://www.semanticscholar.org/paper/db2b4ea57fde0b16ecd5973447bd502ac3ae99e0,FASTTRACK RECOMMENDER SYSTEM,"Network operators are overloaded with numerous recommendations coming from vendors, some of which come from automated recommender systems. Such automated recommendations may or may not apply to a customer’s specific environment, often lack an assessment of priority within the context of the other recommendations, and may or may not apply to an individual customer’s scenario. To address these challenges, techniques are presented herein that provide a novel approach to generating and ranking recommendations coming from a dynamic recommender system where rankings are based on enriched context from, for example, live data on real networks, activities performed by real customers, etc. Such techniques enhance the operational features of existing networks by recommending popular items to new customers, identifying critical items that can be proactively addressed in order to provide additional services, and reducing Technical Assistance Center (TAC) cases when patches exist for common issues. DETAILED DESCRIPTION Network operators are overloaded with numerous recommendations coming from vendors, some of which come from automated recommender systems. Such automated recommendations may or may not apply to a customer’s specific environment. Further, such recommendations often lack an assessment of priority within the context of the other recommendations that such a system provides as well as adoption trends of industry peers. Finally, such recommendations may or may not apply to an individual customer’s scenario. Even high priority recommendations are not of value to a customer who is not running a configuration that would benefit from the recommendations. 2 Jannetta et al.: FASTTRACK RECOMMENDER SYSTEM Published by Technical Disclosure Commons, 2020 2 6564 Recommendations in this context may be sourced from, for example, vendors, experts, expert systems, or machine learning pipelines. Ultimately the customer is presented with an unwieldy list of items that can cause them to miss the important recommendations or tune out the source of the items altogether. The general professional services industry presents a large volume of recommendations to customers, and while valuable, customer feedback suggests that it is difficult to parse and prioritize the insights and recommendations. To address these types of challenges techniques are presented herein that support a recommender system that is designed to cover any cases that meet the following criteria:  New items become available all of the time.  A large population is available to observe the adoption of those items.  When the items become part of a larger set, they are no longer needed if the set itself is used. This is a common scenario across many areas regarding customer recommendations. There is a need to add priority and ranking to the automated recommendations such that the network operator can make a decision whether to implement the recommendation in the short term, the longer term, or not at all. For purposes of exposition, in the narrative that follows aspects of the techniques presented herein will be described with reference to software patches for an operating system (OS). Aspects of the techniques presented herein support a method for recommending only the most relevant items to customers, or as candidates for automated deployment methods. Such ranking of the items ensures that only the highly relevant ones that are deployed in the largest and most active customers will be identified for deployment in customers that may be challenged (by, for example, a lack of skills, a lack of resources, etc.) in keeping up with software patch levels. It is important to note that software patching is only one example of how aspects of the techniques presented herein may be employed. Further aspects of the techniques presented herein may utilize routing protocol features. For example, perhaps most customers who run Open Shortest Path First (OSPF) (i.e., Feature A) on a particular device (i.e., Feature B) choose to change the way OSPF metrics are calculated (i.e., Feature C). 3 Defensive Publications Series, Art. 3788 [2020] https://www.tdcommons.org/dpubs_series/3788 3 6564 In addition, the rate of enabling the new feature C once it became available is relatively high (thus increasing the relevance). This creates a strong recommendation to enable this feature for customers that have not yet enabled it. The framework that is associated with the techniques presented herein may be applied to any feature recommendation, configuration best practice, security advisory remediation, etc. For simplicity of exposition the discussion that follows will focus on a software patching scenario. Aspects of the techniques presented herein include a series of stages to create a ranked list of recommendations. For the purposes of illustration, in the example that is presented below various of the stages are highlighted using a very common yet complex scenario for recommendations – i.e., a base software that has both patch and service pack capability. As will be readily apparent, of particular interest and note are stages one and two (e.g., leveraging off-the-shelf solutions for feature identification) and stages three and four (e.g., developing a score for each feature based on a rate of adoption and continued usage by peers). A first stage of the techniques presented herein encompasses a pairwise recommender system that is built using off-the-shelf algorithms. Pairwise feature comparisons with historical software patch data and lift metrics are used to provide an initial ranking of software patches, as well as an indication of which software patches are ""real"" because they have seen in real network data. For the general recommendations case, this could be classes of recommendations that have been actioned by other customers. A second stage of the techniques presented herein employs the initial rate of deployment to combine with the lift and support metrics from stage one. This is a second scoring method which adjusts the patch ranking to account for deployment statistics seen across thousands of other network devices. Again, for the general recommendations case this would be the relative pace of taking action once recommendations were first shown to a customer (e.g., was it immediate, thus having a high rank, or did it linger, thus having a lower rank). A third stage of the techniques presented herein utilizes network controllers or individual devices to collect the latest configuration data from unseen, new devices, and apply the ranked rules from stages one and two to generate a ranked list of candidate 4 Jannetta et al.: FASTTRACK RECOMMENDER SYSTEM Published by Technical Disclosure Commons, 2020 4 6564 patches for each entry. In the general recommendations case, this could be the current ""network improvement plan"" or ""action list"" from a cloud service. A fourth stage of the techniques presented herein is a filter stage that looks at userto-user similarity (as opposed to item-to-item similarity in stages one and two) to determine the applicability of the recommended patches. One example is configuration similarity (e.g., modeled features as a configuration representation) to determine possible usefulness based on configurations of the device population that has the software patch installed. In the general recommendations case, this could be an industry peer group. A fifth stage of the techniques presented herein integrates with an Information Technology Service Management (ITSM) system to open a ticket that includes, possibly among other things, the recommendation, the benefits to customers, and an option/services to make the change for the customer. This stage may also contain a final ""don't recommend"" list that is generated by expert systems matching (e.g., a patch is not relevant, a patch is in an installed service pack, etc.) as well as the input from stage six (e.g., a customer indicated no interest in the patch). A sixth stage of the techniques presented herein closes the loop by developing a deployment plan (e.g., manual, automated system, etc.) or removing items that are no longer relevant (e.g., based on information from stage five) and then applying the changes in the network via the appropriate process for the type of recommendation. Aspects of the narrative that was presented above may be illustrated through Figure 1, below. Figure 1: Fasttrack Recommender System 5 Defensive Publications Series, Art. 3788 [2020] https://www.tdcommons.org/dpubs_series/3788 5 6564 As described above, stage one encompasses a pairwise recommender system that looks at all of the items in the item sets as pairs. This identifies items that are seen together, and can also double as a reasoner to determine that a patch is contained in other items. For example, as illustrated in Figure 2, below, both patches are contained in the service pack three. In any case of the customer having one but not the other, the system can infer that a service pack is not installed, and the missing feature should be recommended. Figure 2: Illustrative Associations In Figure 2, above, the top graph shows a clear relationship between two items commonly seen together, in a diagram that shows the overall deployment percentage. Any 6 Jannetta et al.: FASTTRACK RECOMMENDER SYSTEM Published by Technical Disclosure Commons, 2020 6 6564 standard recommender system will pick these up as association rules. (Note that the Apriori algorithm was used in this instance.) Figure 3, below, shows another example of patches that show up pairwise together in a different version of software. There are many patches covered, but there are clearly four standouts that are deployed at a much higher rate than any others. Those standouts get scored higher in stage two of the system. From some level of minimum deployment seen in real networks (such as, for example, 20% of the peer devices have the feature), an evaluation of the initial deployment line",,2020.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
dfbf5f757de62732595b67f73dee5989a00a5c25,https://www.semanticscholar.org/paper/dfbf5f757de62732595b67f73dee5989a00a5c25,How to deploy AI software to self driving cars,"The automotive industry is embracing new challenges to deliver self-driving cars, and this in turn requires increasingly complex hardware and software. Software developers are leveraging artificial intelligence, and in particular machine learning, to deliver the capabilities required for an autonomous vehicle to operate. This has driven automotive systems to become increasingly heterogeneous offering multi-core processors and custom co-processors capable of performing the intense algorithms required for artificial intelligence and machine learning. These new processors can be used to vastly speed up common operations used in AI (Artificial Intelligence) and machine learning algorithms. The R-Car V3H system-on-chip (SoC) from the Renesas AutonomyâĎć platform for ADAS (Advanced Driver Assistance Systems) and automated driving supports Level 3 and above (as defined by SAE's automation level definitions). It follows the heterogeneous IP concept of the Renesas Autonomy platformâĎć, giving the developer the choice of high performance computer vision at low power consumption, as well as flexibility to implement the latest algorithms such as those used in machine learning. By examining the architecture of the R-Car hardware we can understand how this differs from HPC and desktop heterogeneous systems, and how this can be mapped to the SYCL and OpenCL programming models. When both power consumption and performance are important, as is the case in the automotive industry, the focus for implementing OpenCL and SYCL on these hardware platforms must be a balanced approach. The memory capacity and layout must be used in the most optimum way to build a pipeline that provides the best throughput. The R-Car hardware provides DMA and on-chip memory where these are used to facilitate efficient data transfer on the device. The memory hierarchy layers can be seen on how it is efficiently mapped to OpenCL paradigm. The R-Car hardware also offers many fixed function IP blocks, each performing a specific function like convolution for deep neural networks, optical flow and more, beyond the programmable processor. The flexibility of OpenCL enables the development of built in kernels so that developers can take advantage of these architecture designs. The OpenCL model enables extensive usage of the heterogenous hardware, including fully programmable IP, efficient data transfer using the DMA to the on-chip device memory via OpenCL extension, and fixed function IP block, such as CNN for enabling high throughput convolution operations, via OpenCL builtin kernels, device triggered DMA using partial subgroups. We examine the memory mapping to bring in efficiency and the software pipelining & parallelism. These hardware architectures include AI accelerator processors specifically designed to be used in the next generation of vehicles. In particular, the processors are designed to tackle complex algorithms whilst limiting the overall consumption of power. Benchmarks will be presented to show how portable code can also deliver performance for developers using this hardware. As well as enabling developers to choose OpenCL or SYCL, we will talk about how these standards enable additional high-level frameworks that can be used to target this hardware. These include libraries for deep neural networks and linear algebra operations.",IWOCL,2019.0,10.1145/3318170.3318184,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
3b9ce2ee4028f5faea850e6e0cedb06b68b57ce4,https://www.semanticscholar.org/paper/3b9ce2ee4028f5faea850e6e0cedb06b68b57ce4,LEARNING THROUGH PRACTICE VIA ROLE-PLAYING: LESSONS LEARNT,"Software engineering is the establishment and use of sound engineering principles in order to obtain economically software that is reliable and works efficiently on real machine. Sound software engineering closely related with socio-technical activity that depends on several human issues which are communication, collaboration, motivation, work environment, team harmony, engagement, training and education. These issues affect everything for students to fully understand software engineering and be prepared for software development careers. Therefore courses offered in the university must also consider the sociological and communication aspects, often called the socio-technical aspects. One popular method is to use role-playing exercises. Role-playing is a less technologically elaborate form of simulation for learning interpersonal skills and is analogous to rehearsal. It is particularly helpful when students are having difficulties to relate lessons learnt in the university to the applicability of the knowledge in the real implementation. This is because many students view software engineering as meaningless bureaucracy and have little interest in the knowledge delivered in the lecture hall. This scenario impedes the expansion of current knowledge and inhibits the possibility of knowledge exploration to solve range of industry problems. Simply lecturing about software engineering will never engage students or convince them that software engineering has value. Given this student bias, the goal of teaching software engineering often becomes convincing students that it has value. To achieve this, students need to experience firsthand the sociological and communication difficulties associated with developing software systems. In this paper, we argue that in teaching software engineering we must cover two essential things; delivery of knowledge and skills required in the software engineering domain in a form of lecture and hands-on practice to experience the value of the knowledge and skills learnt. We report on our experiences gained in deploying role-playing in master degree program. Role-playing is used as pedagogical tool to give students a greater appreciation of the range of issues and problems associated with software engineering in real settings. We believe that the lessons learnt from this exercise will be valuable for those interested in advancing software engineering education and training.",,2012.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
4d7819f1aeff718e590fe08ea944c28d1deda4a6,https://www.semanticscholar.org/paper/4d7819f1aeff718e590fe08ea944c28d1deda4a6,Solving Complex Industrial Problems Without Statistics,"Solving Complex Industrial Problems Without Statistics. 2016. Ralph R. Pawlak. Boca Raton, FL: CRC Press. 143 pages. This book fills a unique gap. For those in our community who are not strong in mathematical and analytical methods (but who wish to contribute more fully to the “analyze” stage of improvement projects), Pawlak provides practical examples to demonstrate the value of qualitative reasoning. For those who want to add a layer of common-sense thinking to continuous improvement efforts that are already data driven, he provides the background and the basis for making sure the analytical and computational parts of projects are well-framed. Despite the title, this book does not suggest industrial problems can (or should) be solved without robust mathematical methods and statistics. Rather, Pawlak spends most of the book describing and explaining the qualitative processes that should accompany all problem-solving efforts in industry. Grounded in 14 short case studies, the techniques he shares are simple and immediately applicable: identifying what characteristics distinguish good output from bad, generating “clues,” visual observation, articulating differences, and staging assembly and operations trials, among others. Even without a complete and rigorous analysis, these “clues” may provide enough information for readers to identify and deploy an appropriate corrective action. The examples cover a range of cases, from the new product development process, to assembly, to design, to machining, to troubleshooting the causes for defects. In summary, this is the only book I have ever encountered that comprehensively explains the thought process of the “fuzzy front end” in Lean Six Sigma problem solving. As a result, it provides excellent case studies that can be used as a first step toward understanding problem solving in common industrial scenarios. Despite a sometimes repetitive feel, the conversational tone of this book makes the material particularly accessible to beginners who might benefit the most from that repetition. “It does not matter that some problems that you encounter will require knowledge beyond your capacity,” Pawlak advises. In fact, the author could have gone further: the primary weakness of this book is that each of the case studies could easily have been two or three times as long and still maintained reader interest. Overall, the “crime scene” approach he recommends helps to frame the process of discovery and learning in a straightforward, accessible way, and provides a unifying thread through all the cases.",,2016.0,10.1080/10686967.2016.11918490,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
83033b64dfb41a92bc6c7946ba9829068077d95b,https://www.semanticscholar.org/paper/83033b64dfb41a92bc6c7946ba9829068077d95b,An Efficient improved technique to retrieve the bug repository,"Bug triage is the most important step in handling the bugs which occur during a software process. In manual bug triaging process the received bug is assigned to a tester or a developer by a triager, hence the bugs are received in huge numbers it is difficult to carry out the manual bug triaging process, and it consumes much resources both in the form of man hours and economy, hence there is a necessity to reduce the exploitation of resources. Hence a mechanism is proposed which facilitates much better and efficient triaging process by reducing the size of the bug data sets, the mechanism here involves techniques like clustering techniques and selection techniques, The approach proved much efficient than the manual bug triaging process when compared with bug data sets which were retrieved from the open source bug repository called bugzilla. INTRODUCTION Most of the software industries spend nearly half of their economy, that is more than 45 percent of their economy in finding out and correcting bugs alone, this is occurring due to the exploitation of resources both in the form of human labor and economy by implementing manual bug triaging process, here the bug are manually allocated to the testers or developers by the triagers, since the incoming bugs are all over from the world it is a tedious process and require much man power and economy to facilitate the process. So, there is a necessity to reduce the expenditure, to deal with this exploitation previously few techniques have been proposed like text classification techniques, Naïve bayes, Support vector machine which facilitated the automation of the process, and also selection mechanisms like feature selection and instance selection have been used to reduce the size of the bug data sets for a better triaging process. As few researchers proposed that triaging produces better results when the data undergoes preprocessing before any machine learning algorithms are applied to it, Hence a combination of clustering mechanism and selection mechanism is used for a better triaging process, primarily the data is prepared by pruning the data with the help of techniques like tokenization, stop words, stemming etc.., and an algorithm called x means algorithm is used to cluster the data, and two selection mechanism called instance selection and feature selection are used to reduce the size of the bug data set and provide better triaging process. ITERATURE SURVEY Software engineering data contains useful information such as code bases, execution traces, bug databases etc.., by exploiting such data we can build a more reliable software system by building dependencies based on the historical data produced in the software process. [2] A.E Hassan Trends in Engineering and Technology (NCTET-2K17) International Journal of Advanced Engineering Research and Science (IJAERS) Special Issue-3 https://dx.doi.org/10.22161/ijaers/nctet.2017.55 ISSN: 2349-6495(P) | 2456-1908(O) www.ijaers.com Page |200 stated that most commonly available repositories of a software system are source control repositories, bug repositories, archived communications, deployment logs and code repositories. The mining software repositories concept which is called as MSR primarily analyzes and cross references all the data which is present in the repository to extract the data which is considered to be useful in the repository, by evolving these repositories from basic repositories which contain records to dynamic repositories which can be used for decision making in the aspect of software engineering. [11] John anvik stated that open source projects generally support open repositories in which both the user and the developers can access and report bugs. The reports which are received in the repository should undergo the process of triaging to determine whether the report is worthy or not, if it is that particular report should be assigned to a developer for further rectification, Hence in the context of large data repositories the [4] Davor C Urbanic stated that, a bug tracking system is necessary to maintain bug reports on large software development projects. Research has been carried out by the researchers in the field of mining software repositories which augment traditional software engineering information by the use of tools and techniques, which are implemented in solving challenging problems in the field of software which the engineers face on a daily during the process. Triage is actually a medical term in which the patients are allocated based on their severity, in a similar way bug triage is a term in which the bugs are allocated to the testers based on their priority. [14]Silvia Breu stated that, in open source projects such as Mozilla, bugzilla and eclipse, the bug tracking systems interact with the user communities, so that the users can be a part of bug fixing process, hence this type of bug tracking systems play an important role in interacting and communicating with the users. [5] Dominuque Matter stated that, the daily incoming of bug reports is high on daily basis, hence triaging these incoming reports is necessary and it is a difficult task assigning a developer to a report is also a part of it. Traditional bug consuming is a very tedious and time consuming job, the repositories which contain the bugs are called bug repositories, bugs are collected into a data base, and the triaging team sort out the bugs based on their priority. [7]Gaeul Jeong stated that, It is important that the bugs should be identified and rectified within time in software engineering process, Most of the bugs are manually allocated by human triagers which is a tedious task. [12] Nicolas Bettenburg stated that bug reports acts as the main source of information for developers, but there exists a difference in the quality of each bug report, on the open source repositories like apache, eclipse and Mozilla a survey has been conducted with the help of the users and developers, in that survey more than 450 responses mismatched with the user requirement, to overcome this methods like reproducing, stack traces are used which are difficult to maintain for large number of bug reports. [13] Shivkumar Shivaji stated that prediction of bugs from source code files are done by using machine learning classifiers, primarily the classifier is linked with historical bug data and used for prediction the possible outcomes, yet these classifiers suffer from a drawback that is insufficient performance real world situations and slow prediction time, some of the classifiers are Naïve Bayes and support vector machine (SVM). Feature selection can defined as the process in which the irrelevant features are deducted and detecting only the relevant ones, an optimal selection of features can bring improvement in overall knowledge of domain, reduced size, generalization capacity etc.., [9] J.Arturo Olvera Lopez stated that sufficient identification of features is necessary in real world scenario, hence the identification of features is important. [20] Yiming Yang stated that, feature selection is the best solution for text classification problems it increases both the classification effectiveness and also computational efficiency. Instance selection is a process, in which the dataset size is reduced , which eventually decreases the runtime, especially in the case of instance based classifiers, the commonly used instance selection mechanisms are wrapper and filter, here Trends in Engineering and Technology (NCTET-2K17) International Journal of Advanced Engineering Research and Science (IJAERS) Special Issue-3 https://dx.doi.org/10.22161/ijaers/nctet.2017.55 ISSN: 2349-6495(P) | 2456-1908(O) www.ijaers.com Page |201 filtering mechanism is used called as Iterative case filtering(ICF) algorithm. [9] J. Arturo Ovlera Lopez stated that historical data that is previously known data is used to classify new instances.",,2017.0,10.22161/IJAERS/NCTET.2017.55,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
aac2edf6a9a174fe181e8055ad9a43c9627f63c7,https://www.semanticscholar.org/paper/aac2edf6a9a174fe181e8055ad9a43c9627f63c7,AI Firm Ambyint’s New Bakken Deal With Equinor Moves the Industry Another Step Closer to the Edge,"In the face of leaner economic times, oil and gas companies want to be able to boost well counts while minimizing any new additions to their field workforce.
 Among the companies answering this call is Ambyint, which was just tapped to deploy its artificial intelligence (AI) systems to help optimize all of Equinor’s Bakken Shale wells running on sucker rod pumps—the oil field’s most common breed of artificial lift.
 The deal is understood to be one of Ambyint’s largest contracts to date. Specific numbers have not been shared, but public data show that Equinor operates more than 800 wells in North Dakota. Ambyint, which is headquartered in Calgary and has an office in Houston, said the project scope may eventually include all of the Norwegian-owned operator’s horizontal wells in the state as they transition to rod pump.
 The large-scale upgrade marks another milestone in the evolution of oilfield automation that for decades has been defined by a nearly-ubiquitous reliance on SCADA (supervisory control and data acquisition) systems.
 One of the drawbacks of the status quo is that it requires small armies of field personnel to interpret SCADA data and then adjust setpoints to get pumping units back into optimal operating ranges. This manual process can consume half an hour per well to complete; downtime that quickly adds up in a field of hundreds.
 “What we are talking about is having the machine do that entire workflow,” Chris Robart, Ambyint’s president of US operations said.
 By equipping wells with its cloud-based AI and edge computing technology, and an application the company calls an autonomous setpoint management system, the man-hours once spent re-setting pumps can hopefully be reallocated to other bottom-line drivers. “We are freeing up individuals to go do other things, like think about new technology, troubleshoot failed equipment, deal with workovers, or new well designs,” Robart added.
 Pilot Hits the Mark for Equinor The Bakken project comes after a pilot that included 50 of Equinor’s wells, which saw a net production increase of 6%—considerably larger uplift figures were seen from those wells suffering from underpumping.
 The encouraging results were realized with zero shutdowns in production and minimal “human interference,” according to the companies.
 “The Ambyint technology has improved the remote data visibility and has delivered a more accurate diagnostic of downhole conditions to our rod pump wells in the Bakken,” a production engineer for Equinor’s Bakken asset, Jack Freeman, said in a statement. “The autonomous speed range management tool has leveraged the power of machine learning to optimize our wells by identifying and acting on real opportunities”
 Last year, Ambyint’s Series A funding round raised $11.5 million from several venture capital groups including Equinor Technology Ventures (formerly known as Statoil Technology Invest).",Journal of Petroleum Technology,2018.0,10.2118/1018-0030-JPT,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
d8ffef877579dffb1d3de19741e58774f65cf8df,https://www.semanticscholar.org/paper/d8ffef877579dffb1d3de19741e58774f65cf8df,"The Pransky interview: Harry Kloor, PhD, PhD – CEO and Co-Founder, Beyond Imagination Inc.; scientist; entrepreneur; inventor; filmmaker","
Purpose
The following article is a “Q&A interview” conducted by Joanne Pransky of Industrial Robot Journal as a method to impart the combined technological, business and personal experience of a prominent, robotic industry PhD-turned successful innovator and entrepreneur regarding turning his lifelong dream into an invention and commercialized product. This paper aims to discuss these issues.


Design/methodology/approach
Harry Kloor is a successful serial entrepreneur, scientist, technologist, educator, policy advisor, author and Hollywood filmmaker. He is the CEO and co-founder of Beyond Imagination, a company that has developed a suite of exponential technology solutions that deploys artificial intelligence (AI), AR, robotics, machine learning and human–computer interaction technology to enhance and revolutionize the world’s workforce. The company early in 2021 completed BEOMNI 1.0, the world’s first fully functional humanoid robotic system with an AI evolving brain, enabling remote work at a high level of fidelity to be done from around the globe. Kloor describes how he transformed his childhood dream into his brainchild and tangible reality.


Findings
Kloor was born a groundbreaker who did not take no for an answer. He was born partially crippled with his legs facing backwards. The doctors said that he would spend his life in braces and would never be able to run. His parents told him not to let those ideas limit him and by the age of seven he ran for the first time and went on to become a martial arts master. Kloor’s childhood dream was to create ways to leave his body and inhabit a robotic body so that he could physically be free from his limited mobility. Kloor built his first computer at the age of seven and invented his first product at the age of eight. Kloor's inspiration to study science came largely from science fiction and his 20,000-plus collection of comic books. Knowing the nature of exponential growth, he spent the next 40 years building the expertise, relationships, networks and experience in all areas of exponential technology. Kloor obtained a BA from Southern Oregon State College, an MEd from Southern Oregon University and two simultaneous PhDs, one in chemistry and one in physics, from Purdue University. Kloor co-founded the company Universal Consultants, where he served as chief science consultant, providing guidance to clients in the development of new technological products, patents and policy positions. Kloor was the founder of Stem CC Inc. – a stem cell company that was sold in 2018 to Celularity, one of the world’s most cutting edge clinical-stage cell therapeutics company. Kloor is also the founder and president of Jupiter 9 Productions and is a credited film writer, director and producer. Since his graduation from Purdue University, he has written for Star Trek: Voyager and was the story editor for Gene Roddenberry’s Earth: Final Conflict, a series he co-created/developed. Kloor helped create Taiwan’s animation industry, bringing Quantum Quest: A Cassini Space Odyssey, the first big animation film that starred major Hollywood stars, to Taiwan. Kloor also sits on the board of Brain Mapping and Therapeutics Society and serves as their Chief Scientific Advisor and Educational Outreach Coordinator.


Originality/value
Kloor is a “creative consultant and universal problem solver, with an emphasis in technology and education.” Kloor has worked with Dr Peter Diamandis since the first class of the International Space University in 1988. Kloor was one of the five founding team members of XPRIZE serving as its CSO until 2005 and was one of the founders of the Rocket Racing League. He was on the founding team of Singularity University and taught at Singularity’s first summer program. In 2016 he created the $10m Avatar XPRIZE, and in 2018 he co-created the Carbon Extraction XPRIZE which obtained the largest incentive prize in history, a $100m, funded by Elon Musk and the Musk Foundation. Kloor is the only person in world history to earn two PhDs simultaneously in two distinct academic disciplines. In recognition of this achievement, he was named ABC World News’ Person of the Week in August 1994. Kloor has received numerous awards, including The Golden Axon Award from the Society for Brain Mapping & Therapeutics. He has recently created the Kloor Cycle, a four-stage experiential autonomous learning process within Beomni’s “AI Brain,” adapted from Kolb’s Learning Cycles.
",Industrial Robot: the international journal of robotics research and application,2022.0,10.1108/ir-06-2022-0148,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
b00106a36f5009ec4ee70ff02ec004dbe83d7438,https://www.semanticscholar.org/paper/b00106a36f5009ec4ee70ff02ec004dbe83d7438,Measuring operational quality of recommendations: industry talk abstract,"With the rise of machine learning in production, we need to talk about operational data science. The talk introduces a pragmatic method on how to measure the response quality of a recommendation service. To that end the definition of a successful response is introduced and guidelines how to capture the rate of successful responses are presented. There are several changes that can happen during the serving phase of a model which negatively affect the quality of the algorithmic response. A few examples are: • The model is updated and the new version is inferior to the previous one. • The latest deployment of the stack that processes the request and serves the model contains a bug. • Changes in the infrastructure lead to performance loss. An example in an e-commerce setting is switching to a different microservice to obtain article metadata used for filtering the recommendations. • The input data changes. Typical reasons might be a client application that releases a bug (e.g., lowercasing a case sensitive identifier) or changes a feature in a way that affects the data distribution such as allowing all users to use the product cart instead of previously allowing it only for logged in users. If the change is not detected training data and serving data diverge. Current monitoring solutions mostly focus on the completion of a request without errors and the request latency. That means the mentioned examples would be hard to detect despite the response quality being significantly degraded, sometimes permanently. In addition to not being able to detect the mentioned changes, it can be argued that current monitoring practices are not sufficient to capture the performance of a recommender system or any other data driven service in a meaningful way. We might for instance have returned popular articles as a fallback in a case where personalized recommendations were requested. We should record that response as unsuccessful. A new paradigm for measuring response quality should fulfil the following criteria: • comparable across models • simple and understandable metrics • measurements are collected in real time • allows for actionable alerting on problems The response quality is defined as an approximation of how well the response fits the defined business and modelling case. The goal is to bridge the gap between metrics used during model learning and technical monitoring metrics. Ideally we would like to obtain Service Level Objectives (SLO)[1] that contain this quality aspect and can be discussed with the different client applications based on the business cases, e.g., ""85% of the order confirmation emails contain personalized recommendations based on the purchase."" A case study will illustrate how algorithmic monitoring was introduced in the recommendation team at Zalando. Zalando is one of Europe's largest fashion retailers and multiple recommendation algorithms serve many online and offline use cases. You will see several examples of how the monitoring helped to identify bugs or diagnose quality problems.",RecSys,2018.0,10.1145/3240323.3241725,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
be766b3bd83e18ed2617440c41a3a0d7262d3c64,https://www.semanticscholar.org/paper/be766b3bd83e18ed2617440c41a3a0d7262d3c64,"Performance models, design and run time management of big data applications","Nowadays the big data paradigm is consolidating its central position in the industry, as well as in society at large. 
Lots of applications, across disparate domains, operate on huge amounts of data and offer great advantages both for business and research. 
As data intensive applications (DIAs) gain more and more importance over time, it is fundamental for developers and maintainers to have the support of tools that enhance their efforts since early design stages and until run time. 
The present dissertation takes this perspective and addresses some pivotal issues with a quantitative approach, particularly in terms of deadline guarantees to ensure quality of service (QoS). 
 
Technically interesting scenarios, such as cloud deployments supporting a mix of heterogeneous applications, pose a series of challenges when it comes to predicting performance and exploiting this information for optimal design and management. 
Performance models, with their potential for what if analyses and informed design choices about DIAs, can be a major tool for both users and providers, yet they bring about a trade-off between accuracy and efficiency that may be tough to generally address. 
The picture is further complicated by the adoption of the cloud technology, which means that assessing operating costs in advance becomes harder, but also that the contention observed in data centers strongly affects big data applications’ behavior. 
For all these reasons, ensuring QoS for novel DIAs is a difficult task that needs to be addressed in order to favor further development of the field. 
 
Over this background, the present dissertation takes two main routes towards facing such challenges. 
At first we describe and discuss a number of performance models based on various formalisms and techniques. 
Among these, there are both basic models aimed at predicting specific metrics, like response time or throughput, and more specialized extensions that target the impact on big data systems of some design decisions, e.g., privacy preserving mechanisms or cloud pricing models. 
On top of this, the proposed models are variously positioned across the spectrum between efficiency and accuracy, thus enabling different trade-offs depending on the main requirements at hand. 
This is relevant in the second main part of this dissertation, where performance prediction is at the core of some formulations for capacity allocation and cluster management. 
In order to obtain optimal solutions to these problems, in one case at design time and in the other at run time, we adopt both mathematical programming and several performance models, according to the different constraints on solving times and accuracy. 
 
More in detail, we propose performance models based on queueing networks (QNs), stochastic well formed nets (SWNs), and machine learning (ML). 
This variety is justified by the different uses of each methodology. 
ML provides algebraic formulas for execution times, which are perfectly fit to be added as constraints in our optimization problems’ mathematical programming formulations, thus yielding initial solutions in closed form. 
Since ML can reliably provide accurate predictions only in regions properly explored during the training phase, the optimal solution is searched via a simulation-optimization procedure based on analytical models like QNs or SWNs, which in contrast are quite insensitive to the parameter range of evaluation, being devised from first principles. 
These kind of models boast relative errors below 10 % on average when predicting response times. 
 
In terms of optimization, first of all we consider the design time problem of capacity allocation in a cloud environment. 
The design space is explored via both ML and simulation techniques, so as to choose the best virtual machine type in the catalog offered by cloud providers and, subsequently, determine the minimum cost configuration that satisfies QoS constraints. 
We show also how this optimization approach was applied during the design phase of a tax fraud detection product developed by industrial partners, i.e., NETF Big Blu. 
Afterwards we also considered the run time issue of finding the minimum tardiness schedule for a set of jobs when the current workload exceeds predictions and the deployed capacity is not enough to ensure the agreed upon QoS. 
Thanks to the varied efficiency of performance models, it is possible to solve the design time problem in a matter of hours, whilst run time instances are solved within minutes, consistently with the different requirements.",,2018.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
8c12838c4659a9190ae7b1021515216399be1ef2,https://www.semanticscholar.org/paper/8c12838c4659a9190ae7b1021515216399be1ef2,Scaling Intelligence,"T A D 5 : 2 Intelligence without action is inert. As the solicited contributions to this volume demonstrate, actionable intelligence relies on a common ground from which architecture and allied disciplines can leverage depths and breadths of knowledge to mobilize new technologies. The Op/Positions essays examine preexisting local knowledge in historical places, enhance discovery through systems-based workflows, and foster the transformational shift from invisible smartness to holistic, design trade-offs that produce more humane and cooperative cities. As Jyoti Hosagrahar notes, place-intelligence provides current generations with a scalable and reflective framework that values the past, promotes deeper foundations, and connects resilient community design and well-being to informed decision-making. Similarly, Azam Khan posits a systems-based approach for leveraging existing knowledge to solve increasingly complex problems holistically. The emergent metaheuristic tools expand architectural design ability, enhance discovery, and yield more energy-efficient and less wasteful buildings. Norbert Streitz advocates for resetting priorities at an urban scale and generating principles that simultaneously privilege the individual and the collective. The resulting types of affordances and ethical alignments could balance data harvesting with people’s need for interactive, communicative, and cooperative spaces and places. The Research Methodology contributions critically examine a site’s latent potential and propose challenging new ways for testing and improving the lived condition at all scales. Whether at the intimate scale of one human-robot interaction or applied to industry-level protocols or full-scale testing scenarios, real-world applied research design necessitates collecting and analyzing large data sets. Jim Tørresen examines predictive intelligent system design, comprising ethical sensor data collection, robot interaction, and human-centric artificial intelligence to anticipate and respond to elderly care needs. Integrating artificial intelligence and problem-solving best practices can interactively adapt to a user’s needs and draw upon years of industry-based construction knowledge. Lukas Kirner, Elisa Lublasser, and Sigrid Brell-Cokcan developed enhanced methods for elevating existing construction industry processes through interdisciplinary collaboration, robot-assisted interaction, laboratory experimentation, factoryto-field investigation, and full-scale testing. The jump from laboratory experiments to full-scale prototyping requires the refinement of previous data exchanges and information flows to produce generalizable results. Maintaining quantitative and qualitative data research design, controlled trials, and procedural rigor requires close monitoring and comparison of real-time data collections and digital simulations. In their Details+ contribution, Jonathan Heppner and Thomas Robinson deployed intelligent testing on an innovative post-tensioned, gravity-resistant, and lateral force-resistant rocking wall system at full scale and detail level. The lab-tested results generated valuable insights into damage-resistant construction methods, informed broader building practices, and demonstrated that their previously unproven assembly could prevent massive failure and save lives. Increasing the use of enhanced digital/computational methods brings renewed attention to gaining greater control over the software and tools used to generate and validate design decisions at all scales. Re/Views addresses these issues, examines interoperable software platforms, compares gaming engines, integrates sensors, and surveys current, emerging, and projected use of autonomous robots across the AEC industry. Karen Kensek presents strategies for improving workflows and overcoming software limitations through customizable add-in solutions for existing Building Information Modeling processes. Enhanced parametric interoperability and data functionality, streamlined procedures, and verified code-compliance bolster deliberative intelligence. Christopher Morse compares game engines that combine visualization, communication, and design in robust, adaptable, flexible, real-time, and interactive environments. Immersive, customizable, connective, and cloud-based integration inform architectural research and professional practice. Peter Kerr investigates scalable interactions with technology, examining affordances and benefits of sensor nodes connected via intelligent Building Management Systems (iBMS). Alvise Simondetti, Nicholas Bachand, Aifric Delahunty, James Griffith, and Julius Sustarevas examine the unfolding paradigm shift toward autonomous robotics, artificial intelligence, and machine learning as architecture moves beyond task-specific operations to inform scalable and sustainable design that augment and complement human capabilities. As shown by these authors, in its performative function, and when viewed through the prismatic lenses of technology, architecture, and design, scaling intelligence successfully narrows the gap between empirical observation, applied research, and professional practice. Scaling Intelligence",Technology|Architecture + Design,2021.0,10.1080/24751448.2021.1967048,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
919d6334d721970a2b2b9428e16013b1e9677715,https://www.semanticscholar.org/paper/919d6334d721970a2b2b9428e16013b1e9677715,Swarms in the Third Offset,"Advances in swarm technology is part of the Department of Defense’s Third Offset Strategy which is a plan for overcoming reduced military force structure and declining technological superiority against potential U.S. adversaries. The components of the Third Offset represent the enabling capabilities of swarm behavior which could be adopted in the future force. Therefore, this paper investigates whether the U.S. military should focus greater research and development efforts on swarm-capable systems that are low-cost, numerous, unmanned, and fast. The first area of discussion includes swarm initiatives that could allow the military to transition away from expensive and heavy weapons platforms. Second, self-driving vehicles, automated logistics, and aerial drones in industry could translate to autonomous supply trains, reduced soldier error, and targeting missions in the military. Third, adversaries are pursuing swarm capabilities. While swarms show great promise, there are some major legal and ethical obstacles to swarm-capable systems. Lastly, recommendations are offered as a way ahead for swarm initiatives. Swarms in the Third Offset The fiercest serpent may be overcome by a swarm of ants. —Admiral Isoroku Yamamoto During the 2017 Super Bowl 51 half-time performance, Intel demonstrated the control of 300 drones and broke the world record a few months earlier with 500 drones controlled by a single operator. In October of 2016, the U.S. Military conducted the largest deployment ever of micro-swarms. Dubbed the Perdix micro drone, these small, inexpensive, battery-powered, propeller-driven air vehicles were launched by three F/A18 Super Hornets. Just over a year prior in 2015, the Advanced Robotic Systems Engineering Laboratory at the Naval Postgraduate School in Monterey, California, held the record with 50 simultaneous airborne unmanned aerial vehicles controlled by a single operator. This author predicts that 500 drones will quickly increase to 1,000 and 10,000 agents in just a few a year’s time while being scalable, adaptable, distributed, and collective. 5 Advances in swarm technology is part of the Department of Defense’s (DoD) Third Offset Strategy which is a plan for overcoming (“offsetting”) reduced military force structure and declining technological superiority against potential U.S. adversaries. 6 Key components of this strategy include dominance in artificial intelligence, machine learning, robotics, unmanned systems, and increased autonomy. Collectively, the components of the Third Offset represent the enabling capabilities of swarm behavior which could be wholly adopted in the future force. Therefore, should the DoD focus research and development efforts on swarm-capable ISR (intelligence, surveillance, reconnaissance) and weapons systems that are lowcost, numerous, unmanned, and fast? This paper will provide an overview of swarms and explore three major areas to address this primary question. While the promise of 2 swarm behavior appears great, there are some major obstacles to swarm-capable systems which will be presented. Lastly, recommendations will be offered as a way ahead for DoD swarm initiatives. Swarms can offer multiple operational advantages in terms of speed, intelligence gathering, coordinated effects (kinetic and non-kinetic), and efforts. Humans typically cannot control more than four to five discrete elements at a time. Swarm-based systems allow for collective and distributed control of hundreds and thousands of agents where the operator is executing mission objectives and not focused on individual agent control. Humans will maintain overall operational control but low-level decisions from individual agents within the swarm could be made autonomously to fulfill mission objectives within pre-defined rules of engagement. The first area of discussion includes swarm initiatives that could allow the DoD to transition away from expensive, heavy, and human-centric weapons platforms such as legacy tanks, manned fighters, and submarines. These systems are difficult to deploy, require large amounts of maintenance and fuel, and may be out-matched in a many operator to one vehicle paradigm. Second, the advent of self-driving vehicles, automated logistics, and aerial drones in the commercial sector could translate to autonomous supply trains, reduced soldier fatigue and error, and targeting missions in the military. Academia partnered with private industry could propel these innovations since the technology has dual-use capabilities in transportation, search and rescue, agricultural monitoring, and homeland security. The open source community has enabled these advances but has also allowed for their proliferation and use among enemy forces and terrorist organizations.",,2017.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
be219da73e66cc1f926bf90858f0fe80479202a6,https://www.semanticscholar.org/paper/be219da73e66cc1f926bf90858f0fe80479202a6,Enclave Computing Paradigm: Hardware-assisted Security Architectures & Applications,"Hardware-assisted security solutions, and the isolation guarantees they provide, constitute the basis for the protection of modern software systems. Hardware-enforced isolation of individual components reduces complexity of the overall software as well as the size and complexity of the individual components. The basic idea is that a reduction in complexity minimizes the probability of vulnerabilities in the software, thus strengthening the system's security. 
In classical system architectures, an application's security depends on the security of all privileged system entities, for example the Operating System. The Trusted Execution Environment (TEE) concept overcomes the dependence of security critical components on the systems overall security. TEEs provide isolated compartments within a single system, allowing isolated operation of a system's individual components and applications. The enclave computing paradigm enhances the TEE concept by enabling self-contained isolation of system components and applications, fulfilling the needs of modern software. It enables novel use cases by providing many parallel mutually isolated TEE-instances without the need to rely on complex privileged entities. 
The TEE solutions developed by industry and deployed in today's systems follow distinct design approaches and come with various limitations. ARM TrustZone, which is widely available in mobile devices, is fundamentally limited to a single isolation domain. Intel's TEE solution Software Guard Extensions (SGX) provides multiple mutually isolated execution environments, called enclaves. However, SGX enclaves face severe threats, in particular side-channel leakage, that can void its security guarantees. Preventing side-channel leakage from enclaves in a universal and efficient way is a non-trivial problem. Nevertheless, these deployed TEE solutions enable various novel applications. However, different TEE architectures come with diverse properties and features that require special consideration in the design of TEE applications. 
Security architectures for embedded systems face additional challenges that have not been solved, neither by industry nor by academic research. These security architectures need to be compliant with and need to preserve all functional requirements of an embedded system. Since network-connected embedded devices are increasingly used in safety critical systems, such as industrial control systems or automotive scenarios, security architectures that combine safety and security aspects are vitally needed. 
Remote Attestation (RA) is a security service that relies on the isolation guarantees of TEEs. It is of particularly high relevance for connected embedded systems. It allows trust establishment between these devices enabling their reliable collaboration in large connected systems. However, many aspects of RA, such as its scalability in large networks or its applicability in autonomous connected systems, are unexplored. 
In this dissertation, we present novel isolation architectures that bring the enclave computing paradigm to mobile and embedded platforms. We present the first security architecture for small embedded systems that provides isolated execution enclaves and real-time guarantees. Moreover, we present a novel multi-TEE security architecture for TrustZone-systems bringing the enclave computing paradigm to mobile systems, overcoming TrustZone's fundamental limitation. 
Furthermore, we deal with Intel SGX's vulnerability to side-channel attacks. We demonstrate the severity of side-channel leakage due to observable memory access patterns of SGX enclaves. To counter side-channel attacks, we present solutions that hide memory access patterns of enclaves for both accesses to enclave-external memory as well as access patterns within enclaves' private memory. 
We present two TEE-applications that follow different design approaches, leveraging the specific capabilities of Intel SGX and ARM TrustZone, respectively. We introduce a cloud-based machine learning solution that enables privacy-preserving speech recognition utilizing isolated execution enclaves. We also demonstrate the limitations of the enclave computing paradigm and show a (remote) policy enforcement solution for mobile devices, which requires an isolated execution environment with elevated privileges. 
Additionally, we investigate novel RA schemes, which tackle many important aspects of RA that are highly relevant in emerging connected systems. We develop solutions to prevent the misuse of remote attestation for Denial-of-Service (DoS) attacks and present the first efficient multi-prover attestation scheme. Furthermore, we introduce the concept of data integrity attestation, which allows the efficient and reliable collaboration of autonomous connected devices.",,2020.0,10.25534/TUPRINTS-00011912,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
42c9527670182855ec693b469e58925e84998e29,https://www.semanticscholar.org/paper/42c9527670182855ec693b469e58925e84998e29,"""How Robotics are Revolutionizing Rehabilitation""","Capitalizing on the new understanding of brain plasticity, we introduced a paradigm shift in clinical practice in 1989 when we initiated the development of the MIT-Manus robot for neuro-rehabilitation and deployed it into the clinic. Since then we collected evidence to support the potential of enhancing and augmenting recovery following a stroke, first during the sub-acute and then the chronic phase. Our efforts and that of others led to the endorsements starting in 2010 from the American Heart Association, the American Stroke Association, and the Veterans Administration for the use of rehabilitation robots for the Upper Extremity, but not yet for the Lower Extremity. AHA recommendations were the same in the 2016 revision. Furthermore, it was demonstrated in the VA system that upper extremity robotic therapy has an economic advantage over manual therapy. More recently we completed a pragmatic study RATULS under the auspices of the National Health Service of the United Kingdom and its NIHR Health Technology Assessment Programme, which enrolled 770 stroke patients. Thus, we have developed novel robotic treatment and evaluation tools and have managed to collect the experimental evidence that demonstrates the unequivocal therapeutic benefits stemming from robot-aided rehabilitation for the upper extremity as well as present shortcomings. This talk will present an overview of our past rehabilitation robotics efforts and more recent efforts addressing the identified shortcomings. ""Novel Biomarkers: Robotics and Machine Learning "" Hermano Igo Krebs, PhD Abstract: In stroke, we demonstrated that robotic devices promoted upper extremity motor recovery. Those studies raised new questions focused on patients who were mildly or completely resistant to therapy, i.e., patients who did not improve, and prompted the hypothesis In stroke, we demonstrated that robotic devices promoted upper extremity motor recovery. Those studies raised new questions focused on patients who were mildly or completely resistant to therapy, i.e., patients who did not improve, and prompted the hypothesis that we could predict who are the responders, quasi-responders, and non-responders to behavioral therapy. There have been other attempts to create biomarkers to predict outcomes employing clinical scales such as the Fugl-Meyer assessment, the neurologic sensory exam, functional impairment scales, neurophysiology and neuro-imaging analysis; but these attempts have had mixed results and these measures are seldom used in practice to optimize therapy. To understand the variability of recovery, we examined the data collected with the robotic group on a recently completed studies. We investigated the potential for building a more sensitive biomarker, composed of robotic measurements collected during evaluation and training, to analyze the performance of patients recovering from stroke and to predict who will respond to movement-based treatment and who will not. We hypothesize that kinematic and kinetic measurements can predict the response to behavioral therapy in stroke and also determine how to optimize care for a particular patient. Here we will discuss our attempts to employ both linear and non-linear approaches and ascertain the correlation levels between our robot-based biomarker and clinical scales. We will discuss robot biomarker effect-size and compare it to clinical scales to determine whether there are some noticeable efficiencies in using the robotassay instead of the clinical scales. We will also present our efforts in developing an expert algorithm that employs data collected during the baseline assessment and during two consecutive training sessions in order to predict patient outcomes as well as to determine patterns of improvement in stroke patients so as to build an alternative machine learning predictor of outcomes. ""Starting a Venture Company"" Hermano Igo Krebs, PhD Abstract: “Imagine being present at the birth of a new industry. . . trends are now starting to converge and I can envision a future in which robotics devices will become a nearly ubiquitous part of our day-to-day lives. Technologies such as distributed computing, voice and visual recognition, and wireless broadband connectively will open the door to a new generation of autonomous devices that enable computers to perform tasks in the physical world on our behalf. We may be on the verge of a new era, when the PC will get up off the desktop and allow us to see, hear, touch and manipulate objects in places where we are not physically present.” Bill Gates Disruptive technology is a term coined to characterize an innovation that disrupts an existing market or way of doing things and creates a new value network. The concept was first described at Harvard Business School by Clayton M. Christensen, who described the concept in 1996 as: ""Generally, disruptive innovations were technologically straightforward, consisting of off-theshelf components put together in a product architecture that was often simpler than prior approaches. They offered less of what customers in established markets wanted and so could rarely be initially employed there. They offered a different package of attributes valued only in emerging markets remote from, and unimportant to, the mainstream."" Eventually with improvement, borrowing from Malcolm Gladwell, the moment of critical mass, the threshold, the boiling point is reached and the old practices and existing value network is abandoned in favor of the new one. Here I will discuss my experience as an entrepreneur and whether rehabilitation robotics has achieved its “tipping point.” “Imagine being present at the birth of a new industry. . . trends are now starting to converge and I can envision a future in which robotics devices will become a nearly ubiquitous part of our day-to-day lives. Technologies such as distributed computing, voice and visual recognition, and wireless broadband connectively will open the door to a new generation of autonomous devices that enable computers to perform tasks in the physical world on our behalf. We may be on the verge of a new era, when the PC will get up off the desktop and allow us to see, hear, touch and manipulate objects in places where we are not physically present.” Bill Gates Disruptive technology is a term coined to characterize an innovation that disrupts an existing market or way of doing things and creates a new value network. The concept was first described at Harvard Business School by Clayton M. Christensen, who described the concept in 1996 as: ""Generally, disruptive innovations were technologically straightforward, consisting of off-theshelf components put together in a product architecture that was often simpler than prior approaches. They offered less of what customers in established markets wanted and so could rarely be initially employed there. They offered a different package of attributes valued only in emerging markets remote from, and unimportant to, the mainstream."" Eventually with improvement, borrowing from Malcolm Gladwell, the moment of critical mass, the threshold, the boiling point is reached and the old practices and existing value network is abandoned in favor of the new one. Here I will discuss my experience as an entrepreneur and whether rehabilitation robotics has achieved its “tipping point.”",,2021.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
c1e231a7694f0c51cbf15924aed10fad578358e6,https://www.semanticscholar.org/paper/c1e231a7694f0c51cbf15924aed10fad578358e6,Novel applications of soft computing techniques for industrial and environmental enterprises,"This special issue compiles recent applications of soft-computing techniques for managing enterprises within industrial and environmental sectors. It is aimed at both researchers and practitioners from academia and industry who are engaged in deploying soft-computing solutions for real-life enterprise problems. Six papers are included in this special issue, covering a wide variety of case studies, ranging from conversational systems to vehicle routing problems. Additionally, a variety of soft-computing techniques have been applied, such as deep learning, random forest, particle swarm optimization, clustering, and metaheuristics among others. Thanks to the wide-range panoramic view presented by these complementary works, readers can get a feel for developing up-to-date soft-computing systems to solve present problems in enterprise contexts. In the first contribution by Cárdenas-Montes, recent machine learning models are applied for finding anomalies in Ozone-urban images generated from an Air Quality Monitoring Network. To do so, deep learning with maps-based data coming from an air-quality sensor network is applied. Differentiating from previous work, time series are not individually handled, but they are used for generating maps. This is carried out thanks to advances in computer vision to process the data. Besides, the manuscript contributes to the outlier detection problem, investigating the most appropriate map-generation technique for different scientific targets. The author's proposal is compared to the direct use of DBSCAN on the intensity of the pixels, as well as other approaches for learning latent variables, such as autoencoders based on Convolutional Neural Networks (undercomplete and denoising). The proposed approach achieves high performance when labelling maps with anomalies at a local and a global scale, including the outliers identified by the direct use of DBSCAN on the intensity of the pixels. By using the author's approach, anomalous measurements in a few monitoring stations are also detected. López-Sánchez et al. propose a new method to solve a bi-objective variant of the Vehicle Routing Problem, taking into account its industrial and environmental implications. More precisely, the transportation of products by industrial enterprises to their customers is addressed, in which has been named the Periodic Vehicle Routing Problem with Service Choice (PVRP-SC) problem. It is defined as the problem of finding a set of routes for each vehicle and each day over a planning horizon in order to minimize total travel cost minus service benefit while satisfying operational constraints. The authors approach PVRP-SC from a novel perspective; instead of considering the problem as a single-objective optimization problem, they have solved the problem as a bi-objective optimization one, that minimizes the total emissions produced by all vehicles and maximizes the service quality measured as the number of times that a customer is visited by a vehicle in order to be served. To do that, a multi-objective approach based on Multi-Start Local Searches is applied. According to the experimental setup that has been carried out, the proposed algorithm outperforms other standard alternatives. The next contribution by Burkart et al. aims to examine how participants, supported by a Decision Support System, revise their initial prediction by four different treatments in a between-subject design study. This research work examines how prediction revision (adjusting an initial prediction on the basis of new information) is affected by the provided explanations. The four treatments (interpretable regression model, Random Forest, black box with a local explanation, and black box with a global explanation) differ in the degree of explainability in order to understand the system decisions. Participants in each treatment were told basic facts about their corresponding model and how each is used for producing a response when presented with a set of predictor values. Amazon Mechanical Turk with Sophie Labs was used as a platform for designing online experiments, in which 80 participants were recruited and randomly assigned to the experiments. The obtained results show that all participants improved their predictions after receiving advice whether it was a complete black box or a black box with an explanation. Authors conclude that naturally interpretable models were not incorporated more heavily in the decision process than black box models and explanations from them. Bayram et al. propose an anomaly detection system based on acoustic signals, especially required to quickly detect and interfere with the abnormal events during the industrial processes. More precisely, the authors propose a real time Acoustic Anomaly Detection System employing sequence-to-sequence Autoencoder models. The proposed processing pipeline uses the audio features extracted from the streaming audio signal captured by a single-channel microphone. The reconstruction error generated by the Autoencoder models is calculated to measure the degree of abnormality of the sound event. The performance of Convolutional Long Short-Term Memory Autoencoder is evaluated and compared with sequential Convolutional Autoencoder using sounds captured from various industrial manufacturing processes. The experimental setup comprises different Signal-to-Noise Ratio (SNR) conditions of sound events such as an explosion, fire, and glass breaking. Four acoustic datasets have been analyzed, containing sounds associated to the following industrial activities: painting, cutting, welding, and robotic arm operation. In the validation that was carried out with the real time system, it is shown that the Long Short-Term Memory version of the Autoencoder outperforms the other version of such model according to the AUC (Area Under the ROC Curve) metric. Received: 5 October 2020 Accepted: 7 October 2020",Expert Syst. J. Knowl. Eng.,2020.0,10.1111/exsy.12654,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
7724522f88daa3d208ac1a7270125e6f870b6920,https://www.semanticscholar.org/paper/7724522f88daa3d208ac1a7270125e6f870b6920,Successful Development and Deployment of a Global ROP Optimization Machine Learning Model,"
 Drilling rate of penetration (ROP) is a major contributor to drilling costs. ROP is influenced by many different controllable and uncontrollable factors that are difficult to distinguish with the naked eye. Thus, machine learning (ML) models such as neural networks (NN) have gained momentum in the drilling industry. Existing models were either field-based or tool-based, which impacted the accuracy outside of the trained field. This work aims to develop one generally applicable global ROP model, reducing the effort needed to re-develop models for every application.
 A drilling dataset was gathered from exploration and development wells in both onshore and offshore operations from a variety of fields and regions. The wells were curated to have different water depths, down hole drive such as Rotary Steerable System (RSS), PDM, Standard Rotary, bit types (Mill Tooth, TCI, PDC) and inclinations (vertical or deviated). A deep neural network was used for modelling the relationship between ROP and inputs taken from real-time surface data, such as Torque, Weight-on-Bit (WOB), rotary speed (RPM), flow and pressure measurements. The performance of the ROP model was analyzed using historical data via summary statistics such as Mean Absolute Percentage Error, as well as graphical results such as residuals distributions, cumulative distribution functions of errors, and plots of ROP vs depth for independent holdout testing wells not included in the model fitting process. Analysis was done both in aggregate, and for each specific well.
 The ROP model was demonstrated to generalize effectively in all cases, with only minor increases in error metrics for the holdout test wells, where the Mean Absolute Percentage Error averaged across wells was ~20%, compared to 17.5% averaged across training wells. Furthermore, residuals distributions were centered close to zero, indicating low systematic error. This work proves the case for a ""global"" ROP prediction model applicable ""out-of-the-box"" to a broad set of drilling operations.
 A global ROP model has the potential to eliminate learning curves, reducing time and costs associated with having to develop a new model for every field. Furthermore, a model that effectively captures the relationships between parameters controllable by drillers and ROP can be used for automatically identifying drilling parameters that improve ROP. Preliminary field-testing of the ROP optimization system yielded positive results, with many examples of increased ROP realized after following drilling parameter recommendations provided by the software.","Day 2 Wed, March 23, 2022",2022.0,10.4043/31680-ms,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
4cb659cf5a8dea8b86263c65ec613f48c2426c51,https://www.semanticscholar.org/paper/4cb659cf5a8dea8b86263c65ec613f48c2426c51,"Analyzing Dilemmas Posed by Artificial Intelligence and 4IR Technologies Requires using all Available Models, Including the Existing International Human Rights Framework and Principles of AI Ethics","We are living in the epoch referred to as the ‘4th industrial revolution'. The 4th industrial Revolution (4IR) is a development characterized by a fusion of technologies that blur the digital, physical, and biological spheres (e.g., cyberspace, virtual and augmented reality, body-machine interface and robotics). 
 
Certain is the guaranteed ubiquitous adoption of these technologies, and futurism. Where the former is a reference to the increasing use and normalization of such technologies in everyday life, government service provision and industry. The latter is a reference to the philosophical/science fiction discussions that are emerging as a result of these changes (e.g. debates around the ‘singularity’, transhumanism, and posthumanism – often presented in utopian/dystopia terms). As such, the definition of digital ethics can be expanded and expressed in terms of the impacts of new digital technologies, through analysis of potential opportunities and risks in contemporary and future contexts. 
 
Many are working on forward‑looking policy frameworks and governance protocols, with broad multistakeholder engagement and buy‑in, to accelerate the adoption of emerging technologies in the global public interest, such as artificial intelligence (AI) and machine learning (ML) blockchain, 5G, data analytics, quantum computing, autonomous vehicles, synthetic biology, the internet of things (IoT), and killer robots or autonomous weapons systems (AWS). We have gained insight into the unequal distribution of the positive and negative impacts of AI on human rights throughout society, and have begun to explore the power of the human rights framework to address these disparate impacts. 
 
Although internationally recognized laws and standards on human rights provide a common standard of achievement for all people in all countries, more work is needed to understand how they can be best applied in the context of disruptive technology. 
 
AI systems raise myriad questions for society and democracy, only some of which are covered or addressed by existing laws. In order to fill these perceived gaps, a vocal group of governments, industry players, academics, and civil society actors have been promoting principles or frameworks for ethical AI. 
 
COVID-19 accelerated the use of AI in all countries and all fields. The pandemic accelerated the transition to a society that is increasingly based on the use of AI. This also increased the threats new risks related to human rights in the context of AI deployment. The human rights implications of governments' aggressive measures targeting the spread of COVID-19-related misinforation is also discussed. 
 
The question of whether corporations can act ethically is particularly relevant for Big Tech. Many of these firms are oligopolies that individuals and governments alike depend on completely, though they have little to no capacity to independently remedy issues when they arise, as Project Maven showed. Artificial intelligence and automated decision-making tools are increasing in power and centrality, and technology companies retain large troves of private data that it sells. These companies are at the forefront of technological innovation and may be caught up with the factual question of what can be done rather than the normative question of whether it should be done. All these issues arise in a field where there is little to no government regulation or intervention. The threats AI poses to society are so new, that the legal system is struggling to impose sufficient values and restrictions. Thus, a coherent approach to addressing AI ethics, values and consequences is, indeed, urgently needed. 
 
In May 2019, 42 countries adopted the Organization for Economic Co-operation and Development (OECD) AI Principles, a legal recommendation that includes five principles and five recommendations related to the use of AI. To ensure the successful implementation of the Principles, the OECD launched the AI Policy Observatory in February 2020. The Observatory publishes practical guidance about how to implement the AI Principles, and supports a live database of AI policies and initiatives globally. It also compiles metrics and measurement of global AI development and uses its convening power to bring together the private sector, governments, academia, and civil society. 
 
The AI ethics and governance initiatives discussed are cause for optimism that the global community will use all available models and brainpower for analysis and ultimately global governance of AI.",SSRN Electronic Journal,2021.0,10.2139/ssrn.3874279,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
0b0a5baa1a4e9e904715502f0505af1eb2687fd1,https://www.semanticscholar.org/paper/0b0a5baa1a4e9e904715502f0505af1eb2687fd1,Impact of artificial intelligence in healthcare,"Artificial intelligence (AI) is prepared to become a transformational force in healthcare. From chronic diseases and cancer to radiology and risk assessment, there are nearly endless opportunities to influence technology to install more precise, efficient, and impactful interventions at exactly the right moment in a patient’s care.AI offers a number of benefits over traditional analytics and clinical decision-making techniques.  Learning algorithms can become more specific and accurate as they interact with training data, allowing humans to gain unique insights into diagnostics, care processes, treatment variability, and patient outcomes (1).   
  
Using computers to communicate is not a new idea by any means, but creating direct interfaces between technology and the human mind without the need for keyboards, mice, and monitors is a cutting-edge area of research that has significant applications for some patients. Neurological diseases and trauma to the nervous system can take away some patients’ abilities to speak, move, and interact meaningfully with people and their environments.  Brain-computer interfaces (BCIs) backed by artificial intelligence could restore those fundamental experiences to those who feared them lost forever. Brain-computer interfaces could drastically improve quality of life for patients with ALS, strokes, or locked-in syndrome, as well as the 500,000 people worldwide who experience spinal cord injuries every year (2). 
  
Radiological images obtained by MRI machines, CT scanners, and x-rays offer non-invasive visibility into the inner workings of the human body.  But many diagnostic processes still rely on physical tissue samples obtained through biopsies, which carry risks including the potential for infection. AI will enable the next generation of radiology tools that are accurate and detailed enough to replace the need for tissue samples in some cases, experts predict. Diagnostic imaging team with the surgeon and the pathologist can be brought together which will be a big challenge (3). 
  
Succeeding in the pursuit may allow clinicians to develop a more accurate understanding of how tumours behave as a whole instead of basing treatment decisions on the properties of a small segment of the malignancy. Providers may also be able to better define the aggressiveness of cancers and target treatments more appropriately. Artificial intelligence is helping to enable “virtual biopsies” and advance the innovative field of radiomics, which focuses on harnessing image-based algorithms to characterize the phenotypes and genetic properties of tumours (1). 
  
Shortages of trained healthcare providers, including ultrasound technicians and radiologists can significantly limit access to life-saving care in developing nations around the world. AI could help mitigate the impacts of this severe deficit of qualified clinical staff by taking over some of the diagnostic duties typically allocated to humans (4). 
  
For example, AI imaging tools can screen chest x-rays for signs of tuberculosis, often achieving a level of accuracy comparable to humans.  This capability could be deployed through an app available to providers in low-resource areas, reducing the need for a trained diagnostic radiologist on site. 
  
However, algorithm developers must be careful to account for the fact that different ethnic groups or residents of different regions may have unique physiologies and environmental factors that will influence the presentation of disease.The course of a disease and population affected by the disease may look very different in India than in the US. As these algorithms are being developed,  it is very important to make sure that the data represents a diversity of disease presentations and populations. we cannot just develop an algorithm based on a single population and expect it to work as well on others (1). 
  
Electronic health records (EHRs) have played an instrumental role in the healthcare industry’s journey towards digitalization, but the switch has brought myriad problems associated with cognitive overload, endless documentation, and user burnout. EHR developers are now using AI to create more intuitive interfaces and automate some of the routine processes that consume so much of a user’s time. Users spend the majority of their time on three tasks: clinical documentation, order entry, and sorting through the in-basket (5). 
  
Voice recognition and dictation are helping to improve the clinical documentation process, but natural language processing (NLP) tools might not be going far enough. Video recording a clinical encounter would be helpful while using AI and machine learning to index those videos for future information retrieval. And it would be just like in the home, where we are using Siri and Alexa.  The future will bring virtual assistants to the bedside for clinicians to use with embedded intelligence for order entry(5). AI may also help to process routine requests from the inbox, like",Biomedicine,2021.0,10.51248/.v41i3.1190,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
0878f21553a04ae10ed1a3e55ddf4091e6238df2,https://www.semanticscholar.org/paper/0878f21553a04ae10ed1a3e55ddf4091e6238df2,ICIN 2020 Program,"For nearly fifty years, beginning with Kleinrock's pioneering work on using queueing theory to model packet flows in communication networks, network modeling has adopted the individual packet as primary level of granularity for network modeling and analysis. With the advent of terabit-switching capabilities, information-centric networking, and data centers with complex workloads and hundreds of thousands of components, the time would seem ripe to raise the level of abstraction beyond the packet. In this talk, we identify higher-level modeling abstractions are already proving useful as well as new needed abstractions. But also we identify cases where packet-level models are still crucial in providing important insights. Wednesday, February 26 10:00 11:00 K2: Keynote 2 Softwarization and IoT evolution Lefteris Mamatas (University of Macedonia, Greece) Room: La Grande Scène Chair: Alex Galis (University College London (UCL), United Kingdom (Great Britain)) Abstract: The Internet of Things (IoT), a main enabler for Industry 4.0, is considered as a system connecting myriads of people, things and services. IoT enables new large-scale applications with diverse constraints (e.g., limited resource availability or mobility) and requirements (e.g., ultra low delays). A main challenge is the evolution beyond large networks of sensing devices to multiple cooperating network deployments that implement context-sensitive communication and cloud processing strategies, through the seamless adoption of Softwarization technologies. The talk includes the following aspects: (i) a motivation of the above vision with two novel use-cases on smart-city and maritime contexts; (ii) a discussion on the evolutionary and clean-slate approaches to the IoT Softwarization; (iii) the missing elements and open issues in Software-Defined IoT and Edge Cloud technologies; and (iv) insights from our practical experience in relevant implementations and real experiments. The Internet of Things (IoT), a main enabler for Industry 4.0, is considered as a system connecting myriads of people, things and services. IoT enables new large-scale applications with diverse constraints (e.g., limited resource availability or mobility) and requirements (e.g., ultra low delays). A main challenge is the evolution beyond large networks of sensing devices to multiple cooperating network deployments that implement context-sensitive communication and cloud processing strategies, through the seamless adoption of Softwarization technologies. The talk includes the following aspects: (i) a motivation of the above vision with two novel use-cases on smart-city and maritime contexts; (ii) a discussion on the evolutionary and clean-slate approaches to the IoT Softwarization; (iii) the missing elements and open issues in Software-Defined IoT and Edge Cloud technologies; and (iv) insights from our practical experience in relevant implementations and real experiments. Wednesday, February 26 11:30 12:30 TS3: Network Slicing Room: La Grande Scène Chair: Prosper Chemouil (Orange Labs (retired), France) TS3.1 A Lightweight Policy-aware Broker for Multi-domain Network Slice Composition Xuan-Thuy Dang (Technische Universität Berlin & DAI Labor, Germany); Fikret Sivrikaya (GT-ARC gGmbH & Technische Universität Berlin, Germany) TS3.2 Enhancing the performance of 5G slicing operations via multi-tier orchestration Miquel Puig Mena (i2cat Foundation, Spain); Apostolos Papageorgiou, Leonardo Ochoa-Aday and Muhammad Shuaib Siddiqui (Fundació i2CAT, Internet i Innovació Digital a Catalunya, Spain); Gabriele Baldoni (ADLINK Technology, France) TS3.3 An Efficient Online Heuristic for Mobile Network Slice Embedding Katja Ludwig (University of Augsburg, Germany); Andrea Fendt (Nokia Bell Labs & University of Augsburg, Germany); Bernhard Bauer (University of Augsburg, Germany) Wednesday, February 26 12:30 13:00 DPS: Demo/Poster ""Elevator Pitch"" Session Room: La Grande Scène Chair: Prosper Chemouil (Orange Labs (retired), France) DPS.1 A QUIC-based proxy architecture for an efficient hybrid backhaul transport Michele Luglio and Mattia Quadrini (University of Rome Tor Vergata Dip. Ing. Elettronica, Italy); Cesare Roseti and Francesco Zampognaro (University of Rome Tor Vergata, Italy); Simon Pietro Romano (University of Napoli Federico II, Italy) DPS.2 A Blockchain-based Brokerage Platform for Fog Computing Resource Federation Marco Savi, Daniele Santoro, Katarzyna Di Meo and Daniele Pizzolli (Fondazione Bruno Kessler, Italy); Miguel Pincheira (OpenIoT Research Area, FBK CREATE-NET & University of Trento, Italy); Raffaele Giaffreda (FBK CREATE-NET, Italy); Silvio Cretti (Fondazione Bruno Kessler, Italy); Seung-woo Kum (Korea Electronics Technology Institute, Korea (South)); Domenico Siracusa (Fondazione Bruno Kessler, Italy) DPS.3 Optimized Network Slicing Proof-of-Concept with Interactive Gaming Use Case José J Alves Esteves, Jr. (Orange Labs & Sorbonne Université, France); Amina Boubendir and Fabrice M. Guillemin (Orange Labs, France); Pierre Sens (Université de Paris 6, France) DPS.4 A Deployable Containerized 5G Core Solution for Time Critical Communication in Smart Grid Van Giang Nguyen, Karl-Johan Grinnemo, Javid Taheri and Anna Brunstrom (Karlstad University, Sweden) DPS.5 FogGuru: a Fog Computing platform based on Apache Flink Davaadorj Battulga (University of Rennes 1 & U-Hopper, Italy); Daniele Miorandi (U-Hopper, Italy); Cedric Tedeschi (University of Rennes I / INRIA, France) DPS.6 5G Experimentation Framework: Architecture Specifications, Design and Deployment Louiza Yala (Orange Labs, France); Sihem Cherrared (University of Rennes 1 & Orange Labs and INRIA, France); Grzegorz Panek (Orange Polska, Poland); Sofiane Imadali and Ayoub Bousselmi (Orange Labs, France) DPS.7 A New Service Management Framework for Vehicular Networks Jose Ramirez, Onyekachukwu Augustine Ezenwigbo, Gayathri Karthick and Ramona Trestian (Middlesex University, United Kingdom (Great Britain)); Glenford E Mapp (MIddlesex University & Cantego Limited, United Kingdom (Great Britain)) DPS.8 Creating trust in automation in intent-based mobile network management Ville Vartiainen (Aalto University, Finland); Dmitry Petrov and Vilho Räisänen (Nokia Bell Labs, Finland) DPS.9 Interoperable and discrete eHealth Data Exchange between Hospital and Patient Andreea Ancuta Corici, Olaf Rode, Ben Kraufmann, Andreas Billig, Jörg Caumanns and Markus Deglmann (Fraunhofer FOKUS, Germany); Viktoria Walter, Janina Rexin and Gunther Nolte (Vivantes Netzwerk für Gesundheit GmbH, Germany) Wednesday, February 26 14:00 15:00 K3: Keynote 3 Network Operations and AI Rafia Inam (Ericsson, Sweden) Room: La Grande Scène Chair: Diego Lopez (Telefonica I+D, Spain) Abstract: The Fifth Generation Mobile Networks (5G) are seen as a key enabler for diverse-natured industry verticals (such as automotive, manufacturing, mining, utility, health, etc.) by providing a platform to support heterogeneous sets of network quality requirements. The presentation will discuss how Artificial Intelligence and automation can support Telecom industry to manage the increased complexity, scalability, and diversity in its use cases. The work presents different aspects of the network operations of the future, done in an automated, proactive, and intent-driven fashion using different AI techniques. The Fifth Generation Mobile Networks (5G) are seen as a key enabler for diverse-natured industry verticals (such as automotive, manufacturing, mining, utility, health, etc.) by providing a platform to support heterogeneous sets of network quality requirements. The presentation will discuss how Artificial Intelligence and automation can support Telecom industry to manage the increased complexity, scalability, and diversity in its use cases. The work presents different aspects of the network operations of the future, done in an automated, proactive, and intent-driven fashion using different AI techniques. Wednesday, February 26 15:00 16:10 TS4: Improving Service Performance Room: La Grande Scène Chair: Amina Boubendir (Orange Labs, France) TS4.1 Multimedia Service Management with Virtualized Cache Migration Reza Shokri Kalan (Ege UniversityTurkey, Turkey); Muge Sayit (Ege University, Turkey); Stuart Clayman (University College London (UCL), United Kingdom (Great Britain)) TS4.2 Proposal of Profile and Event Sharing by Agent Communication Masafumi Katoh (Fujitsu Labotatories Ltd., Japan); Tomonori Kubota and Eiji Yoshida (Fujitsu Laboratories, Japan); Yuji Kojima (Fujitsu Limited, Japan); Yuuichi Yamagishi (FUJITSU LIMITED, Japan) TS4.3 Double Mask: An Efficient Rule Encoding for Software Defined Networking Ahmad Abboud (University of Lorraine, France); Abdelkader Lahmadi (INRIA Nancy Grand Est, France); Michael Rusinowitch (INRIA Nancy-Grand Est, France); Miguel Couceiro (University of Lorraine, France); Adel Bouhoula (Higher School of Communication of Tunis & University of Carthage, Tunisia); Mondher Ayadi (Numeryx, France) Wednesday, February 26 16:35 18:00 TS5: Network Security Room: La Grande Scène Chair: Ved P. Kafle (National Institute of Information and Communications Technology, Japan) TS5.1 Neural network based anomaly detection for SCADA systems Lenhard Reuter (AIT Austrian Institute of Technology, Austria); Oliver Jung (AIT Austrian Institute of Technology GmbH, Austria); Julian Magin (AIT Austrian Institute of Technology, Austria) TS5.2 DDoS Detection System Using Feature Selection and Machine Learning Algorithms in a Distributed System Amjad Alsirhani (Dalhousie University, Faculty of Computer Science & Canada, Canada); Geetanshu Grover and Srinivas Sampalli (Dalhousie University, Canada); Peter Bodorik (Dalhousie University, Faculty of Computer Science, Canada) TS5.3 Configuration of the Detection Function in a Distributed IDS Using Game Theory Clement Weill (Institut Polytechnique de Paris & CEA LIST, France); Alexis Olivereau (CEA, LIST, France); Djamal Zeghlache (Insti","2020 23rd Conference on Innovation in Clouds, Internet and Networks and Workshops (ICIN)",2020.0,10.1109/icin48450.2020.9059372,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
71b2e5ad0a259da10e61e97ac8f8ce1aa1019a5b,https://www.semanticscholar.org/paper/71b2e5ad0a259da10e61e97ac8f8ce1aa1019a5b,"Current Issue: November 2018, Volume 06, Number 3 --- Table of Contents","Software-defined networking (SDN) is reshaping the networking paradigm. Previous research shows that SDN has advantages over traditional networks because it separates the control and data plane, leading to greater flexibility through network automation and programmability. Small business and academia networks require flexibility, like service provider networks, to scale, deploy, and self-heal network infrastructure that comprises of cloud operating systems, virtual machines, containers, vendor networking equipment, and virtual network functions (VNFs); however, as SDN evolves in industry, there has been limited research to develop an SDN architecture to fulfil the requirements of small business and academia networks. This research proposes a network architecture that can abstract, orchestrate, and scale configurations based on academia and small business network requirements. Our results show that the proposed architecture provides enhanced network management and operations when combined with the network orchestration application (NetO-App) developed in this research. The NetO-App orchestrates network policies, automates configuration changes, secures container infrastructure, and manages internal and external communication between the campus networking infrastructure. We develop a new approach for distributed computing of the association rules of high confidence on the attributes/columns of a binary table. It is derived from the D-basis algorithm developed by K.Adaricheva and J.B.Nation (Theoretical Computer Science, 2017), which runs multiple times on sub-tables of a given binary table, obtained by removing one or more rows. The sets of rules retrieved at these runs are then aggregated. This allows us to obtain a basis of association rules of high confidence, which can be used for ranking all attributes of the table with respect to a given fixed attribute. This paper focuses on some algorithmic details and the technical implementation of the new algorithm. Results are given for tests performed on random, synthetic and real data. This study increased the overall knowledge of job satisfaction among non-tenured IT faculty by way of contributing to the body of management knowledge in the IT environment. The study results provided higher education institution IT leaders and management the vision and understanding to handle job satisfaction issues within the IT environment. This information is also crucial in helping higher education institutions perform at high levels of employee retention, flexibility, and employee job satisfaction by focusing on autonomy and the opportunity for advancement. This quantitative research examined the relationship between (a) the extrinsic motivators (predictor/independent variables), operationalized as autonomy; (b) the intrinsic motivators, operationalized as advancement opportunities; and (c) the job satisfaction level of IT faculty in higher educational institutions (dependent variable). This research has added to the body of knowledge regarding what is needed for IT educators to have job satisfaction by examining how much the independent variables affect job satisfaction of IT faculty in higher educational institutions. We have defined and analyzed methods and practices that companies could apply as they formulate the essential skills and resources for predicting job satisfaction among IT faculty. In this area of job satisfaction, additional understanding could support higher education institutions learning how to keep experienced IT faculty. To examine the extent to which autonomy and opportunity for advancement predict job satisfaction, a multiple linear regression was conducted. This study added to the existing body of knowledge regarding what is needed for IT educators to have job satisfaction by examining how much the independent variables of opportunity for advancement and autonomy affect job satisfaction of IT faculty in higher educational institutions. Additional knowledge in this area of job satisfaction may support higher educational institutions in learning how to retain experienced IT faculty. By addressing these factors related to job satisfaction, employers can better understand what motivates and keeps IT workers satisfied in their jobs. Keeping IT employees satisfied will help retain them and prevent them from job-hopping",,,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
6e0cdea54f6ef68adb7870b0777eaa1bc44de42b,https://www.semanticscholar.org/paper/6e0cdea54f6ef68adb7870b0777eaa1bc44de42b,2021 ACTIVITY REPORT Project-Team DIVERSE Diversity-centric Software Engineering,"modify the behavior or non-functional properties of a software. Deep software variability calls to investigate how to systematically handle cross-layer conﬁguration. The diversiﬁcation of the different layers is also an opportunity to test the robustness and resilience of the software layer in multiple environments. Another interesting challenge is to tune the software for one speciﬁc executing environment. In essence, deep software variability questions the generalization of the conﬁguration knowledge. Functional Description: Familiar is an environment for large-scale product customisation. From a model of product features (options, parameters, etc.), Familiar can automatically generate several million variants. These variants can take many forms: software, a graphical interface, a video sequence or even a manufactured product (3D printing). Familiar is particularly well suited for developing web conﬁgurators (for ordering customised products online), for providing online comparison tools and also for engineering any family of embedded or software-based products. Functional Description: PIT and Descartes are mutation testing systems for Java applications, which allows you to verify if your test suites can detect possible bugs, and so to evaluate the quality of your test suites. They evaluate the capability of your test suite to detect bugs using mutation testing (PIT) or extreme mutation testing (Descartes). Mutation testing does it by introducing small changes or faults into the original program. These modiﬁed versions are called mutants. A good test suite should able to kill or detect a mutant. Traditional mutation testing works at the instruction level, e.g., replacing "">"" by ""<="", so the number of generated mutants is huge, as the time required to check the entire test suite. That’s why Extreme Mutation strategy appeared. In Extreme Mutation testing, the whole body of a method under test is removed. Descartes is a mutation engine plugin for PIT which implements extreme mutation operators. Both provide reports combining, line coverage, mutation score and list of weaknesses in the source. reasoning, and (iii) the adaptations and the predictive model of their impact on the trade-off. We use this framework to build three languages with self-adaptive virtual machines and discuss the relevance of the abstractions, effectiveness of correctness envelopes, and compare their code size and performance results to their manually implemented counterparts. We show that the framework provides suitable abstractions for the implementation of self-adaptive operational semantics while introducing little performance overhead compared to a manual implementation. change propagation. This work leverages classical inconsistency repair mechanisms to explore the vast search space of change propagation. Our approach not only suggests changes to repair a given inconsistency but also changes to repair inconsistencies caused by the aforementioned repair. In doing so,our approach follows the developer’s intent where subsequent changes may not contradict or backtrack earlier changes. We argue that consistent change propagation is essential for effective model driven engineering. Our approach and its tool implementation were empirically assessed on 18 case studies from industry, academia, and GitHub to demonstrate its feasibility and scalability. A comparison with two versioned models shows that our approach identiﬁes actual repair sequences that developers that categorize experiments belonging to either safe or failure-prone states. We apply ChaT to a video streaming application use case. The simulation results show the effectiveness of ChaT to achieve our goals: identifying execution classes and detecting failure-prone experiments based on metamorphic relationships with high level of statistical scores. two standards and we provide TOSCA Studio, a model-driven tool chain for TOSCA that conforms to OCCI. TOSCA Studio allows to graphically design cloud applications as well as to deploy and manage them at runtime using a fully model-driven cloud orchestrator based on the two standards. Our contribution is validated by successfully transforming and deploying three cloud applications: WordPress, Node Cellar and Multi-Tier. These achievements constitute basic blocks upon which we will continue building our research and systems, extending for example the applicability to secure supply chains. • Abstract: The aim of the Falcon project is to investigate how to improve the resale of available resources in private clouds to third parties. In this context, the collaboration with DiverSE mainly aims at working on efﬁcient techniques for the design of consumption models and resource consumption forecasting models. These models are then used as a knowledge base in a classical autonomous loop. • Abstract: The GLOSE project develops new techniques for heterogeneous modeling and simulation in the context of systems engineering. It aims to provide formal and operational tools and methods to formalize the behavioral semantics of the various modeling languages used at system-level. These semantics will be used to extract behavioral language interfaces supporting the deﬁnition of coordination patterns. These patterns, in turn, can systematically be used to drive the coordination of any model conforming to these languages. The project is structured according to the following tasks: concurrent xDSML engineering, coordination of discrete models, and coordination of discrete/continuous models. The project is funded in the context of the network DESIR, and supported by the GEMOC initiative. The use case chosen for the demonstrator is the high-level description of a remote control drone system, whose the main objective is to illustrate the design and simulation of the main functional chains, the possible interactivity with the model in order to raise the level of understanding over the models built, and possibly the exploration of the design space. • Abstract: Debug4Science aims to propose a disciplined approach to develop domain-speciﬁc debugging facilities for Domain-Speciﬁc Languages within the context of scientiﬁc computing and numerical analysis. Debug4Science is a bilateral collaboration (2020-2022), between the CEA DAM/DIF and the DiverSE team Summary: The goal of the RESIST associate team is to explore the science of resilient software by foundational work on advanced a priori testing methods such as metamorphic testing and a posteriori continuous improvements through digital twins. resist web site • Abstract: Most modern software systems (operating systems such as Linux, Web browsers such as Firefox or Chrome, video encoders such as x264 or ffmpeg, servers, mobile applications, etc.) are subject to variation or come in many variants. Hundreds of conﬁguration options, features, or plugins can be combined, each potentially with distinct functionality and effects on execution time, memory footprint, etc. Among conﬁgurations, some of them are chosen and do not compile, crash at run time, do not pass a test suite, or do not reach a certain performance quality (e.g., energy consumption, security). In this JCJC ANR project, we follow a thought-provocative and unexplored direction: we consider that the variability boundary of a software system can be specialized and should vary when needs be. The goal of this project is to provide theories, methods and techniques to make variability vary. Speciﬁcally, we consider machine learning and software engineering techniques for narrowing the space of possible conﬁgurations to a good approximation of those satisfying the needs of users. Based on an oracle (e.g., a runtime test) that tells us whether a given conﬁguration meets the requirements (e.g., speed or memory footprint), we leverage machine learning to retroﬁt the acquired constraints into a variability that can be used to automatically specialize the conﬁgurable system. Based on a relative small number of conﬁguration samples, we expect to reach high accuracy for many different kinds of oracles and subject systems. Our preliminary experiments suggest that varying variability can be practically useful and effective. However, much more work is needed to investigate sampling, testing, and learning techniques within a variety of cases and application scenarios. We plan to further collect large experimental data and apply our techniques on popular, open-source, conﬁgurable software (like Linux, Firefox, ffmpeg, VLC, Apache or JHipster) and generators for media content (like videos, models for 3D printing, or technical papers written in LaTeX). • Abstract: in the context of this project, DGA-MI and the INRIA team DiverSE explore the existing approaches to ease the development of formal speciﬁcations of domain-Speciﬁc Languages (DSLs) dedicated to packet ﬁltering, while guaranteeing expressiveness, precision and safety. In the long term, this work is part of the trend to provide to DGA-MI and its partners a tooling to design and develop formal DSLs which ease the use while ensuring a high level of reasoning. • Abstract: The ONEWAY project aims at maturing digital functional bricks for the following capacities: 1) Digitalization, MBSE modeling and synthetic analysis by substitution model, of all the information and under all the points of view necessary for the design and validation across an extended enterprise of the complete aircraft system and at all its levels of decomposition, 2) Generic and instantiable conﬁguration management throughout the life cycle, on products and their support systems, in product lines or on aircraft programs, interactively in the context of an extended enterprise, 3) Decision support for launching, then controlling and steering a Product Development • Abstract: The IPSCO (Intelligent Support Processes and Communities) project aims to develop a new customer support platform for digital companies and public services. Both by implementing intell",,2022.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
924420707edd1e75b735b577b7dc4ff2a2cf1932,https://www.semanticscholar.org/paper/924420707edd1e75b735b577b7dc4ff2a2cf1932,Cybertrust: From Explainable to Actionable and Interpretable AI (AI2),"To benefit from AI advances, users and operators of AI systems must have reason to trust it. Trust arises from multiple interactions, where predictable and desirable behavior is reinforced over time. Providing the system’s users with some understanding of AI operations can support predictability, but forcing AI to explain itself risks constraining AI capabilities to only those reconcilable with human cognition. We argue that AI systems should be designed with features that build trust by bringing decision-analytic perspectives and formal tools into AI. Instead of trying to achieve explainable AI, we should develop interpretable and actionable AI. Actionable and Interpretable AI (AI2) will incorporate explicit quantifications and visualizations of user confidence in AI recommendations. In doing so, it will allow examining and testing of AI system predictions to establish a basis for trust in the systems’ decision making and ensure broad benefits from deploying and advancing its computational capabilities. Decision Making: Humans and Artificial Intelligence (AI) “Can I trust the recommendation of an AI agent?” This question is difficult to answer, especially if the decision at stake is complex and spans different spatial and temporal scales. Such difficulty is exacerbated when the outcomes of an AI-influenced decision may heighten existing risks to humans or introduce new risks altogether. Yet such high-stakes situations have become routine within the diverse systems that currently incorporate AI, like controls for chemical plants, defense systems, and health insurance rate determinations. Stakeholders must be prepared not only to configure AI and its enabling technologies for a given industry or activity, but also to have tools and methodologies to examine and recognize its failures, limitations, and needs for quality control at various stages of its development and implementation. The ultimate goal of AI is to provide users with actionable recommendations that meet both implicit and explicit goals of the decision makers and stakeholders. Recommendations generated from AI-based This is a preprint version of the paper by Igor Linkov, Stephanie Galaitsi, Benjamin D. Trump, Jeffrey M. Keisler, and Alexander Kott, ""Cybertrust: From Explainable to Actionable and Interpretable Artificial Intelligence,"" IEEE Computer, Sept. 2020, pp. 91-96, vol. 53 DOI Bookmark: 10.1109/MC.2020.2993623 ................................. approaches hold advantages over human decision makers through their ability to analyze vast bodies of information in limited time in an objective and logic-centered fashion. In many situations, these benefits are clear and already implemented in practice, such as machine learning systems for the detection of phishing attempts [Khonji et al. 2013]. AI applications are also capable of providing multistep and adaptable strategies, as demonstrated by programs that compete in chess or Go, as well as AI-based cybersecurity systems [Al-Shaer et al. 2019; Kott et al. 2019] However, AI recommendations may not account for decision maker values or specific mission needs. For example, following a cyberattack, an AI-generated decision engine may recommend disabling an application on the compromised computer system. Such an action may neutralize the threat posed by the compromised system, but could simultaneous endanger a mission, negatively impact a critical user, or enable the adversary to extend the duration or scope of the cyberattack. The broader scale impacts of the recommended path forward may not have been incorporated into the AI’s design or scope, causing the AI decision processes to omit critical conditions that a human operator would implicitly account for. Such incomplete scoping of AI-driven analysis is especially problematic when unspoken, unacknowledged, or subjective variables influence or shape what a successful outcome looks like. For example, an AI system may not account for the need for a particular asset to be available to achieve a mission later on, or for psychological impact on the system’s users. The AI solves the problem it is given, but it the human’s role to ensure the recommendation’s suitability in context. Similarly, the human users making this judgment will benefit from understanding the factors that produced the AI’s decision, especially when that understanding helps the users see the value of factors they themselves could have overlooked. While AI-driven analysis enhances our decision making ability, providing insight into AI’s shifts in its analysis of needs, expectations, and mission requirements will ensure its decisions’ relevance and credibility and make its expectations for the future explicit. If AI’s analytical outputs do not account for these and other broader and potentially subjective concerns, an overly myopic focus upon a tactical decision can derail strategic mission requirements. As such, a more effective deployment of AI into decision making must resolve the ‘black box’ concerns of AI – in that it is unclear how to explain, interpret, or act upon AI’s conclusions as its underlying algorithm and parameters are difficult to decipher. Inevitable Disagreements: The Challenge of Fusing Human and AI Decision Making Capabilities We can expect AI recommendations to differ, in some percentage of circumstances, from the choice the operator alone would have made. There are three possibilities on how this plays out for any yes/no decision: the AI is more risk-averse than the human, the AI is more risk-tolerant than the human, or the AI and the human agree (Table 1).",ArXiv,2022.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
b47dd6375863ecfaabeec0fab714300fd66e715d,https://www.semanticscholar.org/paper/b47dd6375863ecfaabeec0fab714300fd66e715d,Proceedings of 2019 IEEE PES Innovative Smart Grid Technologies Europe (ISGT-Europe),"grids designing, managing operating qualitatively new complex socio-technical technical perspective, solutions to limited ability to predict stochastic generation demand in communications, control Abstract. A global sustainable energy system can be realised with predominantly solar and wind as energy source, converted into electricity via solar panels and wind turbines. Where possible, useful and cost effective, the electricity produced is directly used. However, lowest cost solar and wind electricity can be produced far away from the demand, requiring conversion to hydrogen by electrolysis for cheap transport and storage. The lower electricity production cost and cheaper transport and storage cost will compensate for extra energy conversion losses and costs. The hydrogen can be transported in large quantities worldwide by ship or by pipelines and stored in the underground in salt caverns. Volumes and capacities for hydrogen transport and storage are orders of magnitude larger than for electricity. Today, the conversion process to convert hydrogen in heat, electricity or mechanical power is via combustion. However, in future electrochemical conversion via fuel cells will become more important. Fuel cells for mobility, in buildings for power and heat, and for electricity balancing. In the end electrochemical conversion using electrolysers and fuel cells together with heat pump technology will fully replace combustion technologies. A smart integration of electricity and hydrogen systems using electrochemical conversion and heat pump technology can deliver energy demand in all sectors, clean, reliable and affordable. Electricity and hydrogen will be the carbon-free symbiotic energy carriers and smart grids needs to integrate both. Abstract. The term smart grid has been around for well over a decade, yet AI and machine learning are now enabling humans to make smarter decisions in managing the power grid. Augmenting humans with digital enhanced cognitive functions such as sight, touch and hearing, all workflows are being re-imagined to increase grid capacity, worker safety and customer experience: How can a machine learning ‘grid model’ help TSO-operators spot the risks and grid operating limits? How can vegetation growth models, satellite images and accurate weather prediction send crews out to the right place and time to cut tree branches before interfering with power lines? And how can visual recognition, augmented reality and speech recognition help field workers do their jobs safely, efficiently and with a higher job satisfaction? These elements come together in what IBM calls ‘The Cognitive Enterprise’, or in this case 'The Cognitive Grid Enterprise'. Abstract. The electric power utilities are transitioning towards a Smart Grid as a result of the changes in the industry driven by the developments of computer and communication technologies, as well as the increasing penetration of renewable distributed energy resources. The presentation describes the characteristics of the smart grid and the need for digitization in order to meet the requirements of the different applications. It briefly introduces the key components of the IEC 61850 standard and the extensions that are developed to cover the requirements of different domains outside of the substation. It describes the non-conventional sensors that are becoming key components of the digital substations and looks at different architectures, such as distributed, centralized or hybrid systems. Engineering of the digital substations based on the IEC 61850 SCL (system configuration language) is later described. The benefits of the digital grid and its impact on the maintenance and testing of the electric power system are described at the end of the presentation. Abstract: Urban environments are characterized by a high density of energy consumption, and tight constraints on local development (including network infrastructure). The implication is that while initiating the energy transition in urban environments cannot be postponed, the transition pathways must continuously adapt to local conditions and future developments. Against this background, it is essential to unlock the flexibility offered by energy consumers in close proximity, and across different energy vectors, making use of the additional flexibility offered by local sources of energy (prosumers) and the proliferation of IoT-enabled devices. This session brings together panelists that address this challenge from technical, regulatory and socio-economic perspectives. They will present and discuss the potential benefits of unlocking prosumer flexibility, and how these can be achieved in ways that are feasible, fair and resilient. Abstract: Local markets for electricity are gaining interest from academic and industrial researchers alike. At the level of the transmission networks and wholesale electricity markets, locational aspects have been included in the market clearing procedure for decades now. Currently, one of the energy transition challenges is to develop local market mechanisms for the distribution level of the electricity system. These mechanisms should be (1) fair, transparent and posing the right incentives to all stakeholders, (2) scalable, robust, increasing resiliency, and (3) connecting to the wholesale markets such that local and global stakes are balanced. Abstract: The term ""digital twin"" is hot: An established method in avionics or automotive industry now enters new sectors, power systems being one of them. A digital twin represents some real (cyber/physical/social) system in a digital fashion. It can represent the past (forensics, analytics), the present (dashboard), or the future (predictive controls or maintenance, scenario optimization). Depending on the depth of the used models, it can give insights into hidden dynamics, risks, and optima. There are many situations and locations in the power system where digital twins can help: as decision support in the control room, embedded in a substation controller, as a planning tool, or on markets. This panel invites a group of experts, both from research and practice to confront the audience with the bare facts on digital twins in order to discuss their use and limits for power systems and smart grids. The panelists will pitch their propositions as thesis and antithesis, the audience will vote on that. After an interactive discussion with the audience, the vote is repeated, assessed, and summarized. Abstract: In this panel, the lessons learned so far from the EMPOWER project will be discussed by speakers that forms a blend of academia and industry. In particular, Abstract: Flexibility can be defined as the ability of the power system to adapt to variations of the demand, the generation and the grid. New challenges are arising like the increasing penetration of RES but new solutions are also emerging like smarter controls or large scale storage. The OSMOSE H2020 project (2018-2021) aims at enhancing the flexibility of the European power system especially through four large scale demonstrators lead by Transmission System operators: RTE, REE, TERNA and ELES. These demonstrations covers various innovations on flexibility services and providers: grid forming, multi-services by hybrid storage, near real-time cross border exchanges, smart zonal energy management system. Each demo will present in 15 minutes its objectives and current status. A roundtable will be the occasion to discuss with the audience the following questions: Abstract: For large-scale integration of renewable energy, power systems must provide abundant flexibility to accommodate high shares of variable and uncertain renewable resources. Electrification is also becoming the most promising option to bring renewable energy to other sectors, thus greatly increasing (unconventional) electric demand, which poses new challenges and opportunities to the power system. In this panel session, we present different flexibility sources, their modelling and economic challenges for optimal planning and operation of power systems. These flexibility options include demand response, storage, and flexible sector coupling, e.g., power2gas (H2), power2mobility (EVs), and power2heat. Abstract: Although power-electronics interfaced Distributed Renewable Energy Sources (PEI-DRES) are highly proliferated at distribution systems, their intermittent and inertia-less nature still prohibits the overall decommission of bulk synchronous generators (SGs), whose inherent properties are the basis of robust and stable transmission systems. Towards this direction, the increased controllability of PEI-DRES should be exploited so as they can provide Ancillary Services (AS) similarly to SGs. This panel session will provide a further insight on this topic through the ongoing research in the H2020 EU project EASY-RES. Abstract: Real-time simulation and hardware-in-the-loop (HIL) testing has been used in the power industry for over twenty-five years. Originally developed as a solution for flexibly testing the control and protection associated with HVDC projects, the application of the technology is now widespread and varied, and today real-time simulators are used particularly effectively in the smart grid technologies space. The devices associated with and required by smart grids have the potential to interact with each other and with existing technologies, and given the fast-acting nature of modern control and protection systems, the tools required to study them in detail must be capable of representing subcycle phenomena. Real-time simulators offer an electromagnetic transient representation of the power system along with the ability to connect devices in a closed-loop with the simulated network for flexible, controlled, and safe testing prior to deployment. This panel session highlights recent exciting projects from real-time simulator users validating and de-risking enabling technologies for a smarter, more sustainabl",2019 IEEE PES Innovative Smart Grid Technologies Europe (ISGT-Europe),2019.0,10.1109/isgteurope.2019.8905614,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
59e10d1d4cd454635914cfd0ac5160a318fd0473,https://www.semanticscholar.org/paper/59e10d1d4cd454635914cfd0ac5160a318fd0473,UB09 Session 9,"In the domain of Wireless Sensor Networks (WSN), providing an effective security solution to protect the motes and their communications is challenging. Due to the hard constraints on performance, storage and energy consumption, normal network-security related techniques cannot be applied. Focusing on the ""Intrusion Detection"" problem, we propose a realworld application of our WSN Intrusion Detection System (WIDS). WIDS exploits the Weak Process Models to classify potential security issues in the WSN and to notify the operators when an attack tentative is detected. In this demonstration, we show how our IDS works, how it detects some basic attacks and how the IDS can evolve to fullfil the needs of secure WSN deployments. Download Paper (PDF) UB09.2 RESCUE: EDA TOOLSET FOR INTERDEPENDENT ASPECTS OF RELIABILITY, SECURITY AND QUALITY IN NANOELECTRONIC SYSTEMS DESIGN Authors: Cemil Cem Gürsoy1, Guilherme Cardoso Medeiros2, Junchao Chen3, Nevin George4, Josie Esteban Rodriguez Condia5, Thomas Lange6, Aleksa Damljanovic5, Raphael Segabinazzi Ferreira4, Aneesh Balakrishnan6, Xinhui Anna Lai1, Shayesteh Masoumian7, Dmytro Petryk3, Troya Cagil Koylu2, Felipe Augusto da Silva8, Ahmet Cagri Bagbaba8 and Maksim Jenihhin1 1Tallinn University of Technology, EE; 2Delft University of Technology, NL; 3IHP, DE; 4BTU Cottbus-Senftenberg, DE; 5Politecnico di Torino, IT; 6IROC Technologies, FR; 7Intrinsic ID B.V., NL; 8Cadence Design Systems GmbH, DE Abstract The demonstrator will introduce an EDA toolset developed by a team of PhD students in the H2020-MSCA-ITN RESCUE project. The recent trends for the computing systems include machine intelligence in the era of IoT, complex safety-critical applications, extreme miniaturization of technologies and intensive interaction with the physical world. These trends set tough requirements on mutually dependent extra-functional design aspects. RESCUE is focused on the key challenges for reliability (functional safety, ageing, soft errors), security (tamper-resistance, PUF technology, intelligent security) and quality (novel fault models, functional test, FMEA/FMECA, verification/debug) and related EDA methodologies. The objective of the interdisciplinary cross-sectoral team from Tallinn UT, TU Delft, BTU Cottbus, POLITO, IHP, IROC, Intrinsic-ID, Cadence and Bosch is to develop in collaboration a holistic EDA toolset for modelling, assessment and enhancement of these extra-functional design aspects. Download Paper (PDF)The demonstrator will introduce an EDA toolset developed by a team of PhD students in the H2020-MSCA-ITN RESCUE project. The recent trends for the computing systems include machine intelligence in the era of IoT, complex safety-critical applications, extreme miniaturization of technologies and intensive interaction with the physical world. These trends set tough requirements on mutually dependent extra-functional design aspects. RESCUE is focused on the key challenges for reliability (functional safety, ageing, soft errors), security (tamper-resistance, PUF technology, intelligent security) and quality (novel fault models, functional test, FMEA/FMECA, verification/debug) and related EDA methodologies. The objective of the interdisciplinary cross-sectoral team from Tallinn UT, TU Delft, BTU Cottbus, POLITO, IHP, IROC, Intrinsic-ID, Cadence and Bosch is to develop in collaboration a holistic EDA toolset for modelling, assessment and enhancement of these extra-functional design aspects. Download Paper (PDF) UB09.3 ASAM: AUTOMATIC SYNTHESIS OF ALGORITHMS ON MULTI CHIP/FPGA WITH COMMUNICATION CONSTRAINTS Authors: Amir Masoud Gharehbaghi, Tomohiro Maruoka, Yukio Miyasaka, Akihiro Goda, Amir Masoud Gharehbaghi and Masahiro Fujita, The University of Tokyo, JP Abstract Mapping of large systems/computations on multiple chips/multiple cores needs sophisticated compilation methods. In this demonstration, we present our compiler tools for multi-chip and multi-core systems that considers communication architecture and the related constraints for optimal mapping. Specifically, we demonstrate compilation methods for multi-chip connected with ring topology, and multi-core connected with mesh topology, assuming fine-grained reconfigurable cores, as well as generalization techniques for large problems size as convolutional neural networks. We will demonstrate our mappings methods starting from data-flow graphs (DFGs) and equations, specifically with applications to convolutional neural networks (CNNs) for convolution layers as well as fully connected layers. Download Paper (PDF) UB09.4 HEPSYCODE-MC: ELECTRONIC SYSTEM-LEVEL METHODOLOGY FOR HW/SW CO-DESIGN OF MIXED-CRITICALITY EMBEDDED SYSTEMS Authors: Luigi Pomante1, Vittoriano Muttillo1, Marco Santic1 and Emilio Incerto2 1Università degli Studi dell'Aquila DEWS, IT; 2IMT Lucca, IT Abstract Heterogeneous parallel architectures have been recently exploited for a wide range of embedded application domains. Embedded systems based on such kind of architectures can include different processor cores, memories, dedicated ICs and a set of connections among them. Moreover, especially in automotive and aerospace application domains, they are even more subjected to mixed-criticality constraints. So, this demo addresses the problem of the ESL HW/SW co-design of mixed-criticality embedded systems that exploit hypervisor (HPV) technologies. In particular, it shows an enhanced CSP/SystemC-based design space exploration step, in the context of an existing HW/SW co-design flow that, given the system specification is able to (semi)automatically propose to the designer: a custom heterogeneous parallel HPV-based architecture; an HW/SW partitioning of the application; a mapping of the partitioned entities onto the proposed architecture. Download Paper (PDF)Heterogeneous parallel architectures have been recently exploited for a wide range of embedded application domains. Embedded systems based on such kind of architectures can include different processor cores, memories, dedicated ICs and a set of connections among them. Moreover, especially in automotive and aerospace application domains, they are even more subjected to mixed-criticality constraints. So, this demo addresses the problem of the ESL HW/SW co-design of mixed-criticality embedded systems that exploit hypervisor (HPV) technologies. In particular, it shows an enhanced CSP/SystemC-based design space exploration step, in the context of an existing HW/SW co-design flow that, given the system specification is able to (semi)automatically propose to the designer: a custom heterogeneous parallel HPV-based architecture; an HW/SW partitioning of the application; a mapping of the partitioned entities onto the proposed architecture. Download Paper (PDF) UB09.5 CS: CRAZYSQUARE Authors: Federica Caruso1, Federica Caruso1, Tania Di Mascio1, Alessandro D'Errico1, Marco Pennese2, Luigi Pomante1, Claudia Rinaldi1 and Marco Santic1 1University of L'Aquila, IT; 2Ministry of Education, IT Abstract CrazySquare (CS) is an adaptive learning system, developed as a serious game for music education, specifically indicated for young teenager approaching music for the first time. CS is based on recent educative directions which consist of using a more direct approach to sound instead of the musical notation alone. It has been inspired by a paper-based procedure that is currently used in an Italian middle school. CS represents a support for such teachers who prefer involving their students in a playful dimension of learning rhythmic notation and pitch, and, at the same time, teaching playing a musical instrument. To reach such goals in a cost-effective way, CS fully exploits all the recent advances in the EDA domain. In fact, it is based on a framework composed of mobile applications that will be integrated with augmented reality HW/SW tools to provide virtual/augmented musical instruments. The proposed demo will show the main features of the current CS framework implementation. Download Paper (PDF)CrazySquare (CS) is an adaptive learning system, developed as a serious game for music education, specifically indicated for young teenager approaching music for the first time. CS is based on recent educative directions which consist of using a more direct approach to sound instead of the musical notation alone. It has been inspired by a paper-based procedure that is currently used in an Italian middle school. CS represents a support for such teachers who prefer involving their students in a playful dimension of learning rhythmic notation and pitch, and, at the same time, teaching playing a musical instrument. To reach such goals in a cost-effective way, CS fully exploits all the recent advances in the EDA domain. In fact, it is based on a framework composed of mobile applications that will be integrated with augmented reality HW/SW tools to provide virtual/augmented musical instruments. The proposed demo will show the main features of the current CS framework implementation. Download Paper (PDF) UB09.6 LABSMILING: A SAAS FRAMEWORK, COMPOSED OF A NUMBER OF REMOTELY ACCESSIBLE TESTBEDS AND RELATED SW TOOLS, FOR ANALYSIS, DESIGN AND MANAGEMENT OF LOW DATA-RATE WIRELESS PERSONAL AREA NETWORKS BASED ON IEEE 802.15.4 Authors: Carlo Centofanti, Luigi Pomante, Marco Santic and Walter Tiberti, University of L'Aquila, IT Abstract Low data-rate wireless personal area networks (LR-WPANs) are constantly increasing their presence in the fields of IoT, wearable, home automation, health monitoring. The development, deployment and testing of SW based on IEEE 802.15.4 standard (and derivations, e.g. 15.4e), require the exploitation of a testbed as the network grows in complexity and heterogeneity. This demo shows LabSmiling: a SaaS framework which connects testbeds deployed in a real-world-environment and the related SW tools that make available a meaningful (but still scalable) number of physical devices (sensor nodes) to developers. It provides a comforta",,2019.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
cd51bbfd51cde0c089d9dfb7d30bfc124d9b7c55,https://www.semanticscholar.org/paper/cd51bbfd51cde0c089d9dfb7d30bfc124d9b7c55,"Summary for CIFE Seed Proposals for Academic Year 2020-21 Proposal number: 2020-04 Proposal title: Hybrid Physical-Digital Spaces: Transforming the Design, Operation, and Experience of Built Environments to Promote Health and Wellbeing","up to 150 words) Increasing evidence suggests built office features (e.g., lighting, materials, and ventilation) have substantial impacts on occupant wellbeing. A key next direction is field studies at industry partner sites to examine real-world workplaces. We propose to develop innovative Internet of Things (IoT) techniques that integrate data from building instrumentation, personal device sensors, and self-report interfaces and then deploy this platform in-the-wild to capture rich, longitudinal, ecologically-valid data about the status of office workers and the spaces they occupy. Insights will advance scientific knowledge of how buildings impact wellbeing as well as produce practical implications for building designers and operators. A timely component will explore how covid-19 has temporally or fundamentally changed occupant behaviors and operational decisions (e.g., physical distancing desks and ventilation settings that reduce pathogen spread). Overall, our proposed research has the potential to transform the industry’s thinking on how built environments can be designed, operated, and experienced. Hybrid Physical-Digital Spaces: Transforming the Design, Operation, and Experience of Built Environments to Promote Health and Wellbeing Problem and Significance Considering that people in the U.S. spend 87% of their time in indoor spaces , we assert that 1 buildings are powerful yet underleveraged loci for promoting human wellbeing. Imagine an intelligent office that could adapt soundscape systems to manage noise in open floor plans, optimize space reservation or utilization to foster collaborations and save energy, or provide digital information displays that promote employee connectedness and physical activity. Towards actualizing our vision of such hybrid physical-digital spaces, our proposal strives to develop, apply, and evaluate novel scientific and engineering approaches that will transform the industry’s thinking around how built environments can be designed, operated, and experienced. Increasingly, hypotheses suggest that built features of indoor environments (e.g., lighting, materials, and ventilation) have substantial impacts on occupants (e.g., employee recruitment and retention, absenteeism, cognition, creativity, productivity, social interactions, physical activity and health, and psychological wellbeing). In turn, these individual outcomes also drive pivotal organizational outcomes such as product innovation, workforce diversity, employee turnover, market share, and profitability. Examples illustrate how building interventions can have huge impacts : enhancing employee exposure to daylight can save businesses ~$2,000/yr per capita 2 , better air quality can raise cognitive scores of workers by 101% 3 , and increasing indoor access to biophilic elements could recoup $23 billion considering 10% of workplace absenteeism (a $226 billion dollar problem) is attributable to architecture that inadequately connects to nature 4 . However, few of these hypotheses have been tested at scale, over time, and in real world conditions . Instead, most prior efforts are small sample, short-term correlational studies based on potentially biased and sparse self-reported data. A more rigorous, scientific, and human-centered approach to study and engineer buildings that promote wellbeing can have major implications at individual, organizational, and societal levels (see Figure 1), offering both foundational theoretical knowledge as well as practical strategies for building designers and operators. Figure 1. Relations among building features and human outcomes at various levels. Further, “smart buildings” today typically focus on basic sensing and control for energy savings, thermal comfort, and security. Connecting to CIFE’s Vision for the Future of Building Users, we argue buildings of the future can go beyond such bottom line outcomes to be more interactive and human-centered: aware of and responsive to occupants’ cognitive, mental, and physical feelings and needs, while respecting privacy and promoting positive indoor experiences . 1 Klepeis, et al., 2001; 2 Heschong & Mahone, 2003; 3 Allen et al., 2016; 4 Elzeyadi, 2011. <Landay-Billington> < Hybrid Physical-Digital Spaces> 1 Theoretical and Practical Points of Departure It is imperative to increase understanding of exactly what built attributes have what impacts and on whom, in a scalable, longitudinal, and inclusive manner. Thus through technology-driven assessment and hybrid physical-digital interventions, we aim to (a) fundamentally advance the science on how built environments impact human wellbeing and, in turn, (b) generate guidelines that can revolutionize the way spaces are designed, operated, and experienced . Our current scope focuses on office spaces and workers; though an overarching goal is for our developed approaches and insights to establish a foundation that enables future research with additional populations and environments (e.g., physicians and patients in clinical settings, students and teachers in classrooms, and traditionally marginalized shift and temporary workers). In particular, our reusable platform will help others study this wider range of buildings and occupants; and combining these approaches with emerging endeavors such as biophilic design and precision interventions provides a novel opportunity to not only more deeply investigate but also address long-running public health challenges and systemic inequities facing society. In these ways, we hope to positively impact a broad cross-section of stakeholders at individual, organizational, and institutional levels. Moreover, this project will support interdisciplinary fertilization across engineering, computing, psychology, law, and medicine . Research Methods and Work Plan Our research agenda is to support the design and operation of built facilities that augment human capabilities and wellbeing — and have a fundamental positive change on the way indoor spaces are experienced by the people that occupy them. By introducing intelligent systems capable of gathering and interpreting building and occupant data as well as delivering adaptive interventions in response, novel roles will also emerge for managing buildings and the activities that take place inside them. To achieve these goals, our research will comprise three main activities: 1. Developing an extensible and secure data collection and machine learning platform . A key aim of this research is scientifically examining how built spaces impact human wellbeing. To pursue this investigation and develop methods that enable buildings to be more aware of occupants’ states and needs, we have been developing pattern detection software that integrates data from (a) personal devices (smartphones, smartwatches, fitness trackers), (b) building instrumentation or portable environmental sensors (light levels, air quality), and (c) experience sampling interfaces that prompt occupants for subjective information through quick, validated self-report techniques. Figure 2 illustrates examples of these assessment components. This work involves addressing a number of technical challenges, such as selecting sampling rates and window sizes to maximize efficiency, developing methods for analyzing asynchronous and sparse sensor data, and developing privacy-sensitive feature engineering strategies for detecting and predicting wellbeing outcomes of interest. We also plan to package our platform as a reusable toolkit that can be applied by other researchers and building operators. This work is ongoing and a basic version will be ready by summer. Once development is complete, CIFE support would allow us to move onto the next critical phase: moving out of the lab and into the field. <Landay-Billington> < Hybrid Physical-Digital Spaces> 2 Figure 2. Platform to integrate data from personal devices, building sensors, and subjective self-report. 2. Deploying the platform through a mixed-method study with industry partners . The next step in our research is to deploy this platform at field sites in partnership with View, Inc. (specifically, at TIAA offices in Manhattan, this summer/fall) to capture rich, longitudinal, ecologically-valid data about behavioral, psychological, and physiological states of occupants and their everyday work environments. Our plan is to recruit a sample of approximately 150 employees for a period of 18 weeks, which will involve a baseline phase followed by systematic variation of built features (Views/No Views, Plants/No Plants, and Diversity/No Diversity in artwork) and measurement of indicators hypothesized to promote both personal wellbeing and organizational performance, based on the literature and our formative online and lab studies, described below. In combination with the engineering-focused activities to implement and install the platform, deployment will occur in tandem with ethnographic work (e.g., observations, interviews, and surveys) to manually validate reliability of the system’s automated inferences as well as gain a more qualitative portrait of occupant experiences in various spaces. Privacy-centric engagements will additionally investigate stakeholders’ attitudes regarding the capture of various types of information to derive implications about informed consent and personal data management. Along similar lines, it will be critical to responsibly manage captured data, especially potentially sensitive and exploitable data about wellness or performance. Therefore all studies will be conducted with oversight and approval from the Stanford Institutional Review Board (IRB). In addition to obtaining participants’ informed consent, we will also design sensor and data collection mechanisms to use an opt-in model, including partial participation. Our data management systems can also allow individuals to view and delete their personal data, including if purging is desired in the event of study withdrawal. Our research team has exp",,2020.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
8bcb0086d9a4daf9da32cd61aa8958c4d7d2e389,https://www.semanticscholar.org/paper/8bcb0086d9a4daf9da32cd61aa8958c4d7d2e389,Designing Great Products and Building High Performing Teams with DFMA,"Hypertherm designs and manufactures plasma, laser, and water jet cutting equipment. Our products incorporate electro-mechanical assemblies, molded components, and machined parts – all of which present ample opportunities to apply both design for assembly and design for manufacture (DFMA) concepts. In this paper we'll discuss our experiences with forming cross-functional teams to apply DFMA as part of the new product development process as well as integrating it into our Lean manufacturing approach. We'll discuss a specific case study of how DFMA was applied to the recently launched Powermax45 XP plasma arc cutting system. Compared to a previous generation product, this design reduced parts count by 10%, fastener count by 20%, weight by 12%, assembly time by 15% and material cost by 6%. These material cost and operational improvements have a direct and lasting impact on the profitability of this business team. Overview of Hypertherm and the Light Industrial Business. In Lean organizations, the objective is to reduce waste to maximize delivery of value to customers. At Hypertherm DFMA complements Lean in our quest to design great products that deliver maximum value to customers while taking advantage of the collaborative experience to build high-performing teams. The case study we'll share in this paper revolves around the development and introduction of Powermax45 XP Light Industrial plasma cutting system which launched in 2016. This new product shares a lineage of more than 20 years with similar, prior systems designed by Hypertherm, including the MAX43, Powermax600, and Powermax45. These systems are designed to cut metal up to 5/8"" thickness while being very portable and affordable. Figure 1. History of the 3/8-5/8"" Light Industrial cutting systems Hypertherm, Inc., located in Hanover, New Hampshire, is a company dedicated to providing the best industrial cutting solutions in our industry (www.hypertherm.com). The company was founded in 1968 by Chairman of the Board Dick Couch and Dartmouth College Engineering Professor Emeritus Bob Dean when the pair discovered the potential to cut metal with speed, accuracy, and precision using a narrowly-focused plasma arc (Figure 2). Figure 2. The conductive electric arc used for cutting is a result of adding energy to a gas to dissociate its molecules and ionize its atoms. Our focus on technology remains strong with product offerings in several cutting-related markets: plasma arc, laser, and water jet cutting, as well as CNC motion control hardware/software and CAM software solutions (Figure 3). More than 10% of our 1430 Associates work in engineering roles and we have been awarded more than 100 patents. Of those 1430 Associates, approximately 1100 work in one of our eleven Upper Valley facilities located in and around the Hanover/Lebanon, NH area. Nearly all our manufacturing is done in the United States while the company has a global presence in 93 countries. In 2013 North American markets accounted for 45% of sales and exports to Europe, Asia, and South America accounted for the remaining 55%. Figure 3. Hypertherm plasma, laser, and waterjet cutting. Lean Enterprise Culture and Operational Excellence. A culture of continuous improvement has been central to Hypertherm's value system for more than 25 years. The pursuit of Operational Excellence (OpEx) through Lean principles (Duggan, 2011) aligns naturally with such a culture. Design for Manufacture and Assembly (DFMA) has become an essential element in our OpEx strategy, with an initial formal appearance in 2003 and subsequent re-emphasis in the last 3 years. It's well established that DFA when deployed and executed can address several of the seven Lean wastes (Shipulski, 2006) such as inventory, travel, motion, waiting, etc. by eliminating unnecessary parts and reducing parts count. Consequently, the space, time, and effort to maintain a steady stream of supply is reduced as well. Lean improvements to the value stream alone will not lead to Operational Excellence. Design improvements reducing parts count, executed through DFMA methodology, must take place as well. Our experience at Hypertherm is revealing that DFMA, when integrated into each product development cycle, can produce step change improvements in material and labor costs. In between product development cycles operations teams employ Lean principles to drive labor costs lower and reduce material costs as well through strong supply chain partnerships. Labor and material cost savings achieved over the life cycle of a product represent the collaborations of both the design and operations teams. As we'll discuss in this paper, the design community cannot be successful without the input of the operations team. Likewise, the operations community needs the design team to carefully consider design choices such that parts count, along with material and labor costs, are reduced without adverse impact on function. One of the key measures tracked at the business team level is labor cost per system produced. At Hypertherm, Lean and continuous improvement efforts, combined with DFMA, have contributed to 37% decrease in labor/system for the Powermax product family over a 3-year period. These products are manufactured in two mixed-model value streams that have been designed with Lean principles then refined through multiple continuous improvement cycles. The Powermax45 XP is assembled in a 5station flow cell that feeds a shared functional test/burn in area before being packaged for sale (Konstantakos, 2016). The Operations team responsible for this manufacturing value stream has invested in understanding the root causes of imbalance and variability in assembly operations leading to continuous improvement in product throughput. Hypertherm received its formal introduction Design for Manufacture and Assembly in 2003 as part of a major redesign effort with a large, automated cutting system (Shipulski, 2006). Since that project launch DFMA has largely taken place through ""organic"" efforts within the company. Because of the costcompetitive landscape that the 45XP would launch into we decided that a more organized and structured approach would be beneficial. This should not imply that was not already happening within the design team just that an infusion of innovative thinking would be needed to insure the success of the product. Role of Learning DFMA in New Product Development of Powermax45 XP. To accomplish the business objectives of reducing cost and assembly complexity we explored enhancing our long-standing (but somewhat dormant) relationship with Boothroyd-Dewhurst. In addition, organizational learning objectives were also particularly important for many of our younger engineers who may not have had prior exposure to DFMA concepts. During early stages of the product development process we began arrangements for a Value Engineering and DFMA workshop that would combine both learning with execution. The learning element was an important consideration: at Hypertherm we have tried to align our strategic improvement programs with a learning program methodology known as the 6 Disciplines (Jefferson and Wick, 2015). While we are still in the process of implementing several of these elements effectively, these serve as valuable guidelines for our learning organization: Define business outcomes (not just learning objectives). With DFMA the business outcomes are largely built-in: decrease parts count, reduce assembly complexity, cut assembly time, and drive down material costs. For our 45XP project we knew we had a 10% (material) cost reduction goal and that the strategy would necessarily rely heavily on DFMA. In addition, being relatively early in the product development timetable we felt that the team could incorporate a value engineering element into the workshop to promote a more functional approach to the design. Finally, less tangible but equally important goals included promoting collaboration, communication, relationship building, and peer-peer networking in our engineering community. We believe that these are important elements is building a high performing team. Define a complete learning experience (including transfer to work). A secondary, learning oriented goal was to have approximately 20 members of the engineering community trained or refreshed in DFMA concepts as well as software tools for application. With over 120 engineers on staff at Hypertherm we knew that this would be only one of several workshops that would eventually take place. We felt it was important to connect the workshop to a tangible product development project to avoid training for training's sake (i.e. just hoping that learning transfer would come at some later date). Everyone attending the workshop would contribute to the product design in some way. Deliver learning for application and make learning easy. Chris Tsai, Director of DFMA for Boothroyd Dewhurst, became our contact for designing the workshop. Chris shares the philosophy of implementation-based learning so it was relatively easy to adapt the value engineering and DFMA implementation workshop approach for the project. Drive learning transfer and avoid learning ""scrap"". On the Powermax45 XP project we were fortunate to have strong support for DFMA from the Engineering Team leader, Product Development Project leader, and design engineers themselves. After the workshop the application of DFMA principles was reinforced within the team on a regular basis. Deploy performance support. Leadership attended the workshop as well so they were wellequipped to be the first level of performance support. Other key support attributes that we need to improve on are availability of resources (people and subject matter experts; reference documents, tools, and templates), that are practical, clear, and concise. We believe that this will come in the form of key, bite-sized e-learning reference/brush-up modules, as well as a library of tools, templates, and refere",,2017.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
c853b39793005ed00bc58f8903caebad9f3a3971,https://www.semanticscholar.org/paper/c853b39793005ed00bc58f8903caebad9f3a3971,Study on Deep Learning as a Powerful Technology that Revolutionizing Automation in Industries,": Smart production refers to the usage of superior records analytics to complement bodily technology for enhancing device performance and choice making. With the extensive deployment of sensors and Internet of Things, there is a growing need of managing large manufacturing facts characterized through excessive quantity, excessive velocity, and high range. Deep gaining knowledge of present’s superior analytics gear for processing and analyzing huge production facts. This paper affords a comprehensive survey of typically used deep mastering algorithms and discusses their programs in the direction of making production “clever”. The evolvement of deep mastering technologies and their benefits over traditional gadget gaining knowledge of are first of all mentioned. Subsequently, computational strategies based totally on deep getting to know are provided specifically purpose to improve device overall performance in manufacturing. Several consultant deep mastering models are comparably mentioned. Finally, emerging topics of research on deep learning are highlighted, and destiny trends and challenges related to deep getting to know for smart production are summarized. Abstract : Data is a very valuable asset in the world today. The economics of data are based on the idea that data value can be extracted through the use of analytics. Through Big data and analytics are still in their initial growth stage, their importance cannot be undervalued. As big data starts to expand and grow, Importance of big data analytics will continue to grow in everyday lives, both personal and business. In addition, the size and volume of data is increasing every single day, making it important to address the manner in which big data is addressed every day . A huge repository of terabytes of data is generated every day from modern information systems and digital technologies such as Internet of Things and cloud computing. Analysis of these massive data requires a lot of efforts at multiple levels to extract knowledge for decision making. Therefore, big data analysis is a current area of research and development. The basic objective of this paper is to explore the potential impact of big data technologies and challenges associated with it. This paper provides a platform to explore big data at numerous stages. Additionally, it opens a new horizon for researchers to develop the solution, based on the challenges and open research issues. Abstract : In this paper we focus on the initial-value problem of linear plate equations with memory in multi-dimensions, the decay structure of which is of regularity-loss property. We obtain fundamental solutions by using Fourier transform and Laplace transform. By virtue of the point-wise estimate of solutions in the Fourier space, we gain estimates and properties of solution operators, by utilizing which decay estimates of solutions to the linear problem are obtained and the decay rate can be as large as desired if the initial data are sufficiently smooth. A BSTRACT : I O T made the present generation to connect with the different network of devices for exchanging of the data. Nowadays it had made compulsion on wearing the helmet while riding. In our paper we introduce a helmet which is made smart using the latest IOT technologies. The papers’ main objective is to build a safety system that is integrated with the smart helmet and intelligent bike which reduce the chances of two-wheeler accident and drunk drive cases. The pressure sensor check whether the person is wearing helmet or not. Alcohol sensors (Gas sensor) which is installed at the bottom of the helmet detect the alcoholic content in riders’ breath. If the rider is not wearing the helmet or if there is any alcohol content found in rider’s breath, the bike remains off. The bike will start if and only if the rider wears the helmet provided with no alcoholic content present. When the rider crashes, helmet hits the ground, sensors detect the motion of helmet and reports the occurrence of an accident. It sends information of corresponding location and message to the registered number through GPS & GSM module respectively. And along with it we are implementing cooling system inside the helmet which would help the rider to stay cool during the climatic changes which in turn reduces the irritation that is created from the helmet. Abstract In order to safeguard their critical systems against network intrusions, organisations deploys multiple Network Intrusion Detection System (NIDS) to detect malicious packets embedded in network traffic based on anomaly and misuse detection approaches. The existing NIDS deal with a huge amount of data that contains null values, incomplete information, and irrelevant features that affect the detection rate of the IDS, consumes high amount of system resources, and slowdown the training and testing process of the IDS. In this paper, a new feature selection model is proposed based on hybrid feature selection techniques (information gain, correlation, chi squere and gain ratio) and Principal Component Analysis (PCA) for feature reduction. This study employed data mining and machine learning techniques on NSL KDD dataset in order to explore significant features in detecting network intrusions. The experimental results showed that the proposed model improves the detection rates and also speed up the detection process.",International Journal of Computer Applications Technology and Research,2020.0,10.7753/ijcatr0903.1001,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
920b236f530422313ebccf5d43f7d3dd9103beba,https://www.semanticscholar.org/paper/920b236f530422313ebccf5d43f7d3dd9103beba,"International Conference on Computing and Artificial Intelligence ( ICCAI 2019 ) April 19-22 , 2019 Bali , Indonesia","Recently, deep learning (DL) plays important roles in many academic and industrial areas especially in computer vision and image recognition. Deep learning uses a neural network with deep structure to build a high-level feature space. It learns data-driven, highly representative, hierarchical image features, which have proven to be superior to conventional hand-crafted low-level features and mid-level features. In ILSVRC2015 (an Annual competition of image classification at large scale), higher recognition accuracy by deep learning than human has been achieved. Deep learning (DL) has also been applied to medical image analysis. Compared with DL-based natural image analysis, there are several challenges in DL-based medical image analysis due to their high dimensionality and limited number of labeled training samples. We proposed several deep learning techniques for medical image analysis including medical image segmentation, medical image detection and medical image recognition. In this keynote talk, I will talk about current progress and futures of medical image analysis with deep learning. ICCAI 2019 CONFERENCE ABSTRACT 12 Keynote Speaker IV Prof. Qijun Zhao Sichuan University, China Prof. Qijun Zhao is currently a professor in the College of Computer Science at Sichuan University. He obtained his B.Sc. and M.Sc. degrees in computer science both from Shanghai Jiao Tong University, and his Ph.D. degree in computer science from the Hong Kong Polytechnic University. He worked as a post-doc research fellow in the Pattern Recognition and Image Processing Lab at Michigan State University from 2010 to 2012. His recent research interests lie in 3D face modeling and recognition, with applications to forensics, intelligent video surveillance, mobile security, healthcare, and human-computer interactions. Dr. Zhao has published more than 60 papers in academic conferences and journals, including CVPR, ECCV, AAAI, ICB, IEEE Trans., and PR. He is the principal investigator for two projects funded by NSFC, one project funded by the National Key Research and Development Program of China, and many projects funded by companies. Dr. Zhao is a reviewer for many renowned field journals and conferences, such as IEEE TPAMI, IEEE TIFS, IJCV, PR, PRL, ICCV, CVPR, ECCV, and FG. He served as a program committee co-chair in organizing the 11th Chinese Conference on Biometric Recognition (CCBR 2016), the 2018 IEEE International Conference on Identity, Security and Behavior Analysis (ISBA), and the 2018 6th International Conference on Bioinformatics and Computational Biology (ICBCB 2018), and as a face recognition area co-chair for the 9th IEEE International Conference on Biometrics: Theory, Applications, and Systems (BTAS 2018). Topic: ""3D Face Reconstruction in Recognition Perspective"" Abstract—The face reveals a lot of information of humans, for example, identity, race, gender, age, emotion, intention, and health. 3D face models are thus widely studied in many disciplines. Yet, acquisition of 3D faces is still much more expensive and less convenient than acquisition of 2D face images, making it unaffordable to deploy 3D face technology in many real-world applications. Our research aims to reconstruct 3D face shapes from either single or multiple uncalibrated 2D face images from a perspective of identity recognition. This talk will introduce our recent progress along this direction. The methods we propose enable not only efficient generation of 3D face models when only 2D imaging devices are available, but also effective exploration of 3D face information for improving face recognition accuracy. We believe that 3D faces will play increasingly important roles in many applications with the rapid development of both 3D face acquisition techniques and 3D face modeling methods.The face reveals a lot of information of humans, for example, identity, race, gender, age, emotion, intention, and health. 3D face models are thus widely studied in many disciplines. Yet, acquisition of 3D faces is still much more expensive and less convenient than acquisition of 2D face images, making it unaffordable to deploy 3D face technology in many real-world applications. Our research aims to reconstruct 3D face shapes from either single or multiple uncalibrated 2D face images from a perspective of identity recognition. This talk will introduce our recent progress along this direction. The methods we propose enable not only efficient generation of 3D face models when only 2D imaging devices are available, but also effective exploration of 3D face information for improving face recognition accuracy. We believe that 3D faces will play increasingly important roles in many applications with the rapid development of both 3D face acquisition techniques and 3D face modeling methods. ICCAI 2019 CONFERENCE ABSTRACT 13 Keynote Speaker V Assoc. Prof. Ken‘ichi Morooka Kyushu University, Japan Assoc. Prof. Ken’ichi Morooka received his M.S. and Ph.D. degrees from Kyushu University, in 1997 and 2000, respectively. He was a visiting researcher with Institute of Systems & Information Technologies/KYUSHU. From 2000 to 2006, he was an associate professor in Graduate School of Science and Engineering, Tokyo Institute of Technology. He was an associate professor in Digital Medicine Initiative (2006-2010) and Department of Medical Sciences, Kyushu University (2010). Currently, he is an associate professor in Graduate School of Information Science and Electrical Engineering, Kyushu University. Also he was a visiting researcher, Illinois Institute of Technology, U.S. (2016). He has published more than 100 journal and conference articles. He has served as a member of organizing and program committees at numerous conferences, e.g. he has been program committes of MLMI 2018 and 2017, IFMIA 2017, CARS 2014 and EMBC 2013. His research interests cover computer-aided support system for therapy and surgery by image information processing and machine learning. Topic: ""Computer Aided System for Minimally Invasive Surgery Using Deep Learning"" Abstract—Recently, deep neural networks (DNNs) have been paid attention by various research fields including vision, audio and natural language. Of course, there are many DNN-based systems for therapy and diagnosis. Our research group has been doing research about computer-aided support systems for safe and accurate minimally invasive surgeries. Especially, to provide useful information for surgeons, our support systems use stereo endoscopic images, DNNs and 3D shapes and deformations of organs. I will present the fundamental techniques of our support system.Recently, deep neural networks (DNNs) have been paid attention by various research fields including vision, audio and natural language. Of course, there are many DNN-based systems for therapy and diagnosis. Our research group has been doing research about computer-aided support systems for safe and accurate minimally invasive surgeries. Especially, to provide useful information for surgeons, our support systems use stereo endoscopic images, DNNs and 3D shapes and deformations of organs. I will present the fundamental techniques of our support system. ICCAI 2019 CONFERENCE ABSTRACT 14 Invited Speaker Assoc. Prof. Sugiono Sugiono Brawijaya University, Indonesia Sugiono, Ph.D was born in Blitar, Indonesia, in 1978. He finished Bachelor degree in Mechanical Engineering Department at Brawijaya University in 2001, received Master Degree in Industrial Engineering at Sepuluh Nopember Institute of Technology, Surabaya in 2004, and graduated Ph.D. degree of Art, Design and Technology from University of Derby, UK, in 2012. Title of his thesis (PhD) is: Investigating an Intelligent Concept Design Tool for Automotive Car Body Design. His research interests lie in bioengineering ergonomics and intelligent product design. He worked as project analyser in investigating of fuel distribution for industry at PT. Surveyor Indonesia from 2001 to 2002. He also worked as purchasing vice leader at PT. Mitra Saruta (Textile) from 2004 to 2005. Currently, he is working as a lecturer at Department of Industrial Engineering, Brawijaya University start from 2005. He is a head of Work Design and Ergonomics Laboratory and head of Research Committee at Brawijaya University. He is an international reviewer of research, certificated by ISO 17024. He is also working as editor in chief of the Indonesian Journal of Disability Studies (IJDS). He is a senior member of Hong Kong Chemical, Biological and Environmental Engineering Society (HKCBEES), member of Indonesian Ergonomics Society (Perhimpunan Ergonomi Indonesia – PEI) and Member of International Association of Engineers (IAENG). Topic: ""The Importance of Open Innovation Concept to Improve Health and Safety Factors in Transportation"" Abstract—Controlling driver stress level is going to be popular research and put it a very important factor to reduce the risk of a road accident. Understanding the role of road complexity and information technology in transportation issues and their relationship with humans psychophysiological is a good challenge and profitable prospect for the future. Images from the Electrocardiograph (ECG) and Electroencephalography (EEG) are the important tools to identify the driver stress as part of a safety alert system. The Electrocardiograph (ECG) is to monitor every heart rate change and Electroencephalography (EEG) is to record brain signal change correlated with brain functions (thinking, visual, decision, etc.) from three different road types (city road, rural road, and motorway). In this speech, I will deliver a potential open innovation of health and safety factors in transportation (car, train) from the perspective of interaction among human, car, and environment.Controlling driver stress level is going to be popular research and put it a very important factor to reduce the risk of a road accident. Understanding the role of road complexity and information t",,2019.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
b0528c13b684547b8a13241a4d212c32f176cbff,https://www.semanticscholar.org/paper/b0528c13b684547b8a13241a4d212c32f176cbff,IRCETMS 2020 Proceedings,"Internet of Things (IoT), IoT-education, STEM, smartness and digitalization are new techniques amd methods used in the industry 4.0 and Society 5.0 that enable universities, educational institutions to better management of resources and flexibility to respond to the business educational requirements and conditions. Demonstrators of the IoT has an important role, these resources consist of physical implementation of highly innovative experiments of IoT technologies. The demonstrators intend to be didactic labs where the students can make practical experiences in controlled situation but using real equipment or platforms. This learning-by-doing methodology is fundamental when dealing with a topic such as the ones studied in these projects, where practical skills are essential. The demonstrators will be remotely controlled; this will allow the students to use them event distance, ensuring a wider audience. Nowadays, we passed the 1, 2, 3 and 4 waves (ages). Before 1970 various businesses, education and training could affect and improve IT and the other modern technologies. Since we reached the 70ies the IT technologies and smartness able to change and improve other business, education, learning and even impact our lives. Three technological revolutions are shaping the dawn of the 21st century: 1) Decarbonization, 2) Decentralization and 3) Digitization. At the age of communications and technology, education as an irrefutable factor plays a vital role in the developing countries to extend their knowledge in different areas. It is an enabler for the intelligence appended to many central features of the modern world. We provide a survey of some of the major issues challenging the widespread educational management. Keywords— Internet of Things (IoT), Education, Educational Management, IoT-Education, Industry 4.0, Society 5.0, IoT Applications in Education, Educational Challenges, Tomorrow Shocks and the 5th Wave Theory INTRODUCTION Internet of Things (IoT), IoT-education, STEM, smartness and digitalization are new techniques and methods used in the industry 4.0 and Society 5.0 that enable universities, educational institutions to better management of resources and flexibility to respond to the business educational requirements and conditions. Demonstrators of the IoT has an important role, these resources consist of physical implementation of highly innovative experiments of IoT technologies. The demonstrators intend to be didactic labs where the students can make practical experiences in controlled situation but using real equipment or platforms. This learning-by-doing methodology is fundamental when dealing with a topic such as the ones studied in these projects, where practical skills are essential. The demonstrators will be remotely controlled; this will allow the students to use them event distance, ensuring a wider audience. [1,2,3]. Today, IoT application is one of leaders’ new methods considered as an appropriate enable us infrastructure. It is an enabler for the intelligence appended to many central features of the tomorrow world. This paper deals with a new theory called the 5th wave/tomorrow age toward IoT technologies as solution for educational learning management challenges with forecasting, preventing and making readiness for facing educational management by sustainability impacts. In addition, this theory creates societies and business founded on high technologies, D3 (three 21st revolutions: digitalization, decarbonization and decentralization), appropriate business strategies concerned on sustainability which can create new concept and situation of business which is capable of tackled with future concerns entitled ''Tomorrow's Society and Business'' which is a super intelligent society with smart business environment by using AI. In this paper we will discuss how the 5th wave theory and IoB could be as solutions for learning challenges in the 4th industrial revolution. APPLIED RESEARCH INTERNATIONAL CONFERENCES (ARICON) U.K 1 Fig. 1. IoT reference model I will describe how IoT is capable of recognizing challenges in edge of tomorrow and help leaders to find educational sustainable digital solutions for today’s challenges tomorrow’s crises. In this regard, I will discuss about some two case studies and analysis them as experimental fullfilment in the field of IoT in education, learning and training. The proposed paper represents IoT applications to solve educational gap on IoT technologies, Human Resources Competencies (HRC), software programming and hardware equipment. [1,2,3,4,5]. Now, we passed the 1st, 2nd, 3rd and 4th waves (ages), before the 70's various businesses could influence and improve technologies, IT, IoT and after 1970 the digital technologies, smartness, IT and IoT became able to influence, change and improve the various education, training, business, economy, and even impact our lives. At the age of ICT, training as an irrefutable factor plays a vital role in the sustainable countries to extend their knowledge to forecast, prevent and face tomorrow shocks. It may be tempting right now, when it is hard to see beyond the next few weeks, to dismiss the 17 Sustainable Development Goals (SDGs) as a distraction. But they have been described as a “crowd-sourced purchase order from the future” precisely because they offer a tremendous business opportunity. SDGs are an urgent call for action by all countries developed and developing in a global partnership. According to SDGs, goal number is regarding to: Ensure inclusive and equitable quality education and promote lifelong learning opportunities for all. [1,5,9]. Based on the author’s point of view, a new definition for sustainability is required to deal with educational challenges and sustainable development. A new concept of sustainability composing of seven pillars (7PS model) is proposed by the author, instead of the three pillars that make up the traditional notion of sustainability-environmental, economic, and social sustainability. Environmental, economic, social, cultural, educational, political, and technical sustainability are the main components of high sustainability. It is required that these seven pillars are developed fairly equally. In particular, two points are important for high sustainability theory introduced by author: The first point is that seven pillars including education, culture, social, technique, politic, economic, and environment are vital to gain high sustainability. The second point is that these seven pillars of sustainability must develop almost equally Technology development creates new opportunities for business improvement. In recent decades, IoT has been introduced as the implementation of IoT technology into distributed educational systems to optimize the efficiency of sustainability infrastructure as solution for educational and learning management challenges. [12,18]. It can also be extended to more minor levels, manufacturing and operation units. IoT based economy and business make the training procedure faster, safer with higher productivity. People, machines, natural resources, production lines, logistics networks, consumption habits, recycling flows, and virtually every other aspect of economic and social life will be linked via sensors and software to IoT plat-form, continually feeding Big Data to every node busi-nesses, homes, and vehicles moment to moment, in real time. In truth, the IoT provides a nearly endless supply of opportunities to interconnect our devices and equipment. The economic importance of IoT is underlined in several studies: to give an order of magnitude, a recent study of the European Committee estimates that the market value of the IoT in the EU will exceed one trillion euros in 2020. APPLIED RESEARCH INTERNATIONAL CONFERENCES (ARICON) U.K 2 The huge expected growth of IoT in the next years and the planned investments in the sector foresee a high demand of professionals in the sectors. According to a report from the Vision Mobile projects, while in 2014 just 300,000 developers contributed to the IoT, 4.5 mil-lion developers by 2020 are expected, reflecting a 57% compound annual growth rate and a massive market opportunity. European Universities and VET providers are not ready to face this educational challenge. Very few European Universities and VET providers offer courses on IoT nowadays. On the contrary, US Universities and private companies regularly offer courses on IoT to face the demand of professionals At the age of Internet of Things (IoT), education as an irrefutable factor plays a vital role in the developing countries to extend their knowledge in different areas. So the educational institutions’ managers have more responsibilities to survey the situation to enhance the effectiveness. People, machines, natural resources, production lines, logistics networks, consumption habits, recycling flows, and virtually every other aspect of economic and social life will be linked via sensors and software to IoT platform, continually feeding Big Data to every node businesses, homes, and vehicles moment to moment, in real time. In truth, the IoT provides a nearly endless supply of opportunities to interconnect our devices and equipment. The economic importance of IoT is underlined in several studies: to give an order of magnitude, a recent study of the European Committee estimates that the market value of the IoT in the EU will exceed one trillion euros in 2020. The European Commission is aware of the great potential of IoT and in the past has supported several projects for the development of IoT-based applications, protocols and policies for the secure, safe and privacy preserving deployment, mainly in the FP7 and Horizon 2020 programs. Actually, the EC plans to unveil a strategy for IoT for the mid of 2016, launching a series of large-scale pilots with an investment of more than 100 million euros. The huge expected growth of IoT in the next years and the planned investments ",,2020.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
c8003f27c8965688c6c2499759e0350ab6f3c608,https://www.semanticscholar.org/paper/c8003f27c8965688c6c2499759e0350ab6f3c608,Ambient Assisted Living for the Motor Impaired,"The field of Ambient Assisted Living (AAL) has shown great potential in counteracting some of the effects of the worldwide population ageing phenomenon. Its main goal is to promote a safe, healthy, and functional living environment for the elderly and people with disabilities who wish to live independently in their home. To achieve this goal, AAL environments utilize Information and Communication Technologies (ICTs) and the emerging Ambient Intelligence (AmI) paradigm in order to provide sophisticated solutions that can support the needs of an elderly person or a person with disabilities, at home. This chapter will present examples of AAL environments found in research and academic literature and the solutions they offer to cater for the basic needs of motor-impaired people in order to support their independent living and quality of life. The challenges of using such technologies will also be discussed. INTRODUCTION The World Health Organization (WHO) World Report on Disability (World Health Organization, 2011), states that approximately one billion people worldwide experience a disabling condition. This is the first ever global estimate of persons with disabilities in the last 40 years. The term “disabled” according to WHO is used for people who are experiencing a limitation in their movement, activities, and senses due to a physical or mental condition. The report also states that almost everyone will be temporarily or permanently impaired at some point in his or her life, especially when at old age. At the same time, people with disabilities are more susceptible to poorer health outcomes and lower education achievements, which often lead to higher rates of poverty than people without disabilities. Disabled people are in need of rehabilitation services in order to maximize their functioning required to support independence. But in developing countries such access to rehabilitation services is often limited and in some cases nonexistent altogether. Even in high-income countries about 20%40% of people with disabilities have limited assistance for their everyday activities. In the US, for example, 70% of adults have to rely on family and friends for assistance with daily activities. The number of people experiencing motor impairments and other disabilities is only expected to rise in the near future, as the world population continues to age at an unprecedented rate. According to the United Nations World Population Ageing report (United Nations, 2000), worldwide population ageing is enduring and has a growing rate of 2.6%per year, considerably faster than the population as a whole, which is increasing at 1.2 % annually. Europe is currently holding the highest proportion of older persons, with a population of 60 or over currently constituting 24.5% of its total population. In the United States that number is 19.1% respectively. From the above, it can be argued that the increase of the ageing population will have major implications for all aspects of people’s everyday life particularly of socio-economic nature. The number of people that will need some form of institutionalized help is going to increase, adding on the burden of the existing health care systems. Governments around the world have taken serious notice of this reality and of the need to come up with strategies to adapt their social practices and processes in order to accommodate this dynamic population shift in the population. The need to find ways to make it easier for people with age and other related disabilities to live a longer, satisfying and independent life in their own homes is now more imperative than ever. Ambient Assisted Living (AAL) is a domain that has attracted a steadily growing attention in the scientific community because it involves emerging innovative technological solutions that can counteract some of the challenges described above. The main focus in AAL is on supporting persons with disabilities in their own environment and providing the means to increase the degree of independent living. Its aim is to provide integral solutions in the areas of home care, independent living, and institutionalized care homes that will improve the quality of life and lower the costs involved with health, home care and related social services. In order to achieve the above, AAL depends heavily on Information and Communication Technologies (ICTs) and the emerging Ambient Intelligence paradigm. This chapter provides an overview of how Ambient Assisted Living technologies can play a catalytic role in improving the living environment for people with motor impairments by providing solutions that can increase their level of independence. The chapter begins with an overview of the fields of Ambient Intelligence and Ambient Assisted Living, followed by a brief presentation of the latest research initiatives in Europe. It then discusses how AAL can provide solutions for the fulfillment of the four identified requirements for independent living: mobility, environment control, safety, health and emergency assistance, and social inclusion. Finally, the major challenges of AAL are discussed followed by the conclusion. AMBIENT ASSISTED LIVING (AAL) OVERVIEW AAL refers to the use of Information and Communication Technologies (ICT) in a person's living environment in an unobtrusive way enabling them to continue living a comfortable, independent, active life and staying socially connected well into old age. AAL’s main goal is to provide the technological platform to support individuals in living an autonomous life for as long as possible. The roots of AAL are in traditional Assistive Technologies for people with disabilities, ‘Design for All’ approaches to usability and accessibility, as well as in the emerging computing paradigm of Ambient Intelligence (Pieper, Antona, Cortes, 2011). Ambient Intelligence (AmI) is a term that refers to the vision of a world in which smart, intuitively operated devices support users in an unobtrusive way in their everyday life. AmI has enabled the introduction of ubiquitous information, computational, and communication technology in a seamless yet unobtrusive way creating smart everyday living environments (Encarnação & Kriste, 2005). In such smart environments, intelligent applications and devices become aware of the human goals and needs by operating collectively and sharing information and intelligence through a hidden network that connects them in a way that is natural and intuitive to the user (Aarts & Encarnação, 2008). AmI infrastructures have gained a great momentum in today’s world in many industries such as home automation, entertainment, automotive, and healthcare to name a few. The technologies involved have the capacity to transform everyday common objects from CD players to coffee machines into smart objects that support context awareness, personalization, anticipatory behavior, and adaptation, all of which enable a certain degree of autonomous decision-making. Lighting, sound, vision, home appliances, and other electronic devices, all come into play in an AmI environment and share the same purpose, to improve user experience by facilitating the user’s interaction with it (Aarts & Kriste, 2005). Aarts and Encarnação (2008) stated that the notion intelligence reflects that the digital surroundings in a smart environment exhibit certain forms of social interaction, in other words they are able to recognize the occupants, adapt themselves to their needs, learn from their behavior, and possibly act on their behalf. Based on the described notion of intelligence they have synthesized the following list of the most important features of Ambient Intelligence: • Integration through large-scale embedding electronics into the environment • Context-awareness through user, location, and situation identification – the system uses sensors to perceive a situation, the location where that situation is taking place, and the user involved • Personalization through interface and service adjustment – the system can change its behavior according to the needs of the user • Adaptation through learning the user’s behaviors • Anticipatory behavior through reasoning – the system acts on behalf of the user making decisions based on predictions and expectations about future actions AAL with the help of the Ambient Intelligence (AmI) paradigm and new ICT technologies can now provide smart sophisticated solutions that offer the potential to change dramatically the quality of life for a disabled person, often making the difference from living with personal assistance on a daily basis to living an autonomous life. One of AAL’s focal concerns is also to offer user-friendly interfaces that are adaptable to the needs and abilities of the user and user-centric methods of interaction for the individual with his or her immediate environment (Pieper et al, 2011). AAL RESEARCH INITIATIVES IN EUROPE In the recent years, policy initiatives have been launched in Europe on the field of Ambient Assisted Living (AAL) in order to create a favorable ground towards research, development, and deployment of ICT technologies with focus on addressing the challenges, but also the opportunities of ageing. The “Ageing Well in the Information Society” Action Plan was adopted in June 2007 by the European Commission with the goal to bring forward a package of measures that should lead to greater uptake of ICTs by Europe’s senior citizens and stimulate industry to produce technologies appropriate for them (Stephanidis, 2011). For that purpose, the European Commission launched a dedicated action in the 7 Framework Programme and partial funding of the Ambient Assisted Living Joint Research and Innovation Programme, involving most EU Member States (Stephanidis, 2011). By 2013, the EU and Member States, and the private sector will have invested more than €1 billion in research and innovation for ageing well: some €600m in the Ambient Assisted Living Joint Programme (AAL JP), and an ",,2013.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
1a6e847a75917d9f6c3d3c3d3ff6bc8505f50b86,https://www.semanticscholar.org/paper/1a6e847a75917d9f6c3d3c3d3ff6bc8505f50b86,Vehicular Communications And Networks Architectures Protocols Operation And Deployment Woodhead Publishing Series In Electronic And Optical Materials,"A Trust-driven Privacy Architecture for Vehicular Ad-hoc NetworksIntelligent Vehicular Networks and CommunicationsEmerging Wireless Communication and Network TechnologiesVehicular NetworksBio-inspired Routing Protocols for Vehicular Ad-Hoc NetworksInformation Security of Intelligent Vehicles CommunicationVehicular Cloud Computing for Traffic Management and SystemsVehicular ad hoc NetworksMedical Internet of ThingsNetworking and Telecommunications: Concepts, Methodologies, Tools, and ApplicationsInterand Intra-Vehicle CommunicationsIT Convergence and Security 2017Optimisation Des Communications V2V Et V2I Dans Un Réseau de Véhicules OpéréRoadside Networks for Vehicular CommunicationsCloud and IoT-Based Vehicular Ad Hoc NetworksVehicle-To-Vehicle and Vehicle-To-Infrastructure CommunicationsBlockchainenabled Fog and Edge Computing: Concepts, Architectures and ApplicationsWireless Device-to-Device Communications and NetworksInterference in Vehicle-to-vehicle Communication NetworksWireless Networks and Industrial IoTGreen Computing in Network SecurityVehicular Communications for Smart Cars5G-Enabled Vehicular Communications and NetworkingAdvances in Delay-Tolerant Networks (DTNs)Rail Vehicle MechatronicsQuality of Service Architectures for Wireless Networks: Performance Metrics and ManagementCommunication Technologies for VehiclesVehicular NetworkingVehicular-2-X Communication断言Telematics Communication Technologies and Vehicular Networks: Wireless Architectures and ApplicationsVehicular TechnologiesCharacterization, Avoidance and Repair of Packet Collisions in Inter-Vehicle Communication NetworksIntelligent Transportation SystemsResearch Anthology on Architectures, Frameworks, and Integration Strategies for Distributed and Cloud ComputingRoadside Networks for Vehicular Communications: Architectures, Applications, and Test FieldsVehicular-2-X CommunicationEurope and MENA Cooperation Advances in Information and Communication TechnologiesVehicular Communications and NetworksCapacity Analysis of Vehicular Communication Networks During the last 15 years, the interest in vehicular communication has grown, especially in the automotive industry. Due to the envisioned mass market, projects focusing on Car-to-X communication experience high public visibility. This book presents vehicular communication in a broader perspective that includes more than just its application to the automotive industry. It provides, researchers, engineers, decision makers and graduate students in wireless communications with an introduction to vehicular communication focussing on car-to-x and train-based systems. Emphasizes important perspectives of vehicular communication including market area, application areas, and standardization issues as well as selected topics featuring aspects of developing, prototyping, and testing vehicular communication systems. Supports the reader in understanding common characteristics and differences between the various application areas of vehicular communication. Offers both an overview of the application area and an in-depth discussion of key technologies in these areas. Written by a wide range of experts in the field.Universal vehicular communication promises many improvements in terms of acdent avoidance and mitigation, better utilization of roads and resources such as time and fuel, and new opportunities for infotainment applications. However, before widespread acceptance, vehicular communication must meet challenges comparable to the trouble and disbelief that accompanied the introduction of traf c lights back then. The rst traf c light was installed in 1868 in London to signal railway, but only later, in 1912, was invented the rst red-green electric traf c light. And roughly 50 years after the rst traf c light, in 1920, the rst four-way traf c signal comparable to our today’s traf c lights was introduced. The introduction of traf c signals was necessary after automobiles soon became prevalent once the rst car in history, actually a wooden motorcycle, was constructed in 1885. Soon, the scene became complicated, requiring the introduction of the “right-of-way” philosophy and later on the very rst traf c light. In the same way the traf c light was a necessary mean to regulate the beginning of the automotive life and to protect drivers, passengers, as well as pedestrians and other inhabitants of the road infrastructure, vehicular communication is necessary to accommodate the further growth of traf c volume and to signi cantly reduce the number of accidents.This is the second volume of proceedings including selected papers from the International Conference on IT Convergence and Security (ICITCS) 2017, presenting a snapshot of the latest issues encountered in the field. It explores how IT convergence and security issues are core to most current research, industrial and commercial activities and consists of contributions covering topics including machine learning & deep learning, communication and signal processing, computer vision and applications, future network technology, artificial intelligence and robotics. ICITCS 2017 is the latest in a series of highly successful",,2017.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
5caa35987b5df145a5bdc8dd4766dbeff952ebe2,https://www.semanticscholar.org/paper/5caa35987b5df145a5bdc8dd4766dbeff952ebe2,Research on Application and Development of Financial Big Data,"This paper discuss the basic thinking, methods and tools of software engineering, masters the financial business knowledge, the analysis and design theory and methods of financial information systems, and has the ability to analyze, design, implement and maintain financial information systems, and can be used in financial applications. IT companies such as development companies and financial information system providers engage in the analysis, design and implementation of software, or engage in the analysis, design, implementation, maintenance and management of financial information systems in the IT departments of various financial institutions such as banks, securities and insurance. A financial information-based composite software talent with a solid professional foundation, broad knowledge, and the ability to adapt to the future development of information technology. Specifically, it is reflected in four aspects: knowledge system, professional skills, project experience and comprehensive quality: Introduction On July 1, 2017, the General Office of the State Council issued the “Opinions on Strengthening the Service and Supervision of Market Subjects by Using Big Data”; on July 4, the State Council issued the “Guiding Opinions on Actively Promoting the “Internet+” Action” On September 5, the State Council issued the ""Outline for the Promotion of Big Data Development."" The intensive introduction of these heavy documents marks the official establishment of China's big data strategic deployment and top-level design. Benefiting from the rapid expansion of the big data market, the demand for related IT support has exploded. Among them, enterprises that provide big data infrastructure, big data software technology services, and industry big data content consulting services have brought unprecedented Customer group. IDC predicts that by 2020, the company's expenditure based on big data computing and analysis platform will exceed 500 billion US dollars, and the compound growth rate will reach 34.1% in the next 5 years; in the next 3 to 5 years, China needs 1.8 million data talents, but currently only About 300,000 people. At the same time, China's colleges and universities in cloud computing, data science and other majors are still in their infancy, and the talents cultivated each year are far from meeting the needs of the industry. Therefore, it is imperative to open a big data major and accelerate the cultivation of talents. The financial industry is the industry that relies most on data and is the easiest to realize data. In recent years, emerging financial institutions such as consumer loans and P2P are the products of the combination of big data technology and finance. At present, the demand for big data talents in finance is extremely strong in China. Only Internet finance is one year, and the growth rate is 3-5 times per year. It is generally believed that there will be a gap of 1 million talents in Internet finance, and the most lacking is big data risk control talents, including data mining and statistical modeling talents from primary to advanced. Core and Featured Courses Cloud Computing and Introduction to Big Data As an introductory course in the direction of this major, this course introduces students to the concepts, technologies and applications related to cloud computing and big data, and enables students to establish a preliminary understanding of the relevant knowledge, technology and development prospects of the profession, as a guide for the follow-up course. Distributed Computing Framework Foundation As the foundation and core technology course of this major, this course introduces students to the basic concepts, installation and configuration of the Hadoop distributed computing system, distributed programming model (Map/Reduce), distributed file system (HDFS), and related scheduling. , monitoring and maintenance tools enable students to build a basic understanding of distributed computing systems, master the primary distributed application design and implementation methods, and lay the theoretical and practical foundation for subsequent in-depth courses. Distributed Database Management and Development As a core technical course in this major, this course introduces students to the basic concepts of distributed databases, installation and configuration, management and maintenance, data access and development. The course focuses on NoSQL databases such as HBase, MongoDB, Redis, etc., and describes their use and development in a distributed environment. To enable students to establish a basic understanding of distributed databases, master the primary distributed database application system design and development methods, and lay the theoretical and practical foundation for the subsequent in-depth courses. Distributed Computing Framework Component Technology As a core advanced course in this major, this course introduces students to mainstream components on the Hadoop distributed computing platform, including Hive, Pig, Sqoop, Flume, Kafka, Zookeeper and more. Enable students to have a complete Hadoop ecosystem-based design and implementation of big data applications. Real-time Calculation and Memory Calculation As a core advanced course in this major, this course introduces students to high-performance distributed computing frameworks, including Storm and Spark, as a more powerful alternative to the Hadoop framework. Data Visualization Technology As an elective course in this major, this course introduces students to the basics of data visualization and the design and use of platforms and development tools, including Excel, Reporting Services, Chart.js, D3.js, Tableau, etc. Through this course, students will be able to present the results of big data processing in an efficient, flexible and friendly manner. Data Statistics and Analysis As a core advanced course in this major, this course introduces students to statistical analysis techniques based on Python and R. Including data file editing and finishing, basic statistical analysis, parameter estimation and hypothesis testing, non-parametric testing, analysis of variance, correlation analysis, regression analysis, cluster analysis, discriminant analysis, factor analysis, correspondence analysis, reliability analysis, survival Analysis, time series analysis, and the drawing of statistical graphs enable students to master the processing and analysis methods of typical industry business data. Knowledge System Mathematical basis: Including calculus, linear algebra, probability statistics, numerical analysis, etc. IT foundation: Including operating systems, networks, databases, software engineering, programming techniques, data structures and algorithms, etc. Knowledge base in the financial sector. Including international finance, marketing, insurance, securities investment, etc. Professional Skills Database system management and development: MySQL, MongoDB, Redis, HBase, etc. Big Data Application Development Language: Java as the core, supplemented by Python, Scala, R, etc. Construction, configuration, development and deployment of big data processing frameworks: Hadoop, Storm, Spark, etc. Use of data analysis and presentation tools: reporting tools, D3.js, etc. Project Experience Familiar with enterprise software project life cycle, development process, specification, etc. Understand and implement software quality requirements: performance, security, scalability, maintainability, reliability, etc. Understand the financial industry: industry background, business model, market characteristics, and how data and IT systems are used in the financial industry Comprehensive Quality Good professional basic qualities: document writing, presentation reporting, business communication, etc. Strong learning ability and study habits, has a certain degree of microinnovation, data awareness Course Settings Table The professional competence-course structure derivation process mainly includes two stages: “computation ability theme” and “capability-curriculum structure transformation”. The main process of the first phase of the ""computational power theme"" is as follows: Figure 1. The data analysis process of ""ability topic calculation. Hadoop Big Data Integrated Experiment System This experimental system is designed to provide students with a complete set of Hadoop and its environment, design, development, monitoring, maintenance tools, software and services. With this experimental system, the experimental and training environment requirements of the core technology courses of this major can be met. This experimental system is divided into two major components: A virtual lab environment for students to learn big data. The environment is carried out by means of the aforementioned virtualized desktop teaching system, and the network administrator configures the big data learning virtual machine in advance for the students to use. The real environment for research or large-scale case presentations. This environment is carried out through several servers. Main Function A. Basic platform: The basic platform for big data storage and processing, which can realize the storage and management of massive data, support common components of platforms such as Hive, Impala, Pig, Spark, and Yarn, and provide support for data analysis services on the platform. These common components increase the ease of use of platform data, making data manipulation and data analysis easier to use, saving labor and reducing labor time. B. Data integration: support the unified storage of massive structured data, semi-structured data, and unstructured data, deepen the expansion of enterprise intelligence and service capabilities, and improve the decision-making level of enterprises. We can use enterprise-level data ETL tools or open source ETL tools. For example, Flume, Sqoop, Kafka, etc., integrate externally structured, semi-structured and unstructured data into big data platforms. Through this pla","DEStech Transactions on Social Science, Education and Human Science",2019.0,10.12783/dtssehs/aems2018/28012,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
6afcb962a010494b94ba09d36a330e59d560b433,https://www.semanticscholar.org/paper/6afcb962a010494b94ba09d36a330e59d560b433,Integration of High Performance Computing into Engineering Physics Education,"Computational skills are foundational in engineering physics education. Computational exercises, labs, and projects often employ instructive small­scale problems. These small­ scale problems serve to introduce content and process, and as such, serve the purpose for which they were intended.  Small­scale problems do not serve to introduce students to solving problems at industrial­scale or with research­quality as required in the workplace or graduate laboratory This paper describes the integration of industrial­scale and research­quality high­performance computing (HPC) into a senior/graduate level fluid dynamics course. This paper focuses on a combined senior level­graduate level course (enrollment of 12) in fluid dynamics at the University of Central Oklahoma, a predominantly undergraduate institution (PUI) . A HPC cluster, Buddy has been deployed recently at the UCO. The first author operates and administers the Buddy cluster and serves as instructor of the fluid dynamics course, providing an opportunity to advance the course outcomes to include a high impact project that takes advantage of distributed computing. These projects will be transformative for the students and expose them to HPC “at scale.” The projects require the use of computational fluid dynamics (CFD) on an HPC system; intentionally exposing students to a new way of doing things. The issues that students must confront include: 1) complex geometric modeling that result in very large file sizes, 2) meshing geometries that are large or require many nodes, 3) transitioning files generated on a desktop computer to a HPC environment, 4) understanding navigation and use of an HPC system, 5) understanding the use of parallelism in a distributed computing environment, 6) quantifying results, and 7) visualizing results. The goal of this work is to impact the student’s long term ability to deal with computationally intensive problems. Although we cannot determine the impact long term yet, we are using a rubric to gauge the immediate impact and surveying the students to determine their perceptions. Introduction The National Science Foundation (NSF) report entitled “Cyberinfrastructure Vision for 21st Century Discovery”​1​ addresses how high performance computing (HPC) is necessary to science and engineering disciplines to answer the most basic research questions and to solve technical problems of national need. More recently the White House has undertaken the National Strategic Computing Initiative​2​ which is a call to “maximize the benefits of high performance computing (HPC) research, development, and deployment.” The increased use of HPC clearly results in the need to train engineers how to appropriately use HPC in their work as HPC becomes more ubiquitous in industry. The use of computational tools in engineering education is so common it is essentially codified by ABET; currently as part of ​Criterion 3. Student Outcomes​3​. ​Accordingly, students across undergraduate engineering programs get exposed to computer programming, modeling software, mathematical engines, spreadsheets, and simulation. Specific engineering disciplines are exposed to more focused software for computer­aided design, circuit design, machining, data acquisition; and more and more students are using microcontrollers to implement their own electro­mechanical systems. These software, and where appropriate, the attached hardware, are almost exclusively run on or controlled by either desktop or laptop computers. As a result of the availability and accessibility of HPC resources some have been able to enhance traditional engineering and computing curricula using HPC​4­6​. Background This paper documents activities of integrating HPC at the University  of ___ (U__), which is a metropolitan university with an enrollment of over 17,000 students and a predominantly undergraduate institution (PUI). At UCO, undergraduate research has been supported and nurtured across campus; and recognized by the Council on Undergraduate Research (CUR) as a national model for implementing programs in undergraduate research​7​. Campuswide grant programs for faculty grants and student grants are in place. The student grants program is of particular note; in Research, Creative, and Scholarly Activities (RCSA) Grants encourage students to collaborate with a faculty member to write a grant proposal. If funded the student receives up to $500 for supplies and equipment, works five hours per week as a research assistant, and receives a partial tuition waiver. This program has grown considerably in the last several years and now funds over 130 students each year. Within the College of Mathematics and Science (CMS) additional programs are in place to cultivate undergraduate research. Center for Undergraduate Research and Education in Science, Technology, Engineering, and Mathematics (CURE­STEM) Scholars (approx. one­third of CMS faculty) receive funding for reassignment time, travel, student research assistants, and supplies. The CURE­STEM Scholars are required to submit one national­level (e.g. National Science Foundation ­ NSF) grant per year. This program has been in place for eight years and has shown a tremendous return on investment of over $10 brought in for every $1 invested. Co­author Lemley has been a CURE­STEM Scholar and also serves as the director of a computational center within CMS called CREIC (Center for Research and Education in Interdisciplinary Computation), whose goal is to stimulate and enable faculty and their students to embed computation into their research and classes. CREIC has been focused on establishing a HPC resource on campus for several years, and was successful in obtaining an NSF Major Research Instrumentation (MRI) grant, with co­author Lemley as the Principal Investigator (PI), in early 2015. The NSF­MRI has funded the first HPC cluster supercomputer (Buddy) on campus and now co­author Lemley is helping researchers at UCO use Buddy to perform their work. This study described in this paper was conducted, in part, during a 3­semester hour fluid dynamics course, ENGR 4533/5443, in Fall 2015 at UCO. This course is a follow­on course to a junior level engineering fluid mechanics course and was made up of six undergraduates and five graduates. This course covers continuum viscous fluid dynamics; the first portion of the course is focused on understanding and applying the Navier­Stokes equations (NSE), which are a set of partial differential equations describing fluid flow.. The latter part of the course is focused on using computational fluid dynamics (CFD) to solve the NSE. Individual CFD projects were completed by the students. In these projects, students were required to develop a problem that needed significant computational resources ­ such that it was not reasonable to run the simulations on a single computer workstation. The goals for implementing the CFD project in this way was to make an impact on the student’s long term ability to use HPC when they graduate, which is becoming a necessary engineering technical skill. Methodology The CFD projects in ENGR 4533/5443 were started in late September 2015. The students were asked to develop a problem that was either of research interest or industrial­scale, meaning, not only would CFD be required, but HPC using the Buddy supercomputer cluster would also be required. The project was carried out as shown in Table 1. Table 1.​ Details of CFD Project in EN__ 43__/54__. Assignment  Description When Credit Project Proposal  Up to two page proposal with citations. Late September  10% Homework / 2.5% Course Project Description   Five Minute Proposal to Class Early October  10% Homework / 2.5% Course Computer­Aided Design Model Email to Instructor  Mid­October  10% Homework / 2.5% Course Meshed Geometry  Email to instructor  Late October  10% Homework / 2.5% Course Project Poster Session  Large Format Poster ­ Interviews by faculty on oral presentation and appearance (rubric). Early December  50% Project / 12.5% Course Project Poster Technical Review Instructor technical review of poster results using rubric. Mid­December  50% Project / 12.5% Course Survey Survey of student perceptions of the project. January 2016  NA The ​Project Proposal​ was a two page proposal with citations that described the problem on which the student wanted to work. The requirements were the motivation for working on a given problem, the geometry that would be considered, and as much detail about boundary conditions as possible. This proposal was emailed to the instructor, who then either spoke with the student in class or emailed them about tweaking their proposal so that it was appropriate for the class and truly needed a cluster supercomputer for solution. The ​Project Description ​took place roughly two weeks after the ​Project Proposal​ and involved a five minute presentation in a regular class meeting. By this point the students were expected to have a sketch or initial computer­aided design (CAD) of the geometry and detailed boundary conditions. There was a small amount of time for questions by the audience. Over the next several weeks the instructor spent several class periods covering the topics in Table 2, with the goal of taking student through a complete project starting with a CAD drawing all the way to visualizing and calculating results. the goal was to ensure that students had seen the many issues that arise in what is often a tedious process. In addition, by having some sessions where the students worked on their projects using the Buddy Cluster, the instructor could see from the student perspective and see what did and did not work well for them. Also he could observe how well the documentation for the project and using Buddy had been written. Table 2.​ Learning Modules in the CFD Project. Learning Module Approximate Time Used in Class Importing CAD files into Meshing Software 10 minutes Meshing a Geometry and Preparation for CFD 30 minutes Logg",,2016.0,10.18260/p.25426,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
aeefe6929553a42a9a6371ce80aea6094fdb8793,https://www.semanticscholar.org/paper/aeefe6929553a42a9a6371ce80aea6094fdb8793,Spectrum Efficiency and Energy Efficiency in Wireless Communication Networks,"In the October 2020 issue of IEEE Wireless Communications Magazine, we are pleased to present a special issue on “Spectrum and Energy Efficient Wireless Communications” with a collection of 12 articles. In this issue, we are also very glad to present 12 articles accepted from the open call. Driven by new-generation mobile devices and bandwidth consuming applications such as video streaming, wireless traffic volume is expected to continue expanding tremendously in the next few years. Sustaining this growing trend will in turn require higher spectrum capacity from the network side. Research has shown that capacity demand increases much faster than the current spectrum efficiency improvement, in particular at hot spot areas. From recent data, global mobile data traffic increased nearly 11-fold in the last few years. In contrast, the peak data rate from 3G wireless technology to 4G wireless technology only increased 55 percent in the last decade. Clearly there is a huge gap between the capacity growth of new wireless access technologies and the fast growth of wireless traffic volume for the next-generation wireless networks. In the meantime, energy efficiency, commonly defined as information bits per unit of energy, has become another essential requirement for the design of future wireless communication networks besides spectrum efficiency. Energy efficient communications have attracted great attention due to the ever-increasing demand to preserve energy resources and to protect the environment. Furthermore, mobile devices, such as smart phones and tablets, are widely used to conduct new applications such as video content distribution, location-aware advertisement, video chatting, video streaming, music and movie downloading, etc. In the year 2012, mobile video traffic exceeded 50 percent of the total wireless traffic volume for the first time. Mobile video has increased 14-fold since then, accounting for 69 percent of the total mobile data traffic by the end of last year. How to support energy and bandwidth consuming video applications with high QoE is becoming another challenging issue in future wireless networks. Clearly, there is an urgency for a new disruptive paradigm to bridge the gap between the increasing capacity, energy and QoE demands and the deficiency of radio spectrum resources. As wireless channel efficiency is approaching its fundamental limit, improvements in future wireless system capacity can be alternatively realized by networking technologies such as node density increase through underlay and overlay deployments, or by going to a higher spectrum such as millimeter wave to seek more spectrum bandwidth. In addition to delivering the required network spectrum efficiency, energy efficiency and QoE, the anticipated tremendous proliferation of machine-type devices and consumer-wearable devices also makes the underlay wireless network desirable. These small devices usually have limited onboard processing power and battery size. If they need powerful computing capability to process extensive content information, they will have to heavily rely on their surrounding local networks and computing platforms to facilitate these computing-intensive and thus power consuming applications. Using high performance and very low latency communication links to offload mobile device computing load into nearby powerful computing clouds becomes an essential direction to pursue. This paradigm shift in the next decade also calls for the cluster-based underlay networking technologies, in which a cluster head of a number of underlay devices can be selected as the representative of the entire cluster for both communication and control purposes. Spectrum and energy efficient wireless communications is one of the most important topics today in the next generation wireless networking area, and attracting more and more attention from industry, research, and academia. This special issue focuses on the challenges and novel solutions for spectrum and energy efficient wireless communication networks. Thanks to the guest editors, Q. Liang, T. S. Durrani, J. Koh, Q. Wu, and X. Wang, who did an excellent job in editing this special issue for our readers. Please stay tuned for new developments in the research area of spectrum and energy efficient wireless communications, and read the editorial for more details about the papers in this special issue. In addition to the 12 articles in the special issue, we have also included 12 accepted open call articles. The first article, “Dense Small Satellite Networks for Modern Terrestrial Communication Systems: Benefits, Infrastructure, and Technologies” by N. Hassan et al., demonstrates several components of dense small satellite networks (DSSN) infrastructure, including satellite formations, orbital paths, inter-satellite communication links, and communication architectures for data delivery from source to destination. It also reviews important technologies for DSSN as well as the challenges involved in the use of these technologies in DSSN. Several open research directions to enhance the benefits of DSSN for MTCS are also identified in the article. A case study showing the integration benefits of DSSN in mobile terrestrial communication systems is also included. The second article, “Overcoming the Channel Estimation Barrier in Massive MIMO Communication via Deep Learning” by Z. Liu et al., discusses the application of deep learning (DL) for massive MIMO channel estimation in wireless networks by integrating the underlying characteristics of channels in future high-speed cellular deployment. It develops important insights derived from the physical radio frequency channel properties and presents a comprehensive overview on the application of DL for accurately estimating channel state information (CSI) with low overhead. The article provides examples of successful DL application in CSI estimation for massive MIMO wireless systems and highlights several promising directions for future research. In the third article, “Energy-aware Task Offloading in Internet of Things”, J. Li et al. introduce a new energy-aware task offloading scheme in IoT to determine the optimal offloading strategy. First, they investigate the architecture of mobile edge computing (MEC) in IoT. Second, they discuss the challenges of task offloading in MEC for IoT. Third, they propose the framework of task offloading for MEC, the optimal offloading strategy for computing task is achieved. Finally, the article demonstrates experiment results to show that the proposed scheme can significantly improve the efficiency of task offloading compared with the conventional scheme. Yi Qian",IEEE Wirel. Commun.,2020.0,10.1109/mwc.2020.9241874,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
a40cc32b94e828b64707aca8f026cd46a4fa7cfe,https://www.semanticscholar.org/paper/a40cc32b94e828b64707aca8f026cd46a4fa7cfe,Proceedings of First International Conference on Advanced Trends in ICT and Management (ICAITM),"S.No. Paper Title Author Name Page No. 1 Using A Remote Training Model For Technical / Vocational Education And Apprenticeship Enhancement Mr. Emmanuel Fon Tata 1 2 Challenges In Application Of ELearning To Extension In Kenya: A Case Of South Western Kenya Prof. Anakalo Shitandi 2 3 Cloud Computing And Its Related Technologies: Issues And Challenges Mr. Sulaimon Mr. Hakeem Adewale 2 4 A Unified Customer Service Model Optimizing Customer Service Quality for the Nigeria Electric Power Industry through the Implementation of an ITIL Based Service Management Paradigm Mr. Femi Akin-Laguda 3 5 Analysis Of Train Energy Consumption Reduction By Passing Low Passenger Flow Stations In Off-Peak Hour For Addis Ababa LRT Mr. Eshetu Shewalema 4 6 Higher Performance And Cloud Computing Mr. Foldestine Paye 5 7 Multitemporal Remote Sensing Of Landscape Dynamics And Pattern Change In Dire District, Southern Ethiopia Mr. Berhanu Keno 5 8 Assessing Governance Practices Among Micro-Finance Organisations: A Case Study Of Selected Firms In The MicroFinance Industry In Ghana Mr. Justice Paul Donkor 6 9 M-Commerce The New Phase Of Doing Business In The 21St Century Opportunities And Challenges Mr. Darko-Ampem A. Emmanuel 7 10 Influnce of Digital Audio Technicals on research and production in the music Industry in Africa : A case of CoTE D'ivoire, Dr. Anoha Clokou 7 11 Small And Medium Enterprises: A Survival Strategy For Nigeria Youth Empowerment Mr. Agboje Egbonimali Shadrack 8 12 Development Programmes And Follow-Up Procedures For Nigeria Colleges Of Education Trainers Using Cloud Computing Dr. A.A.Ladan 9 13 Work Place Violence, Aggressive Behaviour And Organizational Productivity Mr. Abdulsabur Hassan 9 14 Electricity Challenges On Small Business Enterprises In Nigeria Mr. Sidi Jelani 10 15 E-Commerce And Consumer Rights: Applicability Of Consumer Protection Laws In Online Transactions In East Africa. Mr. Sadat Mulongo Lunani 11 S.No. Paper Title Author Name Page No. 16 Change Management And Organizational Performance Measures Mr. Agboje Chukwuma 12 17 The Leadership And Funding Challenges Of University Information Technology Centres: A Case Study Dr. O. Osunade 13 18 The Impact Of Digital Divide On Cloud Migration In Africa: A Case Ke ya’S Te h ologi al State. Mr. Mainye Marcella Moraa 14 19 MOOCs In Nigeria: Awareness And Adoption Mr. Shamsuddeen H. Muhammad 14 20 Applying Rough Set Theory To Yorùbá Language Translation Mr. Fagbolu O O, Obalalu B S, Udoh S.S 15 21 Enterprise Resource Planning In Africa: A Case Study Of University Of Ibadan O.Osunade,O.Oladele, O.Omolola. 16 22 A Frame Work for Implementation of an E-Classroom System Mr. Shamsuddeen H. Muhammad 16 23 The Need To Fasten Cloud Adoption In Developing Countries Of Africa Mr. Shamsuddeen H. Muhammad 17 24 Usi g Co puter Ga es To I prove First Year Stude ts’ Learning Of Computer Programming Ms. Esther Gyimah 17 25 Impact Of Stakeholder Consultation On The Success Of Road Construction Project Ghana. Mr. Kwame Ofori 18 26 Security Issues Associated with Cloud Computing Mr. Mustapha Muhammad Sani 19 27 The Myths And Facts About Cloud Computing – Examining The Positions Of Start-Ups/SMEs Mr. Edward Daniels 20 28 The Impact Of Employee Empowerment On Service Quality Delivery And Customer Satisfaction At Chicken Republic . Mr. Daniel Adjei 20 29 The Influence Of Television Advertisement On The Youth Purchasing Behaviour Mr.Daniel Adjei Ms. Eunice Akorfa 21 30 Ethical Issues In Business Conduct Mr. Daniel Adjei Ms. Eunice Akorfa 22 31 Adolescent Socialization Environment As Predictors Of Unsafe Internet Behaviour Among Secondary School Students In Ibadan North Local Government Area Of Oyo State, Nigeria Mr. Ruth Ochanya Adio-Moses 23 32 Cloud Computing, An Avenue For Enhancing E-Procurement For Sustainable Development. Mr. Omane Kofi Wilson 23 33 The Introduction Of Human Resource Management In Ghana: Public Universities And Private Sector Perspectives Mr. Abdul-Kahar Adam 25 34 Statistical Analysis Of Knowledge And Utilization Of Cloud Computing By Smart Phone Users – A Case Study Of MTN Data Shop Customers Mr. Akpor-Mensah Edmund ,Mr. Dzivor Nelson Doe 26 35 Statistical Modelling Of Cloud Computing Utility In Tertiary Institutions In Accra (Ghana) Mr. Akpor-Mensah Edmund, Mr. Dzivor Nelson Doe 26 S.No. Paper Title Author Name Page No. 36 Adoption Of Cloud Computing As A Key Strategic Tool For Business Sustainability Mr. Akpor-Mensah Edmund, Mr. Dzivor Nelson Doe 27 37 Web Based Housing Management System Mr. Paul Adeoye Omosebi 27 38 Politics In the Cloud : An Argument for Cloud Based Software in Politics Mr. Nana Amankwah Peprah Mr. Kamal Kant Hiran 28 39 Mobile Assemblages and Development (maendeleo) in Marakwet Kenya Dr. Leah Jerop Komen 30 40 Investigating the Impact of ICT on the Enhancement of Learning amongst Special Needs Students Mr. Emmanuel Fon Tata 30 41 A Remote Training Model: A New Paradigm For Technical and Vocational Apprenticeship In Ghana Mr. Emmanuel Fon Tata 31 42 Cloud Computing as a Suitable Alternative to The Traditional On-Premise ERP And Massive Data Storage Mr. Mbanzabugabo Jean Baptiste 32 43 Gha a’s ICT4AD Poli y Do u e t – A Diminishing Significance Mr. Kubuga, Kumangkem Kennedy Mr. J. Kok Konjaang 33 44 Improving Health Care Delivery in Ghana: A Need of Urgency for NHIS Card Upgrade Mr. Mensah Sitti 33 45 Cloud Computing: A Catalyst in the Agenda of Education for All Prof. Patrick E. Eya, Dr. Samson Sunday Afolabi 34 46 For better or for worse: Effect of technological revolution on family communication Mr. Albert AnaniBossman 35 47 Application of the Excellent Principles of Public Relations in a Different Cultural Context: The Case Study of Ghana Mr. Albert AnaniBossman 36 48 Social Networking and Interpersonal Communication: how online identities impact on off line relationships Mr. Albert AnaniBossman 38 49 The Adoption and Deployment of Technology in Inventory Management Systems of Public Institutions. A Case Study of Electoral Commission of Ghana (EC) Ms. Priscilla Hanson 41 50 Effective use of Cloud Computing Services in Higher Education Mr. Sujith Jayaprakash 41 51 Antecedents of Employee Job Stress: Evidence from the Insurance Industry in Ghana Ms. Evelyn Twumasi 42 52 Analysis of Elman Neural Networks for Wavelet Transforms Based Feature Extraction in the Classification of Epilepsy Risk Levels from EEG Signals Dr. Vijayakumar T and Prof. T. Harikumar 43 53 An analysis of Challenges and opportunities for using Electronic Commerce in Ethiopia. Dr. S. Anbarasu 44 54 Paper Title Author Name Page No. S.No. Internet Use among Senior High School Students in Ghana: A Study of La Presbyterian Senior High School Mr. Philip Dornyo, Ms. Eunice Akorfa Adiko 44 55 Management of Technology and InnovationPerspectives on the Indian Banking Industry Dr. M. Thanikaivel 45 56 Cloud Computing – The Pathway and the Future Hope for Afri a’s sustai a le edu atio Ms. Eva Esther Shalin Mr. Samual Edem 46 57 Botnet Detection Using Data mining Techniques in Cloud networks Mr. S. Nagendra prabhu Dr. D. Shanthi 46 58 Online Password Protection Using Persuasive Cued Click Point Method Dr. Ra. Parivallal Dr. V. S. Prakash Mr. Manikandan 47 59 Knowledge Based Analysis of Microarray Gene Expression Data in Oncology Dr. W. Jai Singh Ms. N. Nivetha Rani Ms. S. Nivitha 47 60 Network On Chip: A New Frontier For Highly Scalable And Energy Efficient Multicore Systems Mr. Charles Saah 48 61 Auto Recovery Of Virtual Machine In A Cloud Based System After Memory Error Attack Mr. Charles Saah 49 62 A Study On Security Issues In Cloud Computing Mr. Vijesh Krishnamoorthy 49",,2016.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
fd9e632b6e9d761909847439bd3598b6f880af7b,https://www.semanticscholar.org/paper/fd9e632b6e9d761909847439bd3598b6f880af7b,Augmented Intelligence: Enhancing the Roles of Health Actuaries and Health Economists for Population Health Management.,"Achieving desired population health and business outcomes requires new levels of productivity, efficiency, and risk minimization. Machine learning (ML), cognitive computing, natural language processing, and other augmented intelligence (AI) tools are increasingly applied to health care initiatives. AI applies to a plethora of artificial intelligence technologies and tools currently used in health care. AI implies any kind of modeling that supplements humans, augmenting human intelligence, not replacing it. Health care decision makers are embracing AI, thereby leveraging information from multiple sources to enable improved individual and population outcomes. ML, for example, applies algorithms and decision-making tools to enormous amounts of clinical data from electronic health records, pharmaceutical databases, and unstructured text data. AI analytics are in predictive risk assessment, clinical decision support, home health monitoring, finance, and resource allocation. These applications are integral to population health management (PHM), Accountable Care Organizations (ACOs), Medicare Advantage, and public health initiatives. A recent hospital survey suggests a promising future for AI that is particularly relevant to PHM. The survey found that, of 7 applications, AI is likely to have the greatest initial impact on population health (24%), clinical decision support (20%), and patient diagnostic tools (20%), followed by precision medicine (14%), and hospital/physician workflow (8%). A variety of AI applications are being tested for health care. But growth and promise can only be met if the additional information is accessible and useful to CEOs and key policy makers. Clinical informatics underlies efforts to improve quality of care and PHM. Considerable work is being done by data scientists, such as Google/Sanofi’s joint venture, Onduo, that will collect and monitor real-time data on people with diabetes. This initiative, like many others, will fail unless there is a way to interpret the data, identify opportunities for intervention, and deliver the information in real time to clinicians and patients. Results are likely to be suboptimal if complex health care AI applications rely only on data scientists. Even with excellent abilities, most data scientists lack industry knowledge and the particular skills of health actuaries (HAs) and health economists (HEs), such as risk analysis and behavioral economics. HA and HE expertise is vital to maximizing value from the use of AI. These players guide health care business decisions, policy making and operations improvements, creating useful outputs for pricing, coverage decisions, business advances, and policy making by applying sophisticated statistical modeling and analytic tools. Although there is overlap between the abilities of actuaries and economists (Table 1), they bring distinct and essential skill sets to an analytic team. AI has been called the world’s most valuable resource as it allows users to extract increasing, ongoing actionable insights and value from myriad sources and uses of data. HAs and HEs can leverage AI to provide expanded and more specific guidance to payers, providers, suppliers, programs, and health systems. For example, AI-enhanced HA expertise helps ACOs to assess and pinpoint risk under capitated contracts. HEs can incorporate the HA evaluation and other AI-enabled analytics to recommend the optimal deployment of care management resources to achieve contracted clinical and financial outcomes. Blending PHM and AI-driven precision medicine could yield a new health care services paradigm. The shift to value-based care has increased the risk borne by providers significantly and increases pressure to identify and manage risk and evaluate outcomes. Innovative use of AI may help identify short-term clinical goals, reduce risk to increase savings, or serve as a return on investment indicator. Opportunities to enhance PHM exist with predictive modeling for identifying high-risk patients. AI tools aid prediction and help identify patients with multiple chronic conditions who are moving across disease states/trajectories that are often associated with increased resource use and cost. Gaps exist. Predictive modeling is hampered by missing pieces – what works and how it works – and gaps in comprehensive information. The common practice of segmenting populations by specific conditions (eg, diabetes) is highly inefficient because most people have multiple chronic conditions. Although HAs use predictive modeling",Population health management,2018.0,10.1089/pop.2017.0146,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
dc7ae05bd6a4104aa96a2c59e92a390f99c24de5,https://www.semanticscholar.org/paper/dc7ae05bd6a4104aa96a2c59e92a390f99c24de5,Internet of things for health and well-being applications,"The Internet of Things (IoT) has become one of the most disruptive revolutions since the advent of the Internet. The IoT brings a shift from a network of personal computers towards a network of systems and devices of a diverse nature which touches upon many aspects of daily life and affects most industries globally. IoT has the capability of augmenting our lives by completely reshaping the world and our interactions with it as we know them. IoT arrives at a time where healthcare and wellbeing needs are a rising demand, with a growingly ageing society in seek of care. Given this demand, IoT technologies have been applied to this domain to increase the quality of health care and overall well-being while reducing related costs and overheads. Examples of this include: IoT-enabled residential environments to provide assisted living for dementia suffers, well-being monitoring and intervention powered by low-cost sensing devices, and quantification of the self. This special collection accepted eight articles submitted by authors from Chile, Mexico, Slovakia, Spain, and the United States of America after undergoing a rigorous peer-review process. These articles bring together the latest experiences, findings, and developments on IoT for the provision of personalised health and well-being services. IoT infrastructures are of great interest because some of them can facilitate obtaining data about potential users or patients unobtrusively. This is particularly true in settings where users must go by their day-to-day activities without much interference. In this direction, López-Nava and Muñoz-Meléndez proposed the use of wearable inertial sensors in combination with machine learning models to detect a set of activities of daily living, such as functional mobility, and instrumental activities of daily living, like preparing meals. These activities are performed by test subjects in their homes in naturalistic conditions, thus transcending from standard in-the-lab studies. González et al. developed and validated a wearable IoT infrastructure for characterising gait in older adults in elderly care homes, in which they used inertial sensors positioned on the upper back of older adults. They validated the proposed infrastructure by carrying out a cross-sectional study with 81 older adults in two nursing homes in Spain. This very problem is also approached by Kessler et al. through the use of similar sensors but embedded directly into the floor. Instead of measuring the inertia of the body, the authors characterised the walking patterns by measuring and analysing the vibrations generated on the floor by the users’ footsteps. López-Medina et al. produced a solution which aims to detect falls in at-risk populations. This involved the use of privacy-preserving, low-resolution, thermal vision sensors intended to be mounted to the ceiling of a domiciliary area. Data from these sensors were processed in real-time using a variety of convolutional neural network architectures which were turned to detect a representation of a fallen individual within a perceived area. This was evaluated and was shown to perform with a high level of accuracy. Smartphones stand out as possibly the richest IoT sensor ecosystems to date. These devices are standardly instrumented with a wide spectrum of hardware and software sensors that can be used in multiple applications. Remarkable efforts have been devoted in recent years to develop frameworks to facilitate the widespread use of smartphones in health and well-being studies as mobile sensing infrastructures. Felix et al. presented a novel tool that leverages mobile phones not only to collect data via their sensors but also to process them on the device as soon as they are gathered. The framework allows researchers to easily configure the required processing routines on mobile phones remotely. In doing so, this work proposes a new approach for rapid deployment of sensing campaigns targeted at scientists with basic technical knowledge and requiring low effort. One major goal of a relevant number of IoT-based health solutions is to change behaviour effectively. Rossel et al. devised, developed, and evaluated a wearable system which assists with the cessation of smoking particularly. The solution developed a low-cost device that may be worn on the body or affixed to the clothing. This device detects levels of atmospheric factors indicating smoke or secondhand smoke. The device is",Int. J. Distributed Sens. Networks,2021.0,10.1177/1550147721999986,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
2fe235d1b325bb6114540bc30ff79cc7c2d496b3,https://www.semanticscholar.org/paper/2fe235d1b325bb6114540bc30ff79cc7c2d496b3,An artificial neural network-based ensemble model for credit risk assessment and deployment as a graphical user interface,"Credit risk is a common threat to the financial industry since improper management of credit risk leads to heavy financial losses in banking and non-banking sectors. Data mining approaches have been employed in the past to assess the credit risk. This study utilises the German credit dataset sourced from UCI machine learning repository for generating an artificial neural network-based ensemble learning model for credit risk assessment. Eleven data mining algorithms have been applied on an open source tool Weka for performing credit ratings on the German credit dataset using supervised learning approach. The performance of each algorithm was evaluated, and algorithms with the most diverse false positive and false negative results and that are highly accurate were selected for generating an ensemble model. The predicted outcomes of the top five ranked algorithms were fed into a feed-forward artificial neural network by employing an 'nnet' package in R. The artificial neural network-based ensemble model attained an accuracy of 98.98%, performing better than the individual component algorithms. Based on this ANN-based ensemble model, an interactive graphical user interface was further developed in R. The user-friendly graphical user interface can be used by financial organisations as a decision-support system for assessing the credit risk.",Int. J. Data Min. Model. Manag.,2017.0,10.1504/IJDMMM.2017.10006638,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
2c8d9135b0b21434c9d652bbe9185715126966e6,https://www.semanticscholar.org/paper/2c8d9135b0b21434c9d652bbe9185715126966e6,COMPUTATIONAL MODELS FOR SUSTAINABLE DEVELOPMENT,"Genetic erosion is a serious problem and computational models have been developed to prevent it. The computational modeling in this field not only includes (terrestrial) reserve design, but also decision modeling for related problems such as habitat restoration, marine reserve design, and nonreserve approaches to conservation management. Models have been formulated for evaluating tradeoffs between socioeconomic, biophysical, and spatial criteria in establishing marine reserves. The percolation theory and shortest path modeling have also been used. In this article we discuss the computational models that have been developed keeping in mind the sustainable developmentConservationists estimate that alarming rate at which biological species are disappearing will have an indelible impact on humanity. Targets which were set in 2002 to reduce the biodiversity loss by 2010 have not been met. The third global diversity outlook report said that loss of wildlife and habitats could not only exacerbate climate change through rising emissions but could also have a negative impact on food sources and industry. Computational Models in Aid Of Developing Policies For Sustainable Development The development of right policies for sustainable development is very important and involves complex decision making about the judicious use of natural resources and about striking a balance between societal, economic and environmental needs. Computational models have been used for policy formulation for example, the impact project (http://www.policy-impact.eu/home, impact: integrated methods for policy making using argument modeling and computer assisted text analysis) aims to make progress in the area of state-of the art of computational models of argumentation about policy issues, contribute to computational linguistics by developing methods for mining arguments in natural language texts, find ways and means to increase the quantity and quality of public participation in consultation processes, and invent user friendly tools (such as graphic interfaces to increase public participation). These tools can also be used for policy formulation on Biodiversity. This will lead to involvement of more people, and key stakeholders to integrate biodiversity considerations into their work. The IMPACT argumentation toolbox aimed to consist of, firstly, an argument reconstruction tool: the manual reconstruction of arguments from natural language texts was done which was supported by a library of argumentation as a constituent of argument reconstruction tool. This was done in order to enable future web logs to mark up the structure of arguments in articles in a way which allows arguments to be automatically aggregated, analyzed and visualized. The legal knowledge extension format formed a part of the basis of this tool. A policy modeling and analysis tool based on the computational models of argumentation about alternative courses of action depending on the goals and values of multiple stakeholders was also included. Prior research on knowledge representation languages for concepts (ontologies), defeasible generalizations (rules) and precedent cases including the legal knowledge interchange format (lkif) was utilized for this. The lkif was developed in the Estrella project (http://www.estrellaproject.org, Estrella:The European project for Standardized Transparent Representations in order to Extend Legal Accessibility IST-2004-027655) aimed to develop and validate an “open, standards-based platform allowing public administrations to both develop and deploy comprehensive legal knowledge management solutions”. Legal document and data management, in addition to knowledge based systems is supported by Estrella, to provide a holistic solution for improving the efficiency and quality of public administration which requires the application of complex legislation and other legal sources. Both the legal and legislative data and its analysis including possible implications on past, present and future scenarios will have to be incorporated to arrive at informed and efficient solutions. The public administration and other users are provided with a variety of competing development environments, inference engines and other tools to choose from. The main technical objectives of the Estrella project are to “develop a Legal Knowledge Interchange Format (LKIF), building upon emerging XML-based standards of the Semantic Web, including RDF and OWL, and Application Programmer Interfaces (APIs) for interacting with legal knowledge-based systems”. The policy modeling and analysis tool is proposed to include a graphical user interface for a dialogue with an inference engine to simulate and analyze the consequences of a proposed policy. The tool has been proposed to be rich in graphical interfaces to enable clear visualization of its reasoning. The comparative analysis of different policy proposals will also be facilitated by this tool. Monendra Grover et al./ Indian Journal of Computer Science and Engineering (IJCSE) ISSN : 0976-5166 Vol. 2 No. 1 55 A structured consultation tool, based on prior research on the PARMENIDES system was a part of the toolbox. The PARMENIDES system was developed by University of Liverpool (http://cgi.csc.liv.ac.uk/~parmenides/index.php). The Parmenides system is a system for deliberative democracy and allows the government and public to interact in a two way fashion. It enables the government to present policy proposals to the public and lets the public submit their views on the policy. Parmenides exploits argumentation schemes and argumentation frameworks to graphically analyze the opinions submitted by the users. The structured consultation tool is an intelligent, advanced, polling and survey tool, based on the computational models of argumentation. The models of argumentation schemes together with the model of the issues and the arguments put forward previously in the ongoing consultation are used to generate questions in the surveys. The tool substantively increases the signal to noise ratio in online discussions, without restricting the solid arguments which can be made, by helping users to apply a model of rational argument. The arguments can be more easily tracked, mapped and visualized, since there is no need to manually reconstruct arguments from natural language texts. An argument analysis, tracking and visualization tool, based on computational models of argument and argument mapping methods is also a constituent of IMPACT argumentation toolbox. There are three main features of this tool. The analysis features of this tool enables citizens to identify the applied argumentation schemes, to list implicit premises helpful for asking questions. The tracking features of this tool enables users to register their interest in particular issues and request as well as receive notification whenever new arguments have been put forward which affect these interests. The visualization feature of this tool provides a variety of graphical and interactive views onto argument graphs. This will enable citizens to appreciate and analyze the complexity of the policy issues in their entirety and contribute to the policy formulation. Besides the policy formulation tools several other issues in sustainability have been addressed by using computational models as detailed below. Computational Sustainability Computational Sustainability is a highly interdisciplinary field, with the vision that information and computing science have a potential to play an indispensable role in increasing the efficiency and effectiveness of management and allocation of our natural resources. Some of the examples of studying computational biodiversity include more efficient use of natural resources, more realistic models of maintaining and increasing biodiversity and more effective large scale computational equilibrium models of renewable energy. Computational sustainability has a unique societal relevance and effective environmental component. Computational thinking and approach is essential to provide effective and efficient solutions which include balancing environmental, economic and societal needs. Computational sustainability takes a holistic approach and encompasses problems in diverse disciplines such as ecology, natural resources, atmospheric science, materials science, renewable energy and biological and environment engineering. Computational sustainability addresses the sustainability issues by translating them into decision and optimization problem. The field of computational sustainability not only draws from computer science and mathematics, it has pushed the boundaries of these disciplines itself. This is in view of the fact that sustainability issues are of unique scale complexity and impact. The sustainability problems require integration of a wide variety of techniques from various areas with in applied mathematics and computer science such as data mining, machine learning, optimization, constraint processing and dynamical systems. The field of complex systems is also relevant to computational sustainability. The systems studied in the realm of computational sustainability consist of highly interconnected components or agents, often with conflicting interests. The Institute of computational sustainability is one of the leading institutes in the world for computational sustainability (http://www.cis.cornell.edu/ics/). The multi-disciplinary, multi-institutional ICS research team is based at Cornell University of and includes leading computer and environmental scientists at Oregon State University, Bowdoin College, Howard University, and The Conservation Fund (TCF). The computational sustainability has had a direct impact on the sustainability research for example. The ICA has developed methods and models to design economically viable conservation corridors for grizly bears and other species in U.S. and understand the impact of climate change in terms of aerosol interac",,2011.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
b9230e5d90babb478fb6fc1eca69398d2553b75a,https://www.semanticscholar.org/paper/b9230e5d90babb478fb6fc1eca69398d2553b75a,An Approach towards Deployable Hybrid Product Recommendation Systems for E-Commerce,"As India is moving fast towards digital economy, E-commerce industry has been on rise. Many platforms provide their customers with a shopping experience better than actual physical stores. Several E-commerce websites use different methods to improve the customer engagement and revenue. One such technique is the use of personalized recommendation systems, which uses customer’s data like interests, purchase history, ratings to suggest new products, which they may like. Recom-mendation systems are used by E-commerce websites to suggest new products to their users. The products can be suggested based on the top merchants on the website, based on the interests of the user or based the past purchase pattern of the cus-tomer. Recommender systems are machine learning based systems that help users discover new products. Due to the recent pandemic situation of 2020 and 2021, many of the local retail stores have been trying to shift their business to online plat-forms such as dedicated websites or social media. The proposed methodology based on Machine Learning aims to enable local online retail business owners to enhance their customer engagement and revenue by providing users with personalized recommendations using past data using methods such as Collaborative Filtering, Popularity-based and Content-Based Filtering.","International Journal of Scientific Research in Computer Science, Engineering and Information Technology",2022.0,10.32628/cseit228332,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
2c598f5071937d27a517595d36b9def9bb3de629,https://www.semanticscholar.org/paper/2c598f5071937d27a517595d36b9def9bb3de629,Professional Visual C++ Activex Intranet Programming,"From the Publisher: 
Long Live the Information Superhighway! 
Wait a minute! I heard some loud heckling from the back row. 
""I thought this is a 'Microsoft ActiveX' technical programming book?!"" 
It is. 
The point is, there are two ways you can show a technology: either in dry, boring isolation, talking about the nuances without reference to the problems the technology is supposed to solve, or as we've attempted to do, applying the technology to a real-world situation. We've chosen to show you ActiveX programming by applying this technology to a large growth area, intranets. As you'll soon see, the ActiveX family of technologies is a very broad family covering the gamut of Internet-intranet, client-server and distributing computing solutions. It has to be this way, since ActiveX represents Microsoft's entire investment into the Internet-intranet (and object based distributed computing) race. Within this book, ActiveX will be used as a vehicle to explore many of the concepts and techniques involved in intranet construction. In many cases, the concepts and approaches explored are generally applicable to your practice whether it's Microsoft-, UNIX-, or even Netscape-centric. What's Covered in this Book Just before the technical reader, fluent with Visual C++, decides to return this book to the bookstore for a refund, I must say that this is by no means a 'lame theoretical treatise'. We turn on the power-throttle, and shine our high beams on the core ActiveX technologies once we've reached Chapter 2. In fact, a discussion of what intranets are about has been relegated to Appendix A, simply because you don't actually need to know about them to gain an understanding of ActiveX. From there we dive into the depths of the Microsoft's Component Object Model (COM) which is fundamental to all of Microsoft's ActiveX technology. Covering the basics, we'll be taking a view that reduces this complex topic to simple programming practices that we're fully familiar with. From there, we examine the concept of COM aggregation and show how it further enhances code reuse and provides a powerful mechanism for COM. In Chapter 3, we take our understanding of COM and put it into practice by writing an ActiveX control from scratch, using just raw C++. Here, we'll become intimate with the complete anatomy of a simple ActiveX control. When we move on to more powerful libraries and code generation wizards, this basic understanding will enable us to adapt and troubleshoot more effectively. In this chapter, we encounter many essential COM interfaces through actual hands-on programming; we'll also get acquainted with some indispensable COM programming tools such as the MIDL compiler and the Object Viewer utility. To handle some more complex problems without coding forever, we'll take a look at programming libraries to simplify the COM object programming task (ActiveX controls, to be precise). We'll explore how to code powerful, yet super efficient and tiny COM objects using the ActiveX Template Library (ATL) 2.1, and we will spend some time explaining many of the new COM interfaces, and show how ATL makes everything simple. Also in this chapter, we'll be learning about the threading models supported by COM objects and the different types of COM servers that can be created. Using ATL to create ActiveX controls is the focus of Chapter 4. Chapters 2, 3 and 4 give us enough background into understanding what ActiveX provides for the intranet development environment. We'll understand how ActiveX controls can be fundamental building blocks (actually software components) in both client- and server-based programming. We'll make excursions into the ActiveX controls (OCX ) specifications in Chapter 5, covering the differences between the OLE Control specification (for Visual controls) and the new OC96 specification (for ActiveX controls). We'll actually be designing an Events Calendar control. This control will display currently active events (for the month) from different company departments for easy and straightforward access. The distributed 'live update' nature of this control eliminates the need for consolidating events information in a centralized database. In Chapters 6 and 7 we put our design into code. Using Visual C++ 5.0 and MFC 4.21, we'll be building the actual Events Calendar intranet control. The control class and custom wizard provided by MFC greatly simplify much of the development. We'll also be building two additional 'back-end' ActiveX controls using ATL 2.1 to do data processing for the Visual Calendar control. Finally, we will test the controls and show that the Calendar control is a bona fide ActiveX control that can be hosted within containers such as Visual Basic 5.0, Internet Explorer 3.0, and FrontPage 97. In Chapter 8, we shift into the highest gear and attempt to put the Calendar control through its paces by using DCOM to run the front-end and back-end ActiveX controls across three separate machines. Along the way, we'll learn a lot about DCOM and how it enables true distributed computing. We'll also be examining the difficult problem of ActiveX control code installation and revision control, and see how the Internet Explorer 3.0 provides us with a ready-made solution to the problem. As part of the installation solution, we'll develop a small program to download controls from remote sites. After the intensive programming in Chapter 8, we shift our focus to a hot intranet issue in Chapter 9: security. We'll examine the topic by drawing a parallel to the Windows NT security model which is fundamental to all other security mechanisms built upon it. We'll learn about the various security, authentication and encryption APIs and COM interfaces available to intranet application developers. Special attention will be paid to DCOM related security issues and how arbitrary distributed objects may be prevented or allowed to execute on certain machines. We conclude our coverage in Chapter 10 by casting aside our overly enthusiastic attitude and examine some real and hard-to-tackle ActiveX and intranet deployment issues, suggesting potential solutions wherever they are available. We'll cover a lot of ground in the following pages. I hope your journey into the exciting world of ActiveX will be as pleasant, productive, and profitable for you as it has been for us. What You Need to Use This Book To use this book you need Visual C++ 5.0, and the latest version of Microsoft's best-selling C++ compiler. This version is 32-bit only, so you'll need to install it on Windows 95, Windows NT 3.51 or NT 4, which means a 486 CPU or better and a minimum 16Mb of memory. For Visual C++, you'll need quite a lot of hard disk space - a typical installation is 170 Mbytes. You can do a minimal installation which takes up around 40 Mbytes, but this will mean longer compile times as the CD-Rom will be utilized more often. Some of the later chapters require you to have access to a network and a second computer to test the code correctly. You'll also need to have DCOM for Windows 95 (information on obtaining this is given in Chapter 8) or Windows NT 4.0. |AUTHORBIO: Bitten by the microcomputer bug since 1978, Sing has grown up with the microprocessor age. His first personal computer was a $99 do-it-yourself Netronics COSMIC ELF computer with 256 bytes of memory, mail ordered from the back pages of Popular Electronics magazine. Currently, Sing is an active author, consultant, and entrepreneur. He has written for popular technical journals and is the creator of the ""Internet Global Phone"", one of the very first Internet phones available. His wide-ranging consulting expertise spans Internet and Intranet systems design, distributed architectures, digital convergence, embedded systems, real-time technologies, and cross platform software design. Recently, he has completed an assignment with Nortel Multimedia Labs working in Computer Telephony Integration, and Advanced Callcenter Management products. Sing is a founder of microWonders, an emerging company specializing in products to fulfill the ubiquitous ""computing anywhere"" vision. Other titles by this author: ATL Programmer's Resource Kit Professional COM Applications with ATL Professional IE4 Programming Visual C++ 4 Master Class|AUTHORBIO: Panos Economopoulos has been the architect, designer and leader for implementations of a number of complex and successful distributed computer systems. Currently, he is Manager of Research and Development at Telesis North. Here, he designed the OnAir series of mobile client-server products that provide efficient and robust remote access to BackOffice servers over a variety of satellite and other wireless networks. He has extensive experience as a consultant to the Industry and has developed, and taught, a variety of courses both at University undergrad level and for mature developers. He's also carried out advanced research at the University of Toronto - results of which have been published in several research journals. Other titles by this author ATL Programmer's Resource Kit, Professional COM Applications with ATL|",,1997.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
1f2f6216100b3abf664a1f1385ad957e3f46efd7,https://www.semanticscholar.org/paper/1f2f6216100b3abf664a1f1385ad957e3f46efd7,Top 10 Read Article in Computer Science & Information Technology: June 2021,"Clouds provide a powerful computing platform that enables individuals and organizations to perform variety levels of tasks such as: use of online storage space, adoption of business applications, development of customized computer software, and creation of a “realistic” network environment. In previous years, the number of people using cloud services has dramatically increased and lots of data has been stored in cloud computing environments. In the meantime, data breaches to cloud services are also increasing every year due to hackers who are always trying to exploit the security vulnerabilities of the architecture of cloud. In this paper, three cloud service models were compared; cloud security risks and threats were investigated based on the nature of the cloud service models. Real world cloud attacks were included to demonstrate the techniques that hackers used against cloud computing systems. In addition,countermeasures to cloud security breaches are presented. ABSTRACT Big Data triggered furthered an influx of research and prospective on concepts and processes pertaining previously to the Data Warehouse field. Some conclude that Data Warehouse as such will disappear; others present Big Data as the natural Data Warehouse evolution (perhaps without identifying a clear division between the two); and finally, some others pose a future of convergence, partially exploring the possible integration of both. In this paper, we revise the underlying technological features of Big Data and Data Warehouse, highlighting their differences and areas of convergence. Even when some differences exist, both technologies could (and should) be integrated because they both aim at the same purpose: data exploration and decision making support. We explore some convergence strategies, based on the common elements in both technologies. We present a revision of the state-of-the-art in integration proposals from the point of view of the purpose, methodology, architecture and underlying technology, highlighting the common elements that support both technologies that may serve as a starting point for full integration and we propose a proposal of integration between the two technologies. ABSTRACT By applying RapidMiner workflows has been processed a dataset originated from different data files, and containing information about the sales over three years of a large chain of retail stores. Subsequently, has been constructed a Deep Learning model performing a predictive algorithm suitable for sales forecasting. This model is based on artificial neural network –ANN- algorithm able to learn the model starting from sales historical data and by pre-processing the data. The best built model uses a multilayer neural network together with an “optimized operator” able to find automatically the best parameter setting of the implemented algorithm. In order to prove the best performing predictive model, other machine learning algorithms have been tested. The performance comparison has been performed between Support Vector Machine –SVM-, k-Nearest Neighbor k-NN-,Gradient Boosted Trees, Decision Trees, and Deep Learning algorithms. The comparison of the degree of correlation between real and predicted values, the average absolute error and the relative average error proved that ANN exhibited the best performance. The Gradient Boosted Trees approach represents an alternative approach having the second best performance. The case of study has been developed within the framework of an industry project oriented on the integration of high performance data mining models able to predict sales using–ERP- and customer relationship management –CRM- tools. ABSTRACT Wireless network implementation is a viable option for building network infrastructure in rural communities. Rural people lack network infrastructures for information services and socio-economic development. The aim of this study was to develop a wireless network infrastructure architecture for network services to rural dwellers. A user-centered approach was applied in the study and a wireless network infrastructure was designed and deployed to cover five rural locations. Data was collected and analyzed to assess the performance of the network facilities. The results shows that the system had been performing adequately without any downtime with an average of 200 users per month and the quality of service has remained high. The transmit/receive rate of 300Mbps was thrice as fast as the normal Ethernet transmit/receive specification with an average throughput of 1 Mbps. The multiple output/multiple input(MIMO) point-to-multipoint network design increased the network throughput and the quality of serviceexperienced by the users ABSTRACT Although initially designed for co-located teams, agile methodologies promise mitigation to the challenges present in distributed software development with their demand for frequent communication. We examinethe application of agile practices in software engineering teams with low geographical distribution in Austria and Germany. To gather insights on challenges and benefits faced by distributed teams we conductinterviews with eleven representatives and analyse the interview transcripts using the inductive category formation method. As a result, we identify four major challenges, such as technical obstructions or theimpediments different language abilities have on communication, and four benefits, regardingcollaboration and information radiation, that agile methods yield in distributed teams. Based on ouranalysis of challenges and benefits, we deduct seven recommendations to improve collaboration, overcomedistance and avoid pitfalls. Key recommendations for teams with low geographical distance include thatteams should get together at certain points to build relationships and trust and share information face-to- face ABSTRACT The world is moving forward at a fast pace, and the credit goes to ever growing technology. One such concept is IOT (Internet of things) with which automation is no longer a virtual reality. IOT connects various non-living objects through the internet and enables them to share information with their community network to automate processes for humans and makes their lives easier. The paper presents the future challenges of IoT , such as the technical (connectivity , compatibility and longevity , standards , intelligent analysis and actions , security), business ( investment , modest revenue model etc. ), societal (changing demands , new devices, expense, customer confidence etc. ) and legal challenges ( laws, regulations, procedures, policies etc. ). A section also discusses the various myths that might hamper the progress of IOT, security of data being the most critical factor of all. An optimistic approach to people in adopting the unfolding changes brought by IOT will also help in its growth ABSTRACT Mobile payment allows consumers to make more flexible payments through convenient mobile devices. While mobile payment is easy and time save, the operation and security of mobile payment must ensure that the payment is fast, convenient, reliable and safety in order to increase the users’ satisfaction. Therefore, this study based on technology acceptance model to explore the impact of external variables through perceived usefulness and perceived ease of use on users’ satisfaction. The data analysis methods used in this study are descriptive statistical analysis, reliability and validity analysis, Pearson correlation analysis and regression analysis to verify the hypotheses. The results show that all hypotheses are supported. However, mobile payment is still subject to many restrictions on development and there are limited related researches. The results of this study provided insight into the factors that affect the users’ satisfaction for mobile payment. Related services development of mobile payment and future research suggestions are also offered. ABSTRACT Big Data is used in decision making process to gain useful insights hidden in the data for business and engineering. At the same time it presents challenges in processing, cloud computing has helped in advancement of big data by providing computational, networking and storage capacity. This paper presents the review, opportunities and challenges of transforming big data using cloud computing resources. ABSTRACT In this paper, we propose a new traffic flow model of the Long Term Evaluation (LTE) network for the Evolved Universal Terrestrial Radio Access Network (E-UTRAN). Here only one Evolve Node B (eNB)nearest to the Mobility Management Entity (MME) and Serving Gateway (S-GW) will use the S1 link tobridge the E-UTRAN and Evolved Packet Core (EPC). All the eNBs of a tracking area will be connected toeach other by the X2 link. Determination of capacity of a links of such a network is a challenging job sinceeach node offers its own traffic and at the same time conveys traffic of other nodes. In this paper, we applymaximum flow algorithm including superposition theorem to solve the traffic flow of radio network. Usingthe total flow per subcarrier, a new traffic model is also developed in the paper. The relation among the traffic parameters: ‘blocking probability’, ‘offered traffic’, ‘instantaneous capacity’, ‘average holdingtime’, and ‘number of users’ are shown graphically under both QPSK and 16 -QAM. The concept of thenetwork will be helpful to improve the SINR of the received signal ofeNBslocated long distance relative to MME/S-GW. ABSTRACT The huge amount of healthcare data, coupled with the need for data analysis tools has made data mining interesting research areas. Data mining tools and techniques help to discover and understand hidden patterns in a dataset which may not be possible by mainly visualization of the data. Selecting appropriate clustering method and optimal number of clusters in healthcare data can be confusing and difficult most times. Presently, a large number of clustering algorithm",,2021.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
e23e0c6529f205cdeb65f226a59fd996b8a25e8b,https://www.semanticscholar.org/paper/e23e0c6529f205cdeb65f226a59fd996b8a25e8b,A proposal to systematize introducing DevOps into the software development process,"The software development industry has been evolving with new development standards and service delivery models. Agile methodologies have reached their completion with DevOps, thereby increasing the quality of the software and creating greater speed in delivery. However, a gap regarding the formalization of its adoption and implementation doubts became relevant. My hypothesis is that, by systematizing the introduction of DevOps into the software development process and defining the function of the members of the DevOps team members, may well make it quicker to implement this process, thus reducing conflicts between the teams. As part of the investigation of this hypothesis, the result of the research will be applied in practical development environments i.e. in a Technology Agency of the State of the Brazilian Government and also at the Brazilian Company Neurotech in order to evaluate its effectiveness from metrics appropriate for DevOps environments. Keywords—DevOps, software development, maturity model I. PROBLEM DevOps is an emerging practice that has been adopted in the software development cycle. It focuses on the convergence of standards between the Development teams and the Operations teams and it seeks to improve cooperation between both teams, hence the origin of the term [1]. However, there is no consensus on the definition of what DevOps is. Wiedemann et al. [2] emphasize that one of the biggest challenges in the industry is the lack of a formal concept for DevOps. This conceptual flaw in what DevOps is, directly impacts the understanding of the objects and actions needed to overcome this flaw [3]. The adoption of Agile and DevOps Methodologies continues to grow, driven by the “need for speed”, agility and flexibility, evidenced in the World Quality Report of 2019 [4], in which 99% of the interviewees said they were using DevOps in at least some of their business. Implementing DevOps has become more difficult due to the lack of formalizing the concept and adoption processes. Although there is an abundance of information, practices and tools related to DevOps, it is still unclear how anyone could take advantage of this rich, yet diffuse information in an organized and structured way to properly adopt DevOps [6]. The study by Zulfahmi [7] provides evidence of this lack of standardization. The author also states that other major challenges are the lack of process and guidelines for implementing the practice of DevOps in Continuous Delivery. In the Systematic Literature Review conducted by Gasparaite; Naudziunaite and Ragaisis [8], 24 DevOps models were identified, but only 04 were considered applicable for practical use: the Focus Area, Bucena-Kirikova, Mohamed, and Radstaak models. However, in the Radstaak model, not all the steps necessary for its practical use have been described; the Mohamed model did not present the evaluation process, only how to apply the model; and the Focus Area and Bucena-Kirikova Models can be applied in practice, since their authors document the evaluation methods adopted in academic publications. Another aspect to be highlighted concerns what must be done or would be appropriate prior to an organization adopting DevOps. Leite et al. [9] ponder this aspect by considering a difficult question that the literature has not yet fully answered. They further point out that it is incomplete and even contradictory in relation to the subject. At the same time, the variety of DevOps tools seems to challenge the idea of there being a single person with the role of administering the entire process. Even mature teams, who have both knowledge of development and infrastructure operations, may find it difficult to be familiar with all these tools, thus making it necessary to define the roles of those involved in this process. The need to systematize the introduction of DevOps in the software development process is therefore necessary. Thus, the need for more research and empirical work is essential to put into practice and validate a proposal to systematize the introduction of DevOps. In view of the above, the following problems are in evidence: a) the lack of a conceptual definition and, consequently, the need for a proposal to systematize the introduction of DevOps in the software development process, that presents systematic improvements, evidenced by its adoption and the results of its effectiveness; and b) the lack of definition of the roles of those involved in this cycle. The present thesis should conduct a study that sets out to identify the sets of good practices already used in software development processes that use DevOps, and based on these to conduct an analysis, validation and standardization of their use, with a view to systematizing the introduction and execution of DevOps in this process. II. RESEARCH HYPOTHESIS A software production line that uses DevOps, has a welldefined automation cycle, starting with the developers' source code commits for the code version-control system. When the CI server identifies the completion of the commits, it performs the necessary tests and, if necessary, provides feedback to the developers [10]. In this thesis we will propose systematic practices for introducing DevOps and improving the efficiency of the software production process that uses DevOps. As a way of conducting the research and delimiting the scope of the study, the following hypothesis is proposed: Having adopted a set of systematized practices for introducing DevOps, and for formalizing and delimiting the role of the members of the DevOps team in the Software Production Process, Software Factories become faster in their implementation, thereby reducing conflicts between teams and providing quality deliverables. To validate the hypothesis, the following questions were constructed: RQ1: What are the gaps in the software development process that use DevOps practices? In order to propose solutions to the existing problems when introducing DevOps, one must first identify the gaps in the process. The purpose of RQ1 is to locate these gaps and the actions that must be taken to resolve such issues. RQ2: What are the practices that support the systematic introduction of DevOps in the software development process? The objective of RQ2 is to identify all the good practices used by the development and academic industry that support introducing it into the software development process. III. EXPECTED CONTRIBUTION The objective of my doctoral thesis is to improve the software development process using DevOps and to propose new ways of introducing it. The expected contributions of this project are summarized below: 1) to systematize the introduction of DevOps in the software development process; and 2) to improve the adoption of DevOps in the software development process. IV. METHODS AND PRELIMINARY FINDINGS As a preliminary study, a Systematic Mapping (SM) of Literature [11] was undertaken, in May 2020, with a view to analyzing the topic, and hence seeking to identify existing gaps in the area of DevOps and difficulties in the implementation process. The SM [11] will serve to underpin the initial conduct of the research, and thus provide a diagnosis of the software production process that uses DevOps, also providing initial data that can be used to build a set of systematic practices for adopting DevOps. The SM [11] showed how DevOps is highlighting the following issues: pipeline security, effective adoption of a cloud environment, adoption of microservices, infrastructure as code, use of container solutions, and tools to automate the development pipeline. Regarding best practices, DevOps was found to have: Infrastructure as Code, Continuous Integration, Continuous Delivery, Continuous Deployment. One of the questions raised in the MS [11] was about the existence of Maturity Models, in which 11 were found. However, only two were validated by companies, and one of them is applied only in IBM solutions. The roles of the actors involved in the process were found in 02 studies, with different names, but performing the same function. In a second phase, a Multi-Vocal Literature Review will be carried out to look for relevant information on emerging industry topics, which were not achieved by using SM. This type of Review has both academic publications and gray literature as input, its main objective being to close the gap between academic research and professional practice [12]. The methodology adopted for practical research will be Design Science Research (DSR), which is a method that establishes and operationalizes research when the desired objective is an artifact or a recommendation [13]. Given the above, there will be a need for an iterative cycle and the production of an artifact, which may be a set of systematic practices for adopting DevOps, which will be validated and improved throughout the research. The research will be applied in a Technology Agency of the State of the Brazilian Government, of which the author is Technical Director and has an Information Systems Coordination Unit with several applications under development using DevOps practices, and in the Brazilian company Neurotech, which develops advanced solutions of Artificial Intelligence, Machine Learning and Big Data, and has a portfolio of more than 100 customers. V. EVALUATION PROCEDURES I am starting my second year of doctorate and by its end, RQ1 and RQ2 will have been answered. The next step will be to propose a handbook that will systematize the introduction of DevOps, during which it will be applied in practice as previously mentioned in a State Technology Agency and Neurotech. The instrument for evaluating the results of the research proposed in the DevOps development process will be conducted by means of specific DevOps metrics, collected throughout the development process. Some studies have presented a set of metrics that can be used to assess the applicability of DevOps in the Software Production Process: Delivery ti",ICSE,2021.0,10.1109/ICSE-Companion52605.2021.00124,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
68aff2731ad4ee1e83309d3b2f7ff240d7987208,https://www.semanticscholar.org/paper/68aff2731ad4ee1e83309d3b2f7ff240d7987208,Design Development And Fabrication Of Sugarcane Bud,"Design, Development and Fabrication of a Precision Autocollimating Solar Sensor /PASS/DESIGN, DEVELOPMENT, AND FABRICATION OF PROTOTYPE LOW-ENRICHED SUPERHEATER FUEL ELEMENTS (LESH).Design, Development and Fabrication of a Deployable/retractable Truss Beam Model for Large Space Structures ApplicationDesign, Development and Testing of Calspan/Chrysler Research Safety Vehicle Phase II. Volume I. Final Technical ReportDesign, Development, and Fabrication of a Sealed, Brushless Dc Motor Final ReportCompendium of Industrial Research, Design, and Development Facilities Available in Uttar PradeshDesign, Development and Fabrication of an Advanced High-Precision Robotic System for MicrosurgeryDesign, development, fabrication and testing of an operational prototype surge protectiveDesign, Development, and Fabrication of the FMU-26/B and FMU-26A/B Bomb FuzesDesign, Test, and Microfabrication of MEMS and MOEMSDesign and Development of RFID and RFID-Enabled Sensors on Flexible Low Cost SubstratesDesign for ManufacturingDesign and Fabrication of an Internally Insulated Filament Wound Liquid Hydrogen Propellant TankCalifornia. Court of Appeal (2nd Appellate District). Records and BriefsScientific and Technical Aerospace ReportsFurniture DesignDesign, Development and Fabrication of a Solar Experiment Alignment Sensor (SEAS)Axiomatic Design and Fabrication of Composite StructuresAdvanced Technology for Design and Fabrication of Composite Materials and StructuresRobotic Fabrication in Architecture, Art and Design 2014Proceedings of the U.S./U.S.S.R. Seminar on Problems of Design, Development, Fabrication and Test of Breeder Reactor ComponentsNuclear Science AbstractsRobotic Fabrication in Architecture, Art and Design 2018Research, Development, and Mechanization in the United States Post Office DepartmentDistributed Intelligence In DesignAeronautical Engineering ReviewDesign Methodologies for Space Transportation SystemsDesign, Development, and Fabrication of a Sealed, Brushless DC MotorDesign, Fabrication, Properties and Applications of Smart and Advanced MaterialsU.S. Government Research ReportsProcesses and Design for ManufacturingAEC Authorizing Legislation, Fiscal Year 1969Smart Material Systems and MEMSMaterials, Design and Manufacturing for Lightweight VehiclesDesign, Development and Fabrication of a New Generation Semiconductor X-ray DetectorNuclear Regulatory Legislation, 109th Congress, 2nd SessionDigital Design and FabricationDesign, Development, and Fabrication of a Electronic Analog Microminiaturized Electronic Analog Signal to Discrete Time Interval ConverterDesign, Development, Fabrication, and Testing of a Synchronous Condenser for a High-power Three-phase Traction DriveProduct Design for Manufacture and Assembly In response to tremendous growth and new technologies in the semiconductor industry, this volume is organized into five, information-rich sections. Digital Design and Fabrication surveys the latest advances in computer architecture and design as well as the technologies used to manufacture and test them. Featuring contributions from leading experts, the book also includes a new section on memory and storage in addition to a new chapter on nonvolatile memory technologies. Developing advanced concepts, this sharply focused book— Describes new technologies that have become driving factors for the electronic industry Includes new information on semiconductor memory circuits, whose development best illustrates the phenomenal progress encountered by the fabrication and technology sector Contains a section dedicated to issues related to system power consumption Describes reliability and testability of computer systems Pinpoints trends and state-of-theart advances in fabrication and CMOS technologies Describes performance evaluation measures, which are the bottom line from the user’s point of view Discusses design techniques used to create modern computer systems, including high-speed computer arithmetic and high-frequency design, timing and clocking, and PLL and DLL designDesign for Manufacturing assists anyone not familiar with various manufacturing processes in better visualizing and understanding the relationship between part design and the ease or difficulty of producing the part. Decisions made during the early conceptual stages of design have a great effect on subsequent stages. In fact, quite often more than 70% of the manufacturing cost of a product is determined at this conceptual stage, yet manufacturing is not involved. Through this book, designers will gain insight that will allow them to assess the impact of their proposed design on manufacturing difficulty. The vast majority of components found in commercial batch-manufactured products, such as appliances, computers and office automation equipment are either injection molded, stamped, die cast, or (occasionally) forged. This book emphasizes these particular, most commonly implemented processes. In addition to chapters on these processes, the book touches upon material process selection, general guidelines for determining whether several components should be combined into a single component or not, communications, the physical and mechanical properties of materials, tolerances, and inspection and quality control. In developing the DFM methods presented in this book, he has worked with over 30 firms specializing in injection molding, die-casting, forging and stamping. Implements a philosophy which allows for easier and more economic production of designs Educates designers about manufacturing Emphasizes the four major manufacturing processesPresenting unified coverage of the design and modeling of smart microand macrosystems, this book addresses fabrication issues and outlines the challenges faced by engineers working with smart sensors in a variety of applications. Part I deals with the fundamental concepts of a typical smart system and its constituent components. Preliminary fabrication and characterization concepts are introduced before design principles are discussed in detail. Part III presents a comprehensive account of the modeling of smart systems, smart sensors and actuators. Part IV builds upon the fundamental concepts to analyze fabrication techniques for silicon-based MEMS in more detail. Practicing engineers will benefit from the detailed assessment of applications in communications technology, aerospace, biomedical and mechanical engineering. The book provides an essential reference or textbook for graduates following a course in smart sensors, actuators and systems.This book introduces various advanced, smart materials and the strategies for the design and preparation for novel uses from macro to micro or from biological, inorganic, organic to composite materials. Selecting the best material is a challenging task, requiring tradeoffs between material properties and designing functional smart materials. The development of smart, advanced materials and their potential applications is a burgeoning area of research. Exciting breakthroughs are anticipated in the future from the concepts and results reported in this book.The work described in this document was performed in compliance with the scope of work as specified in Contract AF 08(635)-2850 tendered Honeywell Ordnance Division on 13 June 1962. All phases of a complete development program were carried out in order to achieve the goal of developing a safe, highly reliable fuze compatible with available subsonic and supersonic delivery systems. The final result of this development program was a multi-purpose fuze operable in three different modes: impact short-delay, impact medium-delay, and airburst. Fuzes were subjected to every environmental, functional, and safety test for development of fuzes required by the Air Force and by the contract. A program for the development of fuzes incorporating a retard-mode capability into the fuze was conducted, but the mode could not be included without extensive fuze redesign. Several recommendations were made by the contractor to expend additional efforts under the production program to effect the following: loading simplification, battery firing device simplification or integration, safing and arming mechanism simplification, and general safety improvements. (Author).The book presents research from Rob|Arch 2018, the fourth international conference on robotic fabrication in architecture, art, and design. In capturing the myriad of scientific advances in robotics fabrication that are currently underway – such as collaborative design tools, computerised materials, adaptive sensing and actuation, advanced construction, on-site and cooperative robotics, machine-learning, human-machine interaction, large-scale fabrication and networked workflows, to name but a few – this compendium reveals how robotic fabrication is becoming a driver of scientific innovation, cross-disciplinary fertilization and creative capacity of an unprecedented kind.pt.1: Considers S. 2880 and companion H.R. 14905, to authorize appropriations for AEC. Focuses on general budget and reactor development program; pt.2: Continuation of hearings on AEC FY69 authorization. Appendix includes reports. a. ""National Accelerator Laboratory, Design Report 1968, Universities Research Associates, "" prepared by AEC 1968 (p. 1223-1456). b. ""Report of Ad Hoc Panel on Low-Beta Toroidal Plasma Research, "" Sept. 1967 (p. 1459-1583). c. ""Bronco Oil Shale Study, "" prepared by AEC, Interior Dept, CER Geonuclear Corp., and Lawrence Radiation Laboratory, Oct. 13, 1967 (p. 1743-1813).Compiles statutues and materials relating to nuclear regulatory legislation through the 109th Congress, 1st Session.Hailed as a groundbreaking and important textbook upon its initial publication, the latest iteration of Product Design for Manufacture and Assembly does not rest on those laurels. In addition to the expected updating of data in all chapters, this third edition has been revised to provide a top-notch textbook for ",,2021.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
2ff29ea3641c45e90157a04bd47ed6d497ccac14,https://www.semanticscholar.org/paper/2ff29ea3641c45e90157a04bd47ed6d497ccac14,Implementation of Data Mining from Social Media for Improved Public Health Care,"To improve public health care outcomes with reduced cost, this research proposed a framework which focuses on the positive and negative symptoms of illnesses and the side effects of treatments. However, previous studies have been limited as they neither identified influential users nor discussed how to model forms of relationships that affect network dynamics and determine the accurate ranking of certain end user’s feedbacks. In this research, a two-step analysis framework is proposed as the system. In the first level, the system utilized exploratory analysis and clustered users and their useful feedbacks through self-organizing maps (SOM). In the second level, the system developed three lists of negative and positive feedbacks and treatment symptoms caused by implanting the SOM that considered accurate ranking by calculating the frequency of each term of interests. The feasibility of the proposed solution is confirmed as performance evaluations of the system in terms of computational costs. The results showed that these solutions are reasonable computational costs relative to memory and processor usage. Keywords—Data mining; social media; medical data; end user feedbacks; positive terms; negative terms; symptoms I. RESEARCH MOTIVATIONS AND BACKGROUND Data mining from social media recently gained the attention of many important businesses and industries. Data mining is empowered by the recent advances in big data analysis as well as the network modeling of social media forums and websites, which are integrated to achieve knowledge discovery solutions and to extract useful information from various fields [1]. The health care industry is one of the most important fields that can be significantly enhanced by modern data mining techniques that allow the discovery of certain trend patterns as a product of the social media feedbacks generated dynamically from the experiences and opinions of end users [2]. These techniques can be applied to drug feedbacks in social media which can help manufacturers continuously enhance their products at reducedcosts. Successful data mining from social media can result in numerous benefits for business owners and manufacturers [3]; however, a number of challenges should be addressed first to reach the acceptable level and wide deployment of this new technology [4]. In addition, classical data mining techniques and algorithms should be empowered with an intelligent pattern recognition tool to predict and visualize the common trends of the data in general. Moreover, it should weigh certain descriptions or feedbacks based on their frequency and eliminate some neutral words as a filtration technique in the preprocessing stage. Different online forums contain feedbacks that should be network-modelled for a more convenient analysis. All these challenges are the main inspirations that motivated this research. The main contributions of this work can be considered as developing a feasible text data-mining solution that can partition different users with certain ID by modeling their existence in different web forums. Furthermore, the accurate clustering of negative and positive feedbacks and the visualization of the overall positive or negative feedback trends in a reasonable computational cost solution is another benefit from this research. The basic concepts are explained in this section as a background of the research field and the proposed solutions. First, the processes of data collection and mining are considered challenging tasks given a large number of networks studied, and this requires a complex representation of the social network structure. The complexity of such structure is derived from network density and levels of the parents and nodes clustering social media contents. Network clustering involves complex, big, and parallel data processing to cover the analysis of each networks’ nodes representing certain user communities. A part of the network usually as small as the sample size is used for data collection. Future Technologies Conference (FTC) 2017 29-30 November 2017| Vancouver, Canada 235 | P a g e Traditionally, structuring and modelling social networks can extract useful information as topic trends and the trends of opinions and the linguistics properties play an effective role in this study. At the beginning, certain word filtration techniques are used to remove unwanted words, such as stop and stemming words [5]. The concept of the self-organizing map (SOM) is utilized in particular research fields; in the simplest terms, SOM is a predefined wordlist used to correlate with the large data from the social networks under the tests to extract positive and negative words [6]. Importantly, certain algorithms are used to determine the frequency of certain positive or negative words, so that weighing the word (in another meaning its effectiveness among other words) can be determined. Finally, simple statistical tools are used to identify a positive or negative trend and the most common words describing the symptoms of drug use. Similar solutions in the literature on disease surveillance for the case of Influenza-related community who share fluposting online is utilized through the technique text and structural data mining of web and social media (WSM) [7] [8]. In the critical analysis of the SOM and WSM techniques in the literature [9], [10], it can be concluded that SOM techniques have more advantages in terms of the capability of investigating the positive and negative feedbacks of treatments. This advantage can be achieved by mapping large dimensional information onto a low dimensional space. II. METHODOLOGIES OF THE PROPOSED SYSTEM A two-step framework is proposed as an investigatory analysis to evaluate the correlations between user posts and positive/negative words under a drug name. The correlation is obtained by using SOM. Using a network-based approach, the system enabled users and their posts to find the possible partition using complete linkages. The two processes involved with inter-social dynamic maps for reviewing SOM results are described below:  The correlation between user and judgment.  The partition between users and their posts. Regarded as an unsupervised technique, Self-OrganizingMap (SOM) is used to explore the survey dataset based on the artificial neural network. The representation of the SOM data is in multidimensional data such as two or three dimension. Based on the data compression of the vector quantization technique, the SOM process is used to reduce the dimensionality of sectors. The information is stored as a topological relationship within the training sets in a network. Therefore, large data sets are visualized with high dimensionality using SOM. The competitive learning approach of SOM has one neuron unconnected to the input and output layers for each training phase. Although the connection between the neurons is absent, communication exists between each phase through a neighborhood function. The proposed SOM approach is used to summarize and visualize the profiles of individual patients. This visualization process helps determine domain experts. The two perspectives involved in the process of accurately obtaining results are computational and scientific perspectives [11]. By using SOM in the computational perspective, feasibility is examined by extracting useful information from questionnaires. In the scientific perspective, the different types of patient diseases such as type-I diabetes are collected to understand the responses in the diabetes survey and suggest about domain experts to clinicians. By contrast, clinicians are required to take a survey about their patients. The mean, skewness, variance, and frequency are the traditional descriptive statistical methods, but it provides simplified conclusions. Thus, data are analyzed based on statistical machine learning tools with black box by clinicians. The SOM algorithm is used for mining correlations and clustering similar responses within the surveys. If the dimension is higher for clustered responses, then the data is visualized in a two-dimensional grid to reduce data complexity. Complexity is reduced by revealing more meaningful relationships and by understanding the dependencies among the survey responses. Previously, SOM is used to visually explore data areas such as health, lifestyle, nutrition, financial, gene expression, marine safety, and linguistics. Recently, SOM is utilized to explore questionnaire-based loneliness survey data. The present research also focuses on improving data interpretation by revealing possible associations between the tendency of item nonresponse and the background variables of participants. The flaw conclusion is obtained by item nonresponse which is related to the background variables of respondents, such as age and gender nonresponses. Considering the undetected non-causative relationships between independent and dependent variables, the nonresponse factors affected patient satisfaction. In the present study, item nonresponse does not refer to participants who fail to return the survey, but to the ones who choose not to respond to all questions. In the proposed approach, the issues involved in this research are included for data analysis. Large surveys have demonstrated that although respondents and non-respondents in patient satisfaction surveys may differ according to several demographic and clinical characteristics, the differences in satisfaction between them tend to be relatively small and non-respondents do not constitute a homogenous group. Many highly sophisticated statistical methods are used as a standard technique to handle the problem of missing responses. In the existing link method, the idea that missing data are not just a statistical nuisance but also contain valuable information as such is tested simply by including the number of item nonresponses per respondent as an explanatory variable in the models. The expected predictors of pati",,2018.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
eb417bd65e7c3d95e209e5310433cbfa704869e1,https://www.semanticscholar.org/paper/eb417bd65e7c3d95e209e5310433cbfa704869e1,AC 2012-4161: A WIRELESS SENSOR NODE POWERED BY SOLAR HAR- VESTER FOR MARINE ENVIRONMENT MONITORING AS A SENIOR DESIGN PROJECT,"Improving the design component in undergraduate engineering education has been an immediate and pressing concern for educators, professional societies, industrial employers and agencies concerned with national productivity and competitiveness. The projects are a valuable component of the science and engineering education. The design experience develops the students’ lifelong learning skills, self-evaluations, self-discovery, and peer instruction in the design’s creation, critique, and justification. Students learn to understand and make use of the manufacturer data sheets, application notes, and technical manuals when developing their design projects. The experience, which would be difficult to complete individually, gives the students a sense of satisfaction and the accomplishment that is often lacking in many engineering courses, using traditional teaching approaches. Furthermore, the design experience motivates student learning and develops skills required in industry. This paper discusses the development of a student project involving a number of senior undergraduate students at our engineering technology program. A simple, low cost solar power system to power a wireless sensor network (WSN) was developed during this project. The proposed WSN may be used for monitoring a coastal shallow water marine environment. It is composed of several sensor nodes or buoys. The description of this harvester, the system characteristics and performances are presented in details. Various aspects of the educational experience are examined such as the educational goals of the project, project organization, and outcomes. Innovative educational approaches are described such as brainstorming session and discussion with students of high-level choices described by a decision tree, component selections, simulations and system performance and characteristics computation. In the second part of the paper the design solution that was adopted is described in details. The adopted design solution includes: power electronics circuitry (DC-DC converter design and test), maximum power point tracking (MPPT) algorithms, control strategies, battery and super-capacitor selection as energy buffers, and overall system performances. Different MPPT and charging algorithms were analyzed and evaluated for their effectiveness in solar energy conversion system, as well as the control algorithms and implementation to maximize the power output. The project is a good example of multi-disciplinary cooperation as well as providing valuable hands-on experience. In addition to providing useful lessons in teamwork and project management, the project will provide a working demonstration wireless sensor network and solar energy system. The goal of the design project is to explore and enhance students understanding of the fundamental engineering principles, power circuit simulation capability and hands-on demonstration of system prototyping. Introduction and Project Rationale The wireless sensor networks (WSNs) is an autonomous network system which consists of large number of micro sensor nodes and has the characteristics of capability of sensing, calculation communication and low cost, and low power. It is a “smart” system that can accomplish various monitoring tasks, according to different environment conditions. Monitoring of water environment is one of its typical applications. Compared with existing real-time automatic water P ge 25120.2 environment monitoring systems, WSNs-based water environment monitoring system has strongpoint as follows : 1) Less effect of the system on ecological environment: nodes transmit water environment parameters to base station by low power and low radiation wireless channel and multi-hop communication protocol. Marine wireless sensor networks offer an unmatched option to a wide range of different domains. The significance of the aforementioned research lies in the fact that it opens the door for a variety of applications as well as new areas of relevant research in wireless networks. The possibility of having hundreds of thousands of sensor nodes diving in the ocean collecting data about the different inhabitants offers a unique opportunity for ocean studies and researchers in the field. The ability to seed wireless sensors that can dive deep in the ocean taking real-time pictures and reporting relevant data about the oceanic life can play a major role in bringing ocean research to new levels. In the following we present the development and designed of a solar energy harvester that can be used to provide power to a WSN for marine environment monitoring system. Coastal marine systems are particularly vulnerable to the effects of human activity attendant on industrial, tourist and urban development. Information and communications technologies offer new solutions for monitoring such ecosystems in real time. Therefore, during the past decade various initiatives emerged, from small-scale networks to complex coastal observation systems. Among small-scale networks, WSNs are a highly attractive solution for its easiness in deployment, operating and dismantling. WSNs are also relatively inexpensive. Energy harvesting or the process of acquiring energy from the surrounding environment has been a continuous human endeavor throughout history, e.g. the use of watermills in ancient Greece, and of sailboats by Phoenicians and Egyptians, circa 4000 B.C. Unlike the conventional electric power generation systems, in energy harvesting concept, fossil fuels are not used and the generation units might be decentralized. There are many sources for harvesting energy. Solar, wind, ocean, hydro, electromagnetic, electrostatic, thermal, vibration, and human body motion are renewable sources of energy. Even the energy of radio frequency waves, propagated due to television and radio broadcasting in the environment, can be harvested. Economic, environmental, and geopolitical constraints on global conventional energy resources started forcing the nations to accelerate energy harvesting from renewable energy sources. Thus, advanced technical methods should be developed to increase the efficiency of devices in harvesting energy from various environmentally friendly resources and converting them into electrical energy. These developments have sparked the interest in engineering community as well as in the engineering education community to develop more energy harvesting applications and new curriculums for renewable energy and energy harvesting topics. Nowadays, there is an increasing interest to harvest energy at a much smaller scale, for applications such as the ones found in many embedded systems where the power requirements are often small (less than 100 mW). Sustaining the power requirement for autonomous wireless and portable devices is an important issue. However, this progress has not been able to keep up with the development of microprocessors, memory storage, and wireless technology applications. For example, in wireless sensor networks, battery-powered sensors and modules are expected to last for a long period of time. However, conducting battery maintenance for a large-scale network consisting of hundreds or even thousands of sensor nodes may be difficult, if not impossible. Ambient power sources, as a replacement for batteries, come into consideration to minimize the maintenance and the cost of operation. Power scavenging may enable wireless and portable electronic devices to be completely self-sustaining, so that battery maintenance can be eventually removed. When P ge 25120.3 compared with energy stored in common storage elements, such as batteries, capacitors, and the like, the environment represents a relatively infinite source of available energy. Systems continue to become smaller, yet less energy is available on board, leading to a short runtime for a device or battery life. Researchers continue to build high-energy density batteries, but the amount of energy available in the batteries is not only finite but also low, which limits the life time of the systems. Extended life of the electronic devices is very important; it also has more advantages in systems with limited accessibility, such as those used in monitoring a machine or an instrument in a manufacturing plant used to organize a chemical process in a hazardous environment. The critical long-term solution should therefore be independent of the limited energy available during the functioning or operating of such devices. 1.1. Objectives of Project Work In response to these demands, universities offering baccalaureate and graduate degrees in electrical engineering or engineering technology must develop curricula to educate a workforce that is well equipped to meet these challenges. Unfortunately, US universities have not kept pace with the growing fields of power electronics, wireless networks or renewable energy, and they are not educating enough students in these recent technology applications, as needed by our industries. Rather this growth has been principally research and industry oriented. Little progress is being reported, with very few notable exceptions on the role of educational institutions in either keeping pace with this growth or addressing the importance of introducing new emerging engineering technologies, applications, and effective classroom and laboratory instruction. Engineering and engineering technology programs must offer a relevant and validated curriculum that prepares students for post-graduation success. Courses that cover traditional subject matter in mathematics, the sciences, engineering economics and other related topics provide the foundation of knowledge upon which specific skill sets are added. However, it is critical for engineering/technology to transition from theoretical work in the classroom towards experiential learning with applications of technology and design. The main objective of senior design courses in engineering and engineering technology",,2012.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
8881dadf57a7515039c978bf3755c336fbb417af,https://www.semanticscholar.org/paper/8881dadf57a7515039c978bf3755c336fbb417af,Customer Relationship Management Customer Satisfaction Ebooks Download,"This book introduces a fuzzy classification approach, which combines relational databases with fuzzy logic for more effective and powerful customer relationship management (CRM). It shows the benefits of a fuzzy classification in contrast to the traditional sharp evaluation of customers for the acquisition, retention and recovery of customers in online shops. The book starts with a presentation of the basic concepts, fuzzy set theory and the combination of relational databases and fuzzy classification. In its second part, it focuses on the customer perspective, detailing the central concepts of CRM, its theoretical constructs and aspects of analytical, operational and collaborative CRM. It juxtaposes fuzzy and sharp customer classes and shows the implications for customer positioning, mass customization, personalization, customer assessment and controlling. Finally, the book presents the application and implementation of the concepts in online shops. A detailed case study presents the application and a separate chapter introduces the fuzzy Classification Query Language (fCQL) toolkit for implementing these concepts. In its appendix the book lists the fuzzy set operators and the query language’s grammar. This work offers a state-of-the art survey of information systems research on electronic customer relationship management (eCRM). It provides important new frameworks derived from current cases and applications in this emerging field. Each chapter takes a collaborative approach to eCRM that goes beyond the analytical and operational perspectives most often taken by researchers in the field. Chapters also stress integration with other enterprise information systems. The book is organized in four parts: Part I presents an overview of the role of CRM and eCRM in marketing and supply chain management; Part II focuses on the organizational success factors behind eCRM implementation; Part III presents cases of eCRM performance enhancement; and Part IV addresses eCRM issues in business-to-consumer commerce. Continuous improvements in digitized practices have created opportunities for businesses to develop more streamlined processes. This not only leads to higher success in day-today production, but it increases the overall success of businesses. Enterprise Information Systems and the Digitalization of Business Functions is a key resource on the latest advances and research for a digital agenda in the business world. Highlighting multidisciplinary studies on data modeling, information systems, and customer relationship management, this publication is an ideal reference source for professionals, researchers, managers, consultants, and university students interested in emerging developments for business process management. Research Paper (postgraduate) from the year 2019 in the subject Business economics Customer Relationship Management, CRM, grade: 1.5, Kwame Nkrumah University of Science and Technology, language: English, abstract: Customer Relationship Management (CRM) practices are business strategies designed to reduce costs and increase profitability by solidifying customer loyalty. With intense competition among insurance companies in Ghana, this study sought to assess Customer Relationship Management practices and Customer Retention in NSIA Insurance. The study was conducted to identify critical factors necessary for customer retention in carrying out customer relationship management practices in the selected insurance company and to develop effective customer relationship management practices to manage customer retention for sustainability within the insurance industry using NSIA Insurance as a case study. Well structured questionnaires and face-to-face interview were the methods adopted for the investigation of the study. A sample size of 40 respondents was considered, they were made up of customers and the staff who are fully involved in customer relationship management of the insurance company. Data collected from the completed questionnaires and the interviews were grouped into frequency tables and expressed in percentages. The researcher relied on the SPSS in interpreting the collected data. The study shows that even though NSIA insurance has policies on customer relationship management practices, these policies are not carried out fully to accomplish the ultimate goal of customer retention. The study recommends that for the insurance company to command an adequate number of loyal customers, NSIA Insurance should consistently improve on its quality of service to address the preference of the customers and consider the five service quality constructs of reliability, assurance, tangibility, empathy and responsiveness. The two-volume set CCIS 143 and CCIS 144 constitutes the refereed proceedings of the International Conference on Electronic Commerce, Web Application, and Communication, ECWAC 2011, held in Guangzhou, China, in April 2011. The 148 revised full papers presented in both volumes were carefully reviewed and selected from a large number of submissions. Providing a forum for engineers, scientists, researchers in electronic commerce, Web application, and communication fields, the conference will put special focus also on aspects such as e-business, e-learning, and e-security, intelligent information applications, database and system security, image and video signal processing, pattern recognition, information science, industrial automation, process control, user/machine systems, security, integrity, and protection, as well as mobile and multimedia communications. Customer Relationship Management, Fourth Edition, is a much-anticipated update of a bestselling textbook, including substantial revisions to bring its coverage up to date with the very latest in CRM practice. The book introduces the concept of CRM, explains its benefits, how and why it can be used, the technologies that are deployed, and how to implement it, providing you with a guide to every aspect of CRM in your business or your studies. Both theoretically sound and managerially relevant, the book draws on academic and independent research from a wide range of disciplines including IS, HR, project management, finance, strategy and more. Buttle and Maklan, clearly and without jargon, explain how CRM can be used throughout the customer life cycle stages of customer acquisition, retention and development. The book is illustrated liberally with screenshots from CRM software applications and case illustrations of CRM in practice. New to this Edition: Updated instructor support materials online Full colour interior Brand",,2021.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
c8187a590c73e685d9ed2c23060bdc5549080540,https://www.semanticscholar.org/paper/c8187a590c73e685d9ed2c23060bdc5549080540,Machine Learning Enabled Techniques for Protecting Wireless Sensor Networks by Estimating Attack Prevalence and Device Deployment Strategy for 5G Networks,"A number of disadvantages of traditional networks may be attributed to the close relationship that exists between the control plane and the data plane inside proprietary hardware designs, as described above. The problem of security is one of the most difficult to deal with. There are a plethora of network hazards and attacks that might be encountered these days. DDoS attacks are one of the most popular and disruptive attacks on the internet today, and they affect a wide range of organisations. Despite a large number of traditional mitigation solutions now available, the frequency, volume, and intensity of distributed denial-of-service (DDoS) attacks continue to rise. According to the findings of this paper, a new network paradigm is necessary to satisfy the requirements of today’s complex security concerns. It was necessary to develop a software-defined network (SDN) in order to meet the real-time needs of the massive network that was expanding at an exponential rate. Many advantages of SDN exist, including simplicity of administration, scalability, and agility, but one of the most critical is security, which is one of the most important considerations when implementing SDN. SDS may be seen as a paradigm in which the implementation of new security regulations in the computer environment is performed via the use of protected software, which is described further below. The goal is to provide a flexible and extensible architecture for DDoS detection and prevention that is both flexible and extendable; the suggested clustering approach, which is based on the Open Day Light (ODL) Controller, is employed to carry out the experimental findings. In this section, we emphasise DDoS penetration techniques from a range of tools, and we evaluate the vulnerability against various tactics. It is necessary to use a Mininet emulation tool to construct a detection and prevention system against distributed denial of service (DDoS) attacks in order to achieve success. There is a range of other simulation tools that are utilised in conjunction with this research in order to bring it to a conclusion. Integration of industry standards such as SNORT and Flow has been accomplished in a variety of situations and parameter settings. During the creation of a framework capable of detecting and mitigating DDoS attacks at an early stage in both the control and application levels, the implementation of this framework has been shown to be crucial in the development of a framework.",Wireless Communications and Mobile Computing,2022.0,10.1155/2022/5713092,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
32480856d3c70b3a5d737352376e553517d8a58b,https://www.semanticscholar.org/paper/32480856d3c70b3a5d737352376e553517d8a58b,List of TA topics: EE1 (Communications and Signal Processing),"Description: The specific goals of the PhD projects are to numerically simulate and demonstrate different optical fiber designs, components, and devices suitable for mid-infrared technology. Student will be thoroughly trained in simulating optical fiber technology using COMSOL Multiphysics and hands-on experience in handling current state-of-the-art Opto-electronics instruments such as CCD sensors, oscilloscope, spectrum analyzer etc. Interested students are encouraged to contact the PI for further discussions. Description: Erbium-doped optical amplifiers are the backbone of optical communication, used both for transmission and detection at the receiving end. Most of the photonic detectors in the transmission window have a typical sensitivity of the -24 dBm. A very low noise signal amplifier is needed which can detect the weak signal and increase the sensitivity of the system to -60/65 dBm or even less. The project will exploit optical parametric amplification, a type of nonlinear optical effect, which allows for low-noise optical amplification with very low degradation of the signal-to-noise ratio. Different gain mediums will be explored. There will be a strong emphasis on compact and robust amplifier systems. The student will have the opportunity to learn and model nonlinearity in different materials, amplifier physics, signal processing, and hands-on experience with current state-of-the-art characterization pieces of equipment such as spectrum analyzers, oscilloscopes, CCD sensors, etc. The project is highly relevant for students looking for the opportunity both in academia and industry. Interested students are encouraged to contact the PI for further discussions. Description: Fiber optics based on long thin strands of glass and electronics based on planar chips have become the backbone of the electronics and telecommunication industry. There have been attempts to fabricate multi-material fibers for exploiting the exotic properties of both semiconductors and glasses. Recent fabrication progress in multi-material fabrication technology has raised interest in this technology. This project will explore and demonstrate next-generation novel semiconductor fibers to exploit the complementary properties of both semiconductors and glasses. We aim to find out suitable compositions of materials that are feasible to fabricate using current state-of-the-art fabrication technology leading to all-fiber optoelectronics systems. The student will have the opportunity to learn to model semiconductor physics and optical fibers using semiconductor physics and RF modules of COMSOL Multiphysics, hands-on experience with current-state-of-the-art cleanroom fabrication facilities, and characterization techniques. Interested students are encouraged to contact the PI for further discussions. Description: The objective of this project is to develop an on-board charger (OBC) system for four-wheeler electric vehicles (EVs), compatible with both 3-phase, level-3 (22 kW) charging and 1-phase, level-2 (7 kW) charging. Such a converter would represent an ""universal"" charging solution allowing both fast, public-space charging from 3-phase EVSEs as well as slow, overnight charging from 1-phase residential power outlets. The overarching design goal will be to develop a very high power-density converter prototype, which can offer significant size and weight benefits to the EV system, compared to conventional approaches. This will be achieved primarily through innovations in converter topology by exploring low device-count, single-stage circuits, which allow interfacing with 3-phase as well as 1-phase ac and can operate with small-sized energy buffer capacitors. Additionally, advances in soft-switching methods (enabling switching frequencies of several 100s of kHz) , modulation, control and high-frequency magnetics design will also be pursued. assay low-cost front-end electronics embedded systems integrated biosensors applications, nucleic in adulterants and biomarkers to techniques to understand fundamental aspects of interaction of biological analytes with such systems as well as realizing commercializable applications. Our works Bioengineering (WRCB) and can forward to a and Description: The goal of this project is to develop a reconfigurable CMOS Receiver System-on-chip (SOC) (RF + Digital) for NavIC and GPS in 28-nm CMOS technology. The receiver SOC will extract the Position, Velocity, and Time (PVT) information of the user/receiver from the satellite signals of NavIC and GPS for Standard Positioning Services (SPS). PhD students will get trained in the area of RF/Analog/Mixed-signal IC design. They will do circuit design and layout of novel High performance circuits such as LNA, Mixed, PLL, VCO, Filters, Analog to Digital Converters (ADC). Students will also be involved in Chip measurements in the lab. Description: The goal of this project is to develop a reconfigurable CMOS Receiver System-on-chip (SOC) (RF + Digital) for NavIC and GPS in 28-nm CMOS technology. The receiver SOC will extract the Position, Velocity, and Time (PVT) information of the user/receiver from the satellite signals of NavIC and GPS for Standard Positioning Services (SPS). PhD students will get trained in the area of digital IC design flow. They will design digital baseband starting from algorithms --> Matlab --> HDL --> FPGA verification --> ASIC implementation --> Timing closure/Place and Rout activities. Students will also be involved in Chip measurements in the lab. The PhD aspect comes in the design of innovative implementation of complex algorithms required for decoding NAVIC and GPS signals (Correlator design). Description: Artificial smart skin is a growing area finding its applications in robotics and medical applications. This technology also brings new aspects of energy provision for the artificial skin. In this project we will work on the flexible layers of generators with a CMOS power management ASIC which will be designed, fabricated and integrated on the artificial skin. Description: Various emerging sensing applications need energy efficient and noise-tolerant analog signal conditioning for the high performance, followed by novel hybrid low-energy high-resolution architectures for the analog to digital conversion (ADC). The purpose of this project is to extend such functionalities with high performance to the various domains of information for better tolerance to the internal and external noise as well as calibration for the selective analog to digital conversion. electric fields, temperature, strain fields, and pressure. NVCs can detect magnetic fields with a high spatial resolution down to the nanoscale and on samples in environmental conditions not suitable for alternative solid-state magnetometers. Similarly, thin flakes of hBN are upcoming platform in the domain of spin-based sensing. This project will be to develop precision quantum sensors. Further, we will investigate a variety of applications of these magnetometers, in particular – investigating bio-magnetic materials and 2D materials with spatially dependent local magnetic field. Description: The project involves design, fabrication and characterization of optical silicon chip based micro-ring resonators for the generation of frequency combs. These study include efficient light confinement within waveguides and resonators and manipulation of nano-scale objects using guided waves. The frequency combs will be used to calibrate optical clocks used for time standards and for spectroscopy. Development of machine learning techniques for ""Beyond Moore"" device simulation platforms. Description: Developing a composite device to circuit modelling platform for spintronic neuromorphic architecture. This will typically feature basic device modelling with the integration into h-spice based circuit models for a given target functionality. Description: Topological insulators are known to host ""dissipation-free"" electrical conduction. The project will aim to evaluate and design electrical interconnects that can exploit this dissipation free channel. The project will involve developing circuit models starting from the physics of topological insulators. Description: Heavy-metal-and-ferromagnet-heterostructure-based spintronic devices exhibit various interesting properties which open up possibilities for using them for neuromorphic computing — a brain-inspired computing paradigm which often uses in-memory-computing architectures, spike-based information flow, and oscillatory behaviour for artificial-intelligence (AI) applications where low energy consumption is a necessary requirement (edge AI/ tiny ML, robotics, etc). This project focuses on carrying out device experiments and device-circuit-system co-simulations to investigate three major aspects of such spintronics-based neuromorphic computing: (a) Use of domain-wall motion for non-volatile analog weight storage, to be used as synapses in crossbar arrays for implementing non-spiking fully connected and convolutional neural networks (b) Use of domain-wall motion as an integrator for mimicking spiking neurons in spiking neural networks (c) Use of the synchronizaion dynamics of spin Hall nano oscillators for oscillator-based pattern recognition.. Description: Large band gap and ease of growth of gallium oxide has led to a new wave in wide bandgap power devices and electronics. At the same time, many fundamental challenges in gallium oxide processing and device fabrication offer an opportunity to explore new device physics and engineer high impact solutions. This project will explore the fabrication of normally off high power gallium oxide transistors for power switching applications. It will involve experimental device fabrication, characterisation and TCAD simulations. Description: India has announced plans to set up up to 300,000,000,000 W of Solar PV by 2030 to combat climate change. This significant amount of PV deployment is going to happen with new techno",,2022.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
50c1184569211e62e320bdea1d87b285e6cd1466,https://www.semanticscholar.org/paper/50c1184569211e62e320bdea1d87b285e6cd1466,Towards a New Safety Assurance Method for Complex Safety-Critical Systems,"Know your enemy and know yourself and you can fight a hundred battles without disaster. Sun Tzu, The Art of War This essay investigates the making of timely and acceptable safety assurance decision throughout a safety-critical system’s lifecycle in a complex environment. This ‘battle’ of assuring a safety-critical system is safe enough to avoid the ‘disaster’ of accident is a long and treacherous journey. It demands a structured and persistent method to support safety analysis despite challenges from the non-deterministic and non-linear environment throughout the system lifecycle. Learning from Sun Tzu, this journey begins by searching for clarity regarding the two important questions: ‘know yourself’ by understanding the current way of conducting safety management throughout a system lifecycle; and ‘know the enemy’ by examining the potential risk and hazards cause by the unique characteristics of complexity. This is followed by a survey of possible methods that can be used to analyse and mitigate the risk under a complex system. Systems Engineering Essay Competition 2015 Page 2 of 25 Jointly organised by Temasek Defence Systems Institute Department of Industrial and Systems Engineering, Faculty of Engineering, NUS The literature reviews make three observations. First, while there are established safety management processes to analyse safety at specific milestones of the system lifecycle, there is a lack of continuity in managing and utilising the system knowledge regarding safety throughout the system lifecycle. Secondly, safety assessment must adapt to changing social-technical context especially when a system acquisition lifecycle comprises multiple distinct phases that introduce different constraints to safety assessment. Lastly, uncertainty in the complex environment can never be eliminated but there should be more efforts to minimise the surprises due to incomplete and imperfect information so as to create the confidence in the safety analysis. The paper concludes by providing a preliminary approach to develop a safety assurance method that aims to be applied throughout the system lifecycle to provide a structured and continual way to support decision making. Introduction: Where are the Hazards? As technology advances, more and more systems in domains like defence, air traffic management, railway transport, nuclear power plant, offshore drilling and health care are becoming highly networked and complex. These are described as safety-critical systems as any failure can potentially lead to the loss of life and damage to property or environment. For example, in the defence industry, networked of large-scale safety-critical systems (often refers to as Network Centric Warfare or System-of-Systems 1 [1]) have been revolutionising the applications of military technology as machines and computers are used to carry out complex and time-critical tasks. These machines are inter-connected to form larger inter-dependent systems and continuing to expand in numbers and complexity as the armed forces attempt to accomplish more challenging missions. Unfortunately, while there are established safety management processes, accidents that led to the loss of Systems Engineering Essay Competition 2015 Page 3 of 25 Jointly organised by Temasek Defence Systems Institute Department of Industrial and Systems Engineering, Faculty of Engineering, NUS life and damage to properties continue to surface from such highly complex safety-critical systems. Some of the most notable accidents and incidents in recent years include:  (Space) NASA space shuttle Challenger (1986) [2] and Columbia (2003) [3] disasters  (Military Operation) B-1B Lancer bomber friendly fire on coalition soldiers in Afghanistan (2014) [4]  (Drilling) Piper Alpha offshore oil production explosion (1988) [5]  (Maintenance) F-111 (Fuel Tank) De-seal/Reseal program leading to chemical exposure (2001) [6]  (Health-care) Overdose of radiation during Therac-25 radiation therapy (1985) [7]  (Rail) Wenzhou high speed railway collision (2011) [8] While the faults for each of these disasters are unique, one common observation is that it is extremely difficult to narrow down to a specific failure mode. Unlike complicated but linear systems where traditional safety analysis method is capable of using linear reductionist approach to deduce the root causes, the failure modes in complex safety-critical systems are different. Reiman [9] observes that effects from a complex system have “several parallel contributing factors, instead of one or few causal chains 2 as in linear systems”. Dekker [10] also believes that “the behaviour of such complex system cannot be reduced to an aggregate of the behaviour of its constituent components”. Hence, even if one root cause has been identified, decision makers may face the frustrating but real challenge of not being able to fully comprehend the full casual chains of complex relations. One good example of a complex safety-critical system is Air Traffic Control (ATC). On 12 Dec 2014, an air traffic disruption at the Swanwick ATC resulted in numerous flight cancellations across Heathrow, Gatwick and London City [11]. The System Flight Servers failed when more workstations were being brought online during the transition between 1 The System-of-Systems (SoS) refers to a set of systems that are cooperating for a common purpose while simultaneously working as independent entities. 2 A causal chain refers to the path or sequence of events that runs from a root cause to problem symptoms in the real world. Systems Engineering Essay Competition 2015 Page 4 of 25 Jointly organised by Temasek Defence Systems Institute Department of Industrial and Systems Engineering, Faculty of Engineering, NUS normal and standby operation [12]. This affected air traffic control as it was impossible for controllers to access aircraft flight plans. While the final report has yet to be released, NATS has announced that the failure is unprecedented in its 13 years of operations. The incident highlights the difficulties in managing system safety in complex environment. There will be intense pressure when failures occur and rightfully so since safety-critical systems are utilised in critical situations (e.g. ATC keeping the airspace safe) where there are severe consequences (e.g. disruption to commercial flights and passengers’ safety) when the systems fail. It is also possible that certain failures may never manifest themselves during system development as it is impossible to predict and conduct safety assessment on all operational scenarios (e.g. overloading of the ATC System Flight Servers). Active sharing of knowledge regarding safety throughout the system lifecycle can help to better anticipate and identify such ‘blind-spots’ during operation. In terms of managing safety-critical projects, social, technical and organisation tensions continue to pose safety challenges when it comes to acquiring a system in such complex environment. The following shows a sample of such concerns highlighted by Atkinson [13] and Saunders [14] from their surveys on managing project uncertainty of complex safety-critical system:  Novelty of design and technology  Diverse and conflicting stakeholders expectations and belief  Failure to anticipate concurrency of activities and capture dependency relationships  Ineffective communication and knowledge management with changing stakeholders throughout system lifecycle  Lack of continuity in personal and responsibilities when managing different interoperating systems  Incomplete and imperfect information  Lack of systematic process to capture corporate knowledge and lessons learned While the list represents uncertainty in managing projects, it is equally relevant to safety management and highlights the diversity of social-technical context that would affect the Systems Engineering Essay Competition 2015 Page 5 of 25 Jointly organised by Temasek Defence Systems Institute Department of Industrial and Systems Engineering, Faculty of Engineering, NUS effectiveness of any safety analysis. Most safety-critical systems comprise human-machine interactions and it will be important to consider explicitly the impact of changing socio-technical context during safety analysis so as to create attention to potential hazards in such circumstances. The System Lifecycle – where it begins One approach to appreciate the safety hazards facing a safety-critical system is to consider its lifecycle. A system can be considered as a “combination, with defined boundaries, of elements that are used together in a defined operating environment to perform a given task or achieve a specific purpose”[15]. Taking reference from the military, a system is realised by following the system acquisition lifecycle. A full-scale system acquisition lifecycle includes multiple stages, milestones and decision points that shape the development of a system till its operationalisation. Two examples of military acquisition lifecycles are shown in Figure 1. They are the UK MoD CADMID 3 acquisition cycle and the US DoD Defence Acquisition Process. Figure 1 Categorisation of System Acquisition Lifecycle in defence A system lifecycle has two distinct phases: development and operation. Both phases are subjected to risk and safety hazards in the complex environment but exhibit different system characteristics. The following table provides a comparison of the two phases. Characteristics System Development System Operation 3 CADMID refers to the six phases of acquisition lifecycle: Concept, Assessment, Demonstration, Manufacture, In-service and Disposal. Systems Engineering Essay Competition 2015 Page 6 of 25 Jointly organised by Temasek Defence Systems Institute Department of Industrial and Systems Engineering, Faculty of Engineering, NUS Characteristics System Development System Operation Type of processes Design, plan, production, testing, and deployment Operation, maintenance and support, ret",,2015.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
8f4957c04ec36563ae31924208d2a0390936a55f,https://www.semanticscholar.org/paper/8f4957c04ec36563ae31924208d2a0390936a55f,Application of Deep Learning in the Deployment of an Industrial SCARA Machine for Real-Time Object Detection,"In the spirit of innovation, the development of an intelligent robot system incorporating the basic principles of Industry 4.0 was one of the objectives of this study. With this aim, an experimental application of an industrial robot unit in its own isolated environment was carried out using neural networks. In this paper, we describe one possible application of deep learning in an Industry 4.0 environment for robotic units. The image datasets required for learning were generated using data synthesis. There are significant benefits to the incorporation of this technology, as old machines can be smartened and made more efficient without additional costs. As an area of application, we present the preparation of a robot unit which at the time it was originally produced and commissioned was not capable of using machine learning technology for object-detection purposes. The results for different scenarios are presented and an overview of similar research topics on neural networks is provided. A method for synthetizing datasets of any size is described in detail. Specifically, the working domain of a given robot unit, a possible solution to compatibility issues and the learning of neural networks from 3D CAD models with rendered images will be discussed.",Robotics,2022.0,10.3390/robotics11040069,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
55f8d4782b98a5f1bb9fbebfe1ef54cb6f4094fc,https://www.semanticscholar.org/paper/55f8d4782b98a5f1bb9fbebfe1ef54cb6f4094fc,Computer Controlled Instrumentation Projects By Sophomore Level Eet Students,"This paper presents student-initiated projects as part of an instrumentation and data acquisition course for sophomore-level electronics engineering technology students. Project objectives and associated assessment methodologies as well as general project management concepts are discussed. Two sample instrumentation projects reported in this paper are an automated street parking system and a computer-controlled bowling game system. Both projects focused on instrumentation system development integrating multiple sensors and actuators, data acquisition hardware, interface electronics, control logic implementation in LabVIEW software, and wood/metal work for prototype development. These end-of-semester course projects were carried out during the final four weeks of the semester after eleven weeks of lecture/laboratory session. Introduction The ability to conduct and design experiments is rated as one of the most desirable technical skills of engineering and engineering technology graduates. Specifically, the referenced survey indicates that employers want graduates with a working knowledge of data acquisition, analysis and interpretation; and an ability to formulate a range of alternative problem solutions. Additionally, potential employers of our EET graduates are in the automated manufacturing and testing sector of the industry providing additional motivation for an instrumentation and data acquisition course at the sophomore level of a four-year EET program. This course consists of two hours of lecture and three hours of laboratory per week. Students have had courses in electrical circuit analysis, electrical machines, and analog and digital electronics before taking this course. The first three weeks of the fifteen-week semester are devoted primarily to LabVIEW programming. During the next eight weeks, the concepts and integration of sensors and actuators, interface electronics, and data acquisition and instrument control hardware /software are covered. The final four weeks are dedicated to student-initiated laboratory design projects. This paper focuses on general approach to implementing end-of-semester course projects and associated assessment tools used to assess the project objectives. Technical details of two sample instrumentation projects, an automated street parking system and a computerized bowling game system, implemented during the spring-2007 semester are also presented. Course project objectives and the associated assessment method The learning and teaching objectives for the project experience are listed in the next page. A list of questions was prepared based on the stated objectives, and the survey was conducted at the end of second and fourth week of the four-week project experience as an indirect assessment tool. The results of the first survey was used to improve the project experience during the second half, and the results of the second survey is to be used to improve the next offering of the instrumentation project experience in spring-2008. Students are also assessed using direct assessment tools for teamwork, oral presentation, final report, successful operation and demonstration of the completed project, and design review meetings. Example rubrics used to assess teamwork and oral presentation are shown in Appendices A and B, respectively. Results of direct and indirect assessment instruments are archived for use as an input to the course P ge 13322.2 continuous improvement process and also as part of display materials for program accreditation visits. Project Learning Objectives Project Teaching Objectives ‚ Gain experience in interpreting technical specifications and selecting sensors and transducers for a given application ‚ Foster discovery, self-teaching, and encourage desire and ability for life-long learning ‚ Understand terminologies associated with instrumentation systems ‚ Provide experience in designing instrumentation system based on specifications ‚ Gain experience in developing computerized instrumentation systems for industrial processes using multiple sensors, interface electronics, data acquisition hardware, and GPIB and serial instruments ‚ Develop soft skills including teamwork, openended problem solving, formal report writing and oral presentation Project management Early in the semester students start developing potential project topics with appropriate feedback and guidance from the instructor leading to a required pre-proposal submission by the fifth week of the semester. Upon approval of the pre-proposal, students are required to submit a formal proposal for a specific project topic by the ninth week of the fifteen-week semester. Use of a minimum of four sensors/transducers and four actuators is required as part of any project. The required proposal is quite detailed as it includes project implementation ideas supported by major outcomes and specifications, I/O interface drawing, circuit schematics, parts list with vendor and price information, LabVIEW program flow chart, and project completion schedule including a Gantt chart. An example student-generated Gantt chart is shown in Appendix C, prepared using Vision Professional. For implementation of the project, students are in charge of selecting the necessary sensors and actuators and are required to use the well-equipped departmental shop for fabrication and metal/wood work. Each group of two students is allocated a nominal budget of $200 for purchasing project-specific parts not normally available in the laboratory. Project deliverables include pre-proposal, proposal, preliminary design review, critical design review, final report, and a formal presentation. Student presentations and final reports are archived for use as part of the display materials for future accreditation visits. Laboratory setup Each station is equipped with a PC, and GPIB/RS-232 interfaced instruments such as digital multimeter, triple output laboratory power supply, arbitrary function generator, and two-channel color digital oscilloscope. The instrumentation and data acquisition specific software and hardware are briefly described below. Software: LabVIEW 8.5 from National Instruments Data acquisition (DAQ) board: Model 6024E from National Instruments ‚ 16 single-ended or 8 differential analog input channels, 12 bit resolution, 200 kS/s ‚ 2 analog voltage output channels, 12 bit resolution, 10 kHz update rate ‚ 8 digital I/O channels with TTL/CMOS compatibility; and Timing I/O GPIB controller board: ‚ IEEE 488.2 compatible architecture (eight-bit parallel, byte-serial, asynchronous data transfer) ‚ Maximum data transfer rate of 1 MB/sec within the worst-case transmission line specifications Signal conditioning accessory: ‚ Model SC-2075 from National Instruments ‚ Desktop signal breakout board with built-in power supplies, connects directly to 6024E DAQ board P ge 13322.3 Sample Project: Automated street parking system The objective of the automated street parking system was to implement a prioritized parking system with prepayment and post payment options including a boot system for parking violators. For this street parking management system, three categories of cars are considered: resident, frequent, and visitor. A resident car can be parked for an unlimited amount of time without accruing any fines, a frequent car can be parked on a daily basis for a limited number of hours to be billed for parking fees on a biweekly basis, and a visitor car would need to pay upfront for parking. Additionally, activation of a boot system from under the street upon expiration of parking credit and/or other violations was an integral part of the system. A block diagram representation of the I/O interface for the street parking system is shown in Figure 1 and a pictorial view of the system is shown in Figure 2. This prototype system consisted of three parking spots along a street. A total of eight analog inputs were used in implementing the system: three inputs for detecting the type of car, three inputs for parking spot availability status, one input for spot selection for prepayment, and an additional input for coin collection system. The coin collection system was based on an inductive proximity sensor while the other seven input signals were based on simple voltage divider networks and/or photoresistors. An example voltage-divider based interface for prepayment spot selection is shown in Figure 3. Figure 1: A block diagram representation of the I/O interface for the automated street parking system. P ge 13322.4 Figure 2: A pictorial view of the automated street parking system. Figure 3: Implementation of the spot selection logic for prepayment. The parking system used a total of nine outputs: six digital outputs for various parking status indicators and three analog (but used as digital) outputs for driving the solenoids for the boot system. In case of malfunctioning of the car type detection system for a given spot, the status light will turn red and draw attention of the police via the end of street display lights. This end of street display and the boot system get activated in case of an unpaid visitor car in a spot. For an activated boot deployment system, only the police personnel can release the boot system. After the car is removed from the spot, the system resets itself for the next car to be parked. The control logic for this system was implemented in LabVIEW software. A typical front panel display for the automated street parking system is shown in Figure 4 and it includes status monitoring of the following subsystems: parking spot, boot activation, prepayment, coin collection, and prepaid parking timer. The corresponding block diagram for implementing the Figure 4: A typical front panel display of the automated street parking system. P ge 13322.5 parking spot logic functions are shown in Figure 5. The major LabVIEW function blocks used are case structure, sequence structure, for loop, subVIs, local variable, various array and string f",,2008.0,10.18260/1-2--3460,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
05301d6f18532022a64ae3a12218b3647b2b1c02,https://www.semanticscholar.org/paper/05301d6f18532022a64ae3a12218b3647b2b1c02,Redefining Next-generation Endpoint Security Solutions,"Enterprise organizations face a difficult situation. Many current endpoint security tools can’t prevent or detect sophisticated exploits or zero-day malware, forcing CISOs to implement an assortment of next-generation endpoint security tools. Unfortunately, this strategy can increase cost and complexity while introducing the potential for resource contention and performance issues on the endpoints themselves. Is there any alternative to this Faustian compromise? Yes—next-generation endpoint security solutions built for centralization, consolidation, and integration that offer functionality for prevention, detection, and response. McAfee’s recently announced Dynamic Endpoint Threat Defense is a next-generation endpoint security solution that can improve security efficacy while streamlining security operations. Solution Showcase Enterprise Strategy Group | Getting to the bigger truth.TM Solution Showcase: Redefining Next-generation Endpoint Security Solutions 2 © 2016 by The Enterprise Strategy Group, Inc. All Rights Reserved. Figure 1. Antivirus Product Challenges Source: Enterprise Strategy Group, 2016 Endpoint security challenges are also exacerbated by the global cybersecurity skills shortage. According to research published earlier this year, 46% of organizations report a problematic shortage of cybersecurity skills. This cybersecurity shortage is compounded by the use of tools that work in isolation and require manual coordination. In many cases, organizations are understaffed and lack the right skills to employ strong endpoint security best practices. Instead, security administrators are relegated to “firefighting” an overwhelming amount of security alerts and using precious time to address risks and update protection through manual processes. The Endpoint Security Continuum New endpoint security requirements have created a flurry of technology innovation and industry buzz around “nextgeneration endpoint security (NGEPS)” products. This has only led to market confusion as infosec professionals try to sort through an avalanche of vendor marketing hype. ESG believes it shouldn’t be this difficult. In its 2016 market landscape report titled, Enterprise Adoption of Next-generation Endpoint Security, ESG defined endpoint security as: 4 Source: ESG Research Report, 2016 IT Spending Intentions Survey, February 2016. 13% 24% 26% 29% 33% 34% 35% 48% 0% 10% 20% 30% 40% 50% 60% We have not experienced any challenges with antivirus products AV management systems don't scale to support enterprise needs Products are too complex to configure and manage to their full potential AV management doesn’t integrate with other security and IT management systems Products are not nearly as effective at blocking and/or detecting malware as they should be Too many false positives that classify benign files/software as malware New product revisions tend to be extremely different from previous versions requiring a lot of time and resources for training and deployment Products impact overall performance of endpoint systems What challenges – if any – has your organization experienced with the antivirus products used as part of its endpoint security strategy? (Percent of respondents, N=340, multiple responses accepted) Solution Showcase: Redefining Next-generation Endpoint Security Solutions 3 © 2016 by The Enterprise Strategy Group, Inc. All Rights Reserved. The policies, processes, and technology controls used to protect the confidentiality, integrity, and availability of an endpoint system. Furthermore, NGEPS was defined as: Endpoint security software controls designed to prevent, detect, and respond to previously unseen exploits and malware. ESG believes that next-generation endpoint security should include capabilities across an overall endpoint security continuum (see Figure 2). At one end, advanced prevention technologies should offer superior efficacy for malware and exploit prevention when compared with traditional AV products. This functionality should include the ability to “learn” from every attack for stronger response, faster performance, and improved efficacy. In this way, next-generation endpoint security can block all but the most sophisticated cyber-attacks, greatly reducing the amount of malicious traffic on the network and system reimaging burden placed on IT operations. At the same time, however, CISOs must assume that sophisticated cyber-criminals and nation-states will discover and exploit advanced prevention technology vulnerabilities over time, so they will also need the right tools for efficient detection and remediation of malicious endpoint activities. Figure 2. The Endpoint Security Continuum Source: Enterprise Strategy Group, 2016 As part of the continuum, next-generation endpoint security is supported with additional types of security controls (i.e., port controls, application controls, DLP/eRM, etc.). These controls are intended to decrease the endpoint and network attack surface, making network penetration and system compromises more difficult for cyber-adversaries. This can improve security, but can also carry costs because of:  Multiple products working in isolation. Security and IT operations teams may be forced to install and manage multiple, isolated products on their endpoints. This introduces an operational burden and can cause contention and performance issues on the endpoint systems. Solution Showcase: Redefining Next-generation Endpoint Security Solutions 4 © 2016 by The Enterprise Strategy Group, Inc. All Rights Reserved.  Multiple management planes. New endpoint software tools for prevention, detection, and response come with their own management consoles for policy management, configuration management, and reporting. Once again, this adds more work for an already overwhelmed security and IT operations staff. Toward Next-generation Endpoint Security Solutions Clearly, large organizations want to improve security efficacy without adding operational overhead or disrupting business processes or user productivity. This is likely why the majority of cybersecurity professionals (58%) claim that their organization would prefer to buy a comprehensive endpoint security solution from a single vendor rather than cobble together a solution out of assorted endpoint security point tools (see Figure 3). Figure 3. Most Attractive Choice of Endpoint Security Controls and Analytics Delivery Source: Enterprise Strategy Group, 2016 A comprehensive endpoint security software solution from a single vendor would need to have all of the elements of the ESG endpoint security continuum, spanning from advanced prevention to advanced detection and response. This would include:  A defense-in-depth architecture for threat and exploit prevention. Strong network security is built using layered security with each security control supporting and complementing others. In this way, packets must pass through an assortment of filters (i.e., firewalls, web threat gateways, AV gateways, etc.) before they reach their ultimate destination. Similarly, next-generation endpoint security tools should contain several preand post-execution filters in order to prevent and detect exploits and malware. These filters will range from tried-and-true AV signatures to an assortment of other technologies including behavioral heuristics, machine learning algorithms, threat intelligence 5 Source: ESG Research Report, The Endpoint Security Paradox, January 2015. A comprehensive endpoint security software suite from a single vendor, 58% An assortment of endpoint security technologies from various vendors, enabling my organization to choose best-of-breed products in each category, 33% A portfolio of endpoint security products from various vendors that establish technical partnerships to integrate their products together into a heterogeneous endpoint security suite, 8% Don’t know, 1% As new endpoint security requirements arise and your organization considers new endpoint security controls and analytics, which of the following choices do you think would be most attractive to your organization? (Percent of respondents, N=340) Solution Showcase: Redefining Next-generation Endpoint Security Solutions 5 © 2016 by The Enterprise Strategy Group, Inc. All Rights Reserved. correlation engines, and isolation technologies that execute files in virtual containers blocking access to real system resources.  Competitive EDR capabilities. In addition to a defense-in-depth endpoint security architecture, next-generation endpoint security tools must be able to monitor and capture system behavior as well as standalone EDR solutions do today. To fulfill this requirement, NGEPS solutions must be able to collect, process, analyze, and present active endpoint behavior data in ways that support organizations’ security analysis processes. The best tools will include tight integration with threat intelligence and offer closed-loop processes that take newly discovered exploits, malware, and vulnerabilities and translate them into remediation rules for blocking future similar attacks.  An architecture designed for consolidation, centralization, and integration. NGEPS solutions can provide real value in a few of the most important areas for enterprise organizations. First, next-generation endpoint security solutions can be built on an integrated infrastructure, creating a single coordinated system and minimizing management overhead. Second, NGEPS solutions offer a consolidated management plane with integrated functionality for policy management, configuration management, and reporting. These solutions should also feature role-based access control to support division of labor and separation of duties between security and IT operations personnel. Finally, enterprise organizations often integrate endpoint security solutions with network security tools, security analytics systems, incident response platforms, and third-party threat intelligence feeds. NGEPS solutions should be desi",,2016.0,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
bd1a8094a14f68528d8910b7455f630e6d4cd36c,https://www.semanticscholar.org/paper/bd1a8094a14f68528d8910b7455f630e6d4cd36c,SIGLOG monthly 198,"Deadline: January 15th, 2018 Submission Deadline: January 22nd, 2018 Rebuttal: March 22 25th, 2018 Notification: April 2nd, 2018 Camera-Ready: May 2nd, 2018 FSCD Conference: July 9 12th, 2018 FLoC Conference: July 6 19th, 2018 * PROGRAM COMMITTEE CHAIR Helene Kirchner, Inria * CONFERENCE & WORKSHOP CHAIR: Paula Severi, Leicester U. 13TH WORKSHOP ON GAMES FOR LOGIC AND PROGRAMMING LANGUAGES (GaLoP 2018) Call for Abstracts Thessaloniki, Greece, 14-15 April, 2018 http://www.gamesemantics.org * GaLoP is an annual international workshop on game-semantic models for logics and programming languages and their applications. This is an informal workshop that welcomes work in progress, overviews of more extensive work, programmatic or position papers and tutorials. * GaLoP XII will be held in Thessaloniki, Greece, on 14-15 April 2018 as a satellite workshop of ETAPS (http://www.etaps.org/). * IMPORTANT DATES Submission: 22 January 2018 Notification: 12 February 2018 Workshop: 14-15 April 2018 * Invited talks Guy McCusker (Bath) Matteo Mio (Lyon) Ulrich Schopp (Munchen) * Programme Chairs Ugo Dal Lago (Bologna, co-chair) Gabriel Sandu (Helsinki, co-chair) 4th ACM CYBER-PHYSICAL SYSTEM SECURITY WORKSHOP (ACM CPSS’18) Incheon, Korea, June 4, 2018 (in conjunction with ACM AsiaCCS’18) http://jianying.5gbfree.com/cpss/CPSS2018/ Extended submission deadline: Jan 27, 2018 (23:59 GMT) * CONFERENCE OUTLINE Cyber-Physical Systems (CPS) consist of large-scale interconnected systems of heterogeneous components interacting with their physical environments. There are a multitude of CPS devices and applications being deployed to serve critical functions in our lives. This workshop will provide a platform for professionals from academia, government, and industry to discuss how to address the increasing security challenges facing CPS. Besides invited talks, we also seek novel submissions describing theoretical and practical security solutions to CPS. Papers that are pertinent to the security of ACM SIGLOG News 37 January 2018, Vol. 5, No. 1 embedded systems, IoT, SCADA, smart grid, and critical infrastructure networks are all welcome. * IMPORTANT DATES Extended submission deadline: Jan 27, 2018 (23:59 GMT) Notification: Mar 10, 2018 Camera-ready due: Mar 31, 2018 * Program Chairs: Dieter Gollmann (Hamburg University of Technology, Germany & NTU, Singapore) Jianying Zhou (SUTD, Singapore) * Further information CPSS Home: http://jianying.5gbfree.com/cpss/ Email: cpss2018@easychair.org CONTINUITY, COMPUTABILITY, CONSTRUCTIVITY: FROM LOGIC TO ALGORITHMS 2017 Second Call for Submissions (Postproceedings) Deadline for submission: 1 February 2018 * After the successful start of the new EU-MSCA-RISE project ""Computing with Infinite Data"" (CID) and the excellent Workshop CCC 2017 in Nancy (France) in June this year, we are planning to publish a collection of papers dedicated to the meeting and to the project as a Special Issue in the open-access journal LOGICAL METHODS IN COMPUTER SCIENCE. * The issue should reflect progress made in Computable Analysis and related areas, and is not restricted to work in the CID project or presented at the Workshop. * Submissions are welcome from all scientists on topics in the entire spectrum from logic to algorithms including, but not limited to: Exact real number computation, Correctness of algorithms on infinite data, Computable analysis, Complexity of real numbers, real-valued functions, etc. Effective descriptive set theory, Constructive topological foundations, Scott’s domain theory, Constructive analysis, Category-theoretic approaches to computation on infinite data, Weihrauch degrees, Randomness and computable measure theory, Other related areas. * EDITORS: Ulrich Berger (Swansea, UK) Pieter Collins (Maastricht, NL) Mathieu Hoyrup (Nancy, FR) Victor Selivanov (Novosibirsk, RUS) Dieter Spreen (Siegen, DE) Martin Ziegler (KAIST, KR) * DEADLINE FOR SUBMISSION: 1 February 2018 * If you intend to submit a paper for the special issue, please inform us by sending email to: spreen@math.uni-siegen.de by 1 January 2018 COMPUTABILITY IN EUROPE: SAILING ROUTES IN THE WORLD OF COMPUTATION (CiE 2018) Final Call for Papers ACM SIGLOG News 38 January 2018, Vol. 5, No. 1 Kiel, Germany July 30 August 3, 2018 http://cie2018.uni-kiel.de * CiE 2018 is the fourteenth conference organized by CiE (Computability in Europe), a European association of mathematicians, logicians, computer scientists, philosophers, physicists and others interested in new developments in computability and their underlying significance for the real world. * THE PROGRAMME COMMITTEE cordially invites all researchers (European and non-European) to submit their papers in all areas related to computability for presentation at the conference and inclusion in the proceedings at https://easychair.org/conferences/?conf=cie2018 Submission guidelines are available on the conference web-site. * The CONFERENCE PROCEEDINGS will be published by LNCS, Springer Verlag. * IMPORTANT DATES: Deadline for abstract submission: January 17, 2018 Deadline for article submission: February 1, 2018 Notification of acceptance: April 6, 2018 Early registration before: May 30, 2018 * TUTORIAL SPEAKERS: Pinar Heggernes (Bergen, Norway), Bakhadyr Khoussainov (Auckland, NZ) * INVITED SPEAKERS: Kousha Etessami (Edinburgh, UK) Johanna Franklin (Hempstead, US) Mai Gehrke (Nice, France) Alberto Marcone (Udine, Italy) Alexandra Silva (London, UK) Jeffrey O. Shallit (Waterloo, Canada) * SPECIAL SESSIONS: Approximation and Optimisation, Bioinformatics and Bio-inspired Computing, Computing with Imperfect Information, Continuous Computation, History and Philosophy of Computing, SAT-Solving. * WORKSHOP: Women in Computability. * Check the web-site for further details on the conference, such as organisation and grants. SPECIAL ISSUE OF IJAR ON ""DEFEASIBLE AND AMPLIATIVE REASONING"" Call for Papers * Classical reasoning is not flexible enough when directly applied to the formalization of certain nuances of decision making as done by humans. These involve different kinds of reasoning such as reasoning with uncertainty, exceptions, similarity, vagueness, incomplete or contradictory information and many others. * Everyday reasoning usually shows the two salient intertwined aspects below: Ampliative aspect: augmenting the underlying reasoning by allowing more conclusions. Defeasible aspect: curtailing the underlying reasoning by either disregarding or disallowing some conclusions that somehow ought not ACM SIGLOG News 39 January 2018, Vol. 5, No. 1 to be sanctioned. * This special issue aims at bringing together work on defeasible and ampliative reasoning from the perspective of artificial intelligence, cognitive sciences, philosophy and related disciplines in a multi-disciplinary way, thereby consolidating the mission of the DARe workshop series. * The submission url is: http://www.evise.com/evise/jrnl/IJA * When submitting your manuscript, please select ‘‘VSI:DARe special issue’’ as the article type. * If you have any enquiries, please feel free to contact us at dare.to.contact.us@gmail.com * IMPORTANT DATES Submission deadline: 15 February 2018 Notification: 1 November 2018 Publication date: 1 January 2019 * Guest editors Richard Booth, Cardiff University, UK Giovanni Casini, University of Luxembourg Szymon Klarman, Semantic Integration Ltd., UK Gilles Richard, Universite Paul Sabatier, France Ivan Varzinczak, CRIL, Univ. Artois & CNRS, France THE 17TH IEEE INTERNATIONAL CONFERENCE ON COGNITIVE INFORMATICS AND COGNITIVE COMPUTING (ICCI*CC’18) UC Berkeley, CA, USA July 15-18, 2018 http://www.ucalgary.ca/icci_cc/iccicc-18 * The IEEE ICCI*CC series is a flagship conference of its field sponsored by IEEE Computer, Computational Intelligence and SMC Societies. The theme of ICCI*CC’18 is on Cognitive Machine Learning, Brain-Inspired Systems and Cognitive Robotics. * You are welcome to submit a paper to IEEE ICCI*CC’18 or to organize a special session related to the theme of the conference. The Proceedings of ICCI*CC’18 will be published by IEEE CS Press (EI Indexed). A good rate of selected papers from the proceedings will be recommended to leading international journals and/or IEEE transactions with ISI/EI indexes. * IMPORTANT DATES Submission deadline: February 16, 2018. 25th WORKSHOP ON LOGIC, LANGUAGE, INFORMATION AND COMPUTATION (WoLLIC 2018) Call for Papers July 24th-27th, 2018, Bogota, Colombia http://wollic.org/wollic2018/ * WoLLIC is an annual international forum on inter-disciplinary research involving formal logic, computing and programming theory, and natural language and reasoning. Each meeting includes invited talks and tutorials as well as contributed papers. * Contributions are invited on all pertinent subjects, with particular ACM SIGLOG News 40 January 2018, Vol. 5, No. 1 interest in cross-disciplinary topics. Typical but not exclusive areas of interest are: foundations of computing and programming; novel computation models and paradigms; broad notions of proof and belief; proof mining, type theory, effective learnability; formal methods in software and hardware development; logical approach to natural language and reasoning; logics of programs, actions and resources; foundational aspects of information organization, search, flow, sharing, and protection; foundations of mathematics; philosophy of mathematics; philosophy of language; philosophical logic. * IMPORTANT DATES: Mar 11, 2018: Paper title and abstract deadline Mar 18, 2018: Full paper deadline Apr 15, 2018: Author notification Apr 22, 2018: Final version deadline (firm). * Further details: http://wollic.org/wollic2018/ WOMEN IN LOGIC WORKSHOP (WiL 2018) Call for Papers July 8, 2018, Oxford UK https://sites.google.com/site/womeninlogic2018/welcome/ * Affiliated with LICS (http://lics.siglog.org/lics18/) Held as part of FLoC (http://www.floc2018.org/) * We are holding the 2nd Women in Logic (WiL) workshop as a LICS 2018 associated w",SIGL,2018.0,10.1145/3183645.3183651,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
1a0cf3eba22c053ad21503e034baf3178899258a,https://www.semanticscholar.org/paper/1a0cf3eba22c053ad21503e034baf3178899258a,Design and Deployment of a Data Lake at a Pilot Plant Scale for a Smart Electropolishing Process,"In order to remain competitive and satisfy the demands of today’s customers in a timely manner, manufacturing industries are embracing the Industry 4.0 philosophy where automation is pushed beyond robotics to new technologies emerging from data science and artificial intelligence. The aim is to reduce time spent on none added value tasks and help learning from past experience in order to enhance efficiency and quality of manufacturing processes.
 Traditional industries, such as electropolishing, need to find ways to automate their, often heavily artisanal-based techniques and develop an intelligent network of machines and processes taking advantage of information and communication technology such as Big Data, IoT (Internet of Things), or Artificial Intelligence (AI). This digital transition can be realized through the application of an IIoT (Industrial Internet of Things) platform that constructs a massive, sophisticated information network of interconnected sensors, equipment, and processes known as cyber-physical systems.
 Within this network, large amounts of data (for example process bath attributes such as temperature or viscosity and part characteristics such as roughness or brightness) can be collected automatically via sensors and through user-friendly applications from manual measurements and observations. All data are uploaded automatically into a cloud-based data storage system. In order for this collected information to be useful, the data needs to be processed to allow pattern discovery and extraction of useful information regarding the system performance, probable faults in the process, and product quality. Besides others, machine learning algorithms play a key role in extracting useful information.
 Classification and processing of such massive, diverse, and rapidly arriving data sets are known to be challenging. As a result, the concept of data lake has arisen in the last decade as an appealing and cost-effective approach for companies to manage large amounts of data. It consists of a large repository of datasets designed to transform raw and unstructured data into structured, usable information to allow further processing. A data lake, organized typically in four layers (ingestion, distillation, processing, and insights layers), stores both old and near real-time data in one location for initial assessment, with comprehensive data organization, analysis, and visualization being performed only when necessary 1,2. This promotes agility by allowing data to be accessed by everyone in the company. 2
 
 In this work, a data lake is designed and implemented in conjunction with a pilot plant to demonstrate how in the electropolishing process of stainless-steel samples in an aging electrolyte, data can be collected and organized for further processing using machine learning techniques in order to optimize the process and part quality based on the data analysis results.
 References: 
 
 N. Miloslavskaya and A. Tolstoy, Procedia Comput. Sci., 88, 300–305 (2016).
 
 
 H. Fang, in 2015 IEEE International Conference on Cyber Technology in Automation, Control, and Intelligent Systems (CYBER),, p. 820–824 (2015).
 
 
 
 
 
 
 
 
 Figure 1
",ECS Meeting Abstracts,2022.0,10.1149/ma2022-01251219mtgabs,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
