doi,title,publisher,content_type,abstract,html_url,publication_title,publication_date,database,query_name,query_value
10.1109/MELECON53508.2022.9842871,A Citizen Science Approach for the Collection of Data to Train Deep Learning Models,IEEE,Conferences,"Machine learning techniques that give good predictions require a considerable amount of data, which sometimes can be a challenge to collect. In this study, citizen science is being introduced as an auxiliary technique to provide more data for the training of a deep learning network for the classification of Maltese Flora; a field which to this date lacks sufficient training data. In the first part of this study, we investigate the training of a deep learning model that makes use of a limited training dataset utilising techniques such as data augmentation, data scraping and transfer learning. This improved experimented off-the-shelf model generated a low accuracy highlighting the relevance of the initial hypothesis that citizen science is needed for the improvement of deep-learning models. In the second phase, citizen science was used as a data crowdsourcing technique through a mobile communication system. A study was conducted to determine the opinion of the public with only a small percentage showing a lack of interest in participating. Therefore, a dynamic educational application was implemented for the public exploiting Artificial Intelligence advancements to identify Maltese Flora in real-time whilst gathering images used to enhance the dataset. The deep learning model was re-trained on this dataset showing a significant increase in performance. Visualizations of the current Maltese flora distribution were also generated utilizing this data. This study demonstrated that the use of citizen science is essential for the improvement of deep learning models so that they can be employed in more widespread applications.",https://ieeexplore.ieee.org/document/9842871/,2022 IEEE 21st Mediterranean Electrotechnical Conference (MELECON),14-16 June 2022,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IC3IOT53935.2022.9767897,"A Deep study of Data science related problems, application and machine learning algorithms utilized in Data science",IEEE,Conferences,"Data Science study utilized for gathering information as data, taking out data from other systems, accumulating information, signifying and safeguarding data collected are used by organizations for marketing purposes and in high-tech implementations. Name of data science denotes a combination of databases and software engineering as well as a number of types of qualitative, quantitative entities and non-mathematical entities are furthermore necessary. Data science and its importance, components in data science included, life cycle of data science, Application of data science, various algorithms in machine learning are used with data science and implementation of machine learning algorithm in real life cases are also described. Finally, wound up with how to decode an obstacle in data Science using machine learning algorithms.",https://ieeexplore.ieee.org/document/9767897/,"2022 International Conference on Communication, Computing and Internet of Things (IC3IoT)",10-11 March 2022,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICBAIE52039.2021.9389899,An Open-Source Programming Language-Based Interactive Device: Popular Science of the Five Cereals for Children,IEEE,Conferences,"In this paper, Arduino is used in combination with switch, lighting and sound facilities to realize human-computer interaction. Arduino is an open-source electronics platform that integrates hardware and software. Given its versatility, expressiveness and operability, designers and artists are free to turn their ideas into reality on this platform, using programming languages to create high-quality interactive installations. In response to children's difficulty telling the difference between the five cereals, this paper seeks to educate children on crops in the form of human-computer interaction through the design and manufacture of a popular science-themed interactive device. Through the external hardware configuration (pressure induction, lamp and sound), children are guided to knowing and grasping the growth stages of common plants. In this way, children's perceptions of nature and crops can be constructed through a combination of technology and art.",https://ieeexplore.ieee.org/document/9389899/,"2021 IEEE 2nd International Conference on Big Data, Artificial Intelligence and Internet of Things Engineering (ICBAIE)",26-28 March 2021,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISAIAM53259.2021.00012,Application of children Artificial Intelligence science popularization books based on Augmented Reality technology,IEEE,Conferences,"Children’s books are enlightenment books for children’s education, and the educational significance of popular science books is even more important. With the development of global intelligent technology, parents pay more attention to children’s intelligent education, and books combined with science and technology become more popular. As one of the important tools in the process of children’s growth, books play an important role in children’s early education. However, most of the popular science books are still natural universe, animal world, military education and other subjects. Today, with the development of intelligent technology, it has become an inevitable thing to carry out artificial intelligence science popularization for children. Therefore, this article uses multimedia technology, uses Kudan SDK to develop AR applications in Unity, explores the application of AR technology in popular science books, and uses artificial intelligence as the theme for creation. It mainly includes the cognitive analysis of artificial intelligence, ar software system design and the design exploration of AR and books for 7-11 year olds, so that children can understand and learn the relevant knowledge of AI in the technical environment.",https://ieeexplore.ieee.org/document/9516578/,2021 International Symposium on Artificial Intelligence and its Application on Media (ISAIAM),21-23 May 2021,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IEEECONF53024.2021.9733750,Augmented Reality Applications in the Training of Computer Science Professionals,IEEE,Conferences,"Various technological resources support different levels of the educational process. This proposal describes an augmented reality application for a data architecture course, a fundamental subject for bachelor degree students studying computer information management at public universities and computer engineering in higher education institutions for technology. We followed the methodology of educational software. The application was developed in Unity, considering five concordant themes in the curricula of both universities (linear, non-linear, recursion, sorting, and search). The objective was to analyze the incorporation of the educational resource. For this, three instruments were used: (1) to examine the students&#x0027; knowledge level about the subject before and after the resource intervention, (2) a rubric to understand their perception of the quality of mobile educational applications, and (3) the technology acceptance model, which allowed identifying attitudes towards the information technology system based on two previously established variables: perceived usefulness and perceived ease of use. The results are encouraging, verifying that technological tools contribute to the training of computer scientists.",https://ieeexplore.ieee.org/document/9733750/,2021 Machine Learning-Driven Digital Technologies for Educational Innovation Workshop,15-17 Dec. 2021,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/BigData.2016.7840859,Building a research data science platform from industrial machines,IEEE,Conferences,"Data Science research has a long history in academia which spans from large-scale data management, to data mining and data analysis using technologies from database management systems (DBMS's). While traditional HPC offers tools on leveraging existing technologies with data processing needs, the large volume of data and the speed of data generation pose significant challenges. Using the Hadoop platform and tools built on top of it drew immense interest from academia after it gained success in industry. Georgia Institute of Technology received a donation of 200 compute nodes from Yahoo. Turning these industrial machines into a research Data Science Platform (DSP) poses unique challenges, such as: nontrivial hardware design decisions, configuration tool choices, node integration into existing HPC infrastructure, partitioning resource to meet different application needs, software stack choices, etc. We have 40 nodes up and running, 24 running as a Hadoop and Spark cluster, 12 running as a HBase and OpenTSDB cluster, the others running as service nodes. We successfully tested it against Spark Machine Learning algorithms using a 88GB image dataset, Spark DataFrame and GraphFrame with a Wikipedia dataset, and Hadoop MapReduce wordcount on a 300GB dataset. The OpenTSDB cluster is for real-time time series data ingestion and storage for sensor data. We are working on bringing up more nodes. We share our first-hand experience gained in our journey, which we believe will benefit and inspire other academic institutions.",https://ieeexplore.ieee.org/document/7840859/,2016 IEEE International Conference on Big Data (Big Data),5-8 Dec. 2016,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICoDT252288.2021.9441481,Classy AA-NECTAR: Personalized Ubiquitous E-Learning Recommender System with Ontology and Data Science Techniques,IEEE,Conferences,"Learners have different learning styles each tailored to their own personality. Incompatibility of learning and teaching style is inconvenient. This paper integrates learner behavior modeling, academic web crawling and content retrieval using state of the art technology. This research work aims to propose a personalized ubiquitous learning model to identify learner learning styles and deploy type of content that is corresponding to the learner's learning style. Felder-Solomon model is one of the models being used for the learner profiling. This gives ease not only to the learners but the pedagogical instructors as well for not making different type of content. Real time monitoring makes the self-adaptive system learn through the learner's gestures and self-adjusts autonomously. Learners' aptitude increases, saving time and inconvenience. This will give an easy access to certifying organizations to get more capable skill oriented people.",https://ieeexplore.ieee.org/document/9441481/,2021 International Conference on Digital Futures and Transformative Technologies (ICoDT2),20-21 May 2021,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/COMPSAC48688.2020.00026,DSLE: A Smart Platform for Designing Data Science Competitions,IEEE,Conferences,"During the last years an increasing number of university-level and post-graduation courses on Data Science have been offered. Practices and assessments need specific learning environments where learners could play with data samples and run machine learning and data mining algorithms. To foster learner engagement many closed-and open-source platforms support the design of data science competitions. However, they show limitations on the ability to handle private data, customize the analytics and evaluation processes, and visualize learners' activities and outcomes. This paper presents Data Science Lab Environment (DSLE, in short), a new open-source platform to design and monitor data science competitions. DSLE offers a easily configurable interface to share training and test data, design group works or individual sessions, evaluate the competition runs according to customizable metrics, manage public and private leaderboards, monitor participants' activities and their progress over time. The paper describes also a real experience of usage of DSLE in the context of a 1st-year M.Sc. course, which has involved around 160 students.",https://ieeexplore.ieee.org/document/9202557/,"2020 IEEE 44th Annual Computers, Software, and Applications Conference (COMPSAC)",13-17 July 2020,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/DSAA.2015.7344825,Data science foundry for MOOCs,IEEE,Conferences,"In this paper, we present the concept of data science foundry for data from Massive Open Online Courses. In the foundry we present a series of software modules that transform the data into different representations. Ultimately, each online learner is represented using a set of variables that capture his/her online behavior. These variables are captured longitudinally over an interval. Using this representation we then build a predictive analytics stack that is able to predict online learners behavior as the course progresses in real time. To demonstrate the efficacy of the foundry, we attempt to solve an important prediction problem for Massive Open Online Courses (MOOCs): who is likely to stopout? Across a multitude of courses, with our complex per-student behavioral variables, we achieve a predictive accuracy of 0.7 AUCROC and higher for a one-week-ahead prediction problem. For a two-to-three-weeks-ahead prediction problem, we are able to achieve 0.6 AUCROC. We validate, via transfer learning, that these predictive models can be used in real time. We also demonstrate that we can protect the models using privacy-preserving mechanisms without losing any predictive accuracy.",https://ieeexplore.ieee.org/document/7344825/,2015 IEEE International Conference on Data Science and Advanced Analytics (DSAA),19-21 Oct. 2015,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/GCCE.2017.8229475,Deductively verifying embedded software in the era of artificial intelligence = machine learning + software science,IEEE,Conferences,"Today Google collects big data, and develops new business using both cloud computing such as search engine and artificial intelligence such as machine learning. The study of artificial intelligence mainly on deep learning is prosperous and is expected to make smart life now and in the future. The mainstream of the present artificial intelligence is machine learning, and the foundation is inductive inference. On the other hand, artificial intelligence from old days is deductive inference, and the deductive inference is the foundation of software science. In this paper, from the viewpoint of “artificial intelligence = machine learning + software science”, we grasp computer technologies. We pay attention to software science, especially deductive inference of embedded systems, and explain deductive verifications for guaranteeing the reliability of embedded systems.",https://ieeexplore.ieee.org/document/8229475/,2017 IEEE 6th Global Conference on Consumer Electronics (GCCE),24-27 Oct. 2017,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICAICA52286.2021.9498218,Eco-climate Intelligent Monitoring System of an Agricultural Science-And-Technology Park Based on Internet of Things,IEEE,Conferences,"An agricultural science and technology park (ASTP) is a new mode of agricultural development supported by science and technology. At present, research focuses on diseases and insect pests, intelligent irrigation, greenhouse environment-specific crop monitoring, and other individual issues, in which the utilization efficiency of agricultural information technology is low. The purpose of this study is to fully grasp the change in the ecological climate of ASTPs and improve their intelligence level. This study, based on the Internet of Things of agricultural information transmission and control, designs a greenhouse climatic and agricultural science and technology park and unifies the field intelligent control and scientific research and production integration management platforms. Standardized planting and a field monitoring center for large data visualization analysis for scientific research and production are used to achieve real-time online mining analysis, monitoring, and early warning and decision support services. A software terminal is used to prompt on-site management measures and early warning according to weather changes. After one growing season, the labor force was reduced by 20%, the use of pesticides by 20%, and the use of water resources and fertilizers decreased. Practical application shows that the system runs stably and meets the demands of ecological environment monitoring in ASTPs.",https://ieeexplore.ieee.org/document/9498218/,2021 IEEE International Conference on Artificial Intelligence and Computer Applications (ICAICA),28-30 June 2021,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCI-CC.2014.6921432,From information revolution to intelligence revolution: Big data science vs. intelligence science,IEEE,Conferences,"The hierarchy of human knowledge is categorized at the levels of data, information, knowledge, and intelligence. For instance, given an AND-gate with 1,000-input pins, it may be described very much differently at various levels of perceptions in the knowledge hierarchy. At the data level on the bottom, it represents a 21,000 state space, known as `big data' in recent terms, which appears to be a big issue in engineering. However, at the information level, it just represents 1,000 bit information that is equivalent to the numbers of inputs. Further, at the knowledge level, it expresses only two rules that if all inputs are one, the output is one; and if any input is zero, the output is zero. Ultimately, at the intelligence level, it is simply an instance of the logical model of an AND-gate with arbitrary inputs. This problem reveals that human intelligence and wisdom are an extremely efficient and a fast convergent induction mechanism for knowledge and wisdom elicitation and abstraction where data are merely factual materials and arbitrary instances in the almost infinite state space of the real world. Although data and information processing have been relatively well studied, the nature, theories, and suitable mathematics underpinning knowledge and intelligence are yet to be systematically studied in cognitive informatics and cognitive computing. This will leads to a new era of human intelligence revolution following the industrial, computational, and information revolutions. This is also in accordance with the driving force of the hierarchical human needs from low-level material requirements to high-level ones such as knowledge, wisdom, and intelligence. The trend to the emerging intelligent revolution is to meet the ultimate human needs. The basic approach to intelligent revolution is to invent and embody cognitive computers, cognitive robots, and cognitive systems that extend human memory capacity, learning ability, wisdom, and creativity. Via intelligence revolution, an interconnected cognitive intelligent Internet will enable ordinary people to access highly intelligent systems created based on the latest development of human knowledge and wisdom. Highly professional systems may help people to solve typical everyday problems. Towards these objectives, the latest advances in abstract intelligence and intelligence science investigated in cognitive informatics and cognitive computing are well positioned at the center of intelligence revolution. A wide range of applications of cognitive computers have been developing in ICIC [http://www.ucalgary.ca/icic/] such as, inter alia, cognitive computers, cognitive robots, cognitive learning engines, cognitive Internet, cognitive agents, cognitive search engines, cognitive translators, cognitive control systems, cognitive communications systems, and cognitive automobiles.",https://ieeexplore.ieee.org/document/6921432/,2014 IEEE 13th International Conference on Cognitive Informatics and Cognitive Computing,18-20 Aug. 2014,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CSEI50228.2020.9142534,How to Use Stock Data for Data Science Education: A Simulated Trading Platform in Classroom,IEEE,Conferences,"The research hereby presents an innovate practice to enhance data science education by integrating an in-house-developed stock trading platform. Students of Data Science area usually face the difficulty of understanding the complex of real-time data and sophisticated statistical indicators and models. Another difficulty is students are easy to lost interests and patience during the process of learning programming and analysis process, such as R-language. Stock market, as a huge data source and an important data disciplinary, is comparatively easy to attract student attentions. Thus, we developed an intuitive trading platform for education purpose. The platform contains three components: an exploring window, a control windows and a report window. The exploring window shows current and historical stock price trends and related indicators. Several representative stocks from different sectors can be picked and specific time frames can be assigned. The control window allows students to develop their own trading strategies. A trading strategy can be created by either intuitive way, or through single or combinations of indicators, or be built generically though plug-in R-programing module. The report window demonstrates the expected return of a stock in a specific time frame through a specific strategy. A more comprehensive report with detailed transaction information is also provided for back-testing purpose. Students are involved into the development of the software and get experience for R-programming. Preliminary version of the product has been tested and surveyed in a data science classroom. About ten junior Data Science students have practiced and provided feedback. The positive survey results show the feasibility of the approach. In the future, artificial intelligent component will be integrated into the platform.",https://ieeexplore.ieee.org/document/9142534/,2020 IEEE 2nd International Conference on Computer Science and Educational Informatization (CSEI),12-14 June 2020,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AIKE.2018.00015,Machine Learning Models to Enhance the Science of Cognitive Autonomy,IEEE,Conferences,"Intelligent Autonomous Systems (IAS) are highly cognitive, reflective, multitask-able, and effective in knowledge discovery. Examples of IAS include software systems that are capable of automatic reconfiguration, autonomous vehicles, network of sensors with reconfigurable sensory platforms, and an unmanned aerial vehicle (UAV) respecting privacy by deciding to turn off its camera when pointing inside a private residence. Research is needed to build systems that can monitor their environment and interactions, learn their capabilities and limitations, and adapt to meet the mission objectives with limited or no human intervention. The systems should be fail-safe and should allow for graceful degradations while continuing to meet the mission objectives. In this paper, we provide an overview of our proposed new methodologies and workflows, and survey the existing approaches and new ones that can advance the science of autonomy in smart systems through enhancements in real-time control, auto-reconfigurability, monitoring, adaptability, and trust. This paper also provides the theoretical framework behind IAS.",https://ieeexplore.ieee.org/document/8527447/,2018 IEEE First International Conference on Artificial Intelligence and Knowledge Engineering (AIKE),26-28 Sept. 2018,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/COGINF.2006.365669,On Intelligence Science and Recent Progresses,IEEE,Conferences,"Summary form only given. Intelligence science is a cross-discipline that dedicates to joint research on basic theory and technology of intelligence by brain science, cognitive science, artificial intelligence and others. Brain science explores the essence of brain, research on the principle and model of natural intelligence in molecular, cell and behavior level. Cognitive science studies human mental activity, such as perception, learning, memory, thinking, consciousness etc. In order to implement machine intelligence, Artificial intelligence attempts simulation, extension and expansion of human intelligence using artificial methodology and technology. The above three disciplines work together to explore new concept, new theory, new methodology. It will be successful and create a brilliant future in the 21 century. Brain science points out that perceptive lobes have special function separately, the occipital lobe processes the visual information, the temporal lobe processes auditory information, the parietal lobe processes the information from the somatic sensors. All of three lobes deal with information perceived from the physical world. Each lobe is covered with cortex where the bodies of neurons are located. Cortex consists of primary, intermediate and advanced areas at least. Information is processed in the primary area first, then is passed to intermediate and advanced areas. Comparing with computer system, the brain is the same as hardware and the mind looks like software. Most work in cognitive science assumes that the mind has mental representations analogous to computer data structures, and computational procedures similar to computational algorithms. Connectionists have proposed novel ideas to use neurons and their connections as inspirations for data structures, and neuron firing and spreading activation as inspirations for algorithms. Cognitive science then works with a complex 3-way analogy among the mind, the brain, and computers. Mind, brain, and computation can each be used to suggest new ideas about the others. There is no single computational model of mind, since different kinds of computers and programming approaches suggest different ways in which the mind might work. The mind contains perception, rational, consciousness and emotion. The long-term scientific goal of artificial intelligence is human-level intelligence. In this lecture, we will discuss basic research topics related to intelligence science, such as learning, memory, thought, language, consciousness etc. We also report the recent progresses containing: visual perception; introspective learning; linguistic cognition; consciousness model; and platform of agent-grid intelligence",https://ieeexplore.ieee.org/document/4216384/,2006 5th IEEE International Conference on Cognitive Informatics,17-19 July 2006,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IRI.2009.5211603,Ontology-based information model development for science information reuse and integration,IEEE,Conferences,"Scientific digital libraries serve complex and evolving research communities. Justifications for the development of scientific digital libraries include the desire to preserve science data and the promises of information interconnectedness, correlative science, and system interoperability. Shared ontologies are fundamental to fulfilling these promises. We present a tool framework, a set of principles, and a real world case study where shared ontologies are used to develop and manage science information models and subsequently guide the implementation of scientific digital libraries. The tool framework, based on an ontology modeling tool, has been used to formalize legacy information models as well as design new models. Within this framework, the information model remains relevant within changing domains and thereby promotes the interoperability, interconnectedness, and correlation desired by scientists.",https://ieeexplore.ieee.org/document/5211603/,2009 IEEE International Conference on Information Reuse & Integration,10-12 Aug. 2009,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/FIE.2018.8658596,Personalizing Computer Science Education by Leveraging Multimodal Learning Analytics,IEEE,Conferences,"This Research Full Paper implements a framework that harness sources of programming learning analytics on three computer programming courses a Higher Education Institution. The platform, called PredictCS, automatically detects lower-performing or “at-risk” students in programming courses and automatically and adaptively sends them feedback. This system has been progressively adopted at the classroom level to improve personalized learning. A visual analytics dashboard is developed and accessible to Faculty. This contains information about the models deployed and insights extracted from student's data. By leveraging historical student data we built predictive models using student characteristics, prior academic history, logged interactions between students and online resources, and students' progress in programming laboratory work. Predictions were generated every week during the semester's classes. In addition, during the second half of the semester, students who opted-in received pseudo real-time personalised feedback. Notifications were personalised based on students' predicted performance on the course and included a programming suggestion from a top-student in the class if any programs submitted had failed to meet the specified criteria. As a result, this helped students who corrected their programs to learn more and reduced the gap between lower and higher-performing students.",https://ieeexplore.ieee.org/document/8658596/,2018 IEEE Frontiers in Education Conference (FIE),3-6 Oct. 2018,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CSEET49119.2020.9206229,Project-Based Learning in a Machine Learning Course with Differentiated Industrial Projects for Various Computer Science Master Programs,IEEE,Conferences,"Graduating computer science students with skills sufficient for industrial needs is a priority in higher education teaching. Project-based approaches are promising to develop practical and social skills, needed to address real-world problems in teams. However, rapid technological transition makes an initial training of contemporary methods challenging. This affects the currently much-discussed machine learning domain as well. The study at hand describes a re-framed teaching approach for a machine learning course, offered to various computer science master programs. Project-based learning is introduced with differentiated projects provided by industrial partners that address the diverse study programs. Course attendees are supported with manuals, tools, and tutoring, passing through the Cross Industry Standard Process for Data Mining (CRISP-DM). Observations made during two iterations are reported, accompanied by a first empiric evaluation of student experiences.",https://ieeexplore.ieee.org/document/9206229/,2020 IEEE 32nd Conference on Software Engineering Education and Training (CSEE&T),9-12 Nov. 2020,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/WORKS49585.2019.00006,Provenance Data in the Machine Learning Lifecycle in Computational Science and Engineering,IEEE,Conferences,"Machine Learning (ML) has become essential in several industries. In Computational Science and Engineering (CSE), the complexity of the ML lifecycle comes from the large variety of data, scientists' expertise, tools, and workflows. If data are not tracked properly during the lifecycle, it becomes unfeasible to recreate a ML model from scratch or to explain to stackholders how it was created. The main limitation of provenance tracking solutions is that they cannot cope with provenance capture and integration of domain and ML data processed in the multiple workflows in the lifecycle, while keeping the provenance capture overhead low. To handle this problem, in this paper we contribute with a detailed characterization of provenance data in the ML lifecycle in CSE; a new provenance data representation, called PROV-ML, built on top of W3C PROV and ML Schema; and extensions to a system that tracks provenance from multiple workflows to address the characteristics of ML and CSE, and to allow for provenance queries with a standard vocabulary. We show a practical use in a real case in the O&G industry, along with its evaluation using 239,616 CUDA cores in parallel.",https://ieeexplore.ieee.org/document/8943505/,2019 IEEE/ACM Workflows in Support of Large-Scale Science (WORKS),17-17 Nov. 2019,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCSA.2007.90,The 2007 International Conference Computational Science and its Applications - Title,IEEE,Conferences,The following topics are dealt with: computational intelligence; high performance technical computing; information systems; Web based learning; component based software engineering; software process model; computational geometry; distributed computing; digital content security; data storage devices; data storage systems; intelligent design technology; intelligent data mining; information services; information technologies; mobile multimedia networks; pattern recognition; ubiquitous computing; computer graphics; computational science; wireless sensor networks; virtual reality; optimization; mobile communication; molecular simulations.,https://ieeexplore.ieee.org/document/4301111/,2007 International Conference on Computational Science and its Applications (ICCSA 2007),26-29 Aug. 2007,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/BigData50022.2020.9378128,~PB&J~ - Easy Automation of Data Science/Machine Learning Workflows,IEEE,Conferences,"The ability to process large amounts of data efficiently is a must in modern Big Data, Data Science and Machine Learning (ML). Easy and succinct workflow expression with highly efficient scaling is key. Existing systems such as Apache Spark and Beam were created for this type of work but have steep learning curve and tremendous operating complexity. Having a simpler tool with less overhead that is well-matched to the typical workflow definition and execution requirements of data science/ML projects will be beneficial.We propose a new framework PB&J that enables succinct definition and data-parallel scale-out execution of workflows with ease. PB&J is easy to learn as it builds on the native Unix shell, with the addition of just a few operators. PB&J supports Maximal Parallelism with its ability to do true pipelining, and fault recovery with Minimal Redo. It is well-matched to the typical data science/ML processing requirements by leveraging the existing command line executables and Python modules. We illustrate the features and strengths of PB&J with real-life Deep Learning data processing workflows. We compare it to existing frameworks such as Apache Spark, Apache Beam, Swift/T and Apache Airflow in terms of ease of authoring, efficiency, scalability and fault recovery.",https://ieeexplore.ieee.org/document/9378128/,2020 IEEE International Conference on Big Data (Big Data),10-13 Dec. 2020,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1093/comjnl/bxy082,Algorithmic Government: Automating Public Services and Supporting Civil Servants in using Data Science Technologies,OUP,Journals,"The data science technologies of artificial intelligence (AI), Internet of Things (IoT), big data and behavioral/predictive analytics, and blockchain are poised to revolutionize government and create a new generation of GovTech start-ups. The impact from the ‘smartification’ of public services and the national infrastructure will be much more significant in comparison to any other sector given government's function and importance to every institution and individual. Potential GovTech systems include Chatbots and intelligent assistants for public engagement, Robo-advisors to support civil servants, real-time management of the national infrastructure using IoT and blockchain, automated compliance/regulation, public records securely stored in blockchain distributed ledgers, online judicial and dispute resolution systems, and laws/statutes encoded as blockchain smart contracts. Government is potentially the major ‘client’ and also ‘public champion’ for these new data technologies. This review paper uses our simple taxonomy of government services to provide an overview of data science automation being deployed by governments world-wide. The goal of this review paper is to encourage the Computer Science community to engage with government to develop these new systems to transform public services and support the work of civil servants.",https://ieeexplore.ieee.org/document/8852885/,The Computer Journal,March 2019,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ACCESS.2019.2929800,EXTES: An Execution-Time Estimation Scheme for Efficient Computational Science and Engineering Simulation via Machine Learning,IEEE,Journals,"In recent years, computational science and engineering (CSE) simulations using high-performance computing resources are actively exploited to solve complex domain-specific problems. Thanks to the remarkable advance of IT technology, the CSE community is challenging more complex and difficult problems than ever before, by running these simulations online. In this regard, we often witness that 1) online simulation users suffer from knowing little about the estimated termination time of their launched simulations and 2) the limited computing resources are squandered by wrong input that leads the simulations to run forever. To address such issues, we propose a novel execution time estimation scheme, termed EXTES, using machine learning techniques for more efficient online CSE simulations. With a large amount of existing provenance data, the EXTES scheme trains a suite of models rooted from classification, regression, and a hybrid of the two and utilize these models to estimate the execution time for specified input parameters for simulations. In the experiments on real simulation data, our proposed models achieved about 73% accuracy on average in execution time estimation across 16 simulation programs taken from a variety of CSE fields. In the meantime, the overhead incurred by the training and estimation is almost negligible.",https://ieeexplore.ieee.org/document/8766089/,IEEE Access,2019,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ACCESS.2019.2899985,Using Machine Learning Ensemble Methods to Predict Execution Time of e-Science Workflows in Heterogeneous Distributed Systems,IEEE,Journals,"Effective planning and optimized execution of the e-Science workflows in distributed systems, such as the Grid, need predictions of execution times of the workflows. However, predicting the execution times of e-Science workflows in heterogeneous distributed systems is a challenging job due to the complex structure of workflows, variations due to input problem-sizes, and heterogeneous and dynamic nature of the shared resources. To this end, we propose two novel workflow execution time-prediction methods based on the machine learning ensemble models. In this paper, we showcase our approach for two different real Grid environments. Our approach can effectively predict the execution time of the scientific workflow applications in the Grid for various problem sizes, Grid sites, and runtime environments. We characterized the workflow performance in the Grid using the attributes that define structure of workflow as well as the execution environment. Contrary to common ensembles, our ensemble systems employed three strong learners, which balance the weaknesses of each other by their strengths to model the workflow execution times. The proposed methods have been thoroughly evaluated for three real-world e-science workflow applications. The experimental results demonstrated that our proposed multi-model ensemble models can significantly decrease the prediction error (by 50%, on average) as compared with methods based on the radial basis function neural network, local learning, and performance templates. The proposed methods can also be applied with similar effectiveness and without any major modification for other heterogeneous distributed environments, such as the Cloud.",https://ieeexplore.ieee.org/document/8643927/,IEEE Access,2019,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/TITB.2004.826724,Using XML technology for the ontology-based semantic integration of life science databases,IEEE,Journals,"Several hundred internet accessible life science databases with constantly growing contents and varying areas of specialization are publicly available via the internet. Database integration, consequently, is a fundamental prerequisite to be able to answer complex biological questions. Due to the presence of syntactic, schematic, and semantic heterogeneities, large scale database integration at present takes considerable efforts. As there is a growing apprehension of extensible markup language (XML) as a means for data exchange in the life sciences, this article focuses on the impact of XML technology on database integration in this area. In detail, a general architecture for ontology-driven data integration based on XML technology is introduced, which overcomes some of the traditional problems in this area. As a proof of concept, a prototypical implementation of this architecture based on a native XML database and an expert system shell is described for the realization of a real world integration scenario.",https://ieeexplore.ieee.org/document/1303558/,IEEE Transactions on Information Technology in Biomedicine,June 2004,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SKG.2011.31,5W1H-based Conceptual Modeling Framework for Domain Ontology and Its Application on STPO,IEEE,Conferences,"It is laborious and time consuming to build domain ontology, which models a specific domain and specifies the concepts in a particular subject. In this paper, a 5W1H-based conceptual modeling framework for domain ontology is proposed, which is used to analysis domain concepts and relations from six aspects including Who, When, Where, What, Why and How. The definition of domain ontology, domain concept and domain relation are also presented. According to the framework, the conceptual model of Science & Technology Project Ontology (STPO) in science and technology domain is designed, in which main concepts and relations within the STPO are clearly described. From the analysis, the 5W1H conceptual modeling framework can be mapped to the class model in Object-Oriented method, which is used to model things in real world. Application shows that the framework is effective to model domain knowledge and scalable to business changes and user requirements.",https://ieeexplore.ieee.org/document/6088118/,"2011 Seventh International Conference on Semantics, Knowledge and Grids",24-26 Oct. 2011,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/COGINF.2007.4341895,A Cognitive Psychology Approach for Balancing Elicitation Goals,IEEE,Conferences,"The difference between success and failure in a project depends on the initial identification and construction of goals, which are the mechanism of identifying software requirements. The result of applying goal-oriented methods, which are top-down requirements analysis, is in most cases a goal graph. Goal graphs are implemented from customers needs as initial goals to requirements specifications as final goals. Their analysis includes negotiation as an important issue, which becomes extremely difficult, as clients often cannot know exactly what they need. To overcome this situation, aiming at improving stakeholder's negotiation, we reduce the gap of misunderstanding between them by the use of cognitive science. The objective of this paper is to introduce a cognitive approach to help in the solution of discordances among stakeholders when applying goal- oriented methods. We depict our proposal with a real case study.",https://ieeexplore.ieee.org/document/4341895/,6th IEEE International Conference on Cognitive Informatics,6-8 Aug. 2007,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISORC49007.2020.00030,A Cross-Layer Review of Deep Learning Frameworks to Ease Their Optimization and Reuse,IEEE,Conferences,"Machine learning and especially Deep Learning (DL) approaches are at the heart of many domains, from computer vision and speech processing to predicting trajectories in autonomous driving and data science. Those approaches mainly build upon Neural Networks (NNs), which are compute-intensive in nature. A plethora of frameworks, libraries and platforms have been deployed for the implementation of those NNs, but end users often lack guidance on what frameworks, platforms and libraries to use to obtain the best implementation for their particular needs. This paper analyzes the DL ecosystem providing a structured view of some of the main frameworks, platforms and libraries for DL implementation. We show how those DL applications build ultimately on some form of linear algebra operations such as matrix multiplication, vector addition, dot product and the like. This analysis allows understanding how optimizations of specific linear algebra functions for specific platforms can be effectively leveraged to maximize specific targets (e.g. performance or power-efficiency) at application level reusing components across frameworks and domains.",https://ieeexplore.ieee.org/document/9112939/,2020 IEEE 23rd International Symposium on Real-Time Distributed Computing (ISORC),19-21 May 2020,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICAIIC51459.2021.9415186,A Data Augmented Bayesian Network for Node Failure Prediction in Optical Networks,IEEE,Conferences,"Failures in optical network backbone can cause significant interruption in internet data traffic. Hence, it is very important to reduce such network outages. Prediction of such failures would be a step forward to avoid such disruption of internet services for users as well as operators. Several research proposals are available in the literature which are applications of data science and machine learning techniques. Most of the techniques rely on significant amount of real time data collection. Network devices are assumed to be equipped to collect data and these are then analysed by different algorithms to predict failures. Every network element which is already deployed in the field may not have these data gathering or analysis techniques designed into them initially. However, such mechanisms become necessary later when they are already deployed in the field. This paper proposes a Bayesian network based failure prediction of network nodes, g., routers etc., using very basic information from the log files of the devices and applying power law based data augmentation to complement for scarce real time information. Numerical results show that network node failure prediction can be performed with high accuracy using the proposed mechanism.",https://ieeexplore.ieee.org/document/9415186/,2021 International Conference on Artificial Intelligence in Information and Communication (ICAIIC),13-16 April 2021,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/HPCC/SmartCity/DSS.2018.00201,A Deep Learning Approach to Sensory Navigation Device for Blind Guidance,IEEE,Conferences,"Sensory navigation device is an important trend in the field of machine learning and data science. Nowadays, more and more sensory navigation devices are built for blind people. The core of such sensory navigation devices for blind people usually is implemented by an Image Recognition Method. To build an image recognition model, many tools and online machine learning platforms are proposed. However, these tools or platforms are not able to completely satisfy the requirements for sensory navigation device. To build a sensory navigation device with satisfying requirements for blind people, an ability of reducing the cost of model training and a capability of user-centric image recognition are the two main issues. Therefore, to address the above issues, we propose a novel approach, namely, DLSNF (Deep-Learning-based Sensory Navigation Framework). Our proposed DLSNF is built based on the YOLO architecture to deal with the reducing cost of model training and NVIDIA Jetson TX2 to take the user-centric image recognition into account. Based on our proposed DLSNF, the real-time image recognition can be trained well and conduct a sensory navigation to help blind people. At the same time, the train model is embedded in NVIDIA Jetson TX2 which is the fastest, most power-efficient embedded AI computing device. For the experiments, we evaluated our proposed DLSNF with a real-world dataset consisting of 4,570 images collected by part-time workers. The extensive experimental results show that our proposed DLSNF more effectively and efficiently beyond the existing baselines.",https://ieeexplore.ieee.org/document/8622939/,2018 IEEE 20th International Conference on High Performance Computing and Communications; IEEE 16th International Conference on Smart City; IEEE 4th International Conference on Data Science and Systems (HPCC/SmartCity/DSS),28-30 June 2018,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IDAP.2018.8620736,A Framework Based on Compiler Design Techniques for Programming Learning Environments,IEEE,Conferences,"It has been observed that the studies for starting programming learning at an early age have increased in recent years. Programming that enhances the high level cognitive skills of individuals is seen as a competency that must be acquired for everyone nowadays. In current programming learning environments, programming is usually done using visual components. However, it is emphasized that there is little contribution to the transition to actual programming languages by programming with visual components in the studies carried out. For this reason, a new programming learning environment has been developed in this study in which everyone can learn programming. To facilitate programming learning and contribute to the transition of actual programming languages, a new programming language is described which is simple syntax and similar to real programming languages. This language is controlled in terms of meaning and grammar with lexical and syntax analysis steps. The specified compiler design techniques are implemented using finite state machines. Analysis of the created programming language is performed by regular expressions. Instead of providing a free workspace for the user, an environment with quests covering programming concepts is presented. This quest based environment aims at the successful learning process by guiding to the user through feedbacks. The programming learning environment has been developed as an open source software by using computer science and engineering techniques and it is a framework for researchers seeking to develop a similar environment.",https://ieeexplore.ieee.org/document/8620736/,2018 International Conference on Artificial Intelligence and Data Processing (IDAP),28-30 Sept. 2018,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICITE50838.2020.9231486,A Genetic Algorithm-based AutoML Approach for Large-scale Traffic Speed Prediction,IEEE,Conferences,"With the continuous innovation of computer science as well as the big data acquisition technology, machine learning (ML), developed as a state-of-art framework, now has been comprehensively applied in speed prediction tasks. However, ML methods usually require intensive hyper-parameter tuning, which hinders the practical deployment of ML models. In view of this, this paper proposes an automated machine learning (AutoML) framework for speed prediction, which enables the prediction work to be accomplished in a much more timesaving and convenient way as well as in high prediction accuracy. The proposed framework utilizes the Genetic Algorithm (GA) following its four major procedures: Genome coding, Crossover, Mutation and Selection to automatically search for the optimal neural network architectures and hyperparameters. The proposed framework is examined on a real-world large-scale dataset in the city of Berlin, Germany. The experimental results demonstrate that the proposed method outperforms other benchmarking methods by a significant margin. Sensitivity analysis is also conducted to show the robustness of the proposed method. This study demonstrates the great penitential of using AutoML in traffic speed prediction and other related transportation applications.",https://ieeexplore.ieee.org/document/9231486/,2020 IEEE 5th International Conference on Intelligent Transportation Engineering (ICITE),11-13 Sept. 2020,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SYNASC.2009.12,A Hierarchy of Tractable Subclasses for SAT and Counting SAT Problems,IEEE,Conferences,"Finding subclasses of formulae for which the SAT problem can be solved in polynomial time has been an important problem in computer science. We present a new hierarchy of propositional formulæ subclasses for which the SAT and counting SAT problems can be solved in polynomial time. Our tractable subclasses are those propositional formulae in conjunctive normal form where any set of k + 1 clauses are related, i.e., there exists at least one literal in one clause that appears negated in another clause of the considered set of k + 1 clauses. We say this subclass of formulæ is of rank k and it is different from previously known subclasses that are solvable in polynomial time. This is an improvement over the SAT Dichotomy Theorem and the counting SAT Dichotomy Theorem, since our subclass can be moved out from the ¿P-complete class to the P class. The membership problem for this new subclass can be solved in O(n·lk+1), where n, l and k are the number of variables, clauses and the rank (1 ¿ k ¿ l - 1), respectively. We give an efficient algorithm to approximate the number of assignments for any arbitrary conjunctive normal form propositional formula by an upper bound.",https://ieeexplore.ieee.org/document/5460868/,2009 11th International Symposium on Symbolic and Numeric Algorithms for Scientific Computing,26-29 Sept. 2009,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SMC.2019.8914223,A Julia Package for manipulating Brain-Computer Interface Data in the Manifold of Positive Definite Matrices,IEEE,Conferences,"The Riemannian geometry of positive definite matrices yields state-of-the-art classification accuracy for brain-computer interface (BCI) data. The use of this framework is steadily increasing in the BCI community, sustained by its excellent classification accuracy and ability to operate transfer learning. Currently, open-source code libraries exist for the Matlab and Python programming language. Julia is a young open-source cross-platform language specifically conceived for scientific computing, which is rapidly gaining momentum in the data science community thanks to its efficiency and compatibility with the best available computing protocols. By means of this article we present and release a state-of-the-art open-source Julia package for the Riemannian geometry of positive definite matrices, named PosDefManifold. It supports nine metrics for the manifold of both real and complex positive definite matrices and includes all fundamental tools for manipulating data in them.",https://ieeexplore.ieee.org/document/8914223/,"2019 IEEE International Conference on Systems, Man and Cybernetics (SMC)",6-9 Oct. 2019,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CESA.2006.4281933,A Knowledge-Based Approach for Semantic Service Composition,IEEE,Conferences,"The successful application of grid and Web service technologies to real-world problems, such as e-Science, requires not only the development of a common vocabulary and meta-data framework as the basis for inter-agent communication and service integration but also the access and use of a rich repository of domain-specific knowledge for problem solving. Both requirements are met by the respective outcomes of ontological and knowledge engineering initiatives. In this paper we discuss a novel, knowledge-based approach to resource synthesis (service composition), which draws on the functionality of semantic Web services to represent and expose available resources. The approach we use exploits domain knowledge to guide the service composition process and provide advice on service selection and instantiation. The approach has been implemented in a prototype workflow construction environment that supports the runtime recommendation of a service solution, service discovery via semantic service descriptions, and knowledge-based configuration of selected services. The use of knowledge provides a basis for full automation of service composition via conventional planning algorithms. Workflows produced by this system can be executed through a domain-specific direct mapping mechanism or via a more fluid approach such as WSDL- based service grounding. The approach and prototype have been used to demonstrate practical benefits in the context of the Geodise initiative.",https://ieeexplore.ieee.org/document/4281933/,"The Proceedings of the Multiconference on ""Computational Engineering in Systems Applications""",4-6 Oct. 2006,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/GLOBECOM46510.2021.9685807,A Machine Learning Approach for Rate Prediction in Multicast File-stream Distribution Networks,IEEE,Conferences,"Large-volume scientific data is one of the prominent driving forces behind next generation networking. In particular, Software Defined Network (SDN) makes leveraging path-based network multicast services practically feasible. In our prior work, we have developed a cross-layer architecture for supporting reliable file-streams multicasting over SDN-enabled Layer-2 network, and implemented the architecture for a meteorology data distribution application in atmospheric science. However, it is challenging to determine an optimal rate for this application with the varying type, volume, and quality of meteorological data. In this paper, we propose a Quality of Service (QoS)-driven rate management pipeline to determine the optimal rate based on the input traffic characteristics and performance constraints. Specifically, the pipeline employs a feedtype classifier using Multi-Layer Perception (MLP) to recognize the type of meteorological data and a delay prediction regressor using stacked Long Short-Term Memory (LSTM) to predict per-file delay for the file-streams. Finally, we determine the optimal rate for the given file-streams using the trained regressor. We implement this pipeline to test the real-world file-stream data collected from a trial deployment, and the results show that our regressor outperforms all baselines by selecting the optimal rate in the presence of varying file set sizes.",https://ieeexplore.ieee.org/document/9685807/,2021 IEEE Global Communications Conference (GLOBECOM),7-11 Dec. 2021,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SITIS.2019.00066,A Microservice-Based Building Block Approach for Scientific Workflow Engines: Processing Large Data Volumes with DagOnStar,IEEE,Conferences,"The impact of machine learning algorithms on everyday life is overwhelming until the novel concept of datacracy as a new social paradigm. In the field of computational environmental science and, in particular, of applications of large data science proof of concept on the natural resources management this kind of approaches could make the difference between species surviving to potential extinction and compromised ecological niches. In this scenario, the use of high throughput workflow engines, enabling the management of complex data flows in production is rock solid, as demonstrated by the rise of recent tools as Parsl and DagOnStar. Nevertheless, the availability of dedicated computational resources, although mitigated by the use of cloud computing technologies, could be a remarkable limitation. In this paper, we present a novel and improved version of DagOnStar, enabling the execution of lightweight but recurring computational tasks on the microservice architecture. We present our preliminary results motivating our choices supported by some evaluations and a real-world use case.",https://ieeexplore.ieee.org/document/9067951/,2019 15th International Conference on Signal-Image Technology & Internet-Based Systems (SITIS),26-29 Nov. 2019,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICAI52893.2021.9639638,A Modelling & Simulation Tier Design for the EMULSION IoT Platform,IEEE,Conferences,"This paper ♠ introduces some design aspects of the EMULSION IoT platform, which is an example of the new IoT platforms of horizontal type, evolving as a replacement of the existing vertical-type IoT platforms. The EMULSION’s architectural overview, main characteristics, and multi-tiered structure are presented, with attention dedicated to its modelling & simulation tier.♠This publication has emanated from joint research conducted with the financial support of the Bulgarian National Science Fund (BNSF) under the Grant No. (КП-06-ИП-КИТАЙ/1) and the S&T Major Project of the Science and Technology Ministry of China, Grant No. 2017YFE0135700.",https://ieeexplore.ieee.org/document/9639638/,2021 International Conference Automatics and Informatics (ICAI),30 Sept.-2 Oct. 2021,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/MENACOMM50742.2021.9678232,A Novel Classification of Machine Learning Applications in Healthcare,IEEE,Conferences,"In recent years, machine learning has become widely used in various applications and research. It plays a crucial role in numerous fields such as medical science and the healthcare system. In such scenarios, machine learning is used to diagnose sizeable medical data patterns or predict diseases. This paper presents a survey about different machine learning algorithms with their applications in various domains and shows the advantages of machine learning techniques that help create efficient support infrastructure for medical fields and improve healthcare services. This survey&#x2019;s main objective is to highlight the previous work of machine learning algorithms implemented in the healthcare system and provide all necessary information to the researchers who want to explore machine learning in the healthcare system.",https://ieeexplore.ieee.org/document/9678232/,2021 3rd IEEE Middle East and North Africa COMMunications Conference (MENACOMM),3-5 Dec. 2021,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AICI.2009.31,A Novel Clustering Algorithm for Graphs,IEEE,Conferences,"Graph or network clustering is one of the fundamental multimodal combinatorial problems that have many applications in computer science. Many algorithms have been devised to obtain a reasonable approximate solution for the problem. Current approaches, however, suffer from the local optimum drawback and then have difficulty splitting two clusters with very confused structures. In this paper we propose a novel genetic-based algorithm incorporating with modularity(QN) for the quality of partitioning of graphs. The theoretical analysis and experimental results on synthetic and real networks demonstrate superior performance over Newman's fast agglomerative algorithms in accuracy.",https://ieeexplore.ieee.org/document/5376354/,2009 International Conference on Artificial Intelligence and Computational Intelligence,7-8 Nov. 2009,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICACITE53722.2022.9823457,A Novel Implementation of Esophagus Diagnosis Using Deep Learning,IEEE,Conferences,"The presented work is a systematic review on recent technologies in deep learning for Barrett&#x0027;s esophagus (BE), a disease which affects the food pipe. Evaluation of this disorder can be made easy with the help of model developed with deep learning and artificial intelligence. The dysplasia and adenocarcinoma exhibits a complex pattern for detecting through endoscopic diagnosis. The diagnosis and automatic detection using computer analysis is beneficial for assisting the endoscopy procedure. The deep learning technique developed through manual and automated segmentation for the evaluation of BE disorders. The review is done by compilation of works published in Springer, Binda Wi, IEEEXplore, and Association for Computing Machinery, Science Direct and Pubmed on the category of automatic detection of regions for classification purposes. The problem statement, methodology, objective and result of the selected work have been analyzed.",https://ieeexplore.ieee.org/document/9823457/,2022 2nd International Conference on Advance Computing and Innovative Technologies in Engineering (ICACITE),28-29 April 2022,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/GLOBECOM38437.2019.9013255,A Reinforcement Learning Based Network Scheduler for Deadline-Driven Data Transfers,IEEE,Conferences,"We consider a science network that runs applications requiring data transfers to be completed within a given deadline. The underlying network is a software defined network (SDN) that supports fine grain real-time network telemetry. Deadline-aware data transfer requests are made to a centralized network controller that schedules the flows by setting pacing rates of the deadline flows and metering the background traffic at the ingress routers. The goal of the scheduling algorithm is to maximize the number of flows that meet the deadline while maximizing the network utilization. In this paper, we develop a Reinforcement Learning (RL) agent based network controller and compare its performance with well-known heuristics. For a network consisting of a single bottleneck link, we show that the RL-agent based network controller performs as well as Earliest Deadline First (EDF), which is known to be optimal. We also show that the RL-agent performs significantly better than an idealized TCP protocol in which the bottleneck link capacity is equally shared among the competing flows. We also study the sensitivity of the RL-agent controller for different parameter settings and reward functions.",https://ieeexplore.ieee.org/document/9013255/,2019 IEEE Global Communications Conference (GLOBECOM),9-13 Dec. 2019,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICIRCA48905.2020.9183088,A Survey of Application of ML and Data Mining Techniques for Smart Irrigation System,IEEE,Conferences,"This paper reviews our current research in agriculture analytics on an open-source platform using data mining and machine learning techniques. Various sensors are used to collect data that provides real-time analytics on the weather forecast, soil moisture, air temperature, PH, humidity. The smart irrigation system is paired with different hardware and development application. The science of machine learning and data mining plays a significant role here. It is a data analysis process using multiple models and algorithms to trained data directly. Machine learning and data mining to hold the irrigation system in r leaner manner. For instance, it optimizes the use of water and provides a critical amount of water and fertility to increase production efficiency, reduce manpower involvement, and reduce crop diseases. The survey tests the effect of applicable techniques and how these techniques boost efficiency.",https://ieeexplore.ieee.org/document/9183088/,2020 Second International Conference on Inventive Research in Computing Applications (ICIRCA),15-17 July 2020,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/COMITCon.2019.8862251,A Survey: How Python Pitches in IT-World,IEEE,Conferences,"This paper lights on Python amongst other different programming paradigms used in the IT World, which enhances development speed. Although, Python was conceptualized in the late 1980s and after its implementation in 1989, it has emerged as a new multi-paradigm language platform with advent of Big Data. Python includes various data structures, standard libraries with the implementation of sentiment analysis and data science code. The real aim is to provide awareness to all the programmers about various facts of python language. It tells how Python works with various commercial and social communities and provides complete and desirable results. There are many areas and applications where Python makes its own stand as compared to other programming languages.",https://ieeexplore.ieee.org/document/8862251/,"2019 International Conference on Machine Learning, Big Data, Cloud and Parallel Computing (COMITCon)",14-16 Feb. 2019,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICIEVicIVPR52578.2021.9564229,A Vision-Based Lane Detection Approach for Autonomous Vehicles Using a Convolutional Neural Network Architecture,IEEE,Conferences,"Autonomous vehicles no longer belong to the realm of science fiction. They have become a prominent area of research in the last two decades because of the integration of Artificial Intelligence in the automobile industry. Apart from the development of various complex learning algorithms, the advancement of cameras, sensors, and geolocation technology as well as the escalation in the capacity of machines have played a crucial role in bringing this technology into reality. We have had significant breakthroughs in the development of autonomous cars within the last ten years. However, despite the success of multiple prototypes in navigating within the borders of a delimited area, researchers are yet to overcome several drawbacks before embodying them in the transport system; and one of those hurdles lies in the lane detection system of the cars. Therefore, in this article, we present an intelligent lane detection algorithm incorporating fully-connected Neural Networks with a secondary layer protection scheme to detect the borders of a lane. We achieved over 98% classification accuracy using the proposed lane detection model. We also implemented the model in a small prototype to take a look at its performance. Experimental results infer that the algorithm is capable of lane detection and ready for practical use.",https://ieeexplore.ieee.org/document/9564229/,"2021 Joint 10th International Conference on Informatics, Electronics & Vision (ICIEV) and 2021 5th International Conference on Imaging, Vision & Pattern Recognition (icIVPR)",16-20 Aug. 2021,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IMIS.2016.116,A Wireless Body Sensor Network and Its Applications: Rehearsal with a Smartphone,IEEE,Conferences,"Wireless body sensor network (WBSN) technologies are considered one of attracting research areas in computer science. When combined with the healthcare application, it provides high value technology of comprehensive healthcare monitoring solution in extreme situations including high altitude or disaster area enabling the ground controller to monitor remote pilots or earthquake victims in real time by combination of wireless sensors and sensor networks. Sensor networks are irregular clusters of communicating sensor nodes, which collect and process information from onboard sensors, and they can share some of this information with neighboring nodes. By recent technological advances in the integration of ultra-low power networks of miniatured sensors, embedded microcontrollers and radio interfaces on an all-in-one chip can collect important physiologic informations from the surrounding environment efficiently. In addition, the sensed physiological data transmitted to a remote healthcare provider can be modulated and fed back by processing software such as artificial intelligence system enriching capability of the sensor network. In this study, we develop a wireless body sensor network which integrates different physiological sensors with radiointerface to sense physiological data from a human body, and then rehearses to transmits the data to a remote healthcare cloud through a smartphone as an interface. Test results for simulated measurements of wireless transmission and emulation in a smartphone are given and discussed.",https://ieeexplore.ieee.org/document/7794501/,2016 10th International Conference on Innovative Mobile and Internet Services in Ubiquitous Computing (IMIS),6-8 July 2016,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AIMSEC.2011.6010129,A broad-spectrum temperature tester based on SCM,IEEE,Conferences,"With the development of the modern science and technology, the requirements of temperature control are higher and higher. In most cases, we need real-time detection of the environmental temperature and communications. In order to adapt to the rapid development of technology, temperature tester need to be more effective and accurate. This paper describes the system board based on a broad spectrum of MSP430 MCU temperature measurement instrument, the major technical indicators of the system and some related tests are discussed in detail. The results show that the system is accurate, sensitive, efficient, stable, reliable and suitable for the application of intelligent automated environment.",https://ieeexplore.ieee.org/document/6010129/,"2011 2nd International Conference on Artificial Intelligence, Management Science and Electronic Commerce (AIMSEC)",8-10 Aug. 2011,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/COGINF.2010.5599732,A mind model for brain-like computer,IEEE,Conferences,"Mind is all mankind's spiritual activities, including emotion, will, perception, consciousness, representation, learning, memory, thinking, intuition, etc. Mind model is for explaining what individuals operate in the cognitive process for something in the real world. It is the internal sign or representation for external realistic world. If the neural network is a hardware of the brain system, then the mind model is the software of the brain system. The key issue in intelligence science is to construct the mind model of the brain system, which will guide the development of brain-like computer in engineering through structure, dynamics, function and behavioral reverse engineering of the brain. This paper will discuss the computational model of memory and consciousness in the mind model named Consciousness And Memory model(CAM).",https://ieeexplore.ieee.org/document/5599732/,9th IEEE International Conference on Cognitive Informatics (ICCI'10),7-9 July 2010,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ACIIW.2019.8925291,A multi-layer artificial intelligence and sensing based affective conversational embodied agent,IEEE,Conferences,"Building natural and conversational virtual humans is a task of formidable complexity. We believe that, especially when building agents that affectively interact with biological humans in real-time, a cognitive science-based, multilayered sensing and artificial intelligence (AI) systems approach is needed. For this demo, we show a working version (through human interaction with it) our modular system of natural, conversation 3D virtual human using AI or sensing layers. These including sensing the human user via facial emotion recognition, voice stress, semantic meaning of the words, eye gaze, heart rate, and galvanic skin response. These inputs are combined with AI sensing and recognition of the environment using deep learning natural language captioning or dense captioning. These are all processed by our AI avatar system allowing for an affective and empathetic conversation using an NLP topic-based dialogue capable of using facial expressions, gestures, breath, eye gaze and voice language-based two-way back and forth conversations with a sensed human. Our lab has been building these systems in stages over the years.",https://ieeexplore.ieee.org/document/8925291/,2019 8th International Conference on Affective Computing and Intelligent Interaction Workshops and Demos (ACIIW),3-6 Sept. 2019,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ITSC.2017.8317664,A new modelling framework over temporal graphs for collaborative mobility recommendation systems,IEEE,Conferences,"Over the years, collaborative mobility proved to be an important but challenging component of the smart cities paradigm. One of the biggest challenges in the smart mobility domain is the use of data science as an enabler for the implementation of large scale transportation sharing solutions. In particular, the next generation of Intelligent Transportation Systems (ITS) requires the combination of artificial intelligence and discrete simulations when exploring the effects of what-if decisions in complex scenarios with millions of users. In this paper, we address this challenge by presenting an innovative data modelling framework that can be used for ITS related problems. We demonstrate that the use of graphs and time series in multi-dimensional data models can satisfy the requirements of descriptive and predictive analytics in real-world case studies with massive amounts of continuously changing data. The features of the framework are explained in a case study of a complex collaborative mobility system that combines carpooling, carsharing and shared parking. The performance of the framework is tested with a large-scale dataset, performing machine learning tasks and interactive realtime data visualization. The outcome is a fast, efficient and complete architecture that can be easily deployed, tested and used for research as well in an industrial environment.",https://ieeexplore.ieee.org/document/8317664/,2017 IEEE 20th International Conference on Intelligent Transportation Systems (ITSC),16-19 Oct. 2017,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IJCNN.2000.859443,A recurrent neural network for solving nonlinear projection equations,IEEE,Conferences,"In this paper, we are concerned with the nonlinear projection equations of the following form P/sub /spl chi//(u-F(u))=u. Despite the particular structure of the feasible set /spl chi/, the problem is still a very general problem in mathematics programming. Moreover, there are a number of important applications which lead to this special class of variational inequalities such as equilibrium models arising in fields of economics and transportation science, etc. Various numerical solution procedures for the problem have been investigated over decades. Because of the nature of digital computers, conventional algorithms are time-consuming for large-scale optimization problems. It is well-known that one promising approach to optimization problems in real time is to employ artificial neural networks implemented in hardware. Recurrent neural networks for solving optimization problems are readily hardware-implementable. Thus, neural networks are a top choice of real-time solvers for optimization problems. Since the seminal work of Hopfield and Tank (1985), the neural network approach to optimization has been investigated and many neural networks for optimization problems have been proposed.",https://ieeexplore.ieee.org/document/859443/,Proceedings of the IEEE-INNS-ENNS International Joint Conference on Neural Networks. IJCNN 2000. Neural Computing: New Challenges and Perspectives for the New Millennium,27-27 July 2000,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/GLOCOM.2002.1188542,A scalable on-line multilevel distributed network fault detection/monitoring system based on the SNMP protocol,IEEE,Conferences,"Traditional centralized network management solutions do not scale to present-day large-scale computer/communication networks. Decentralization/distributed solutions can solve some of these problems (Goldszmidt, G. and Yemini, Y., 1995), and thus there is considerable interest in distributed/decentralized network management applications. We present the design and evaluation of an SNMP-based distributed network fault detection/monitoring system. We integrate into the SNMP framework our ML-ADSD algorithm (Su, M.-S. et al., Proc. 39th Annual Allerton Conf. on Commun., Control, and Computers, 2001; Su, ""Multilevel distributed diagnosis and the design of a distributed network fault detection system based on the SNMP protocol"", Ph.D. Thesis, School of Computer Science, University of Oklahoma, 2002) for fault diagnosis in a distributed processor system. The algorithm uses the multilevel paradigm and requires only minor modifications to be scalable to networks of varying sizes. The system is fault tolerant, allowing processor failure and/or recovery during the diagnosis process. We have implemented the system on an Ethernet network of 32 machines. Our results show that the diagnosis latency (or time to termination) is much better than that of earlier solutions. Also, the system's bandwidth utilization is insignificant, demonstrating the practicality of its deployment in a real network. We have successfully integrated three modern disciplines: network management, distributed computing and system level diagnosis.",https://ieeexplore.ieee.org/document/1188542/,"Global Telecommunications Conference, 2002. GLOBECOM '02. IEEE",17-21 Nov. 2002,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICVRV47840.2019.00064,ARTowerDefend: A Shooting Mobile Game Based on Augmented Reality,IEEE,Conferences,"The rapid development of science and technology in today's era has led to the development and application of augmented reality. This article analyzes an Android augmented reality game “ARTowerDefend”. From design to implementation, it describes the clever application of augmented reality in all aspects. Utilizing the Unity game engine combined with GoogleARcore and multiple applications of artificial intelligence to achieve game highlights and innovation, giving players a great sense of gaming experience.",https://ieeexplore.ieee.org/document/9212902/,2019 International Conference on Virtual Reality and Visualization (ICVRV),18-19 Nov. 2019,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/FCCM.2017.58,Accelerating Large-Scale Graph Analytics with FPGA and HMC,IEEE,Conferences,"Graph analytics that explores the relationship among interconnected entities is becoming increasingly important due to its broad applicability from machine learning to social science. However, one major challenge for graph processing systems is the irregular data access pattern of graph computation which can significantly degrade the performance. The algorithms, software, and hardware that have been tailored for mainstream parallel applications are, as a result, generally not effective for massive-scale sparse graphs from the real world due to their complexity and irregularity. To address the performance issues in large-scale graph analytics, we combine the emerging Hybrid Memory Cube (HMC) with a modern FPGA in order to achieve exceptional random access performance without any loss of flexibility or efficiency in computation. In particular, we develop collaborative software/hardware techniques to perform a level-synchronized breadth first search (BFS) on the FPGA-HMC platform. From the software perspective, we develop an architecture-aware graph clustering algorithm that fully exploits the platform's capability to improve data locality and memory access efficiency. For each input graph, this algorithm provides an efficient data layout that allows the FPGA to coalesce memory requests into the largest possible HMC payload requests so that the number of memory requests, which is the primary factor in runtime, can be minimized. From the hardware perspective, we further improve the FPGA-HMC graph processor architecture by adding a merging unit. The merging unit takes the best advantage of the increased data locality resulting from graph clustering. We evaluated the performance of our BFS implementation using the AC-510 development kit from Micron over a set of benchmarks from a wide range of applications. We observed that the combination of the clustering algorithm and the merging hardware achieved 2.8 × average performance improvement compared to the latest FPGA-HMC based graph processing system.",https://ieeexplore.ieee.org/document/7966655/,2017 IEEE 25th Annual International Symposium on Field-Programmable Custom Computing Machines (FCCM),30 April-2 May 2017,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ELINFOCOM.2016.7562974,Accelerating forwarding computation of artificial neural network using CUDA,IEEE,Conferences,"Recently, graphics processing units (GPUs) are widely used for accelerating general purpose workloads using programming models such as open computing language (OpenCL) or compute unified device architecture (CUDA). In this paper, we accelerated the Artificial Neural Network (ANN) algorithm, one of the popular algorithm in machine learning and cognitive science, since the ANN algorithm needs to be faster for solving more complex problem or operating in real-time. The ANN algorithm has great potential for GPU acceleration since it is constructed with large data-parallel computations. We implemented forwarding computation of ANN in CUDA and optimized it using scratchpad memory of GPUs and leveraging the thread block size. As a results, our method shows 2.32 times faster performance compared to conventional CPU.",https://ieeexplore.ieee.org/document/7562974/,"2016 International Conference on Electronics, Information, and Communications (ICEIC)",27-30 Jan. 2016,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/BigData.2014.7004374,Access-averse framework for computing low-rank matrix approximations,IEEE,Conferences,"Low-rank matrix approximations play important roles in many statistical, scientific, and engineering applications. To compute such approximations, different algorithms have been developed by researchers from a wide range of areas including theoretical computer science, numerical linear algebra, statistics, applied mathematics, data analysis, machine learning, and physical and biological sciences. In this paper, to combine these efforts, we present an “access-averse” framework which encapsulates some of the existing algorithms for computing a truncated singular value decomposition (SVD). This framework not only allows us to develop software whose performance can be tuned based on domain specific knowledge, but it also allows a user from one discipline to test an algorithm from another, or to combine the techniques from different algorithms. To demonstrate this potential, we implement the framework on multicore CPUs with multiple GPUs and compare the performance of two representative algorithms, blocked variants of matrix power and Lanczos methods. Our performance studies with large-scale graphs from real applications demonstrate that, when combined with communication-avoiding and thick-restarting techniques, the Lanczos method can be competitive with the power method, which is one of the most popular methods currently used for these applications. InIn addition, though we only focus on the truncated SVDs, the two computational kernels used in our studies, the sparse-matrix dense-matrix multiply and tall-skinny QR factorization, are fundamental building blocks for computing low-rank approximations with other objectives. Hence, our studies may have a greater impact beyond the truncated SVDs.",https://ieeexplore.ieee.org/document/7004374/,2014 IEEE International Conference on Big Data (Big Data),27-30 Oct. 2014,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IISA.2018.8633586,Adding Value to Sensor Data of Civil Engineering Structures: Automatic Outlier Detection,IEEE,Conferences,"This paper discusses the problem of outlier detection in datasets generated by sensors installed in large civil engineering structures. Since outlier detection can be implemented after the acquisition process, it is fully independent of particular acquisition processes as well as sit scales to new or updated sensors. It shows a method of using machine learning techniques to implement an automatic outlier detection procedure, demonstrating and evaluating the results in a real environment, following the Design Science Research Methodology. The proposed approach makes use of Manual Acquisition System measurements and combine them with a clustering algorithm (DBSCAN) and baseline methods (Multiple Linear Regression and thresholds based on standard deviation) to create a method that is able to identify and remove most of the outliers in the datasets used for demonstration and evaluation. This automatic procedure improves data quality having a direct impact on the decision processes with regard to structural safety.",https://ieeexplore.ieee.org/document/8633586/,"2018 9th International Conference on Information, Intelligence, Systems and Applications (IISA)",23-25 July 2018,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IGARSS.2011.6048931,Aggregation of parallel computing and hardware/software co-design techniques for high-performance remote sensing applications,IEEE,Conferences,"Developing computationally efficient processing techniques for massive volumes of hyperspectral data is critical for space-based Earth science and planetary exploration. In particular, many remote sensing imaging applications require a response in real time in areas such as environmental modeling and assessment, target detection for military and homeland defense/security purposes, and risk prevention and response. This paper propose the aggregation of parallel computing and HW/SW co-design techniques using processor arrays (PAs) units as specialized hardware architectures for the real time enhancement of remote sensing imagery. An extended descriptive experiment design regularization (DEDR) method that incorporates projections onto convex solution sets (POCS) for spatial spectrum pattern (SSP) reconstruction is used to be efficiently implemented (i.e., HW-level) via the new proposition of the aggregation techniques. Finally, it is reported and discussed the Xilinx Virtex-5 FPGA implementation and high-performance issues related to real time enhancement of large-scale real-world RS imagery.",https://ieeexplore.ieee.org/document/6048931/,2011 IEEE International Geoscience and Remote Sensing Symposium,24-29 July 2011,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICDABI53623.2021.9655815,An Application on Ensemble Learning Using KNIME,IEEE,Conferences,"Machine learning is the science of computers behaving and learning like humans with the knowledge and data of people&#x2019;s observations, without being directly programmed. In fact, machine learning is inspired by the learning processes of humans. Among machine learning methods, Bayes&#x2019; theorem is an important subject studied in probability theory.Naive Bayes classifier is based on Bayes theorem. The way the algorithm works is that it calculates the probability of each state for an element and classifies it according to its highest probability value. Decision tree is a method based on classification by decomposing the data set according to common features. It consists of &#x201C;branches&#x201D;, &#x201C;leaves&#x201D; and &#x201C;roots&#x201D;, just like real-world trees. In decision trees, the superstructure is the root and the substructure is the leaves. It creates a structure that allows the branches to decide between the root and the leaf. Ensemble learning algorithms improve classification performance by combining many machine learning methods. In this study, decision trees from data mining techniques and naive bayes technique were applied on 215 data set &#x201C;Academic and Employability Factors Affecting Placement&#x201D;. As a result of this study, the decision tree accuracy rate is 91,892, the naive bayes accuracy rate is 94,595 and the ensemble learning result is 97,297. Thus, a better result is obtained than the result of both algorithms used. The program is implemented on &#x201C;Knime&#x201D; program called as &#x201C;end-to-end data science&#x201D;.",https://ieeexplore.ieee.org/document/9655815/,2021 International Conference on Data Analytics for Business and Industry (ICDABI),25-26 Oct. 2021,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/INCOS.2009.51,An Architecture for Adaptive Collaboration Support Guided by Learning Design,IEEE,Conferences,"A CSCL environment provides support to manage collaborative tasks. However, these systems do not usually provide the personalization features required to adapt the learning experience to the student needs, a drawback that can affect the collaboration objective and ultimately a successful learning. To alleviate this disadvantage we propose an architecture that provides adaptive collaboration support for a CSCL environment framed in an open and standards-based LMS. Our proposal combines adaptation rules defined in IMS Learning Design specification and dynamic support through recommendations via an accessible and adaptive guidance system. The implementation offers CSCL courses following a methodology called Collaborative Logical Framework. This system has been tested on a real world scenario at the Madrid Science Week 2009.",https://ieeexplore.ieee.org/document/5369352/,2009 International Conference on Intelligent Networking and Collaborative Systems,4-6 Nov. 2009,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/iThings-GreenCom-CPSCom-SmartData-Cybermatics50389.2020.00111,An Assessment of the Usability of Machine Learning Based Tools for the Security Operations Center,IEEE,Conferences,"Gartner, a large research and advisory company, anticipates that by 2024 80% of security operation centers (SOCs) will use machine learning (ML) based solutions to enhance their operations.11https://www.ciodive.com/news/how-data-science-tools-can-lighten-the-load-for-cybersecurity-teams/572209/ In light of such widespread adoption, it is vital for the research community to identify and address usability concerns. This work presents the results of the first in situ usability assessment of ML-based tools. With the support of the US Navy, we leveraged the national cyber range-a large, air-gapped cyber testbed equipped with state-of-the-art network and user emulation capabilities-to study six US Naval SOC analysts' usage of two tools. Our analysis identified several serious usability issues, including multiple violations of established usability heuristics for user interface design. We also discovered that analysts lacked a clear mental model of how these tools generate scores, resulting in mistrust a and/or misuse of the tools themselves. Surprisingly, we found no correlation between analysts' level of education or years of experience and their performance with either tool, suggesting that other factors such as prior background knowledge or personality play a significant role in ML-based tool usage. Our findings demonstrate that ML-based security tool vendors must put a renewed focus on working with analysts, both experienced and inexperienced, to ensure that their systems are usable and useful in real-world security operations settings.",https://ieeexplore.ieee.org/document/9291520/,"2020 International Conferences on Internet of Things (iThings) and IEEE Green Computing and Communications (GreenCom) and IEEE Cyber, Physical and Social Computing (CPSCom) and IEEE Smart Data (SmartData) and IEEE Congress on Cybermatics (Cybermatics)",2-6 Nov. 2020,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICDMW.2009.26,An Effective Network Partitioning Algorithm Based on Two-Point Diffusing Strategy,IEEE,Conferences,"The network modeling and analysis have played important roles in fields of physics, sociology, biology, and computer science. Recently, community structure has been considered as an important character for complex networks, and its detection can bring great benefit in real world affairs. In the paper, a new heuristic algorithm based on two-point diffusing strategy is proposed. At first, two pseudo-core points are identified according to the clue of the longest path in a network. Then, two embryonic communities and an undecided node set are generated through performing diffusing operation on such two points. Subsequently, an experience rule is used to classify the undecided nodes to form the final community structure. In addition, the effectiveness and efficiency are validated by comparison experiments with four real-world networks. The experiment results show that our TPD algorithm can yield better community partition results and shorter computing time than the existing classical community detecting algorithms.",https://ieeexplore.ieee.org/document/5360433/,2009 IEEE International Conference on Data Mining Workshops,6-6 Dec. 2009,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICACITE53722.2022.9823643,An Empirical Analysis of Python Programming for Advance Computing,IEEE,Conferences,"Python is an object-oriented, scripting, and interpretive programming language that may be used for mentoring and real-world applications. This paper focusses primarily on Python software packages used in data science, pattern recognition, and IoT. This paper will first explain Python as a language, then introduce Data Science, Machine learning, and IOT, describing prominent packages in the Data Science and Machine learning community, such as NumPy, SciPy, TensorFlow, Keras, Matplotlib. This paper will also demonstrate the significance of Python in the development of the industry. Throughout, we shall utilize many code samples. We review so many research papers to analyze the usage of python in different fields and easily import packages in the programming software.",https://ieeexplore.ieee.org/document/9823643/,2022 2nd International Conference on Advance Computing and Innovative Technologies in Engineering (ICACITE),28-29 April 2022,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SNPD.2019.8935648,An Multi-client Web-based Interactive HCI for Interactive Supercomputing,IEEE,Conferences,"Importance of the real-time data analysis even in the HPC area rapidly increase in the recent data intensive science. Once the real-time data analysis becomes available, the interactive feedback to the data source will be happening to make thing improve. In order to realize such an environment for scientific simulation, the framework for interactive supercomputing has been developed so far. However, user interaction of that framework was limited to a certain closed environment. In this paper, web-based ubiquitous interface that allows simultaneous sharing of on-going simulation among multiple users is proposed.",https://ieeexplore.ieee.org/document/8935648/,"2019 20th IEEE/ACIS International Conference on Software Engineering, Artificial Intelligence, Networking and Parallel/Distributed Computing (SNPD)",8-11 July 2019,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.23919/OCEANS44145.2021.9705666,An Underwater Simulation Server Oriented to Cooperative Robotic Interventions: The Educational Approach,IEEE,Conferences,"Experiments that require the use of Supervised Autonomous Underwater Vehicles for Intervention (I-AUV) are not easy to be performed, specially when deployed in the sea or in scenarios where the robot might face lack of space and communication (e.g. interior of pipes). Also, there are some applications where the robots need to cooperate in a closed manner, for example when transporting and assembling big pipes. In fact, these two scenarios are being studied in the context of the H2020-ElPeacetolero and TWINBOT (TWIN roBOTs for cooperative underwater intervention mission) [1] projects, being necessary to have a simulation tool that offer more realistic rendering and being compatible with the real robot Application Programming Interface.This paper presents a new underwater simulation server, implemented using video game and robotic techniques, which operates by enabling the researchers control the robots in the scene in a simple and efficient manner, while using HTTP commands that have demonstrated a huge facility in the project integration process. Moreover, this simplicity has allowed the application of the simulation server in the educational context. The use of this tool has resulted to be very adequate for the students, who have used it to learn computer science and artificial intelligence algorithms to solve problems like a cooperative transportation robotic task. As case study, four educational experiments are presented, performed by master’s degree students, focusing on user interfaces, image compression for underwater channels, autonomous cooperative grasping and robot arm movement in a AUV.",https://ieeexplore.ieee.org/document/9705666/,OCEANS 2021: San Diego – Porto,20-23 Sept. 2021,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICBDACI.2017.8070822,An approach towards data visualization based on AR principles,IEEE,Conferences,"In today's era, the computer science and technology has evolved upto a great extent. Augmented Reality (AR) is a technology evolved from artificial intelligence and also follows the principle of pervasive computing. AR can be achieved by encapsulating algorithms and snippets into smart devices such as laptops, android phones and other smart devices. The digitization of data can be done in a very aesthetic way by this new technology. AR and big data share a logical and mature relationship that inevitably converges them. The paper describes the advantages of merging AR and big data to invent new interesting applications is starting to have a tangible presence. The main aim is to uncover the problems and ease the issues in visualization of Big Data and at the same time the objective of finding valid solutions for the problems in Big Data Visualization remains. The sections mentioned below elaborates the present tools, techniques and platforms which can be used for Visualization in Big Data. We reveal disadvantages of currently existing visualization methods based on the results. Based on the results, a not so common approach is proposed: the capabilities and methods of virtual and augmented reality could be implemented to achieve Visualization of the big data. We also discuss about the applications of AR and big data and fields where it is used. Further in later sections we discuss about the user interface with presence of tangibility, advantages and disadvantages of trending technologies, such as VR and AR displays on the Big Data visualization.",https://ieeexplore.ieee.org/document/8070822/,2017 International Conference on Big Data Analytics and Computational Intelligence (ICBDAC),23-25 March 2017,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/VSMM.2012.6365917,An ecosystem of tools and methods for archeological research,IEEE,Conferences,"The present paper describes the design strategies of an ecosystem of tools and services, currently under construction, developed in cooperation among archaeologists and computer science experts of the Università degli Studi di Milano and architects and topographers of the Politecnico di Milano. This ecosystem is based on the archaeological experience carried out in Tarquinia since 1982 in the frame of the “Tarquinia Project”. The project takes into account the analysis and processing of multifaceted archaeological evidences in a context-oriented environment, in which interdisciplinary contributions of several scientist are combined and integrated, in order to grasp the original system of interaction of different branches of the ancient reality. Such a cooperation needs a system of ""query/communication"" able to integrate archaeological data, artefacts and architectural structures (subsoil and over-ground), cartographic and photographic documentation and scientific contents, achieved in the past and implemented during the field research. The proposed ecosystem aims to provide a set of services for federating different existing data-sources (GIS, including 3D tools), through the definition of a semantic network of relationships among landscapes, stratigraphic layers, structures and artefacts of an excavation site (ArchMatrix). This ecosystem is based on an innovative global design method, focused on the management of raw-data captured and analyzed by different experts in a collaborative way. The aim is to develop a solution able to support analyses and studies grounded in the real needs of the archaeological investigation, by enabling archaeologists in producing archaeological and historical interpretations starting from the real core of the documentation they deal with. In such a framework, the present paper focuses on a novel approach to identify the object of the archaeological research, starting from the needs of field archaeology, and on the design of a system meant to solve problems according to an integrated approach in a unique context of analysis.",https://ieeexplore.ieee.org/document/6365917/,2012 18th International Conference on Virtual Systems and Multimedia,2-5 Sept. 2012,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICBASE51474.2020.00073,Analysis of 3D Image Reconstruction System Based on Virtual Reality Technology,IEEE,Conferences,"With the continuous development of science and technology, two-dimensional image technology can no longer meet people's needs, so three-dimensional image reconstruction technology has been studied by people. Three-dimensional image reconstruction has been applied in many fields. For example: medical, military, detection, etc. This article mainly introduces the research of 3D geological simulation system based on virtual reality technology and virtual reality technology.",https://ieeexplore.ieee.org/document/9403827/,2020 International Conference on Big Data & Artificial Intelligence & Software Engineering (ICBASE),30 Oct.-1 Nov. 2020,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IMITEC50163.2020.9334099,Application of Bellman's Equation in Ant-Like Robotic Device Path Decisions,IEEE,Conferences,"Swarm Intelligence is about emergency of collective intelligence from groups of homogeneous robotic devices deployed for a purpose. Ant Colony Systems, in particular, are inspiring. They commonly have drawn inspiration from the behaviors of real ants in nature in order to construct routes between the food sources and the nest. There are still gaps in alternative options for path decision in ant agents. Bellman's equation has been successfully used to solve path decision problems in machine learning. We proposed to investigate impact of a Bellman's equation inspired algorithm for path decision on stigmergic ant agent robotic devices. A design science research paradigm was used to design our research experiment in which a simulated environment was designed to simulate the behavior of ant agents when using a Bellman's equation inspired algorithm for path decision. We introduced a reward function to the orientation process of ant agents. Reward function rewards a decision made when an ant moves from one point to an adjacent cell. The Bellman's inspired algorithm for ant orientation led to convergence of ant agents even though there was reduced quality of convergence. Evaluation of results show that Bellman's equation can be used in path decision processes for ant agent robotic devices. Our results contributed to adding an alternative way of implementing path decision for ant agents. This will help in growing the knowledge around ant agents and finding better ways to implementing path decisions for ant agents.",https://ieeexplore.ieee.org/document/9334099/,2020 2nd International Multidisciplinary Information Technology and Engineering Conference (IMITEC),25-27 Nov. 2020,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/USSEC53120.2021.9655754,Application of Long Short-Term Memory for Energy Load Prediction in the Microgrid Using Python Software,IEEE,Conferences,"The development of distributed power supply systems, microgrids is recognized as relevant and requires intensive study. Research on microgrid management systems is inextricably linked with data science. The paper presents a study of the use of software for predicting the load consumed by a typical microgrid over a monthly interval. The formulation of the problem of forecasting time series, applied to classical stationary series, is described. The process of data processing using the open-source machine software libraries NumPy, Keras is presented. A class is developed in the Python environment based on the use of recurrent neural networks-long short-term memory, the applicability for the task is shown. The model was trained using iterative optimization of the series value, and the data sampling window. The satisfactory accuracy of forecasting based on the developed model is shown. The conclusions for further study of the applicability of this algorithm in the practice of managing distributed power supply systems are presented.",https://ieeexplore.ieee.org/document/9655754/,2021 Ural-Siberian Smart Energy Conference (USSEC),13-15 Nov. 2021,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICSME.2019.00057,Application of Philosophical Principles in Linux Kernel Customization,IEEE,Conferences,"Philosophical principles are very useful in customization of Linux kernel, e.g., the answer for the question: ""For the pointer to the start address of page table, is it a physical address or a virtual address?"" can be derived by one simple philosophical principle: the depth of recursion is limited. This is because if the pointer were a virtual address, there would be another new page table to store the translation information of this virtual address, but who was responsible for storing the translation information of the start address of this new page table? This would result an infinite recursion. So the pointer definitely is a physical address. In fact, the usefulness of philosophical principles comes from the reduction of searching space. And this reduction is very important in customization of Linux kernel, for it could cut down the size of the new code needed to be read. This is especially valuable when considering that Linux kernel is continuously updating and huge now. Another example to further demonstrate the reduction of searching space in customization is showed in the following: in customization of file system in kernel version 3.10, the question: ""Does the Linux kernel itself maintain the consistency between the buffer cache and the page cache?"". This is a hard problem in practice, for without any guidance of philosophical principle, a developer has to read all of the code in Linux kernel to get a precise answer. The tricky part of this question is that if the developer only read a part of the codes and doesn't find any mechanisms for maintenance of cache consistency, the conclusion of non-existence of such mechanisms still can not be drawn, for there's still a possibility that such mechanisms exist in the codes not explored. Besides, if the developer search internet to find the answer, assume that the developer is lucky enough, he/she finally finds one program example on a web page shows that the inconsistency may raise between buffer cache and page cache. He/she still can not get the conclusion that Linux kernel does not maintain such consistency, because that program example maybe is only valid in a specific scenario, e.g. in kernel version 2.26, not 3.10. But we can get a satisfied answer by using the philosophical principle: the cost of management process should be far less than the value created by being managed process. By this principle, it can be drawn that Linux kernel doesn't maintain the consistency between the buffer cache and page cache in kernel 3.10. This is because that the data in buffer cache and page cache is highly dependent on application logic, so if Linux kernel wanted to maintain such consistency, it would have to track all these applications, which cost was much higher than the benefits that these applications could produce. However, the successful application of philosophical principles depends on two factors: firstly, establishment of a mapping between concepts in Linux system and well-known concepts in human society. This is not a new idea, e.g. the word of ""cost"" is a concept first appeared in human society, not in computer science, but nowadays, developers establish a mapping between this concept and concepts in computer science. Although the idea is very old, it is still very effective. Since well-known concepts in human society are familiar to most developers and are what they have in common, the cost of applying philosophical principles is reduced. Besides this, already existing cause-effect relations among concepts in human society can be highly possible to be reused in philosophical deduction in Linux kernel. E.g, in the mapping we established, process is treated as a human and since in religion of human society, God creates humankind, it is natural to derive that there's one process that creates all other processes in Linux system with high probability. Secondly, a concrete model with many qualitative and quantitative details should be the basis of philosophical deduction. We build such model according to our past experiences and the construction of the model follows the philosophical principle: unfold the complexity only when it is necessary. E.g., in this model, for a specific detail, it is covered only when it is required in practice. This is to lower down the cost of modelling huge and continuously evolving Linux kernel. This model is very important, without it, philosophical deduction is impossible. But it is really a hard work, according to our experiences, it needs at least 6-years of work on Linux kernel for one developer to build it. Although philosophical principles are very useful in practice, there's a big gap on the recognition of philosophical principles between academic researchers and industry practioners. E.g., some academic researcher seriously doubts whether the mapping above, which mentioned God, is helpful. In fact, it is, for by this mapping, a developer will know that the existence of the process, which is the origin of all other processes, is highly possible and also that process maybe is not easily observed. This is true, for that process is the process which PID is zero and that process can not be observed by Linux command: ""ps -e"". That process is a very valuable point of customization, e.g., by modifying that process, all processes in the Linux will be affected. Why does this big gap exist? We believe there're at least three reasons: i. The bias on philosophical principles. This usually comes from the observation that some developers establish wrong mapping between the philosophical principles and the objects in real world. But is that true for those that has been verified many times in practice? ii. Wrong expectations. E.g., hope to get the precise answer when applying philosophical principles, instead of reducing the searching space. iii. Some academic researchers do not realize that a good philosophical principle usually is the result of a deep learning process of many years by human brain. Finally, we suggest that more efforts should be put on the studying of philosophical principles in program understanding and we believe that in the near future, the philosophical principles plus AI will be a trend in program understanding.",https://ieeexplore.ieee.org/document/8919057/,2019 IEEE International Conference on Software Maintenance and Evolution (ICSME),29 Sept.-4 Oct. 2019,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/RTSE.1998.766515,Application of mobile autonomous robots to artificial intelligence and information systems curricula,IEEE,Conferences,"Applies pedagogical ideas of teaching curricula by using strategies of themes and breadth-first coverage, together with the technology of intelligent agents (e.g. mobile autonomous robots), to a system of courses in computer science (artificial intelligence) and information systems (systems engineering). The project brings the issues and constraints of real-time systems, especially the programming component, to students in computer science and information systems curricula. This project's background started in June 1997 and continued during the first part of the 1997-1998 academic year. The actual project work started in January 1998 and is still continuing.",https://ieeexplore.ieee.org/document/766515/,Proceedings Real-Time Systems Education III,21-21 Nov. 1998,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICE.2018.8436264,Applying Entrepreneurial Teaching Methods to Advanced Technical STEM Courses,IEEE,Conferences,"A vast majority of Science, Technology, Engineering, Mathematics (STEM) courses and pedagogical frameworks concentrate on teaching the fundamental concepts and theoretical underpinnings of the tools related to the subject. While this aspect is important, we recognize that the teaching methods in a majority of the STEM courses today are broken; there is a major discrepancy between the skills and mindsets in technical classes and the ones that are useful to solve actual problems in “the real world”. Therefore, we suggest a new teaching framework called Data-X where entrepreneurial teaching methods developed in the Berkeley Method of Entrepreneurship are applied to advanced technical topics. Through inductive learning and by practicing story creation, stakeholder generation, adaptation, ideation, innovation processes, and by having a diverse mix of students being coached by a network of expert advisors, this highly applied teaching method empowers students to pursue and find solutions to open-ended projects and problems. The Data-X framework has been implemented and tested for three semesters in a UC Berkeley course called Applied Data Science for Venture Applications. In the class the students pick up, become comfortable, and utilize state-of-the-art tools in Data Science, Machine Learning, and Artificial Intelligence. The results, feedback, and testimonials we have received upon offering the class have been overwhelmingly positive, and we propose that the ideas and concepts behind Data-X can help fix many problems in modern STEM education.",https://ieeexplore.ieee.org/document/8436264/,"2018 IEEE International Conference on Engineering, Technology and Innovation (ICE/ITMC)",17-20 June 2018,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/EDUCON.2014.6826199,Applying learning analytics to simplify serious games deployment in the classroom,IEEE,Conferences,"In this paper we present our approach to introduce educational videogames as class exercises in face-to-face education. The main objective is to simplify teachers' task when using games by providing real-time information of the actual students' use of the games while in the classroom. The approach is based on defining the educational goals for the exercise/game precisely, designing a game that captures these goals, establishing relations between game interactions and educational goals and finally, create data capturing and visualizations of the relevant information to support the teacher. We applied this approach to a real case study, creating an educational videogame about the XML markup language that substituted the usual exercises in a Web Technologies class. This was tested with 34 computer science students with positive and promising results.",https://ieeexplore.ieee.org/document/6826199/,2014 IEEE Global Engineering Education Conference (EDUCON),3-5 April 2014,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CCCS.2018.8586801,Approving Psycho-Neuro-Computer Systems to prevent (Systemic Vs Individualistic Perspective) Cybercrimes in Information Highway,IEEE,Conferences,"In this paper the authors described the importance of Schizophrenia in Medical Systems versus Computer Systems. Dissimilar Schizophrenias were identified. The schizophrenia is thoughtful, consequently it could be discovered in future. But there is no Intelligence Quotient (IQ). The author contrasting this perception with present research work i.e., Cybercrimes in Higher Educations. This encouraging Psycho-Neuro-Computer Systems are very essential things to control and prevent the Cybercrimes in Cyber space. On each successive day the thoughts, feelings, behavior of humans are going in destructive manner. Especially in internet by using this Communication Technologies and facilities the Cybercrimes are ever-increasing very quickly. The Online Networking Systems are anguish, the people who have Schizophrenia is a serious psychological disorder in which people understand reality abnormally. Schizophrenia may result in some combination of illusions and tremendously disordered thinking and behavior that damages daily functioning, and can be disabling. The author developing positive psychology between the higher education students and faculties to reduce the Online Cybercrimes. In our research Cognitive Systems are showing main role which is interrelated with computer science and psychology. It provides us with a systematic foundation in the principles, ethics, morals, values and techniques used by intelligent systems (both natural and artificial) to interact with the web world. My research required Emotional Intelligence (or) Knowledge (or) Emotional Quotient (EQ) use emotions to enhance positive thoughts. People with high emotions can control, evaluate towards negative thoughts and perceive others emotions and thoughts, uniform estimation calculated of cleverness.",https://ieeexplore.ieee.org/document/8586801/,"2018 IEEE 3rd International Conference on Computing, Communication and Security (ICCCS)",25-27 Oct. 2018,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCSA50381.2020.00019,Architecture Design of a Smart Farm System Based on Big Data Appliance Machine Learning,IEEE,Conferences,"The size of the world's population increased at a Revolution. The modern expansion of human numbers started but environmental degradation with lack of urban services. To satisfy the growing of human food, worldwide demand for grain the area under production should be increased, and productivity must be improved on yields area firstly. To evaluate the Smart Farming sub-use cases' overall outcome, each economic and environmental benefits, social aspects, and the technical evolution path were evaluated. We have like an significant improvement in the economic outcome of the farm. This paper proposed an implementation of BMS (Big Data Application Machine Learning-based Smart Farm System) with an emphasis on crop productivity and the importance of farmers' income increase. Increasing crop productivity is also important to increase essentials' income, enhance farmer field-level insights, and actionable knowledge to produce when the crop is of the best quality or selling it with a good price. Therefore, in the Smart Farm system proposed in this paper specially in case of big data science, we need to consider data analysis and machine learning as the most important steps and then we can include the value of big data science. Machine learning is an essential ability to learn from data and provide data-driven information, decisions, and forecasts. Traditional approaches to machine learning were developed in a different era, like the data set that fully integrates memory. In addition to the characteristics of Big Data, they create obstacles to traditional techniques. One of the objectives of this document is to summarize the challenges of machine learning with Big Data.",https://ieeexplore.ieee.org/document/9257527/,2020 20th International Conference on Computational Science and Its Applications (ICCSA),1-4 July 2020,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.23919/CISTI49556.2020.9141124,Artificial Intelligence Applied to Software Testing: A Literature Review,IEEE,Conferences,"In the last few years Artificial Intelligence (AI) algorithms and Machine Learning (ML) approaches have been successfully applied in real-world scenarios like commerce, industry and digital services, but they are not a widespread reality in Software Testing. Due to the complexity of software testing, most of the work of AI/ML applied to it is still academic. This paper briefly presents the state of the art in the field of software testing, applying ML approaches and AI algorithms. The progress analysis of the AI and ML methods used for this purpose during the last three years is based on the Scopus Elsevier, web of Science and Google Scholar databases. Algorithms used in software testing have been grouped by test types. The paper also tries to create relations between the main AI approaches and which type of tests they are applied to, in particular white-box, grey-box and black-box software testing types. We conclude that black-box testing is, by far, the preferred method of software testing, when AI is applied, and all three methods of ML (supervised, unsupervised and reinforcement) are commonly used in black-box testing being the “clustering” technique, Artificial Neural Networks and Genetic Algorithms applied to “fuzzing” and regression testing.",https://ieeexplore.ieee.org/document/9141124/,2020 15th Iberian Conference on Information Systems and Technologies (CISTI),24-27 June 2020,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/GCAT52182.2021.9587507,Artificial Intelligence Based Predictive Threat Hunting In The Field of Cyber Security,IEEE,Conferences,"Artificial intelligence (AI) is a broad field of computer science that focuses on designing smart machines capable of performing tasks typically requiring human intelligence. Despite the fact that security solutions are growing progressively modern and stable, cyberattacks are still evolving and are at their extreme. The main reason is that conventional methods of malware detection fail. Cyber attackers are actively developing new ways to prevent defence programmes from infecting malware networks and servers. Most anti-malware and antivirus applications currently use signature-based detection to identify attacks, which is unsuccessful in detecting new threats. This is where Artificial Intelligence is most handy. The standardised models for threatened hunting and performance quantification from the start of hazard hunting to the end still allow methodological rigour and completeness to be studied remain undefined. The organised practise of hazard hunts seeks to disclose the presence of TTP in the field of detection that has not already been detected. In this study, a realistic and comprehensive model is outlined to detect attackers in six stages: aim, scale, equipment, planning, execution and input. This study describes Threat Hunting in an ecosystem as the constructive, analyst-driven scanning mechanism for attackers TTP. The model has been checked for real-world data sets using a variety of threats. The effectiveness and practicality of this research have been shown with and without a blueprint through danger hunts. In addition, the article presents an analysis of the concept of threat hunting based on data from Ukrainian electricity grid attacks in an online environment to highlight the effects of this model on threat hunting in a simulated environment. The findings of this analysis include an effective and repetitive way to search for and quantify honesty, coverage and rigour.",https://ieeexplore.ieee.org/document/9587507/,2021 2nd Global Conference for Advancement in Technology (GCAT),1-3 Oct. 2021,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IVS.2018.8500457,Artificial Intelligence Course Design: iSTREAM-based Visual Cognitive Smart Vehicles,IEEE,Conferences,"New intelligent era calls for new learners and thus urgently needs a series of artificial intelligence. As a good educational platform for teaching artificial intelligence, smart cars have aroused concern and practices of all parties. However, at present, most courses and training pay more attention to basic knowledge and technology of smart cars, seldom to training based on artificial intelligence curriculum system and comprehensive competency integrating science, technology, art and management. Therefore, based on concept of iSTREAM (intelligence for Science, Technology, Robotics, Engineering, Art, and Management) and Raspberry intelligent vehicle teaching platform, this paper introduced a smart car-themed artificial intelligence courses including basic courses, specialized courses, specialized technical courses and elective courses. This course can guide learners to develop smart cars based on visual cognition, in-depth learning, VR and 3D printing integrated artistic creativity. It combines disciplines such as science, technology, art, games and management to upgrade a single knowledge and technology course into a comprehensive competency course that integrates knowledge, skills, emotion and management. Practice in Beijing NO.13 and NO.101 High School shows that this course allows students to experience scientific research process, learn artificial intelligence related knowledge and skills, understand scientific way of thinking and scientific research methods, stimulate learners' responsibility and scientific passion, and cultivate leadership skills through self-learning and partly project management.",https://ieeexplore.ieee.org/document/8500457/,2018 IEEE Intelligent Vehicles Symposium (IV),26-30 June 2018,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/EHB52898.2021.9657707,Artificial Intelligence Expert System Based on Continuous Glucose Monitoring (CGM) Data for Auto-Adaptive Adjustment Therapy Protocol – How to Make Sensors and Patients to Think Forward and Work Together?,IEEE,Conferences,"Worldwide adoption of continuous glucose monitoring (CGM) portable miniature sensors in routine clinical practice is increasing for type 1 diabetes treatment in children. The commonly used sensors provide the blood glucose concentration and its change rate in real time (samples every 5 minutes or even 1 minute). Due to these sensors we are in position to collect a significant amount of data from various patients. To better process the data, artificial intelligence (AI) techniques are used to personalize the patient's recommendation and to create decision support expert system with self-adjustment. The focus of this paper is the system’s architecture and use case scenarios of a research project aimed to develop an Artificial Intelligence expert system based on continuous glucose monitoring (CGM) data. The project is developed by a mixed team composed by medical doctors in France and computer science specialists in Romania. By regularly analyze and interpret the CGM data, the decision support expert system will generate detailed reports for doctor and patient use and real-time alerts. One of the key components is the implementation of an auto-adaptive adjustment therapy protocol for type 1 diabetes treatment in children.",https://ieeexplore.ieee.org/document/9657707/,2021 International Conference on e-Health and Bioengineering (EHB),18-19 Nov. 2021,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICACITE51222.2021.9404749,Artificial Intelligence and Robotics: Impact & Open issues of automation in Workplace,IEEE,Conferences,"In engineering province robotics is one of the cognitive perspective to human communication or it concern with synod of perception of action. In Today's Tech World Artificial Intelligence is an essential tool which provides effective analytical business solutions & plays significant role in the domain of robotics and have several similarities like human behavior which may drive the real world. This paper shows the significant blend of Artificial Intelligence and robotics which transform entire industries, technological improvement of robotics application & utilization. It also focuses on different aspects of targets like marketing, home appliances, medical science, Smart agriculture and many more which includes open issues and technological challenges arises by this combination and conclude that robotics with AI can work in real world with real objects. Further AI based robotics are very important area in economics and organizational consequence, implementation of automation in any organizational design give impact on overall economy and infrastructure provide a wider direction for further research on Robotics and IoT are two terms each covering a myriad of technologies and concepts.",https://ieeexplore.ieee.org/document/9404749/,2021 International Conference on Advance Computing and Innovative Technologies in Engineering (ICACITE),4-5 March 2021,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/I-SMAC.2018.8653700,Artificial Neural Network for Prediction of Breast Cancer,IEEE,Conferences,"Technologies play a vital role in cancer care. Data mining approach has helped a lot in medical science due to its high efficiency in the prediction of the future health condition, and also helps in reduction of medical cost and improving the health of people and quality in real time which helps in saving lives' of people. Breast Tissue is the reason due to which breast cancer develops. It takes the second place for the most cancer diagnoses in the women after skin cancer. There are humungous data and information which gives an opportunity for analysing and understanding the process and make some researches using machine learning techniques. The main part of this paper is to create a tool for early prediction of breast cancer with the highest accuracy possible and low error rate. This was done by applying machine learning algorithms and with help of Artificial Neural Network (ANN) using Wisconsin Breast Cancer (Diagnostic) Dataset. Experimental results show that ANN gives the accuracy up to 98% with low error rate. The Experiment is conducted using Dev.-C++ software and implemented using C-language.",https://ieeexplore.ieee.org/document/8653700/,"2018 2nd International Conference on I-SMAC (IoT in Social, Mobile, Analytics and Cloud) (I-SMAC)I-SMAC (IoT in Social, Mobile, Analytics and Cloud) (I-SMAC), 2018 2nd International Conference on",30-31 Aug. 2018,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/TENCON.2018.8650180,Automata Generating Mobile Application (A.G.M.A),IEEE,Conferences,"Finite state machine is a model of computation based on a theoretical machine, made of one or more states. Computing Science students and practitioners use it to model real world objects and simulate problems in several disciplines such as mathematics, artificial intelligence, games or linguistics. Based on the assessment made by the proponents, little percentage of the students who took the course were able to retain knowledge on its theoretical foundation, while none on the application side. It was further supported by some literature that some computing students find difficulties in learning this course. The main reason is the knowledge retention because they cannot remember their previous lessons. This research can be a useful contribution in designing a better e-learning application particularly for the Computing Science students. Given the right feature, the mobile application can benefit both the educational institution, students and faculty members. In view of this study, it is highly recommended to pursue the development of the application, implement it and conduct a pre- and post-test assessment on the knowledge retention of students who will utilize this system. Provide a wider population and test other parameters to determine what really affects the student's knowledge retention.",https://ieeexplore.ieee.org/document/8650180/,TENCON 2018 - 2018 IEEE Region 10 Conference,28-31 Oct. 2018,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/EDOCW49879.2020.00022,Automatic Generation of Conceptual Enterprise Models,IEEE,Conferences,"The world organizations operate in is becoming increasingly complex and uncertain, due to technical disruptions, changes in operational environments and social structures, global markets, or, as recently seen, through global diseases. Those changes require the development of smart technological and human systems, based on Conceptual Enterprise Models. Through the increasing complexity, modeling is difficult for human actors and therefore requiring machine-support, for example through Artificial Intelligence approaches, leading to Hybrid Intelligence systems and automated generation of Conceptual Models. While various approaches for the automated generation of Conceptual Enterprise Models exist, for example in requirements engineering, process mining or in the context of Digital Twins, the approaches mostly support only parts of the modeling process. Additionaly, the approaches lack a common base, such as standardized frameworks. Therefore, my further research aims at showing how conceptual modeling can be automated, developing a modeling standard for Digital Twins in enterprise context, showing its technical feasibility and evaluating the implementation in real-world use cases. The research is performed through design science research, systematic literature research, argumentative-deductive reasoning, prototyping and use cases. At this stage, a first literature review is completed, and the next research steps are in preparation.",https://ieeexplore.ieee.org/document/9233131/,2020 IEEE 24th International Enterprise Distributed Object Computing Workshop (EDOCW),5-5 Oct. 2020,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/FIE.2006.322654,Autonomous Robots as a Generic Teaching Tool,IEEE,Conferences,"An undergraduate bioengineering laboratory course using small autonomous robots has been developed to demonstrate control theory, learning, and behavior. The lab consists of several modules that demonstrate concepts in classical control theory, fuzzy logic, neural network control, and genetic algorithms. The autonomous agents are easy-to-build, inexpensive kit robots. Each robot functions independently in a real-world environment. Students program and retrieve data wirelessly using handheld computers. The hands-on nature of the lab modules engages students in ways that lectures, readings and software simulations cannot. By interacting with these robots, students directly experience the effects of unexpected environmental factors on designs and deviations from software simulations. The robots are easily adapted for use in many different aspects of two-year college and K-12 STEM education. Students are motivated to understand engineering, math and science principles in order to control the robots. Examples of use of the robots and modules by a local community college are presented",https://ieeexplore.ieee.org/document/4117154/,Proceedings. Frontiers in Education. 36th Annual Conference,27-31 Oct. 2006,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCICC53683.2021.9811311,Autonomous Software Requirement Specifications towards AI Programming,IEEE,Conferences,"Autonomous software requirement specifications and code generation are not only an ultimate goal of AI Programming (AIP), but also a persistent challenge to theories and technologies of software engineering. A cognitive system is demanded to autonomously elicit and rigorously refine software requirements in order to generate a set of formal specifications as the front-end of AIP. This paper presents a novel methodology for the design of an Intelligent Tool for Autonomous Software Specifications (ITASS) based on latest advances in software science and intelligent mathematics. ITASS is implemented as an interactive system for capturing software requirements and generating mathematic-based specifications for code generation in the back-end of the AIP system. The ITASS methodology and experiments are demonstrated for solving real-world and complex software engineering problems enabled by the AIP theories underpinned by intelligent mathematics.",https://ieeexplore.ieee.org/document/9811311/,2021 IEEE 20th International Conference on Cognitive Informatics & Cognitive Computing (ICCI*CC),29-31 Oct. 2021,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1049/cp.2012.1047,Autority aware expert search: Algorithm and system for NSFC,IET,Conferences,"This paper describes the peer reviewer finding algorithm used in an aided system to assist the proposal reviewing task in the National Science Foundation of China (NSFC)[1]. We propose a new probabilistic language model in which expert authority is taken into consideration, by introducing a prior probability of candidate into the model. Application codes representing research areas are defined by NSFC and are available for both experts and proposals. We integrate code information into our model as we add a code probability term into the model, thus promote rankings of experts with matched code. All proposals submitted to NSFC are written in Chinese, and since Chinese word segmentation is non-trivial task whose accuracy affects the final results heavily, we try to improve segment accuracy by adding domain-specific terms to a user-editable dictionary. Terminologies are extracted from certain field of bibliography data in NSFC system. Experiments show that our algorithm is effective in real world system like NSFC.",https://ieeexplore.ieee.org/document/6492654/,International Conference on Automatic Control and Artificial Intelligence (ACAI 2012),3-5 March 2012,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/DSAA.2019.00070,"Bighead: A Framework-Agnostic, End-to-End Machine Learning Platform",IEEE,Conferences,"With the increasing need to build systems and products powered by machine learning inside organizations, it is critical to have a platform that provides machine learning practitioners with a unified environment to easily prototype, deploy, and maintain their models at scale. However, due to the diversity of machine learning libraries, the inconsistency between environments, and various scalability requirement, there is no existing work to date that addresses all of these challenges. Here, we introduce Bighead, a framework-agnostic, end-to-end platform for machine learning. It offers a seamless user experience requiring only minimal efforts that span feature set management, prototyping, training, batch (offline) inference, real-time (online) inference, evaluation, and model lifecycle management. In contrast to existing platforms, it is designed to be highly versatile and extensible, and supports all major machine learning frameworks, rather than focusing on one particular framework. It ensures consistency across different environments and stages of the model lifecycle, as well as across data sources and transformations. It scales horizontally and elastically in response to the workload such as dataset size and throughput. Its components include a feature management framework, a model development toolkit, a lifecycle management service with UI, an offline training and inference engine, an online inference service, an interactive prototyping environment, and a Docker image customization tool. It is the first platform to offer a feature management component that is a general-purpose aggregation framework with lambda architecture and temporal joins. Bighead is deployed and widely adopted at Airbnb, and has enabled the data science and engineering teams to develop and deploy machine learning models in a timely and reliable manner. Bighead has shortened the time to deploy a new model from months to days, ensured the stability of the models in production, facilitated adoption of cutting-edge models, and enabled advanced machine learning based product features of the Airbnb platform. We present two use cases of productionizing models of computer vision and natural language processing.",https://ieeexplore.ieee.org/document/8964147/,2019 IEEE International Conference on Data Science and Advanced Analytics (DSAA),5-8 Oct. 2019,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ACOMP.2019.00011,Blueprinting the Workflow of Medical Diagnosis through the Lens of Machine Learning Perspective,IEEE,Conferences,"The association of machine learning into medical data and healthcare communities embraces substantial improvement in both health care and machine learning itself. Many companies are racing to integrate machine learning into medical diagnosis process that boosts the automatic medical decision, reducing the inferior effects of data overload and increasing the accurate prediction and time effectiveness. It is one of today's most rapidly growing technical fields, lying at the intersection between health care and computer science in general. Thus, there is an urgent need to optimize medical processes, guidelines and workflows to increase the workload capacity while reducing costs and improving efficiencies. Moreover, no medical doctor or experts can manually keep pace today due to increasingly large and complex datasets. In this paper, the authors aim at addressing the mentioned issue by proposing a workflow of medical diagnosis through the lens of the machine learning perspective. An intensive comparison has been conducted applying 5 well-known machine learning algorithms on 8 real-world categorized datasets. A mobile application has been also deployed to enhance the incorporation from hospital experts.",https://ieeexplore.ieee.org/document/9044271/,2019 International Conference on Advanced Computing and Applications (ACOMP),26-28 Nov. 2019,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CSCS52396.2021.00073,Bluetooth Communications in Educational Robotics,IEEE,Conferences,"In a world in a continuous and rapid change, it is absolutely necessary for our students to keep up with the rapid progress of new technologies: Internet of Things (IoT), Robotics, Artificial Intelligence (AI), Virtual Reality (VR), Augmented Reality (AR) etc. The rapid evolution and diversification of these emerging technologies has recently led to their introduction into the educational offer of the school curriculum for the gymnasium. The discipline of Information and Communication Technology (ICT) has already been implemented, a discipline that involves both the formation of skills to use new technologies and the formation of computational thinking necessary for the efficient and intelligent use of these technologies. In order to teach and learn Physics from a STEM (Science, Technology, Engineering and Mathematics) educational perspective, we initiated optional school courses of IoT, Robotics and AI (approached through Machine Learning). These courses stimulate, at the level of students, computational thinking, creativity and innovation and lead, from an interdisciplinary perspective, to the development of emerging specializations such as Mathematics-Physics-Automation, Mathematics-Physics-Electronics, Mathematics-Physics-Informatics-Robotics etc. In this paper we presented a method of approaching, in the school educational space, the study of wireless communication technologies between smart devices, through an Educational Robotics project. The project consisted of creating a wireless controlled mobile robotic platform (robot car) via a Bluetooth module connected to an Arduino Uno board.",https://ieeexplore.ieee.org/document/9481012/,2021 23rd International Conference on Control Systems and Computer Science (CSCS),26-28 May 2021,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CBI.2013.34,Business and Information Systems Engineering -- In Quest for Research and Education Agenda in Europe,IEEE,Conferences,"The new complex digital and information services systems and industries are calling for new transdisciplinary approaches on how to achieve technical, social, and cultural knowledge and skills to serve future needs of the industry and society. The leading digital industry is in demand of engineer's with knowledge and capabilities to collaborate and move between highly complex digital business and technical systems domains. The new reality is under constant transformation, highly intangible and nonlinear interconnected system. Transformative digital business innovations, rapidly evolving business models, architectures enabled business model scalability and ultra large-scale of systems are the new characteristics of this software-dominant-logic. This paper aims at covering contemporary challenges of the interdisciplinary in business, service, software and systems engineering by analyzing different research ontologies and curriculum models. The paper analyses recent Computer Science (CS) and Information Systems (IS) curriculum developments and reflects through different ontologies and recent research. Paper uses Service Design and Engineering (SDE) as a didactic and curriculum model for future Information Systems Engineering (ISE) and Business and Information Systems Engineering (BISE). The new curriculums serve the needs of global information start-ups, businesses, governments, and societies. This paper aims at describing the ontological foundations and conflicts, the axiology of the new curriculum model and proposes an integrated multi-ontology as the foundation for BISE new curriculum.",https://ieeexplore.ieee.org/document/6642875/,2013 IEEE 15th Conference on Business Informatics,15-18 July 2013,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AICCSA50499.2020.9316494,Challenges and Opportunities for Composable AI-Integrated Applications at the Digital Continuum: Keynote,IEEE,Conferences,"Summary form only given. The complete presentation was not made available for publication as part of the conference proceedings. Cyberinfrastructure is everywhere in diverse forms. From IoT to extreme scale computing, data and computing has never been as distributed with potential for real-time integration via fast networking and container management. The growth of new processors over the last decade including GPUs, FPGAs, and edge accelerators opened the way to a diverse set of applications using machine learning on top of distributed nontraditional hardware. The common theme to these applications, mostly composed of artificial intelligence (AI) workloads, is their need to run in specialized environments for reasons such as on demand or 24×7 nature of the tasks they are performing, and difficulties regarding their portability, latency, privacy and performance optimization. In many data-driven scientific applications there is a need for integration of these AI-workloads with traditional high-throughput computing (HTC) or high-performance computing (HPC) tasks for AI-integrated science. This talk will discuss example AI-integrated applications, describe some of the new systems that enabled these applications, and overview our recent research to enable composable applications including an application development methodology, intelligent middleware and workflow composition.",https://ieeexplore.ieee.org/document/9316494/,2020 IEEE/ACS 17th International Conference on Computer Systems and Applications (AICCSA),2-5 Nov. 2020,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/FiCloudW.2017.79,Changes of Cyber-Attacks Techniques and Patterns after the Fourth Industrial Revolution,IEEE,Conferences,"In this paper, we predicted the changes of cyber-attacks techniques and patterns after the fourth industrial revolution with the epochal shift of information and communication technology and innovation of science and technology. Cyber space will be hyper-connection, cross-domain, and super intelligence space as connecting everything in the world due to a fusion of information and communication technologies such as artificial intelligence, internet of things, and cyber-physical systems. Cyber-attacks will use all electronic devices including wireless or wire networks, hardware, software, and cyber-physical systems as a route. The hacking tool's functions will evolve into a variety of forms reflecting human thought and behavioral procedures. The attack target will not be limited to a specific object. The purpose of the cyber-attack is to focus on secondary effects and indirect attacks as well as direct attacks.",https://ieeexplore.ieee.org/document/8113773/,2017 5th International Conference on Future Internet of Things and Cloud Workshops (FiCloudW),21-23 Aug. 2017,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/REET51203.2020.00007,Chatbot-based Interview Simulator: A Feasible Approach to Train Novice Requirements Engineers,IEEE,Conferences,"Introduction: Although the interview is the most important and widely used requirements elicitation technique, novice engineers do not receive adequate training in Requirements Engineering (RE) courses. Objectives: Develop an AI-based interview simulator for helping novice requirements engineers in gaining interview skills. Methods: The research is based on the Design Science Methodology for Information Systems. The simulator is the outcome of six cycles; in each cycle, a proof of concept with additional features is created. Each cycle finishes with evaluation and improvement suggestions. Results: The simulator has been tested with students and results have been promising. The interview simulator understands context-free questions, retrieving the right information related to RE concepts such as goals, tasks, users, benefits, and constraints. The simulator also answers questions based on the context, makes summaries of the conversation, responds to meta-questions, and adds ambiguity and incompleteness to the conversation. Conclusions: The results have been promising. The simulator has been tested with degree and master level students. They were able to create a requirements specification using the simulator, and the feedback has been generally positive. The simulator will be tested in a real RE course in the academic year 2020-2021. Once it proves effective in the classroom, it will be opened to the RE community for free use and improvement.",https://ieeexplore.ieee.org/document/9216178/,2020 10th International Workshop on Requirements Engineering Education and Training (REET),31-31 Aug. 2020,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICIP42928.2021.9506200,Classification of RIGID and Non-Rigid Transformations with Autoencoder Representations,IEEE,Conferences,"Feature matching in transformed images is critical to many fields of computer science, from autonomous robots to video analysis. However, most widely used feature matching algorithms vary in their ability to track features depending on whether rigid or non-rigid image transformations occur. This makes it critical, especially in real-time calculations, to be able to identify what kind of transformation is taking place quickly in order to deploy the best feature matching algorithm for that type of transformation. The proposed research uses a combined autoencoder and neural network classification model to classify rigid or non-rigid transformations in order to improve feature matching on the image pairs. This system is the first to perform this kind of analysis with representation learning and opens new ways to improving feature matching performance. We show that using this method improves the amount of feature matches found between correctly identified image pairs.",https://ieeexplore.ieee.org/document/9506200/,2021 IEEE International Conference on Image Processing (ICIP),19-22 Sept. 2021,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/Informatics47936.2019.9119306,Classification of Tire Tread Images by Using Neural Networks,IEEE,Conferences,"Image classification represents a complex problem which occurs in variety of science and real life applications. Neural network is one of the excellent tools which could be used to solve this problem. The research presented in this paper is motivated by several consultations with active crime scene investigators. During these consultations, it became apparent, that criminology department is in critical need of advanced software for tire tread print identification. The current software used by crime scene investigators is outdated, time demanding and hard to work with. Application for tire tread print identification should be one of results of project created on the basis of our work with criminology department. This paper presents one of the first steps of this identification - tire tread position identification.",https://ieeexplore.ieee.org/document/9119306/,2019 IEEE 15th International Scientific Conference on Informatics,20-22 Nov. 2019,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AFRICON46755.2019.9134020,Cloud-Based Business Intelligence for a Cellular IoT Network,IEEE,Conferences,"This paper presents a cloud-based business intelligence (BI) implementation for a cellular Internet of Things (IoT) network. A Design Science Research (DSR) paradigm, combined with elaborated Action Design Research (eADR) was used to ensure a workable artifact is delivered. The real-world problem is that, in the cellular network considered here, network health status was not initially visible in an intelligent and actionable way. The network health status is used to ensure service availability and includes different health indicators, of which measurements are made at regular intervals. Not all IoT edge devices have health indicators available, but the network under evaluation provided sufficient data from which to extract anomalies. Experiments were conducted to identify the most appropriate anomaly detection technique from three options, namely SARIMA, SVM and LSTM techniques. Anomalies were linked to system operational failures, in turn to be addressed by appropriate standard operating procedures of a larger main-tenance system. Finally, a clustering algorithm was evaluated for automated recognition of anomalous events, showing that anomalies may be clustered in a useful way using the Mean-Shift clustering algorithm, and also identifying additional health indicators that support anomaly classification.",https://ieeexplore.ieee.org/document/9134020/,2019 IEEE AFRICON,25-27 Sept. 2019,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICeLeTE.2013.6644359,Clustering moodle data as a tool for profiling students,IEEE,Conferences,"This paper describes the first step of a research project with the aim of predicting students' performance during an online curriculum on a LMS and keeping them from falling behind. Our research project aims to use data mining, machine learning and artificial intelligence methods for monitoring students in e-learning trainings. This project takes the shape of a partnership between computer science / artificial intelligence researchers and an IT firm specialized in e-learning software. We wish to create a system that will gather and process all data related to a particular e-learning course. To make monitoring easier, we will provide reliable statistics, behaviour groups and predicted results as a basis for an intelligent virtual tutor using the mentioned methods. This system will be described in this article. In this step of the project, we are clustering students by mining Moodle log data. A first objective is to define relevant clustering features. We will describe and evaluate our proposal. A second objective is to determine if our students show different learning behaviours. We will experiment whether there is an overall ideal number of clusters and whether the clusters show mostly qualitative or quantitative differences. Experiments in clustering were carried out using real data obtained from various courses dispensed by a partner institute using a Moodle platform. We have compared several classic clustering algorithms on several group of students using our defined features and analysed the meaning of the clusters they produced.",https://ieeexplore.ieee.org/document/6644359/,2013 Second International Conference on E-Learning and E-Technologies in Education (ICEEE),23-25 Sept. 2013,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICDCS.2018.00118,Cognified Distributed Computing,IEEE,Conferences,"Cognification - the act of transforming ordinary objects or processes into their intelligent counterparts through Data Science and Artificial Intelligence - is a disruptive technology that has been revolutionizing disparate fields ranging from corporate law to medical diagnosis. Easy access to massive data sets, data analytics tools and High-Performance Computing (HPC) have been fueling this revolution. In many ways, cognification is similar to the electrification revolution that took place more than a century ago when electricity became a ubiquitous commodity that could be accessed with ease from anywhere in order to transform mechanical processes into their electrical counterparts. In this paper, we consider two particular forms of distributed computing - Data Centers and HPC systems - and argue that they are ripe for cognification of their entire ecosystem, ranging from top-level applications down to low-level resource and power management services. We present our vision for what ""Cognified Distributed Computing"" might look like and outline some of the challenges that need to be addressed and new technologies that need to be developed in order to make it a reality. In particular, we examine the role cognification can play in tackling power consumption, resiliency and management problems in these systems. We describe intelligent software-based solutions to these problems powered by on-line predictive models built from streamed real-time data. While we cast the problem and our solutions in the context of large Data Centers and HPC systems, we believe our approach to be applicable to distributed computing in general. We believe that the traditional systems research agenda has much to gain by crossing discipline boundaries to include ideas and techniques from Data Science, Machine Learning and Artificial Intelligence.",https://ieeexplore.ieee.org/document/8416381/,2018 IEEE 38th International Conference on Distributed Computing Systems (ICDCS),2-6 July 2018,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/HICSS.1996.495429,Combining neural networks with other prediction techniques,IEEE,Conferences,"The complexity and the inherent heterogeneity of real world problems are still one of the major challenges in computer science. Due to the necessity of using different data processing technologies general interest in hybrid systems is a fast growing research area. To support the integration of intercommunicating hybrids the paper suggests the use of distributed software architectures. The main advantages of the approach presented in the paper are the encapsulation of different paradigms, the separation of control and domain knowledge and the reduction of the complexity of individual problem solvers. The first section of the paper gives an overview of the state of the art in hybrid processing. A taxonomy of currently known hybrid system approaches is discussed. Because of the special importance of distributed artificial intelligence (DAI) the author examines issues and research directions in this field and concludes with the presentation of DAI as an integrative paradigm. The author then describes a case study. He gives an overview of the domain (economics) and discuss some prediction methods usually used in this area. After introducing some simple economic relationships and a description of how to use neural networks in multivariate prediction the paper shows how connectionist techniques are embedded in a distributed problem solving framework, called PREDICTOR. The last section shows an example in which a hybrid system out performs the homogenous approaches by combining them intelligently. PREDICTOR is a case study of how to design a so called intercommunicating hybrid system.",https://ieeexplore.ieee.org/document/495429/,Proceedings of HICSS-29: 29th Hawaii International Conference on System Sciences,3-6 Jan. 1996,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICICIP.2011.6008299,Comparison on driving behavior between manned and unmanned ground vehicles,IEEE,Conferences,"In order to enhance the traffic road safety, applying the artificial intelligence theory and the technology in road traffic control system, that is intelligent driving vehicle, may effectively decrease traffic accidents, which are caused by drivers' neglect, fatigue and so on. This article is based on cognitive science to analyze driving behaviors. First, we estimated the Information Processing Model of Intelligent Vehicle. Second, based on prototype pattern recognition theory, we also built Pattern Recognition Model of Intelligent Driving Behavior, which provided the advantageous basis for the further research on intelligent vehicle. Third, virtual scene simulation system was established by using Creator and Vega Prime. Finally, according to test data, we investigated the features of speed and latitudinal distance in both artificial and intelligent U-Turn driving behavior. Our results describe that the intelligent vehicle does have the ability of cognizance, and it can make correct reactions corresponding to the driving environment, but its intelligence still needs improving.",https://ieeexplore.ieee.org/document/6008299/,2011 2nd International Conference on Intelligent Control and Information Processing,25-28 July 2011,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AICAI.2019.8701407,Comparitive Analysis and Implication of UAV and AI in Forensic Investigations,IEEE,Conferences,"Unmanned Aerial Vehicles (UAVs) are having applications in different areas including remote sensing, real-time monitoring, environmental monitoring, agriculture, land use surveys, traffic surveillance, metereology and goods deleivery etc. In this study a comparative analysis is being made on various investigation techniques in forensic science with and without the use of UAVs. The analysis and impact of telecommunication and related aspects of a UAV is being examined in the area of forensic science to investigate the complex crime scenes more sophisticatedly and to help the investigators and forensic professionals in the proper scrutiny and mapping of the scene of crime. The present study is focused on the estimation of the practicality of the use of UAV and AI in crime scene investigation (CSI), feasibility of UAV in forensic science and its logistical implementation.",https://ieeexplore.ieee.org/document/8701407/,2019 Amity International Conference on Artificial Intelligence (AICAI),4-6 Feb. 2019,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCIT.2007.152,Concept Analysis of OWL Ontology Based on the Context Family Model,IEEE,Conferences,"The core of the semantic Web is ontology, which supports interoperability among semantic Web applications and enables developer to reuse and share domain knowledge. The process of building an ontology is a high-cost process. The reality is that the construction of ontologies is an art rather than a science. Therefore, methodologies and supporting tools are essential to help the developer construct suitable ontologies for the given purposes and to verify the ontology its fitness of purpose and its reusability. In this paper we propose a novel approach for analyzing ontologies based on the formal concept analysis (FCA) with context family model and build a new tool that extracts main elements (class, property and individual etc.) from the source code of Web ontology language (OWL) and then detects some structural problems. By using the tool, ontology developer can build and/or reconstruct ""well-defined"" and ""good"" ontologies.",https://ieeexplore.ieee.org/document/4420373/,2007 International Conference on Convergence Information Technology (ICCIT 2007),21-23 Nov. 2007,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICDMW.2018.00152,Constructing Graph Node Embeddings via Discrimination of Similarity Distributions,IEEE,Conferences,"The problem of unsupervised learning node embeddings in graphs is one of the important directions in modern network science. In this work we propose a novel framework, which is aimed to find embeddings by discriminating distributions of similarities (DDoS) between nodes in the graph. The general idea is implemented by maximizing the earth mover distance between distributions of decoded similarities of similar and dissimilar nodes. The resulting algorithm generates embeddings which give a state-of-the-art performance in the problem of link prediction in real-world graphs.",https://ieeexplore.ieee.org/document/8637376/,2018 IEEE International Conference on Data Mining Workshops (ICDMW),17-20 Nov. 2018,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/BDEE52938.2021.00029,Construction and Implementation of Embodied Mixed-Reality Learning Environments,IEEE,Conferences,"As the research paradigm of cognitive science changes from disembodied to embodied, people realize that the body and its interaction with the environment have an important effect on learning activities more and more intensely. Focus on obtaining physical experience, embodied learning encourages the participation of the body’s sensor-motor system and the multimodal interaction between the body and the environment, and it puts forward some new requirements for the design and construction of learning environments. At the same time, the development of artificial intelligence, mixed reality, human-computer interaction and other technologies in recent years has provided a good support for the construction and application of embodied learning environments, especially the functions of virtual-real fusion, natural interaction and dynamic generation of the mixed reality. For this reason, we proposed to design embedded mixed -reality learning environments (EMRLE) for supporting and promoting embodied activities by using these technologies and the embodied cognitive theory. Through combing the origin and background of EMRLE, on the one hand, we can analyze the construction goals, basic characteristics, construction models and realization methods of EMRLE from two dimensions of theory and technology; on the other hand, combined with the existing relevant research and application status in the world, the main forms of EMRLE in education and teaching practice and its teaching activity design cases can be summarized to further demonstrate and analyze the necessity, rationality and technical feasibility of EMRLE construction. For this reason, embodied learning activities designed on the basis of building EMRLE also provide new ideas for promoting the innovation of teaching and learning forms.",https://ieeexplore.ieee.org/document/9626334/,2021 International Conference on Big Data Engineering and Education (BDEE),12-14 Aug. 2021,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICBASE53849.2021.00027,Construction and Research of Virtual Forest Environment Based on Spatial Data,IEEE,Conferences,"Virtual Forest environment is a virtual geographical environment in the application of scientific forestry practices; it is a combination of forestry science and virtual geographical environment. Based on forestry spatial data and virtual reality technology, it constructs forest objects and expresses extremely complex forest phenomena. This article introduces how to create a virtual environment object of forest, forest-related technology, the virtual environments research, and pointes out the importance of creating a virtual forest environment.",https://ieeexplore.ieee.org/document/9696141/,2021 2nd International Conference on Big Data & Artificial Intelligence & Software Engineering (ICBASE),24-26 Sept. 2021,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SMAP49528.2020.9248454,Crowd Sourcing as an Improvement of N-Grams Text Document Classification Algorithm,IEEE,Conferences,"A common task in a world of natural language processing is text classification useful for e.g. spam filters, documents sorting, science articles classification or plagiarism detection. This can still be done best and most accurately by human, on the other hand, we can of ten accept certain error in the classification in exchange for its speed. Here, natural language processing mechanism transforms the text in natural language to a form understandable by a classifier such as K-Nearest Neighbour, Decision Trees, Artificial Neural Network or Support Vector Machines. We can also use this human element to help automated classification to improve its accuracy by means of crowdsourcing. This work deals with classification of text documents and its improvement through crowdsourcing. Its goal is to design and implement text documents classifier prototype based on documents similarity and to design evaluation and crowdsourcing-based classification improvement mechanism. For classification the N-grams algorithm has been chosen, which was implemented in Java. Interface for crowdsourcing was created using CMS WordPress. In addition to data collection, the purpose of interface is to evaluate classification accuracy, which leads to extension of classifier test data set, thus the classification is more successful. We have tested our approach on two data sets with promising preliminary results even across different languages. This led to a real-world implementation started at the beginning of 2019 in cooperation of two universities: VSB-TUO and OSU.",https://ieeexplore.ieee.org/document/9248454/,2020 15th International Workshop on Semantic and Social Media Adaptation and Personalization (SMA,29-30 Oct. 2020,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AIVR46125.2019.00070,CrowdAR Table - An AR Table for Interactive Crowd Simulation,IEEE,Conferences,In this paper we describe a prototype implementation of an augmented reality (AR) system for accessing and interacting with crowd simulation software. We identify a target audience and tasks (access to the software in a science museum) motivate the choice of AR system (an interactive table complemented with handheld AR via smartphones) and describe its implementation. Our system has been realized in a prototypical implementation verifying its feasibility and potential. Detailed user testing will be part of our future work.,https://ieeexplore.ieee.org/document/8942269/,2019 IEEE International Conference on Artificial Intelligence and Virtual Reality (AIVR),9-11 Dec. 2019,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCE-Berlin47944.2019.8966202,Data generators: a short survey of techniques and use cases with focus on testing,IEEE,Conferences,"The process of data generation plays a significant role in various areas of computer science. Software testing is probably the seminal example for usage of artificially created data. An appropriate data generator is suitable and necessary for almost every type of testing (including automated): the regression tests, null value tests, coverage, security and performance test. With the rise of data science, the data generation is as well used in machine learning, data mining, and data visualization. Other industries such as financial and health-care have great benefits from artificial data as well. Important aspect of the generated data is that the data needs to be realistic but not real, which embrace the confidentiality and privacy. In this paper, we give a short survey on the different types of generators from the architecture point of view and their intended usage, as well as we list their pros and cons. Finally, we give an overview of the used data generation algorithms and the best practices in different areas.",https://ieeexplore.ieee.org/document/8966202/,2019 IEEE 9th International Conference on Consumer Electronics (ICCE-Berlin),8-11 Sept. 2019,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/BigData.2018.8622509,Deep Learning for Enhancing Fault Tolerant Capabilities of Scientific Workflows,IEEE,Conferences,"In the history of Computer Science, the act of `delegation' has been the greatest multiplier of society's problem-solving ability. A scientist working on detecting anomalies in a phenomenon, does not need to re-invent matrix multiplication techniques to solve her problem. Scientific workflows provide ultimate `delegation' mechanism - where a domain scientist can completely forget the specifics of `how' her program will execute on a large cluster in an efficient and cost-effective manner and can instead focus on the mathematical formulation and theoretical robustness of her solution. We present here an approach that directly aims to make the execution of Scientific Workflows more reliable, robust and efficient. We aim that the work presented in this paper will propel the larger effort, from the scientific workflow community, of making scientific workflow execution as simple, efficient and robust as a JOIN operation in a modern database. Specifically, we apply Deep Learning techniques to develop a mechanism that forecasts the final state (success or failure) of a dynamic job in a large-scale particle physics experiment, with minimal data gathering, and as early as possible in job's life cycle. The key advantage of having a predictive mechanism to identify and anticipate failure-prone jobs is the potential for designing intelligent Fault Tolerance mechanisms to handle anomalous events. We achieve a 14% improvement in computational resources utilization, and an overall classification accuracy of 85% on real tasks executed in a High Energy Physics Computing workflow. To the best of our knowledge, this is the most exhaustive and first of its kind study of neural network architectures in context of a real-dataset profiled from a large-scale scientific workflow.",https://ieeexplore.ieee.org/document/8622509/,2018 IEEE International Conference on Big Data (Big Data),10-13 Dec. 2018,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ETFA.2019.8869108,Deployment of territorial structures to reduce gender gap in technology and some real cases in Catalonia,IEEE,Conferences,"Gender gap in computer science related fields is one of the most critical among all technological areas. This paper describes the methodology used to create gender commissions and providing them of a territorial structure around informatics engineering and artificial intelligence, two close areas where the presence of women is critical both at professional level, as well as training level. Territorial structure provides an additional potential very useful for the impact of the activities of the commission. Here the creation of two gender structures in Catalonia is presented.",https://ieeexplore.ieee.org/document/8869108/,2019 24th IEEE International Conference on Emerging Technologies and Factory Automation (ETFA),10-13 Sept. 2019,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/MLBDBI54094.2021.00110,Design and Application of Rehabilitation Psychology Practical Teaching System Based on VR Technology,IEEE,Conferences,"Research purposes: The characteristics of VR technology, such as immersion, interaction, construction and sociality, make its application in the field of education have unique advantages. How to make use of the advantages of VR technology to design a functional, purposeful and interesting practical experience teaching in the teaching of Rehabilitation Psychology is an important research direction of psychology curriculum reform. Based on the innovative teaching mode of Rehabilitation Psychology, this paper aims to improve the teaching method and content of the course by using VR virtual reality, so as to provide reference for the reform of teaching mode. Research methods: 122 college students majoring in applied psychology and 85 college students majoring in rehabilitation medicine in our university were selected as the research objects. Based on the latest research results of computer science, psychology and other related subjects, this paper analyzes and compares the teaching scheme design, implementation and results of the practical course of rehabilitation psychology with that of the traditional course of rehabilitation psychology by comprehensively using the methods of literature analysis, teaching action research, teaching experience summary and teaching reflection. Conclusion: The teaching experience system of Rehabilitation Psychology based on VR Virtual Reality is effective, so it is feasible to integrate VR Virtual Reality into the experimental teaching of psychology. This study provide a research basis for exploring innovative teaching mode and teaching reform of applied psychology specialty.",https://ieeexplore.ieee.org/document/9731082/,"2021 3rd International Conference on Machine Learning, Big Data and Business Intelligence (MLBDBI)",3-5 Dec. 2021,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICSSE.2019.8823516,Design and Implementation of Chatbot Framework For Network Security Cameras,IEEE,Conferences,"In recent years, with the development of science and technology Internet Protocol camera is getting and getting popular and widely used. In this paper, we present a chatbot framework to help user get the human detection information from the cameras via Facebook messenger instead of observing 24/7 called Security Bot (Sbot). To build Sbot, we design a system including camera network, Human Detection Server (HDS), and Sbot server. In the system, Sbot transfer information between user and HDS using Facebook Messenger Platform. In the human detection task, we use SSD-MobileNet1 network architecture for detecting human in real-time and updating dataset for retraining using transfer learning method with more case taken from surveillance camera.",https://ieeexplore.ieee.org/document/8823516/,2019 International Conference on System Science and Engineering (ICSSE),20-21 July 2019,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IEEECONF53024.2021.9733780,Design and Implementation of an Educational Technology Kit Aligned to the Conceptual Framework of Educational Mechatronics,IEEE,Conferences,"Educational systems continuously seek new educational technologies that improve their students&#x0027; knowledge and help them acquire the necessary skills for the new industrial era. Studies indicate that technology plays a significant role in student performance when implemented correctly, stimulates better interactions between instructors and the students, and encourages cooperative learning, collaboration, problem-solving, and communication competencies. A necessary skill for a good learning process is spatial thinking. Studies show that students who do not have good spatial thinking will have more trouble passing science, technology, engineering, and mathematics (STEM) courses. This paper presents the design and implementation of a novel educational kit to aid in the development of spatial thinking, specifically applied to the two-dimensional Cartesian Coordinate Systems (2D-CACSET) designed, from its conception, to apply the Educational Mechatronics Conceptual Framework (EMCF). This innovative 2D-CACSET comprises two Tags, four Anchors, and a Listener that use a real-time location system based on Ultra-Wide Band (UWB) technology as a cornerstone of operation. In addition, it has a 2D dashboard and a Graphical User Interface that allow mechatronic concepts from the concrete, graphic and abstract levels of thinking. Thus, in this practice, students acquired knowledge and developed skills to apply when working with more complex prototypes throughout their lives.",https://ieeexplore.ieee.org/document/9733780/,2021 Machine Learning-Driven Digital Technologies for Educational Innovation Workshop,15-17 Dec. 2021,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/WCCIT.2013.6618739,Design and application of academic frontier-based approach in engineering courses,IEEE,Conferences,"The reform of teaching approach has been the focus of modern opening educational reform and research. The real science thrives on both revolutionary and frontier progress that the textbook can not include. Based on the analysis of the characteristics of several the engineering courses, we build the academic frontier-based approach (AFA) for the purpose of combination of the basic and academic frontier knowledge by the application of the theory of constructivist learning. AFA is a learner-centred instructional approach used to promote active and deep learning by involving learners in learning academic frontier topics in an open and collaborative environment. The design and implementation of the approach enrich the teaching mode and the content of the key curriculums in depth, have an effective role in expansion knowledge, fun learning, and dig the potential of the learners in the group and team.",https://ieeexplore.ieee.org/document/6618739/,2013 World Congress on Computer and Information Technology (WCCIT),22-24 June 2013,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/FUZZ.2003.1209341,Design enhancement by fuzzy logic in architecture,IEEE,Conferences,"Fuzzy logic systems find application especially in engineering systems due to their suitability for applications dealing with the concept of Takagi-Sugeno fuzzy model. However, the fuzzy concept is particularly valid also in the areas where information is qualitative. Exact science applications deal with the information by modeling and thereafter identifying the relationships in the model by suitable computation. In contrast, the fuzzy logic employment in soft sciences is not as straightforward as it is in exact sciences and special care should be taken in the former case. Analogous to exact sciences, the majority of soft information sources belongs to soft sciences where the quantities dealt with are usually not measurable in the engineering sense. Therefore, for soft sciences fuzzy logic is an important means for dealing with associated imprecise information processing. In spite of this, the employment of fuzzy logic in soft sciences is not common. In this work, aspects of fuzzy logic implementation in the areas of soft sciences is pointed out. This is exemplified by a design application in building technology using a soft design data set from a real life environment.",https://ieeexplore.ieee.org/document/1209341/,"The 12th IEEE International Conference on Fuzzy Systems, 2003. FUZZ '03.",25-28 May 2003,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ITAIC49862.2020.9339101,Design of Embedded Real-time Target Tracking System Based on KNN Algorithm,IEEE,Conferences,"With the rapid development of science and technology, more and more high-tech products have been applied to all aspects of life. In the field of security and defense, video monitoring plays an important role. In order to solve the problem of real-time and miniaturization of target tracking system, this paper applies a tracking algorithm based on KNN(k-nearest neighbor) background splitter combined with Kalman filter. When tracking a moving target, it is important to determine the moving target in the initial frame. In this paper, the motion detection method in KNN background segmentation is adopted to automatically initialize the moving target. Kalman filter can predict the motion state of the moving target in the next frame according to the information of the moving target in the current frame, and track the target with the predicted value as the input of CamShift algorithm. The implementation of the tracking algorithm on the embedded raspberry pi platform is beneficial to the miniaturization of smart devices. The experimental results show that the algorithm has better tracking ability, faster operation speed and stronger robustness for moving targets in raspberry pi embedded environment, which improves the performance of the algorithm and gets good effects.",https://ieeexplore.ieee.org/document/9339101/,2020 IEEE 9th Joint International Information Technology and Artificial Intelligence Conference (ITAIC),11-13 Dec. 2020,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/TENCON.2016.7848657,"Development of a predictive model for on-demand remote river level nowcasting: Case study in Cagayan River Basin, Philippines",IEEE,Conferences,"DOST-Advanced Science and Technology Institute has installed various hydro-meteorological devices, such as Automated Rain Gauge(ARG), Water Level Monitoring Stations (WLMS), and Tandem Stations, all over the Philippines since 2010. While the stations provide valuable near real-time data for monitoring major riven basins, ahead-of-time flood estimations are of interest for early warning purposes especially for local communities situated along the river basin. This study addresses the need on developing a predictive model that can provide an ahead of time nowcasting system for water level and flood hazard to provide a decision support tool for the local communities. A data driven approach using machine learning is implemented to generate ahead-of-time water level estimation. Results from the testing data shows that the resulting model was able to provide an accurate ahead of time water level prediction without relying on rainfall-runoff models.",https://ieeexplore.ieee.org/document/7848657/,2016 IEEE Region 10 Conference (TENCON),22-25 Nov. 2016,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SERVICES.2019.00036,Dimensional Situation Analytics: An Introduction and Its Application Prospects,IEEE,Conferences,"Dimensional situation analytics provides a formal framework to analyze situations from Data Information Knowl-edge Wisdom (DIKW) point of view. To date, the advent of big data driven applications opens up many frontiers in artificial intelligence and computer science, and yet it also raises a series of challenges due to theirs empirical and experimental nature. In particular, to systematically and analytically understand the logical connection through which intelligence and knowledge is derived from data and information deterministically is one of the far-reaching future objectives in computer science. Starting from an earlier result on Dimensional Situation Analytics (DSA), where the initial efforts targeted at the integration of situations and DIKW ontology, this paper brings the prospect of real world applications of DSA into perspective. A good example is UAV path planning for efficient radiation detection and monitoring, which links the pervious result, i.e., the DSA formal framework, with real world experimentation and thereof further explores on the effectiveness and future work for the DSA.",https://ieeexplore.ieee.org/document/8817250/,2019 IEEE World Congress on Services (SERVICES),8-13 July 2019,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/MLHPC49564.2019.00013,DisCo: Physics-Based Unsupervised Discovery of Coherent Structures in Spatiotemporal Systems,IEEE,Conferences,"Extracting actionable insight from complex unlabeled scientific data is an open challenge and key to unlocking data-driven discovery in science. Complementary and alternative to supervised machine learning approaches, unsupervised physics-based methods based on behavior-driven theories hold great promise. Due to computational limitations, practical application on real-world domain science problems has lagged far behind theoretical development. However, powerful modern supercomputers provide the opportunity to narrow the gap between theory and practical application. We present our first step towards bridging this divide - DisCo - a high-performance distributed workflow for the behavior-driven local causal state theory. DisCo provides a scalable unsupervised physics-based representation learning method that decomposes spatiotemporal systems into their structurally relevant components, which are captured by the latent local causal state variables. In several firsts we demonstrate the efficacy of DisCo in capturing physically meaningful coherent structures from observational and simulated scientific data. To the best of our knowledge, DisCo is also the first application software developed entirely in Python to scale to over 1000 machine nodes, providing good performance along with ensuring domain scientists' productivity. Our capstone experiment, using newly developed and optimized DisCo workflow and libraries, performs unsupervised spacetime segmentation analysis of CAM5.1 climate simulation data, processing an unprecedented 89.5 TB in 6.6 minutes end-to-end using 1024 Intel Haswell nodes on the Cori supercomputer obtaining 91% weak-scaling and 64% strong-scaling efficiency. This enables us to achieve state-of-the-art unsupervised segmentation of coherent spatiotemporal structures in complex fluid flows.",https://ieeexplore.ieee.org/document/8950692/,2019 IEEE/ACM Workshop on Machine Learning in High Performance Computing Environments (MLHPC),18-18 Nov. 2019,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IPDPS.2018.00068,Do Developers Understand IEEE Floating Point?,IEEE,Conferences,"Floating point arithmetic, as specified in the IEEE standard, is used extensively in programs for science and engineering. This use is expanding rapidly into other domains, for example with the growing application of machine learning everywhere. While floating point arithmetic often appears to be arithmetic using real numbers, or at least numbers in scientific notation, it actually has a wide range of gotchas. Compiler and hardware implementations of floating point inject additional surprises. This complexity is only increasing as different levels of precision are becoming more common and there are even proposals to automatically reduce program precision (reducing power/energy and increasing performance) when results are deemed """"good enough.'"""" Are software developers who depend on floating point aware of these issues? Do they understand how floating point can bite them? To find out, we conducted an anonymous study of different groups from academia, national labs, and industry. The participants in our sample did only slightly better than chance in correctly identifying key unusual behaviors of the floating point standard, and poorly understood which compiler and architectural optimizations were non-standard. These surprising results and others strongly suggest caution in the face of the expanding complexity and use of floating point arithmetic.",https://ieeexplore.ieee.org/document/8425212/,2018 IEEE International Parallel and Distributed Processing Symposium (IPDPS),21-25 May 2018,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CSNet47905.2019.9108976,Dynamic security management driven by situations: An exploratory analysis of logs for the identification of security situations,IEEE,Conferences,"Situation awareness consists of ""the perception of the elements in the environment within a volume of time and space, the comprehension of their meaning, and the projection of their status in the near future"". Being aware of the security situation is then mandatory to launch proper security reactions in response to cybersecurity attacks. Security Incident and Event Management solutions are deployed within Security Operation Centers. Some vendors propose machine learning based approaches to detect intrusions by analysing networks behaviours. But cyberattacks like Wannacry and NotPetya, which shut down hundreds of thousands of computers, demonstrated that networks monitoring and surveillance solutions remain insufficient. Detecting these complex attacks (a.k.a. Advanced Persistent Threats) requires security administrators to retain a large number of logs just in case problems are detected and involve the investigation of past security events. This approach generates massive data that have to be analysed at the right time in order to detect any accidental or caused incident. In the same time, security administrators are not yet seasoned to such a task and lack the desired skills in data science. As a consequence, a large amount of data is available and still remains unexplored which leaves number of indicators of compromise under the radar. Building on the concept of situation awareness, we developed a situation-driven framework, called dynSMAUG, for dynamic security management. This approach simplifies the security management of dynamic systems and allows the specification of security policies at a high-level of abstraction (close to security requirements). This invited paper aims at exposing real security situations elicitation, coming from networks security experts, and showing the results of exploratory analysis techniques using complex event processing techniques to identify and extract security situations from a large volume of logs. The results contributed to the extension of the dynSMAUG solution.",https://ieeexplore.ieee.org/document/9108976/,2019 3rd Cyber Security in Networking Conference (CSNet),23-25 Oct. 2019,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCI-CC.2017.8109760,Early diagnosis of mild cognitive impairment: A case study in approaches to inductive-logic programming,IEEE,Conferences,"Recent rapid advances in data collection routines in clinical science have led to a trend of storing patient data in a heterogeneous database. The lack of existing computing tools to enable operability to use machine learning on these heterogeneous data sources is a barrier to the healthcare sciences. Healthcare data is usually complex and highly context-dependent, and it requires modern computational tools to handle the complexity of such data. This study sought to utilize the data collected from virtual reality (VR)-based software and a leap-motion device used for learning in mild cognitive impairment (MCI) cases to enable early detection of MCI by analyzing the classification rules for errors (action slips) based on finger-action transitions when performing instrumental activities of daily living (IADL). Finger motion was recorded as a time-series database. An induction technique known as Inductive-Logic Programming (ILP), which uses logical and clausal language to represent the training data, was then used to discover a concise classification rule using logical programming. We were able to generate rules on how action transitions of the finger in the experiments were related to the pattern of micro-errors that indicate the difference of error regarding the length of the no-motion state of the finger.",https://ieeexplore.ieee.org/document/8109760/,2017 IEEE 16th International Conference on Cognitive Informatics & Cognitive Computing (ICCI*CC),26-28 July 2017,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/eScience.2019.00047,Efficient Runtime Capture of Multiworkflow Data Using Provenance,IEEE,Conferences,"Computational Science and Engineering (CSE) projects are typically developed by multidisciplinary teams. Despite being part of the same project, each team manages its own workflows, using specific execution environments and data processing tools. Analyzing the data processed by all workflows globally is a core task in a CSE project. However, this analysis is hard because the data generated by these workflows are not integrated. In addition, since these workflows may take a long time to execute, data analysis needs to be done at runtime to reduce cost and time of the CSE project. A typical solution in scientific data analysis is to capture and relate the data in a provenance database while the workflows run, thus allowing for data analysis at runtime. However, the main problem is that such data capture competes with the running workflows, adding significant overhead to their execution. To mitigate this problem, we introduce in this paper a system called ProvLake, which adopts design principles for providing efficient distributed data capture from the workflows. While capturing the data, ProvLake logically integrates and ingests them into a provenance database ready for analyses at runtime. We validated ProvLake in a real use case in the O&G industry encompassing four workflows that process 5 TB datasets for a deep learning classifier. Compared with Komadu, the closest solution that meets our goals, our approach enables runtime multiworkflow data analysis with much smaller overhead, such as 0.1%.",https://ieeexplore.ieee.org/document/9041720/,2019 15th International Conference on eScience (eScience),24-27 Sept. 2019,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICSCC.2019.8843619,Enhancing STEM Education using Augmented Reality and Machine Learning,IEEE,Conferences,"Learning Science, Technology, Engineering and Mathematics (STEM) in the 21st century has been evolved from the conventional textbook to the interactive platform using electronic devices. This paper presents the implementation of a mobile application system, named AUREL (Augmented Reality Learning) in enhancing the learning experience by projecting Augmented Reality (AR) objects onto 2D images. This AR visualization is used to improve the understanding of STEM subjects and increases the enthusiasm of students towards STEM subjects. In this implementation, Google's Cloud Tensor Processing Units (TPUs) are used to train specific datasets alongside Cloud Vision API to detect a wide range of objects. ML Kit for Firebase is used to host the custom TensorFlow Lite models for specific use cases for better accuracy. On the other hand, Google Cloud Platform (GCP) is used to harvest STEM data, manage STEM 3D information and data processing. Subsequently, the processed information will be displayed in AR in the mobile application using ARCore's Sceneform SDK. The application of AUREL could be extended to all science subjects so that students can learn using an interactive platform.",https://ieeexplore.ieee.org/document/8843619/,2019 7th International Conference on Smart Computing & Communications (ICSCC),28-30 June 2019,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IPCC.1989.102101,"Esperanto, to simplify and clarify international communication",IEEE,Conferences,"Esperanto can solve the problem of the international language barrier. Esperanto, designed explicitly for this purpose, is a real and elegant language, fully operational, proven in practice, and ready for general application by the public at large. Application of the fringe benefits to public education inherent in Esperanto would provide the means for the painless implementation of the needed international language. The practical use of Esperanto as a language of science and technology is discussed.<>",https://ieeexplore.ieee.org/document/102101/,"International Professional Communication Conference 'Communicating to the World.',",18-20 Oct. 1989,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
,Exploiting intelligent decision supports for model-driven biomedical system analysis,IEEE,Conferences,"Both linear modeling and nonlinear modeling provide special means to simplify problem-solving in science and engineering, which also represents a critical stage for intelligent decision support systems. Model-driven decision support systems have numerous real world applications, such as the feature extraction, image compression, pattern recognition, medical diagnosis and telecommunication. In this article, an intelligent decision support system has been exploited for biomedical system analysis. The PCA (Principal Component Analysis) and ICA (Independent Component Analysis) based linear and nonlinear models have been implemented onto the sample characterization of the biomedical systems for decision making, together with applications of neural networks training. The result indicates the effectiveness of the proposed approaches, where the merit and drawback are discussed between linear and nonlinear modeling. Numerical simulations have also been made.",https://ieeexplore.ieee.org/document/6615809/,2013 8th Iberian Conference on Information Systems and Technologies (CISTI),19-22 June 2013,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISMSIT.2018.8567057,FPGA-Based Multi Heart Diseases Classification System with the Aid of LabVIEW,IEEE,Conferences,"Electrocardiography (ECG) is one of the most important recording processes used in medicine; it provides a clear description of situation of the heart. The development of technological and computer science, which led to the emergence of high-resolution screens placed on the wrist and able to record the heart signal, increased the importance of developing a real time and portable multi heart diseases diagnosis system. In this paper, we propose an FPGA-based multi heart diseases classification system that identify eight different heart malfunctions depending on the standard ECG features. Our proposed ECG system, achieved with the aid of LabView, consists of three parts: Acquisition System, Feature Extraction and Making Decision using two different classifiers; Threshold Decision (TD) and Numeral Virtual Generalizing Random-Access Memory (NVG-RAM) weightless neural network. The proposed classifiers were implemented using Verilog HDL and Xilinx Spartan 3AN FPGA. The FPGA mapping showed that TD classifier occupy 1% of the hardware platform slices with execution time of 16 ns, while the NVG-RAM classifier utilize 21% of the FPGA slices with an increase of the execution time equal to 12.81 μs. On the other hand, the NVG-RAM outperforms the TD algorithm and the other proposed classifiers in the literature. In case of the experimental data, the probability of correct classification (PCC) of heart conditions was 100% for NVG-RAM and 98.84% for TD classifier. Whereas, the success rates in case of generated data for the executed TD and NVG-RAM classifiers were 98% and 100 %, respectively.",https://ieeexplore.ieee.org/document/8567057/,2018 2nd International Symposium on Multidisciplinary Studies and Innovative Technologies (ISMSIT),19-21 Oct. 2018,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISDFS.2019.8757511,Face Verification and Recognition for Digital Forensics and Information Security,IEEE,Conferences,"In this paper, we present an extensive evaluation of face recognition and verification approaches performed by the European COST Action MULTI-modal Imaging of FOREnsic SciEnce Evidence (MULTI-FORESEE). The aim of the study is to evaluate various face recognition and verification methods, ranging from methods based on facial landmarks to state-of-the-art off-the-shelf pre-trained Convolutional Neural Networks (CNN), as well as CNN models directly trained for the task at hand. To fulfill this objective, we carefully designed and implemented a realistic data acquisition process, that corresponds to a typical face verification setup, and collected a challenging dataset to evaluate the real world performance of the aforementioned methods. Apart from verifying the effectiveness of deep learning approaches in a specific scenario, several important limitations are identified and discussed through the paper, providing valuable insight for future research directions in the field.",https://ieeexplore.ieee.org/document/8757511/,2019 7th International Symposium on Digital Forensics and Security (ISDFS),10-12 June 2019,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCMC53470.2022.9753712,Face-Crypt Messenger: Enhancing Security of Messaging Systems using AI based Facial Recognition and Encryption,IEEE,Conferences,"Information Security has always been a major discipline in the field of Computer Science. Many software has been developed around the world for fulfilling various tasks. However, it is important to ensure the security of the data involved in using that particular software. In the recent times, the issue of security related to chat systems has been trending. There have been instances where an unauthorized user has viewed the messages of a particular user. That is where this project presents a pragmatic solution. This project implements a chat system which authorizes a user into the application using facial authentication. The user messages are encoded using cryptographic algorithms. In case of sending a secret message, only the user, whose face matches with that at the time of sign-up can view the message. This feature is implemented using Machine Learning algorithms.",https://ieeexplore.ieee.org/document/9753712/,2022 6th International Conference on Computing Methodologies and Communication (ICCMC),29-31 March 2022,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SPMB47826.2019.9037834,Fast Automatic Artifact Annotator for EEG Signals Using Deep Learning,IEEE,Conferences,"Electroencephalogram (EEG) is a widely used non-invasive brain signal acquisition technique that measures voltage fluctuations from neuron activities of the brain. EEGs are typically used to diagnose and monitor disorders such as epilepsy, sleep disorders, and brain death and also to help the advancement of various fields of science such as cognitive science, and psychology. EEG signals usually suffer from a variety of artifacts caused by eye movements, chewing, muscle movements, and electrode pops, which disrupts the diagnosis and hinders precise representation of brain activities. This paper proposes a deep learning based model to detect the presence of the artifacts and to classify the kind of the artifact to help clinicians resolve problems regarding artifacts immediately during the signal collection process. The model is optimized to map the 1-second segments of raw EEG signals to detect 4 different kinds of artifacts and the real signal. The model achieves a 5-class classification accuracy of 67.59%, and a true positive rate of 80% with a 25.82% false alarm for binary artifact classification with time-lapse. The model is lightweight and could potentially be deployed in portable machines.",https://ieeexplore.ieee.org/document/9037834/,2019 IEEE Signal Processing in Medicine and Biology Symposium (SPMB),7-7 Dec. 2019,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CANOPIEHPC54579.2021.00007,Feasibility of Running Singularity Containers with Hybrid MPI on NASA High-End Computing Resources,IEEE,Conferences,"This work investigates the feasibility of a Singularity container-based solution to support a customizable computing environment for running users' MPI applications in “hybrid” MPI mode-where the MPI on the host machine works in tandem with MPI inside the container-on NASA's High-End Computing Capability (HECC) resources. Two types of real-world applications were tested: traditional High-Performance Computing (HPC) and Artificial Intelligence/Machine Learning (AI/ML). On the traditional HPC side, two JEDI containers built with Intel MPI for Earth science modeling were tested on both HECC in-house and HECC AWS Cloud CPU resources. On the AI/ML side, a NVIDIA TensorFlow container built with OpenMPI was tested with a Neural Collaborative Filtering recommender system and the ResNet-50 computer image system on the HECC in-house V100 GPUs. For each of these applications and resource environments, multiple hurdles were overcome after lengthy debugging efforts. Among them, the most significant ones were due to the conflicts between a host MPI and a container MPI and the complexity of the communication layers underneath. Although porting containers to run with a single node using just the container MPI is quite straightforward, our exercises demonstrate that running across multiple nodes in hybrid MPI mode requires knowledge of Singularity, MPI libraries, the operating system image, and the communication infrastructure such as the transport and network layers, which are traditionally handled by support staff of HPC centers and hardware or software vendors. In conclusion, porting and running Singularity containers on HECC resources or other data centers with similar environments is feasible but most users would need help to run them in hybrid MPI mode.",https://ieeexplore.ieee.org/document/9652607/,2021 3rd International Workshop on Containers and New Orchestration Paradigms for Isolated Environments in HPC (CANOPIE-HPC),14-14 Nov. 2021,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCST50977.2020.00021,Food object recognition and intelligent billing system based on Cascade R-CNN,IEEE,Conferences,"With the development of information technology and artificial intelligence, using science and technology to change the low efficiency of the catering industry is a very effective means. The existing system of food identification and intelligent billing in the market includes artificial billing, RFID induction, photo recognition, etc. Based on the Cascade R-CNN algorithm and computer vision technology, this paper proposes an intelligent food identification and intelligent billing system. First, the database is created for algorithm training, then the mobile device is used to collect the food image, and the depth neural network is used to identify the food in the image. Finally, the price calculation result of each is found and returned to the user. In this paper, the basic principle and implementation method of the system are described in detail, and the experimental phenomenon is analyzed. The experimental results show that the system has good accuracy.",https://ieeexplore.ieee.org/document/9262826/,2020 International Conference on Culture-oriented Science & Technology (ICCST),28-31 Oct. 2020,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/NCCC49330.2021.9428820,FoodieCal: A Convolutional Neural Network Based Food Detection and Calorie Estimation System,IEEE,Conferences,"According to recent studies across the world, we can see that a healthy diet is the key to having a sound health and body. People nowadays are more concerned with their diets than ever before. With the advancement of science, it is now viable to construct a unique food identification system for keeping track of day to day calorie intake. However, building this kind of system creates several complications on constructing and implementing the model. In our paper, we have developed a new neural network based model which will predict the food items from a given image and show us the estimated calorie of the detected food items. In order to achieve our goal, we have prepared a dataset of around 23000 images for 23 different food categories. For this, we have built a system which can detect multiple foods by training CNN with features extracted by Inception V3. We have achieved 89.48% accuracy for this model and we deployed our system on a webpage. The user has to upload an image of food item in the webpage and our system will predict the food item along with the estimated calories in real time.",https://ieeexplore.ieee.org/document/9428820/,2021 National Computing Colleges Conference (NCCC),27-28 March 2021,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/RTSE.1998.766514,Formal methods in the classroom: the logic of real-time software design,IEEE,Conferences,"In recent years, much progress has been made towards the development of mathematical methods (""formal methods"") through which it is possible, in principle, to specify and design software to conform to specifications. In this paper, we provide an overview of how formal methods-and particularly real-time formal methods-can be used throughout the software development cycle, and what methods and tools can be introduced in the computer science curriculum to support software development.",https://ieeexplore.ieee.org/document/766514/,Proceedings Real-Time Systems Education III,21-21 Nov. 1998,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICMULT.2010.5629720,Framework and Key Technologies for the Construction of a Virtual Mine,IEEE,Conferences,"The application of virtual reality technology is a trend that holds considerable promise as demonstrated in many applications. This technology includes the means to visually integrate and interact with diverse multidimensional data in 3D and time context, extend real-time decision-making into mine maintenance, implement virtual mine-planning simulations, training simulations, and risk management. Virtual mine is the result of integrating various disciplines including mine science, information science, artificial intelligence, computer science and 3S techniques, which will radically change the traditional mine production and our lifestyles. The functions and main characters of virtual mine are analyzed. A framework for a virtual mine operation system is provided. Key techniques concerned in the implementation of virtual mine are discussed in detail. The future development of virtual mine is prospected.",https://ieeexplore.ieee.org/document/5629720/,2010 International Conference on Multimedia Technology,29-31 Oct. 2010,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCAD45719.2019.8942082,High-performance Hardware Architecture for Tensor Singular Value Decomposition: Invited Paper,IEEE,Conferences,"Tensor provides a brief and natural representation for large-scale multidimensional data by way of appropriate low-rank approximations, thus we can discover significant latent structures of complex data and generalize data representation. To date, tensor has gained tremendous success in various science and technology fields, especially in machine learning and big data applications. However, tensor computation, especially tensor decomposition, is usually expensive due to the inherent large-size characteristic of tensors, and hence would potentially hinder their future wide deployment. In this paper, we develop a hardware architecture to accelerate tensor singular value decomposition (t-SVD), which is a new tensor decomposition technique that has been successfully applied to high-dimensional data classification and video recovery. Specifically, design consideration of each key computing unit is analyzed and discussed. Then, the proposed t-SVD hardware architecture is implemented and synthesized using CMOS 28nm technology. Comparison with real-world CPU-based implementations shows that the proposed hardware accelerator is expected to provide average 14× speedup on various t-SVD workloads.",https://ieeexplore.ieee.org/document/8942082/,2019 IEEE/ACM International Conference on Computer-Aided Design (ICCAD),4-7 Nov. 2019,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AISP53593.2022.9760596,Human Activity Recognition with Privacy Preserving using Deep Learning Algorithms,IEEE,Conferences,"Human activity recognition is an extensively researched topic in the field of computer vision. Recognizing human activities without revealing a person&#x2019;s identity is one such use case. To solve this, we propose a practical method for human activity recognition (HAR) while maintaining anonymity. It captures and distributes data from a variety of sources while respecting the privacy of the individuals concerned. At the core of our approach is (DBN-RGMAA) based on deep neural networks, which are not only more accurate but can also be deployed in real-time video surveillance systems. Hence, this work presents a deep learning-based scheme for privacy-preserving human activities. Initially, for extracting the features from raw video data, a Deep Belief Network (DBN) is used. To increase the HAR identification rate, Hybrid Deep Fuzzy Hashing Algorithm (HDFHA) is employed to capture dependencies between two actions. Finally, the privacy model enhances the privacy of humans while permitting a highly accurate approach towards action recognition by the Recursive Genetic Micro-Aggregation Approach (RGMAA). The implementation is executed and the performances are evaluated by Accuracy, Precision, Recall, and F1 Score. A dataset named HMDB51 is used for empirical study. Our experiments using the Python data science platform reveal that the OPA-PPAR outperforms existing methods.",https://ieeexplore.ieee.org/document/9760596/,2022 2nd International Conference on Artificial Intelligence and Signal Processing (AISP),12-14 Feb. 2022,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/DEST.2010.5610664,Hybrid ontologies and social semantics,IEEE,Conferences,"Semantic Web, Social Web, and new economic challenges are causing major shifts in the pervasive fabric that the internet has become, in particular for the business world. The internet's new role as participatory medium and its ubiquity lead to dense tri-sortal communities of humans and businesses mixed with computer systems, and semantically interoperating in a well-defined sense. Many of the challenges and ongoing (r)evolutions appear to produce as yet seemingly contradictory requirements and thus produce potentially very interesting research areas. We argue that linguistics, community-based real world “social” semantics and pragmatics, scalability, the tri-sortal nature of the communities involved, the balance between usability and reusability, and the methodological requirements for non-disruptive adoption by enterprises of the new technologies provide vectors for fundamental computer science research, for interesting new artefacts, and for new valorisations of enterprise interoperability. We posit that one such development will likely result in hybrid ontologies and their supporting social implementation environments -such as semantic wikis- that accommodate the duality and co-existence of formal reasoning requirements inside systems on the one hand and of declarative knowledge manipulation underlying human communication and agreement on the other hand.",https://ieeexplore.ieee.org/document/5610664/,4th IEEE International Conference on Digital Ecosystems and Technologies,13-16 April 2010,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/Cluster48925.2021.00104,Hybrid workflow of Simulation and Deep Learning on HPC: A Case Study for Material Behavior Determination,IEEE,Conferences,"Nowadays, machine learning (ML), especially deep learning(DL) methods, provide ever more real-life solutions. However, the lack of training data is often a crucial issue for these learning algorithms, the performance accuracy of which relies on the amount and the quality of the available data. This is particularly true when applying ML/DL based methods for specific areas e.g. material characteristics identification, as it requires huge cost of time and manual power getting observational data from real life. In the mean while, simulations on HPC have already been commonly used in computational science due to the fact that it has the ability of generating sufficient and noise free data, which can be used for training the ML/DL based models. However, in order to achieve accurate simulation results the input parameters usually have to be determined and validated by a large number of tests. Furthermore, the evaluation and validation of such input parameters for the simulation often require a deep understanding of the domain specific knowledge, software and programming skills, which can in turn be solved by ML/DL based methods. In this paper, a novel hybrid workflow combining a multi-task neural network and the simulation on high performance computers(HPC) is proposed, which can address the problem of data sparsity and reduce the demand for expertise, resources, and time in determining the validated parameters for simulation. This work is demonstrated through experiments on determination of material behaviors, and the results prove a promising performance (MSE = 0.0386) through this workflow.",https://ieeexplore.ieee.org/document/9556083/,2021 IEEE International Conference on Cluster Computing (CLUSTER),7-10 Sept. 2021,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/COMPSAC.2018.00073,IT Professional 20th Anniversary Panel: The New Realities of AI,IEEE,Conferences,"Summary form only given, as follows. A complete record of the panel discussion was not made available for publication as part of the conference proceedings. Artificial intelligence (AI) is now creating a lot of excitement and hype among professionals and across all kinds of business and industry, as well as among individuals. It is no longer just the theme of science fiction essays and movies. It is emerging as new, innovative approach for solving challenging problems that we encounter in practice, and as an enabler of disruptive innovations and smarter world. AI’s renaissance is driven by recent complementary developments, including major advances in the AI arena, realistic expectations, and recent success in its applications. AI is also raising some major concerns, real and perceived. Nevertheless, AI is trending to become a new normal and is increasingly being adopted in many applications despite its concerns and limitations. This panel will examine the new realities of AI and offer its perspectives and recommendations. It’ll deliberate on: Where is AI headed? What new applications and innovations will emerge, and how they might impact? What are the risks and concerns? How we can leverage AI for good, and address its major risks and concerns? How do we get prepared for the new AI age?",https://ieeexplore.ieee.org/document/8377698/,2018 IEEE 42nd Annual Computer Software and Applications Conference (COMPSAC),23-27 July 2018,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/HPEC43674.2020.9286139,Identifying Execution Anomalies for Data Intensive Workflows Using Lightweight ML Techniques,IEEE,Conferences,"Today's computational science applications are increasingly dependent on many complex, data-intensive operations on distributed datasets that originate from a variety of scientific instruments and repositories. To manage this complexity, science workflows are created to automate the execution of these computational and data transfer tasks, which significantly improves scientific productivity. As the scale of workflows rapidly increases, detecting anomalous behaviors in workflow executions has become critical to ensure timely and accurate science products. In this paper, we present a set of lightweight machine learning-based techniques, including both supervised and unsupervised algorithms, to identify anomalous workflow behaviors. We perform anomaly analysis on both workflow-level and task-level datasets collected from real workflow executions on a distributed cloud testbed. Results show that the workflow-level analysis employing k-means clustering can accurately cluster anomalous, i.e. failure-prone and poorly performing workflows into statistically similar classes with a reasonable quality of clustering, achieving over 0.7 for Normalized Mutual Information and Completeness scores. These results affirm the selection of the workflow-level features for workflow anomaly analysis. For task-level analysis, the Decision Tree classifier achieves >80% accuracy, while other tested classifiers can achieve >50% accuracy in most cases. We believe that these promising results can be a foundation for future research on anomaly detection and failure prediction for scientific workflows running in production environments.",https://ieeexplore.ieee.org/document/9286139/,2020 IEEE High Performance Extreme Computing Conference (HPEC),22-24 Sept. 2020,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICAACI50733.2020.00040,Image Segmentation Technology and Its Application in Digital Image Processing,IEEE,Conferences,"With the development of electronic technology, computer software and hardware technology, image processing and technology related to human vision, the technology of machine vision image processing has also achieved rapid development. With the rapid development of science and technology, digital image processing technology has been widely used in various fields, among which image segmentation is an extremely important technology. Image segmentation is an important intermediate technology in digital image processing. It relies on the underlying technology of image digital processing to serve high-level applications such as pattern recognition. Processing the information interaction between machine and the real world is the first step in the study of artificial intelligence, and the goal of computer vision research is to make the computer have the ability to perceive and understand image information similar to human. This paper expounds the basic principle of image segmentation technology, analyzes and discusses the methods of image segmentation, and studies the application of image segmentation technology in digital image processing.",https://ieeexplore.ieee.org/document/9355361/,2020 International Conference on Advance in Ambient Computing and Intelligence (ICAACI),12-13 Sept. 2020,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/NetCIT54147.2021.00088,Implementation of Recruitment Website Data Analysis System Based on Web Crawler,IEEE,Conferences,"With continuous development of science and technology, China is in the era of big data. In order to extract useful information from massive data, we must use data analysis system. The data analysis system greatly improves the use efficiency of data, and can get the hidden information in the data through the analysis of some complex data. This topic designs a recruitment website data analysis system based on web crawler, uses web crawler to collect the recruitment information in the recruitment website, analyzes the recruitment information, and finally displays the relevant information. The system uses the proceptron algorithm in deep learning to realize the salary prediction function for job seekers. In this system, the crawler function is written in Python language, and multi-threaded, anti-crawler and other technologies are used to crawl the data of the recruitment website. The application of this system can extract, store, analyze and display the complex and diverse recruitment information in the recruitment website, so that job seekers can more comprehensively understand the post information and find the most suitable job.",https://ieeexplore.ieee.org/document/9731330/,"2021 International Conference on Networking, Communications and Information Technology (NetCIT)",26-27 Dec. 2021,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCES51350.2021.9489023,Information Detection in Brain Using Wavelet Features and K-Nearest Neighbor,IEEE,Conferences,"Brain signals provide information about the various activities happening in brain at the time of occurrence of different events. Information specifically known to a person can be assessed using electroencephalogram (EEG) signals. Recognition of specific domain knowledge can be useful for screening individuals by job placement and law enforcement agencies. An individual possessing specific domain knowledge in real life can benefit the society with his/her knowledge and can be distinguished from the individuals possessing no domain knowledge. There is a huge demand of computer science professionals in information technology organizations, government institutions, security agencies, research laboratories, and private organizations. Hence, in the present work it is proposed to detect the knowledge of computer science in the subjects using brain signals. The text stimuli related to computer science domain was presented to the participants on a computer screen. Wavelet decomposition was used to extract features in the delta frequency band of EEG signals. Principal component analysis (PCA) gives reduced dimensionality of wavelet features and leads to better classification than the original features. It is observed that k-nearest neighbor (k-NN) classifier performs significantly better than other classifiers with a classification accuracy of 80%. These results show the efficacy and easy implementation of wavelet features with k-NN classifier in effectively recognizing the experts of a particular field.",https://ieeexplore.ieee.org/document/9489023/,2021 6th International Conference on Communication and Electronics Systems (ICCES),8-10 July 2021,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/Inforino53888.2022.9782936,Instrumental Software Environment for Teaching Students the Technology of Designing Intelligent Decision Support Systems,IEEE,Conferences,"The transition to the ""Digital economy"" requires training of specialists in the field of modern information technologies using artificial intelligence. The National Research University ""Moscow Power Engineering Institute"" in a department of ""Applied Mathematics and Computer Science"" in the master's program ""Artificial Intelligence"" provides training of students in methods and tools for designing intelligent systems, including their most complex representatives &#x2013; intelligent decision support systems in real time. This paper considers the application of the Decision-making Modeling System developed at the Department of Applied Mathematics and Artificial Intelligence, used by students when performing laboratory classes in the disciplines ""Methods and decision support systems"" and ""Intelligent decision support systems"".",https://ieeexplore.ieee.org/document/9782936/,2022 VI International Conference on Information Technologies in Engineering Education (Inforino),12-15 April 2022,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCP.2018.8516592,Intelligent Decision Support for Pervasive Home Monitoring and Assisted Living,IEEE,Conferences,"The current trend of population ageing leads to an increasingly larger population of older adults, who understandabl desire to continue living an independent and fulfilling life in their home and within their communities. While traditionally seen as a societal issue, we are currently at a point where advancement in science and technology enables us to augment human help with ambient assisted living solutions. This paper is the result of research and development undertaken within the framework of a European Union research project targeting the development of a cyber-physical system for assisted living and home monitoring. The system integrates an unobtrusive networkof wireless sensors with server software to provide ambient monitoring, location detection and real-time alerting. The present paper is focused on the system's intelligent software components.The first is a business rules engine that can be configured to send real-time alerts in the case of certain ambient conditions or whe the location of the monitored person shows signification alteration from their usual movement patterns. The second component is a artificial intelligence-based location predictor, used to provide the monitored person's location. Discrepancies between actual an expected location are used by the rule engine to trigger real-time alerts to caregivers.",https://ieeexplore.ieee.org/document/8516592/,2018 IEEE 14th International Conference on Intelligent Computer Communication and Processing (ICCP),6-8 Sept. 2018,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/APPEEC.2009.4918442,Intelligent Implementation Technologies on Sensing Dam Safety Based on Neural Network,IEEE,Conferences,"The equipments on sensing dam safety works usually under extremely bad working conditions. The reliability, stability and accuracy, etc, are very difficult to be guaranteed. The signal is sensitive to noise. The fault is often caused. Micro-electronics technology, computer science and artificial intelligence technologies provide strong technical support and security on improving the shortage of technologies on sensing dam safety, raising the level of automation, intelligent of dam safety monitoring. The Artificial Neural Network (ANN) has strong nonlinear fitting ability, learning function and parallel processing ability. Above excellent features of ANN are used to implement the adaptive suppression for noise and self-diagnosis for faults of sensors. The intelligent principle, method and realization way are presented. The constitution and training algorithm of an adaptive neural network filter are proposed. With this filter, the useful quantitative information can be extracted automatically from noise data. The information can describe the characteristics of detected objects. A fault diagnosis method of nonlinear observer is proposed. The nonlinear dynamic relation between input and output of the system can be obtained by use of the learning function of radial basis function neural network. The error can be calculated and the logical judgment can be made in real time with the proposed observer.",https://ieeexplore.ieee.org/document/4918442/,2009 Asia-Pacific Power and Energy Engineering Conference,27-31 March 2009,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/GCCE.2015.7398635,Intelligent control of USM using a modified NN with PSO,IEEE,Conferences,"As aging society problem goes severe In not only Japan but also the whole world, more and more attentions are attracted to the social and welfare fields. Many researches on science and technology for elders are implemented in rencent years. With the background, there are a lot of needs for techonologies with novel features for improvement. In this paper, a control method with attractive features for the actuator of Ultrasonic Motors (USMs) is introduced. In medical and welfare areas, the USMs are expected to play more important roles owing to their special characteristics. In this research, an intelligent PID control method using Neural Network (NN) combined with type Particle Swarm Optimization (PSO) is developed for the control of USM. In the method, the intelligent controller is designed based on variable gain type PID control using NN. The learning of the NN unit is implemented by the PSO. The gains of PID control are adjusted by the proposed method in real-time. The effectiveness of the method is verified by experimental results.",https://ieeexplore.ieee.org/document/7398635/,2015 IEEE 4th Global Conference on Consumer Electronics (GCCE),27-30 Oct. 2015,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ASONAM.2018.8508264,Interactive Kernel Dimension Alternative Clustering on GPUs,IEEE,Conferences,"Machine learning has seen tremendous growth in recent years thanks to two key advances in technology: massive data generation and highly-parallel accelerator architectures. The rate that data is being generated is exploding across multiple domains, including medical research, environmental science, web-search, and e-commerce. Many of these advances have benefited from emergent web-based applications, and improvements in data storage and sensing technologies. Innovations in parallel accelerator hardware, such as GPUs, has made it possible to process massive amounts of data in a timely fashion. Given these advanced data acquisition technology and hardware, machine learning researchers are equipped to generate and sift through much larger and complex datasets quickly. In this work, we focus on accelerating Kernel Dimension Alternative Clustering algorithms using GPUs. We conduct a thorough performance analysis by using both synthetic and real-world datasets, while also modifying both the structure of the data, and the size of the datasets. Our GPU implementation reduces execution time from minutes to seconds, which enables us to develop a web-based application for users to, interactively, view alternative clustering solutions.",https://ieeexplore.ieee.org/document/8508264/,2018 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining (ASONAM),28-31 Aug. 2018,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CIMCA.2006.133,International Conference on Computational Inteligence for Modelling Control and Automation and International Conference on Intelligent Agents Web Technologies and International Commerce - Title,IEEE,Conferences,"The following topics are dealt with: intelligent agents and ontologies; data mining, knowledge discovery and decision making; intelligent systems; Web technologies and Web services; virtual reality and games; image processing and image understanding techniques; adaptive control and automation; modelling, prediction and control; multi-agent systems and computational intelligence; agent systems, personal assistant agents and profiling; fuzzy systems for industrial automation; control strategies; neural network applications; clustering, classification, data mining and risk analysis; dynamics systems; innovative control systems, hardware design and implementation; robotics and automation; e-business, e-commerce, innovative Web applications; Web databases; diagnosis and medical applications; learning systems; optimization, hybrid systems, genetic algorithms and evolutionary computation control applications; online learning and ERP; knowledge acquisition and classification; nanomechatronics; simulation and control; mobile network applications; information retrieval; Bayesian networks; human computer interaction; cognitive science; mobile agents; knowledge management; intelligent control; e-search and navigation; security.",https://ieeexplore.ieee.org/document/4052645/,2006 International Conference on Computational Inteligence for Modelling Control and Automation and International Conference on Intelligent Agents Web Technologies and International Commerce (CIMCA'06),28 Nov.-1 Dec. 2006,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1049/icp.2022.0311,Introducing self-sustainable cloud platform for data management and extraction of actionable knowledge for smart healthcare industry: a COVID-19 case study,IET,Conferences,"The novel COVID-19 is a highly contagious disease. Data scientists worldwide are attempting to respond to the pandemic by building Artificial Intelligence (AI) solutions like forecasting pandemic growth, speculating possible mutations, identifying the symptoms caused, and many more. The models require vast quantities of data to make predictions. For a newly identified virus, it may take many months or sometimes years to collect related data and prepare it for data analysis purposes, which can further delay the process of making AI solutions. Hence, there is a need for a pipeline system which can facilitate a quick transmission of medical data from healthcare providers to data scientists. This paper proposes a cloud computing platform that allows smart cities to respond to the pandemic faster, with the collaboration of public health centers and data scientists. The platform provides a structured way of identifying and utilizing collaboration opportunities between health centers and the data science community, generating actionable knowledge. The system consists of two parts: 1) The software on the hospital's side, allowing real-time data gathering and automated uploading to cloud servers, 2) The cloud system to facilitate data storing along with model building and deploying. Customers can use the deployed models on a prepaid basis, the money collected will be divided among data scientists and data providers. This unique feature ensures the healthy participation of data providers in the process of making healthcare solutions for the smart cities. A sponsor can also sponsor a project. Hence, the system will sustain on its own with the involvement of stakeholders.",https://ieeexplore.ieee.org/document/9770607/,4th Smart Cities Symposium (SCS 2021),21-23 Nov. 2021,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IEEM45057.2020.9309776,Job Shop Scheduling Problem Neural Network Solver with Dispatching Rules,IEEE,Conferences,"Job Shop Scheduling Problem (JSSP) is an optimization problem in computer science and operations research. Many problems in real-world manufacturing processes can be translated into JSSP. In recent years, Machine Learning has shown great promises in solving optimization problems and can be used to solve JSSP instances. In this paper, an Artificial Neural Network (ANN) was designed and trained to solve JSSP instances using the priority of the operations as the learning output. Dispatching rules were implemented to break ties during the decoding of the priorities. Our experiment results showed that a hybrid algorithm that combines the best of ANN with dispatching rules and standalone dispatching rule-based heuristic outperforms previously reported results.",https://ieeexplore.ieee.org/document/9309776/,2020 IEEE International Conference on Industrial Engineering and Engineering Management (IEEM),14-17 Dec. 2020,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCPCT.2013.6529027,Knowledge representation: Predicate logic implementation using sentence-type for natural languages,IEEE,Conferences,"Representing the content of the text is really an important issue of knowledge representation. Natural language processing (NLP) is a field of computer science, artificial intelligence, and linguistics concerned with the interactions between computers and human languages. It processes the data through lexical analysis, Syntax analysis, Semantic analysis, Discourse processing, Pragmatic analysis. This paper compares various knowledge representation schemes. The algorithm in this paper splits the English sentences into phrases and then represents these in predicate logic by considering the types of sentences (Simple, Interrogative, Exclamatory, Passive etc.). The algorithm has been tested on real sentences of English. The algorithm has achieved an accuracy of 75%. This representation would be used in future for Semantic based Text summarization.",https://ieeexplore.ieee.org/document/6529027/,"2013 International Conference on Circuits, Power and Computing Technologies (ICCPCT)",20-21 March 2013,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SSCI44817.2019.9002803,Learning of Multivariate Beta Mixture Models via Entropy-based component splitting,IEEE,Conferences,"Finite mixture models are progressively employed in various fields of science due to their high potential as inference engines to model multimodal and complex data. To develop them, we face some crucial issues such as choosing proper distributions with enough flexibility to well-fit the data. To learn our model, two other significant challenges, namely, parameter estimation and defining model complexity have to be addressed. Some methods such as maximum likelihood and Bayesian inference have been widely considered to tackle the first problem and both have some drawbacks such as local maxima or high computational complexity. Simultaneously, the proper number of components was determined with some approaches such as minimum message length. In this work, multivariate Beta mixture models have been deployed thanks to their flexibility and we propose a novel variational inference via an entropy-based splitting method. The performance of this approach is evaluated on real-world applications, namely, breast tissue texture classification, cytological breast data analysis, cell image categorization and age estimation.",https://ieeexplore.ieee.org/document/9002803/,2019 IEEE Symposium Series on Computational Intelligence (SSCI),6-9 Dec. 2019,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICTAI.2018.00055,Legal Reasoning in Answer Set Programming,IEEE,Conferences,"Answer Set Programming is a declarative problem solving approach, initially tailored to modelling problems in the area of Knowledge Representation and Reasoning. In this article, we provide a knowledge-based system, capable of representing and reasoning about legal knowledge in the context of Answer Set Programming - thus, modelling non-monotonicity that is inherent in legal arguments. The work, although limited to a specific indicative domain, namely, university regulations, has a variety of extensions. The overall approach constitutes a representative implementation of the Answer Set Programming's modelling methodology, as well as an enhancing of the bond between Artificial Intelligence and Legal Science, bringing us a step closer to a successful development of an automated legal reasoning system for real-world applications.",https://ieeexplore.ieee.org/document/8576053/,2018 IEEE 30th International Conference on Tools with Artificial Intelligence (ICTAI),5-7 Nov. 2018,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SNPD.2012.81,Lessons Learned from Collaborative Research in Software Engineering: A Student's Perspective,IEEE,Conferences,"Time zone, different work schedule, limited real-time information sharing, steep learning curve and different personal specialties, these are common limitations in the collaborative studies, especially when a researcher has just been introduced to the research field or working in a different environment (i.e. Internship programs). This paper introduces you with the experiences, challenges, difficulties, lessons learned, common fallacies and pitfalls in the collaborative software engineering research through the experience of 2-months collaborative research program between Kasetsart University in Thailand and Nara Institute of Science and Technology in Japan. Good mentoring and flat-style communication between professors and students are good indicators of the high quality result in the internship program. These information can be useful to professors, young researchers and internship students who will be conducting researches in such manner.",https://ieeexplore.ieee.org/document/6299327/,"2012 13th ACIS International Conference on Software Engineering, Artificial Intelligence, Networking and Parallel/Distributed Computing",8-10 Aug. 2012,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/PDP50117.2020.00041,Lessons learned from comparing C-CUDA and Python-Numba for GPU-Computing,IEEE,Conferences,"Python as programming language is increasingly gaining importance, especially in data science, scientific, and parallel programming. It is faster and easier to learn than classical programming languages such as C. However, usability often comes at the cost of performance and applications written in Python are considered to be much slower than applications written in C or FORTRAN. Further, it does not allow the usage of GPUs-besides of pre-compiled libraries.However, the Numba package promises performance similar to C code for compute intensive parts of a Python application and it supports CUDA, which allows the use of GPUs inside a Python application.In this paper we compare the performance of Numba-CUDA and C -CUDA for different kinds of applications. For compute intensive benchmarks, the performance of the Numba version only reaches between 50% and 85% performance of the CCUDA version, despite the reduction operation, where the Numba version outperforms CUDA. Analyzing the PTX code and CUDA performance counters revealed that index-calculation is one limiting factor in Numba. Another problem is the type interference for single precision computations, as some values are computed in double precision. By optimizing this within the Numba package, the performance of Numba improves. However, C-CUDA applications still outperform the Numba versions. Further analysis with the CloverLeav Mini App shows that Numba performance further decreases for applications with multiple different compute kernels. The non-GPU part slows down these applications, due to the slow Python interpreter. This leads to a worse GPU utilization.Today Python is widely used in industry and academia and has been the first choice of coding languages among software programmers in the last years. Currently, according to the TIOBE index [5], it is the 3rd most popular programming language and the number one in IEEE Spectrum's fifth annual interactive ranking of the top programming languages [4]. One reason for this is that is easier to learn than classical programming languages like C. However, the other reason is the increasing popularity of Data Science, where Python is the most used language. A collection of libraries such as NumPy [22], and Matplotlib [1] or Scipy [8] provide a rich set of functions for scientific computing [16]. Packages like Dask [19], PyCompss [21] and MPI for Python [6] allow running Python applications on large, parallel machines, promising high performance. However, the performance of Python is considered slow compared to compiled languages such as C, C++, and FORTRAN, especially for heavy computations. In recent years, more and more tools have been developed to counter this prejudice. Numpy [22], for example, uses C-like arrays to store data and offers fast functions implemented in C to speed up calculations. The CuPy [14] package provides a similar set of functions, but these functions are implemented for GPUs using CUDA. The SciPy library is based on NumPy and provides a rich set on functionalities for scientific computing. Still, the high performance of these libraries is provided by the underling C-implementations. Internally, they use libraries like OpenBlas or IntelMKL to reach high performance and therefore, they are limited by the functions which are provided by theses libraries. Therefore, a performance problem always arises when the required functionality is not implemented within these libraries. In this case, the application falls back to the Python interpreter. Compared to ""bare metal"" code, interpreted code is slow. In addition, in Python it is not possible to use GPUs or other accelerators directly, as the Python interpreter cannot execute code on these machines. Therefore, the usage is only possible with precompiled libraries. To overcome this limitation, different approaches where developed to mix C, CUDA or OpenCL with Python. Cython [2] allows integrating C-code in Python applications to improve performance of critical sections. It also allows an easy development of wrappers for C-libraries. Similar, packages such as PyCuda and PyOpenCL [9] support wrappers for CUDA or OpenCL code within a Python script. Both approaches require the mixture of different programming languages.Numba [10] follows a different approach. Instead of merging C/CUDA code with Python, it allows the development of efficient applications for both, CPUs and GPUs in Python style. When a Python script using Numba is executed, marked functions are compiled just-in-time (JIT) using the LLVM framework. Using Python for GPU programming can mean a considerable simplification in the development of parallel applications.But often a simplification of comes at the expense of performance, and one expects a performance loss from Python compared to pure C code. In this paper, we want to understand the differences between native C-CUDA code and CUDA-code written in Python with Numba. We also want to share some basic tips how to improve the performance of applications written in Numba.We will first analyse a few micro benchmarks in detail. We are using these simple benchmarks, as it is easier to understand the differences with small code examples. We will use the collected information to derive some optimization for Numba. Finally, we evaluate and compare the performance of a more application like mini-app, written in C-CUDA and Numba accelerated Python. We will evaluate if our insights from the microbenchmarks to real applications.",https://ieeexplore.ieee.org/document/9092407/,"2020 28th Euromicro International Conference on Parallel, Distributed and Network-Based Processing (PDP)",11-13 March 2020,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISSC55427.2022.9826147,Log4jPot: Effective Log4Shell Vulnerability Detection System,IEEE,Conferences,"The global digital landscape is changing rapidly with the advances in science and technology. A plethora of new breakthroughs are being made every day in several different fields, such as Internet infrastructure, Web 3.0, and AR/VR technologies. With these ever-increasing digital advancements, cybersecurity threats and vulnerabilities are also being exploited daily. In recent times, another critical vulnerability &#x201C;Log4jshell&#x201D; is identified in the logging tool Log4j. The ubiquity of this logging tool among many worldwide online services has exposed millions of devices to this vulnerability. In order to address this critical issue, the paper presents a framework for improving system security against Log4j attacks. The proposed framework deploys an in-house honeypot to detect and defend against various types of Log4j payloads. Experimental results prove the efficiency and accuracy of Log4j payload detection with an average execution time of 80.104 milliseconds for all utilized HTTP methods. In addition, the paper describes Log4shell vulnerabilities, webhooks, and provides a comparative assessment with previously proposed solutions.",https://ieeexplore.ieee.org/document/9826147/,2022 33rd Irish Signals and Systems Conference (ISSC),9-10 June 2022,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CONFLUENCE.2019.8776981,Logistic Regression based DFS for Trip Advising Software (ASCEND),IEEE,Conferences,"Graphs have played a pivotal role in the field of computer science and has been an efficient method for representing and modeling abstractions in various fields. They can be used to represent several real life models. Several domains in today's world use the concept of graphs extensively such as GPS Navigation systems, Computer networks, WebCrawler, Social Networking websites, peer to peer networking, medical and biological field, neural networks etc. Taking into account the numerous applications of the concept of graphs in today's world, graph searching becomes inevitably significant. In this scenario it is important to note that several graph searching algorithms that were proposed to give exhaustive searches doesn't provide the most satisfying outcome in terms of asymptotic time complexity. Through this paper we intend to highlight the significance of machine learning as a useful tool that can be incorporated in various graph searching algorithms that can reduce its complexity. We classify the existing graph searching techniques as subsets or modifications of two major conventional graph searching algorithms namely BFS(Breadth First Search) and DFS(Depth First Search) and suggest the application of logistic regression to improve their performance. It is confounding that only few research papers explore the application of machine learning to the aforementioned graph searching algorithms. Hence, it is evident that there exists scope for future research on this topic and we aim to suggest directions for the same.",https://ieeexplore.ieee.org/document/8776981/,"2019 9th International Conference on Cloud Computing, Data Science & Engineering (Confluence)",10-11 Jan. 2019,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRAI54018.2021.9651371,Lower Limb Prosthesis Gait Cycle Prediction Analysis Using Gyroscope and Load Cell,IEEE,Conferences,"Prediction of the accurate and continuous ankle angles during gait cycle is a great challenge faced in biomedical science. To overcome this challenge, potential of two sensors i.e., a gyroscope and load cell (strain gauge) in predicting ankle angles and load during a Gait cycle was evaluated. Precision values presented by the gyroscope are 95% accurate however values offered by the load cell have a precision of around 88.2%. In addition to this, the image of the current Gait stance is also displayed in real-time. The results obtained using these sensors proved the ability of the method to be efficient and capable in the prediction of Gait cycle ankle angles. This accuracy model can be further refined as a future work for implementation in balance-based calculations and control of lower limb prosthesis.",https://ieeexplore.ieee.org/document/9651371/,2021 International Conference on Robotics and Automation in Industry (ICRAI),26-27 Oct. 2021,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/DEST.2010.5610637,MADBE: A Multi-Agent Digital Business Ecosystem,IEEE,Conferences,"In this paper we propose MADBE, a Multi-Agent Digital Business Ecosystem. The purpose of this system is to provide a digital software environment for small organisations where they can interact and collaborate with each other and create new joint ventures. We believe that a multidisciplinary approach based on biology, computer science, and business concepts is necessary to produce an evolutionary self-organising system for networked business of small and medium sized enterprises (SMEs). In particular, we propose the first multi-agent digital business ecosystem based on ecological metaphors, which will enable us to define certain characteristics, based on real nature interactions and will also permit us to study the resulting network of businesses from an ecological perspective. An interaction-centred approach based on interaction protocols for knowledge sharing is adopted for the implementation of this system.",https://ieeexplore.ieee.org/document/5610637/,4th IEEE International Conference on Digital Ecosystems and Technologies,13-16 April 2010,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/DSPA53304.2022.9790767,Machine Learning Technologies for Bakery Management Decisions,IEEE,Conferences,"The paper discusses using Deep Stream technologies in tasks for predicting best locations for deployment the bakeries. Such methods provides the calculation of people going through possible bakery and use convolutional neural networks for detection people and some effective algorithms for counting. The proposed solution allows deploying successful bakeries and keeping money in real production. Furthermore a lot of applied data science models were used for data analysis in these conditions. The paper discusses in detail the regression, factorial, cluster and discriminant analysis on the example of real data on the operation of a chain of bakeries with changes to preserve trade secrets. The analysis made it possible to simplify the decision-making process for managers among many factors. Moreover, the proposed method made it possible to predict profitability when opening a new point and explore various models of its development. Comparison results are provided for 3 models. The choice was made in favor of one of them. This choice resulted in the opening of a profitable bakery with a high profit margin for the retail market. The regression, factor and cluster analysis results show good opportunities to apply the results of the analysis for making management decisions when choosing the location of bakeries.",https://ieeexplore.ieee.org/document/9790767/,2022 24th International Conference on Digital Signal Processing and its Applications (DSPA),30 March-1 April 2022,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.23919/SPA.2019.8936736,Machine Learning for Embodied Agents: From Signals to Symbols and Actions,IEEE,Conferences,"The aim of this tutorial lecture is to show the role of machine learning and some other AI-related techniques in embodied autonomous agents, and autonomous robots in particular. In this tutorial we bring to the forefront the aspects of robotics that are closely related to computer science. We believe that the progress in algorithms and data processing methods together with the rapid increase in the available computing power were the driving forces behind the successes of modern robotics in the last decade. During this period robots of various classes migrated from university laboratories to commercial companies and then to our everyday life, as now everybody can buy an autonomous vacuum cleaner or lawnmower, while self-driving cars and drones for goods delivery are waiting for proper legal regulations to enter the market. Robotics and Artificial Intelligence already went a long path of mutual inspiration and common development, starting from the symbolic AI (aka Good Old-Fashioned Artificial Intelligence) and its extensive use in early autonomous robots, such as Shakey the robot, created in SRI International by Nils Nilsson, considered one of the ""fathers"" of modern AI. We briefly characterize the range of the most important applications of typical AI methods in modern robotics, including motion planning algorithms [2,3], interpretation of sensory data leading to creation of a world model [4 ,5], and classical learning methods, such as reinforcement learning [6]. However, what made robotics a part of the new wave of AI applications was the recent ""revolution"" of machine learning, mostly grounded in the enormous success of the deep learning paradigm and its many variants that proved to outclass classic methods in a broad range of problems related to the processing of images and other types of signals. The quick adoption of the recent advances in Machine Learning (ML) in robotics seems to be motivated by the fact that ML gives the possibility to infer solutions from data, as opposed to the classic model-based paradigm that was for decades used in robotics. Whereas the modelbased solutions are mathematically elegant and theoretically provable (with respect to stability, convergence, etc.) they often fail once confronted with real-world problems and real sensory data, as their underlying mathematical models are only a very rough approximation of the real world. Therefore, a wider adoption of ML in robotics gives a chance to make robots more robust and adaptive. On the other hand, we should try to use the new techniques without discarding the knowledge and expertise we already have - machine learning methods can benefit a lot from the prior knowledge and the known structure of the problem that has to be solved by learning. This knowledge and structure can be adopted from the model-based methods that a re already well-established in robotics. In the lecture robots are understood in a broad sense, as all embodied agents that have means to physically interact with the environment. They can be either manipulators, mobile robots, aerial vehicles, self-driving cars, and various ""smart"" devices and sensors. In the second part of the lecture attention is paid to specific problems that appear in application of machine learning to embodied agents, such as the need to search a for solution in huge, multi-dimensional spaces (""curse of dimensionality""), and the ever-present problem of representation and incorporation of uncertainty in the processing of real-world data. Some examples of applications of autonomous robots are given, which were successful due to the use of AI - in particular the probabilistic representation of knowledge and machine learning. The most prominent examples are the DARPA competitions: ""Grand Challenge"", ""Urban Challenge"" and ""Robotics Challenge"" (DRC), and the ""Amazon Picking Challenge"", which proves the interest of large corporations in the development of AI-based robotics [7]. In the third part of the lecture new research directions offered by machine learning and the increased availability of training data are discussed. An overview of the most popular application areas of ML in robotics and other autonomous systems is presented along with the typical machine learning paradigms applied in these areas. The focus is on deep learning, mostly using convolutional neural networks to process various sensory data. We discuss three aspects of embodied agents that make machine learning in robotics quite specific with respect to other application areas, such as medical images or natural language processing. The first aspect is dealing with the ""open world"", in which autonomous robots usually operate. This situation breaks the assumptions underlying some popular ML methods, and creates the need to face the problem of unknown classes identification [8] incremental learning [9], and the uncertainty of sensory data [10]. We also stress out that an embodied agent has the ability to actively acquire information [11]. The second aspect is the inference about the scene seen by the agent, where in the case of robotics, semantics and geometry intermingle [12], because the robot has to work in a three-dimensional world, although it often perceives it through twodimensional images [13,14]. The third aspect of our analysis is related to the most important feature of robots that distinguishes them from all other learning agents (software-based). Robots are embodied agents, that is they have a physical ""body"", and are subject to physical constraints, such as the maximum speed of motion or maximum range of perception. Therefore, in ML for robots analysis of the spatio-temporal dependencies in data is very important [15]. Robots support advanced learning methods thanks to the possibility of interaction with the environment - a simple example is active vision with moving camera, a much more complex one is manipulation with active testing of the behavior of objects (repositioning, pushing) [16]. At the end of the lecture, in the context of specific needs and limitations characteristic to the applications of ML in robotics, new concepts of machine learning (e.g. deep reinforcement learning [17], interactive perception [18]) are presented. The lecture is summarized with a brief discussion of the most important challenges and open problems of ML applied to embodied agents.",https://ieeexplore.ieee.org/document/8936736/,"2019 Signal Processing: Algorithms, Architectures, Arrangements, and Applications (SPA)",18-20 Sept. 2019,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/FIE44824.2020.9273981,Machine learning for middle-schoolers: Children as designers of machine-learning apps,IEEE,Conferences,"This Research to Innovative Practice Full Paper presents a multidisciplinary, design-based research study that aims to develop and study pedagogical models and tools for integrating machine-learning (ML) topics into education. Although children grow up with ML systems, few theoretical or empirical studies have focused on investigating ML and data-driven design in K-12 education to date. This paper presents the theoretical grounds for a design-oriented pedagogy and the results from exploring and implementing those theoretical ideas in practice through a case study conducted in Finland. We describe the overall process in which middle-schoolers (N = 34) co-designed and made ML applications for solving meaningful, everyday problems. The qualitative content analysis of the pre-and post-tests, student interviews, and the students' own ML design ideas indicated that co-designing real-life applications lowered the barriers for participating in some of the core practices of computer science. It also supported children in exploring abstract ML concepts and workflows in a highly personalized and embodied way. The article concludes with a discussion on pedagogical insights for supporting middle-schoolers in becoming innovators and software designers in the age of ML.",https://ieeexplore.ieee.org/document/9273981/,2020 IEEE Frontiers in Education Conference (FIE),21-24 Oct. 2020,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AIIoT54504.2022.9817247,Major threats to the continued adoption of Artificial Intelligence in today&#x0027;s hyperconnected world,IEEE,Conferences,"From the golden era of science fiction which dates to the late 1930s, scientific and technological advances in artificial intelligence (AI), along with one of its key subsets, machine learning (ML) have been growing significantly, especially in recent years. In 2021 alone, notable feats included an AI program capable of creating images from seen or previously unseen textual captions, an AI model that effectively integrates computer vision and natural language processing, and a novel AI framework for diagnosing dementia in 24 hours with real-world feasibility underway amongst a host of other fascinating breakthroughs. This paper briefly delves into AI/ML and recaps some key essentials, covering AI and ML subfields, ML methods, industries where AI/ML finds relevance, key stages and the common technical challenges in ML development. Importantly, the paper shifts attention from the latter to underscore the duo of transparency and ethics in AI, highlighting specifically what these are and why they are important, subsequently positing a PESTEL (Political, Economic, Social, Technological, Environmental and Legal) framework for AI design, build and implementation. It is anticipated that the upshot of this would be the facilitation of continuous adoption and long-term sustainability of AI/ML.",https://ieeexplore.ieee.org/document/9817247/,2022 IEEE World AI IoT Congress (AIIoT),6-9 June 2022,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SmartWorld.2018.00087,Mining the Critical Conditions for New Hypotheses of Materials from Historical Reaction Data,IEEE,Conferences,"The new findings in material science often require a high research cost for the following two aspects. First is that the chemical reaction craft needs continuous optimization and may consume lots of valuable reactants and apparatus during daily experiments. Second, the success of a designed experiment relies heavily on researchers' experience. With the starting of the Materials Genome Initiative (MGI) project, researchers are beginning to record historical reaction data, and seek new solutions via computer techniques, such as data mining and machine learning. In this paper, we study the reaction data of inorganic-organic hybrid materials from the Dark Reaction Project from Haverford College with simple machine learning algorithms (i.e., Bayes Net, SVM and C4.5), ensemble learning models (i.e., Random Forest, Stacking, Gradient Boosting Decision Tree (GBDT) and XGBoost), and deep neural network models. Besides accuracy of the prediction models, we also analyze the reaction conditions that have important reflecting in chemistry with different ranking algorithms. With a series of evaluation, we find that the welldesigned stacking-based ensemble learning model can reach the highest prediction accuracy of 61% (8% higher than GBDT and 5% higher than XGBoost) on the top50 subsets based on 'symmetrical uncertainty ranking' on the standalone data set which was not used in the Dark Reaction Project before.",https://ieeexplore.ieee.org/document/8560065/,"2018 IEEE SmartWorld, Ubiquitous Intelligence & Computing, Advanced & Trusted Computing, Scalable Computing & Communications, Cloud & Big Data Computing, Internet of People and Smart City Innovation (SmartWorld/SCALCOM/UIC/ATC/CBDCom/IOP/SCI)",8-12 Oct. 2018,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICNISC54316.2021.00053,Motion Detection and Object Detection: Yolo (You Only Look Once),IEEE,Conferences,"Recently, the field of artificial intelligence has seen many advances thanks to deep learning and image processing. It is now possible to recognize images or even find objects inside an image with a standard GPU. Image processing is a recent science that aims to provide specialists from different areas, as to the general public, tools for manipulating these digital data from the real world. The detection of moving objects is a crucial step for systems based on image processing. The movements detected by the classic algorithms are not necessarily interesting for a thorough information search, and the need to distinguish the coherent movements of parasitic movements exists in most cases. In this paper we are going to use a simply webcam and YOLO algorithm for this implementation. The YOLOv3 (Version 3) model makes predictions with a single network evaluation, making this method extremely fast, running in real time with a capable GPU. From there we&#x0027;ll use OpenCV, Python, and deep learning to apply the YOLOv3 object to images and apply YOLOv3 to video streams.",https://ieeexplore.ieee.org/document/9603899/,2021 7th Annual International Conference on Network and Information Systems for Computers (ICNISC),23-25 July 2021,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CEC.2019.8789969,Multiobjective memetic algorithm for vital nodes identification in complex networks,IEEE,Conferences,"Vital nodes identification, that is, finding a set of nodes whose absence would cause a collapse of the network, is a significant project in network science. Despite there are a plenty of methods to identify the vital nodes, two major problems still need to be solved, that is how to select these nodes and how to determine the number of them. In this study, we focus on dealing with these two problems via proposing a multiobjective memetic algorithm for vital nodes identification task. First, vital nodes identification task is modeled as a biobjective optimization problem by analyzing the characteristic of vital nodes. Then, a memetic strategy and specific evolutionary operators inspired by multiple centralities are designed to execute local and global search. In addition, a long-tail property is found from the Pareto front of this bi-objective optimization problem, and the simulation results always show an obvious knee region. Hence, an adaptive learning method to determine the size of vital nodes is designed by searching for the knee point. At last, the proposed framework is tested on the scale free networks and real-life networks, and the simulation results validate its effectivity in contrast with the state-of-art greedy methods.",https://ieeexplore.ieee.org/document/8789969/,2019 IEEE Congress on Evolutionary Computation (CEC),10-13 June 2019,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CEE-SECR.2009.5501170,Neuro-automata based controlling,IEEE,Conferences,"Artificial intelligence - one of the most interesting theoretical and applied areas of computer science. There is a wide range of techniques and approaches for creation of artificial intelligence in the applications and real-time control systems. In this paper, we propose a realization of neuro-automata controlling based on neural networks and automaton paradigm and considering an example of how this paradigm could be used in real application.",https://ieeexplore.ieee.org/document/5501170/,2009 5th Central and Eastern European Software Engineering Conference in Russia (CEE-SECR),28-29 Oct. 2009,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/BigData.2015.7363882,Open research challenges with Big Data — A data-scientist's perspective,IEEE,Conferences,"In this paper, we discuss data-driven discovery challenges of the Big Data era. We observe that recent innovations in being able to collect, access, organize, integrate, and query massive amounts of data from a wide variety of data sources have brought statistical data mining and machine learning under more scrutiny and evaluation for gleaning insights from the data than ever before. In that context, we pose and debate the question - Are data mining algorithms scaling with the ability to store and compute? If yes, how? If not, why not? We survey recent developments in the state-of-the-art to discuss emerging and outstanding challenges in the design and implementation of machine learning algorithms at scale. We leverage experience from real-world Big Data knowledge discovery projects across domains of national security, healthcare and manufacturing to suggest our efforts be focused along the following axes: (i) the `data science' challenge - designing scalable and flexible computational architectures for machine learning (beyond just data-retrieval); (ii) the ` science of data' challenge - the ability to understand characteristics of data before applying machine learning algorithms and tools; and (iii) the `scalable predictive functions' challenge - the ability to construct, learn and infer with increasing sample size, dimensionality, and categories of labels. We conclude with a discussion of opportunities and directions for future research.",https://ieeexplore.ieee.org/document/7363882/,2015 IEEE International Conference on Big Data (Big Data),29 Oct.-1 Nov. 2015,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISCID.2016.1055,Path Planning for UUV in Dynamic Environment,IEEE,Conferences,"From naval operations to ocean science missions, the importance of autonomous vehicles is increasing with the advances in underwater robotics technology. Due to the dynamic and intermittent underwater environment and physical limitations of underwater unmanned vehicle (UUV), feasible and optimal path planning is crucial for autonomous underwater operations. According to different mission, the path planning method of UUV is divided into two categories: the point to point path planning and the complete coverage path planning. The objective of this thesis is to develop and demonstrate an efficient underwater path planning method that is adapted to complicated ocean environment. In this thesis, existing path planning method for the fields of ocean science and robotics are first reviewed, and then local dynamic obstacle avoidance method is proposed to avoid dynamic obstacles. Based on this again, the path planning of UUV in local dynamic environment can be efficiently implemented by adopting rolling window path planning method and local dynamic obstacle avoidance method. This method with the guide point strategy combines global path planning with local dynamic path planning, so that not only the requirements of real-time on-line path planning for UUV are met, the global optimality is also considered. A navigation route for UUV is planned in advance by using priori environmental information based on ant colony algorithm, so it provides the reference information for the selection of guide point. In order to solved the problem of area coverage search, a complete coverage path planning method is proposed by combining ant colony algorithm with biologically inspires neural network. In order to demonstrate underwater path planning method, all of the above ideas and methods developed were tested in simulation experiments.",https://ieeexplore.ieee.org/document/7830329/,2016 9th International Symposium on Computational Intelligence and Design (ISCID),10-11 Dec. 2016,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/DISA.2018.8490605,Path Planning on Robot Based on D Lite Algorithm,IEEE,Conferences,"The increasing need of autonomous behavior of robots in fields of science and technology formed the requirement for path planning implemented by the robot without the human assistance. In this paper, D* Lite, which is a path planning graph-based algorithm, was used in order to compute the shortest path from a start to goal point in a real environment and make a Pepper robot move in a computed trajectory. The movement of robot was conducted in a static environment, with the map of the environment already known. This paper is a first step in the research focusing on a creation of a so-called intelligent workspace.",https://ieeexplore.ieee.org/document/8490605/,2018 World Symposium on Digital Intelligence for Systems and Machines (DISA),23-25 Aug. 2018,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CERMA.2006.73,Pedagogical Virtual Agents to Support Training of Human Groups,IEEE,Conferences,"From the last decade, intelligent virtual environments (IVEs) have become a quite popular tool for applying computer science to education. The intelligence of the systems generated using IVEs usually falls on the denominated pedagogical virtual agent. Pedagogical virtual agents have characteristics that allow to increase the computer's ability to engage and motivate students along their learning process. In this paper, a human groups (teams) training strategy is presented, supporting one of its stages by the use of a pedagogical virtual agent playing the role of team leader",https://ieeexplore.ieee.org/document/4019729/,"Electronics, Robotics and Automotive Mechanics Conference (CERMA'06)",26-29 Sept. 2006,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICALT.2004.1357653,Pedagogical embodied conversational agent,IEEE,Conferences,"A virtual reality instructor that autonomously trains a human learner in network virtual environments, respond to multi-modal input across computer networks, and applies proven pedagogical techniques during instruction has the potential to improve human learning performance anytime, anywhere, and at any pace. Building virtual instructors, however, have challenged researchers because of multidisciplinary expertise required in areas such as education, cognitive science, sociology, artificial intelligence, 3D computer graphics, linguistics, and more. This paper discusses a model for building human computer interactive virtual instructor systems using an innovative, system/software architecture.",https://ieeexplore.ieee.org/document/1357653/,"IEEE International Conference on Advanced Learning Technologies, 2004. Proceedings.",30 Aug.-1 Sept. 2004,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISMSIT50672.2020.9254863,Personalized Quality of Experience (QOE) Management using Data Driven Architecture in 5G Wireless Networks,IEEE,Conferences,"the aim of this paper is to Personalized Quality of Experience (QOE) Management using Data driven Architecture in 5G Wireless Networks that consume less resources. The proposed research will be the part of the overall research project, which focuses on addressing a problem that many organizations experience that introduce an Enterprise Architecture to support the integration of different services across the enterprise. With the rapid growth in mobile network usage and video streaming being the most popular service, Quality of Experience of video in mobile networks is of extreme importance to both service providers and their customers. The ability to effectively predict Quality of Experience of video is key for QoE adaptation and higher levels of customer satisfaction. In this work machine learning algorithms were used to create models that predict QoE with network QoS parameters, including wireless-specific and 5Gspecific parameters. An 5G simulation that reflects the current mobile traffic landscape was created to obtain the data set for training. An objective tool for video QoE evaluation was used to gather QoE data necessary to train the prediction models. Support Vector Machines, Random Forest, Gradient Boosted Trees and Neural Networks were chosen as the machine learning algorithms for Quality of Experience prediction, and it was shown that they achieve high accuracy. Influence of wireless-specific parameters on QoE prediction was also investigated, and it was discovered that they are suitable for use in Quality of Experience prediction models.The problem is that; organizations do not know where they either have or may encounter weaknesses in their Enterprise Architecture with Data Driven Architecture (DDA). The framework presented is based on concepts from Wireless Networks with Driven Architecture will be designed to support both Transitional Gap Analysis (TGA) and Comparative Gap Analysis (CGA). TGA is supported by comparing a baseline Data Driven Architecture (DDA) to a target QoE where both DDA have been defined from the management perspective. DDA is facilitated by mapping a QoE to two or more 5G networks. The research methodology used in the paper is design science research for the QoE management based 5G network. The QOE for implementation of 5th generation network and apply it in many different real-world organizations. The goal of the paper is to present a framework in the form an implementation and management model, called QOE, that visualizes the gaps (weaknesses) in proposed or existing enterprise architectures and to support a comparative analysis process for different a5Grnative solution approaches. a set of requirements on the QOE management can be presented and the frameworks are applied on Matlab for implementation.",https://ieeexplore.ieee.org/document/9254863/,2020 4th International Symposium on Multidisciplinary Studies and Innovative Technologies (ISMSIT),22-24 Oct. 2020,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SNAMS53716.2021.9732126,Point-of-care Ultrasound (POCUS) Device Utilisation at the Edge in Lower and Middle Income Countries,IEEE,Conferences,"Summary form only given, as follows. The complete presentation was not made available for publication as part of the conference proceedings. The genomics revolution in Africa began almost a decade ago to identify genetic and environmental determinants of human disease and wellness. This momentum is accelerating to advance science globally through large initiatives on the continent of Africa, for example, Human Hereditary and Health in Africa (H3Africa), Data Science Initiative for Africa (DSI-Africa), African Open Science Platform (AOSP), and the recent call by the Wellcome Trust (WT) for the setting up African Population Cohort Consortia (APCC) amongst others. All these efforts are aimed at transforming &#x201C;data- poor Africa&#x201D; into a data-rich Africa. We want to tackle multiple SDGs, centred on health and wellness and climate change, for effective policy implementation for maximum impact. To harness these opportunities, we will leverage cutting-edge technologies and innovative methods to deliver better tools for disease diagnosis and prognosis; and novel targeted treatmentand prevention that is suitable for LMICs settings. This talk highlights real-life examples and challenges for incorporating handheld or point of care ultrasound (POCUS) device data into routine healthcare and research participant surveillance, and the opportunities for application of deep learning/AI for aiding clinical decision making leveraging the emerging Mobile fog computing (MFC) approaches.",https://ieeexplore.ieee.org/document/9732126/,"2021 Eighth International Conference on Social Network Analysis, Management and Security (SNAMS)",6-9 Dec. 2021,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/GUCON50781.2021.9573606,Possible Role of AI and IoT in Smart Buildings,IEEE,Conferences,"There are an increasing number of Artificial Intelligence (AI) and Internet of Things (IoT) devices and applications which are used across the globe in different areas. One such area is IoT-enabled smart buildings. There is a growing interest among government and non-government organizations in IoT-enabled smart buildings. Despite the perceived benefits of being smart, there is also a key concern of human safety in an emergency situation. This research aims to address this important concern and will develop an initial framework, called here the Safe'Tech, for human safety in IoT enabled smart buildings. The SafeTech will be developed and evaluated using the well-known design science research method. The SafeTech will comprise of both a design and software prototype solution, which can be used to ensure the safety of humans during an emergency through the collection, processing and communication of real-time data from the IoT sensors embedded in the smart building such as smart campus and shopping centre. The SafeTech will interact with the smart building through sensors and collect the state of the physical environment and sense any emergency and then also interact with the humans in the smart building and inform and direct them about the emergency and guide them for the safe evacuation. The applicability and evaluation of the framework software prototype will be demonstrated with the help of smart building scenarios in both Australia and Qatar, involving the fire emergency exercise and automatic and directed evacuation using the SafeTech. This research has several implications, in particular, the impact of reducing human life loss or injury reduction.",https://ieeexplore.ieee.org/document/9573606/,"2021 IEEE 4th International Conference on Computing, Power and Communication Technologies (GUCON)",24-26 Sept. 2021,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/WI.2018.00013,Proposed Computational Classification System of Human Cognitive Biases,IEEE,Conferences,"Despite our aspirations to do so, we humans don't always make optimal or rational decisions. Researchers from psychology, behavioral economics, anthropology, decision sciences, and other related fields have described many human cognitive biases which help to explain such decisions. Most of the time, these cognitive biases are relatively harmless and relatively costless. However, sometimes they do result in significant costs to individuals, companies, governments and societies in the form of wasted or misdirected money, time, effort, and sometimes even in the form of lives lost. The antidote to such decisions has long been recognized to lie in algorithmic decision making. Until relatively recently, though, requirements and complexity of such algorithms have limited their deployment in real-world situations. However, we now enjoy a convergence of computing power, decrease in computing costs, and computational and predictive methods born of data science, artificial intelligence (AI), and machine learning (ML), such that we can begin to mitigate some of the most negative effects of some of these cognitive biases. This paper proposes a method for classifying these human cognitive biases for purposes of mitigation by means of computing methods, describes some of these biases that are most ripe for mitigation through computing, and proposes future research directions that build upon this work.",https://ieeexplore.ieee.org/document/8609688/,2018 IEEE/WIC/ACM International Conference on Web Intelligence (WI),3-6 Dec. 2018,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISSRE52982.2021.00055,PyGuard: Finding and Understanding Vulnerabilities in Python Virtual Machines,IEEE,Conferences,"Python has become one of the most popular pro-gramming languages in the era of data science and machine learning, and is also widely deployed in safety-critical fields like medical treatment, autonomous driving systems, etc. However, as the official and most widely used Python virtual machine, CPython, is implemented using C language, existing research has shown that the native code in CPython is highly vulnerable, thus defeats Python&#x0027;s guarantee of safety and security. This paper presents the design and implementation of PyGuard, a novel software prototype to find and understand real-world security vulnerabilities in the CPython virtual machines. With PyGuard, we carried out an empirical study of 10 different versions of CPython virtual machines (from version 3.0 to the latest 3.9). By scanning a total of 3,358,391 lines native code, we have identified 598 new vulnerabilities. Based on our study, we describe a taxonomy to classify vulnerabilities in CPython virtual machines. Our taxonomy provides a guidance to construct automated and accurate bug-finding tools. We also suggest systematic remedies that can mediate the threats posed by these vulnerabilities.",https://ieeexplore.ieee.org/document/9700331/,2021 IEEE 32nd International Symposium on Software Reliability Engineering (ISSRE),25-28 Oct. 2021,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/I-SMAC49090.2020.9243322,Quantum Neural Networks for Dynamic Route Identification to avoid traffic,IEEE,Conferences,"Computation is the primary task performed for the evaluation of the solution for a specific problem, and in realtime, having better challenges to implementing the solution path with the better computational mechanisms. The concept of quantum computation mechanism using the neural networks is having the highest amount of the success rate in prediction models design and implementation. The idea of a dynamic routing mechanism using quantum computing and neural networks are the main essence. A better prediction model is performed for this specific kind of problem, which needs a particular focus on the latest problem-solving mechanisms. The problem-solving tools like neural networks will dynamically perform with real-time data, but a new add-on is needed to add like big data to implement the live data. The live data can help implement and understand the importance of solving the problem like dynamic routing mechanism. There is a chance of random growth in such a field of computer science. This computational mechanism using quantum computing and the neural network will track the live operations and form the dynamic route changes in the real-time scenario. This real-time scenario worked with a 95% accuracy rate. The accuracy will differ based on the number of connecting nodes are being considered to evaluate the hidden layers of the problem-solving mechanism.",https://ieeexplore.ieee.org/document/9243322/,"2020 Fourth International Conference on I-SMAC (IoT in Social, Mobile, Analytics and Cloud) (I-SMAC)",7-9 Oct. 2020,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICTAI.2004.103,Quartet based phylogeny reconstruction with answer set programming,IEEE,Conferences,"Evolution is an important subarea of study in biological science, where given a set of species, the goal is to reconstruct their evolutionary history, or phylogeny. Many kinds of data associated with the species can be deployed for this task and many reconstruction methods have been proposed and examined in the literature. One very recent approach is to build a local phylogeny for every subset of 4 species, which is called a quartet for these 4 species, and then to assemble a phylogeny for the whole set of species satisfying these predicted quartets. In general, those predicted quartets might not always agree each other; and thus the objective function becomes to satisfy a maximum number of predicted quartets. This is the well-known maximum quartet consistency (MQC) problem, which is studied by a lot of researchers in the last two decades. We present a new equivalent representation for the MQC problem, that is, to search for an ultrametric matrix to satisfy the maximum number of those predicted quartets. We examine a few number of structural properties of the MQC problem in this new representation, through formulating it into answer set programming (ASP), a recent powerful logic programming tool for modeling and solving searching problems. The efficiency and usefulness of our approach are confirmed by our computational experiments on the artificial data as well as two real datasets.",https://ieeexplore.ieee.org/document/1374243/,16th IEEE International Conference on Tools with Artificial Intelligence,15-17 Nov. 2004,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/DigitalHeritage.2013.6744821,REVEAL: One future for heritage documentation,IEEE,Conferences,"It can be frustrating to ensure that cultural heritage work, from archaeological excavations to historic surveys, is documented fully and that the evidence is recorded properly and thoroughly. Automated computer-based documentation and research tools would seem to offer many benefits. They can be more accurate and cost effective, saving time and ensuring that all finds and their contexts are appropriately and fully recorded. And if designed well, new digital field data acquisition systems can enable new types of hypothesis testing, new insight into the past, and new visualizations that in turn can lead to a paradigm shift in how heritage sites are managed and information disseminated. There have been many computer-based data collection systems for heritage management; many databases, many digital archives, and many digital publication options. REVEAL is special. REVEAL (Reconstruction and Exploratory Visualization: Engineering meets ArchaeoLogy), is the product of a US National Science Foundation collaboration among the Institute for the Visualization of History, Brown University's Division of Engineering, Laboratory for Man/Machine Systems, and the University of North Carolina, Charlotte's Department of Electrical and Computer Engineering. The project uses computer-vision, pattern-recognition, and machine-learning research to augment applications for archaeology and the humanities. REVEAL is a single piece of (free and open-source) software that coordinates all data types (e.g., photos, drawings, 3D models, and tabular information) with semi-automated tools for documenting sites, trenches and objects, recording excavation and site-evaluation progress, researching and analyzing the collected evidence, and creating 3D models and virtual worlds. Search and retrieval, building interactive visualizations, and testing hypo",https://ieeexplore.ieee.org/document/6744821/,2013 Digital Heritage International Congress (DigitalHeritage),28 Oct.-1 Nov. 2013,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SMC-IT.2009.40,Rapid Prototyping of Planning & Scheduling Tools,IEEE,Conferences,"The Advanced Planning and Scheduling Initiative, or APSI, is an ESA programme to design and implement an Artificial Intelligence (AI) software infrastructure for planning and scheduling that can generically support different types and classes of space mission operations. The goal of the APSI is twofold: (1)~creating a software framework to improve the cost-effectiveness and flexibility of mission planning support tool development; (2)~bridging the gap between AI planning and scheduling technology and the world of space mission planning. A key aspect of the success of this project is the presence of a flexible timeline representation module that allows to exploit alternatives in the modeling of mission features. This paper shows an example of such a flexibility by using a real problem in the space realm - the HERSCHEL Science Long Term Planning process.",https://ieeexplore.ieee.org/document/5226820/,2009 Third IEEE International Conference on Space Mission Challenges for Information Technology,19-23 July 2009,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCCE.2008.4580693,Real time implementation of NARMA L2 feedback linearization and smoothed NARMA L2 controls of a single link manipulator,IEEE,Conferences,"Robotics is a field of modern technology which requires knowledge in vast areas such as electrical engineering, mechanical engineering, computer science as well as finance. Nonlinearities and parametric uncertainties are unavoidable problems faced in controlling robots in industrial plants. Tracking control of a single link manipulator driven by a permanent magnet brushed DC motor is a nonlinear dynamics due to effects of gravitational force, mass of the payload, posture of the manipulator and viscous friction coefficient. Furthermore uncertainties arise because of changes of the rotor resistance with temperature and random variations of friction while operating. Due to this fact classical PID controller can not be used effectively since it is developed based on linear system theory. Neural network control schemes for manipulator control problem have been proposed by researchers; in which their competency is validated through simulation studies. On the other hand, actual real time applications are rarely established. Instead of simulation studies, this paper is aimed to implement neural network controller in real time for controlling a DC motor driven single link manipulator. The work presented in this paper is concentrating on neural NARMA L2 control and its improvement called to as Smoothed NARMA L2 control. As proposed by K. S Narendra and Mukhopadhyay, Narma L2 control is one of the popular neural network architectures for prediction and control. The real time experimentation showed that the Smoothed NARMA L2 is effective for controlling the single link manipulator for both point-to-point and continuous path motion control.",https://ieeexplore.ieee.org/document/4580693/,2008 International Conference on Computer and Communication Engineering,13-15 May 2008,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/FIE.1999.839219,Real time object recognition for teaching neural networks,IEEE,Conferences,"Undergraduate students in computer science learn best when they are given the opportunity to apply hardware and software concepts to real world systems, and neural-network applications present attractive possibilities for giving them such opportunity. An example of how to take advantage of these possibilities is given in this paper, which describes a specific neural network technique that has been developed and applied to the problem of identifying real world objects in real time. Those objects can be as simple as paper cut-outs or they can be mechanical objects such as a nut or a bolt. These objects are placed on a plane, and are ""examined"" by an ""identifying system"" consisting of a camera attached to a PC through a video capture card. The pixels collected from the image of the object are fed to a set of neural network nodes for pattern recognition. Patterns are recognized by a multi-layer neural network where the output of a neuron, which can be characterized by sigmoid ""activation function"", is a function of weighted inputs. Lisp is used as the programming language due to its simple syntax and powerful recursive features for processing lists. The binary equivalent of the computed output is evaluated as a recognition signature which is compared to the signature of objects in a list. Of course, the artificial neural network is ""trainable"". To master the technique, the students start by learning about neurons and forward and backpropagation methods, but they soon find themselves ""training"" a multilayered neural network that they themselves have built. The learning experience encompasses video capturing, image handling, filtering, and image compression, and it demystifies neural network programming.",https://ieeexplore.ieee.org/document/839219/,FIE'99 Frontiers in Education. 29th Annual Frontiers in Education Conference. Designing the Future of Science and Engineering Education. Conference Proceedings (IEEE Cat. No.99CH37011,10-13 Nov. 1999,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/RADAR.2008.4720778,Real-time autonomous disturbance detection and monitoring system with L-band UAVSAR,IEEE,Conferences,"We developed an autonomous disturbance detection and monitoring system with imaging radar that combines the unique capabilities of imaging radar with high throughput onboard processing technology and onboard automated response capability based on specific science algorithms. This smart sensor development leverages off recently developed technologies in real-time onboard synthetic aperture radar (SAR) processor and onboard automated response software as well as science algorithms previously developed for radar remote sensing applications. In this project, we use a high rate data interface to ingest NASA’s UAVSAR data and compute SAR imagery in real-time complete with motion compensation and antenna beam steering capabilities. NASA’s UAVSAR is a compact, L-band 80 MHz bandwidth, fully polarimetric radar. It is designed for repeat-pass InSAR and has had engineering flights in 2007 and successful science data collections in 2008. The fidelity of the onboard SAR processor is tuned by implementing polarimetric calibration capabilities. Science algorithms are implemented for detecting and monitoring fire disturbances over the US forests. We additionally developed artificial intelligence for decision-making, and adapted existing onboard activity re-planning and execution software to interface with the UAVSAR radar controller. The product of this development is a prototype closed loop smart sensor.",https://ieeexplore.ieee.org/document/4720778/,2008 IEEE Radar Conference,26-30 May 2008,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICICAS51530.2020.00065,Realtime Multi-Person Pose Estimation Based on Android System,IEEE,Conferences,"Human pose estimation has always been a very popular direction. This technology can be applied to human-computer interaction, abnormal behavior detection, intelligent security and other fields, and it will promote the development of artificial intelligence in the future. Human pose estimation has been developed for a long time, and with the advancement of science and technology, many emerging algorithms have spewed out, but most of these algorithms use a multistage network structure. These network structures have achieved better results in accuracy, but the parameters have become larger and larger. It's a big problem that deploying these networks to mobile and embedded devices with limited system resources. We use the MobileNet and TensorFlow Lite frameworks launched by Google to successfully optimize the complex network model and deploy it to Android phones. The human pose estimation can be quickly realized only by using local calculations, avoiding the delay trouble caused by cloud computing solutions.",https://ieeexplore.ieee.org/document/9402763/,"2020 International Conference on Intelligent Computing, Automation and Systems (ICICAS)",11-13 Dec. 2020,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICECA52323.2021.9675885,Recent Advancements in Autonomous Emergency Braking: A Survey,IEEE,Conferences,"Background/Objectives: This paper is aimed to present a study on the need for Autonomous Emergency Braking System (AEBS) in Autonomous Vehicles (AVs), terminologies, important AEBS metrics, and recent research on AEBSs in AVs. Methods/Statistical analysis: This paper discusses the AEBS research carried out in the recent past, with integrated technologies. The study articles are sourced from databases like IEEE Xplore, Springer Link, Science Direct, and Elsevier Journal Finder, etc. The keywords used to fetch the relevant articles are: autonomous vehicles, self-driving vehicles, automatic emergency braking, autonomous emergency braking system, and collision avoidance in AVs. The surveyed literature was published between 2009 and 2020. Findings: The observations in this study reveal the following: various tools used to simulate AVs and AEBSs, widely adopted metrics for consideration and evaluation, real-time implementations of AEBS (sensors and prototypes), and different technologies used for the effective implementation. Novelty/Applications: The detailed study on AEBS in this paper emphasizes various interesting factors to make AEBS more robust and reliable. The findings are outlined in the summary section as the future research agenda. Hence, there is a great room for effective AEBS implementation with the blend of our findings.",https://ieeexplore.ieee.org/document/9675885/,"2021 5th International Conference on Electronics, Communication and Aerospace Technology (ICECA)",2-4 Dec. 2021,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICIP.2009.5414281,Recent advances in remote sensing image processing,IEEE,Conferences,"Remote sensing image processing is nowadays a mature research area. The techniques developed in the field allow many real-life applications with great societal value. For instance, urban monitoring, fire detection or flood prediction can have a great impact on economical and environmental issues. To attain such objectives, the remote sensing community has turned into a multidisciplinary field of science that embraces physics, signal theory, computer science, electronics, and communications. From a machine learning and signal/image processing point of view, all the applications are tackled under specific formalisms, such as classification and clustering, regression and function approximation, image coding, restoration and enhancement, source unmixing, data fusion or feature selection and extraction. This paper serves as a survey of methods and applications, and reviews the last methodological advances in remote sensing image processing.",https://ieeexplore.ieee.org/document/5414281/,2009 16th IEEE International Conference on Image Processing (ICIP),7-10 Nov. 2009,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CVIDLICCEA56201.2022.9824310,Research Analysis of Stream Processing System Storm Based on Knowledge Map,IEEE,Conferences,"With the development of the Internet of Things and 5G technology, stream processing systems are more and more widely used, especially the open source Storm platform. A comprehensive understanding of the research status and progress of Storm is of a certain reference value for the academic and industry to carry out related work. Based on bibliometrics theory, this paper takes the citation index of CNKI (China national knowledge infrastructure) and WOS (web of science) databases from 2013 to 2021 as the data source, and the relevant literature of Storm research and application as the research object, which is visualized and analyzed by the CiteSpace software. The knowledge maps are drawn from the aspects of temporal and spatial distribution of literature, distribution of authors and institutions, keyword co-occurrence, citation frequency and keyword prominence, and the research status, research basis, research hotspots and research trends of Storm at home and abroad are analyzed. The research results can provide references for the follow-up theoretical and application research of Storm system.",https://ieeexplore.ieee.org/document/9824310/,"2022 3rd International Conference on Computer Vision, Image and Deep Learning & International Conference on Computer Engineering and Applications (CVIDL & ICCEA)",20-22 May 2022,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRIS.2018.00047,Research and Application of Traffic Sign Detection and Recognition Based on Deep Learning,IEEE,Conferences,"Nowadays, with the rapid development of society and economy, automobiles have become almost one of the convenient modes of transport for every household. This makes the road traffic environment more and more complicated, and people expect to have an intelligent Vision-assisted applications that provide drivers with traffic sign information, regulate driver operations, or assist in vehicle control to ensure road safety. As one of the more important functions, traffic sign detection and recognition[1], has become a hot research direction of researchers at home and abroad. It is mainly the use of vehicle cameras to capture real-time road images, and then to detect and identify the traffic signs encountered on the road, thus providing accurate information to the driving system. However, the road conditions in the actual scene are very complicated. After many years of hard work, researchers have not yet made the recognition system practical, and further research and improvement are still needed. Traditionally, traffic signage has been detected and categorized using standard computer vision methods, but it also takes considerable time to manually process important features of the image. With the development and progress of science and technology, more and more scholars use deep learning technology to solve this problem. The main reason that the deep learning method is widely accepted is that the model can learn the deep features inside the image autonomously from the training samples, especially for many cases that do not know how to design the feature extractor, such as expression recognition, target detection Wait. Based on the application of road traffic sign detection and recognition, this article focuses on the correctness and high efficiency of detection and recognition. Through Caffe[2] which is the open-source framework, a deep convolution neural network algorithm is proposed to train traffic sign training sets to get a model that can classify traffic signs and to learn and identify the most critical of these traffic signs Features, so as to achieve the purpose of identifying traffic signs in the real scene.",https://ieeexplore.ieee.org/document/8410256/,2018 International Conference on Robots & Intelligent System (ICRIS),26-27 May 2018,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AIAM48774.2019.00062,Research and Application of the Intelligent Biogas Monitoring System Based on LoRa Technology,IEEE,Conferences,"In the towns, there are problems such as poor real-time meter reading and biogas management, and this paper constructs an intelligent biogas monitoring system based on LoRa (Long Range) technology to meet biogas science monitoring and management. First, the architecture design is carried out, and the system is divided into a physical acquisition layer, a network link layer, and a management application layer. Then, it is developed for repeaters, gateways, and wireless networks commonly used in LoRa systems to realize biogas data relay, receive commands, and real-time upload. Then, the system adopts a modular design, develops the software protocol for the application layer, and partitions the network nodes in the area. Finally, the electrical drawing design, the power distribution cabinet design, the control port layout of the gas supply process monitoring and control system are carried out, and the application is formed at the demonstration point. Through application, the biogas monitoring system has the advantages of long communication distance, low power consumption, and accurate and timely data collection, and can be applied and promoted.",https://ieeexplore.ieee.org/document/8950841/,2019 International Conference on Artificial Intelligence and Advanced Manufacturing (AIAM),16-18 Oct. 2019,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CVIDL51233.2020.00-39,Research and Practice of Virtual Simulation Experiment of Computer Major in Colleges,IEEE,Conferences,"At present, in the innovative practice of computer courses and teaching reform in colleges and universities, we have adopted the innovative teaching mode of virtual simulation experiments in programming languages, combined the methods of online and offline, and the combination of virtual and real, to improve the experimental teaching mode and use experiments. The teaching virtual simulation program has set a virtual simulation experiment platform for order language. This can not only achieve the goal of improving experimental teaching, but also make up for the problems of traditional experimental teaching students' lack of initiative and poor theoretical learning effect. This article expounds on the construction of virtual simulation experiment platform and the design innovation of the teaching mode. It reflects on the way of using virtual simulation experiment in the teaching of computer science in colleges and universities, hoping to have reference value for improving the teaching level of virtual experiment teaching.",https://ieeexplore.ieee.org/document/9270549/,"2020 International Conference on Computer Vision, Image and Deep Learning (CVIDL)",10-12 July 2020,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICBAIE52039.2021.9389893,Research and implementation of network information security management system based on face recognition,IEEE,Conferences,"In recent years, with the prosperity and development of social economy and the continuous progress of science and technology, people's identity recognition field has been paid more and more attention, and face recognition has higher security than traditional verification manual, which is a hot research direction of artificial intelligence. In this paper, face recognition is carried out in the order of face preprocessing, face detection and face recognition (training). Face preprocessing mainly includes image graying, image filtering, histogram equalization and other processing steps. Face detection adopts Ada Boost based face detection algorithm[1]. The main face recognition algorithms are studied: lbphface Face recognition algorithm based on LBP, this paper uses this algorithm for face recognition. The design and implementation of the security management system based on face recognition is completed.",https://ieeexplore.ieee.org/document/9389893/,"2021 IEEE 2nd International Conference on Big Data, Artificial Intelligence and Internet of Things Engineering (ICBAIE)",26-28 March 2021,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/HPBDIS53214.2021.9658461,Research on Graph Network Recommendation Algorithm Based on Random Walk and Convolutional Neural Network,IEEE,Conferences,"As a general form of describing and modeling complex systems, networks widely exist in different scenes and tasks in various fields of the real world. Therefore, how to effectively calculate (graph Computing) and analyze (graph mining) data based on network structure has always been the core basic research direction in the field of computing science and data mining, and has been continuously studied by scholars from computer, sociology, biology and other disciplines. Network representation learning can better analyze the information hidden in complex networks, and with the help of graph neural network, it provides a universal method to solve various practical problems under the background of network structure data, which has attracted the common attention of academia and industry. At the same time, traditional recommendation algorithms generally analyze the user's rating data on items and then make preference recommendations. These methods have some problems, such as difficulty in extracting deep features, single data processing method, etc. These problems will lead to low prediction accuracy and unreasonable results. Therefore, in this paper. The recommendation algorithm for heterogeneous networks based on feature embedding is improved (RW-CNN), and the random walk algorithm of convolution and graph networks in deep learning is used to process the text feature of item names. Firstly, the network topology similarity calculation module is used to measure the consistency and complementarity of each view network, and then the word vector learning technology is used to generate the representation vector of multi view network: the word vector module is used to distinguish the complementarity between views, and the hierarchical hidden state based on multi views is used to extract these information, To retain the unique complementary information of each view; The word vector module is used to recommend multiple views with stronger consistency. Through the middle hidden state shared by multiple views, the graph convolution technology is used to aggregate information and realize the fusion of consistent information. Finally, the learned representation vector is used for link prediction and node classification tasks. The experimental results on a variety of real data sets show that the effect of this model is significantly improved.",https://ieeexplore.ieee.org/document/9658461/,2021 International Conference on High Performance Big Data and Intelligent Systems (HPBD&IS),5-7 Dec. 2021,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICAIE53562.2021.00145,Research on improving students&#x2019; interest in learning based on CiteSpace.5.7.R2,IEEE,Conferences,"To comprehensively understand the research situation and research trend in the field of ""learning interest"", this paper uses CiteSpace.5.7.R2 information visualization software as a research tool, selects 2400 literature data from the Web of Science core database from 1975 to 2021 and 984 literature data from the CNKI database from 1900 to 2021 for visual analysis, respectively from the aspects of national cooperation, institutional cooperation, keyword co-occurrence, co-citation network, etc. The results show that the current international research hotspot is the interest development model, the application of virtual reality technology, and so forth. The research hotspot in China is teaching reform. Finally, the paper puts forward the prospect of the research in the field of ""learning interest"" in China: strengthening the cooperation among colleges and universities; using science and technology in teaching, such as virtual reality, to improve students&#x2019; interest in learning.",https://ieeexplore.ieee.org/document/9534593/,2021 2nd International Conference on Artificial Intelligence and Education (ICAIE),18-20 June 2021,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICESIT53460.2021.9696498,Research on the Application of Artificial Intelligence in Computer Network Technology in Big Data Era,IEEE,Conferences,"The 21st century is an information age, and it is also a data age. The importance of data information resources is highlighted under the background of big data age. While the development of computer and its derivative technology promotes the growth of computer science, it also makes the artificial intelligence technology gradually mature. A lot of problems need to be considered, mainly including the efficiency, real-time. However, the computer network structure is becoming more and more complex, resulting in the low efficiency of computer network communication. With the development of artificial intelligence technology, it provides an effective solution to this problem. Artificial intelligence algorithm can optimize the whole network according to the scale of the network and the deployment of equipment, so as to improve the communication quality and efficiency of computer network.",https://ieeexplore.ieee.org/document/9696498/,2021 IEEE International Conference on Emergency Science and Information Technology (ICESIT),22-24 Nov. 2021,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICBASE53849.2021.00112,Research on the Application of Artificial Intelligence in Taekwondo Sport,IEEE,Conferences,"As an emerging discipline integrating computer, mathematics, linguistics, psychology, brain science, physics, computer, software, and philosophy, the field of artificial intelligence has grown to be the hottest technology field in China. The Country has been paying great attention to the innovation and development of AI, AI technology has been integrated into various aspects and scenarios such as manufacturing, life and leisure, medical and recreation, road traffic, security and surveillance. Premier Li Keqiang clearly proposed in the Government Work Report to ""expand and strengthen new industrial clusters, implement big data development action, strengthen the research and application of a new generation of artificial intelligence, promote the 'Internet +' in sports and other fields, develop intelligent industries, and expand intelligent life. The national leaders have been promoting a strong sports nation from the height of national strategy, attaching importance to the development of sports industry and emphasizing the importance of Internet + sports and artificial intelligence + sports. Taekwondo, as a popular sport worldwide, has been practiced by more than 200 countries and nearly 80 million people worldwide. However, in the process of rapid development, taekwondo also has some urgent problems that need to be solved, such as the decline in the attractiveness of the event, the difficulty of improving the performance of competition and training. Therefore, it is meaningful to contemplate on how to apply artificial intelligence technology in taekwondo for the healthy development and popularization of the sport in the new era.",https://ieeexplore.ieee.org/document/9696042/,2021 2nd International Conference on Big Data & Artificial Intelligence & Software Engineering (ICBASE),24-26 Sept. 2021,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ASE51524.2021.9678889,Restoring the Executability of Jupyter Notebooks by Automatic Upgrade of Deprecated APIs,IEEE,Conferences,"Data scientists typically practice exploratory programming using computational notebooks, to comprehend new data and extract insights. To do this they iteratively refine their code, actively trying to re-use and re-purpose solutions created by other data scientists, in real time. However, recent studies have shown that a vast majority of publicly available notebooks cannot be executed out of the box. One of the prominent reasons is the deprecation of data science APIs used in such notebooks, due to the rapid evolution of data science libraries. In this work we propose RELANCER, an automatic technique that restores the executability of broken Jupyter Notebooks, in near real time, by upgrading deprecated APIs. RELANCER employs an iterative runtime-error-driven approach to identify and fix one API issue at a time. This is supported by a machine-learned model which uses the runtime error message to predict the kind of API repair needed - an update in the API or package name, a parameter, or a parameter value. Then RELANCER creates a search space of candidate repairs by combining knowledge from API migration examples on GitHub as well as the API documentation and employs a second machine-learned model to rank this space of candidate mappings. An evaluation of RELANCER on a curated dataset of 255 un-executable Jupyter Notebooks from Kaggle shows that RELANCER can successfully restore the executability of 56% of the subjects, while baselines relying on just GitHub examples and just API documentation can only fix 38% and 36% of the subjects respectively. Further, pursuant to its real-time use case, RELANCER can restore execution to 49% of subjects, within a 5 minute time limit, while a baseline lacking its machine learning models can only fix 24%.",https://ieeexplore.ieee.org/document/9678889/,2021 36th IEEE/ACM International Conference on Automated Software Engineering (ASE),15-19 Nov. 2021,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ROBOT.2001.933270,Robotic Antarctic meteorite search: outcomes,IEEE,Conferences,"Automation of the search for and classification of Antarctic meteorites offers a unique case for early demonstration of robotics in a scenario analogous to geological exploratory missions to other planets and to the Earth's extremes. Moreover, the discovery of new meteorite samples is of great value because meteorites are the only significant source of extraterrestrial material available to scientists. In this paper we focus on the primary outcomes and technical lessons learned from the first field demonstration of autonomous search and in situ classification of Antarctic meteorites by a robot. Using a novel autonomous control architecture, specialized science sensing, combined manipulation and visual servoing, and Bayesian classification, the Nomad robot classified five indigenous meteorites during an expedition to the remote site of Elephant Moraine in January 2000. Nomad's expedition proved the rudiments of science autonomy and exemplified the merits of machine learning techniques for autonomous geological classification in real-world settings. On the other hand, the expedition showcased the difficulty in executing reliable robotic deployment of science sensors and a limited performance in the speed and coverage of autonomous search.",https://ieeexplore.ieee.org/document/933270/,Proceedings 2001 ICRA. IEEE International Conference on Robotics and Automation (Cat. No.01CH37164),21-26 May 2001,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AICAI.2019.8701282,Role of Distributed Ledger Technology (DLT) to Enhance Resiliency in Internet of Things (IoT) Ecosystem,IEEE,Conferences,"So far Internet has connected humans and now with technological advancements it is inter connecting `Things'. With more globalization and technological advancement, The Internet of Things (IoT) has been matured into self sustaining and evolving technology that has the capacity to change the way how physical and cyber worlds interact. We can also say IoT is about anything that can connect everything. With Involvement of Global Corporations, new IoT-based systems are being proposed in almost every sector which humans have so far envisioned. The thoughts (or Science Fiction) which was once fictional and unbelievable are becoming reality, whatever we desire will be available at touch of our finger someday. Mankind is moving fast towards connected future, where not only autonomous vehicles but entire cities infrastructure will be completely internet connected to support rapid urbanization. To reap the full benefit of IoT, it is imperative that the infrastructure we depend upon is adequate to deliver the services envisioned and has the necessary resilience, robustness and security. IoT can be described as an large scale, heterogeneous, ultra-complex Ecosystem acting as bridge between cyber and physical worlds. Recently, the Distributed Ledger Technology (Blockchain and Block less-different implementation of Data Structures with cryptographic algorithmic functions) has gained much attention in IoT solutions from security perspective. In This Research Paper we have explained the concepts about the functioning of Distributed Ledger Technology with focus on how it can provide security (for System Resiliency) in IoT Ecosystem.",https://ieeexplore.ieee.org/document/8701282/,2019 Amity International Conference on Artificial Intelligence (AICAI),4-6 Feb. 2019,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IPAPS52181.2020.9375539,Role of Joint 5G-IoT Framework for Smart Grid Interoperability Enhancement,IEEE,Conferences,"The ever-growing development in communication technology and very fast advances in data science are transferring the power systems in a new era. The level of autonomy is improving by means of Internet-of-Things (IoT), while the level of intelligence is improving through artificial intelligence. The applications of big data analytics and cloud computing techniques in smart grids are also new topics, which have been paid particular attention recently. These paradigms can be used in both grid-scale and local-scale, while the central grid operation center has interoperability with an abundant number of sub-controllers, and aggregators in a wide variety of scales. On the one hand, the system operator must deal with multiple parameters with different kinds of uncertainties. On the other hand, the new structures are evolving toward transactive energy trading models in microgrids. In such a circumstance, a myriad of elements is producing critical data, which should be acquired, transferred, stored, analyzed, and finally, proper controlling actions must be sent. These data are producing at different intervals, even in a fraction of a second. This matter makes it possible to maintain grid security and better real-time operation as well as to get better demand responsiveness. A smart grid consists of many embedded or interconnected systems that are linked to each other through various communication platforms in the cyber layer. A flexible, highly-autonomous, and intelligent smart grid entails an agile communication system, whether wired or wireless. However, cellular networks have prominent benefits. Hence, 5G technology, which is state-of-the-art technology in this field, can be deployed. The communication infrastructure links many components to each other in cyber-physical smart grids. The velocity of data exchange has a profound importance for some purposes, while 5G technology can be the best solution. The joint integration of IoT and 5G procures more reliability, resiliency, security, and economy.",https://ieeexplore.ieee.org/document/9375539/,2020 15th International Conference on Protection and Automation of Power Systems (IPAPS),30-31 Dec. 2020,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/eScience.2019.00014,SATVAM: Toward an IoT Cyber-Infrastructure for Low-Cost Urban Air Quality Monitoring,IEEE,Conferences,"Air pollution is a public health emergency in large cities. The availability of commodity sensors and the advent of Internet of Things (IoT) enable the deployment of a city-wide network of 1000's of low-cost real-time air quality monitors to help manage this challenge. This needs to be supported by an IoT cyber-infrastructure for reliable and scalable data acquisition from the edge to the Cloud. The low accuracy of such sensors also motivates the need for data-driven calibration models that can accurately predict the science variables from the raw sensor signals. Here, we offer our experiences with designing and deploying such an IoT software platform and calibration models, and validate it through a pilot field deployment at two mega-cities, Delhi and Mumbai. Our edge data service is able to even-out the differential bandwidths from the sensing devices and to the Cloud repository, and recover from transient failures. Our analytical models reduce the errors of the sensors from a best-case of 63% using the factory baseline to as low as 21%, and substantially advances the state-of-the-art in this domain.",https://ieeexplore.ieee.org/document/9041703/,2019 15th International Conference on eScience (eScience),24-27 Sept. 2019,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IC_ASET53395.2022.9765866,SMART Calibration Platform for a Set of Metrology Equipments: Application for spectrum analyzer,IEEE,Conferences,"With the great progress it has made, metrology is increasingly essential in various fields such as health, biology, food, chemistry and medicine. If we refer to etymology, ""metrology"" means ""science of measurement"". The business measurement approach should therefore fall within the realm of rigor and rationality by performing periodic calibration. By using new technologies (IT, Machine learning, etc.), work methods can be innovated by modernizing measurement techniques and by introducing the notion of intelligence. ""SMART Metrology"" or ""intelligent metrology"" is therefore required to take care of the automatic calibration and monitoring of equipment and will provide reliable and repeatable measurements, which will be verified by legal metrology. It is therefore used to create SMART measuring instruments that transform measurement results into controlled information. More and more companies are opting for Smart metrology for the simple reason that it modernizes the techniques of taking measurements introducing new concepts, in particular Big Data. This technology offers metrologists the opportunity to develop their profession, allowing them to actively participate in the compliance of measuring instruments and the reliability of measurements in the company. This paper presents a SMART platform for calibrating spectrum analyzers available in a metrology laboratory using Python programming software. This Human Machine Interface (HMI) makes it possible to communicate in real time with the spectrum analyzers, to take measurements automatically, to supervise the instrument remotely in order to carry out the calibration and verification of the equipment and to judge the conformity of the equipment.",https://ieeexplore.ieee.org/document/9765866/,2022 5th International Conference on Advanced Systems and Emergent Technologies (IC_ASET),22-25 March 2022,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AIVR52153.2021.00060,"ScienceVR: A Virtual Reality Framework for STEM Education, Simulation and Assessment",IEEE,Conferences,"This paper addresses the use of Virtual Reality (VR) in Science, Technology, Engineering, and Math (STEM) education. There are limited studies investigating the proper design and effectiveness of VR in STEM education, and current VR frameworks and applications lack explicit links to the established learning theories and assessment mechanisms to evaluate learning outcomes. We present ScienceVR, an educational virtual reality design framework, illustrated through a science laboratory prototype, to bridge some of the gaps identified in the design and development of a VR environment for learning. We established design guidelines and implemented an in-app data collection system to measure users’ learning, performance, and task completion rate. Our evaluation using ANOVA and other non-parametric methods with 36 participants in three groups: immersive VR (IVR), desktop VR(DVR), and 2D indicated improved usability and learning outcomes for the IVR group. Task completion rate in the IVR group was higher (68% compared to DVR with 50%). For memorability, the IVR condition performed better than DVR while for learnability, IVR&DVR performed significantly better than 2D. IVR group has performed better and faster with more accuracy compared to the DVR group in completing the tasks.",https://ieeexplore.ieee.org/document/9644266/,2021 IEEE International Conference on Artificial Intelligence and Virtual Reality (AIVR),15-17 Nov. 2021,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICICRS46726.2019.9555857,Security Protocols in Internet of Things (IoT) - A Review,IEEE,Conferences,"The technical and scientific domains have undergone a major revolution in this twenty-first century. With several new spheres have been explored to make lives simpler and solve complex challenges. The area of Computer Science and Information technology has eased people lives by introduction of Artificial Intelligence, Virtual reality. One such useful domains are the Internet of Things (IoT) that has now started a recent trend in making things simpler and easy to access. This has enabled several daily use objects to be smartly connected with various computing devices, and it can also be helpful to control them by a click of a button or sending a simple command. However, every technology gets threat with increased users and to prevent these, various security protocols have been implemented in IoT.",https://ieeexplore.ieee.org/document/9555857/,2019 International Conference on Intelligent Computing and Remote Sensing (ICICRS),19-20 July 2019,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICECCO.2015.7416901,Selection of the most prominent lines of research in ICT domain,IEEE,Conferences,"The paper is devoted to selection of the most crucial directions of research in ICT domain that could be implemented in the Republic of Kazakhstan. In the paper we evaluated the dynamics of the annual changes in the number of publications and convergence of ICT sub-domains based on data of Scopus, EBSCO (Information Science & Technology Abstracts, Academic Search Complete) and Google Scholar. To analyze the place of Kazakhstan, we considered indexes shown in the Global Competitiveness Report. As a result, the most rapidly developing areas of research were revealed (big data, machine learning, 5G, augmented reality, and etc.). The semantic network of the most modern concepts of the ICT domain was constructed that visualizes the binary relationship between the components and their relative importance. By using comparative analysis of the number of publications in the leading countries and some other countries including Kazakhstan, we selected some key domains which need to be seriously improved onto the way of development science in RK.",https://ieeexplore.ieee.org/document/7416901/,2015 Twelve International Conference on Electronics Computer and Computation (ICECCO),27-30 Sept. 2015,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISMAR.2009.5336511,SixthSense: Integrating information and the real world,IEEE,Conferences,"Pattie Maes is an associate professor in MIT's Program in Media Arts and Sciences and associate head of the Program in Media Arts and Sciences. She founded and directs the Media Lab's Fluid Interfaces research group ‹http://fluid. media.mit.edu/› which develops technologies for seamless integration of the digital world and physical world. Previously, she founded and ran the Software Agents group ‹http://fluid.media.mit.edu/›. Prior to joining the Media Lab, Maes was a visiting professor and a research scientist at the MIT Artificial Intelligence Lab. She holds bachelor's and PhD degrees in computer science from the Vrije Universiteit Brussel in Belgium. Her areas of expertise are human-computer interaction and intelligent user interfaces. Maes is the editor of three books, and is an editorial board member and reviewer for numerous professional journals and conferences. She has received several awards: /Newsweek/ magazine named her one of the “100 Americans to watch for” in the year 2000; /TIME/ Digital selected her as a member of the Cyber-Elite, the top 50 technological pioneers of the hightech world; the World Economic Forum honored her with the title “Global Leader for Tomorrow”; Ars Electronica awarded her the 1995 World Wide Web category prize; and in 2000 she was recognized with the “Lifetime Achievement Award” by the Massachusetts Interactive Media Council.",https://ieeexplore.ieee.org/document/5336511/,2009 8th IEEE International Symposium on Mixed and Augmented Reality,19-22 Oct. 2009,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICIRCA.2018.8597327,Smart Cap - Wearable Visual Guidance System for Blind,IEEE,Conferences,"Science and technology always try to make human life easier. The people who are having complete blindness or low vision faces many difficulties during their navigation. Blindness can occur due to many reasons including disease, injury or other conditions that limit vision. The main purpose of this paper is to develop a navigation aid for the blind and the visually impaired people. In this paper, we design and implement a smart cap which helps the blind and the visually impaired people to navigate freely by experiencing their surroundings. The scene around the person will be captured by using a NoIR camera and the objects in the scene will be detected. The earphones will give a voice output describing the detected objects. The architecture of the system includes the processor Raspberry Pi 3, NoIR camera, earphones and a power source. The processor collects the frames of the surroundings and convert it to voice output. The device uses TensorFlow API, open-source machine learning library developed by the Google Brain Team for the object detection and classification. TensorFlow helps in creating machine learning models capable of identifying and classifying multiple objects in a single image. Thus, details corresponding to various objects present within a single frame are obtained using TensorFlow API. A Text to Speech Synthesiser (TTS) software called eSpeak is used for converting the details of the detected object (in text format) to speech output. So the video captured by using the NoIR camera is finally converted to speech signals and thus narration of the scene describing various objects is done. Objects which come under 90 different classes like cell phone, vase, person, couch etc are detected.",https://ieeexplore.ieee.org/document/8597327/,2018 International Conference on Inventive Research in Computing Applications (ICIRCA),11-12 July 2018,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/RE.2019.00078,Specifying Requirements through Interaction Design,IEEE,Conferences,"When the requirements and the interaction design of a system are separated, they will most likely not fit together, and the resulting system will be less than optimal. Even if all the real needs are covered in the requirements and also implemented, errors may be induced by human-computer interaction through a bad interaction design and its resulting user interface. Such a system may even not be used at all. Alternatively, a great user interface of a system with features that are not required will not be very useful as well. This tutorial explains joint modeling of (communicative) interaction design and requirements, through discourse models and ontologies. Our discourse models are derived from results of human communication theories, cognitive science and sociology (even without employing speech or natural language). While these models were originally devised for capturing interaction design, it turned out that they can be also viewed as specifying classes of scenarios, i.e., use cases. In this sense, they can also be utilized for specifying requirements. Ontologies are used to define domain models and the domains of discourse for the interactions with software systems. User interfaces for these software systems can be generated semi-automatically from our discourse models, domain-of-discourse models and specifications of the requirements. This is especially useful when user interfaces for different devices are needed. Specific usability requirements can be dealt with in our approach through advanced customization approaches. Hence, inter-action design facilitates requirements engineering to make applications both more useful and usable.",https://ieeexplore.ieee.org/document/8920541/,2019 IEEE 27th International Requirements Engineering Conference (RE),23-27 Sept. 2019,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/HPCS48598.2019.9188104,Staged deployment of interactive multi-application HPC workflows,IEEE,Conferences,"Running scientific workflows on a supercomputer can be a daunting task for a scientific domain specialist. Workflow management solutions (WMS) are a standard method for reducing the complexity of application deployment on high performance computing (HPC) infrastructure. We introduce the design for a middleware system that extends and combines the functionality from existing solutions in order to create a high-level, staged usercentric operation/deployment model. This design addresses the requirements of several use cases in the life sciences, with a focus on neuroscience. In this manuscript we focus on two use cases: 1) three coupled neuronal simulators (for three different space/time scales) with in-transit visualization and 2) a closed-loop workflow optimized by machine learning, coupling a robot with a neural network simulation. We provide a detailed overview of the application-integrated monitoring in relationship with the HPC job. We present here a novel usage model for large scale interactive multi-application workflows running on HPC systems which aims at reducing the complexity of deployment and execution, thus enabling new science.",https://ieeexplore.ieee.org/document/9188104/,2019 International Conference on High Performance Computing & Simulation (HPCS),15-19 July 2019,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/HYDCON48903.2020.9242803,Story and Task Issue Analysis for Agile Machine Learning Projects,IEEE,Conferences,"The usage of Agile methodology in planning and executing machine learning (ML) and data science related software engineering projects is increasing. However, there are very few studies using real data on how effective such planning is or guidelines on how to plan such projects. In this paper, we analyze data taken from several software projects using Scrum tools. We compare the data for data science/ML and non-ML projects, in an attempt to understand if data science and ML projects are planned or executed any differently compared to normal software engineering projects. We also perform a story classification task using machine learning to analyze story logs for agile tasks for several teams. We find there are differences in what makes a good ML story as opposed to a non ML story. After analyzing this data, we propose a few ways in which software projects, whether machine learning related or not, can be better logged and executed using Scrum tools like Jira.",https://ieeexplore.ieee.org/document/9242803/,2020 IEEE-HYDCON,11-12 Sept. 2020,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ART.2002.1107002,Student projects using ARToolKit,IEEE,Conferences,"This paper describes three student projects developed at the University of Applied Science at Hagenberg (media technology and design) using ARToolKit. All projects were implemented by students during their semester projects or during their master thesis. MusicAR is the first application presented in this paper. It describes the master thesis of a student and shows a nice music learning program for children. The second application is ASR (augmented sound reality), realized by two students during their semester project. ASR allows the user to place 3D sound sources and gives the user an acoustic 3D impression of the sound sources. Finally, two students implemented ARoom, a furnishing program using AR.",https://ieeexplore.ieee.org/document/1107002/,"The First IEEE International Workshop Agumented Reality Toolkit,",29-29 Sept. 2002,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ETCCE51779.2020.9350872,Students Online Exam Proctoring: A Case Study Using 360 Degree Security Cameras,IEEE,Conferences,"Online courses, online exams and online certificates are conducted by various universities and Information Technology (IT) institutes worldwide. Delivery tools have been created for conducting the exams from any place. Applying this will lead saving time and travelling cost. Nowadays, due to the COVID-19 pandemic, there is a big demand on the online courses and exams. This paper introduces a new approach for exam proctoring using 360-degree security camera. Mainly, online exams' security is a major concern. Thus, a delivery tools must not only ensure the identity of a test- taker but also the overall test integrity. In this paper, the usage of the 360-degree security camera over the traditional webcam was investigated in order to enhance the exam security and to minimize the stressful restrictions. To verify this goal, a case study on a group of volunteer students within the college of computer science and engineering was made. In addition, an automated proctoring model that will eliminate the need for a real-time proctoring and remove any scheduling constraints in order to prevent cheating is proposed in this paper. The machine learning algorithms is exploited to enrich the proposed system. A secure frame work using the biometric is applied in order to ensure authentication and running the online exam smoothly.",https://ieeexplore.ieee.org/document/9350872/,"2020 Emerging Technology in Computing, Communication and Electronics (ETCCE)",21-22 Dec. 2020,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CISIS.2016.114,Study on Cross-Cutting and Systematic Regional Community Networks,IEEE,Conferences,"Currently, the issues for the time being have become complex and diverse in the region, such as the disappearance of the local government by the super aging population is feared. Therefore from being considered as a tool for problem-solving is ICT to progress rapidly, among the information and communication technology such as IOT(internet of things), CPS(cyber physical system), AI(Artificial intelligence), wearable devices, SNS, a variety of communication pattern and the like. On the other hand, the negative aspects of bringing a rapid development of science and technology that makes light and dark, there is a reality that has been manifest. Moreover it influences big impact on future with economic growth in Asia particular in China, and globalization of the world. What can we do in such situation? First is to visualize a variety of initiatives according to find challenges. Second is to construct the cross-cutting and systematic network for problem-solving. The regional community network is important due to mutually complementary and coordination. Their activities in cooperation with Asian countries, aims to lead to the independence of the local ICT companies.",https://ieeexplore.ieee.org/document/7791953/,"2016 10th International Conference on Complex, Intelligent, and Software Intensive Systems (CISIS)",6-8 July 2016,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/FOCS.2017.98,Subdeterminant Maximization via Nonconvex Relaxations and Anti-Concentration,IEEE,Conferences,"Several fundamental problems that arise in optimization and computer science can be cast as follows: Given vectors v1, ..., vm ∈ ℝd and a constraint family B ⊆ 2[m], find a set S ∈ B that maximizes the squared volume of the simplex spanned by the vectors in S. A motivating example is the ubiquitous data-summarization problem in machine learning and information retrieval where one is given a collection of feature vectors that represent data such as documents or images. The volume of a collection of vectors is used as a measure of their diversity, and partition or matroid constraints over [m] are imposed in order to ensure resource or fairness constraints. Even with a simple cardinality constraint (B = (r[m])), the r problem becomes NP-hard and has received much attention starting with a result by Khachiyan [1] who gave an rO(r) approximation algorithm for this problem. Recently, Nikolov and Singh [2] presented a convex program and showed how it can be used to estimate the value of the most diverse set when there are multiple cardinality constraints (i.e., when B corresponds to a partition matroid). Their proof of the integrality gap of the convex program relied on an inequality by Gurvits [3], and was recently extended to regular matroids [4], [5]. The question of whether these estimation algorithms can be converted into the more useful approximation algorithms - that also output a set - remained open. The main contribution of this paper is to give the first approximation algorithms for both partition and regular matroids. We present novel formulations for the subdeterminant maximization problem for these matroids; this reduces them to the problem of finding a point that maximizes the absolute value of a nonconvex function over a Cartesian product of probability simplices. The technical core of our results is a new anti-concentration inequality for dependent random variables that arise from these functions which allows us to relate the optimal value of these nonconvex functions to their value at a random point. Unlike prior work on the constrained subdeterminant maximization problem, our proofs do not rely on real-stability or convexity and could be of independent interest both in algorithms and complexity where anti-concentration phenomena has recently been deployed.",https://ieeexplore.ieee.org/document/8104130/,2017 IEEE 58th Annual Symposium on Foundations of Computer Science (FOCS),15-17 Oct. 2017,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCICA52458.2021.9697222,Supervised Classification for Analysis and Detection of Potentially Hazardous Asteroid,IEEE,Conferences,"The use of Artificial Intelligence (AI) in solving real- time problems are increasing day by day with the increase in the availability of data and computation power. It is now substantial to use AI-based tools and techniques in space science. Asteroids, rocky objects that orbit around the sun, often produce an array of effects that cause harm to humans and biodiversity on earth. Such effects can cause wind blast, overpressure shock, thermal radiation, cratering, seismic shaking, ejecta deposition, tsunami, and many more. With the availability of data on asteroid parameters and nature, it provides an opportunity to use Machine Learning (ML) to address this problem and reduce the risk. This paper presents a thorough study on the impact of Potentially Hazardous Asteroids (PHAs) and proposes a supervised machine learning method to detect whether an asteroid with specific parameters is hazardous or not. We compare manifold classification algorithms that were implemented on the data. Random forest gave the best performance in terms of accuracy (99.99%) and average F1- score (99.22%).",https://ieeexplore.ieee.org/document/9697222/,2021 International Conference on Computational Intelligence and Computing Applications (ICCICA),26-27 Nov. 2021,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICAIE50891.2020.00111,Supplemental Cultivation Plan of Innovation Quality for the Undergraduates of Building Environment and Energy Engineering in the Applied Technology Universities,IEEE,Conferences,"One of the most fundamental qualities for science research and technology development is the innovation quality, which covers many aspects such as consciousness, method and spirit. The innovation quality developed in the undergraduate period would be the most basic foundation in their over forty-years working life. A supplemental cultivation plan with a routine consisting of hardware, programming, simulation and optimization was presented in this paper for the undergraduates of Building Environment and Energy Engineering (BEEE). The hardware manufacture of digital devices was trained to promote the freshmen manipulative capability, while software programming with general-purpose languages was introduced to enhance the comprehension of data analysis in the 2nd academic year. During the period when some professional courses were taught, the building simulation modular could be introduced to enhance the understanding for the building thermal process and its corresponding Heating, Ventilating, Air-conditioning and Refrigeration (HVAC & R) system. A building management system (BMS) would be developed step by step and interactive with virtual building platform. That BMS and its virtual cases could be employed as the testbed for optimization of HVAC & R system, such as energy consumption prediction, fault detection and diagnosis. Python and EnergyPlus were used in the custom training program oriented to the employment direction for the system operation and maintenance. Group achievements and individual cases show that the presented supplemental cultivation plan played a very important role in the cultivation of BEEE graduates' innovative quality. The effect for the graduated students need pay more attention to investigate the long-term effects, and more volunteers will participate in this plan to find out their better future.",https://ieeexplore.ieee.org/document/9262525/,2020 International Conference on Artificial Intelligence and Education (ICAIE),26-28 June 2020,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICDMW51313.2020.00082,SynC: A Copula based Framework for Generating Synthetic Data from Aggregated Sources,IEEE,Conferences,"A synthetic dataset is a data object that is generated programmatically, and it may be valuable to creating a single dataset from multiple sources when direct collection is difficult or costly. Although it is a fundamental step for many data science tasks, an efficient and standard framework is absent. In this paper, we study a specific synthetic data generation task called downscaling, a procedure to infer high-resolution, harder-to-collect information (e.g., individual level records) from many low-resolution, easy-to-collect sources, and propose a multi-stage framework called SynC (Synthetic Data Generation via Gaussian Copula). For given low-resolution datasets, the central idea of SynC is to fit Gaussian copula models to each of the low-resolution datasets in order to correctly capture dependencies and marginal distributions, and then sample from the fitted models to obtain the desired high-resolution subsets. Predictive models are then used to merge sampled subsets into one, and finally, sampled datasets are scaled according to low-resolution marginal constraints. We make four key contributions in this work: 1) propose a novel framework for generating individual level data from aggregated data sources by combining state-of-the-art machine learning and statistical techniques, 2) perform simulation studies to validate SynC's performance as a synthetic data generation algorithm, 3) demonstrate its value as a feature engineering tool, as well as an alternative to data collection in situations where gathering is difficult through two real-world datasets, 4) release an easy-to-use framework implementation for reproducibility and scalability at the production level that easily incorporates new data.",https://ieeexplore.ieee.org/document/9346329/,2020 International Conference on Data Mining Workshops (ICDMW),17-20 Nov. 2020,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISCAS.2011.5937705,Systematic configuration and automatic tuning of neuromorphic systems,IEEE,Conferences,"In the past recent years several research groups have proposed neuromorphic Very Large Scale Integration (VLSI) devices that implement event-based sensors or biophysically realistic networks of spiking neurons. It has been argued that these devices can be used to build event-based systems, for solving real-world applications in real-time, with efficiencies and robustness that cannot be achieved with conventional computing technologies. In order to implement complex event-based neuromorphic systems it is necessary to interface the neuromorphic VLSI sensors and devices among each other, to robotic platforms, and to workstations (e.g. for data-logging and analysis). This apparently simple goal requires painstaking work that spans multiple levels of complexity and disciplines: from the custom layout of microelectronic circuits and asynchronous printed circuit boards, to the development of object oriented classes and methods in software; from electrical engineering and physics for analog/digital circuit design to neuroscience and computer science for neural computation and spike-based learning methods. Within this context, we present a framework we developed to simplify the configuration of multi-chip neuromorphic VLSI systems, and automate the mapping of neural network model parameters to neuromorphic circuit bias values.",https://ieeexplore.ieee.org/document/5937705/,2011 IEEE International Symposium of Circuits and Systems (ISCAS),15-18 May 2011,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AIMS.2013.4,Table of contents,IEEE,Conferences,"The following topics are dealt with: artificial intelligence; neural networks and fuzzy systems; evolutionary computation; bioinformatics and bioengineering; data and semantic mining; games, VR and visualization; intelligent systems and applications; systems intelligence; control intelligence; e-science and e-systems; robotics, cybernetics, engineering, and manufacturing; operations research; discrete-event and real-time systems; image, speech and signal processing; industry, business, management, human factors and social issues; energy, power, transport, logistics, harbour, shipping and marine simulation; parallel, distributed, and software architectures and systems; mobile-ad hoc wireless networks, Mobicast, sensor placement, and target tracking; performance engineering of computer and communications systems; and circuits and devices.",https://ieeexplore.ieee.org/document/6959881/,"2013 1st International Conference on Artificial Intelligence, Modelling and Simulation",3-5 Dec. 2013,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCEA.2010.4,Table of contents - Volume 1,IEEE,Conferences,The following deals with the following topics: algorithms; artificial intelligence; software engineering; bioinformatics; computer graphics; computer architecture; information systems; computer aided instruction; computer games; virtual reality; data security; digital simulation; computer aided design; ethical aspects; database systems; digital libraries; signal processing; image processing; logic design; e-commerce; human computer interaction; embedded systems; Internet; mobile computing; multimedia systems; natural language processing; neural networks; programming languages; robotics; control systems; theoretical computer science; and wireless sensor networks.,https://ieeexplore.ieee.org/document/5445635/,2010 Second International Conference on Computer Engineering and Applications,19-21 March 2010,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCRD.2011.5764067,Table of contents vol. 01,IEEE,Conferences,The following topics are dealt with: computer research and development; event driven programming; artificial intelligence; expert systems; algorithm analysis; high performance computing; automated software engineering; human computer interaction; bioinformatics; scientific computing; image processing; information retrieval; compilers; interpreters; computational intelligence; computer architecture; embedded systems; computer animation; Internet; Web applications; communication/networking; knowledge data engineering; computer system implementation; logics; VLSI; mathematical software; information systems; computer based education; mathematical logic; mobile computing; computer games; multimedia applications; computer graphics; virtual reality; natural language processing; neural networks; computer modeling; parallel computing; distributed computing; computer networks; pattern recognition; computer security; computer simulation; computer vision; probability; statistics; performance evaluation; computer aided design/manufacturing; computing ethics; programming languages; problem complexity; control systems; physical sciences; engineering; discrete mathematics; reconfigurable computing systems; data communications; robotics; automation; system security; cryptography; data compression; data encryption; data mining; database systems; document processing; text processing; educational technology; digital library; technology management; digital signal processing; theoretical computer science; digital systems; logic design; ubiquitous computing; and visualizations.,https://ieeexplore.ieee.org/document/5764067/,2011 3rd International Conference on Computer Research and Development,11-13 March 2011,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICDE53745.2022.00322,Tell Me How to Survey: Literature Review Made Simple with Automatic Reading Path Generation,IEEE,Conferences,"Recent years have witnessed the dramatic growth of paper volumes with plenty of new research papers published every day, especially in the area of computer science. How to glean papers worth reading from the massive literature to do a quick survey or keep up with the latest advancement about a specific research topic has become a challenging task. Existing academic search engines return relevant papers by individually calculating the relevance between each paper and query. However, such systems usually omit the prerequisite chains of a research topic and cannot form a meaningful reading path. In this paper, we introduce a new task named Reading Path Generation (RPG) which aims at automatically producing a path of papers to read for a given query. To serve as a research benchmark, we further propose SurveyBank, a dataset consisting of large quantities of survey papers in the field of computer science as well as their citation relationships. Furthermore, we propose a graph-optimization-based approach for reading path generation which takes the relationship between papers into account. Extensive evaluations demonstrate that our approach outperforms other baselines. A real-time Reading Path Generation (RePaGer) system has been also implemented with our designed model. Our source code and SurveyBank dataset can be found here<sup>1</sup><sup>1</sup>https://github.com/JiayuanDing100/Reading-Path-Generation.",https://ieeexplore.ieee.org/document/9835398/,2022 IEEE 38th International Conference on Data Engineering (ICDE),9-12 May 2022,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCSE51940.2021.9569551,The Integration Development of Artificial Intelligence and Education,IEEE,Conferences,"With the rapid progress and development of modern information science and technology, artificial intelligence technology has become more and more extensive in many fields. How to incorporate artificial intelligence into education has become a hot topic of the whole society. In this paper, analysis of artificial intelligence used to extract application potential and value of intelligent correction, real-time monitoring, education fairness and campus safety. But there are also challenges in personality education, safety ethics, teaching efficiency, etc. In order to make artificial intelligence better serve the education industry, it is necessary to increase the infrastructure construction and environment configuration of artificial intelligence equipment. And then improving the education practitioners’ awareness and correct cognition of the relationship between intelligent machine safety ethics and artificial intelligence.",https://ieeexplore.ieee.org/document/9569551/,2021 16th International Conference on Computer Science & Education (ICCSE),17-21 Aug. 2021,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ITAIC54216.2022.9836597,The Logistics Barcode ID Character Recognition Method Based on AKAZE Feature Localization,IEEE,Conferences,"With the continuous development of science and technology, the rapid rise of e-commerce, and the continuous influx of the logistics industry into our lives. In the transportation of logistics, the sorting is the mainly work, it needs machines or staff using barcode scanners to identify barcodes to achieve sorting. Meantime, it is necessary to align the barcode of the item manually or mechanically with the barcode scanner to accurately identify it. Undoubtedly, a lot of manpower and material resources are required in the sorting process, so the method of computer vision processing is used to achieve automatic detection and identification of barcode ID will have great significance. The authors propose a logistics barcode ID character recognition method based on local positioning of AKAZE features to realize the detection and identification of logistics barcode ID in a long distance and a large range. First, extracting the AKAZE feature of the barcode in the image, using the matching algorithm to localize the barcode area, and then use the OCR (Optical Character Recognition) method based on deep learning to perform the character recognition of the barcode ID. A large number of experiments have been carried out with the real existing parcel barcodes, in which the accuracy rate reaches 99.6&#x0025;, the speed reaches 0.35s/frame. Besides, the detection and identification of barcode IDs of different types and different length codes can be realized. The adoption in the future will further promote the feasibility of the logistics and transportation industry.",https://ieeexplore.ieee.org/document/9836597/,2022 IEEE 10th Joint International Information Technology and Artificial Intelligence Conference (ITAIC),17-19 June 2022,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ELECTR.1994.472691,The broad use of neural networks in real-time engineering systems,IEEE,Conferences,"Neural networks is a very popular emerging technology that has applications in a wide variety of fields such as: electrical and computer engineering, signal processing, computer science, cognitive science, neurobiology, optics, mathematics, and physics. Properties of this multidisciplinary field, originally inspired by biological systems, are adaptiveness and self organization, massively parallel nature, robustness and fault- and noise-tolerance. This paper, after presenting an overview of neural networks, their properties and advantages over conventional computers, will discuss how novel neural network architectures and implementations can be included in larger systems to provide solutions to scientific and engineering problems. An example of a neural network hearing model is discussed where neural networks and time-frequency analysis are used to extract essential information from complex inputs.<>",https://ieeexplore.ieee.org/document/472691/,Proceedings of ELECTRO '94,10-12 May 1994,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICACITE51222.2021.9404738,The learning approaches using Augmented Reality in learning environments: Meta-Analysis,IEEE,Conferences,"With the emergence of Industrial Revolution 4.0, the educational settings are changing quickly. Augmented Reality (AR) is one of the upcoming technologies. AR enhances the real world by overlaying/augmenting the virtual/digital information over it. It provides the user with the ability to interact with the created virtual world in real space. The aim of this study is to classify the learning approaches implemented through AR technology. The technique used for the analysis is derived from systematic search of online literature databases like Taylor Francis, Web of Science, Springer, ScienceDirect and Scopus. The keywords used for the search include learning approaches, AR, AR in education, AR in learning and teaching and integration approaches. The findings of this research work highlights 4 categories of educational learning approaches that highlight AR. The approaches are experimental learning, game-based, interactive and collaborative learning. The research findings can be referred by other researchers and educators to identify the potential of AR in education and the learning approaches currently used with AR for their further research on how these approaches can be effectively and efficiently implemented in educational settings.",https://ieeexplore.ieee.org/document/9404738/,2021 International Conference on Advance Computing and Innovative Technologies in Engineering (ICACITE),4-5 March 2021,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCI-CC.2017.8109799,The power of cognitive computing: An example of cognitive dynamic regional development modeling,IEEE,Conferences,"This presentation concerns some idea of what could be done, in the author's view, to help make Wang's cognitive informatics a powerful and viable source of tools and techniques for solving various real life problems. First, we give a brief account of cognitive informatics meant as a multidisciplinary field within informatics, or computer science, that is based on results of cognitive and information sciences, and which deals with human information processing mechanisms and processes and their decision theoretic, engineering, etc. applications in broadly perceived computing. We focus on its purpose, i.e. to develop and implement technologies to facilitate and extend the information acquisition, comprehension and processing capacity of humans. Emphasis is on underlying processes in the brain. However, we advocate an extended approach in which though the very cognitive informatics is the foundation, as those processes in the brain are crucial, some sort of an “outer” cognitive informatics is needed which explicitly makes reference not what proceeds “internally” in the brain, because we do not “see” this, but “externally”, i.e. what people can see, judge, evaluate, etc., and what is clearly a result of cognitive information specific processes in the brain. This line of reasoning is in line with the very essence of comprehension, memorizing, learning, choice and decision making, satisfaction with partial truth, allowing for not perfect solutions, etc. dealt with using tools and techniques derived from many areas like psychology, behavioral science, neuroscience, artificial intelligence, linguistics, neuroeconomics etc. In our case, we will concentrate on some cognitive informatics type elements that mostly have been inspired by psychology and behavioral sciences, as our problem is inherently related to human judgments and perceptions, but we will mentioned some inspirations from neuroscience, notably along the lines of neuroeconomics. Cognitive informatics constitutes a foundation of its related new field, cognitive computing, which is basically a new direction in broadly perceived intelligent computing and systems that synergistically combines results from many areas, e.g., information science, computational sciences, computer science, artificial and computational intelligence, cybernetics, systems science, cognitive science, (neuro)psychology, brain science, linguistics, etc. to just mention a few. We try to show on an example of a dynamic systems modeling, more specifically scenario based regional development planning, that cognitive computing can provide new conceptual and implementation vistas. Basically, we consider a region that is characterized by 7 life quality indicators related to economic, social, environmental, etc. qualities, which evolve over some planning horizon due to some investments, mostly by some regional or governmental agencies. There are some scenarios of investment levels over the planning horizon, meant for the development of the particular life quality indexes, and some desired levels of these indexes, both objective, i.e. set by authorities, and subjective, i.e. perceived by the inhabitant groups. As a result of a particular investment scenario, the life quality indexes evolve over the planning horizon, and their temporal evolution is evaluated by the authorities and inhabitants. This evaluation has both an objective, i.e. against the “officially” set thresholds, and subjective, i.e. as perceived by various humans and their groups. Basically, we employ Kacprzyk's fuzzy dynamic programming based approach to the modeling and planning/programming of sustainable regional development, with soft constraints and goals, but we advocated a more sophisticated assessment of variability, stability, balancedness of consecutive investments. In this process we try to develop evaluation measures, and then the optimization type model using concepts that can be effectively and efficiently handled by cognitive computing, notably the inclusion of the so called decision making and behavioral biases, biases in probability and belief, social biases, memory errors, etc. Moreover, we strongly reflect the so called status quo and minimal change biases. By using many results from social sciences, psychology, behavioral economics, neuroeconomics, etc. on human judgments and human centric evaluations, we augment a traditional purely effectiveness and efficiency oriented analysis by a more sophisticated analysis of effects of variability of temporal evolution of some life quality indicators on the human perception of its goodness. The model presented, which has been employed for years as part of large mathematical modeling projects for sustainable regional development in many regions in Asia and Europe, is illustrated on an example with scenario analysis for a rural region plagued by social and economic difficulties in which subsidies should properly be distributed over time to obtain a best overall socioeconomic effect. In this talk we present the model in a different perspective, based first on the basic Wang's cognitive informatics and its Wang and Ruhe's decision making application, and then based on new, more comprehensive cognitive computing. We show that this provides a novel insight.",https://ieeexplore.ieee.org/document/8109799/,2017 IEEE 16th International Conference on Cognitive Informatics & Cognitive Computing (ICCI*CC),26-28 July 2017,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SIIE.2014.7017703,The use of SCRUM for laboratory sessions monitoring and evaluation in a university course enforcing transverse competencies,IEEE,Conferences,"This paper introduces a pilot experience on the use of SCRUM methodology, borrowed from software project development and management, in group laboratory sessions during a university course of agent-based programming (fourth year of the Degree on Computer Science in the University of Granada). It aims to foster a higher awareness of students and to enforce a series of transverse competencies like real, structured and self-organized teamwork, better planning of time during the laboratory sessions. As a side effect, it empowers leadership amongst the members of the team as well as it enables a high quality evaluation of students.",https://ieeexplore.ieee.org/document/7017703/,2014 International Symposium on Computers in Education (SIIE),12-14 Nov. 2014,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICBASE53849.2021.00073,Time series simulation method of meteorological elements based on ARIMA model,IEEE,Conferences,"With the development of science and technology and economy, environmental monitoring technology and means are constantly enriched and improved, and the information of meteorological big data is gradually diversified, which brings convenience to the study of meteorological trend that thus provides corresponding guidance for industrial and agricultural production. In recent years, the construction of environmental monitoring stations has been accelerated, and they are popular in the study of meteorological trends in various countries because of their real-time characteristics and wide range of data. In this paper, the characteristics of key meteorological elements at different time scales are statistically analyzed firstly based on the measured meteorological data of regional meteorological stations that meteorological elements have significant tendency and seasonality. Secondly, the obtained meteorological big data is pre-processed to eliminate bad data, and then the probability distribution of meteorological elements and their change rules is extracted. Finally, a time series method based on ARIMA model is proposed to simulate the rule of weather change, so as to obtain the time series of temperature and rainfall in the next ten years, which can provide theoretical and data support for various industries, and help meteorological and security departments to make work plans in time and correctly.",https://ieeexplore.ieee.org/document/9696084/,2021 2nd International Conference on Big Data & Artificial Intelligence & Software Engineering (ICBASE),24-26 Sept. 2021,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/MACISE49704.2020.00022,Toward General AI: Consciousness Computational Modeling Under Uncertainty,IEEE,Conferences,"The modern expert and statistical computing systems of Artificial Intelligence do not understand what they are doing when they operate under uncertainty and changing situations in the fuzzy environment. Towards a solution this problem, we propose the method, model and system of Consciousness Computational Modeling Under Uncertainty for Systems of General Artificial Intelligence. The functionality of this concept is implemented by developed methods and models of organization of neural semantic network Memory, using the self-organized Subject Area Thesaurus (Knowledge Base), of situational Fuzzy Control of data, information, knowledge and objects, Fuzzy Inference, Decision Making, Knowledge Representation, Knowledge Generalization, Knowledge Explanation, Reasoning, Systems Thinking, Cognition, Intelligent User Interface, Computational Systemic Mind, Awareness and Full Understanding of the reality. For modeling of this functionality, the main achievements of Systems Approach, Fuzzy Logic, Situational Control, Linguistics and Computer Science at whole were applied.",https://ieeexplore.ieee.org/document/9195575/,2020 International Conference on Mathematics and Computers in Science and Engineering (MACISE),14-16 Jan. 2020,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/OCEANSChennai45887.2022.9775330,Towards Real-time Analysis of Marine Phytoplankton Images Sampled at High Frame Rate by a YOLOX-based Object Detection Algorithm,IEEE,Conferences,"Rapid and quantitative analysis of phytoplankton cells in natural seawater is of great need for marine ecological science research and harmful algae bloom monitoring applications. In this paper, we propose a YOLOX network-based object detection algorithm exclusively for high-throughput real-time analysis of phytoplankton fluorescence images collected by the FluoSieve<sup>&#x00AE;</sup> imaging flow cytometer. Based on an active learning strategy, we first annotate and construct a FluoPhyto dataset of red tide phytoplankton species fluorescence images commonly found in the South and East China Sea, which contains a total of 30,339 images in 32 different categories. Using the dataset, we train the Faster-RCNN, SSD, YOLOv3 and YOLOX networks, and the comparison result shows that the performance of YOLOX network outperforms the other networks, which can reach a mean average precision (mAP) of 90.9%. The trained YOLOX model is then deployed on an embedded GPU module and the inference speed is tested to reach 20 fps with the help of TensorRT optimization, which can hopefully meet the real-time detection requirements of the instrument. In addition, the algorithm is run on the embedded platform for detection of images collected in a red tide event that happened near the Pearl River Estuary, and the key parameters such as abundance and size spectrum of the dominant species, Cochlodinium geminatum, are obtained, which confirms the feasibility and superior performance of the detection algorithm.",https://ieeexplore.ieee.org/document/9775330/,OCEANS 2022 - Chennai,21-24 Feb. 2022,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCI-CC.2018.8482093,Towards a Methodology for RTPA-MATLAB Code Generation Based on Machine Learning Rules,IEEE,Conferences,"Autonomous program code generation by machine learning is not only an ultimate goal but also a theoretical challenge to software science and engineering. A methodology and case study for code generation based on Real-Time Process Algebra (RTPA) by machine learning are presented in this paper. It describes a machine learning approach for code generation in MATLAB based on acquired RTPA rules and formal specifications. The design and implementation of the RTPA-MATLAB code generator is introduced, which is implemented by an RTPA parser and an MATLAB code builder. The experimental case studies have demonstrated the novelty of the theories and methodologies for code generation based on machine-learnt programming rules.",https://ieeexplore.ieee.org/document/8482093/,2018 IEEE 17th International Conference on Cognitive Informatics & Cognitive Computing (ICCI*CC),16-18 July 2018,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CCGrid.2015.74,Towards a Realistic Scheduler for Mixed Workloads with Workflows,IEEE,Conferences,"Many fields of modern science require huge amounts of computation, and workflows are a very popular tool in e-Science since they allow to organize many small, simple tasks to solve big problems. They are used in astronomy, bioinformatics, machine learning, social network analysis, physics, and many other branches of science. Workflows are notoriously difficult to schedule, and the vast majority of research on workflow scheduling is concerned with scheduling single workflows with known runtimes. The goal of this PhDresearch is to bring more realism to the problem of workflow scheduling in actual systems. First, in real systems, multiple workflows may be contending for the available resources. Second, task runtime estimates are not always known, and task runtime estimates may be wrong. Third, workflows are usually not the only type of jobs submitted to a system, there may for instance also be parallel applications and bags-of-tasks. Accordingly, the purpose of this PhD research is to create and analyze policies for online scheduling of workloads of workflows with and without known task runtimes that also contain jobs of other types. We are in the process of simulating policies, and we will validate our results by means of an implementation and real-world experiments with the KOALA-Workflow processing system.",https://ieeexplore.ieee.org/document/7152549/,"2015 15th IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing",4-7 May 2015,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CISIS.2015.20,Towards an Analytical Framework to Enhance Teaching Support in Digital Systems Design Course,IEEE,Conferences,"Learning digital systems design is a difficult skill that students of Bachelors on Computer Science, Electronic Engineering or Telecommunications have to acquire in the initial courses. The problem aggravates when the student is learning in a virtual environment with no face-to-face interaction with the instructor. In this case, simulators or intelligent tutoring systems, such as Veril UOC [1], are used in order to acquire this skill. This paper describes several improvements introduced in the instructor view of this framework Veril UOC. The new information that can be extracted of the system can improve the personalized feedback that the instructor can perform to the students, and, at the same time, the real-time information gives insights related to the course's operation.",https://ieeexplore.ieee.org/document/7185179/,"2015 Ninth International Conference on Complex, Intelligent, and Software Intensive Systems",8-10 July 2015,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CA.2001.982382,Towards the Holodeck: building emotional virtual humans for training,IEEE,Conferences,"I describe a collaborative effort between members of the entertainment and research communities to advance the state of immersive training technology. Pulling together Hollywood's expertise in story, visual effects and production, with expertise in graphics, gaming, artificial intelligence, linguistics, cognitive science, and audio processing, a group of researchers, storytellers and graphic artists is trying to approximate the Holodeck (the ultimate training and entertainment device of the 24th century, as popularized by the TV series Star Trek).",https://ieeexplore.ieee.org/document/982382/,Proceedings Computer Animation 2001. Fourteenth Conference on Computer Animation (Cat. No.01TH8596),7-8 Nov. 2001,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/COMPSAC48688.2020.0-230,Training Confidence-Calibrated Classifier via Distributionally Robust Learning,IEEE,Conferences,"Supervised learning via empirical risk minimization, despite its solid theoretical foundations, faces a major challenge in generalization capability, which limits its application in real-world data science problems. In particular, current models fail to distinguish in-distribution and out-of-distribution and give over confident predictions for out-of-distribution samples. In this paper, we propose an distributionally robust learning method to train classifiers via solving an unconstrained minimax game between an adversary test distribution and a hypothesis. We showed the theoretical generalization performance guarantees, and empirically, our learned classifier when coupled with thresholded detectors, can efficiently detect out-of-distribution samples.",https://ieeexplore.ieee.org/document/9201929/,"2020 IEEE 44th Annual Computers, Software, and Applications Conference (COMPSAC)",13-17 July 2020,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCAD45719.2019.8942103,Tucker Tensor Decomposition on FPGA,IEEE,Conferences,"Tensor computation has emerged as a powerful mathematical tool for solving high-dimensional and/or extreme-scale problems in science and engineering. The last decade has witnessed tremendous advancement of tensor computation and its applications in machine learning and big data. However, its hardware optimization on resource-constrained devices remains an (almost) unexplored field. This paper presents an hardware accelerator for a classical tensor computation framework, Tucker decomposition. We study three modules of this architecture: tensor-times-matrix (TTM), matrix singular value decomposition (SVD), and tensor permutation, and implemented them on Xilinx FPGA for prototyping. In order to further reduce the computing time, a warm-start algorithm for the Jacobi iterations in SVD is proposed. A fixed-point simulator is used to evaluate the performance of our design. Some synthetic data sets and a real MRI data set are used to validate the design and evaluate its performance. We compare our work with state-of-the-art software toolboxes running on both CPU and GPU, and our work shows 2.16 – 30.2× speedup on the cardiac MRI data set.",https://ieeexplore.ieee.org/document/8942103/,2019 IEEE/ACM International Conference on Computer-Aided Design (ICCAD),4-7 Nov. 2019,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CEC.2014.6900322,Type-II opposition-based differential evolution,IEEE,Conferences,"The concept of opposition-based learning (OBL) can be categorized into Type-I and Type-II OBL methodologies. The Type-I OBL is based on the opposite points in the variable space while the Type-II OBL considers the opposite of function value on the landscape. In the past few years, many research works have been conducted on development of Type-I OBL-based approaches with application in science and engineering, such as opposition-based differential evolution (ODE). However, compared to Type-I OBL, which cannot address a real sense of opposition in term of objective value, the Type-II OBL is capable to discover more meaningful knowledge about problem's landscape. Due to natural difficulty of proposing a Type-II-based approach, very limited research has been reported in that direction. In this paper, for the first time, the concept of Type-II OBL has been investigated in detail in optimization; also it is applied on the DE algorithm as a case study. The proposed algorithm is called opposition-based differential evolution Type-II (ODE-II) algorithm; it is validated on the testbed proposed for the IEEE Congress on Evolutionary Computation 2013 (IEEE CEC-2013) contest with 28 benchmark functions. Simulation results on the benchmark functions demonstrate the effectiveness of the proposed method as the first step for further developments in Type-II OBL-based schemes.",https://ieeexplore.ieee.org/document/6900322/,2014 IEEE Congress on Evolutionary Computation (CEC),6-11 July 2014,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SCCC.1999.810151,Virtual guides to assist visitors in the SAGRES virtual museum,IEEE,Conferences,"The SAGRES system is an environment on the Web, that facilitates the presentation of museum information, in a manner adapted to the individual characteristics of each visitor. The interaction with the system may occur individually or in groups of students. The system also offers resources to support some forms of cooperative learning, allowing visitors to interact both synchronously and asynchronously, and both locally, inside the museum, and non-locally, in places geographically distant from the museum. The visitors are assisted by virtual guides, software agents, which monitor the visitors' actions. The agents use the directed improvisation human-computer interaction paradigm to interact with visitors. They improvise, while helping the visitor, a group of behaviors similar to human behaviors (happiness, satisfaction and vibration), making the interaction more friendly. This system has been developed in the Museum of Science and Technology at PUCRS, Porto Alegre, Brazil.",https://ieeexplore.ieee.org/document/810151/,Proceedings. SCCC'99 XIX International Conference of the Chilean Computer Science Society,13-13 Nov. 1999,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/FIE.2006.322736,Work in Progress: An Intelligent Tutoring System for Forensic Biology,IEEE,Conferences,"The Interactive Virtual Intelligent System for Scientific Inquiry in a Biology Learning Environment (INVISSIBLE) is software environment being developed as a intelligent tutoring system that provides high school biology students a virtual, hands-on, multimedia learning environment. Using interactive, intelligent software, a student is placed in goal driven scenarios that reflect the authentic experiences of a scientist engaged in using scientific inquiry methods. This paper describes the first of three planned modules, one which involves forensic science and the use of DNA evidence in combination with hairs, fibers, and other evidence in solving a crime scene problem. Core objectives of this module are to increase student learning regarding: (a) knowledge acquisition of content, concepts and principles relevant to genetics, forensic science, and evolutionary biology, (b) relevant scientific process skills and knowledge, and (c) knowledge of nature and methods of science. A demonstration of the software will be given",https://ieeexplore.ieee.org/document/4117057/,Proceedings. Frontiers in Education. 36th Annual Conference,27-31 Oct. 2006,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/FIE.2010.5673167,Work in progress — A biomedical motif for teaching Artificial Intelligence in Context,IEEE,Conferences,"The majority of the classes in a typical computer science program focus on the computer as an entity and the resulting development infrastructure. Examples include hardware principles, architecture and theory, along with software development tools and processes. Recent studies have suggested that a focus on the computational infrastructure in the absence of an external application domain may reinforce the stereotype of CS student fixation on the computer for some, while alienating a subset of students, especially women, who may be attracted to using the computer to solve broader problems. Recognizing this situation has inspired recent explorations on teaching computer science within an application context. Even classes traditionally focused on techniques and problems beyond the computer itself, such as Computer Graphics and Artificial Intelligence (AI), are often presented as a collection of techniques, programs, and principles without systematic ""real-world"" grounding. Inspired by work on teaching Computer Graphics in context, this work-in-progress describes an infrastructure for embedding a biomedical motif into the traditional Artificial Intelligence course.",https://ieeexplore.ieee.org/document/5673167/,2010 IEEE Frontiers in Education Conference (FIE),27-30 Oct. 2010,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IPDPSW.2019.00142,Workflow-Driven Distributed Machine Learning in CHASE-CI: A Cognitive Hardware and Software Ecosystem Community Infrastructure,IEEE,Conferences,"The advances in data, computing and networking over the last two decades led to a shift in many application domains that includes machine learning on big data as a part of the scientific process, requiring new capabilities for integrated and distributed hardware and software infrastructure. This paper contributes a workflow-driven approach for dynamic data-driven application development on top of a new kind of networked Cyberinfrastructure called CHASE-CI. In particular, we present: 1) The architecture for CHASE-CI, a network of distributed fast GPU appliances for machine learning and storage managed through Kubernetes on the high-speed (10-100Gbps) Pacific Research Platform (PRP); 2) A machine learning software containerization approach and libraries required for turning such a network into a distributed computer for big data analysis; 3) An atmospheric science case study that can only be made scalable with an infrastructure like CHASE-CI; 4) Capabilities for virtual cluster management for data communication and analysis in a dynamically scalable fashion, and visualization across the network in specialized visualization facilities in near real-time; and, 5) A step-by-step workflow and performance measurement approach that enables taking advantage of the dynamic architecture of the CHASE-CI network and container management infrastructure.",https://ieeexplore.ieee.org/document/8778399/,2019 IEEE International Parallel and Distributed Processing Symposium Workshops (IPDPSW),20-24 May 2019,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AIVR46125.2019.00065,XR for Augmented Utilitarianism,IEEE,Conferences,"Steady progresses in the AI field create enriching possibilities for society while simultaneously posing new complex challenges of ethical, legal and safety-relevant nature. In order to achieve an efficient human-centered governance of artificial intelligent systems, it has been proposed to harness augmented utilitarianism (AU), a novel non-normative ethical framework grounded in science which can be assisted e.g. by Extended Reality (XR) technologies. While AU provides a scaffold to encode human ethical and legal conceptions in a machine-readable form, the filling in of these conceptions requires a transdisciplinary amalgamation of scientific insights and preconditions from manifold research areas. In this short paper, we present a compact review on how XR technologies could leverage the underlying transdisciplinary AI governance approach utilizing the AU framework. Towards that end, we outline pertinent needs for XR in two hereto related contexts: as experiential testbed for AU-relevant moral psychology studies and as proactive AI Safety measure and enhancing policy-by-simulation method preceding the deployment of AU-based ethical goal functions.",https://ieeexplore.ieee.org/document/8942355/,2019 IEEE International Conference on Artificial Intelligence and Virtual Reality (AIVR),9-11 Dec. 2019,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/BigData50022.2020.9378023,Zero-Shot Machine Learning Technique for Classification of Multi-User Big Data Workloads,IEEE,Conferences,"During the last decade machine learning has revolutionized computer science applications. Supervised machine learning algorithms have become especially successful in many industries including health, legal, security, finance, travel, and others. Training supervised learning algorithms, however, is expensive because the real world contains a very large number of different classes that need to be covered by the training set. This is especially true for the highly variable, multi-user workload data produced by the strategically vital big data and cloud software stacks. It is very important, however, to be able to accurately classify these complex workloads in order to enable autonomic management and optimization. Zero-Shot Learning (ZSL) is an advanced machine learning approach that enables classification of objects without having to explicitly train on examples of those objects. In this paper we present a new ZSL technique intended to reduce the expense of assembling workload training sets for big data analytic workloads. We demonstrate that multi-user big data workloads can be treated as hybrids of simpler, single-user workload classes, and classified accurately without having to explicitly train on example instances of multi-user workloads. Our technique is able to accurately classify both unseen multi-user workloads, and seen single-user workloads using the same classifier. We demonstrate 83% classification accuracy for the unseen multi-user workloads, and 92% classification accuracy for the seen, single-user workload classes.",https://ieeexplore.ieee.org/document/9378023/,2020 IEEE International Conference on Big Data (Big Data),10-13 Dec. 2020,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICSMC.2011.6083632,[Copyright notice],IEEE,Conferences,The following topics are dealt with: brain-machine interface; machine learning technology; service systems; homeland security systems; virtual reality; agent-based modeling; human centered transportation systems; awareness science and engineering; soft computing; enterprise information systems; social signal processing; infrastructure system; manufacturing systems; pattern recognition; medical mechatronics; minimally invasive surgery; medical robotics; medical technology; intelligent power systems; discrete event systems; Petri nets; biometrics; bioinformatics; computational intelligence; supply chain management; shared control; fault diagnosis; systems engineering; Internet; support vector machines; knowledge acquisition; cloud computing; grey systems; humanoid robots; redundant manipulators; formal methods; granular computing; wireless sensor networks; nonlinear control systems; gesture-based interaction; software engineering; multi-agent systems; cognitive computing; social robotics; natural language processing; conflict resolution; intelligent transportation systems; human-robot interaction; image processing; medical informatics; decision support systems; assistive technology; human-centered design; data mining; and anti-terrorism applications.,https://ieeexplore.ieee.org/document/6083632/,"2011 IEEE International Conference on Systems, Man, and Cybernetics",9-12 Oct. 2011,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/UKSim.2012.123,[Cover art],IEEE,Conferences,The following topics are dealt with: neural networks; evolutionary computation; adaptive dynamic programming; re-enforcement learning; bio-informatics; bio-engineering; computational finance; economics; semantic mining; data mining; virtual reality; data visualization; intelligent systems; soft computing; hybrid computing; e-science; e-systems; robotics; cybernetics; manufacturing; engineering; operations research; discrete event systems; real time systems; image processing; speech processing; signal processing; industry; business; social issues; human factors; marine simulation; power systems; logistics;parallel systems; distributed systems; software architectures;Internet modelling; semantic Web; ontologies; mobile ad hoc wireless networks; Mobicast; sensor placement; target tracking; circuits; sensors and devices.,https://ieeexplore.ieee.org/document/6205540/,2012 UKSim 14th International Conference on Computer Modelling and Simulation,28-30 March 2012,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AIMS.2014.74,[Keynote Speaker-2] Challenges in Handling and Processing Huge Data,IEEE,Conferences,"Summary form only given, as follows. Data-intensive computing is considered as the fourth paradigm in science. The term “data-intensive computing” did not establish in other communities although they are also confronted with enormous amounts of data. Nowadays, Big Data refers to data sets that are too large, too complex, too distributed for analysing them by conventional methods. One strategy for handling Big Data is known as “software to the data” which is applicable when it is more efficient to bring the analysis tools to the data than, vice versa, to apply traditional methods where, for example, all data are collected at some place and analysed there. The data production rate is expected to increase exponentially for the time being. This is particularly true in science where the resolution power of experiments is steadily improving. Sooner or later it has to be taken into account that it is not feasible to store all data anymore. A new era is on the horizon: Huge Data. Huge Data have to be pre-analysed during the data-taking period in order to extract a sufficiently small subset of data that is worth to be analysed in more detail later on. An effective and efficient preselection in real-time or near-realtime is most critical for successfully handling Huge Data. This is made more challenging if during the pre-analysis that has to be done in parallel, intermediate results have to be exchanged. The talk considers selected challenges of Huge Data. Some examples from different scientific communities are presented. The complete presentation was not made available for publication as part of the conference proceedings.",https://ieeexplore.ieee.org/document/7102425/,"2014 2nd International Conference on Artificial Intelligence, Modelling and Simulation",18-20 Nov. 2014,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AIMS.2015.1,[Title page i],IEEE,Conferences,The following topics are dealt with: artificial intelligence; neural networks; fuzzy systems; evolutionary computation; bioinformatics; bioengineering; data mining; semantic mining; games; VR; visualization; intelligent systems applications; hybrid computing; soft computing; intelligent systems control; control intelligence; e-science; e-systems; robotics; cybernetics; manufacturing system; operations research; discrete event systems; real time systems; signal processing; speech processing; image processing; natural language processing; human factors; social issues; shipping; marine simulation; transport; logistics; mobile ad hoc wireless networks; Mobicast; sensor placement; target tracking; software architectures; distributed systems; parallel systems; power simulation; performance engineering; communication systems; and circuits.,https://ieeexplore.ieee.org/document/7604531/,"2015 3rd International Conference on Artificial Intelligence, Modelling and Simulation (AIMS)",2-4 Dec. 2015,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICISCE.2017.93,iWalk: Let Your Smartphone Remember You,IEEE,Conferences,"The advance of science and technology brings not only the convenience to people, but also the issues of the property and privacy security. Therefore, the identity recognition technology is particularly important. Walking is one of the most common daily activity of a person, but everyone walks differently. So we designed a new system based on the data of walking called iWalk to identify the user's identity. The most challenging task is to choose the features of walking that can represent each person's identity. IWalk collects data based on accelerometers built into Commercial Off-The-Shelf (COTS) smartphones and uses the Human Activity Recognition (HAR) method to determine whether the current user is the owner of the phone. The data we used for training and evaluation were collected by 16 volunteers from real activities. The results of our experiments show that iWalk can achieve 97. 80 % true positive (TP) rate in judging the identity of the user's segments. Through the judgment of segments, it recognizes the user identity in the final.",https://ieeexplore.ieee.org/document/8110320/,2017 4th International Conference on Information Science and Control Engineering (ICISCE),21-23 July 2017,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICDM.2008.72,Fast Counting of Triangles in Large Real Networks without Counting: Algorithms and Laws,IEEE,Conferences,"How can we quickly find the number of triangles in a large graph, without actually counting them? Triangles are important for real world social networks, lying at the heart of the clustering coefficient and of the transitivity ratio. However, straight-forward and even approximate counting algorithms can be slow, trying to execute or approximate the equivalent of a 3-way database join. In this paper, we provide two algorithms, the eigentriangle for counting the total number of triangles in a graph, and the eigentrianglelocal algorithm that gives the count of triangles that contain a desired node. Additional contributions include the following: (a) We show that both algorithms achieve excellent accuracy, with up to sime 1000x faster execution time, on several, real graphs and (b) we discover two new power laws (degree-triangle and triangleparticipation laws) with surprising properties.",https://ieeexplore.ieee.org/document/4781156/,2008 Eighth IEEE International Conference on Data Mining,15-19 Dec. 2008,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AICAI.2019.8701334,Finding the Real News in News Streams,IEEE,Conferences,"The current trend of the modern highly mobile society is in the receiving information of any kind by people from one handheld device. This way is simple, convenient, cheap, and practical. The users tend to have only one application on their devices to be universal for any purposes. The social networks have advantages over traditional mass-media instruments such as TV, radio, newspapers, and magazines. This advantage is in the speed of information spreading. Unfortunately, not all of published on-line articles are factual or true. Finding the real news in the news streams is not easy for ordinary users. Even state news agencies do not publish the real news in many cases. In this paper, we discuss different approaches to get the real news from news streams. Among them are the move from the social media to a trusted state agency, the move from one stat agency to several ones to get information on the breaking news from state agencies of different countries. This strategy is the most realistic for the serious users. We introduce the methods to support this strategy and automatically retrieve publications from the trusted sources specified by the user in order to simplify his/her work to obtain the information helping to get and understand the real news better.",https://ieeexplore.ieee.org/document/8701334/,2019 Amity International Conference on Artificial Intelligence (AICAI),4-6 Feb. 2019,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SMART52563.2021.9676205,Fine Grainded Sentiment Analysis on COVID-19 Vaccine,IEEE,Conferences,"The most talked about topic of interest in the medical realm as of today, is the debate on the impact that COVID-19 vaccine has on individuals, and their response in encountering the virus. While there are quite a few vaccine variants that have been developed, there has always been a lingering ambiguity in declaring that an individual can be completely immune to the virus. There have been many studies whilom this cognition of analysing the sentiment perception of vaccines, however the data utilization from various sources and the apropos implementation using the language processing methodologies have lagged a great deal. This paper pivots on the data drawn from social media platforms, and optimizes the sentiments using the Natural Language processing Toolkit (NLTK). The process of word embedding, with TFIDF vectorizer commingled with data unsheathing through fine-grained sentiment analysis and machine learning algorithms such as Linear SVC, SVM and Naïve bayes on the covid19 dataset have aided in stratifying the public tweet sentiments based on their polarity, precision, recall, f1-score value and support. The simulations have been implemented using the lexicon, rubric-based analytical tool VADER (Valence Aware Dictionary and sentiment Reasoner) incorporated in Python specifically for optimized extraction of sentiments from data.",https://ieeexplore.ieee.org/document/9676205/,2021 10th International Conference on System Modeling & Advancement in Research Trends (SMART),10-11 Dec. 2021,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CCNC49033.2022.9700613,First Experimental Results on Real-Time Cleaning Activity Monitoring System,IEEE,Conferences,"The COVID-19 pandemic has presented social challenges to establish the new normal lifestyle in our daily lives. The goal of this paper is to enable easy and low-cost monitoring of cleaning activity to keep a clean environment for preventing infection. Although human activity recognition has been a hot research topic in pervasive computing, existing schemes have not been optimized for monitoring cleaning activities. To address this issue, this paper provides an initial concept and preliminary experimental results of cleaning activity recognition using accelerometer data and RFID tags. In the proposed scheme, machine learning technologies and short range wireless communication are employed for recognizing the time and place of wiping as an example of cleaning activities, because it is an important activity for shared places to avoid infection. This paper reports the evaluation results on the recognition accuracy using the proof-of-concept (PoC) implementation to clarify the required sampling rate and time-window size for further experiments. Also, a real-time feedback system is implemented to provide the monitoring results for users. The proposed scheme contributes for efficient monitoring of cleaning activities for creating the new normal era.",https://ieeexplore.ieee.org/document/9700613/,2022 IEEE 19th Annual Consumer Communications & Networking Conference (CCNC),8-11 Jan. 2022,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SMARTCOMP50058.2020.00080,Flood Detection Framework Fusing The Physical Sensing & Social Sensing,IEEE,Conferences,"We investigate the practical challenge of localized flood detection in real smart city environment using the fusion of physical sensor and social sensing models to depict a reliable and accurate flood monitoring and detection framework. Our proposed framework efficiently utilize the physical and social sensing models to provide the flood-related updates to the city officials. We deployed our flood monitoring system in Ellicott City, Maryland, USA and connect it to the social sensing module to perform the flood-related sensor and social data integration and analysis. Our ground-based sensor network model record and performs the predictive data analytic by forecasting the rise in water level (RMSE=0.2) that demonstrates the severity of upcoming flash floods whereas, our social sensing model helps collect and track the flood-related feeds from Twitter. We employ a pre-trained model and inductive transfer learning based approach to classify the flood-related tweets with 90% accuracy in the use of unseen target flood events. Finally our flood detection framework categorizes the flood relevant localized contextual details into more meaningful classes in order to help the emergency services and local authorities for effective decision making.",https://ieeexplore.ieee.org/document/9239657/,2020 IEEE International Conference on Smart Computing (SMARTCOMP),14-17 Sept. 2020,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/PerComWorkshops51409.2021.9430985,Flood Detection using Semantic Segmentation and Multimodal Data Fusion,IEEE,Conferences,"Real-time flood detection and notifying the citizens about its risk is of utmost importance. This work discusses the real-time deployment of one such notification system called Flood-Bot. FloodBot is a vision-powered flood detection and notification prototype deployed in a flash flood-prone Ellicott City, Maryland. We discuss the real-time deployment of FloodBot and our approach in detecting the flood event using semantic segmentation and multimodal data fusion. We implement the state-of-the-art semantic segmentation model U-Net and its modified version to track landmass with an accuracy of above 80%. We augment the parsed scene data with actual flood level sensor readings and ambient weather data for better scene representation. We validate the deep learning model&#x0027;s outcome using the flood sensor before posting risk message into social media. We then articulate the learning and challenges around our deployment from June &#x2013; November 2020.",https://ieeexplore.ieee.org/document/9430985/,2021 IEEE International Conference on Pervasive Computing and Communications Workshops and other Affiliated Events (PerCom Workshops),22-26 March 2021,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/R10-HTC.2018.8629835,Foody - Smart Restaurant Management and Ordering System,IEEE,Conferences,"Customers play a vital role in the contemporary food industry when determining the quality of the restaurant and its food. Restaurants give considerable attention to customers’ feedback about their service, since the reputation of the business depends on it. Key factors of evaluating customer satisfaction are, being able to deliver the services effectively to lessen the time of consumption, as well as maintaining a high quality of service. In most cases of selecting a prominent restaurant, customers focus on their choice of favorite food in addition to available seating and space options. Long waiting times and serving the wrong order is a common mistake that happens in every restaurant that eventually leads to customer dissatisfaction. Objectives of this online application “Foody” is to address these deficiencies and provide efficient and accurate services to the customer, by providing unique menus to each customer considering their taste. This concept is implemented as a mobile application using latest IT concepts such as Business Intelligence, Data Mining, Predictive Analysis and Artificial Intelligence. This includes graphics and 3D modeling that provide existent physical information related to food such as colors, sizes and further user can view the ingredients of the meal as well as the available tables. In addition, the app shows the real-time map to the restaurant. Current table reservation status is indicated by the color change of the table. Unique food recommendation and it’s order for each customer is generated by analyzing their social media information and the system notifies the customer the wait time by calculating it. Preparation of food and allocation is done subjectively. The expected outcome of the research is to develop a fully automated restaurant management system with the mentioned features as well as to avoid confusions between orders, provide better view of food and allow the customer to choose the menu according to their taste in a minimum time.",https://ieeexplore.ieee.org/document/8629835/,2018 IEEE Region 10 Humanitarian Technology Conference (R10-HTC),6-8 Dec. 2018,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/UIC-ATC.2017.8397484,Forecasting car rental demand based temporal and spatial travel patterns,IEEE,Conferences,"Recent years, shared mobility services have gained momentum across the world. Meanwhile, rental car industry has seen great developments in China and has reached a scale of economy. Knowing the rental behavior pattern and forecasting the demand become more important for rental businesses. To this end, in this paper, we aim to analyze the rental mobility pattern by examining multiple factors in a holistic manner. A special goal is to predict the demand of a given region. Specifically, we first analyze regular mobility based on real trips of rental cars. Then, we extract key features from multiple types of rental-related data, such as rental behavior profiles and geo-social information of regions. Next, based on these features, we develop a multi-task learning based regression approach for predicting rental cars' demand. This approach can effectively learn not only fundamental features but also relationships between regions by considering multiple factors. Finally, we conduct extensive experiments on real-world rental trip data collected in Beijing. The experimental results validate the effectiveness of the proposed approach for forecasting rental demand in the real world.",https://ieeexplore.ieee.org/document/8397484/,"2017 IEEE SmartWorld, Ubiquitous Intelligence & Computing, Advanced & Trusted Computed, Scalable Computing & Communications, Cloud & Big Data Computing, Internet of People and Smart City Innovation (SmartWorld/SCALCOM/UIC/ATC/CBDCom/IOP/SCI)",4-8 Aug. 2017,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICEngTechnol.2017.8308166,Forecasting of Twitter hashtag temporal dynamics using locally weighted projection regression,IEEE,Conferences,"Popularity of social networks opens great opportunities for market such as advertisement. Using hashtags increasingly used in twits helps us to realize popular topics on the internet. Since most of new hashtags become popular and then fade away quickly, there is a limited time to predict the trend. Therefore, this paper proposes a fast incremental method to forecast the rate of the used hashtags in hour like time series. Two main parts for forecasting system are applied Preprocessing and Supervised Learning. Normalization is one of most popular preprocessing of dataset also proposed to have larger dataset. Moreover, the efficiency of the system under changing number of input (number of past hours from hashtag history) and output (number of next hours which is going to be predicted) are evaluated. Locally Weighted Projection Regression as one of the most powerful machine learning methods with no metaparameter are applied in this paper as real-time learning method. The performance of the system is verified by implementation of “Volume Time Series of Memetracker Phrases and Twitter Hashtags”. The results show that the errors of forecasting system are good enough to understand the trend of the hashtag.",https://ieeexplore.ieee.org/document/8308166/,2017 International Conference on Engineering and Technology (ICET),21-23 Aug. 2017,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICONSTEM.2017.8261274,Forecasting stock price using soft computing techniques,IEEE,Conferences,"Forecasting stock price is a important challenging task in the real world because more and more money is involved and they are affected by many social, economic, political and psychological factors. Numerous machine learning procedures have been used to fore see developments in stock cost. Machine learning classifiers include expanding the past encounters into the future actives. The proposed framework presents another hereditary calculation for forecast of monetary execution with information sets from a known source. The objective is to deliver a GA-based procedure for expectation of securities exchange execution alongside an acquainted principle classifier from an information set. We developed a genetic fuzzy based model with the ability of rule based extraction to predict next day stock price. We evaluate capacity of the proposed approach by applying it to the banking sector stocks and compare the outcome with the previous models.",https://ieeexplore.ieee.org/document/8261274/,2017 Third International Conference on Science Technology Engineering & Management (ICONSTEM),23-24 March 2017,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SII.2011.6147520,Forming an artificial pheromone potential field using mobile robot and RFID tags,IEEE,Conferences,"In the biological world, social insects such as ants and bees use a volatile substance called pheromone for their foraging or homing tasks. This study deals with how to utilize the concept of the chemical pheromone as an artificial potential field for robotic purposes. This paper first models a pheromone-based potential field, which is constructed through the interaction between mobile robot and RFID tags. The emphasis in the modeling of the system is on the possibility of the practical implementable ideas. The stability analysis of the pheromone potential field is carried out with the aim of implementing the model on a real robotic system. The comprehensive analysis on stability provides the criteria for how the parameters are to be set for the proper potential field, and has also led to a new filter design scheme called pheromone filter. The designed filter satisfies both the stability and accuracy of the field, and facilitates a more straightforward and practical implementation for building and shaping the potential field. The effectiveness of the proposed algorithm is validated through both computer simulation and real experiment.",https://ieeexplore.ieee.org/document/6147520/,2011 IEEE/SICE International Symposium on System Integration (SII),20-22 Dec. 2011,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/PacificVis.2018.00029,FraudVis: Understanding Unsupervised Fraud Detection Algorithms,IEEE,Conferences,"Discovering fraud user behaviors is vital to keeping online websites healthy. Fraudsters usually exhibit grouping behaviors, and researchers have effectively leveraged this behavior to design unsupervised algorithms to detect fraud user groups. In this work, we propose a visualization system, FraudVis, to visually analyze the unsupervised fraud detection algorithms from temporal, intra-group correlation, inter-group correlation, feature selection, and the individual user perspectives. FraudVis helps domain experts better understand the algorithm output and the detected fraud behaviors. Meanwhile, FraudVis also helps algorithm experts to fine-tune the algorithm design through the visual comparison. By using the visualization system, we solve two real-world cases of fraud detection, one for a social video website and another for an e-commerce website. The results on both cases demonstrate the effectiveness of FraudVis in understanding unsupervised fraud detection algorithms.",https://ieeexplore.ieee.org/document/8365989/,2018 IEEE Pacific Visualization Symposium (PacificVis),10-13 April 2018,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CCECE.2019.8861892,From natural language to graph queries,IEEE,Conferences,"Automatic code generation can drastically improve software (SW) engineering and SW development projects. In the last decade we have been conducting research which has been advancing the field of code generators for small and mid-size Web-based DBMS systems [4], [5], [7]. We developed a number of tool prototypes for automatic source code debugging by the source-to-source code transformation for real C and C++ applications [8]. Additionally we investigated Natural Language Processing (NLP) for software code generation and application of it to Graph databases. Graph databases are becoming more and more popular for their applications in Artificial Intelligence (AI) systems, social analytics and many other fields. Query languages like Cypher allow users to search them without direct programming. But even queries of modest complexity like “relatives in a family & friends graph” require some skill to write. In this paper we describe the use of natural language as a more intuitive interface for untrained users and demonstrate 3 use-cases, where translation of typical English phrases to OpenCypher and/or specialized graph engines like Huawei EYWA.",https://ieeexplore.ieee.org/document/8861892/,2019 IEEE Canadian Conference of Electrical and Computer Engineering (CCECE),5-8 May 2019,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ROMAN.2017.8172498,Functional imitation task in the context of robot-assisted Autism Spectrum Disorder diagnostics: Preliminary investigations,IEEE,Conferences,"This paper presents a functional imitation task aimed at facilitating Autism Spectrum Disorder (ASD) diagnostics in children. Imitation plays a key role in the development of social skills at a young age, and studies have shown that the ability to imitate is impaired in children with ASD. Therefore, we expect imitation-based tasks to have diagnostic value. In this paper, we introduce two novel elements of human-robot interaction in the context of autism diagnostics. Instead of pure motoric imitation, we propose imitation tasks involving real objects in the environment. The introduction of physical objects strongly emphasizes joint attention skills, another area that is typically impaired in children with ASD. Furthermore, we present simple object detection, manipulation, tracking and gesture recognition algorithms, suitable for real-time, onboard execution on the small-scale humanoid robot NAO. The proposed system paves the way for fully autonomous execution of diagnostic tasks, which would simplify the deployment of robotic assistants in clinical settings. The source code for all described functionalities has been made publicly available as open-source software. We present a preliminary evaluation of the proposed system with a control group of typically developing preschool children and a group of seven children diagnosed with ASD.",https://ieeexplore.ieee.org/document/8172498/,2017 26th IEEE International Symposium on Robot and Human Interactive Communication (RO-MAN),28 Aug.-1 Sept. 2017,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICISS49785.2020.9315885,Generalized method to validate social distancing using median angle proximity methodology,IEEE,Conferences,"Social distancing in industries as well as all public places due to the breakdown of pandemic of COVID-19 is currently of global interest. An advanced global solution for social distancing monitoring among the group of people in real time using computer vision and deep learning is proposed. A state of art method, Median angle proximity method is proposed on the coordinates obtained from the YOLOv3 (You only look once) algorithm to determine the predefined threshold resulting in the social distancing violating bounding boxes. The violating bounding boxes are then saved as single images using deep sort method. All the parts of the work are done using opensource programming language. The work has met all the requirements planned and is considered to be ready for deployment in real time with an average accuracy of 85%",https://ieeexplore.ieee.org/document/9315885/,2020 3rd International Conference on Intelligent Sustainable Systems (ICISS),3-5 Dec. 2020,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/COMPSAC.2019.10256,Generating Real Time Cyber Situational Awareness Information Through Social Media Data Mining,IEEE,Conferences,"With the rise of the internet many new data sources have emerged that can be used to help us gain insights into the cyber threat landscape and can allow us to better prepare for cyber attacks before they happen. With this in mind, we present an end to end real time cyber situational awareness system which aims to efficiently retrieve security relevant information from the social networking site Twitter.com. This system classifies and aggregates the data retrieved and provides real time cyber situational awareness information based on sentiment analysis and data analytics techniques. This research will assist security analysts to evaluate the level of cyber risk in their organization and proactively take actions to plan and prepare for potential attacks before they happen as well as contribute to the field through a cybersecurity tweet dataset.",https://ieeexplore.ieee.org/document/8753997/,2019 IEEE 43rd Annual Computer Software and Applications Conference (COMPSAC),15-19 Jul 2019,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CGames.2014.6934140,Generating dynamic narratives with real time interactions utilizing mobile technology,IEEE,Conferences,"The popularity of smartphones and other mobile technologies provides unique and exciting opportunities for game developers to create interesting game experiences and simultaneously study how players react and interact to these new environments. By studying and enhancing these new interactive environments and utilizing these technologies, we can break free of traditional game experiences and create newer and more exciting games with the flexibility of mobile devices. This study explores how we expand our idea of what a traditional game should be and poses a newer, fresher look on mobile game play and social connectivity. We have added to the notion of playing a game without actively engaging in it from the foundation of previous research. We have done this in order to build players' unique stories that will allow players to make more meaningful choices throughout their gameplay experience. By offering the player real time choices and active engagement in a mobile game that currently offers none, we expect to increase the levels of enjoyment and usefulness, from both educational and recreational viewpoints.",https://ieeexplore.ieee.org/document/6934140/,"2014 Computer Games: AI, Animation, Mobile, Multimedia, Educational and Serious Games (CGAMES)",28-30 July 2014,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICDE48307.2020.00018,Group Recommendation with Latent Voting Mechanism,IEEE,Conferences,"Group Recommendation (GR) is the task of suggesting relevant items/events for a group of users in online systems, whose major challenge is to aggregate the preferences of group members to infer the decision of a group. Prior group recommendation methods applied predefined static strategies for preference aggregation. However, these static strategies are insufficient to model the complicated decision making process of a group, especially for occasional groups which are formed adhoc. Compared to conventional individual recommendation task, GR is rather dynamic and each group member may contribute differently to the final group decision. Recent works argue that group members should have non-uniform weights in forming the decision of a group, and try to utilize a standard attention mechanism to aggregate the preferences of group members, but they do not model the interaction behavior among group members, and the decision making process is largely unexplored.In this work, we study GR in a more general scenario, that is Occasional Group Recommendation (OGR), and focus on solving the preference aggregation problem and the data sparsity issue of group-item interactions. Instead of exploring new heuristic or vanilla attention-based mechanism, we propose a new social self-attention based aggregation strategy by directly modeling the interactions among group members, namely Group Self-Attention (GroupSA). In GroupSA, we treat the group decision making process as multiple voting processes, and develop a stacked social self-attention network to simulate how a group consensus is reached. To overcome the data sparsity issue, we resort to the relatively abundant user-item and user-user interaction data, and enhance the representation of users by two types of aggregation methods. In the training process, we further propose a joint training method to learn the user/item embeddings in the group-item recommendation task and the user-item recommendation task simultaneously. Finally, we conduct extensive experiments on two real-world datasets. The experimental results demonstrate the superiority of our proposed GroupSA method compared to several state-of-the-art methods in terms of HR and NDCG.",https://ieeexplore.ieee.org/document/9101842/,2020 IEEE 36th International Conference on Data Engineering (ICDE),20-24 April 2020,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SNPD.2013.55,Grouping Methods for Generating Friendship Based on Network Properties,IEEE,Conferences,"This paper investigates the effect of group work with the assumption of three motivators to make friends. Obeying the assumption we proposed twelve variation of methods for grouping students. The effects are evaluated by some measures from social network analysis and by the changes of real friendship networks, which are observed by a friendship prediction method. The proposed methods brought new friendship among students to classes and made rearrange of community structure.",https://ieeexplore.ieee.org/document/6598501/,"2013 14th ACIS International Conference on Software Engineering, Artificial Intelligence, Networking and Parallel/Distributed Computing",1-3 July 2013,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/VECIMS.2007.4373924,HI3 Project: Design and Implementation of the Lower Level Layers,IEEE,Conferences,"We are interested in the development of human-centered and ubiquitous technologies in social environments. In this line, and in the framework of a global software architecture (HI3) for this type of applications, the paper is devoted to the presentation of the work carried out for the design and implementation of the layer that is more closely linked to the hardware. It is in charge of communicating with the physical layer and it is responsible for the abstraction of the field elements. Special attention has been paid to the coherence with the philosophy, design premises and functionalities of the whole system. We have also determined a hardware configuration that, integrating standards where possible, is better adapted to the requirements of the architecture. The elements introduced here were validated on a real implementation of the system.",https://ieeexplore.ieee.org/document/4373924/,"2007 IEEE Symposium on Virtual Environments, Human-Computer Interfaces and Measurement Systems",25-27 June 2007,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/I4CS.2015.7294480,Happy hour - improving mood with an emotionally aware application,IEEE,Conferences,"Mobile sensing in Cyber-Physical Systems has been evolving proportionally with smartphones. In fact, we are witnessing a tremendous increase in systems that sense various facets of human beings and their surrounding environments. In particular, the detection of human emotions can lead to emotionally-aware applications that use this information to benefit people's daily lives. This work presents the implementation of a Human-inthe- loop emotionally-aware Cyber-Physical System that attempts to positively impact its user's mood through moderate walking exercise. Data from smartphone sensors, a smartshirt's electrocardiogram and weather information from a web API are processed through a machine learning algorithm to infer emotional states. When negative emotions are detected, the application timely suggests walking exercises, while providing real-time information regarding nearby points of interest. This information includes events, background music, attendance, agitation and general mood. In addition, the system also dynamically adapts privacy and networking configurations based on emotions. The sharing of the user's location on social networks and the device's networking interfaces are configured according to user-defined rules in order to reduce frustration and provide a better Quality of Experience.",https://ieeexplore.ieee.org/document/7294480/,2015 15th International Conference on Innovations for Community Services (I4CS),8-10 July 2015,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/BigData.2018.8621926,Harnessing the Nature of Spam in Scalable Online Social Spam Detection,IEEE,Conferences,"Disinformation in social networks has been a worldwide problem. Social users are surrounded by a huge volume of malicious links, biased comments, fake reviews, or fraudulent advertisements, etc. Traditional spam detection approaches propose a variety of statistical feature-based models to filter out social spam from a historical dataset. However, they omit the real word situation of social data, that is, social spam is fast changing with new topics or events. Therefore, traditional approaches cannot effectively achieve online detection of the ""drifting"" social spam with a fixed statistic feature set. In this paper, we present Sifter, a system which can detect online social spam in a scalable manner without the labor-intensive feature engineering. The Sifter system is two-fold: (1) a decentralized DHT-based overlay deployment for harnessing the group characteristics of social spam activities within a specific topic/event; (2) a social spam processing with the support of Recurrent Neural Network (RNN) to get rid of the traditional manual feature engineering. Results show that Sifter achieves graceful spam detection performances with the minimal size of data and good balance in group management.",https://ieeexplore.ieee.org/document/8621926/,2018 IEEE International Conference on Big Data (Big Data),10-13 Dec. 2018,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IJCNN48605.2020.9206610,Heterogeneous Information Network Embedding with Convolutional Graph Attention Networks,IEEE,Conferences,"Heterogeneous Information Networks (HINs) are prevalent in our daily life, such as social networks and bibliography networks, which contain multiple types of nodes and links. Heterogeneous information network embedding is an effective HIN analysis method, it aims at projecting network elements into a lower-dimensional vector space for further machine learning related evaluations, such as node classification, node clustering, and so on. However, existing HIN embedding methods mainly focus on extracting the semantic-related information or close neighboring relations, while the high-level proximity of the network is also important but not preserved. To address the problem, in this paper we propose CGAT, a semi-supervised heterogeneous information network embedding method. We optimize the graph attention network by adding additional convolution layers, thereby we can extract multiple types of semantics and preserve high-level information in HIN embedding at the same time. Also, we utilize label information in HINs for semi-supervised training to better obtain the model parameters and HIN embeddings. Experimental results on real-world datasets demonstrate the effectiveness and efficiency of the proposed model.",https://ieeexplore.ieee.org/document/9206610/,2020 International Joint Conference on Neural Networks (IJCNN),19-24 July 2020,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CRV.2010.55,Human Upper Body Pose Recognition Using Adaboost Template for Natural Human Robot Interaction,IEEE,Conferences,"In this paper, we propose a novel Adaboost template to recognize human upper body poses from disparity images for natural human robot interaction (HRI). First, the upper body poses of standing persons are classified into seven categories of views. For each category, a mean template, variance template, and percentage template are generated. Then, the template region is divided into positive and negative regions, corresponding to the region of bodies and surrounding open space. A weak classifier is designed for each pixel in the template. A new EM-like Adaboost learning algorithm is designed to learn the Adaboost template. Different from existing Adaboost classifiers, we show that the Adaboost template can be used not only for recognition but also for adaptive top-down segmentation. By using Adaboost template, only a few positive samples for each category are required for learning. Comparison with conventional template matching techniques has been made. Experimental results show that significant improvements can be achieved in both cases. The method has been deployed in a social robot to estimate human attentions to the robot in real-time human robot interaction.",https://ieeexplore.ieee.org/document/5479162/,2010 Canadian Conference on Computer and Robot Vision,31 May-2 June 2010,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA.2013.6630610,Human-friendly robot navigation in dynamic environments,IEEE,Conferences,"The vision-based mechanisms that pedestrians in social groups use to navigate in dynamic environments, avoiding obstacles and each others, have been subject to a large amount of research in social anthropology and biological sciences. We build on recent results in these fields to develop a novel fully-distributed algorithm for robot local navigation, which implements the same heuristics for mutual avoidance adopted by humans. The resulting trajectories are human-friendly, because they can intuitively be predicted and interpreted by humans, making the algorithm suitable for the use on robots sharing navigation spaces with humans. The algorithm is computationally light and simple to implement. We study its efficiency and safety in presence of sensing uncertainty, and demonstrate its implementation on real robots. Through extensive quantitative simulations we explore various parameters of the system and demonstrate its good properties in scenarios of different complexity. When the algorithm is implemented on robot swarms, we could observe emergent collective behaviors similar to those observed in human crowds.",https://ieeexplore.ieee.org/document/6630610/,2013 IEEE International Conference on Robotics and Automation,6-10 May 2013,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICITR51448.2020.9310890,Hybrid Approach and Architecture to Detect Fake News on Twitter in Real-Time using Neural Networks,IEEE,Conferences,"Fake news has been a key issue since the dawn of social media. Currently, we are at a stage where it is merely impossible to differentiate between real and fake news. This directly and indirectly affects people's decision patterns and makes us question the credibility of the news shared via social media platforms. Twitter is one of the leading social networks in the world by active users. There has been an exponential spread of fake news on Twitter in the recent past. In this paper, we will discuss the implementation of a browser extension which will identify fake news on Twitter using deep learning models with a focus on real-world applicability, architectural stability and scalability of such a solution. Experimental results show that the proposed browser extension has an accuracy of 86% accuracy in fake news detection. To the best of our knowledge, our work is the first of its kind to detect fake news on Twitter real-time using a hybrid approach and evaluate using real users.",https://ieeexplore.ieee.org/document/9310890/,2020 5th International Conference on Information Technology Research (ICITR),2-4 Dec. 2020,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/APSEC.2002.1182989,IBistro: a learning environment for knowledge construction in distributed software engineering courses,IEEE,Conferences,"We have taught several distributed software engineering project courses with students and real clients. During these projects, students in Pittsburgh and Munich, Germany collaborated in the development of a single system. Our experiences showed that software development is communication intensive and requires the collaboration of many stakeholders. Communication is challenging in distributed contexts: participants do not all know each other and work at different times and locations; the number of participants and their organization change during the project; and participants belong to different communities. Hence, to deal with the global marketplace, it is critical to provide students with distributed collaboration skills. To improve the teaching of collaboration in software engineering, we propose iBistro, an augmented, distributed, and ubiquitous communication space. iBistro aims to overcome problems resulting from miscommunications and information loss in informal or casual meetings. iBistro enables distributed groups to collaborate and cooperate in software projects and therefore provides an environment for learning in diverse aspects such as project management, programming skills, and social skills. With the addition of techniques from artificial intelligence, such as student modeling, and intelligent support mechanisms, such as computer supported group formation, distributed tutoring becomes feasible.",https://ieeexplore.ieee.org/document/1182989/,"Ninth Asia-Pacific Software Engineering Conference, 2002.",4-6 Dec. 2002,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICEARS53579.2022.9751864,Identifying Fake News using Machine Learning,IEEE,Conferences,"Fake data is purposely or accidentally transmitted throughout the internet. It has long been a social issue, and in the digital age, the average person now has easy access to all of the information available online. This is affecting a growing population of people who are technologically blind. One of the most serious problems in the modern day is fake news, which has the capacity to affect people's minds and influence their judgments. On web browsers, there are a few plugins that provide real-time information about the veracity of news. The algorithms used to create these plugins have a significant impact on them. The goal is to create a project that will propose which of the three implemented algorithms is the best for further development by the developer. Machine learning classification methods such as SVM, naive bayes, logistic regression, decision tree, and random forest are taught to detect if news is fake or real, and then compared based on metrics.",https://ieeexplore.ieee.org/document/9751864/,2022 International Conference on Electronics and Renewable Systems (ICEARS),16-18 March 2022,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCMC51019.2021.9418419,Image classification for user feedback using Deep Learning Techniques,IEEE,Conferences,"In this digital era, automatic human facial expression recognition is considered as an important component in computer vision. Also, it is challenging for machine learning algorithms, where humans can significantly show their expressions. Henceforth, in machine learning methods, deep learning is considered as a novel technology that can classify the images of human faces into different facial expression recognition categories using convolutional neural networks (CNN). In this system, the facial expression recognition is implemented by using CNN network based model with LeNet architecture to improve the prediction of expression results. Here, the proposed research work has utilized a facial expression dataset, which is loaded from Kaggle web resources and this dataset contains seven facial expression tags such as happy, anger, neutral, fear, sad, disgust, and surprise. In this system, along with emotion classifications, gender classification is also merged. Because automatic gender recognition has relevant to the addition of its usages in software applications whereas in social media and social networking websites. With this system, gender and facial expression recognition are explored through face detection using Convolution Neural Network (CNN).The whole motivation behind the work is to improve the way human movement is detected for different legal purposes. The usage of computer vision on the field of customer service, user security, user feedback and many other things. The gender and expression recognition can be used to deal with many real world problems.",https://ieeexplore.ieee.org/document/9418419/,2021 5th International Conference on Computing Methodologies and Communication (ICCMC),8-10 April 2021,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCWAMTIP53232.2021.9674087,Immersive 4D Intelligent Interactive Platform Based on Deep Learning,IEEE,Conferences,"With the advent of the Internet era, the cost of realizing virtual characters has been greatly reduced. The high cost and low efficiency exhibited by traditional virtual technology can&#x0027;t meet social needs. People are seeking fast, convenient and accurate virtual character reproduction technology. Through researching the characteristics of the characters appearance, language habits, voice tone and so on, the research direction of this paper is to simulate and reshape voice and images, construct a cloud platform-based on user-side and client-side, and integrate deep learning, natural language processing, digital twins and other technologies to an immersive 4D intelligent interactive platform. The platform under in the form of application software provides integrated services of intelligent voice interaction and virtual character interaction. In the industrial diagnosis mode, the transition from traditional video retention and voice retention to a new intelligent voice recognition and simulation mode is realized.",https://ieeexplore.ieee.org/document/9674087/,2021 18th International Computer Conference on Wavelet Active Media Technology and Information Processing (ICCWAMTIP),17-19 Dec. 2021,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AIMSEC.2011.6011030,"Impact from Effective distribution coefficient Change on the economic system constituted by two factors, three departments and four categories of residents — CGE model simulation",IEEE,Conferences,"A CGE model is built for the economic system constructed by two factors, three departments and four categories of residents and the impact effect of Effective distribution coefficient Change on this economie system is simulated by virtue of computer simulation. The fixed points of the economic system and the impact on distortion of general equilibrium of the economic system are computed by dint of MATLAB software and self-made program (genetic algorithm is adopted). Results: a Effective distribution coefficient Change may change the equilibrium price system (fixed points) of the economic system, the transaction costs of the economic system (in particular costs of property rights), residents' maximum utility and the department's equilibrium quantity through influence on employment resource quantity, demand structure, production structure, residents' real income and so on of the economic system. Conclusion: By the Effective distribution coefficient Change (Table 2), the total social utility increases, the Production of the three departments and the Effective distribution coefficient are positively correlated",https://ieeexplore.ieee.org/document/6011030/,"2011 2nd International Conference on Artificial Intelligence, Management Science and Electronic Commerce (AIMSEC)",8-10 Aug. 2011,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SMC.2017.8122654,Implementation of human-robot VQA interaction system with dynamic memory networks,IEEE,Conferences,"One of the major functions of intelligent robots such as social or home service robots is to interact with users in natural language. Moving on from simple conversation or retrieval of data stored in computer memory, we present a new Human-Robot Interaction (HRI) system which can understand and reason over environment around the user and provide information about it in a natural language. For its intelligent interaction, we integrated Dynamic Memory Networks (DMN), a deep learning network for Visual Question Answering (VQA). For its hardware, we built a robotic head platform with a tablet PC and a 3 DOF neck. Through an experiment where the user and the robot had question answering interaction in our customized environment and in real time, the feasibility our proposed system was validated, and the effectiveness of deep learning application in real world as well as a new insight on human robot interaction was demonstrated.",https://ieeexplore.ieee.org/document/8122654/,"2017 IEEE International Conference on Systems, Man, and Cybernetics (SMC)",5-8 Oct. 2017,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ATC.2011.6027460,Implementing un-identified communication in Ubiquitous phone System,IEEE,Conferences,"Context awareness plays an important role in Ubiquitous applications based on the context information to provide services adapting to users' needs. The advantages of some context-aware smart phone systems, including iCAM[1], contextPhone[2], Live Contact Project [3], Enhanced Telephony[4] and Ubiquitous phone System Ubiphone [5] show themselves to be typical examples of Ubiquitous service ones. Using a context-aware smart phone, we can contact individuals in many ways, including by home, office, or cell phone, short message service (SMS), instance message(IM), and email. Therefore, a human-centered smart phone could discover how best to reach a contact at any given moment. However, in these systems, the users can contact to people in the contact list only, and the context information is managed and referred on only one ontology tree in one domain which contraries to the reality that in Ubiquitous environment. In this environment, the context information such as concepts and relationships are also managed and referred on heterogeneous ontology in several domains, which drive new challenges in the discovery of knowledge sources relevant to a users' request. New efficient technique and approach for developing and extending Ubiquitous phone intelligence is presented in this paper, which combines agent-based technologies and Artificial Neural Networks (ANNs) models to match ontology, helping users search and contact with anyone outside the contact list satisfying the users' requests even there is no social relationship between them.",https://ieeexplore.ieee.org/document/6027460/,The 2011 International Conference on Advanced Technologies for Communications (ATC 2011),2-4 Aug. 2011,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCKE.2011.6413358,Improving the accuracy and efficiency of tag recommendation system by applying hybrid methods,IEEE,Conferences,"Recently applications of social tagging systems have increased. These systems allow users to organize, manage and search the required resource freely, thus by combination and integration of recommendation systems in social software, assisting users to appropriately assign tag to resources and try to improve annotation among users. The challenges of recommendation systems are large-scale data, inconsistence data, usage of time-consuming machine learning algorithms, long and unreasonable time of recommendation and not being scalable to the demands of real world applications. Recently more efforts have been conducted to solve these problems. In this paper we proposed a tag recommendation system that is able to work with large-scale data and being applied in real world. The proposed system's evaluation performed on a dataset collected from Delicious.com. The results demonstrated the efficiency and accuracy of proposed system.",https://ieeexplore.ieee.org/document/6413358/,2011 1st International eConference on Computer and Knowledge Engineering (ICCKE),13-14 Oct. 2011,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ACII.2019.8925479,Imputing Missing Social Media Data Stream in Multisensor Studies of Human Behavior,IEEE,Conferences,"The ubiquitous use of social media enables researchers to obtain self-recorded longitudinal data of individuals in real-time. Because this data can be collected in an inexpensive and unobtrusive way at scale, social media has been adopted as a “passive sensor” to study human behavior. However, such research is impacted by the lack of homogeneity in the use of social media, and the engineering challenges in obtaining such data. This paper proposes a statistical framework to leverage the potential of social media in sensing studies of human behavior, while navigating the challenges associated with its sparsity. Our framework is situated in a large-scale in-situ study concerning the passive assessment of psychological constructs of 757 information workers wherein of four sensing streams was deployed - bluetooth beacons, wearable, smartphone, and social media. Our framework includes principled feature transformation and machine learning models that predict latent social media features from the other passive sensors. We demonstrate the efficacy of this imputation framework via a high correlation of 0.78 between actual and imputed social media features. With the imputed features we test and validate predictions on psychological constructs like personality traits and affect. We find that adding the social media data streams, in their imputed form, improves the prediction of these measures. We discuss how our framework can be valuable in multimodal sensing studies that aim to gather comprehensive signals about an individual's state or situation.",https://ieeexplore.ieee.org/document/8925479/,2019 8th International Conference on Affective Computing and Intelligent Interaction (ACII),3-6 Sept. 2019,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICBDA51983.2021.9402992,"Incremental Real-time Learning Framework for Sentiment Classification: Indian General Election 2019, A Case Study",IEEE,Conferences,"Indian General Election 2019 was one of the major global political events. The prominence of social media in contemporary life, and the ubiquity of political messaging on it has necessitated a systematic study of sentiments and make inferences about future moods and trends. In absence of standard tools or software we propose a machine learning-based generic system framework and REST (Representation State Transfer) plugin component, that extracts and filters authentic tweets from Twitter, captures the prevalent mood, and predicts the sentiment of any live incoming tweet in a resource-constrained setup. The system can predict user sentiments by discovering completely new features from the web, along with the process of continuous incremental learning and improvement of model accuracy.",https://ieeexplore.ieee.org/document/9402992/,2021 IEEE 6th International Conference on Big Data Analytics (ICBDA),5-8 March 2021,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICEE50131.2020.9260698,"Indoor and Outdoor Face Recognition for Social Robot, Sanbot Robot as Case Study",IEEE,Conferences,"The interaction between human and robots is of paramount importance in comforting robot and human in the context of social demand. For the purpose of human-robot interaction, the robot should have the ability to perform a variety of actions including face recognition, path planning, etc. In this paper, face recognition has been implemented on the Sanbot robot. Since the Sanbot robot is intended to work in real environment, therefore indoor and outdoor environment is taken into account in proposing the corresponding face recognition algorithm. For each case a robust pre-processing algorithm should be designed and which can circumvent a challenging problem in face recognition, namely, different lighting conditions (light intensity, angle of radiation, etc.). In case of indoor environment, faces in an captured image by the robot HD camera are found using a Haar-cascade algorithm. Afterwards, a histogram equalization is applied to face images in order to standardize them. Then commonly practiced Deep convolutional neural network structures such as Inception and ResNet are used to design a model and trained end-to-end on a customized dataset with strong augmentation. Finally, by using a voting method, proper prediction is carried out on each face. In what concerns the outdoor environment, which has more challenges, upon applying histogram Equalization on the captured image, faces are found using a MultiTask Cascaded Convolutional Neural Network. Then face images are aligned as head orientation are corrected. Finally, cropped face image is fed to Siamese Network in order to extract face features and verifying individuals. From several practical results it has been inferred that the accuracy of the indoor method is nearly 93% without voting and with voting 97%, and the outdoor method is about 95%.",https://ieeexplore.ieee.org/document/9260698/,2020 28th Iranian Conference on Electrical Engineering (ICEE),4-6 Aug. 2020,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ROBOT.1991.131908,Instinctive behaviors and personalities in societies of cellular robots,IEEE,Conferences,"A description is presented of the social organization of societies of cellular mobile units featuring instinctive behavior. Each robotic unit has its own personality and lives independently from the others. Useful tasks are carried out through collaboration rather than by individual effort. The behavior of each unit derives from a subsumption-like control structure, which emphasizes the roles of innate personality, external stimuli, and communication. A number of different robotic personalities are described and techniques of implementing them in real robot units are outlined. The implementation of instinctive behavior is described for the case of a robotic vehicle system (ROBBIE).<>",https://ieeexplore.ieee.org/document/131908/,Proceedings. 1991 IEEE International Conference on Robotics and Automation,9-11 April 1991,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.23919/FRUCT53335.2021.9599974,Intelligent Identification of Fake Accounts on Social Media,IEEE,Conferences,The paper presents an original research of fake accounts on social media using an artificial neural network for their identification. Specifically designed and implemented application was used to identify specific features of fake accounts and study the principles and reasons of their generation. Considering the possible use cases and scenarios of its practical use including everyday social media surfing there was examined a possibility to implement a mobile application based on Java in the Android Studio programming environment. Based on the study of 500 real and 500 fake VKontakte accounts there was determined a number of conclusions on the original features of fake accounts. The provided research allowed extending the list of criteria for identifying fake accounts by an original set of patterns. The developed conclusions allow formulating the statements on what criteria can be used for further identification of fake accounts in practical applications.,https://ieeexplore.ieee.org/document/9599974/,2021 30th Conference of Open Innovations Association FRUCT,27-29 Oct. 2021,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CIT/IUCC/DASC/PICOM.2015.151,Intelligent Mobile Framework Based on Swarm Computation,IEEE,Conferences,"Mobile Terminal Crowd Sourcing provides social network information and services for users through data processing and data mining. Intelligent engine provides recommendation based on historical information and real-time environment, which are primarily used in intelligent life, mobile social networking, and other fields. Firstly, the concept of intelligent engine is introduced, the research strategy and the overall structure of the mobile terminal crowd sourcing.",https://ieeexplore.ieee.org/document/7363192/,"2015 IEEE International Conference on Computer and Information Technology; Ubiquitous Computing and Communications; Dependable, Autonomic and Secure Computing; Pervasive Intelligence and Computing",26-28 Oct. 2015,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SEA-STEM53614.2021.9668057,Intelligent Traffic Light System Using Image Processing,IEEE,Conferences,"Nowadays, there are more cars used on the road. Traffic congestion problems can cause the economy and the environment both directly and indirectly problems, also part of the problem of air pollution. The traffic light management system in the current situation has used a fixed waiting time, which inability to be flexible according to the traffic at different times such as in rush hour and other. It is not efficient enough to manage traffic with fixed waiting time. The organizers came up with the idea of developing an intelligent traffic light system with flexibility according to the number of cars in real-time by reducing waiting time. The paper was designed and developed by implementing the intelligent traffic light system using image processing technology to process the appropriate waiting time from each image frame. Lazarus and OpenGL were used to program based on Pascal language. The software has been developed for receiving traffic video at the intersection to process car image segmentation of each frame and to calculate the distance of the length of the car in each route in addition. It is also possible to calculate the appropriate time for green-light and red-light duration and corresponding to the length of the waiting vehicles in each route at the intersection. This investigated software can be used to reduce the waiting time at the traffic light intersection by 45.35%. In addition, the intelligent traffic light system is also a social development towards a smart city. The project has created the learning environment and computational thinking for society through the process of STEM Education with using IoT and Artificial Intelligence.",https://ieeexplore.ieee.org/document/9668057/,2021 2nd SEA-STEM International Conference (SEA-STEM),24-25 Nov. 2021,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ITSC.2017.8317730,Intelligent traffic light control using distributed multi-agent Q learning,IEEE,Conferences,"The combination of Artificial Intelligence (AI) and Internet-of-Things (IoT), which is denoted as AI powered Internet-of-Things (AIoT), is capable of processing huge amount of data generated from large number of devices and handling complex problems in social infrastructures. As AI and IoT technologies are becoming mature, in this paper, we propose to apply AIoT technologies for traffic light control, which is an essential component for intelligent transportation system, to improve the efficiency of smart city's road system. Specifically, various sensors such as surveillance cameras provide real-time information for intelligent traffic light control system to observe the states of both motorized traffic and non-motorized traffic. In this paper, we propose an intelligent traffic light control solution by using distributed multi-agent Q learning, considering the traffic information at the neighboring intersections as well as local motorized and non-motorized traffic, to improve the overall performance of the entire control system. By using the proposed multi-agent Q learning algorithm, our solution is targeting to optimize both the motorized and non-motorized traffic. In addition, we considered many constraints/rules for traffic light control in the real world, and integrate these constraints in the learning algorithm, which can facilitate the proposed solution to be deployed in real operational scenarios. We conducted numerical simulations for a real-world map with real-world traffic data. The simulation results show that our proposed solution outperforms existing solutions in terms of vehicle and pedestrian queue lengths, waiting time at intersections, and many other key performance metrics.",https://ieeexplore.ieee.org/document/8317730/,2017 IEEE 20th International Conference on Intelligent Transportation Systems (ITSC),16-19 Oct. 2017,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/EDUCON52537.2022.9766555,Internet of Things Meets Machine Learning: A Water Usage Alert Example,IEEE,Conferences,"The rapid growth of the electronics industry resulted in numerous, amazing and cheap devices, while fluent documentation and user-friendly programming environments are available for them. Modern educational systems worldwide have exploited this dynamic by including in their didactic curricula innovative practices that are usually called STEM actions. Added to this, enriching educational methods with real-world problem solving techniques increases students&#x2019; interest and prepares them for their future role in the society. Apparently, such challenging problems are not missing, with the depletion of natural resources to be one of the most intense ones. In this context, promising modern technological flavors like Internet of Things (IoT) and Machine Learning (ML) can join their potential to form educationally fruitful and also practically important activities targeted at increasing the social awareness for the water misuse problem, like the ones proposed herein. These activities also encourage the deployment of low-cost appliances that, only with minor modifications, can respond to a wide variety real problems in either urban or rural environments.",https://ieeexplore.ieee.org/document/9766555/,2022 IEEE Global Engineering Education Conference (EDUCON),28-31 March 2022,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/DeSE.2011.10,Introducing a Round Robin Tournament into Evolutionary Individual and Social Learning Checkers,IEEE,Conferences,"In recent years, much research attention has been paid to evolving self-learning game players. Fogel's Blondie24 is a demonstration of a real success in this field, inspiring many other scientists. In this paper, artificial neural networks are used as function evaluators in order to evolve game playing strategies for the game of checkers. We introduce a league structure into the learning phase of an individual and learning system based on the Blondie24 architecture. We show that this helps eliminate some of the randomness in the evolution. The best player we evolve is tested against an implementation of an evolutionary checkers program, and also against a player, which utilises the proposed round robin tournament and finally against an individual and social learning checkers program. The results are promising, suggesting many other research directions.",https://ieeexplore.ieee.org/document/6149996/,2011 Developments in E-systems Engineering,6-8 Dec. 2011,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IJCNN.2003.1223995,Investigating models of social development using a humanoid robot,IEEE,Conferences,"Human social dynamics rely upon the ability to correctly attribute beliefs, goals, and percepts to other people. The set of abilities that allow an individual to infer these hidden mental states based on observed actions and behavior has been called a ""theory of mind"". Drawing from the models of Baron-Cohen (1995) and Leslie (1994), a novel architecture called embodied theory of mind was developed to link high-level cognitive skills to the low-level perceptual abilities of a humanoid robot. The implemented system determines visual saliency based on inherent object attributes, high-level task constraints, and the attentional states of others. Objects of interest are tracked in real-time to produce motion trajectories which are analyzed by a set of naive physical laws designed to discriminate animate from inanimate movement. Animate objects can be the source of attentional states (detected by finding faces and head orientation) as well as intentional states (determined by motion trajectories between objects). Individual components are evaluated by comparisons to human performance on similar tasks, and the complete system is evaluated in the context of a basic social learning mechanism that allows the robot to mimic observed movements.",https://ieeexplore.ieee.org/document/1223995/,"Proceedings of the International Joint Conference on Neural Networks, 2003.",20-24 July 2003,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CogInfoCom47531.2019.9089994,Investigating the Social Robots’ Role in Improving Children Attitudes toward Recycling. The case of PeppeRecycle,IEEE,Conferences,"In this paper we investigate the impact of a social robot in the context of serious games in which the robot plays the role of a game opponent by challenging and, at the same time, teaching the child to correctly recycle waste materials. To this aim we performed a study in which we investigated the dimensions that are used to evaluate serious games integrated with those that are typical of the interaction with a social robot. To endow the robot with the capability to play as a game opponent in a real-world context, we implemented an image recognition module based on a Convolutional Neural Network so that the robot could detect and classify the waste material as a child would do, by seeing it. After a preliminary evaluation of the approach, we started a formal experiment in which we measured the effectiveness of game design, the robot evaluation and the evaluation of cognitive and affective elements that can form the pro-environmental attitude and then the tendency to recycling. A primary school classroom was involved in the study and, results obtained so far, are encouraging and drew promising possibilities for robotics education in changing recycling attitude for children since Pepper is positively evaluated as trustful and believable and this allowed to be concentrated on the `memorization' task during the game.",https://ieeexplore.ieee.org/document/9089994/,2019 10th IEEE International Conference on Cognitive Infocommunications (CogInfoCom),23-25 Oct. 2019,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICAICA50127.2020.9182385,KEAN: Knowledge Embedded and Attention-based Network for POI Recommendation,IEEE,Conferences,"In recent years, with the rapid development of location-based social networks (LBSN) in the Internet, point of interest (POI) recommendation has become a hot spot. Most existing researches make use of contextual information to model users' interest preferences. However, the existing methods for extracting various auxiliary information still need to be improved, such as how to treat the users' social relations equally. In order to obtain users' actual preferences more accurately, in POI recommendation, we propose a deep learning framework KEAN (Knowledge Embedded and Attention Based Network) based on knowledge graph and attention model. The framework includes knowledge-graph embedding method, preference extraction network based on attention mechanism and recommendation network. Our study used knowledge-graph embedding method to get the embedding of each user and POI. In addition, an LSTM network based on attention mechanism was proposed, which uses LSTM network to learn the user's preferences according to the user's check-in sequence. Besides, the attention mechanism was used to extract friends' preferences and merge them with the user's preferences to generate end-user preferences. Finally, our model use fully-connected neural networks to realize recommendations. The effectiveness of the model was proved by the experimental results based on real LBSN datasets.",https://ieeexplore.ieee.org/document/9182385/,2020 IEEE International Conference on Artificial Intelligence and Computer Applications (ICAICA),27-29 June 2020,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/VR.2019.8798046,Keynote Speaker: Virtual Reality for Enhancing Human Perceptional Diversity Towards an Inclusive Society,IEEE,Conferences,"We conducted research project towards an inclusive society from the viewpoint of the computational assistive technologies. This project aims to explore AI-assisted human-machine integration techniques for overcoming impairments and disabilities. By connecting assistive hardware and auditory/visual/tactile sensors and actuators with a user-adaptive and interactive learning framework, we propose and develop a proof of concept of our “xDiversity AI platform” to meet the various abilities, needs, and demands in our society. For example, one of our studies is a wheelchair for automatic driving using “AI technology” called “tele wheelchair”. Its purpose is not fully automated driving but labor saving at nursing care sites and nursing care by natural communication. These attempts to solve the challenges facing the body and sense organs with the help of AI and others. In this keynote we explain the case studies and out final goal for the social design and deployment of the assistive technologies towards an inclusive society.",https://ieeexplore.ieee.org/document/8798046/,2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR),23-27 March 2019,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCTA48790.2019.9478839,Keynote Speech II: Readiness for the Impact of Emerging Technologies,IEEE,Conferences,"Summary form only given. The complete presentation was not made available for publication as part of the conference proceedings. The digital world is becoming increasingly intertwined with the physical world of machines, to which it is bringing ubiquitous intelligence and a perpetual flow of information. These trends are driving us towards a very different future. That future has already started. A new wave of social, economic, and psychological changes is expected to abruptly affect almost everything we do. With change, many opportunities come along. Those who anticipate the course of the future, and prepare for it, will be ready to seize these opportunities and will come out winners. Those who chose to ignore the signs of change, will risk losing their livelihood and eventually hurting their families, businesses, and societies. Those who see the storm coming but react by standing still in panic, disgruntlement, and lamentation will be defenseless when the inevitable waves hit their shores. This presentation overviews the trends in technology and applications, including Artificial Intelligence, Big Data Analytics, Robotics, Internet of Things, Industry 4.0, etc. The impact that such advances are likely to have on the high-tech as well as the low-tech job markets is outlined. Some actions and initiatives are proposed and discussed, with the purpose of triggering a larger debate on how individuals, businesses, academic institutions, and governments should prepare for the anticipated massive changes that are already beginning to affect our world.",https://ieeexplore.ieee.org/document/9478839/,2019 29th International Conference on Computer Theory and Applications (ICCTA),29-31 Oct. 2019,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/BigData47090.2019.9006554,KryptoOracle: A Real-Time Cryptocurrency Price Prediction Platform Using Twitter Sentiments,IEEE,Conferences,"Cryptocurrencies, such as Bitcoin, are becoming increasingly popular, having been widely used as an exchange medium in areas such as financial transaction and asset transfer verification. However, there has been a lack of solutions that can support real-time price prediction to cope with high currency volatility, handle massive heterogeneous data volumes, including social media sentiments, while supporting fault tolerance and persistence in real time, and provide real-time adaptation of learning algorithms to cope with new price and sentiment data. In this paper we introduce KryptoOracle, a novel real-time and adaptive cryptocurrency price prediction platform based on Twitter sentiments. The integrative and modular platform is based on (i) a Spark-based architecture which handles the large volume of incoming data in a persistent and fault tolerant way; (ii) an approach that supports sentiment analysis which can respond to large amounts of natural language processing queries in real time; and (iii) a predictive method grounded on online learning in which a model adapts its weights to cope with new prices and sentiments. Besides providing an architectural design, the paper also describes the KryptoOracle platform implementation and experimental evaluation. Overall, the proposed platform can help accelerate decision-making, uncover new opportunities and provide more timely insights based on the available and ever-larger financial data volume and variety.",https://ieeexplore.ieee.org/document/9006554/,2019 IEEE International Conference on Big Data (Big Data),9-12 Dec. 2019,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/BigData.2018.8622221,Large Scale Open Source Video Recommender Tool Using Metadata Surrogates,IEEE,Conferences,"Video and multi-media sharing is a significant activity on social media platforms. Learning patterns of activities using raw video data is computationally intensive and impractical, and manual inspection is not scalable and prohibitively expensive. An alternate strategy is to learn information about video content using far less compute intensive metadata surrogates. This paper describes a video recommender tool implemented in GovCloud using a novel approach of using lightweight video metadata to learn and classify video content. In contrast to popular video recommender systems that use consumption models for classification, the new approach used in our tool is based solely on the video metadata along with domain expertise used to truth a relatively small subset of relevant video content. The tool is very user-friendly and captures practical knowledge of the user resulting in good learning model. The architecture and implementation specifics of the tool is outlined in this paper. The classifier performance using metadata from tens of thousands of real postings exceeds 90% for both recall and ROC metrics. This tool has shown promise in providing a console for aggregating social media videos for analysts to train the system consistent with the context and task at hand.",https://ieeexplore.ieee.org/document/8622221/,2018 IEEE International Conference on Big Data (Big Data),10-13 Dec. 2018,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/BigData50022.2020.9377817,Learning Embeddings of Directed Networks with Text-Associated Nodes—with Application in Software Package Dependency Networks,IEEE,Conferences,"A network embedding consists of a vector representation for each node in the network. Its usefulness has been shown in many real-world application domains, such as social networks and web networks. Directed networks with text associated with each node, such as software package dependency networks, are commonplace. However, to the best of our knowledge, their embeddings have hitherto not been specifically studied. In this paper, we propose PCTADW-1 and PCTADW-2, two algorithms based on neural networks that learn embeddings of directed networks with text associated with each node. We create two new node-labeled such networks: The package dependency networks in two popular GNU/Linux distributions, Debian and Fedora. We experimentally demonstrate that the embeddings produced by our algorithms resulted in node classification with better quality than those of various baselines on these two networks. We observe that there exist systematic presence of analogies (similar to those in word embeddings) in the network embeddings of software package dependency networks. To the best of our knowledge, this is the first time that such systematic presence of analogies is observed in network and document embeddings. We further demonstrate that these network embeddings can be novelly used for better understanding software attributes, such as the development process and user interface of software, etc.",https://ieeexplore.ieee.org/document/9377817/,2020 IEEE International Conference on Big Data (Big Data),10-13 Dec. 2020,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICTAI.2016.0048,Learning User Embedding Representation for Gender Prediction,IEEE,Conferences,"Predicting the gender of users in social media has aroused great interests in recent years. Almost all existing studies rely on the the content features extracted from the main texts like tweets or reviews. It is sometimes difficult to extract content information since many users do not write any posts at all. In this paper, we present a novel framework which uses only the users' ids and their social contexts for gender prediction. The key idea is to represent users in the embedding connection space. A user often has the social context of family members, schoolmates, colleagues, and friends. This is similar to a word and its contexts in documents, which motivates our study. However, when modifying the word embedding technique for user embedding, there are two major challenges. First, unlike the syntax in language, no rule is responsible for the composition of the social contexts. Second, new users were not seen when learning the representations and thus they do not have embedding vectors. Two strategies circular ordering and incremental updating are proposed to solve these problems. We evaluate our methodology on two real data sets. Experimental results demonstrate that our proposed approach is significantly better than the traditional graph representation and the state-of-the-art graph embedding baselines. It also outperforms the content based approaches by a large margin.",https://ieeexplore.ieee.org/document/7814608/,2016 IEEE 28th International Conference on Tools with Artificial Intelligence (ICTAI),6-8 Nov. 2016,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICDM.2015.10,Learning User Preferences across Multiple Aspects for Merchant Recommendation,IEEE,Conferences,"With the pervasive use of mobile devices, Location Based Social Networks(LBSNs) have emerged in past years. These LBSNs, allowing their users to share personal experiences and opinions on visited merchants, have very rich and useful information which enables a new breed of location-based services, namely, Merchant Recommendation. Existing techniques for merchant recommendation simply treat each merchant as an item and apply conventional recommendation algorithms, e.g., Collaborative Filtering, to recommend merchants to a target user. However, they do not differentiate the user's real preferences on various aspects, and thus can only achieve limited success. In this paper, we aim to address this problem by utilizing and analyzing user reviews to discover user preferences in different aspects. Following the intuition that a user rating represents a personalized rational choice, we propose a novel utility-based approach by combining collaborative and individual views to estimate user preference (i.e., rating). An optimization algorithm based on a Gaussian model is developed to train our merchant recommendation approach. Lastly we evaluate the proposed approach in terms of effectiveness, efficiency and cold-start using two real-world datasets. The experimental results show that our approach outperforms the state-of-the-art methods. Meanwhile, a real mobile application is implemented to demonstrate the practicability of our method.",https://ieeexplore.ieee.org/document/7373403/,2015 IEEE International Conference on Data Mining,14-17 Nov. 2015,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA46639.2022.9811662,Learning to Socially Navigate in Pedestrian-rich Environments with Interaction Capacity,IEEE,Conferences,"Existing navigation policies for autonomous robots tend to focus on collision avoidance while ignoring human-robot interactions in social life. For instance, robots can pass along the corridor safer and easier if pedestrians notice them. Sounds have been considered as an efficient way to attract the attention of pedestrians, which can alleviate the freezing robot problem. In this work, we present a new deep reinforcement learning (DRL) based social navigation approach for autonomous robots to move in pedestrian-rich environments with interaction capacity. Most existing DRL based methods intend to train a general policy that outputs both navigation actions, i.e., expected robot&#x0027;s linear and angular velocities, and interaction actions, i.e., the beep action, in the context of reinforcement learning. Different from these methods, we intend to train the policy via both supervised learning and reinforcement learning. In specific, we first train an interaction policy in the context of supervised learning, which provides a better understanding of the social situation, then we use this interaction policy to train the navigation policy via multiple reinforcement learning algorithms. We evaluate our approach in various simulation environments and compare it to other methods. The experimental results show that our approach outperforms others in terms of the success rate. We also deploy the trained policy on a real-world robot, which shows a nice performance in crowded environments.",https://ieeexplore.ieee.org/document/9811662/,2022 International Conference on Robotics and Automation (ICRA),23-27 May 2022,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SMC.2019.8914455,Learning waste Recycling by playing with a Social Robot,IEEE,Conferences,"In this paper we investigate the use of a social robot as an interface to a serious game aiming to train kids in how to recycle materials correctly. Serious games are mostly used to induce motivations and engagement in users and support knowledge transfer during playing. They are especially effective when the goal of the game concerns behavior change. In addition, social robots have been used effectively in educational settings to engage children in the learning process. Following this trend, we designed a serious game in which the social robot Pepper plays with a child to teach him to correctly recycle the materials. To endow the robot with the capability of detecting and classifying the waste material we developed an image recognition module based on a Convolutional Neural Network. Preliminary experimental results show that the implementation of a serious game about recycling into the Pepper robot improves its social behavior. The use of real objects as waste items during the game turns out to be a successful approach not only for perceived learning effectiveness but also for engagement of the children.",https://ieeexplore.ieee.org/document/8914455/,"2019 IEEE International Conference on Systems, Man and Cybernetics (SMC)",6-9 Oct. 2019,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/HRI.2019.8673212,Lifespan Design of Conversational Agent with Growth and Regression Metaphor for the Natural Supervision on Robot Intelligence,IEEE,Conferences,"Human's direct supervision on robot's erroneous behavior is crucial to enhance a robot intelligence for a `flawless' human-robot interaction. Motivating humans to engage more actively for this purpose is however difficult. To alleviate such strain, this research proposes a novel approach, a growth and regression metaphoric interaction design inspired from human's communicative, intellectual, social competence aspect of developmental stages. We implemented the interaction design principle unto a conversational agent combined with a set of synthetic sensors. Within this context, we aim to show that the agent successfully encourages the online labeling activity in response to the faulty behavior of robots as a supervision process. The field study is going to be conducted to evaluate the efficacy of our proposal by measuring the annotation performance of real-time activity events in the wild. We expect to provide a more effective and practical means to supervise robot by real-time data labeling process for long-term usage in the human-robot interaction.",https://ieeexplore.ieee.org/document/8673212/,2019 14th ACM/IEEE International Conference on Human-Robot Interaction (HRI),11-14 March 2019,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/NCTM.2017.7872829,Linguistic features based personality recognition using social media data,IEEE,Conferences,"Social media has become a prominent platform for opinions and thoughts. This stated that the characteristics of a person can be assessed through social media status updates. The purpose of this research article is to provide a web application in order to detect one's personality using linguistic feature analysis. The personality of a person has classified according to Eysenck's Three Factor personality model. The proposed technique is based on ontology based text classification, linguistic feature-vector matrix using LIWC (Linguistic Inquiry and Word Count) features including semantic analysis using supervised machine learning algorithms and questionnaire based personality detection. This is vital for HR management system when recruiting and promoting employees, R&D Psychologists can use the dynamic ontology for storage purposes and all the other API users including universities and sports clubs. According to the test results the proposed system is in an accuracy level of 91%, when tested with a real world personality detection questionnaire based application, and results demonstrate that the proposed technique can detect the personality of a person with considerable accuracy and a speed.",https://ieeexplore.ieee.org/document/7872829/,2017 6th National Conference on Technology and Management (NCTM),27-27 Jan. 2017,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCCN49398.2020.9209681,LogParse: Making Log Parsing Adaptive through Word Classification,IEEE,Conferences,"Logs are one of the most valuable data sources for large-scale service (e.g., social network, search engine) maintenance. Log parsing serves as the the first step towards automated log analysis. However, the current log parsing methods are not adaptive. Without intra-service adaptiveness, log parsing cannot handle software/firmware upgrade because learned templates cannot match new type of logs. In addition, without cross-service adaptiveness, the logs of a new type of service cannot be accurately parsed when this service is newly deployed. We propose LogParse, an adaptive log parsing framework, to support intra-service and cross-service incremental template learning and update. LogParse turns the template generation problem into a word classification problem and learns the features of template words and variable words. We evaluate LogParse on four public production log datasets. The results demonstrate that LogParse supports accurate adaptive template update (increased from 0.559 to nearly 1.0 parsing accuracy), and a trained LogParse is adaptive for a brand new service&#x2019;s log parsing. Because of LogParse&#x2019;s adaptiveness, we also apply LogParse to an interesting application, log compression and deployed log compression in a top cloud service provider. We package LogParse into an open-source toolkit.",https://ieeexplore.ieee.org/document/9209681/,2020 29th International Conference on Computer Communications and Networks (ICCCN),3-6 Aug. 2020,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICSTCEE54422.2021.9708587,Logistic Regression versus XGBoost: Machine Learning for Counterfeit News Detection,IEEE,Conferences,"In this age of globalization, the unstoppable spreading of fake news via the internet is unstoppable. The spread of false news cannot be supported due to the negative consequences. Society is extremely concerning. In addition, itleads to more serious problems and possible threats, like confusion, misunderstandings, defamation and falsehoods that induce users to share inflammatory content. With the convenience and tremendous increase in information gathering on social networks, it is becoming difficult to differentiate between what is false and what is real. Information can be easily disseminated through sharing, which has contributed to the exponential growth of their forgeries. Machine learning played an important role, in classifying information, although there are some limitations. This article explores various machine learning techniques used to detect fake and fabricated messages. The limitations are discussed using deep learning implementation. In this project, the methodology used is model development and Logistic Regression classifier is considered to detect false news. Based on previous research, this classifier performed well in classification tasks. In this approach, TF-IDF feature is used for the construction of this fake news model to get higher accuracy. The goal of this project is to detect false news using NLP and Machine Learning based on the news content of the article. Following the development of the appropriate Machine Learning model to detect fake/true news, it is deployed into a web interface using Python Flask.",https://ieeexplore.ieee.org/document/9708587/,"2021 Second International Conference on Smart Technologies in Computing, Electrical and Electronics (ICSTCEE)",16-17 Dec. 2021,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICMLA.2015.152,MLaaS: Machine Learning as a Service,IEEE,Conferences,"The demand for knowledge extraction has been increasing. With the growing amount of data being generated by global data sources (e.g., social media and mobile apps) and the popularization of context-specific data (e.g., the Internet of Things), companies and researchers need to connect all these data and extract valuable information. Machine learning has been gaining much attention in data mining, leveraging the birth of new solutions. This paper proposes an architecture to create a flexible and scalable machine learning as a service. An open source solution was implemented and presented. As a case study, a forecast of electricity demand was generated using real-world sensor and weather data by running different algorithms at the same time.",https://ieeexplore.ieee.org/document/7424435/,2015 IEEE 14th International Conference on Machine Learning and Applications (ICMLA),9-11 Dec. 2015,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCMC.2018.8487992,Machine Learning Based Twitter Spam Account Detection: A Review,IEEE,Conferences,"Online social networks (OSNs) are emerging communication medium for people to establish and manage social relationships. In OSNs, regularly billions of users are involved in social interaction, content and opinion dissemination, networking, recommendations, scouting, alerting, and social campaigns. The popularization of OSNs open up a new perspectives and challenges to the study of social networks, being of interest to many fields. Social network is a place where social activities, business oriented activities, entertainment, and information are exchanged. It establish a worldwide connectivity environment where communities of people share their interests and activities, or who are interested in interests and activities of others Although social network has given immense benefits to people at the same time harming people with various mischievous activities that take place on social platforms. This causes significant economic loss to our society and even threaten the national security. All the social networks Facebook, Twitter, LinkedIn, etc. are highly susceptible to malware activities. Twitter is one of the biggest microblogging networking platform, it has more than half a billion tweets are posted every day in average by millions of users on Twitter. Such a versatility and wide spread of use, Twitter easily get intruded with malicious activities. Malicious activities includes malware intrusion, spam distribution, social attacks, etc. Spammers use social engineering attack strategy to send spam tweets, spam URLs, etc. This made twitter an ideal arena for proliferation of anomalous spam accounts. The impact stimulates researchers to develop a model that analyze, detects and recovers from defamatory actions in twitter. Twitter network is inundated with tens of millions of fake spam profiles which may jeopardize the normal user's security and privacy. To improve real users safety and identification of spam profiles become key parts of the research.",https://ieeexplore.ieee.org/document/8487992/,2018 Second International Conference on Computing Methodologies and Communication (ICCMC),15-16 Feb. 2018,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/eIT53891.2022.9837113,Machine Vision Surveillance System - Artificial Intelligence For Covid-19 Norms,IEEE,Conferences,"This paper presents the design and implementation of the Machine Vision Surveillance System Artificial Intelligence (MaViSS-AI) for Covid-19 Norms using jetson nano. This system is designed to be cost-effective, accurate, efficient, and secure. The proposed system tracks and counts humans for monitoring social distancing and detects face masks using object detection methods. We used YOLO as an object detection method and neural network to detect a person and count them. And for social distancing monitoring the concept of the centroid is based on calculating the distance between pairs of centroids, and thus checking whether there are any violations of the threshold or not. To detect the face mask, a YOLO V4 deep learning model is used as the mask detection algorithm. The system also raises alerts when any suspicious event occurs. Given this alert, security personnel can take relevant actions. This research aims to provide a holistic approach to overcoming the real-time challenges encountered during the monitoring of Covid-19 norms.",https://ieeexplore.ieee.org/document/9837113/,2022 IEEE International Conference on Electro Information Technology (eIT),19-21 May 2022,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICMLA.2015.72,Malware Detection in Android-Based Mobile Environments Using Optimum-Path Forest,IEEE,Conferences,"Nowadays, people use smartphones and tablets with the very same purposes as desktop computers: web browsing, social networking and home-banking, just to name a few. However, we are often facing the problem of keeping our information protected and trustworthy. As a result of their popularity and functionality, mobile devices are a growing target for malicious activities. In such context, mobile malwares have gained significant ground since the emergence and growth of smartphones and handheld devices, becoming a real threat. In this paper, we introduced a recently developed pattern recognition technique called Optimum-Path Forest in the context of malware detection, as well we present ""DroidWare"", a new public dataset to foster the research on mobile malware detection. In addition, we also proposed to use Restricted Boltzmann Machines for unsupervised feature learning in the context of malware identification.",https://ieeexplore.ieee.org/document/7424412/,2015 IEEE 14th International Conference on Machine Learning and Applications (ICMLA),9-11 Dec. 2015,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/WETICE.2019.00013,Meaning Extraction in a Domotic Assistant Agent Interacting by Means of Natural Language,IEEE,Conferences,"This paper presents a software architecture to let users interact with their smart home devices, through the commonly used social network channels. The software, called FABULOS, is the result of the combination of components used to interact with devices and social networks, and a rule-based artificial intelligence, which implements the base logic for the automation. The core of the software architecture is the translation service, which has the objective of extracting the meaning of the sentences provided by text and voice messages by users in natural language, transforming them into proper device commands. In order to achieve this, the proposed solution relies on an approach which has the capability to interpret and extract the meaning of the intentions plus the entities involved, associated to the sentences sent by the users. A description of a realistic case-study, which shows an example of how the proposed software behaves in a real interaction with an user, is also included in the paper.",https://ieeexplore.ieee.org/document/8795403/,2019 IEEE 28th International Conference on Enabling Technologies: Infrastructure for Collaborative Enterprises (WETICE),12-14 June 2019,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/DTPI52967.2021.9540077,Mechanical Design Paradigm based on ACP Method in Parallel Manufacturing,IEEE,Conferences,"Parallel Manufacturing is a new manufacturing paradigm in industry, deeply integrating informalization, automation, and artificial intelligence. In this paper we propose a new mechanical design paradigm in Parallel Manufacturing based on ACP method. The key is to regard the design procedure based on artificial design and emulation method as two independent procedures, which can be modeled as a parallel system. The design procedure based on ACP method does not include a real system, which is an inventive extension of the traditional parallel system. This method can be implemented with social information by introducing the definition of SDV, SDM, and Intelligent Design Manager, making it highly adaptive for social manufacturing and Parallel Manufacturing.",https://ieeexplore.ieee.org/document/9540077/,2021 IEEE 1st International Conference on Digital Twins and Parallel Intelligence (DTPI),15 July-15 Aug. 2021,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCSMT54525.2021.00083,Medical Analysis of Social Media Data Based on Spark and Machine Learning in China,IEEE,Conferences,"Social media embracing a huge amount of real-time data of all kinds plays an important role in data analysis in the era of big data. Knowledge between medical workers and ordinary people can be popularized and exchanged via social media. At the same time, the collection and utilization of medical data on social media can effectively grasp the public health situation and provide better help to improve people&#x0027;s health status. From the perspective of medical care and health, this paper uses Weibo, the largest public social media in China, to obtain data for analysis. The study was developed under the Spark framework, using naive Bayes, random forest and two different feature extraction methods to clean, pre-process and classify data. Furthermore, the accuracy rate and F1 Score were used to evaluate the model, to find the most appropriate method. The result of this research shows that the data obtained from Weibo within certain age groups has a good reference value in the public awareness and current situation, and are good for grasping the trend of diseases.",https://ieeexplore.ieee.org/document/9786983/,2021 2nd International Conference on Computer Science and Management Technology (ICCSMT),12-14 Nov. 2021,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CSCI54926.2021.00305,Metricam: Fast and Reliable Social Distancing Analysis in Online Security Cameras,IEEE,Conferences,"Distance measurements taken from 2D camera images are subject to the correct estimation of the camera&#x2019;s perspective, that is, the spatial mapping from 2D points imaged by a camera to the correspondent 3D ones in the real world. Current solutions to solve this 3D reconstruction are either dependent on the estimation of vanishing points through the detection of straight lines on targeted images or by employing sophisticated sensors and deep learning algorithms, which require expensive training on huge annotated datasets. Nevertheless, none of those approaches provide the required level of precision and accuracy for social distancing evaluation. In this paper we present Metricam, a real-time lightweight software system for security cameras that computes a 2D to 3D mapping using computational geometry and uses the DBSCAN clustering algorithm to evaluate social distancing evaluation. With Metricam, we have been able to identify several places prone to agglomeration inside the Butant&#x00E3; campus of the University of S&#x00E3;o Paulo, and provide the local authorities with valuable information to fight off the pandemic.",https://ieeexplore.ieee.org/document/9799026/,2021 International Conference on Computational Science and Computational Intelligence (CSCI),15-17 Dec. 2021,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SNPD.2014.6888682,Microcredit risk assessment using crowdsourcing and social networks,IEEE,Conferences,"The task of automated risk assessment is attracting significant attention in the light of the recent microloan popularity growth. The industry requires a real time method for the timely processing of the extensive number of applicants for short-term small loans. Owing to the vast number of applications, manual verification is not a viable option. In cooperation with a microloan company in Azerbaijan, we have researched automated risk assessment using crowdsourcing. The principal concept behind this approach is the fact that a significant amount of information relating to a particular applicant can be retrieved from the social networks. The suggested approach can be divided into three parts: First, applicant information is collected on social networks such as LinkedIn and Facebook. This can only occur with the applicant's permission. Then, this data is processed using a program that extracts the relevant information segments. Finally, these information segments are evaluated using crowdsourcing. We attempted to evaluate the information segments using social networks. To that end, we automatically posted requests on the social networks regarding certain information segments and evaluated the community response by counting “likes” and “shares”. For example, we posted the status, “Do you think that a person who has worked at ABC Company is more likely to repay a loan? Please “like” this post if you agree.” From the results, we were able to estimate public opinion. Once evaluated, each information segment was then given a weight factor that was optimized using available loan-repay test data provided to us by a company. We then tested the proposed system on a set of 400 applicants. Using a second crowdsourcing approach, we were able to confirm that the resulting solution provided a 92.5% correct assessment, with 6.45% false positives and 11.11% false negatives, with an assessment duration of 24 hours.",https://ieeexplore.ieee.org/document/6888682/,"15th IEEE/ACIS International Conference on Software Engineering, Artificial Intelligence, Networking and Parallel/Distributed Computing (SNPD)",30 June-2 July 2014,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICODSE.2018.8705834,Microtask Crowdsourcing Marketplace for Social Network,IEEE,Conferences,"Crowdsourcing is a powerful way to process and collect data that needs human's logic and perception. Crowdsourcing can take part in many hard to compute problems such as data entry, multimedia transcriptions and many case of artificial intelligence. While this is a powerful approach, crowdsourcing needs a relatively large marketplace to works optimum. In context of Indonesia, we hardly hear about task crowdsourcing even though there are some marketplace like poin-web.co.id. In this paper, we propose a social network for crowdsourcing marketplace to penetrate the market. People tends to waste most of their time on social network and game rather than other mobile application in case of mobile usage. The idea is to make crowdsourcing as a filler while people are using social networks like waiting for a chat or scrolling the timeline. And by applying microtask as the task, people will not have much burden on doing the task. On the other hand, he/she will get additional income. We implemented LINE as a basis of our social network marketplace. LINE does provide the most interactive way of provide message. Additionally, the user of LINE in Indonesia is growing in a fast pace. We conduct an experiment focused on worker's coverage and ease of use. By using usability testing as a basis of ease of use evaluation, we received good feedbacks as 90.9% of the users feel easier to answer through LINE and is excited to use the platform in case it goes with real money.",https://ieeexplore.ieee.org/document/8705834/,2018 5th International Conference on Data and Software Engineering (ICoDSE),7-8 Nov. 2018,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/RO-MAN47096.2020.9223436,Migratable AI: Effect of identity and information migration on users' perception of conversational AI agents,IEEE,Conferences,"Conversational AI agents are proliferating, embodying a range of devices such as smart speakers, smart displays, robots, cars, and more. We can envision a future where a personal conversational agent could migrate across different form factors and environments to always accompany and assist its user to support a far more continuous, personalized and collaborative experience. This opens the question of what properties of a conversational AI agent migrates across forms, and how it would impact user perception. To explore this, we developed a Migratable AI system where a user's information and/or the agent's identity can be preserved as it migrates across form factors to help its user with a task. We validated the system by designing a 2x2 between-subjects study to explore the effects of information migration and identity migration on user perceptions of trust, competence, likeability and social presence. Our results suggest that identity migration had a positive effect on trust, competence and social presence, while information migration had a positive effect on trust, competence and likeability. Overall, users report highest trust, competence, likeability and social presence towards the conversational agent when both identity and information were migrated across embodiments.",https://ieeexplore.ieee.org/document/9223436/,2020 29th IEEE International Conference on Robot and Human Interactive Communication (RO-MAN),31 Aug.-4 Sept. 2020,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICEBE.2018.00015,Mining Emotions of the Public from Social Media for Enhancing Corporate Credit Rating,IEEE,Conferences,"The proliferation of online social media has been changing the ways how individuals interact with corporations. Previous studies have examined how to extract investors' sentiments captured on social media to enhance stock prediction. However, little work was done to leverage public's emotions captured on social media to predict corporate credit risks. Our research fills the current research gap by developing a new computational method to extract public's emotions embedded in social postings to supplement common financial indicators (e.g., return-on-assets) for predicting corporate credit ratings. Grounded in Plutchik's Wheel of Emotions, the proposed computational framework can automatically extract the distribution of eight basic emotions from textual postings on online social media. In particular, one main contribution of our work is the development of the new emotion latent dirichlet allocation (ELDA) model for textual emotion analysis. In addition, we develop an ensemble learning model with random forest (RF) as the basis classifier to improve the performance of corporate credit rating. Based on the real-world data crawled from Twitter, our experimental results confirm that the proposed ELDA model can effectively and efficiently extract public's emotions from social postings to enhance the prediction of corporate credit ratings. To our best knowledge, this is the first successful research of developing a new computational model of extracting public's emotions from social postings to enhance corporate credit risk prediction.",https://ieeexplore.ieee.org/document/8592626/,2018 IEEE 15th International Conference on e-Business Engineering (ICEBE),12-14 Oct. 2018,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICASI.2017.7988441,Mining tweets for education reforms,IEEE,Conferences,"Microblogging and social networking sites have become a popular means of communication channels among internet users. They provide tools for people to voice their opinions. These sites contain vast amounts of opinionated data, leading to an increased growth in research on sentiment analysis and opinion mining. The study aims at using Twitter, a major and popular platform for microblogging and social communication, to conduct sentiment analysis. Real time data was automatically streamed using the Twitter API to collect the public's sentiments regarding education. A survey was also used to capture the public's opinions. The study will help overcome frustrations during implementation of education policies and reforms by taking into account the public's views and opinions.",https://ieeexplore.ieee.org/document/7988441/,2017 International Conference on Applied System Innovation (ICASI),13-17 May 2017,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IEMCON53756.2021.9623097,Missing Data: Comparison of Multiple-Imputation Algorithms for Social Determinants of Health in Cervical Cancer Stage Detection,IEEE,Conferences,"Social Determinants of Health impact general health conditions within a population. However, missing data affect statistical analysis and forecasting of diseases. Multiple imputation has gained momentum and several machine learning algorithms have been used for data imputation. As most statistical analysis and machine learning software have already implemented these algorithms, their performance is usually taken for granted without further analysis. Furthermore, we notice a discrepancy between how imputation must be carried out and how it is usually performed in real-word practice. Thus, in this work we examine different machine learning algorithms for multiple imputation in two datasets with Social Determinants of Health in Cervical Cancer. The results of this comparison are presented.",https://ieeexplore.ieee.org/document/9623097/,"2021 IEEE 12th Annual Information Technology, Electronics and Mobile Communication Conference (IEMCON)",27-30 Oct. 2021,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICETC.2010.5529259,Model construct and case study of interpersonal interaction in virtual learning community,IEEE,Conferences,"Based on the interaction among the members of virtual learning community, the social computing software is a powerful tool. Exploring the connotative features of the model construction and the application process of the Interpersonal interaction among the community members from macro theoretical perspective to micro technical perspective will deepen understanding and value judgments of educational technology about social computing.",https://ieeexplore.ieee.org/document/5529259/,2010 2nd International Conference on Education Technology and Computer,22-24 June 2010,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/WI-IAT.2015.242,Modeling Curiosity in Virtual Companions to Improve Human Learners' Learning Experience,IEEE,Conferences,"A key design aspect for virtual learning companions is their believability. A lot of attention has been paid to emotion modeling which is at the core of believability. However, most of the existing emotion models neglect the epistemology-based emotions, which are knowledge-related emotions that affect the human learning process. Studies have shown that curiosity is an important epistemology-based emotion that positively influences social learning. Hence, modeling curiosity in learning companions may improve human learners' learning experience in a virtual environment. However, existing curiosity models assume simplified cognitive processes and fail to capture multiple sources of curiosity stimuli. In this paper, we propose a novel model of curiosity for learning companions to capture salient curiosity stimuli through a psychologically inspired approach. Our model is built based on Berlyne's theory and considers three most salient appraisal variables in a virtual learning environment, including novelty, surprise, and uncertainty. The model is built on planbased knowledge representations augmented with planning. Two internal processes are modeled for learning companions to demonstrate curiosity: curiosity appraisal and learning. The proposed model of curiosity is implemented in a learning companion and evaluated through user studies. The evaluation results show that the learning companion's curiosity significantly improves human learners' learning experience from multiple aspects.",https://ieeexplore.ieee.org/document/7397337/,2015 IEEE/WIC/ACM International Conference on Web Intelligence and Intelligent Agent Technology (WI-IAT),6-9 Dec. 2015,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCMS.2010.265,Modeling and Simulating Multi-agent Model Connections: An Application to Information Paradigm,IEEE,Conferences,"This paper presents the information paradigm as a new vision which attempts to connect the conscious experience with the physical world. Two issues concerning this paradigm are investigated: the multi-agent model, which is the base of this theory, and the agent's relations, actually their information states. The relations between them, including their attributes and characteristics, participate in forming so called `baby universe' structures. The simulation of this multi-agent model is done in NetLogo 4.0.4., which is a programmable modeling environment for simulating natural and social phenomena. This environment is particularly well suited for modeling complex systems developing over time. We will simulate fully and partly connected networks in random and circuit arrangements. They will be investigated from mathematical perspective, as a strong support for the theory. As an application to this theory we'll calculate the rest mass ratio of the muon, electron and tauon sub particles, compared with the results obtained at CERN. The integration of the two perspectives offers the appropriate scientific foundation for the new paradigm and see the offering of the new vision of physical reality enabling to resolve some of the main barriers known in contemporary quantum mechanics and artificial intelligence.",https://ieeexplore.ieee.org/document/5421127/,2010 Second International Conference on Computer Modeling and Simulation,22-24 Jan. 2010,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SNPD.2012.138,Modelling Large Complex Systems Using Multi-agent Technology,IEEE,Conferences,"The paper outlines a method for modelling largescale commercial, social, socio-technological and engineering problems. The method is derived from twelve years of experience in designing and implementing large complex systems for real-time scheduling of taxis, air taxis, car rentals, seagoing tankers, trucks, space crafts; dynamic data mining; dynamic knowledge discovery and semantic search. The same approach has been also used for designing adaptive engineering systems and for research into social issues such as eradication of poverty.",https://ieeexplore.ieee.org/document/6299317/,"2012 13th ACIS International Conference on Software Engineering, Artificial Intelligence, Networking and Parallel/Distributed Computing",8-10 Aug. 2012,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICACCI.2014.6968416,Modified MapReduce framework for enhancing performance of graph based algorithms by fast convergence in distributed environment,IEEE,Conferences,"The amount of data which is produced is huge in current world and more importantly it is increasing exponentially. Traditional data storage and processing techniques are ineffective in handling such huge data [10]. Many real life applications require iterative computations in general and in particular used in most of machine learning and data mining algorithms over large datasets, such as web link structures and social network graphs. MapReduce is a software framework for easily writing applications which process large amount of data (multi-terabyte) in parallel on large clusters (thousands of nodes) of commodity hardware. However, because of batch oriented processing of MapReduce we are unable to utilize the benefits of MapReduce in iterative computations. Our proposed work is mainly focused on optimizing three factors resulting in performance improvement of iterative algorithms in MapReduce environment. In this paper, we address the key issues based on execution of tasks, the unnecessary creation of new task in each iteration and excessive shuffling of data in each iteration. Our preliminary experiments have shown promising results over the basic MapReduce framework. The comparative study with existing solutions based on MapReduce framework like HaLoop, has also shown better performance w.r.t algorithm run time and amount of data traffic over Hadoop Cluster.",https://ieeexplore.ieee.org/document/6968416/,"2014 International Conference on Advances in Computing, Communications and Informatics (ICACCI)",24-27 Sept. 2014,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/TrustCom50675.2020.00243,Monitoring Social Media for Vulnerability-Threat Prediction and Topic Analysis,IEEE,Conferences,"Publicly available software vulnerabilities and exploit code are often abused by malicious actors to launch cyberattacks to vulnerable targets. Organizations not only have to update their software to the latest versions, but do effective patch management and prioritize security-related patching as well. In addition to intelligence sources such as Computer Emergency Response Team (CERT) alerts, cybersecurity news, national vulnerability database (NBD), and commercial cybersecurity vendors, social media is another valuable source that facilitates early stage intelligence gathering. To early detect future cyber threats based on publicly available resources on the Internet, we propose a dynamic vulnerability-threat assessment model to predict the tendency to be exploited for vulnerability entries listed in Common Vulnerability Exposures, and also to analyze social media contents such as Twitter to extract meaningful information. The model takes multiple aspects of vulnerabilities gathered from different sources into consideration. Features range from profile information to contextual information about these vulnerabilities. For the social media data, this study leverages machine learning techniques specially for Twitter which helps to filter out non-cybersecurity-related tweets and also label the topic categories of each tweet. When applied to predict the vulnerabilities exploitation and analyzed the real-world social media discussion data, it showed promising prediction accuracy with purified social media intelligence. Moreover, the AI-enabling modules have been deployed into a threat intelligence platform for further applications.",https://ieeexplore.ieee.org/document/9343128/,"2020 IEEE 19th International Conference on Trust, Security and Privacy in Computing and Communications (TrustCom)",29 Dec.-1 Jan. 2021,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CyberSecurity49315.2020.9138894,Moving Targets: Addressing Concept Drift in Supervised Models for Hacker Communication Detection,IEEE,Conferences,"In this paper, we are investigating the presence of concept drift in machine learning models for detection of hacker communications posted in social media and hacker forums. The supervised models in this experiment are analysed in terms of performance over time by different sources of data (Surface web and Deep web). Additionally, to simulate real-world situations, these models are evaluated using time-stamped messages from our datasets, posted over time on social media platforms. We have found that models applied to hacker forums (deep web) presents an accuracy deterioration in less than a 1-year period, whereas models applied to Twitter (surface web) have not shown a decrease in accuracy for the same period of time. The problem is alleviated by retraining the model with new instances (and applying weights) in order to reduce the effects of concept drift. While our results indicated that performance degradation due to concept drift is avoided by 50% relabelling, which is challenging in real-world scenarios, our work paves the way to more targeted concept drift solutions to reduce the re-training tasks.",https://ieeexplore.ieee.org/document/9138894/,2020 International Conference on Cyber Security and Protection of Digital Services (Cyber Security),15-19 June 2020,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/RACSS.2012.6212691,MuGRAM: An approach for multi-labelled graph matching,IEEE,Conferences,"Graph mining has been a widely studied domain over the years. Graph representation of real world problems has enabled the development of simple solutions bringing in better clarity. Graph mining has various sub domains among which graph matching is a prominent one having a number of algorithms. With the rise of new applications involving large sets of networked data, the performance of these algorithms has become important. The graph based representations for social networks and communication networks have led to the evolution of multi-labelled large graphs which are still not completely handled by the existing algorithms. The requirement of a fast and efficient indexing process so as to accommodate dynamic graphs without having to opt for incremental indexing is another major challenge. In this paper, we propose MuGRAM - a multi-labelled graph matching approach aimed at addressing the above mentioned issues. This approach is capable of handling multiple labels for vertices as well as edges of reference graphs. An enhanced indexing method proposed in the paper ensures a fast indexing process. A breadth first search oriented spanning tree along with a novel technique for neighbourhood matching ensures fast query processing. Experimental evaluation of MuGRAM in comparison with some of the recent algorithms in the field highlights its superior performance.",https://ieeexplore.ieee.org/document/6212691/,2012 International Conference on Recent Advances in Computing and Software Systems,25-27 April 2012,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IJCNN52387.2021.9534428,Multi-Modal Multi-Instance Multi-Label Learning with Graph Convolutional Network,IEEE,Conferences,"When applying machine learning to tackle realworld problems, it is common to see that objects come with multiple labels rather than a single label. In addition, complex objects can be composed of multiple modalities, e.g. a post on social media may contain both texts and images. Previous approaches typically treat every modality as a whole, while it is not the case in real world, as every post may contain multiple images and texts with quite diverse semantic meanings. Therefore, Multi-modal Multi-instance Multi-label (M3) learning was proposed. Previous attempt at M3 learning argues that exploiting label correlations is crucial. In this paper, we find that we can handle M3 problems using graph convolutional network. Specifically, a graph is built over all labels and each label is initially represented by its word embedding. The main goal for GCN is to map those label embed dings into inter-correlated label classifiers. Moreover, multi-instance aggregation is based on attention mechanism, making it more interpretable because it naturally learns to discover which pattern triggers the labels. Empirical studies are conducted on both benchmark datasets and industrial datasets, validating the effectiveness of our method, and it is demonstrated in ablation studies that the components in our methods are essential.",https://ieeexplore.ieee.org/document/9534428/,2021 International Joint Conference on Neural Networks (IJCNN),18-22 July 2021,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICAIS53314.2022.9742828,Multi-Parameter Smart Health Monitoring System using Internet of Things,IEEE,Conferences,"Low-cost, lightweight, tiny, and intelligent physiological sensor nodes have been designed in the recent technical developments in sensor systems, low power integrated circuits and wireless communications. These sensor nodes can detect analyses and transmit one or more vital signs and can be incorporated smoothly into healthcare social sensor networks. This network promises to change healthcare by enabling cheap, invasive, ongoing ambulatory health surveillance with online medical data updated nearly in real time. Despite several continuous research efforts, several technological, economic and societal issues are essential to the There are still some technological challenges to be addressed in order to develop diverse social sensor networks applicable to medical, economic and social and power efficiency applications. In this proposed work, novel method which is used to track patients in hospitals at home as well. The experimental analysis starts with the implementation of IoT sectors, mainly an Arduino-UNO health observation scheme. Patient&#x2019;s cardiac rates and body temperature is monitor in the proposed work. Arduino-UNO is used as the 8-bit microcontroller, ATMEGA 328. LM 35 is used for body temperature sensing, and XD-58C for cardiac beat rate measurement is used for DIY pulse tracker. Wi-Fi module EP8266 is used to move the data of the patient from the Arduino uno node. For IoT purposes, the BLYNK programmer is used. A new algorithm is proposed which is named as CBHA (Sensor clustering based Human Activities Recognition) that analyze state of the Patient. The data transferred from the WiFi module can be used from anywhere in the app, meaning that doctors can watch patients remotely and take prompt decisions if something goes wrong with the information that has been detected.",https://ieeexplore.ieee.org/document/9742828/,2022 Second International Conference on Artificial Intelligence and Smart Energy (ICAIS),23-25 Feb. 2022,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/LCN52139.2021.9524999,Multi-level Graph Attention Network based Unsupervised Network Alignment,IEEE,Conferences,"Network alignment is the matching of two networks with corresponding nodes that belong to the same user or entity. The most common application is to analyze which accounts belong to the same user in two social networks. Most of existing techniques rely on matrix factorization so that they cannot be scaled to large-scale networks, are constrained by strict constraints, and cannot learn node embedding without a training set. In this paper, we propose an unsupervised network alignment model based on multi-level graph attention networks. The model uses multi-level graph attention network to learn the embedded representation of nodes, satisfying attribute and structure constraints of alignment. Augmented learning process is proposed to simulate attribute noise and structural noise to improve adaptability of the model. Extensive experiments on real datasets show that the proposed model performs better than the state-of-the-art network alignment model. We also demonstrate the robustness of the proposed model.",https://ieeexplore.ieee.org/document/9524999/,2021 IEEE 46th Conference on Local Computer Networks (LCN),4-7 Oct. 2021,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SMARTCOMP50058.2020.00051,Multi-modal Adversarial Training for Crisis-related Data Classification on Social Media,IEEE,Conferences,"Social media platforms such as Twitter are increasingly used to collect data of all kinds. During natural disasters, users may post text and image data on social media platforms to report information about infrastructure damage, injured people, cautions and warnings. Effective processing and analysing tweets in real time can help city organisations gain situational awareness of the affected citizens and take timely operations. With the advances in deep learning techniques, recent studies have significantly improved the performance in classifying crisis-related tweets. However, deep learning models are vulnerable to adversarial examples, which may be imperceptible to the human, but can lead to model's misclassification. To process multi-modal data as well as improve the robustness of deep learning models, we propose a multi-modal adversarial training method for crisis-related tweets classification in this paper. The evaluation results clearly demonstrate the advantages of the proposed model in improving the robustness of tweet classification.",https://ieeexplore.ieee.org/document/9239612/,2020 IEEE International Conference on Smart Computing (SMARTCOMP),14-17 Sept. 2020,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/BIBM52615.2021.9669328,Multi-modal Information Fusion-powered Regional Covid-19 Epidemic Forecasting,IEEE,Conferences,"With the current raging spread of the COVID19, early forecasting of the future epidemic trend is of great significance to public health security. The COVID-19 is virulent and spreads widely. An outbreak in one region often triggers the spread of others, and regions with relatively close association would show a strong correlation in the spread of the epidemic. In the real world, many factors affect the spread of the outbreak between regions. These factors exist in the form of multimodal data, such as the time-series data of the epidemic, the geographic relationship, and the strength of social contacts between regions. However, most of the current work only uses historical epidemic data or single-modal geographic location data to forecast the spread of the epidemic, ignoring the correlation and complementarity in multi-modal data and its impact on the disease spread between regions. In this paper, we propose a Multimodal InformatioN fusion COVID-19 Epidemic forecasting model (MINE). It fuses inter-regional and intra-regional multi-modal information to capture the temporal and spatial relevance of the COVID-19 spread in different regions. Extensive experimental results show that the proposed method achieves the best results compared to state-of-art methods on benchmark datasets.",https://ieeexplore.ieee.org/document/9669328/,2021 IEEE International Conference on Bioinformatics and Biomedicine (BIBM),9-12 Dec. 2021,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICECE48499.2019.9058508,Multi-modal Neural Network for Traffic Event Detection,IEEE,Conferences,"Cities are composed of complex systems with Cyber, Physical, and Social (CPS) components. The advances in the Internet of Things (IoTs) and social networking services help people understand the dynamics of cities. Traffic event detection is an important while complex task in transportation modeling and management of smart cities. In this paper, we address the task of detecting traffic events using two types of data, i.e. physical sensor observations and social media text. Unlike most existing studies focused on either analysing sensor observations or social media data, we identify traffic events with both types of data that may complement each other. We propose a Multi-modal Neural Network (MMN) to process sensor observations and social media texts simultaneously and detect traffic events. We evaluate our model with a real-world CPS dataset consisting of sensor observations, event reports, and tweets collected from Twitter about San Francisco over a period of 4 months. The evaluation shows promising results and provides insights into the analysis of multi-modal data for detecting traffic events.",https://ieeexplore.ieee.org/document/9058508/,2019 IEEE 2nd International Conference on Electronics and Communication Engineering (ICECE),9-11 Dec. 2019,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISPA-BDCloud-SocialCom-SustainCom52081.2021.00189,Multi-network Embedding for Missing Point-of-Interest Identification,IEEE,Conferences,"The large volume of data flowing throughout location-based social networks (LBSNs) provides an opportunity for human mobility behavior understanding and prediction. However, data quality issues (e.g., historical check-in POI missing, data sparsity) limit the effectiveness of existing LBSN-oriented studies, e.g., Point-of-Interest (POI) recommendation or prediction. Contrary to previous efforts in next POI recommendation or prediction, we focus on identifying the missing POI which the user has visited at a past specific time and proposed a multi-network Embedding (MNE) method. Specifically, the model jointly captures temporal cyclic effect, user preference and sequence transition influence in a unified way by embedding five relational information graphs into a shared dimensional space from both POI- and category-instance levels. The proposed model also incorporates region-level spatial proximity to explore geographical influence, and derives the ranking score list of candidates for missing POI identification. We conduct extensive experiments to evaluate the performance of our model on two real large-scale datasets, and the experimental results show its superiority over other competitors. Significantly, it also proves that the proposed model can be naturally transferred to general next POI recommendation and prediction tasks with competitive performances.",https://ieeexplore.ieee.org/document/9644693/,"2021 IEEE Intl Conf on Parallel & Distributed Processing with Applications, Big Data & Cloud Computing, Sustainable Computing & Communications, Social Computing & Networking (ISPA/BDCloud/SocialCom/SustainCom)",30 Sept.-3 Oct. 2021,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICTAI.2010.123,Multi-relational Topic Model for Social Recommendation,IEEE,Conferences,"Various attribute and relation information is used in social recommendation systems. However, previous approaches fail to use them in a unified way. In this paper, we propose a unified framework for social recommendation. Entities like users and items are described by their tags. We model each entity using topic models like Latent Dirichlet Allocation(LDA) and then connect these topic models to form a multi-relational network. Various relations between entities in recommender systems such as rating relation or user friend relation can be expressed as edges in the multi-relational network. We evaluate our model on a real-life dataset collected from a commercial recommender website. Experiments validate the generative performance and predictive performance of our model.",https://ieeexplore.ieee.org/document/5670084/,2010 22nd IEEE International Conference on Tools with Artificial Intelligence,27-29 Oct. 2010,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CVPR42600.2020.01469,Multimodal Categorization of Crisis Events in Social Media,IEEE,Conferences,"Recent developments in image classification and natural language processing, coupled with the rapid growth in social media usage, have enabled fundamental advances in detecting breaking events around the world in real-time. Emergency response is one such area that stands to gain from these advances. By processing billions of texts and images a minute, events can be automatically detected to enable emergency response workers to better assess rapidly evolving situations and deploy resources accordingly. To date, most event detection techniques in this area have focused on image-only or text-only approaches, limiting detection performance and impacting the quality of information delivered to crisis response teams. In this paper, we present a new multimodal fusion method that leverages both images and texts as input. In particular, we introduce a cross-attention module that can filter uninformative and misleading components from weak modalities on a sample by sample basis. In addition, we employ a multimodal graph-based approach to stochastically transition between embeddings of different multimodal pairs during training to better regularize the learning process as well as dealing with limited training data by constructing new matched pairs from different samples. We show that our method outperforms the unimodal approaches and strong multimodal baselines by a large margin on three crisis-related tasks.",https://ieeexplore.ieee.org/document/9157116/,2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),13-19 June 2020,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/GRC.2009.5255152,Multiview spectral clustering via ensemble,IEEE,Conferences,"Clustering on multiple views is witnessing increasing interests in both real-world application and machine learning community. A typical application is to discover communities of joint interests in social network, such as Facebook and Twitter. The network can be simply modeled as a graph in which the nodes are the people while the links show relationship between the people. There may exist many relationships between a pair of nodes, such as classmates, collaborators, playmates and so on. It is important to consider how to use these graphs together rather than a single graph if we want to understand the network and their participants effectively. Motivated by the fact, we present a clustering algorithm using spectral analysis in which multiple graphs are considered to get the clusters. Our study can also be considered as an instance of multi-views learning. The experimental results on UCI data set and Corel image data demonstrate the promising results that validate our proposed algorithm.",https://ieeexplore.ieee.org/document/5255152/,2009 IEEE International Conference on Granular Computing,17-19 Aug. 2009,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ITNG.2011.116,Nerve: A Lightweight Middleware for Quality-of-service Networked Robotics,IEEE,Conferences,"Social robots must adapt to dynamic environments, human interaction partners and challenging new stringent tasks. Their inner software should be designed and deployed carefully because slight changes in the robot's requirements can have an important impact in the existing code. This paper focus on the design and implementation of a lightweight middleware for networked robotics called \textit{Nerve}, which guarantees the scalability and quality-of-service requirements for this kind of real-time software. Its benefits have been proved through its use in a Robot Learning by Imitation control architecture, but its design guidelines are general enough to be also applied with common distributed and real-time embedded applications.",https://ieeexplore.ieee.org/document/5945314/,2011 Eighth International Conference on Information Technology: New Generations,11-13 April 2011,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/TENCON.2009.5396250,Neuro-fuzzy decision trees for content popularity model and multi-genre movie recommendation system over social network,IEEE,Conferences,"In this paper, we propose a framework of multi-genre movie recommender system based on neuro-fuzzy decision tree (NFDT) methodology. The system is capable of recommending list of movies in descending order of preference in response to user queries and profiles. The system also takes care of attempt to vote stuffing using novel application of fuzzy c-means clustering algorithm. Typical user query and profiles consists of content ratings for multiple genres like action, comedy, drama, music and many others. The distinctive point of the proposed approach is to handle recommender system generation as a supervised pattern classification problem, where in user reviews for multiple genres are conditions and overall star ratings are decisions. The entire recommender system is represented in the form of NFDT. Rules represented by NFDT also acts as a tool for understanding the combinations of contents driving popularity (and unpopularity) over certain social network. We have also proposed a modified inference mechanism based on matching and ordering of firing strength of each fuzzy decision tree path in response to user queries. The computational experiments have been presented on a sample real-world movie review database to judge the efficiency of the proposed recommender system.",https://ieeexplore.ieee.org/document/5396250/,TENCON 2009 - 2009 IEEE Region 10 Conference,23-26 Jan. 2009,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICECET52533.2021.9698712,OCR Error Correction Using BiLSTM,IEEE,Conferences,"Language models have critical importance in the pre- and post-processing of optical character recognition (OCR). The quality of documents and scanners is important for OCR systems, with inferior quality leading to more erroneous output. For long time intervals of sequences, long short-term memory (LSTM) fulfills the requirements because it can solve problems with long-term dependencies. In this study, we evaluate the performance of error correction for OCR data using LSTM. The results show that we have good performance for correcting error words by using bidirectional LSTM (BiLSTM). We obtain 98.13&#x0025; better performance in correcting error words by using OCRd data and 97.18&#x0025; better performance by using social media data. In this respect, we show that the method we have applied can be used for error corrections.",https://ieeexplore.ieee.org/document/9698712/,"2021 International Conference on Electrical, Computer and Energy Technologies (ICECET)",9-10 Dec. 2021,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CLEI53233.2021.9639989,ODROM: Object Detection and Recognition supported by Ontologies and applied to Museums,IEEE,Conferences,"In robotics, object detection in images or videos, obtained in real-time from sensors of robots can be used to support the implementation of service robot tasks (e.g., navigation, model its social behavior, recognize objects in a specific domain), usually accomplished in indoor environments. However, traditional deep learning based object detection techniques present limitations in such indoor environments, specifically related to the detection of small objects and the management of high density of multiple objects. Coupled with these limitations, for specific domains (e.g., hospitals, museums), it is important that the robot, apart from detecting objects, extracts and knows information of the targeted objects. Ontologies, as a part of the Semantic Web, are presented as a feasible option to formally represent the information related to the objects of a particular domain. In this context, this work proposes an object detection and recognition process based on a Deep Learning algorithm, object descriptors, and an ontology. ODROM, an Object Detection and Recognition algorithm supported by Ontologies and applied to Museums, is an implementation to validate the proposal. Experiments show that the usage of ontologies is a good way of desambiguating the detection, obtained with a and <tex>$\mathbf{mAP}{@}0.5=0.88$</tex> and a <tex>$\mathbf{mAP}{@}[0.5:0.95]=61\%$</tex>.",https://ieeexplore.ieee.org/document/9639989/,2021 XLVII Latin American Computing Conference (CLEI),25-29 Oct. 2021,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/BigData52589.2021.9671426,Object Interaction Recommendation with Multi-Modal Attention-based Hierarchical Graph Neural Network,IEEE,Conferences,"Object interaction recommendation from Internet of Things (IoT) is a crucial basis for IoT related applications. While many efforts are devoted to suggesting object for interaction, the majority of models rigidly infer relationships from human social network, overlook the neighbor information in their own object social network and the correlation of multiple heterogeneous features, and ignore multi-scale structure of the network. To tackle the above challenges, this work focuses on object social network, formulates object interaction recommendation as multi-modals object ranking, and proposes Multi-Modal Attention-based Hierarchical Graph Neural Network (MM-AHGNN), that describes object with multiple knowledge of actions and pairwise interaction feature, encodes heterogeneous actions with multi-modal encoder, integrates neighbor information and fuses correlative multi-modal feature by intra-modal hybrid-attention graph convolution and inter-modal transformer encoder, and employs multi-modal multi-scale encoder to integrate multi-level information, for suggesting object interaction more flexibly. With extensive experiments on real-world datasets, we prove that MMAHGNN achieves better recommendation results (improve 3-4% HR@3 and 4-5% NDCG@3) than the most advanced baseline. To our knowledge, our MM-AHGNN is the first research in GNN design for object interaction recommendation. Source codes are available at: https://github.com/gaosaroma/MM-AHGNN.",https://ieeexplore.ieee.org/document/9671426/,2021 IEEE International Conference on Big Data (Big Data),15-18 Dec. 2021,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICTUS.2017.8285967,Ongoing research agenda on the Internet of Things (IoT) in the context of Artificial Intelligence (AI),IEEE,Conferences,"This talk presents our ongoing research agenda on the Internet of Things (IoT) in the context of Artificial Intelligence (AI). sThree initiatives define this agenda: integration of IoT into business process management, agentification of things, and mutation of things. IoT is among the latest ICT developments that is making the boundaries between reality and fiction vanish. According to Mark Weiser, “…The most profound technologies are those that disappear. They weave themselves into the fabric of everyday life until they are indistinguishable from it”i. And according to Gartnerii, 6.4 billion connected things were in use in 2016, up 3% from 2015, and will reach 20.8 billion by 2020. In the first initiative, we adopt storytelling principles to design and develop Process of Things (PoT). A PoT is specified as a story whose script indicates the characters that things will play as well as the scenes that will feature these things. A PoT also allows things to collaborate by offering value-added services to end-users. A system implementing PoT will be presented during the talk. In the second initiative, we shed the light on some obstacles that are slowing downIoTexpansion and adoption, for instance diversity of things' development technologies and communication standards, users' reluctance and sometimes rejection due to privacy invasion, lack of killer applications that would demonstrate their necessity, lack of an IoT-oriented software engineering discipline, and finally, the passive nature of things. Because of this nature, things are restricted to sending data to third parties or (basic) processing data prior to their transfer to third parties, too. We are examining how to empower things with additional capabilities that would make them proactive. This means that things can for instance, reach out to peers that expose collaborative attitude, form dynamic communities when necessary, avoid peers that expose malicious attitude, be accountable for their actions, etc. While we already see some encouraging signs of thing empowerment through initiatives like semantic things, Internet of social things, Internet of agents, and agents of things, we propose the agentification of things from a conceptual perspective exemplified with norms and an operational perspective exemplified with commitments. In conjunction with thing agentification, we present during the talk the third initiative that examines thing mutation in the sense that things will bind and/or unbind capabilities on the fly (and as they see fit). To ensure mutation success we consider first, the context in which things operate and second, policies that impact things' decisions to bind/unbind capabilities. We motivate mutation decisions with 3 factors: performance so that a thing remains competitive/attractive, adaptation so that a thing remains responsive, and survivability so that a thing remains in business.",https://ieeexplore.ieee.org/document/8285967/,2017 International Conference on Infocom Technologies and Unmanned Systems (Trends and Future Directions) (ICTUS),18-20 Dec. 2017,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AiDAS53897.2021.9574386,Online Shopping Preferences Visualization System Using Web Content Mining,IEEE,Conferences,"Recently, social media sites have been attracting people to start and operate online business. It has been gaining more attention especially during this COVID-19 pandemic, where most activities are conducted via online platform. For instance, Instagram is one of the social media platform where people share and post their pictures and videos on their account, but some people take this opportunity to promote their business and shop online via features provided by Instagram. Based on observation, there is no specific function that visualizes the information of Instagram's business accounts to help people in gaining information and making decisions during shopping process. Thus, a system that specifically visualizes Instagram's extracted data was developed using Web Content Mining technique with the assistance of hashtags as data-points and Phantombuster API. Tableau software integrated with JavaScript library is used as a visualization tool to display the users shopping activities. The result of the extracted data was visualized in the form of Bubble Chart, Bar Chart, and Multiple Bar Chart that were placed in different dashboards. The results from the obtained functional testing and users' feedbacks have indicated 64.3% of the respondents agreed that the proposed system helps users in making the right choice during online shopping. However, this system has some limitation that could be further enhanced such as updating the information automatically in real time without using any manpower and to increase the interactivity between system and user instead of using static visualization graphs.",https://ieeexplore.ieee.org/document/9574386/,2021 2nd International Conference on Artificial Intelligence and Data Sciences (AiDAS),8-9 Sept. 2021,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SSP.2016.7551730,Online low-rank subspace learning from incomplete data using rank revealing ℓ2/ℓ1 regularization,IEEE,Conferences,"Massive amounts of data (also called big data) generated by a wealth of sources such as social networks, satellite sensors etc., necessitate the deployment of efficient processing tools. In this context, online subspace learning algorithms that aim at retrieving low-rank representations of data constitute a mainstay in many applications. Working with incomplete (partially observed) data has recently become commonplace. Moreover, the knowledge of the real rank of the sought subspace is rarely at our disposal a priori. Herein, a novel low-rank subspace learning algorithm from incomplete data is presented. Its main premise is the online processing of incomplete data along with the imposition of low-rankness on the sought subspace via a sophisticated utilization of the group sparsity inducing ℓ2/ℓ1 norm. As is experimentally shown, the resulting scheme is efficient in accurately learning the subspace as well as in unveiling its real rank.",https://ieeexplore.ieee.org/document/7551730/,2016 IEEE Statistical Signal Processing Workshop (SSP),26-29 June 2016,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IMCCC.2015.348,Ontology Based Service Recommendation System for Social Network,IEEE,Conferences,"The development of recommendation systems, such as traditional content-based, collaborative filtering and hybrid recommendation approaches have enabled the practical use of big data processing in WEB 3.0. In this paper, we propose an ontology based service recommendation system for social network. In this paper, implementation methods of the system are explained in detail. In order to extract user interests more exactly, the TF-IDF (term frequency-inverse document frequency) algorithm is improved according to the features of Micro logs and integrated with the Text Rank algorithm. Also, we have improved the Hownet based semantic similarity algorithm with the consideration of the density of sememe tree. Experimental results show that recommendation results of our system can well reflect the real interests of users, and the improved algorithms can make the results more accurate.",https://ieeexplore.ieee.org/document/7406129/,"2015 Fifth International Conference on Instrumentation and Measurement, Computer, Communication and Control (IMCCC)",18-20 Sept. 2015,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/COMPSAC.2017.4,Ontology-Based Social Media Analysis for Urban Planning,IEEE,Conferences,"This paper reports a social media analysis study proposed by the Beijing Municipal Institute of Urban Planning and Design. The purpose is to explore techniques that can help the urban planning administrations to improve the social sensing and social perception abilities under the evolving data and technology environments. A framework integrating a comprehensive set of text mining algorithms is presented to conduct topic modeling, text clustering, event evolution detection, sentiment analysis, opinion mining, and information extraction on user-generated contents in Chinese social media. A domain ontology of Beijing urban planning is constructed to facilitate the text mining processes. Evaluations on two large, real-world datasets composed of microblogs and WeChat articles about the residential community and school education in Beijing demonstrate the effectiveness of our framework. The study illustrates the power of combining machine learning with knowledge-based, semantic approaches in analyzing social media for the domain of interest.",https://ieeexplore.ieee.org/document/8029714/,2017 IEEE 41st Annual Computer Software and Applications Conference (COMPSAC),4-8 July 2017,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISM52913.2021.00035,Open-Domain Trending Hashtag Recommendation for Videos,IEEE,Conferences,"We describe a novel algorithm for an open-domain trending hashtag recommendation task using zero-shot hashtag prediction in an online learning paradigm. Our method utilizes joint representation learning of latent embeddings for features extracted from long-form videos and semantic embeddings of hashtags trending on social platforms. In particular, we apply graph convolutional networks to a link prediction task using videos and hashtags as nodes in a heterogeneous graph. Comparing it to the existing models for closely related tasks, we demonstrate state-of-the-art results in trending hashtag recommendations for videos. The architecture is designed to be modular in a plug-and-play fashion to enable quick and easy incorporation of the latest advances in natural language understanding and image and video processing, with a practical view to its implementation in a real-time, online setting.",https://ieeexplore.ieee.org/document/9666058/,2021 IEEE International Symposium on Multimedia (ISM),29 Nov.-1 Dec. 2021,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCIS49240.2020.9257675,Optimal Short Term Power Load Forecasting Algorithm by Using Improved Artificial Intelligence Technique,IEEE,Conferences,"Electrical load forecasting plays a significant impact in terms of future power generation systems such as smart grid, power demand approximation, and better energy management system. Therefore, high accuracy is needed for different time horizons related to regulating, dispatch and scheduling of power system grid. However, it is difficult to do energy prediction with high precision because of influencing factors such as climate, social and seasonal factors. Artificial Intelligence (AI) and Support Vector Machine (SVM) are proved to be capable of handle complex systems and deployed worldwide in many applications due to its superiority on other techniques. The improved short term load forecasting algorithm has been introduced in this research to analyze, discuss and deal with the enhanced electrical power system. The related constraints, influential factors are given and the experimental results can be validated by the effective outcome.",https://ieeexplore.ieee.org/document/9257675/,2020 2nd International Conference on Computer and Information Sciences (ICCIS),13-15 Oct. 2020,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICAL.2009.5262851,Optimal control of continuous annealing process using PSO,IEEE,Conferences,"The continuous annealing furnace is one of the main equipments in the continuous annealing production line for iron and steel enterprise, and has a direct impact on the quality of cold-rolled strip steel, production and cost. The temperature control of continuous annealing furnace is a complex industrial process which is difficult to control. It is difficult to obtain a good control result by using the traditional control method, so a temperature control method with good performance is of great significance. Particle swarm optimization (PSO) has evolved recently as an important branch of stochastic techniques to explore the search space for optimization (Kennedy & Eberhart, 1995). The motivation for the development of this method is based on simulation of simplified social behavior such as bird flocking or fish schooling. Nowadays, PSO has been developed to be real competitor with other well-established techniques for population-based evolutionary computation. PSO has many advantages over other evolutionary computation techniques (for example, genetic algorithms (GAs)), such as simpler implementation, faster convergence rate and fewer parameters to adjust. The proposed scheme is applied to the optimal of the continuous annealing process. Simulation shows the proposed approach is effective.",https://ieeexplore.ieee.org/document/5262851/,2009 IEEE International Conference on Automation and Logistics,5-7 Aug. 2009,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ECDC.2014.6836746,Out-network traffic terms: A case study in LinkedIn 1,IEEE,Conferences,"Terms in the use of LinkedIn, as one of the important elements in this website will serve an important role in regulating the flow of traffic. The study of the integration of these terms can be a significant step to avoid congestion and (Users') rejection in the network. Integration of Traffic Control Terms (TCT) between LinkedIn and other social networks is one of the best solutions to establish consequent traffic flow out of the network. Depending on the conditions, integration can be done in different ways. This article examines the direct pass and indirect pass methods. Node phase differences are key elements for integration of LinkedIn considered in this review. To calculate the optimal phasing and appropriate phase difference, first the status of a user in LinkedIn has been analyzed using software, and then the results have been simulated by charting. In this article, the Integration of TCT was done based on evaluation of the TCT performance through determining the appropriate phase differences and reference rule and calculating the measurement of Latency (LT) in the main network port. This latency was calculated as the difference of the expected time and real time of connection or login to the network in simulated conditions. The achieved results indicate that in unsaturated conditions using indirect pass method and in saturated conditions, due to rejection of user and inability of network TCT, using indirect pass method is the best choice for integration of the TCT for LinkedIn and other social networks.",https://ieeexplore.ieee.org/document/6836746/,8th International Conference on e-Commerce in Developing Countries: With Focus on e-Trust,24-25 April 2014,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/DeSE51703.2020.9450783,Overcoming Speech Anxiety Using Virtual Reality with Voice and Heart Rate Analysis,IEEE,Conferences,"Social phobias are afflictions that millions of people suffer from. One common social phobia is speech anxiety (Glossophobia), which makes it difficult for people to talk in public or with others. To address this problem, this system was created to help people with Glossophobia practice making presentations or having personal interviews with less fear. The objective is to train them before their presentations or interviews by simulating 360° video environments with virtual reality (VR) technology. During the practice, the system analyzes the voice and heart rate of the person to discover any emotional and physical symptoms of speech anxiety using Arduino heart rate sensors, machine learning, and speech recognition techniques. The system will generate advice based on the symptoms to help make the user more confident. Additionally, after several training sessions, the system will present a report showing the progress in the user's performance. The system has been fully implemented and has demonstrated its operational effectiveness.",https://ieeexplore.ieee.org/document/9450783/,2020 13th International Conference on Developments in eSystems Engineering (DeSE),14-17 Dec. 2020,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.23919/ICMU48249.2019.9006662,PBEM: A Pattern-Based Embedding Model for User Location Category Prediction,IEEE,Conferences,"With the rapid popularity of mobile devices, a vast amount of trajectory-based check-in data are shared in many social network applications, which is an important data source for user location prediction. The location category prediction, a branch of location prediction, is a vital task in a wide range of areas, including urban planning, advertising and recommendation systems. In this paper, we propose a novel two-step Pattern-Based Embedding Model (PBEM) for predicting the next location category that user will go to. Based on the observation that some users behave frequently in a similarity pattern, a new feature termed as user cluster label is defined. In order to mine user's behavior patterns and extract the cluster label, a Category-Importance-Decay learning strategy is proposed and implemented, which provides a quantitative standard for evaluating the importance of each category. Thus, a comprehensive feature set is obtained including user, time, historical location category, text content, and user cluster label, which greatly enhances the robustness of data representation and contains more knowledge. Then the extracted feature set is fed into Recurrent Neural Network (RNN) in a unified framework, which improves the prediction accuracy. We evaluate the performance of PBEM on two real-life trajectory-based check-in datasets. Experimental results demonstrate that the proposed model can outperform the state-of-the-art methods.",https://ieeexplore.ieee.org/document/9006662/,2019 Twelfth International Conference on Mobile Computing and Ubiquitous Network (ICMU),4-6 Nov. 2019,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IPDPS47924.2020.00100,PCGCN: Partition-Centric Processing for Accelerating Graph Convolutional Network,IEEE,Conferences,"Inspired by the successes of convolutional neural networks (CNN) in computer vision, the convolutional operation has been moved beyond low-dimension grids (e.g., images) to high-dimensional graph-structured data (e.g., web graphs, social networks), leading to graph convolutional network (GCN). And GCN has been gaining popularity due to its success in real-world applications such as recommendation, natural language processing, etc. Because neural network and graph propagation have high computation complexity, GPUs have been introduced to both neural network training and graph processing. However, it is notoriously difficult to perform efficient GCN computing on data parallel hardware like GPU due to the sparsity and irregularity in graphs. In this paper, we present PCGCN, a novel and general method to accelerate GCN computing by taking advantage of the locality in graphs. We experimentally demonstrate that real-world graphs usually have the clustering property that can be used to enhance the data locality in GCN computing. Then, PCGCN proposes to partition the whole graph into chunks according to locality and process subgraphs with a dual-mode computing strategy which includes a selective and a full processing methods for sparse and dense subgraphs, respectively. Compared to existing state-of-the-art implementations of GCN on real-world and synthetic datasets, our implementation on top of TensorFlow achieves up to 8.8× speedup over the fastest one of the baselines.",https://ieeexplore.ieee.org/document/9139807/,2020 IEEE International Parallel and Distributed Processing Symposium (IPDPS),18-22 May 2020,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IDAACS53288.2021.9660949,PER-COVID (PEople pRoximity based on Certified and coOperative VIDeo-intelligence): A Software Tool for Physical Distancing and PPE Monitoring,IEEE,Conferences,"The recent COVID-19 pandemic has led to a growing interest in IT tools for monitoring social distance and for checking the presence of personal protective equipment and whether it is worn properly. Correct monitoring in outdoor and indoor areas is essential to limit the spread of the virus and the risk of being infected. This paper presents PER-COVID, a software platform capable of monitoring crowds of people and the correct use of personal protective equipment in real time using innovative computer vision algorithms. The proposed system architecture and functional characteristics are illustrated, as well as some user interface screens are provided for simple interpretation and monitoring of critical events.",https://ieeexplore.ieee.org/document/9660949/,2021 11th IEEE International Conference on Intelligent Data Acquisition and Advanced Computing Systems: Technology and Applications (IDAACS),22-25 Sept. 2021,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IPDPSW.2017.178,Parallel Computing for Machine Learning in Social Network Analysis,IEEE,Conferences,"Machine learning, especially deep learning, is revolutionizing how many engineering problems are being solved. Three critical ingredients are needed to apply deep machine learning to significant real world problems: i.) large data sets; ii.) software to implement deep learning and; iii.) significant computing cycles. This paper discusses the state of each ingredient with a specific focus on: a.) how deep learning can apply to large-scale social network analysis and; b.) the computing resources required to make such analyses feasible.",https://ieeexplore.ieee.org/document/7965209/,2017 IEEE International Parallel and Distributed Processing Symposium Workshops (IPDPSW),29 May-2 June 2017,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISPAN.2004.1300454,Particle swarm optimization algorithm in signal detection and blind extraction,IEEE,Conferences,"The particle swarm optimization (PSO) algorithm, which originated as a simulation of a simplified social system, is an evolutionary computation technique. In this paper the binary and real-valued versions of PSO algorithm are exploited in two important signal processing paradigm: multiuser detection (MUD) and blind extraction of sources (BES), respectively. The novel approaches are effective and efficient with parallel processing structure and relatively feasible implementation. Simulation results validate either PSO-MUD or PSO-BES has a significant performance improvement over conventional methods.",https://ieeexplore.ieee.org/document/1300454/,"7th International Symposium on Parallel Architectures, Algorithms and Networks, 2004. Proceedings.",10-12 May 2004,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CCIS.2016.7790307,Passengers' choices on airport drop-off service: A decision forecast based on social learning and machine learning,IEEE,Conferences,"Airport drop-off service provided by airlines is a chauffeur-driven service (i.e. Uber and DiDi) as an emerging travel choice for travelers. More and more passenger enjoy the drop-off service. In practice, we find an interesting question: if a passenger has ever choice the drop-off service, whether they are willing to recommend this service to other traveler? Although the acknowledgment that social learning is related to travel decision is promoted, quantitative analysis about how social learning shape and impact the decision of passengers is still limited. We study and estimate a diffusion probability between different passengers by proposing a CCM (Co-travel Link Cascade Model) based on a modified EM iterative algorithm. Then, we segment passengers into three types (Influenced, Unchecked and Immune). The three types of passengers are predicated by approaches of IC-like model, Random Forest model and probabilistic model, respectively. In addition, we also design a parallel implementation of our proposed algorithm in the Apache Spark distributed data processing environment. Experimental results on a real aviation data set demonstrate that CCM can efficiently infer the decision of travelers.",https://ieeexplore.ieee.org/document/7790307/,2016 4th International Conference on Cloud Computing and Intelligence Systems (CCIS),17-19 Aug. 2016,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCWorkshops50388.2021.9473673,Passive WiFi CSI Sensing Based Machine Learning Framework for COVID-Safe Occupancy Monitoring,IEEE,Conferences,"The COVID-19 pandemic requires social distancing to prevent transmission of the virus. Monitoring social distancing is difficult and expensive, especially in ""travel corridors"" such as elevators and commercial spaces. This paper describes a low-cost and non-intrusive method to monitor social distancing within a given space, using Channel State Information (CSI) from passive WiFi sensing. By exploiting the frequency selective behaviour of CSI with a cubic SVM classifier, we count the number of people in an elevator with an accuracy of 92%, and count the occupancy of an office to 97%. As opposed to using a multi-class counting approach, this paper aggregates CSI for the occupancies below and above a COVID-Safe limit. We show that this binary classification approach to the COVID safe decision problem has similar or better accuracy outcomes with much lower computational complexity, allowing for real-world implementation on IoT embedded devices. Robustness and scalability is demonstrated through experimental validation in practical scenarios with varying occupants, different environment settings and interference from other WiFi devices.",https://ieeexplore.ieee.org/document/9473673/,2021 IEEE International Conference on Communications Workshops (ICC Workshops),14-23 June 2021,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICDMW51313.2020.00067,Pelican: Continual Adaptation for Phishing Detection,IEEE,Conferences,"An increasing number of people are using social media services and with it comes a more attractive outlet for phishing attacks. Our initial focus is to analyze Twitter as it is one of the most popular social media services. Phishers on Twitter curate tweets that lead users to websites that download malware. This is a major issue as phishers can then gain access to the user's digital identity and perform malicious acts. Phishing attacks have the potential to be similar in different regions, perhaps at different times. We have developed a novel semi-supervised machine learning algorithm, which we call Pelican, that detects potential phishing attacks in real-time on Twitter. Pelican can be used for early detection of potential phishing attacks and is able to detect potential new attacks without pre-existing assumptions about the type of data or understanding of the characteristics of the attacks. The technique uses ensembles and sampling methods to handle class imbalances in real-world applications. The technique continuously detects unusual behaviour or changes in Twitter. We have investigated changes in trends across Twitter to detect changes in online behaviour of potential phishing links. The technique uses a change detector that enables automatic retraining when there is unusual behaviour detected. Pelican is a novel technique that adapts to changes within phishing attacks in real-time. The technique detects 93.94% of the phishing tweets in real-world data that we collected over a 9 month period, which is higher than benchmark algorithms.",https://ieeexplore.ieee.org/document/9346336/,2020 International Conference on Data Mining Workshops (ICDMW),17-20 Nov. 2020,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/iV.2015.88,Perceived Realism of Crowd Behaviour with Social Forces,IEEE,Conferences,"This paper investigates the development of an urban crowd simulation for the purposes of psychophysical experimentation. Whilst artificial intelligence (AI) is advancing to produce more concise and interesting crowd behaviours, the number or sophistication of the algorithms implemented within a system does not necessarily guarantee its perceptual realism. Human perception is highly subjective and does not always conform to the reality of the situation. Therefore it is important to consider this aspect when dealing with A implementations within a crowd system aimed at humans. In this research an initial two-alternative forced choice (2AFC) with constant stimuli psychophysical experiment is presented. The purpose of the experiment is to assess whether human participants perceive crowd behaviour with a social forces model to be more realistic. Results from the experiment suggest that participants do consider crowd behaviour with social forces to be more realistic. This research could inform the development of crowd-based systems, especially those that consider viewer perception to be important, such as for example video games and other media.",https://ieeexplore.ieee.org/document/7272647/,2015 19th International Conference on Information Visualisation,22-24 July 2015,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/WiMob52687.2021.9606263,Performance Modelling and Assessment for Social VR Conference Applications in 5G Radio Networks,IEEE,Conferences,"One of the most challenging applications targeted by evolving (beyond-)5G technology is virtual reality (VR). Particularly, 'Social VR' applications provide a fully immersive experience and sense of togetherness to users residing at different locations. To support such applications the network must deal with huge traffic demands, while keeping end-to-end latencies low. Moreover, the radio access network must deal with the volatility and vulnerability of mmWave radio channels, where even small movements of the users may have substantial effects on the Quality of Experience. We present an integral modelling framework for feasibility assessment and performance optimization of the radio access network for Social VR applications in indoor office scenarios. Using the presented modelling approach, we conduct an extensive simulation-based assessment to determine the performance impact of head motion, the frequency band (3.5 GHz, 26 GHz) and radio network configurations, and derive the required carrier bandwidth for a range of 'Social VR' scenarios. Insights into these issues are a prerequisite for setting up guidelines for network deployment and configuration as well as for the development of (AI/ML-based) methods for dynamic resource management to optimally support Social VR applications.",https://ieeexplore.ieee.org/document/9606263/,"2021 17th International Conference on Wireless and Mobile Computing, Networking and Communications (WiMob)",11-13 Oct. 2021,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/eCrime.2012.6489521,PhishAri: Automatic realtime phishing detection on twitter,IEEE,Conferences,"With the advent of online social media, phishers have started using social networks like Twitter, Facebook, and Foursquare to spread phishing scams. Twitter is an immensely popular micro-blogging network where people post short messages of 140 characters called tweets. It has over 100 million active users who post about 200 million tweets everyday. Phishers have started using Twitter as a medium to spread phishing because of this vast information dissemination. Further, it is difficult to detect phishing on Twitter unlike emails because of the quick spread of phishing links in the network, short size of the content, and use of URL obfuscation to shorten the URL. Our technique, PhishAri, detects phishing on Twitter in realtime. We use Twitter specific features along with URL features to detect whether a tweet posted with a URL is phishing or not. Some of the Twitter specific features we use are tweet content and its characteristics like length, hashtags, and mentions. Other Twitter features used are the characteristics of the Twitter user posting the tweet such as age of the account, number of tweets, and the follower-followee ratio. These twitter specific features coupled with URL based features prove to be a strong mechanism to detect phishing tweets. We use machine learning classification techniques and detect phishing tweets with an accuracy of 92.52%. We have deployed our system for end-users by providing an easy to use Chrome browser extension. The extension works in realtime and classifies a tweet as phishing or safe. In this research, we show that we are able to detect phishing tweets at zero hour with high accuracy which is much faster than public blacklists and as well as Twitter's own defense mechanism to detect malicious content. We also performed a quick user evaluation of PhishAri in a laboratory study to evaluate the usability and effectiveness of PhishAri and showed that users like and find it convenient to use PhishAri in real-world. To the best of our knowledge, this is the first realtime, comprehensive and usable system to detect phishing on Twitter.",https://ieeexplore.ieee.org/document/6489521/,2012 eCrime Researchers Summit,23-24 Oct. 2012,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IPEC51340.2021.9421213,Pneumatic Training Glove Based on Electromyographic Signal Control,IEEE,Conferences,"In modern society, much public attention is paid to the rights and interests of the disabled, which become a social problem. In order to satisfy the pursuit of disabled for a better quality of life, it is necessary to give them a normal life. For example, people with degeneration or atrophy of hand muscles caused by accidents or other reasons need an auxiliary system to help them recover their hand muscles. However, after consulting a large amount of data, I found that most hospitals in the market use hard levers and springs as mechanical structures, which will result in excessive resistance, poor finger curls, and even aggravate the disease due to improper use. In contrast, the deformable structure is softer. Meanwhile, it can actively bend the fingers of patient through the inflation control method to achieve the purpose of safety and effectiveness. In this paper, the design and casting of pneumatic muscle are completed through independent modeling and pouring in the experiment [1]. At the same time, this paper also provides and tests two machine learning pneumatic muscle control algorithms based on real-time binning, which are idempotent and real-time, and can be fully controlled in combination with the designed pneumatic muscle. Its core signal comes from EMG signals on key muscle groups of the arm. The signal is collected from muscle in real time by wet electrode patch. For the control model of machine learning, we have tested and implemented a variety of gesture postures, including common palm curling and stretching, common finger movements, etc. The pneumatic muscle hand rehabilitation auxiliary glove based on electromyographic signal control has the characteristics such as passive exercise, safety, less pain, individuation, low price, full and effective exercise, etc. The pneumatic muscle-assisted rehabilitation system based on electrical signal control in this paper has broad application prospects and extremely high value.",https://ieeexplore.ieee.org/document/9421213/,"2021 IEEE Asia-Pacific Conference on Image Processing, Electronics and Computers (IPEC)",14-16 April 2021,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AXMEDIS.2007.19,Poolcasting: A Social Web Radio Architecture for Group Customisation,IEEE,Conferences,"Poolcasting is a social Web radio architecture in which groups of listeners influence in real time the music played on each channel. Poolcasting users contribute to the radio with songs they own, create radio channels and evaluate the proposed music, while an automatic intelligent technique schedules each channel with a group-customised sequence of musically associated songs. The benefits of this approach are multiple: on one hand music producers can increase the exposure of their songs to specific target audiences; on the other hand, music consumers can easily discover new songs that match their preferences and group with people with whom they share similar listening experiences.",https://ieeexplore.ieee.org/document/4402867/,Third International Conference on Automated Production of Cross Media Content for Multi-Channel Distribution (AXMEDIS'07),28-30 Nov. 2007,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/PuneCon52575.2021.9686508,Post Cyclone Damage Assessment Using CNN Based Transfer Learning and Grad-CAM,IEEE,Conferences,"Evaluating the aftermath of a cyclone is an expensive, arduous and time-consuming process which delays rehabilitation of affected victims. As multiple requests for grant aids pour in, it is necessary to cross-check the reality of the damage. At present, this investigation is mostly done by the concerned authorities manually. Now-a-days, social media can be considered as a repository for a plethora of post-disaster images. This pool of images may be used as a database to extract features to draw conclusions on the severity of cyclones. Since, tropical cyclones are frequently occurring in the Indian subcontinent, our research can be pivotal in curtailing the efforts of disaster management. The proposed CNN based architecture aims to detect whether an area has been damaged by a cyclone or not, and also the adversity of the same. Transfer learning has been deployed to fine tune the models, VGG-16 and VGG-19 to perform the task. This damage assessment system categorizes the images depicting distinct features like trees, posts, damaged property which have been explicitly cross-validated with Grad-CAM. The proposed model exhibits high accuracy in classifying post catastrophic destruction in a proficient and cost-effective manner.",https://ieeexplore.ieee.org/document/9686508/,2021 IEEE Pune Section International Conference (PuneCon),16-19 Dec. 2021,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/TENCON.2019.8929317,Practical Significance of GA PartCC in Multi-Label Classification,IEEE,Conferences,"Multi-label classification (MLC) can be defined as the objective of learning a classification model which has the capability to infer the accurate labels of new, previously unseen, objects where it is a likely situation that each object of the dataset may rightfully belong to multiple class labels. While single-label classification problems have been thoroughly researched, the same cannot be said for MLC. A gradually increasing number of problems are now being tackled as multi-label, allowing for richer and more accurate knowledge mining in real-world domains, such as medical diagnoses, social media, text classification, etc. Currently, there are two ways of solving MLC problems; Problem Transformation Approach and Algorithm Adaptation Method. Of the two, the former has in its domain Classifier Chains (CC) which is the most effective and popular method of solving MLC problems because of its simplicity in implementation. Unfortunately, CC is not favoured due to 2 drawbacks, [1] ordering of the labels for classification are randomly decided without a fixed logic or algorithm to it which results in varying accuracy, [2] all the labels, even those which may be redundant for a particular dataset are put into the chain despite the probability that some may be carrying irrelevant details. Through the research conducted for the purpose of this study, both challenges are tackled along with others detailed further on simultaneously using Genetic Algorithms (GA) over a Partial CC (PartCC) model, which is a modification over CC. A toxic comments dataset is used since its classification is a multi-label text classification problem with a highly imbalanced dataset. This paper aims to create a prototype model that is capable of detecting various types of toxicity like neutral, toxic, severe toxic, threats, obscenity, insults and identity hate. With the explosion of social media in the modern world and the resulting increasing phenomenon of social media hatred and bullying, there is a need for an advanced prototype model to predict the toxicity of each class of comments.",https://ieeexplore.ieee.org/document/8929317/,TENCON 2019 - 2019 IEEE Region 10 Conference (TENCON),17-20 Oct. 2019,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCICC46617.2019.9146087,Predicting Link Sign in Online Social Networks based on Social Psychology Theory and Machine Learning Techniques,IEEE,Conferences,"Online social networks provide a great platform for internet users to share their views and ideas. Social media provides a dynamic platform that includes the formation and deformation of connections. Two types of connections, i.e., Positive and Negative, can exist in a social network. Positive connections are a sign of friendship or trust, while negative connections show enmity or distrust. Various applications in several fields have networks containing both positive and negative edges. Reliable prediction of edge sign can greatly influence in recommending friendly relationships while preventing enemy relationships across the network. Prediction of edge signs has been explored previously also. However, we intend to predict the sign of edges based on extracted features of nodes constructed upon theories of social psychology that includes classical balance theory and the status theory. Moreover, we employ emotional information theory and use the combined extracted features from all the theories to analyze networks for better prediction. Our results show that the proposed methodology has obtained significant accuracy when implemented on two real-life datasets, namely Slashdot and Epinions.",https://ieeexplore.ieee.org/document/9146087/,2019 IEEE 18th International Conference on Cognitive Informatics & Cognitive Computing (ICCI*CC),23-25 July 2019,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICDMW.2016.0026,Predicting User Roles in Social Networks Using Transfer Learning with Feature Transformation,IEEE,Conferences,"How can we recognise social roles of people, given a completely unlabelled social network? We may train a role classification algorithm on another dataset, but then that dataset may have largely different values of its features, for instance, the degrees in the other network may be distributed in a completely different way than in the first network. Thus, a way to transfer the features of different networks to each other or to a common feature space is needed. This type of setting is called transfer learning. In this paper, we present a transfer learning approach to network role classification based on feature transformations from each network's local feature distribution to a global feature space. We implement our approach and show experiments on real-world networks of discussions on Wikipedia as well as online forums. We also show a concrete application of our approach to an enterprise use case, where we predict the user roles in ARIS Community, the online platform for customers of Software AG, the second-largest German software vendor. Evaluation results show that our approach is suitable for transferring knowledge of user roles across networks.",https://ieeexplore.ieee.org/document/7836657/,2016 IEEE 16th International Conference on Data Mining Workshops (ICDMW),12-15 Dec. 2016,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICMTMA54903.2022.00163,Prediction Method of Pollutant Concentration Based on GA-ELM Model,IEEE,Conferences,"With the rapid development of social economy, air pollution has attracted more and more attention. In order to effectively predict the situation of pollutants and improve pollution prevention and control, this paper presents a prediction model of limit learning machine optimized by genetic algorithm (GA-ELM). Collect the data of an air quality monitoring point, conduct simulation experiments with MATLAB software, finally analyze the prediction results of pollutant concentration, and compare them with the prediction results of WRF-CMAQ system. The results show that GA- ELM model can predict the pollutant concentration more accurately, the fitting degree between the prediction curve and the real value curve is high, and the relative error fluctuates up and down in the range of -0.2~0.2, which further shows that the model has good robustness.",https://ieeexplore.ieee.org/document/9724075/,2022 14th International Conference on Measuring Technology and Mechatronics Automation (ICMTMA),15-16 Jan. 2022,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ElConRus51938.2021.9396292,Prediction of Relation Based on Social Network Parsing,IEEE,Conferences,"The problem of searching a potential partner among the particular groups in dating applications is studied in this article. Moreover, the assessment of the current methods is carried out and the ways to improve their efficiency are presented. To begin with, there are two main disadvantages of the dating applications nowadays. The first one is imperfect algorithms of selection and the second one is the lack of ""human touch"". It means, that people with common friends in real life will communicate better in the Internet. Due to that, the major features of the research are searching among the common social circle and processing the data by parsing the social network of the potential partner. As a result, the program compares the personal information of every user and makes the pairs with more similar data. All information is processed by Linear neural network which was trained by results of social surveys .The search algorithm is implemented using specialized libraries for Python 3.8 and public methods of VK.API. Also, there are data and findings in the end of this article.",https://ieeexplore.ieee.org/document/9396292/,2021 IEEE Conference of Russian Young Researchers in Electrical and Electronic Engineering (ElConRus),26-29 Jan. 2021,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICBASE51474.2020.00055,Prediction of Trust Propagation in Social Commerce based on Ensemble Learning,IEEE,Conferences,"Accurate prediction of trust propagation in social commerce is vital to recommendation and promotion of commodities. Existing prediction models have some shortages, such as simple process of influencing factors of trust and low prediction precision. To address these problems, a prediction model based on Soft-Voting ensemble learning was proposed. Firstly, features of influencing factors of information propagation in social commerce were constructed from user attributes, information text and user interaction. Secondly, XGBoost, LightGBM and Catboost models were trained according to the above constructed features to predict trust propagation in social commerce. Finally, results of XGBoost, LightGBM and Catboost models were integrated using Soft-Voting technique as the final prediction results. An experiment on a real dataset of Sina Weibo was carried out, which proved the higher precision of Soft-Voting ensemble learning compared to those of XGBoost, LightGBM and Catboost models.",https://ieeexplore.ieee.org/document/9403760/,2020 International Conference on Big Data & Artificial Intelligence & Software Engineering (ICBASE),30 Oct.-1 Nov. 2020,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AICAI.2019.8701417,Predictive Framework to Measure Mental Distress Caused by Economic Crises,IEEE,Conferences,"Over the years, research has continuously emphasized that economic calamities are negatively correlated with the mental stability of public. Psychologists and sociologists continue to probe downturns influence of economic crisis on mental health and propose social protection programs to promote recovery. The problem, however, is that more than having a social recovery program, it requires personal level efforts to realize and understand what may be coming ahead and rationally solving it, by being more in control of the situation. Considering this context, there are various aims of this paper to evaluate theoretical as well as conceptual illustrations on the reasons behind the negative associations of an economic downturn on population's mental stability, followed by closely examining critical mental distress factors as well as economic distress factors and establishing connection through literature analysis. The study proposes a framework to intelligently detect economic distress in real time and successfully link it with human distress factors to predict any anticipated alleviation in stress level. The proposed framework is based on cloud architecture and uses software application that will perform intelligent data mining for knowledge acquisition purpose.",https://ieeexplore.ieee.org/document/8701417/,2019 Amity International Conference on Artificial Intelligence (AICAI),4-6 Feb. 2019,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICTAI.2016.0020,Preference-Like Score to Cope with Cold-Start User in Recommender Systems,IEEE,Conferences,"In recent years, there has been an explosion of social recommender systems (SRS) research. However, the dominant trend of these studies has been towards designing new prediction models. The typical approach is to use social information to build those models for each new user. Due to the inherent complexity of this prediction process, for full cold-start user in particular, the performance of most SRS fall a great deal. We, rather, propose that new users are best served by models already built in system. Selecting a prediction model from a set of strong linked users might offer better results than building a personalized model for full cold-start users. We contribute to this line of work comparing several matrix factorization based SRS under full cold-start user scenario, and proposing a general model selection approach, called ToSocialRec, that leverages existing recommendation models to offer items for new users. Our framework is not only able to handle several social network connection weight metrics, but any metric that can be correlated with preference similarity among users, named here as Preference-like score. We perform experiments on real life datasets that show this technique is as efficient or more than current state-of-the-art techniques for cold-start user. Our framework has also been designed to be easily deployed and leveraged by developers to help create a new wave of SRS.",https://ieeexplore.ieee.org/document/7814580/,2016 IEEE 28th International Conference on Tools with Artificial Intelligence (ICTAI),6-8 Nov. 2016,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICETCI51973.2021.9574068,Prevention of Emotional Entrapment of Children on Social Media,IEEE,Conferences,"Increasing usage of social media and its widespread reaches across the younger as well as older demographics of the population, along with the added advantage of anonymity, has propelled the pressing need to ensure that these platforms remain a secure place for children so as to safeguard them from issues like child grooming. Over the last few years, the rate of online grooming has seen a massive increase along with an increase in the number of teenage users. The proposed system, Prevention of Emotional Entrapment of Children (PEEC), consists of an age detection module using Deep Neural Network (DNN) as well as a grooming detection module using Support Vector Machine (SVM) to monitor the conversations of only those users who come under the category of minor (below 18). The resultant system gave an accuracy of 84% for age detection and 91% for grooming detection.",https://ieeexplore.ieee.org/document/9574068/,2021 International Conference on Emerging Techniques in Computational Intelligence (ICETCI),25-27 Aug. 2021,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/RO-MAN46459.2019.8956461,Privacy First: Designing Responsible and Inclusive Social Robot Applications for in the Wild Studies,IEEE,Conferences,"Deploying social robots applications in public spaces for conducting in the wild studies is a significant challenge but critical to the advancement of social robotics. Real world environments are complex, dynamic, and uncertain. Human-Robot interactions can be unstructured and unanticipated. In addition, when the robot is intended to be a shared public resource, management issues such as user access and user privacy arise, leading to design choices that can impact on users' trust and the adoption of the designed system. In this paper we propose a user registration and login system for a social robot and report on people's preferences when registering their personal details with the robot to access services. This study is the first iteration of a larger body of work investigating potential use cases for the Pepper social robot at a government managed centre for startups and innovation. We prototyped and deployed a system for user registration with the robot, which gives users control over registering and accessing services with either face recognition technology or a QR code. The QR code played a critical role in increasing the number of users adopting the technology. We discuss the need to develop social robot applications that responsibly adhere to privacy principles, are inclusive, and cater for a broad spectrum of people.",https://ieeexplore.ieee.org/document/8956461/,2019 28th IEEE International Conference on Robot and Human Interactive Communication (RO-MAN),14-18 Oct. 2019,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ARITH.2019.00047,Privacy-Preserving Deep Learning via Additively Homomorphic Encryption,IEEE,Conferences,"We aim at creating a society where we can resolve various social challenges by incorporating the innovations of the fourth industrial revolution (e.g. IoT, big data, AI, robot, and the sharing economy) into every industry and social life. By doing so the society of the future will be one in which new values and services are created continuously, making people's lives more conformable and sustainable. This is Society 5.0, a super-smart society. Security and privacy are key issues to be addressed to realize Society 5.0. Privacy-preserving data analytics will play an important role. In this talk we show our recent works on privacy-preserving data analytics such as privacy-preserving logistic regression and privacy-preserving deep learning. Finally, we show our ongoing research project under JST CREST “AI”. In this project we are developing privacy-preserving financial data analytics systems that can detect fraud with high security and accuracy. To validate the systems, we will perform demonstration tests with several financial institutions and solve the problems necessary for their implementation in the real world.",https://ieeexplore.ieee.org/document/8877418/,2019 IEEE 26th Symposium on Computer Arithmetic (ARITH),10-12 June 2019,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CASA.2003.1199296,Proceedings 16th International Conference on Computer Animation and Social Agents,IEEE,Conferences,"The following topics are dealt with: social and conversational agents; physics-based animation; interaction, control and planning; AI-based animation and A-life; modeling of groups and crowds; virtual humans: past, present and future; and design issues in games and virtual environments.",https://ieeexplore.ieee.org/document/1199296/,Proceedings 11th IEEE International Workshop on Program Comprehension,8-9 May 2003,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ASONAM.2018.8508242,ProfileGen: Generation of Automatic and Realistic Artificial Profiles,IEEE,Conferences,"One of the most effective approaches for detecting malicious activity in online social networks (OSNs) involves the use of social network honeypots - artificial profiles. Therefore, there is a growing need for the ability to reliably generate realistic artificial honeypot profiles in OSNs. In this research we present `ProfileGen' - a method for the automated generation of profiles for professional social networks, giving particular attention to producing realistic education and employment records. `ProfileGen' creates honeypot profiles that are similar to actual data by extrapolating the characteristics and properties of real data items. Evaluation by 70 domain experts confirms the method's ability to generate realistic artificial profiles that are indistinguishable from real profiles, demonstrating that our method can be applied to generate realistic artificial profiles for a wide range of applications.",https://ieeexplore.ieee.org/document/8508242/,2018 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining (ASONAM),28-31 Aug. 2018,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/MASS.2018.00019,Q-Learning Based Edge Caching Optimization for D2D Enabled Hierarchical Wireless Networks,IEEE,Conferences,"Caching at the edge of mobile networks can significantly offload network traffic while satisfying content requests from mobile users locally. The contents can be requested from the proximity users via Device-to-device (D2D) communications while proactive caching the popular content to local users. However, the assumptions that content popularity is equal to user preference in several existing studies, which are invalid and not rigorous due to the fact that content popularity is calculated by the statistic of user requests within a certain period while user preference reflects the probability of a content requested by the individual user. Motivated by this, in this paper, we study the edge caching optimization of hierarchical wireless networks. Our aiming is to maximize the size of content offload by D2D communications. In particular, the edge caching policy with D2D sharing model based on the analysis of user mobility and social relationship is derived. We first prove the problem is NP-hard and then formulate it as a Markov Decision Process (MDP) problem, finally a Q-learning based distributed content replacement strategy is proposed. The large-scale real trace based experiment results show the effectiveness of our proposed framework.",https://ieeexplore.ieee.org/document/8567541/,2018 IEEE 15th International Conference on Mobile Ad Hoc and Sensor Systems (MASS),9-12 Oct. 2018,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IntelCIS.2015.7397269,Ranking of news items in rule-stringent social media based on users' importance: A social computing approach,IEEE,Conferences,"In this paper an innovative social media news items ranking scheme is proposed. The proposed unsupervised architecture takes into consideration user-content interactions, since social media posts receive likes, comments and shares from friends and other users. Additionally the importance of each user is modeled, based on an innovative algorithm that borrows ideas from the PageRank algorithm. Finally, a novel content ranking component is introduced, which ranks posted news items based on a social computing method, driven by the importance of the social network users that interact with them. Initial experiments on real life social networks news items illustrate the promising performance of the proposed architecture. Additionally comparisons with three different ranking ways are provided (SUMF, RSN-CO and RSN-nCO), in terms of user satisfaction.",https://ieeexplore.ieee.org/document/7397269/,2015 IEEE Seventh International Conference on Intelligent Computing and Information Systems (ICICIS),12-14 Dec. 2015,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA.2017.7989184,Rapidly exploring learning trees,IEEE,Conferences,"Inverse Reinforcement Learning (IRL) for path planning enables robots to learn cost functions for difficult tasks from demonstration, instead of hard-coding them. However, IRL methods face practical limitations that stem from the need to repeat costly planning procedures. In this paper, we propose Rapidly Exploring Learning Trees (RLT*), which learns the cost functions of Optimal Rapidly Exploring Random Trees (RRT*) from demonstration, thereby making inverse learning methods applicable to more complex tasks. Our approach extends Maximum Margin Planning to work with RRT* cost functions. Furthermore, we propose a caching scheme that greatly reduces the computational cost of this approach. Experimental results on simulated and real-robot data from a social navigation scenario show that RLT* achieves better performance at lower computational cost than existing methods. We also successfully deploy control policies learned with RLT* on a real telepresence robot.",https://ieeexplore.ieee.org/document/7989184/,2017 IEEE International Conference on Robotics and Automation (ICRA),29 May-3 June 2017,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/I-SMAC.2017.8058261,Real world smart chatbot for customer care using a software as a service (SaaS) architecture,IEEE,Conferences,"It's being very important to listen to social media streams whether it's Twitter, Facebook, Messenger, LinkedIn, email or even company own application. As many customers may be using this streams to reach out to company because they need help. The company have setup social marketing team to monitor this stream. But due to huge volumes of users it's very difficult to analyses each and every social message and take a relevant action to solve users grievances, which lead to many unsatisfied customers or may even lose a customer. This papers proposes a system architecture which will try to overcome the above shortcoming by analyzing messages of each ejabberd users to check whether it's actionable or not. If it's actionable then an automated Chatbot will initiates conversation with that user and help the user to resolve the issue by providing a human way interactions using LUIS and cognitive services. To provide a highly robust, scalable and extensible architecture, this system is implemented on AWS public cloud.",https://ieeexplore.ieee.org/document/8058261/,"2017 International Conference on I-SMAC (IoT in Social, Mobile, Analytics and Cloud) (I-SMAC)",10-11 Feb. 2017,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/UrgentHPC54802.2021.00009,Real-Time COVID-19 Infection Risk Assessment and Mitigation based on Public-Domain Data,IEEE,Conferences,"A number of models have been developed to predict the spreads of the COVID-19 pandemic and how non-pharmaceutical interventions (NPIs) such as social distancing, facial coverings, and business and school closures can contain this pandemic. Evolutionary artificial intelligence (AI) approaches have recently been proposed to automatically determine the most effective interventions by generating a large number of candidate strategies customized for different countries and locales and evaluating them with predictive models. These epidemiological models and advanced AI techniques assist policy makers by providing them with strategies in balancing the need to contain the pandemic and the need to minimize their economic impact as well as educating the general public about ways to reduce the chance of infection. However, they do not advise an individual citizen at a specific moment and location on taking the best course of actions to accomplish a task such as grocery shopping while minimizing infection.Therefore, this paper describes a new project aiming to develop a mobile-phone-deployable, real-time COVID-19 infection risk assessment and mitigation (RT-CIRAM) system which analyzes up-to-date data from multiple open sources leveraging urgent HPC/cloud computing, coupled with time-critical scheduling and routing techniques. Implementation of a RT-CIRAM prototype is underway, and it will be made available to the public. Facing the increasing spread of the more contagious Delta (B.1.617.2) and Delta Plus (AY.4.2) variants, this personal system will be especially useful for individual citizen to reduce her/his infection risk despite increasing vaccination rates while contributing to containing the spread of the current and future pandemics.",https://ieeexplore.ieee.org/document/9651302/,2021 IEEE/ACM HPC for Urgent Decision Making (UrgentHPC),19-19 Nov. 2021,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CISAI54367.2021.00190,Real-Time Facial Expression Driving based on 3D Facial Feature Point,IEEE,Conferences,"Facial expression driving can make the expression of virtual characters more real and natural. It is widely used in movies, games and some social software. The existing real-time facial expression driving algorithms have the limitation of physical hardware, prefabricated model or long-time training. In order to get rid of the limitation of existing algorithms, this paper proposes a real-time expression driving algorithm based on 3D facial feature points with RGBD data as input. In the face capture, we use ICP algorithm to get the rigid data of the face and deformation transmission algorithm to capture the non-rigid data of the face. Moreover, the whole face process only takes 0.4ms to complete. Because the calculation of the algorithm is based on 3D feature points of the face, there is no need to prefabricate a specific face model and a lot of time training. Our algorithm can not only use real faces to drive virtual faces, but also use virtual characters to drive virtual faces.",https://ieeexplore.ieee.org/document/9718961/,2021 International Conference on Computer Information Science and Artificial Intelligence (CISAI),17-19 Sept. 2021,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ic-ETITE47903.2020.394,Real-time Acoustic based Depression Detection using Machine Learning Techniques,IEEE,Conferences,"Depression disorder is predicted to rise to the second leading cause of disability by 2030 as per the identifications of the World Health organization (WHO). Though well trained clinicians, medical and psychological treatments are available for depression treatment, persons or families are reluctant to speak out/reach doctors about this disorder for various social reasons. Diagnosis of depression disorder includes numerous interviews with patient and family, clinical analysis, questionnaires which is time consuming and also demands well trained clinicians. In the present era of Machine learning, automation of depression detection is not complicated and can easily be deployed. However, the automation should use fewer resources, provide accurate results with more reachability. In this paper, acoustic features are used to train a classification model to categorize a human as Depressed or not-Depressed. DIAC-WOZ database available with AVEC2016 challenge is considered for training the classifiers. Prosodic, Spectral and Voice control features are extracted using the COVAREP toolbox and are feature fused. SMOTE analysis is used for overcoming the class imbalance and 93% accuracy is obtained with the SVM algorithm resulting in Depression Classification Model (DCM). An android application cureD Deployed on Cloud is developed to self assess depression using DCM and PHQ-8 questionnaire. The application is tested on real time data of 50 subjects under the supervision of a qualified psychiatrist and an accuracy of 90% is obtained.",https://ieeexplore.ieee.org/document/9077698/,2020 International Conference on Emerging Trends in Information Technology and Engineering (ic-ETITE),24-25 Feb. 2020,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICNSC.2019.8743331,Real-time Sentiment Analysis On E-Commerce Application,IEEE,Conferences,"Opinion mining is one of the most important tasks of natural language processing, which is also known as sentiment analysis, used to identify about what people have an impression about their services and products in social media platforms. To improve marketing strategies using product reviews, an effective method should be used for predicting the sentiment polarity. In this research article, a Machine learning technique called Support Vector Machine (SVM) is used to design a model and this model has been implemented on an E-commerce application. The data used in this study are online product reviews which are collected from Amazon.com. The experiments of sentiment analysis are performed for two levels of categorization: review level and sentence level. The main focus of this paper is to present a real-time sentimental analysis on the product reviews of e-commerce application so that the user experience can be enhanced.",https://ieeexplore.ieee.org/document/8743331/,"2019 IEEE 16th International Conference on Networking, Sensing and Control (ICNSC)",9-11 May 2019,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICITE54466.2022.9759552,Real-time Social Distance Detection using YOLO-v5 with Bird-eye View Perspective to Suppress the Spread of COVID-19,IEEE,Conferences,"The COVID-19 virus outbreak has continued to spread since the end of 2019 worldwide. All people also implement health protocols not to contract this disease. One of the health protocols that must be implemented is to limit interactions between humans to a length of 1&#x2013;2 meters or what is usually done with social distancing. Social distance detection system to ensure that people do not violate social distancing could be a solution to this problem. Using the YOLO-v5 method, which is the latest version of the YOLO (You Only Look Once) method with a detection speed of up to 140 Frames Per Second (FPS) and 90 percent smaller than the previous version, this system detects people who violate social distancing and then gives a voice warning to keep their distance to avoid spreading the COVID-19 virus. The human detection rate in the detection system reaches 93,5&#x0025;, and the accuracy for social distance detection reaches 95&#x0025;. Based on the research that has been done, it can be said that this system can work well for detecting social distance, but the detection will start detecting the distance between the camera and the object exceeding 10 meters.",https://ieeexplore.ieee.org/document/9759552/,2022 2nd International Conference on Information Technology and Education (ICIT&E),22-22 Jan. 2022,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/INTECH.2017.8102423,Real-time emotional state detection from facial expression on embedded devices,IEEE,Conferences,"From the last decade, researches on human facial emotion recognition disclosed that computing models built on regression modelling can produce applicable performance. However, many systems need extensive computing power to be run that prevents its wide applications such as robots and smart devices. In this proposed system, a real-time automatic facial expression system was designed, implemented and tested on an embedded device such as FPGA that can be a first step for a specific facial expression recognition chip for a social robot. The system was built and simulated in MATLAB and then was built on FPGA and it can carry out real time continuously emotional state recognition at 30 fps with 47.44% accuracy. The proposed graphic user interface is able to display the participant video and two dimensional predict labels of the emotion in real time together.",https://ieeexplore.ieee.org/document/8102423/,2017 Seventh International Conference on Innovative Computing Technology (INTECH),16-18 Aug. 2017,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICDSP.2015.7251991,Real-time sociometrics from audio-visual features for two-person dialogs,IEEE,Conferences,"This paper proposes a real time sociometric system to analyze social behavior from audio-visual recordings of two-person face-to-face conversations in English. The novelty of the proposed system lies in this automatic inference of ten social indicators in real time. The system comprises of a Microsoft kinect device that captures RGB and depth data to compute visual cues and microphones to capture speech cues from an on-going conversation. With these non-verbal cues as features, machine learning algorithms are implemented in the system to extract multiple indicators of social behavior including empathy, confusion and politeness. The system is trained and tested on two carefully annotated corpora that consist of two person dialogs. Based on leave-one-out cross-validation test, the accuracy range of developed algorithms to infer social behaviors is 50% - 86% for audio corpus, and 62% - 92% for audio-visual corpus.",https://ieeexplore.ieee.org/document/7251991/,2015 IEEE International Conference on Digital Signal Processing (DSP),21-24 July 2015,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/MIPRO.2016.7522376,Reciprocal payers identification in banking logs using SAT solvers,IEEE,Conferences,"In this paper we presented solvers for satisfiability testing (SAT) as a novel approach to finding reciprocal payers in banking logs. A term “reciprocal payers” is usually treated as general fraud by using standard techniques such as expert systems, machine learning and in recent times social network analysis. SAT as a technique for data analysis was abandoned due to the unfeasibility of SAT solvers. SAT solvers, however continued to develop in the hardware and software verification communities. We presented a proof-of-concept solution for identification of reciprocal payers (formally called a clique), which is a group of bank clients that issue payments to each other (each member to each member). We do not use real data due to client confidentiality, but the reader can see the principle. In the basic approach it is assumed that each client has only one account, and in the extended, second approach, it was allowed that a client can have more than one account.",https://ieeexplore.ieee.org/document/7522376/,"2016 39th International Convention on Information and Communication Technology, Electronics and Microelectronics (MIPRO)",30 May-3 June 2016,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IRDS.2002.1043897,Recognizing and remembering individuals: online and unsupervised face recognition for humanoid robot,IEEE,Conferences,"Individual recognition is a widely reported phenomenon in the animal world, where it contributes to successful maternal interaction, parental care, group breeding, cooperation, mate choice, etc. This work addresses the question of how one may implement such social competence in a humanoid robot. We argue that the robot must be able to recognize people and learn about their various characteristics through embodied social interaction. Thus, we proposed an initial implementation of an online and unsupervised face recognition system for Kismet, our sociable robotic platform. We show how specific features of this particular application drove our decision and implementation process, challenged by the difficulty of the face recognition problem, which has so far been explored in the supervised manner. Experimental results are reported to illustrate what was solved and the lessons learned from the current implementation.",https://ieeexplore.ieee.org/document/1043897/,IEEE/RSJ International Conference on Intelligent Robots and Systems,30 Sept.-4 Oct. 2002,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA.2017.7989267,Recognizing social touch gestures using recurrent and convolutional neural networks,IEEE,Conferences,"Deep learning approaches have been used to perform classification in several applications with high-dimensional input data. In this paper, we investigate the potential for deep learning for classifying affective touch on robotic skin in a social setting. Three models are considered, a convolutional neural network, a convolutional-recurrent neural network and an autoencoder-recurrent neural network. These models are evaluated on two publicly available affective touch datasets, and compared with models built to classify the same datasets. The deep learning approaches provide a similar level of accuracy, and allows gestures to be predicted in real-time at a rate of 6 to 9 Hertz. The memory requirements of the models demonstrate that they can be implemented on small, inexpensive microcontrollers, demonstrating that classification can be performed in the skin itself by collocating computing elements with the sensor array.",https://ieeexplore.ieee.org/document/7989267/,2017 IEEE International Conference on Robotics and Automation (ICRA),29 May-3 June 2017,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IPIN.2019.8911776,Regression Assisted Crowdsourcing Approach for Fingerprint Radio Map Construction,IEEE,Conferences,"Due to the proliferating social and commercial interest on location-based services (LBS), research and development of indoor positioning system (IPS) have been expanded. Most of the efficient IPS methods utilize radio-based solutions to meet the accuracy requirement of indoor LBS. Typically fingerprinting localization is realized, which requires a site survey process where radio signatures of a localization area are annotated with their actual recorded locations. The site survey is time-consuming and labor-intensive that intensifies practical limits and challenges in realizing a reliable and scalable IPS. In this paper, we propose a crowdsourcing-based approach to acquire the training data set for Gaussian process regression (GPR). In particular, we suggest combining access point (AP) proximity information and pedestrian dead reckoning (PDR) to collect labeled data without any human intervention. The crowdsourced training data are fed to model a Gaussian process, which predicts the mean RSS and its corresponding variance across the testbed. To validate the proposed method, we compared the predicted data with the manually measured one and utilized the predicted data for localization using weighted k-nearest neighbor (Wk-NN) and maximum likelihood (ML) based fingerprinting localization. Experimental results obtained by real field deployment show that the average difference between the predicted RSS and manually measured RSS is 3.87 dBm and 80% of the localization estimation error are below 5.5m.",https://ieeexplore.ieee.org/document/8911776/,2019 International Conference on Indoor Positioning and Indoor Navigation (IPIN),30 Sept.-3 Oct. 2019,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SNPD.2012.39,Relationships between Periodic Behaviors in Micro-blogging and the Users' Baseline Mood,IEEE,Conferences,"Twitter messages are real-time, spontaneous reports of what the users are feeling, thinking, and doing. The frequency of posting ""Tweets"" oscillates periodically in one-day and seven-day cycles. These periodic patterns may be related to the individual users' baseline affective state. In order to investigate individual periodic behavior in social media, we performed a Fourier series expansion and PCA on intra-week Tweet-frequency changes of 11,570 individuals. Moreover, the relationships between the users' baseline mood and the principal component scores were investigated. High frequency in daytime tweets on weekdays was found to be linked to a low positive affective state. The larger number of posting tweets was related to the negative affective state.",https://ieeexplore.ieee.org/document/6299313/,"2012 13th ACIS International Conference on Software Engineering, Artificial Intelligence, Networking and Parallel/Distributed Computing",8-10 Aug. 2012,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IJCNN52387.2021.9534136,Relevance-Aware Anomalous Users Detection in Social Network via Graph Neural Network,IEEE,Conferences,"Anomalous users detection in social network is an imperative task for security problems. Motivated by the great power of Graph Neural Networks(GNNs), many current researches adopt GNN-based detectors to reveal the anomalous users. However, the increasing scale of social activities, explosive growth of users and manifold technical disguise render the user detection a difficult task. In this paper, we propose an innovate Relevance-aware Anomalous Users Detection model (RAU-GNN) to obtain a fine-grained detection result. RAU-GNN first extracts multiple relations of all types of users in social network, including both benign and anomalous users, and accordingly constructs the multiple user relation graph. Secondly, we employ relevance-aware GNN framework to learn the hidden features of users, and discriminate the anomalous users after discriminating. Concretely, by integrating Graph Convolution Network(GCN) and Graph Attention Network(GAT), we design a GCN-based relation fusion layer to aggregate initial information from different relations, and a GAT-based embedding layer to obtain the high-level embeddings. Lastly, we feed the learned representations to the following GNN layer in order to consolidate the node embedding by aggregating the final users' embeddings. We conduct extensive experiment on real-world datasets. The experimental results show that our approach can achieve high accuracy for anomalous users detection.",https://ieeexplore.ieee.org/document/9534136/,2021 International Joint Conference on Neural Networks (IJCNN),18-22 July 2021,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/DSC.2018.00139,Representation Learning Based on Influence of Node for Multiplex Network,IEEE,Conferences,"The research of social network and mining of multi-source/ multi-view network has gradually been a focus in the field of social network recently. Existing studies on building social network are mainly based on the single source data, instead of the multi-source data. In this paper, we use multiplex network (e.g. multi-relation network) to model multi-source data, then propose a node learning representation of multiplex network. First, we propose a method of extracting node influence of multiplex network. Next, taking account into the influence of node and the random walk in multiplex network, we propose a biased random walk method to learn the embedding of node in multiplex network. Finally, we compare existing state-of-the-art techniques on edge reconstruction accuracy and link prediction in several real-world networks from diverse domains. Experiments on real datasets validate the effectiveness of our network representation method, enrich the quantity of conserving multilayer network structure information, and make the description of the node embedding in multiplex network more accurate.",https://ieeexplore.ieee.org/document/8411960/,2018 IEEE Third International Conference on Data Science in Cyberspace (DSC),18-21 June 2018,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/RE.2019.00009,Requirements We Live By,IEEE,Conferences,"Enlightened requirements engineering (RE) researchers and practitioners generally accept that RE is as much about understanding the world as it is about understanding the software and systems that will be built to inhabit that world. As a result, the RE field has fostered a multi-disciplinary following of researchers and practitioners who are prepared to engage deeply in application domains, to apply a range of technical and socio-technical skills to understand those domains, and to accept that the outcome of an effective RE process may not deliver a software system at all. The RE community has also developed, deployed, and evaluated a wide range of contributions that reflect such enlightenment: conceptual models that reflect the relationships between the world and the machine, domain models and scenarios that reflect understandings of problem domains, and enterprise models that reflect the organisations and processes that build and deploy systems. All these in addition to the models that capture the all-important behaviour of systems and software. It seems to me however that the RE discipline is at a crossroads. The mechanics of the discipline appear to be established - much of the published research is now empirical - or technical, but only in so far as it responds to technological advances elsewhere, such as mobile and ubiquitous technologies represented by the Internet of Things, richer application domains such as Industrie 4.0 and Smart Cities, or more advanced computational techniques that are maturing, such AI, machine learning, and blockchains. As a community, we reassure ourselves that our discipline is safe and thriving, after all RE is a “forever problem”: all systems we wish to build will have requirements, now and forever. But this is to be complacent. RE has no protected status to study and deploy requirements. The formal models we elicit, design, and build are increasingly deployable by other disciplines, as are the values that we seek our modern, AI-driven systems to embody. A new and potentially radical re-framing of our discipline may be needed, and I will speculate what this may look like. It may require letting go of what we have considered to be the boundaries of our discipline, while embracing new but fluid boundaries. I have advocated and explored “software without boundaries” as one such framing that challenges the separation of `world and the machine', not because I don't accept the separation of the `what' and the `how', the `indicative' and the `optative', or the `problem' and the `solution', but because the world we live in no longer accepts these separations. Society, more often than not, does not think of systems, of technology, or indeed of software; it thinks of ways of working, ways of interacting, ways of living. Requirements, such as they are, are `requirements we live by' not requirements of systems in the world. At an extreme, if one believes the AI hype, `the world and the machine' will increasingly be replaced by the `world in the machine'. Where does the RE community stand on this, and what can this community do to contribute to the framing and solving of this new reality? My own work in recent years has evolved to reflect the above. I still revisit, with some pride, the `RE Roadmap' that Steve Easterbrook and I published in 2000 - many of the fundamental RE principles we presented still hold today. But I cringe at how we missed the changing nature of the world in which we operate: a world populated by autonomous and adaptive systems, populated by big data and associated analytics, and populated by stakeholders whose multiple perspectives reflect a multitude of ethical and social values, not all of which are wholesome, and many of which are actively subversive or malicious. My own research on security and privacy requirements only scratches the surface of this evolving reality. I invite the RE community to reflect on how it frames its own research in this context.",https://ieeexplore.ieee.org/document/8920530/,2019 IEEE 27th International Requirements Engineering Conference (RE),23-27 Sept. 2019,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/DTPI52967.2021.9540104,Research and practice of lightweight digital twin speeding up the implementation of flexible manufacturing systems,IEEE,Conferences,"Parallel manufacturing in Industry 5.0 requires digital twin to digitize physical systems, building virtual models to open up channels connecting physical systems, information systems, and social systems, and transforming the physical models of the existing production environment to achieve two-way feedback of virtual and real is the current research direction. This paper proposes the modeling idea of lightweight digital twin, extracts core dimensions and performs digital virtual simulation, so as to quickly realize the complete process of two-way feedback, and realize a set of chess flexible parallel manufacturing production lines as a practice for the design of complete lightweight digital twin.",https://ieeexplore.ieee.org/document/9540104/,2021 IEEE 1st International Conference on Digital Twins and Parallel Intelligence (DTPI),15 July-15 Aug. 2021,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICAIIS49377.2020.9194885,Research on Dimension Reduction Method of Public Opinion Short Text Representation Model Based on Combined Neural Network,IEEE,Conferences,"With the rapid development of the Internet and the popularization of social software such as Twitter and Weibo, online public opinion has an increasing influence on public opinion in the entire society. In order to effectively prevent and control vicious incidents, real-time monitoring of online public opinion is becoming increasingly important. important. According to the characteristics of short-sentence information in public opinion, this paper proposes an automatic clustering method based on combinatorial neural network to construct a short-sentence representation model of public opinion, and construct word clusters based on the semantic similarity of characteristic words. The test results show that: in the process of mass text analysis and public opinion discovery, this method can effectively reduce the accuracy of the short text public opinion construction representation model by reducing the dimension of the text representation model, while ensuring the real-time acquisition of public opinion while greatly improving Public opinion finds efficiency.",https://ieeexplore.ieee.org/document/9194885/,2020 IEEE International Conference on Artificial Intelligence and Information Systems (ICAIIS),20-22 March 2020,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICEIEC54567.2022.9835053,Research on Emergency Navigation Path Planning System for Plateau Regions,IEEE,Conferences,"The study of emergency navigation path planning methods in plateau regions has very important economic and social significance. In this paper, we proposed a system design scheme combining road network database and satellite remote sensing images with a flat plateau region as the research object. The proposed system performs image pre-processing operations on real-time remote sensing images based on historical remote sensing data, road network extraction and land classification using remote sensing semantic segmentation techniques, and rasterized maps are established in combination with the road network database, taking into account the different types of motion carriers and the influence of image noise for fast emergency navigation path planning. The proposed system architecture approach is a reference value for research in related fields.",https://ieeexplore.ieee.org/document/9835053/,2022 IEEE 12th International Conference on Electronics Information and Emergency Communication (ICEIEC),15-17 July 2022,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICITBS53129.2021.00178,Research on Feature Extraction and Tendency Analysis of Chinese Language Sentimental Expression in Network Context,IEEE,Conferences,"The current network language only considers the expression of its basic meaning, and lacks in-depth analysis of sentimental characteristics and tendencies. For this reason, this paper investigates the sentimental analysis methods of network language based on the Chinese text information of network social media platform. Through the training of the existing data sets, we complete the data cleaning and feature word extraction. Then, a multi-dimensional fine-grained sentiment classification algorithm is designed by using neural network to reduce the dimension of feature vector. Finally, the structured linguistic features are used to optimize the sentiment score step by step, and the text sentiment classification is implemented, and a tendency analysis model considering the polarity of sentiment is further established. The validity of the model is verified by the real data collected in the network. The results show that the method has a significant advantage in Chinese data sets, and the recognition rate is higher than that of similar sentiment analysis methods.",https://ieeexplore.ieee.org/document/9525484/,"2021 International Conference on Intelligent Transportation, Big Data & Smart City (ICITBS)",27-28 March 2021,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AIID51893.2021.9456477,Research on Privacy Protection Technology in Face Identity Authentication System Based on Edge Computing,IEEE,Conferences,"In today's society, the rapid development of the Internet makes People's Daily life become more intelligent and diversified. Today's society has entered a multifaceted era where everything is interconnected. Artificial intelligence technology is gradually replacing some traditional human services, such as intelligent robot customer service instead of traditional human customer service, intelligent face scanning security check in railway stations instead of traditional manual ticket checking, unmanned supermarket automatic checkout has liberated some social labor costs. All these changes are the result of the development of artificial intelligence technology in today's society. In recent years, unicorn startups focused on biometrics have sprung up all around us, such as BTU and its MEG VII (Face ++). Thanks to the development of Internet and artificial intelligence technology, in many application fields, the traditional access control and identity authentication technology based on password verification is gradually transforming to the scheme based on biometric identification verification. Secure identity authentication is very important to the application of Internet. Face recognition is the most popular technology among all biometric identification technologies. In the field of biometric identification technology, it has become the most widely used technology in the field of identity authentication because of its unique non-invasive, support for infrared and visible light, no need for user cooperation and many other advantages. In the field of education, examinee identification, pedestrian identification detection at the entrance of railway stations, face electronic payment, intelligent video surveillance system, intelligent attendance and access control system, intelligent unmanned supermarkets and customs clearance ports become the pioneer fields of face recognition applications. It can be seen that the era of “national face brushing” has arrived, and the application of face recognition technology will only be more and more widespread in the current era and in the future. However, due to the sensitivity of biometric data and the heterogeneity and openness of network environment, the privacy leakage of biometric data is difficult to avoid. At present, fog computing and edge computing have been paid more and more attention in many fields. In the case that cloud service providers are unable to provide sufficient security, edge computing shows its advantages. In this paper, mobile edge computing is introduced for the first time into the face privacy protection identity authentication system based on cloud server outsourcing computing. It can not only greatly reduce the interaction frequency between users and cloud server, improve the availability and fault tolerance of the system, but also contribute to the implementation of privacy protection scheme. A deep constitutional neural network for face feature extraction is trained using deep learning framework Cafe. Cosine similarity is used to complete face verification. A privacy protection scheme based on the secure nearest neighbor algorithm is proposed, which can not only protect the security of the face feature data at the edge computing node, but also allow the edge computing node to complete the face recognition operation against the encrypted face feature data. In addition, the encryption scheme does not require large computing resources, and the accuracy of face recognition in cipher text is exactly the same as that in explain. At present, most of the solutions either have high computational complexity or poor security performance. How to reduce the computational complexity and improve the real-time performance of the system while ensuring the high security of the private data has important research significance and value. Therefore, in the cloud server outsourcing computing environment, how to complete biometric identification on the premise of protecting the privacy of biological data has become a research hot spot.",https://ieeexplore.ieee.org/document/9456477/,2021 IEEE International Conference on Artificial Intelligence and Industrial Design (AIID),28-30 May 2021,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICBASE51474.2020.00084,Research on multi UAV target detection algorithm in the air based on improved CenterNet,IEEE,Conferences,"In recent years, with the rapid development of UAV technology, the following problems and hidden dangers are increasingly prominent: “unauthorized flight” incidents occur frequently, security risks are serious, and social security is threatened. In order to quickly find UAV targets in these events and take relevant measures, this paper proposes a fast UAV target detection algorithm based on improved CenterNet, which can extract the depth features of the collected images, and use Gaussian kernel function to generate keypoint feature map, and output multiple target location information and category information. By modifying the structure and parameters of feature extraction network, the size of the model is greatly reduced and the running speed of the model is accelerated. At the same time, the detection effect of small target is improved obviously after adding DIoU loss. We train and test the algorithm on our own UAV dataset, and the experimental results achieve high accuracy (48.9 AP) and real-time detection speed (143fps). The improved algorithm is easy to be integrated into the mobile terminal or embedded system, providing support for the next step of alarming, jamming and drive away actions.",https://ieeexplore.ieee.org/document/9403787/,2020 International Conference on Big Data & Artificial Intelligence & Software Engineering (ICBASE),30 Oct.-1 Nov. 2020,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICTC46691.2019.8939870,Residual Seq2Seq model for Building energy management,IEEE,Conferences,"According to the UN Department of Economic and Social Affairs (DESA), the global urbanization rate is expected to reach 68% in 30 years from 55% in 2018. As global urbanization progresses, the proportion of total energy consumption consumed in buildings is increasing, and efficient energy management in buildings is becoming increasingly important in terms of efficient use of global natural resources and air quality management. This paper presents an implementation detail of the energy consumption prediction deep learning model for efficient building energy management. The developed model is multilayer LSTM seq2seq model which predicts energy consumption for one day by using environmental data and energy consumption data measured in a real testbed. In a multi-zone building, one zone of data is used, which is characterized by the fact that data patterns over time are not neatly repeated. This paper presents deep learning depth and performance changes as adding layers of seq2seq model in building energy consumption forecasting. In addition, it presents performance comparison with other algorithms.",https://ieeexplore.ieee.org/document/8939870/,2019 International Conference on Information and Communication Technology Convergence (ICTC),16-18 Oct. 2019,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICSAI.2017.8248529,Resource quality prediction based on machine learning algorithms,IEEE,Conferences,"Many resources today are shared freely through social network or cloud storage platforms, which are helpful for uses to acquire data or exchange information. Unfortunately, due to the unrestricted participations, some resources with advertisements or fraud are also uploaded, which force users to hit the ad websites or steal users' data. Therefore, the quality evaluation of one resource is needed for users to judge whether to utilize or install it. In this paper, we implement a system to evaluate the quality based on software install packages, which applies four algorithms to forecast the quality scores. We conduct an extensive experimental study on a real dataset and find that the prediction can be performed in less than one second (0.002s~0.04s) and with a high accuracy (82.84%~90.52%).",https://ieeexplore.ieee.org/document/8248529/,2017 4th International Conference on Systems and Informatics (ICSAI),11-13 Nov. 2017,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ACII.2013.88,Reversal Learning Based on Somatic Markers,IEEE,Conferences,"One of the main aspects in the field of Artificial Intelligence is the creation of agents with the ability to learn like human beings do. Based on made experiences humans are able to adapt their behaviour in order to solve tasks. Another important aspect of human decision making is the ability to discard learned behaviour when the usual decisions, concerning a stimulus, lead to a bad outcome. For robots intended to be embedded in a social environment, the adaptability of behaviour is an important factor. Research of human decision behaviour shows, that emotions play a decisive role, even for learning and reversal learning. In this paper, improvements and further results of a previously presented framework for decision making based on an emotional memory are presented. The improvements include the reduction of the amount of previous knowledge that has to be implemented and an evaluation concerning reversal learning. For evaluation purposes, a typical reversal learning task, performed by real subjects, has been used. The results show that this framework allows the adaption of behaviour comparable to human subjects and offers decisive improvements, which lead to better results in reversal learning tasks without the need to directly declare a task as such one.",https://ieeexplore.ieee.org/document/6681479/,2013 Humaine Association Conference on Affective Computing and Intelligent Interaction,2-5 Sept. 2013,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICICPI.2016.7859671,Review and study of internet of things: It's the future,IEEE,Conferences,"Internet of things represents a groundbreaking vision of technological, economic and social significance. This paper is designed to help technology enthusiasts navigate the dialogue surrounding the same, be well versed in IoT and understand its potential to change everything we know to be true today. In this new era of technology, Internet of Things (IoT) has the true potential to make computing ubiquitous - a concept introduced by Mark Weiser in early 1990s. Machine to machine, computer to objects, environment to computers, intelligent systems - “The internet of everything”, is where the potential seems to be endless. IoT can be seen as a universal global neural network of connected things in the cloud where smart machines interact and communicate with other machines, objects, infrastructure and the environment. As a result of which, exponential amount of data is generated and processed into actions that has the ability to command and control things making our everyday life much easier. This paper concludes with a discussion around social and commercial issues likely to surface when Internet of Things becomes a household reality.",https://ieeexplore.ieee.org/document/7859671/,2016 International Conference on Intelligent Control Power and Instrumentation (ICICPI),21-23 Oct. 2016,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA48506.2021.9560893,Robot Navigation in Constrained Pedestrian Environments using Reinforcement Learning,IEEE,Conferences,"Navigating fluently around pedestrians is a necessary capability for mobile robots deployed in human environments, such as buildings and homes. While research on social navigation has focused mainly on the scalability with the number of pedestrians in open spaces, typical indoor environments present the additional challenge of constrained spaces such as corridors and doorways that limit maneuverability and influence patterns of pedestrian interaction. We present an approach based on reinforcement learning (RL) to learn policies capable of dynamic adaptation to the presence of moving pedestrians while navigating between desired locations in constrained environments. The policy network receives guidance from a motion planner that provides waypoints to follow a globally planned trajectory, whereas RL handles the local interactions. We explore a compositional principle for multi-layout training and find that policies trained in a small set of geometrically simple layouts successfully generalize to more complex unseen layouts that exhibit composition of the structural elements available during training. Going beyond walls-world like domains, we show transfer of the learned policy to unseen 3D reconstructions of two real environments. These results support the applicability of the compositional principle to navigation in real-world buildings and indicate promising usage of multi-agent simulation within reconstructed environments for tasks that involve interaction. https://ai.stanford.edu/&#x223C;cdarpino/socialnavconstrained/",https://ieeexplore.ieee.org/document/9560893/,2021 IEEE International Conference on Robotics and Automation (ICRA),30 May-5 June 2021,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA.2018.8462969,Robust Human Following by Deep Bayesian Trajectory Prediction for Home Service Robots,IEEE,Conferences,"The capability of following a person is crucial in service-oriented robots for human assistance and cooperation. Though a vast variety of following systems exist, they lack robustness against dynamic changes of the environment and relocating to continue following a lost target. Here we present a robust human following system that has the extendability to commercial service robot platforms having a RGB-D camera. The proposed framework integrates deep learning methods for perception and variational Bayesian techniques for trajectory prediction. Deep learning modules enable robots to accompany a person by detecting the target, learning the target and following while avoiding collision within the dynamic home environment. The variational Bayesian techniques robustly predict the trajectory of the target by empowering the following ability of the robot when target is lost. We experimentally demonstrate the capability of the deep Bayesian trajectory prediction method on real-time usage, following abilities, collision avoidance and trajectory prediction of the system. The proposed system was deployed at the RoboCup@Home 2017 Social Standard Platform League and successfully demonstrated its robust functions and smooth person following capability resulting in winning the 1st place.",https://ieeexplore.ieee.org/document/8462969/,2018 IEEE International Conference on Robotics and Automation (ICRA),21-25 May 2018,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SNPD.2017.8022736,Routing algorithm based on ant colony optimization for mobile social network,IEEE,Conferences,"Mobile social network (MSN) is a type of delay tolerant network explicitly considering social characteristics of the terminal nodes. The existing Ad Hoc routing protocols assume that there is at least one complete communication path between the source node and the target node. So they cannot be applied to MSN directly. The key to solve the problem of content distribution in mobile social network is how to transmit the data to the target node in the case of there is no complete communication path between the source node and the target node. As the routing algorithm based on ant colony optimization has great ability to adapt it, it is an effective method to deal with the dynamic topologies of MSN. Based on the social network characteristics of MSN, this paper proposes a new MSN routing algorithm ACOMSN based on ant colony optimization. The algorithm uses the method of processing the node information on the transmission path to get the information list between the node pairs, so as to select the appropriate relay node to provide effective information when forwarding data to other nodes. In addition, ACOMSN designs methodologies for pheromone updating and data forwarding. The simulation experiments on real data sets show that comparing with typical MSN routing algorithms, ACOMSN can effectively improve the critical performance of data transmission with considerable overhead in MSN.",https://ieeexplore.ieee.org/document/8022736/,"2017 18th IEEE/ACIS International Conference on Software Engineering, Artificial Intelligence, Networking and Parallel/Distributed Computing (SNPD)",26-28 June 2017,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CLOUD.2009.80,Rule-Based Problem Classification in IT Service Management,IEEE,Conferences,"Problem management is a critical and expensive element for delivering IT service management and touches various levels of managed IT infrastructure. While problem management has been mostly reactive, recent work is studying how to leverage large problem ticket information from similar IT infrastructures to probatively predict the onset of problems. Because of the sheer size and complexity of problem tickets, supervised learning algorithms have been the method of choice for problem ticket classification, relying on labeled (or pre-classified) tickets from one managed infrastructure to automatically create signatures for similar infrastructures. However, where there are insufficient preclassified data, leveraging human expertise to develop classification rules can be more efficient. In this paper, we describe a rule-based crowdsourcing approach, where experts can author classification rules and a social networking-based platform (called xPad) is used to socialize and execute these rules by large practitioner communities. Using real data sets from several large IT delivery centers, we demonstrate that this approach balances between two key criteria: accuracy and cost effectiveness.",https://ieeexplore.ieee.org/document/5283873/,2009 IEEE International Conference on Cloud Computing,21-25 Sept. 2009,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/UBMYK48245.2019.8965480,Rumor Detection in Social Media Using Machine Learning Methods,IEEE,Conferences,"Throughout history, people have always wondered whether a word or writing is real. Whether or not that word was true depends on the person who spoke it. While the word of a trusted person was respected, the word of an unreliable person was believed to be rumor. Today, with the development of social networks, the amount of information circulating on the internet has increased rapidly. However, the reliability of this data is disputable. It is an undeniable fact that there will be gossip in the environment with so many unconfirmed data. Thus, efficiently detection of rumor is an important and hot topic. In this study, the detection of rumor in online social media is modeled as a classification problem, and the success of supervised machine learning methods in real data is assessed. OneR (One Rule), Naive Bayes, ZeroR, JRip, Random Forest, Sequential Minimal Optimization, and Hoeffding Tree algorithms are applied on rumor detection problem and comprehensive evaluation is provided.",https://ieeexplore.ieee.org/document/8965480/,2019 1st International Informatics and Software Engineering Conference (UBMYK),6-7 Nov. 2019,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICSESS.2018.8663917,Rumor Detection on Twitter with Hierarchical Attention Neural Networks,IEEE,Conferences,"Social media has become an integral part of peoples agenda life, however social media also plays a vital role in communication and information dissemination, making it an ideal platform for spreading rumors. Automatically debunking rumors at their stage of diffusion is known as early rumor detection. However, automatic rumor detection is technically very difficult and uncertain. In this work, we try to build a model to learn extracted features from tweets content by concentrating their posts and generate more meaningful representations to identify different type of rumors. This paper presents a deep hierarchical attention model on the basis of recurrent neural networks (RNN) for rumor representation learning and classification. Extensive experiments on real datasets which collected from social media web sites (mainly on twitter) demonstrate that the deep hierarchical attention based RNN model outperforms state-of-the-arts that rely on hand-crafted features.",https://ieeexplore.ieee.org/document/8663917/,2018 IEEE 9th International Conference on Software Engineering and Service Science (ICSESS),23-25 Nov. 2018,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/MASS.2019.00022,SHAD: Privacy-Friendly Shared Activity Detection and Data Sharing,IEEE,Conferences,"Nowadays, there is a growing demand for sharing multimedia data among participants in the same activity. With existing social applications, users need to conduct friending and data sharing operations manually, which is troublesome due to changing attendees and highly diverse data content of different activities. To tackle this issue, in this work we propose a novel system SHAD to achieve privacy-friendly shared activity detection and multimedia data auto-sharing based on users' historical multimodal data. Facing noisy, incomplete and asynchronous data, as well as inaccurate recognition results of machine learning models, we design an algorithm to aggregate multimodal data relevant to the same activity and propose an activity-semantic graph to comprehensively characterize each activity by fusing knowledge of multimodal data. Based on the activity-semantic graph, the privacy-preserving shared activity detection and data sharing method is designed, which protects both raw data and semantic information of data. We implemented our system and conducted comprehensive evaluations with real-life multimodal data (including photos and motion sensor data). The results show the efficacy of our system. We can achieve 94.9% precision and 91.5% recall for shared activity detection.",https://ieeexplore.ieee.org/document/9077362/,2019 IEEE 16th International Conference on Mobile Ad Hoc and Sensor Systems (MASS),4-7 Nov. 2019,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICter53630.2021.9774824,SMART Garbage Bin Kit: Expandable and Intelligent Waste Management System using Deep Learning and IoT for Modern Organizations,IEEE,Conferences,"According to published statistics, Sri Lanka produces garbage around 7000MT per day, and every organization directly contributes this national amount depending on the waste management practices. 'Waste contamination' is a critical issue that affects waste management, and it should be addressed during the garbage collection process. This has led to environmental hazards resulting in health and other social issues. Hence, it is a responsibility of an organization to separate the garbage during the collection process using a suitable technique. In this paper, we are proposing a smart garbage bin kit that automates the separation of garbage collection, which minimizes human error using AI-based technologies. IoT-based devices connected to a smart garbage bin kit guide the user to the correct bin. At the same time, our proposed system can be easily expanded for new special waste categories as well. The other important issue of the current garbage management is improper time management of the garbage removal process in organizations. This happens due to the lack of real-time data on waste bins, and collection is based on the fixed time interval irrespective of the status and location of garbage bins. In the proposed system of SMART Garbage Bin Kit, the group of all interconnected garbage bins is monitored in real-time to identify the optimum collection path considering the location and the status of garbage bins using an optimized algorithm. Hence, the study presented in this paper integrates several intelligent approaches together with IoT based network to build a cutting-edge device, declared as SMART Garbage Bin kit. The prototype system has been built as a part of the research study to demonstrate its feasibility and sustainability.",https://ieeexplore.ieee.org/document/9774824/,2021 21st International Conference on Advances in ICT for Emerging Regions (ICter),2-3 Dec. 2021,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ANTS.2017.8384099,SMEAD: A secured mobile enabled assisting device for diabetics monitoring,IEEE,Conferences,"Wearable health devices, mobile apps and diagnostic tools revolutionize the medical field by introducing new assisting devices for patients in a way to create comfort, communication and augmented intelligence. Internet of Things involved in this transformation to provide an environment where a patient's vital parameters get transmitted by sensor devices via a gateway onto secure cloud-based platforms where it is stored, aggregated and analyzed. It also helps to store data for millions of patients and performs analysis in real time, ultimately promoting an evidence-based medicine system. Privacy and security are concerns in this environment. Based on the latest trends, this paper introduces a new healthcare paradigm named as SMEAD by developing an end-to-end secured system for assisting diabetic patients. It includes wearables to monitor different parameters thus observe and predict the diabetes status of the patient. The proposed system employs a MEDIBOX which is used to configure the dosage required and provides an alert to the users reminding them to take medication on time. In this case, the insulin dosage is maintained at suitable cooling conditions and is continuously monitored using the mentioned system. To keep all the data secure and to enable access to this data by the doctor and other trusted parties, a Blockchain-based disruptive technology is implemented which facilitates cryptographic security and formalized data access through smart contracts for medical communities. In case of an emergency like missing a dosage, abnormal blood sugar levels or any security lapse, an alert is sent to the caretakers via social networks like Twitter, Facebook or WhatsApp using mobile as a gateway which can continuously communicate the data over the internet that could save patients from fatal effects of the disease.",https://ieeexplore.ieee.org/document/8384099/,2017 IEEE International Conference on Advanced Networks and Telecommunications Systems (ANTS),17-20 Dec. 2017,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/MDM48529.2020.00041,STIMULATE: A System for Real-time Information Acquisition and Learning for Disaster Management,IEEE,Conferences,"Real-time information sharing and propagation using social media such as Twitter has proven itself as a potential resource to improve situational awareness in a timely manner for disaster management. Traditional disaster management systems work well for analyzing static and historical information. However, they cannot process dynamic streams of data that are being generated in real-time. This paper presents STIMULATE - a System for Real-time Information Acquisition and Learning for Disaster Management that can (1) fetch and process tweets in real-time, (2) classify those tweets into FEMA defined categories for rescue priorities using pre-trained deep learning models and generate useful insights, (3) find FEMA defined stranded people for rescue missions of varying priorities, and (4) provide an interactive web interface for rescue management given the available resources. The STIMULATE prototype is primarily built using the Python Flask framework for web interaction. Additionally, it is deployed in the cloud environment using Hadoop and MongoDB for scalable storage, and on-demand computing for processing extensive social media data. The deep learning models in the STIMULATE prototype use Python Keras and the TensorFlow library. We use Bi-directional Long Short-Term Memory (BLSTM) and Convolutional Neural Network (CNN) for developing the tweet classifier. Further, we use the Python PyWSGI WebSocket server for rescue scheduling operations. We present a deep learning system trained on hurricane Harvey and Irma datasets only. The tweet classifier is evaluated using 15 different disaster datasets. Finally, we present the results of multiple simulations using synthetic data with different sizes to measure the performance and effectiveness of the tweets processor and rescue scheduling algorithm.",https://ieeexplore.ieee.org/document/9162220/,2020 21st IEEE International Conference on Mobile Data Management (MDM),30 June-3 July 2020,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IPDPSW52791.2021.00138,ScaDL 2021 Invited Speaker-3: AI for Social Impact: Results from multiagent reasoning and learning in the real world,IEEE,Conferences,"Summary form only given, as follows. The complete presentation was not made available for publication as part of the conference proceedings. With the maturing of AI and multiagent systems research, we have a tremendous opportunity to direct these advances towards addressing complex societal problems. I focus on the problems of public health and conservation, and address one key cross-cutting challenge: how to effectively deploy our limited intervention resources in these problem domains. I will present results from work around the globe in using AI for HIV prevention, Maternal and Child care interventions, TB prevention and COVID modeling, as well as for wildlife conservation. Achieving social impact in these domains often requires methodological advances. To that end, I will highlight key research advances in multiagent reasoning and learning, in particular in, computational game theory, restless bandits and influence maximization in social networks. In pushing this research agenda, our ultimate goal is to facilitate local communities and non-profits to directly benefit from advances in AI tools and techniques.",https://ieeexplore.ieee.org/document/9460613/,2021 IEEE International Parallel and Distributed Processing Symposium Workshops (IPDPSW),17-21 June 2021,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICMLC.2003.1264525,Scenario-based agent design,IEEE,Conferences,"Application designers, such as non-computer professionals, cannot directly describe interaction by agent communication languages (e.g., KQML and FIPA ACL) for designing proper applied agents. To bridge the gap between agents and their social usage, Q language - a scenario description language for describing interaction among agents and users is emerging. In order to explore Q language's capability in real agent environment, we applied it in the fields such as semantic information searching on the semantic web and agent-mediated negotiation in e-commerce.",https://ieeexplore.ieee.org/document/1264525/,Proceedings of the 2003 International Conference on Machine Learning and Cybernetics (IEEE Cat. No.03EX693),5-5 Nov. 2003,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCC51575.2020.9345086,Secure and Transparent Public-key Management System for Vehicular Social Networks,IEEE,Conferences,"Vehicular Social Networks (VSNs) are expected to become a reality soon, where commuters having common interests in the virtual community of vehicles, drivers, passengers can share information, both about road conditions and their surroundings. This will improve transportation efficiency and public safety. However, social networking exposes vehicles to different kinds of cyber-attacks. This concern can be addressed through an efficient and secure key management framework. This study presents a Secure and Transparent Public-key Management (ST-PKMS) based on blockchain and notary system, but it addresses security and privacy challenges specific to VSNs. ST-PKMS significantly enhances the efficiency and trustworthiness of mutual authentication. In ST-PKMS, each vehicle has multiple short-lived anonymous public-keys, which are recorded on the blockchain platform. However, public-keys get activated only when a notary system notarizes it, and clients accept only notarized public-keys during mutual authentication. Compromised vehicles can be effectively removed from the VSNs by blocking notarization of their public-keys; thus, the need to distribute Certificate Revocation List (CRL) is eliminated in the proposed scheme. ST-PKMS ensures transparency, security, privacy, and availability, even in the face of an active adversary. The simulation and evaluation results show that the ST-PKMS meets real-time performance requirements, and it is cost-effective in terms of scalability, delay, and communication overhead.",https://ieeexplore.ieee.org/document/9345086/,2020 IEEE 6th International Conference on Computer and Communications (ICCC),11-14 Dec. 2020,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IWISA.2009.5073107,Self-Organization Behaviors of Intelligent Antagonism Target Team of Air Combat Based on Pi-Calculus,IEEE,Conferences,"In order to improve the reality and the reliability of the current air combat simulation and the airborne weapon system evaluation in uncertain, hostile, complex and dynamic air combat environment, we propose an intelligent antagonism target team of air combat based on MAS (multi-agent system) under presented research results. To improve the social mental attribution of the target team, we develop a BGOP (belief, goal, obligation, plan) model under the basis of the traditional BDI(belief, desire and intention) model, which represents the mental states of each target agent, and which has the mental attributions related to the combat utilities which affecting the command control decision of the target agent firstly, and then an organization structure of the command control system of target team, is presented, and the self-organization process of the target team combat behaviors, such as task-assignment, task-maintain, task-change, and the task-coordination etc. are described by pi-calculus.",https://ieeexplore.ieee.org/document/5073107/,2009 International Workshop on Intelligent Systems and Applications,23-24 May 2009,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ASONAM.2012.66,Semi-Supervised Policy Recommendation for Online Social Networks,IEEE,Conferences,"Fine grain policy settings in social network sites is becoming a very important requirement for managing user's privacy. Incorrect privacy policy settings can easily lead to leaks in private and personal information. At the same time, being too restrictive would reduce the benefits of online social networks. This is further complicated with the growing adoption of social networks and with the rapid growth in information uploading and sharing. The problem of facilitating policy settings has attracted numerous access control, and human computer interaction researchers. The solutions proposed range from usable interfaces for policy settings to automated policy settings. We propose a fine grained policy recommendation system that is based on an iterative semi-supervised learning approach that uses the social graph propagation properties. Active learning and social graph properties were used to detect the most informative instances to be labeled as training sets. We implemented and tested our approach using real Facebook dataset. We compared our proposed approach to supervised learning and random walk approaches. Our proposed approaches provided high accuracy and precision when compared to the other approaches.",https://ieeexplore.ieee.org/document/6425738/,2012 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining,26-29 Aug. 2012,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/BigMM.2015.26,Semi-supervised Multimodal Clustering Algorithm Integrating Label Signals for Social Event Detection,IEEE,Conferences,"Photo-sharing social media sites provide new ways for users to share their experiences and interests on the Web, which aggregate large amounts of multimedia resources associated with a wide variety of real-world events in different types and scales. In this work, we aim to tackle social event detection from these large amounts of image collections by devising a semi-supervised multimodal clustering algorithm, denoted by SSMC, which exploits label signals to guide the fusion of the multimodal features. Particularly, SSMC takes advantage of the distribution over the similarities on a small amount of labeled data to represent the images, fusing multiple heterogeneous features seamlessly. As a result, SSMC has low computational complexity in processing multimodal features for both initial and updating stages. Experiments are conducted on the Mediaeval social event detection challenge, and the results show that our approach achieves better performance compared with the baseline algorithms.",https://ieeexplore.ieee.org/document/7153853/,2015 IEEE International Conference on Multimedia Big Data,20-22 April 2015,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CEEICT.2016.7873080,Sentiment analysis on facebook group using lexicon based approach,IEEE,Conferences,"Internet is one of the primary sources of Big Data. Rise of the social networking platforms are creating enormous amount of data in every second where human emotions are constantly expressed in real-time. The sentiment behind each post, comments, likes can be found using opinion mining. It is possible to determine business values from these objects and events if sentiment analysis is done on the huge amount of data. Here, we have chosen FOODBANK which is a very popular Facebook group in Bangladesh; to analyze sentiment of the data to find out their market values.",https://ieeexplore.ieee.org/document/7873080/,2016 3rd International Conference on Electrical Engineering and Information Communication Technology (ICEEICT),22-24 Sept. 2016,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SNPD.2017.8022756,Share-housing allocation information service system based on Pareto optimality,IEEE,Conferences,"Compared to living alone, sharing housing with others is usually much cheaper. Meanwhile, share-housing life would exert mammoth impact on people's social activity and self development by help them save house rental cost and improve their life quality. This paper proposes an allocation information system for share-housing management, which takes into account preference on roommates' living habits, working performance and geography distribution. Moreover, Pareto optimality is utilized to facilitate the effect of share-housing allocation. Results of case study show that the three factors of living habits, working performance as well as geography distribution should be synthetically applied as a decision support for share-housing.",https://ieeexplore.ieee.org/document/8022756/,"2017 18th IEEE/ACIS International Conference on Software Engineering, Artificial Intelligence, Networking and Parallel/Distributed Computing (SNPD)",26-28 June 2017,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/COMPSAC51774.2021.00240,Shared-latent Variable Network Alignment,IEEE,Conferences,"The increasing popularity and diversity of social media sites, has encouraged many people to participate in different online social networks to enjoy a variety of services. Linking the same users across different social networks, also known as social network alignment, is a critical task of great research challenges. Many existing works usually focus on finding a projection function from one subspace to another for network alignment, however, the projection functions proposed in their papers are independent and updated individually, which could not effectively exploit the non-parallel data, and yield inferior alignment performance. In this paper, we propose a Shared-latent Variable Network Alignment (SVNA) architecture to effectively exploit the non-parallel data for network alignment, and jointly train projection functions and decoders in a unified framework with the shared latent variable z. Specifically, SVNA first employs the graph convolutional networks to preserve the structural information of the network. By introducing the shared latent variable z, SVNA simultaneously integrates two projection functions and two decoders for jointly training. Both projection functions and decoders share the same latent space, therefore both projection directions can learn from the non-parallel data more effectively. Thereafter, SVNA utilizes the Generative Adversarial Networks (GANs) framework to further train the projection functions, and adopts a probability-based semi-supervised method to achieve the network alignment. Experiments on three real-world datasets show that SVNA generally outperforms the state-of-the-art methods in network alignment task.",https://ieeexplore.ieee.org/document/9529755/,"2021 IEEE 45th Annual Computers, Software, and Applications Conference (COMPSAC)",12-16 July 2021,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ACII.2009.5349428,Simulation of the dynamics of virtual characters' emotions and social relations,IEEE,Conferences,"One of the main challenges is to give life to believable virtual characters. Research shows that emotions and social relations, closely related, play a key role in determining the behavior of individuals. In order to improve the believ-ability of virtual characters' behavior, we propose in this article a method to compute virtual characters emotions based on attitudes and a model of their influence on the dynamics of social relations. Based on this work, a tool aiming at the simulation of the evolution of emotions and social relations of virtual characters have been implemented.",https://ieeexplore.ieee.org/document/5349428/,2009 3rd International Conference on Affective Computing and Intelligent Interaction and Workshops,10-12 Sept. 2009,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/EEBDA53927.2022.9744906,Smart Guide Trolley Mango: Smart Blind Guide Device Based on Raspberry Pi,IEEE,Conferences,"Nowadays, life support equipment for the visually impaired is not perfect, and the current blind guide equipment cannot meet the needs of the blind in the face of emergencies. Therefore, this paper proposes an intelligent traction guidance trolley &#x201C;Mango&#x201D;, With good portability and accurate positioning, it is more suitable for the needs of blind people than traditional guidance tools. Mode 1 is the traction and obstacle avoidance mode. This mode firstly plays the current temperature, humidity, location and weather informa2tion to the blind. If the human body infrared sensor detects that the blind person is following, the voice will then broadcast &#x201C;Blind person is following&#x201D; and activa.go automatically advances and executes the tracing algorithm to guide blind along the blind path. Meanwhile, the best way to avoid obstacles is selected according to the ultrasonic obstacle avoidance algorithm, and information about the identified obstacles ahead is broadcast by voice to guide the blind person around the blind path or other obstacles in the walking path. Mode 2 is the remote monitoring mode. This means that by observing the view of the real-time camera, the family or friends of the blind can remotely control the trolley to find them or guide them with the trolley movements by opening the designated website on any computer or mobile phone and entering commands to control the movement of the Mango. In summary, the smart trolley Mango in this paper is a smart guided trolley that can help the blind to travel safely, and provides a new social direction for improving the quality of life of the visually impaired.",https://ieeexplore.ieee.org/document/9744906/,"2022 IEEE International Conference on Electrical Engineering, Big Data and Algorithms (EEBDA)",25-27 Feb. 2022,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ASONAM.2018.8508382,Social Bots for Online Public Health Interventions,IEEE,Conferences,"According to the Center for Disease Control and Prevention, hundreds of thousands initiate smoking each year, and millions live with smoking-related diseases in the United States. Many tobacco users discuss their opinions, habits and preferences on social media. This work conceptualizes a framework for targeted health interventions to inform tobacco users about the consequences of tobacco use. We designed a Twitter bot named Notobot (short for No-Tobacco Bot) that leverages machine learning to identify users posting pro-tobacco tweets and select individualized interventions to curb their tobacco use. We searched the Twitter feed for tobacco-related keywords and phrases, and trained a convolutional neural network using over 4,000 tweets manually labeled as either pro-tobacco or not pro-tobacco. This model achieved a 90% accuracy rate on the training set and 74% on test data. Users posting protobacco tweets were matched with former smokers with similar interests who posted anti-tobacco tweets. Algorithmic matching, leveraging the power of peer influence, allows for the systematic delivery of personalized interventions based on real anti-tobacco tweets from former smokers. Experimental evaluation suggested that our system would perform well if deployed.",https://ieeexplore.ieee.org/document/8508382/,2018 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining (ASONAM),28-31 Aug. 2018,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AIVR52153.2021.00034,Social Crowd Simulation: The Challenge of Fragmentation,IEEE,Conferences,"Crowd simulation is the process of simulating the movement and behavior of a large number of people. This field is continuously being improved by incorporating different theories of how humans move and interact with their surroundings, steadily increasing the realism of the simulation. Furthermore, new techniques for calibrating simulation parameters, and evaluating the accuracy of simulation output, keep being proposed. This paper presents a brief overview of these foundations and argues that a fragmentation of the field into multiple incompatible solutions may impede progress towards comprehensive social behavior models. It finally argues that abstractions of human intent and behavior, proposed within the Embodied Conversational Agents community, may suggest a useful path towards bringing social crowds to new levels of realism.",https://ieeexplore.ieee.org/document/9644355/,2021 IEEE International Conference on Artificial Intelligence and Virtual Reality (AIVR),15-17 Nov. 2021,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/APCCAS51387.2021.9687720,Social Distancing Surveillance System via Inverse Perspective Mapping and Fixed-point Quantization,IEEE,Conferences,"During the Coronavirus Disease 2019 (COVID-19) pandemic, many countries have introduced the social distancing policy in public areas to stop the spread of disease by maintaining a physical distance between people. This paper proposes an Artificial Intelligence (AI)-powered social distancing surveillance system that can detect pedestrians through video surveillance and monitor the social distance between them via Inverse Perspective Mapping (IPM) in real-time. The proposed system was deployed on the devices located at the network edge such as IoT devices and mobile devices to enable real-time response with low data transmission latency. To bypass the restriction on the computational and memory capacity for the edge devices, the proposed system was optimized through fixed-point quantization. From the evaluation results, the optimized models are almost 4 times smaller as compared to the original models. The best trade-off between speed and accuracy can be achieved with a 27.1&#x0025; improvement in speed and 2&#x0025; degradation in accuracy.",https://ieeexplore.ieee.org/document/9687720/,2021 IEEE Asia Pacific Conference on Circuit and Systems (APCCAS),22-26 Nov. 2021,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/WISA.2015.48,Social Emotion Analysis System for Online News,IEEE,Conferences,"Social emotion analysis of online users has become an important task for mining public opinions, which aims at detecting the readers' emotions evoked by online news articles. In this paper, we focus on building a social emotion analysis system (SEAS) for online news. The system has implemented a text data crawler for mainstream online news websites, the modules of document preprocessing, document representation, and also integrates successful emotion analysis methods and provides the corresponding performance evaluation. SEAS will automatically analyze the emotions towards certain news articles and output the predicted emotions and probabilities of being classified into these emotion categories. The experiments on the real dataset from online news service demonstrate the high practicability and reliability of SEAS.",https://ieeexplore.ieee.org/document/7396605/,2015 12th Web Information System and Application Conference (WISA),11-13 Sept. 2015,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/icABCD49160.2020.9183860,Social Event Invitation and Recommendation for Event based Social Networks,IEEE,Conferences,Event based social networks enable users to make real connections with both the real and virtual worlds. This research aims at providing a way to increase user satisfaction at the events they attend from the event based social network. Two approaches are explained and used to determine the user's interests from the events they have attended and also through the sentiment analysis of the reviews they have given for these events. A profile specific to each user is developed to support event recommendations to users in the system based on a matching profile and a set of at least K friends on the social network. The paper uses content-based filtering by using the events details to make recommendations and collaborative filtering to find users who have similar interests. Lastly the sentiment of event reviews is determined to gain an understanding about how users feel about the events they attended. This sentiment data is integrated into the recommendation of events. The paper also discusses the results achieved and the limitations of the current state of the algorithm.,https://ieeexplore.ieee.org/document/9183860/,"2020 International Conference on Artificial Intelligence, Big Data, Computing and Data Communication Systems (icABCD)",6-7 Aug. 2020,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCCNT51525.2021.9579699,Social Media Intelligence for Brand Analysis,IEEE,Conferences,"With the rise of social media users, the amount of data being generated every day from the social media websites is increasing. Social media web sites like Instagram, Twitter, Snapchat, etc. are allowing the users to express their opinions on any subject and share their opinion with people sitting in any corner of the world. Numerous posts are also posted about various brands belonging to multiple domains like technology, sports, fashion, etc. These posts, when analysed, can provide valuable insights about the social media users' opinion and prove to be an untapped gold mine for the brands. These insights can in turn help the brands to make better business decisions based on the insights that are extracted from the posts. This paper aims to provide a real-time solution to stream the tweets from Twitter related to a specific brand and analyse them in real-time in order to extract insights from the tweet data that allows the brands to know what exactly are the social media users' perceptions about them. This solution consists of multiple stages where scripts are deployed on servers and run around the clock to process the raw streamed tweet data using artificial intelligence techniques in order to extract location, sentiments, named entities, date and time to further categorise the information based on numerous factors mentioned in the paper and present the data in the form of charts displayed on a webpage which will enable the end user to gain insights and uncover hidden patterns from the data.",https://ieeexplore.ieee.org/document/9579699/,2021 12th International Conference on Computing Communication and Networking Technologies (ICCCNT),6-8 July 2021,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SKG.2013.38,Social Sensing Enhanced Time Ruler for Real-Time Bus Service,IEEE,Conferences,Social sensing helps realize the real-time prediction of bus routes by perceiving social events and evaluating their influence on the time when a bus passes through a road section. This paper proposes a social sensing enhanced real-time bus service that integrates sensing ability and social networks to better understand and measure social events that influence vehicle velocity. The establishment of the real-time service revolves around the PT service quality attributions PEAs and the road condition attributions PRCAs. The processes that collect bus relevant events and further categorize them into PEA events or PRCA events were proposed respectively. A method of scoring PEAs for travelers according to social events was discussed. An artificial neural network based prediction model was proposed to estimate the bus arrival time by analyzing PRCA events.,https://ieeexplore.ieee.org/document/6816599/,"2013 Ninth International Conference on Semantics, Knowledge and Grids",3-4 Oct. 2013,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/WIIAT.2008.367,Social and Personal Context Modeling for Contact List Recommendation on Mobile Device,IEEE,Conferences,"This paper presents a social and personal context modeling method on mobile device. It infers userpsilas contexts, such as amity with others and emotional state, from uncertain logs stored in mobile devices using Bayesian networks. Proper services are then provided to the user based on the semantic compatibility between current and past contexts. Here, the contexts are hierarchically matched after each context is expanded to the context-tree using domain ontology. We have implemented a contact list recommendation application on mobile device that recommends phone numbers in a phonebook according to the user's situation. Experimental results on real-user data show that the method provides an efficient and accurate means for mobile based social networking applications.",https://ieeexplore.ieee.org/document/4740803/,2008 IEEE/WIC/ACM International Conference on Web Intelligence and Intelligent Agent Technology,9-12 Dec. 2008,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IJCNN.2017.7965906,Social recommendation using Euclidean embedding,IEEE,Conferences,"Traditional recommender systems assume that all the users are independent, and they usually face the cold start and data sparse problems. To alleviate these problems, social recommender systems use social relations as an additional input to improve recommendation accuracy. Social recommendation follows the intuition that people with social relationships share some kinds of preference towards items. Current social recommendation methods commonly apply the Matrix Factorization (MF) model to incorporate social information into the recommendation process. As an alternative model to MF, we propose a novel social recommendation approach based on Euclidean Embedding (SREE) in this paper. The idea is to embed users and items in a unified Euclidean space, where users are close to both their desired items and social friends. Experimental results conducted on two real-world data sets illustrate that our proposed approach outperforms the state-of-the-art methods in terms of recommendation accuracy.",https://ieeexplore.ieee.org/document/7965906/,2017 International Joint Conference on Neural Networks (IJCNN),14-19 May 2017,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISC2.2016.7580848,Social smart city: A platform to analyze social streams in smart city initiatives,IEEE,Conferences,"A central issue in the context of smart cities is how to analyze a large amount of data generated by different kinds of sources in real time. This paper reports a case study in real-time acquisition of crime detection information from social media messages, built on top of a plataform for fast processing and visualization of data from Twitter. The purpose is to allow city managers to act timely on preventing crime occurence as detected from tweets posted by real users. Key issues here are the processing of a large volume of data and modularization and customization capabilities implemented through pipelined modules for robust, fast, real time tweet acquisition and storage. In particular, the customization is reflected by modules of filtering of several kinds, natural language processing tasks, topped by a machime learning analysis that allows for the classification of the input messages according to the local policy category system.",https://ieeexplore.ieee.org/document/7580848/,2016 IEEE International Smart Cities Conference (ISC2),12-15 Sept. 2016,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ACAIT53529.2021.9731340,SocialBully: A Social Information-Driven Cyberbullying Detector with Similarity-Based Word Embedding,IEEE,Conferences,"Cyberbullying depicts the form of bullying that is spread through information and communication technologies, most common of which is through the Internet. When compared to traditional bullying, cyberbullying can spread quicker and reach a wider audience and, because of this, can have severe effects on people&#x2019;s mental health. In this paper, we propose a new cyberbullying detection method &#x2013; SocialBully. SocialBully systematically integrates both users&#x2019; social information and their posted textual information to achieve a high detection accuracy. It uses a new word embedding method &#x201C;SimWord&#x201D; to represent words based on the similarity of their co-occurrence vectors and exploits a graph embedding method to obtain the social representation of users. Based on these representations, SocialBully uses a bidirectional Long Short-Term Memory to detect bullying texts. Extensive experiments are conducted on four real-world datasets to validate the effectiveness of the proposed method.",https://ieeexplore.ieee.org/document/9731340/,2021 5th Asian Conference on Artificial Intelligence Technology (ACAIT),29-31 Oct. 2021,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/PCCC.2018.8710834,Socialite: Social Activity Mining and Friend Auto-labeling,IEEE,Conferences,"As people's friend lists grow longer, it becomes more and more difficult to manage a friend list by labeling or grouping friends manually. In this paper, we leverage on-board sensors of smart devices and propose a social activity mining framework Socialite, which is able to achieve social group discovering and friend auto-labeling by exploring users' interactions in physical word. Socialite considers different deployment strategies and mainly contains two stages: social activity recognition and social group detection. Together with several data analysis approaches, a voting based lightweight neural network is designed for high accuracy diverse activity recognition. Then we propose a novel algorithm for social interaction feature generation and measure correlation among features of even asynchronous social activities. For system evaluation, we conduct extensive real life experiments. Results demonstrate that Socialite can recognize diverse social activities with above 94% accuracy, and 100% accuracy with our voting scheme. Socialite can also detect social groups in different scenarios with high accuracy. For example in two people activities, our proposed method achieves 92.2% accuracy for walk and 92.6% accuracy for table tennis.",https://ieeexplore.ieee.org/document/8710834/,2018 IEEE 37th International Performance Computing and Communications Conference (IPCCC),17-19 Nov. 2018,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA.2018.8460968,Socially Compliant Navigation Through Raw Depth Inputs with Generative Adversarial Imitation Learning,IEEE,Conferences,"We present an approach for mobile robots to learn to navigate in dynamic environments with pedestrians via raw depth inputs, in a socially compliant manner. To achieve this, we adopt a generative adversarial imitation learning (GAIL) strategy, which improves upon a pre-trained behavior cloning policy. Our approach overcomes the disadvantages of previous methods, as they heavily depend on the full knowledge of the location and velocity information of nearby pedestrians, which not only requires specific sensors, but also the extraction of such state information from raw sensory input could consume much computation time. In this paper, our proposed GAIL-based model performs directly on raw depth inputs and plans in real-time. Experiments show that our GAIL-based approach greatly improves the safety and efficiency of the behavior of mobile robots from pure behavior cloning. The real-world deployment also shows that our method is capable of guiding autonomous vehicles to navigate in a socially compliant manner directly through raw depth inputs. In addition, we release a simulation plugin for modeling pedestrian behaviors based on the social force model.",https://ieeexplore.ieee.org/document/8460968/,2018 IEEE International Conference on Robotics and Automation (ICRA),21-25 May 2018,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/INCoS.2014.15,Software Agents in Large Scale Open E-learning: A Critical Component for the Future of Massive Online Courses (MOOCs),IEEE,Conferences,"MOOCs or massive open online courses are a recent trend in online education. They combine online resources with social tools and have unique challenges due to the large number of simultaneous participants. This paper analyzes some of the challenges in the areas of MOOC design, delivery and assessment. Then the authors present an approach using software agents to overcome some of the challenges that have been identified, as well as optimize efficiency, reduce costs, and ensure the pedagogical effectiveness and educational quality of large scale online learning courses. This paper is a first step towards research in the usage of software agents in massive online courses that we hope will shed more light on potential real life applications.",https://ieeexplore.ieee.org/document/7057089/,2014 International Conference on Intelligent Networking and Collaborative Systems,10-12 Sept. 2014,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/GCIS.2012.41,Software Fault Diagnosing System Based on Multi-agent,IEEE,Conferences,"Software faults are the underlying and important roots which result in the mistake, failure and even breakdown of system. So software Fault Diagnosis and the Spectrum-based Fault Localization (SFL) is very significant to software quality assurance. The current fault diagnosis technique based on the artificial intelligence and Multi-Agent attracts more and more attention. Multi-Agent System (MAS) have autonomy, intelligence and social ability, which is well-suited for Fault Diagnosis, and which can easily be used in software testing schemes of software intensive equipment. This paper gives a Fault Diagnosis system based on MAS and the proposed system is applied to real software Fault Diagnosis to validate its effectiveness.",https://ieeexplore.ieee.org/document/6449546/,2012 Third Global Congress on Intelligent Systems,6-8 Nov. 2012,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRITO.2015.7359361,Software reliability modeling with different type of faults incorporating both imperfect debugging and change point,IEEE,Conferences,"In past four decades, many nonhomogeneous Poisson process (NHPP) based software reliability growth models (SRGMs) have been proposed to measure and assess the reliability growth of software. During the testing process, the faults which causes failure are detected and removed. One common assumption of many traditional SRGMs is that the fault removal rate is constant. In practical, the fault removal rate increases with time as learning and maturity of software engineer increases. Hence, time variant fault removal rate has been considered in this study. A complex software system may contain different category of faults. Some of faults can be easily detected and removed and some of faults required more effort to be detected and removed. Therefore, in this article, a NHPP based SRGM has been proposed which incorporates mainly two type of faults, major and minor. The concepts of imperfect debugging and change point have also been incorporated in the proposed SRGM. The parameters of the proposed SRGM is estimated using Statistical Package for Social Sciences (SPSS) software and validation of the proposed SRGM has been done using real life data set.",https://ieeexplore.ieee.org/document/7359361/,"2015 4th International Conference on Reliability, Infocom Technologies and Optimization (ICRITO) (Trends and Future Directions)",2-4 Sept. 2015,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICIA.2006.305887,Solution to 0/1 Knapsack Problem Based on Improved Ant Colony Algorithm,IEEE,Conferences,"Ant colony algorithms analogize the social behaviour of ant colonies, they are a class of meta-heuristics which are inspired from the behavior of real ants. It was applied successfully to the well-known traveling salesman problem and other hard combinational optimization problems. In order to apply it to the classical 0/1 knapsack problem, this paper compares the difference between the traveling salesman problem and the 0/1 knapsack problem and adapts the ant colony optimization (ACO) model to meet researches' purpose. At the same time, the parameters in ACO model are modified accordingly. The experiments based on improved ant colony algorithms show the robustness and the potential power of this kind of meta-heuristic algorithm.",https://ieeexplore.ieee.org/document/4097820/,2006 IEEE International Conference on Information Acquisition,20-23 Aug. 2006,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICDE.2007.367865,Source-aware Entity Matching: A Compositional Approach,IEEE,Conferences,"Entity matching (a.k.a. record linkage) plays a crucial role in integrating multiple data sources, and numerous matching solutions have been developed. However, the solutions have largely exploited only information available in the mentions and employed a single matching technique. We show how to exploit information about data sources to significantly improve matching accuracy. In particular, we observe that different sources often vary substantially in their level of semantic ambiguity, thus requiring different matching techniques. In addition, it is often beneficial to group and match mentions in related sources first, before considering other sources. These observations lead to a large space of matching strategies, analogous to the space of query evaluation plans considered by a relational optimizer. We propose viewing entity matching as a composition of basic steps into a ""match execution plan"". We analyze formal properties of the plan space, and show how to find a good match plan. To do so, we employ ideas from social network analysis to infer the ambiguity and related-ness of data sources. We conducted extensive experiments on several real-world data sets on the Web and in the domain of personal information management (PIM). The results show that our solution significantly outperforms current best matching methods.",https://ieeexplore.ieee.org/document/4221668/,2007 IEEE 23rd International Conference on Data Engineering,15-20 April 2007,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IDAP.2018.8620910,Spam detection in online social networks by deep learning,IEEE,Conferences,"Twitter spam is one of the most important problems that professionals have to deal with in social networks on the internet. For this problem, the researchers presented some solutions, mostly based on a number of different methods considering learning. The means and techniques used at the current time has achieved a good ratio of the accuracy based on the so-called methods of blacklisting in order to determine the undesirable activities in relation to send and receive an e-mail on social networks that based on the conclusions obtained from previous experiments and studies. However, methods that rely on automated learning are not capable of detecting spam activities in proportion to the real scenarios. We see that methods called blacklist methods are not able to meet the disparities we see in activities related to the transmission of such a message, because manually checking Unique Resources Locaters (URLs) is more time-consuming task. In this study, we present a deep learning method for spam detection in witter. For this purpose, the Word2Vec based on representation is first trained. Then we use binary classification methods to distinguish the spam and the nonspam tweets. The empirical results conducted on tweets prove that the selected methods outperform the classical approaches.",https://ieeexplore.ieee.org/document/8620910/,2018 International Conference on Artificial Intelligence and Data Processing (IDAP),28-30 Sept. 2018,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/INFOCOM.2016.7524568,Spice: Socially-driven learning-based mobile media prefetching,IEEE,Conferences,"Mobile online social networks (OSNs) are emerging as the popular mainstream platform for information and content sharing among people. In order to provide Quality of Experience (QoE) support for mobile OSN services, in this paper we propose a socially-driven learning-based framework, namely Spice, for media content prefetching to reduce the access delay and enhance mobile user's satisfaction. Through a large-scale data-driven analysis over real-life mobile Twitter traces from over 17,000 users during a period of five months, we reveal that the social friendship has a great impact on user's media content click behavior. To capture this effect, we conduct social friendship clustering over the set of user's friends, and then develop a cluster-based Latent Bias Model for socially-driven learning-based prefetching prediction. We then propose a usage-adaptive prefetching scheduling scheme by taking into account that different users may possess heterogeneous patterns in the mobile OSN app usage. We comprehensively evaluate the performance of Spice framework using trace-driven emulations on smartphones. Evaluation results corroborate that the Spice can achieve superior performance, with an average 67.2% access delay reduction at the low cost of cellular data and energy consumption. Furthermore, by enabling users to offload their machine learning procedures to a cloud server, our design can achieve speed-up of a factor of 1000 over the local data training execution on smartphones.",https://ieeexplore.ieee.org/document/7524568/,IEEE INFOCOM 2016 - The 35th Annual IEEE International Conference on Computer Communications,10-14 April 2016,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IBIGDELFT.2018.8625353,Static and Dynamic Analysis of Third Generation Cerber Ransomware,IEEE,Conferences,"Cyber criminals have been extensively using malicious Ransomware software for years. Ransomware is a subset of malware in which the data on a victim's computer is locked, typically by encryption, and payment is demanded before the ransomed data is decrypted and access returned to the victim. The motives for such attacks are not only limited to economical scumming. Illegal attacks on official databases may also target people with political or social power. Although billions of dollars have been spent for preventing or at least reducing the tremendous amount of losses, these malicious Ransomware attacks have been expanding and growing. Therefore, it is critical to perform technical analysis of such malicious codes and, if possible, determine the source of such attacks. It might be almost impossible to recover the affected files due to the strong encryption imposed on such files, however the determination of the source of Ransomware attacks have been becoming significantly important for criminal justice. Unfortunately, there are only a few technical analysis of real life attacks in the literature. In this work, a real life Ransomware attack on an official institute is investigated and fully analyzed. The analysis have been performed by both static and dynamic methods. The results show that the source of the Ransomware attack has been shown to be traceable from the server's whois information.",https://ieeexplore.ieee.org/document/8625353/,"2018 International Congress on Big Data, Deep Learning and Fighting Cyber Terrorism (IBIGDELFT)",3-4 Dec. 2018,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SP.2019.00032,Stealthy Porn: Understanding Real-World Adversarial Images for Illicit Online Promotion,IEEE,Conferences,"Recent years have witnessed the rapid progress in deep learning (DP), which also brings their potential weaknesses to the spotlights of security and machine learning studies. With important discoveries made by adversarial learning research, surprisingly little attention, however, has been paid to the real-world adversarial techniques deployed by the cybercriminal to evade image-based detection. Unlike the adversarial examples that induce misclassification using nearly imperceivable perturbation, real-world adversarial images tend to be less optimal yet equally effective. As a first step to understand the threat, we report in the paper a study on adversarial promotional porn images (APPIs) that are extensively used in underground advertising. We show that the adversary today's strategically constructs the APPIs to evade explicit content detection while still preserving their sexual appeal, even though the distortions and noise introduced are clearly observable to humans. To understand such real-world adversarial images and the underground business behind them, we develop a novel DP-based methodology called Male`na, which focuses on the regions of an image where sexual content is least obfuscated and therefore visible to the target audience of a promotion. Using this technique, we have discovered over 4,000 APPIs from 4,042,690 images crawled from popular social media, and further brought to light the unique techniques they use to evade popular explicit content detectors (e.g., Google Cloud Vision API, Yahoo Open NSFW model), and the reason that these techniques work. Also studied are the ecosystem of such illicit promotions, including the obfuscated contacts advertised through those images, compromised accounts used to disseminate them, and large APPI campaigns involving thousands of images. Another interesting finding is the apparent attempt made by cybercriminals to steal others' images for their advertising. The study highlights the importance of the research on real-world adversarial learning and makes the first step towards mitigating the threats it poses.",https://ieeexplore.ieee.org/document/8835391/,2019 IEEE Symposium on Security and Privacy (SP),19-23 May 2019,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICDM51629.2021.00171,Streaming Dynamic Graph Neural Networks for Continuous-Time Temporal Graph Modeling,IEEE,Conferences,"Dynamic graphs are suitable for modeling structured data that evolve over time and have been widely used in many application scenarios such as social networks, financial transaction networks, and recommendation systems. Recently, many dynamic graph methods are proposed to deal with temporal networks. However, due to the limitations of storage space and computational efficiency, most approaches evolve node representations by aggregating the latest state information of neighbor nodes, thus losing a lot of information about neighbor nodes’ state changes. Besides, high computational complexity makes it challenging to deploy dynamic graph algorithms in real-time. To tackle these challenges, we propose a novel streaming dynamic graph neural network (SDGNN) for modeling continuous-time temporal graphs, which can fully capture the state changes of neighbors and reduce the computational complexity of inference. Under SDGNN, an incremental update component is designed to incrementally update node representation based on the interaction sequence, an inference component is utilized for specific downstream tasks, and a message propagation component is employed to propagate interactive information to the influenced nodes by considering the update time interval, position distance, and influence strengths. Extensive experiments demonstrated that the proposed approach significantly outperforms state-of-the-art methods by capturing more state change information and efficient parallelization.",https://ieeexplore.ieee.org/document/9679075/,2021 IEEE International Conference on Data Mining (ICDM),7-10 Dec. 2021,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1145/3341161.3342939,Strengthening Social Networks Analysis by Networks Fusion,IEEE,Conferences,"The relationship extraction and fusion of networks are the hotspots of current research in social network mining. Most previous work is based on single-source data. However, the relationships portrayed by single-source data are not sufficient to characterize the relationships of the real world. To solve this problem, a Semi-supervised Fusion framework for Multiple Network (SFMN), using gradient boosting decision tree algorithm (GBDT) to fuse the information of multi-source networks into a single network, is proposed in this paper. Our framework aims to take advantage of multi-source networks fusion to enhance the accuracy of the network construction. The experiment shows that our method optimizes the structural and community accuracy of social networks which makes our framework outperforms several state-of-the-art methods.",https://ieeexplore.ieee.org/document/9073317/,2019 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining (ASONAM),27-30 Aug. 2019,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SusTech.2017.8333534,"Strike-alert: Towards real-time, high resolution navigational software for whale avoidance",IEEE,Conferences,"Over the past few years, it has been shown that collisions with ships have become one of the major threats for whales. In order to reduce whale-ship strikes, we have started to develop schemes for identifying areas where whales are likely to be present in order to produce maps updated in real time for ships. Our case study is set in the Mediterranean Sea and our goal is to gather all the data available to improve our knowledge on whale distribution using machine learning techniques. The wide variety of data sources (e.g. very high resolution sensors on­board satellites, acoustical measurements, satellite tagging, direct reports from commercial ships, and social media along with streaming earth observation data) and the use of real time and streaming data will allow the development of high precision, real time maps of the likelihood of whale encounters. Our work seeks to dramatically improve the marine spatial effort by moving beyond ecological/environmental models to harness the full array of data and machine learning techniques. The driving idea is not to just create models of where strikes are likely to be, but to develop high resolution maps of probability of whale encounters in real time using all available data sources.",https://ieeexplore.ieee.org/document/8333534/,2017 IEEE Conference on Technologies for Sustainability (SusTech),12-14 Nov. 2017,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IRASET48871.2020.9092051,Student learning communities’ detection based on betweenness centrality algorithm: Validation and Optimization,IEEE,Conferences,"Real-world observations from streaming data sources such as sensors, click streams, and social media are becoming more common in many areas, including education, with the emergence of universities towards “Digital University” which has already been present for a few years. A new approach is to leverage this behavioural data provided by learners' activity in a learning management system, and use it to group students into communities using standard community detection algorithms, to create qualitative and accessible software systems that will allow teachers to constantly improve their teaching approaches. This paper provides a statistical and a correlation analysis of the results of the method of detection of student learning communities which is based on the notion of centrality, in order to verify its effectiveness, optimize it and improve decision-making.",https://ieeexplore.ieee.org/document/9092051/,"2020 1st International Conference on Innovative Research in Applied Science, Engineering and Technology (IRASET)",16-19 April 2020,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICACITE53722.2022.9823851,Survey on Detection of Face Mask and Social Distancing using Tensorflow,IEEE,Conferences,"The global COVID-19 epidemic has had a significant impact on the entire world, infecting more than eight million people worldwide. The two most important safety measures that can be taken by the people in puplic in order to prevent this disease is wearing face masks and following social distancing. In order to create and keep an eye on this type of environment, we put forward an efficient and convenient CV(ComputerVision)-based approach mainly focusing on the automated real-time surveillance system to identify face masks, voilation and maintain social distancing in all public areas by creating a model on a system that monitors all activity and manages violations using a camera that can be a webcam, a phone, or even a networked device. After Detecting a violation, the system is able to send an alert popup/alert box including real-time pictures of the group or an individual breaching the basic COVID-19 Protocols to a privacy group of Telegram that was made for these type of cause. In this paper we have proposed a system which is a mixture of both modern deep learning algorithm and geometric techniques for making a model satisfying robustness condition. The proposed model is totally based on three aspects detection,validation and report. Hence.this model can solve a problem by allowing us to use it and save time and make the most out of it to decrease the spread of the pandemic and by reducing the total COVID-19 cases. It can be successfully implemented in present scenarios where strong public safety measures are required, such as public meetings, shopping malls, movies, grocery stores, and so on. Since the goal is to minimize the time taken and also the cost by decreasing the number of individuals to inspect in public places at a time,we make use of Automated inspection.",https://ieeexplore.ieee.org/document/9823851/,2022 2nd International Conference on Advance Computing and Innovative Technologies in Engineering (ICACITE),28-29 April 2022,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IACC.2016.111,Sybil Detection in Online Social Networks (OSNs),IEEE,Conferences,"Peer to peer and distributed systems are generally susceptible to sybil attacks. Online social networks (OSNs), due to their fat user base and open access nature are also prone to such attacks. Current state-of-art algorithms for sybil attack detection make use of the inherent social graph created among registered users of OSN service. They rely on the inherent trust relationship among these users. No effort is made to combine other characteristic behavior of sybil users with properties of social graph of OSNs to improve detection accuracy of sybil attacks. Sybil identities are also used as gateways for spreading spam content in OSNs. The proposed approach exploits this behavior of sybil users to improve detection accuracy of existing sybil detection algorithms. In the proposed approach, content generated/published by each user is used along with the topological properties of the social graph of registered users. A machine learning model is used for assigning a fractional value called ""trust value"" which denotes the amount of legitimate content generated by the user. A modification to sybil detection algorithm is proposed which makes use of the trust value of each user to improve the accuracy of detecting a sybil identity. Real dataset from Facebook is crawled and used for analysis and experiments. Analytical results show the superiority of proposed solution. Results are compared with SybilGuard and SybilShield which shows ~14% decrease in false positive rates with very minimal effect on acceptance rate or false negative rate of the sybil detection algorithms. Also, the proposed modification does not affect the performance of existing sybil detection algorithms and can be implemented in a distributed manner.",https://ieeexplore.ieee.org/document/7544900/,2016 IEEE 6th International Conference on Advanced Computing (IACC),27-28 Feb. 2016,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/INFOCOM.2017.8057066,SybilSCAR: Sybil detection in online social networks via local rule based propagation,IEEE,Conferences,"Detecting Sybils in online social networks (OSNs) is a fundamental security research problem as adversaries can leverage Sybils to perform various malicious activities. Structure-based methods have been shown to be promising at detecting Sybils. Existing structure-based methods can be classified into two categories: Random Walk (RW)-based methods and Loop Belief Propagation (LBP)-based methods. RW-based methods cannot leverage labeled Sybils and labeled benign users simultaneously, which limits their detection accuracy, and they are not robust to noisy labels. LBP-based methods are not scalable, and they cannot guarantee convergence. In this work, we propose SybilSCAR, a new structure-based method to perform Sybil detection in OSNs. SybilSCAR maintains the advantages of existing methods while overcoming their limitations. Specifically, SybilSCAR is Scalable, Convergent, Accurate, and Robust to label noises. We first propose a framework to unify RW-based and LBP-based methods. Under our framework, these methods can be viewed as iteratively applying a (different) local rule to every user, which propagates label information among a social graph. Second, we design a new local rule, which SybilSCAR iteratively applies to every user to detect Sybils. We compare SybilSCAR with a state-of-the-art RW-based method and a state-of-the-art LBP-based method, using both synthetic Sybils and large-scale social network datasets with real Sybils. Our results demonstrate that SybilSCAR is more accurate and more robust to label noise than the compared state-of-the-art RW-based method, and that SybilSCAR is orders of magnitude more scalable than the state-of-the-art LBP-based method and is guaranteed to converge. To facilitate research on Sybil detection, we have made our implementation of SybilSCAR publicly available on our webpages.",https://ieeexplore.ieee.org/document/8057066/,IEEE INFOCOM 2017 - IEEE Conference on Computer Communications,1-4 May 2017,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CCST.2016.7815686,System for monitoring natural disasters using natural language processing in the social network Twitter,IEEE,Conferences,"This paper presents the design and implementation of an automated system that allows monitoring the social network Twitter, making a connection to the API, to filter content according to four categories (volcanic, telluric, fires and climatological) which affect Ecuador because of its geographical location, taking into account that these cannot be easily predicted, and stores all tweets in a database for analysis. The filtering process is performed by using the NLTK tool with which the frequency of a word is determined within a tweet, to be classified later in one of the proposed categories. The results for each category are displayed on a web page that contains real-time statistics of the database. This work provides access to information on natural disasters because they are classified.",https://ieeexplore.ieee.org/document/7815686/,2016 IEEE International Carnahan Conference on Security Technology (ICCST),24-27 Oct. 2016,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICMLA.2014.81,TSD: Detecting Sybil Accounts in Twitter,IEEE,Conferences,"Fake identities and user accounts (also called ""Sybils"") in online communities represent today a treasure for adversaries to spread fake product reviews, malware and spam on social networks, and Astroturf political campaigns. State-of-the-art in the defense mechanisms includes Automated Turing Tests (ATTs such as CAPTCHAs) and graph-based Sybil detectors. Sybil detectors in social networks leverage the assumption that Sybils will find it hard to befriend real users which leads to Sybils being connected to each other forming strongly connected sub graphs that can be detected using graph theory. However, the large majority of Sybils are in fact successful in integrating themselves into real user communities (such as the case in Twitter and Facebook). In this paper, we first study and compare the current detection mechanisms of Sybil accounts. We also explore various types of Twitter Sybil accounts detection features with the objective of building an effective and practical classifier. In order to build and evaluate our classifier, we collect and manually label a dataset of twitter accounts, including human users, bots, and hybrid (i.e., Tweets are posted by both human and bots). We believe this Twitter Sybils corpus will help researchers in conducting sound measurement studies. We also develop a browser plug-in (that we call Twitter Sybils Detector or TSD for short) that utilizes our classifier and warns the user about possible Sybil accounts before accessing them, upon clicking on a Twitter account.",https://ieeexplore.ieee.org/document/7033160/,2014 13th International Conference on Machine Learning and Applications,3-6 Dec. 2014,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICPR48806.2021.9413200,Temporal Collaborative Filtering with Graph Convolutional Neural Networks,IEEE,Conferences,"Temporal collaborative filtering (TCF) methods aim at modelling non-static aspects behind recommender systems, such as the dynamics in users' preferences and social trends around items. State-of-the-art TCF methods employ recurrent neural networks (RNNs) to model such aspects. These methods deploy matrix-factorization-based (MF -based) approaches to learn the user and item representations. Recently, graph-neural-network-based (GNN-based) approaches have shown improved performance in providing accurate recommendations over traditional MF -based approaches in non-temporal CF settings. Motivated by this, we propose a novel TCF method that leverages GNNs to learn user and item representations, and RNNs to model their temporal dynamics. A challenge with this method lies in the increased data sparsity, which negatively impacts obtaining meaningful quality representations with GNNs. To overcome this challenge, we train a GNN model at each time step using a set of observed interactions accumulated time-wise. Comprehensive experiments on real-world data show the improved performance obtained by our method over several state-of-the-art temporal and non-temporal CF models.",https://ieeexplore.ieee.org/document/9413200/,2020 25th International Conference on Pattern Recognition (ICPR),10-15 Jan. 2021,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IJCNN48605.2020.9207151,Temporal Fusion Pointer network-based Reinforcement Learning algorithm for Multi-Objective Workflow Scheduling in the cloud,IEEE,Conferences,"Cloud computing is emerging as a deployment promising environment for hosting exponentially increasing scientific and social media applications, but how to manage and execute these applications efficiently depends mainly on workflow scheduling. However, scheduling workflows in the cloud is an NP-hard problem and its existing solutions have certain limitations when applied to real-world scenarios. In this paper, a Temporal Fusion Pointer network-based Reinforcement Learning algorithm for multi-objective workflow scheduling (TFP-RL) is proposed. Through adopting reinforcement learning, our algorithm can discover its heuristics over time by continuous learning according to the rewards resulting from good scheduling solutions. To make more comprehensive scheduling decisions as the influence of historical actions, a novel temporal fusion pointer network (TFP) is designed for the reinforcement learning agent, which can improve the quality of our resulting solutions and the ability of our algorithm in dealing with versatile workflow applications. To decrease convergence time, we train the proposed TFP-RL model independently by the Asynchronous Advantage Actor-Critic method and use its resulting model for scheduling workflows. Finally, under a multi-agent reinforcement learning framework, a Pareto dominance-oriented criterion for reasonable action selection is established for a multi-objective optimization scenario. We first train our TFP-RL model by taking randomly generated workflows as inputs to validate its effectiveness in scheduling, then compare our trained model with other existing scheduling approaches through practical compute- and data-intensive workflows. Experimental results demonstrate that our proposed algorithm outperforms the benchmarking ones in terms of different metrics.",https://ieeexplore.ieee.org/document/9207151/,2020 International Joint Conference on Neural Networks (IJCNN),19-24 July 2020,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SMC42975.2020.9283377,TentNet: Deep Learning Tent Detection Algorithm Using A Synthetic Training Approach,IEEE,Conferences,"Homelessness is a complex social problem and there have been limited attempts to use machine learning algorithms to understand the various issues that public health agencies would like to solve. For instance, it is important for the policy makers to know where homeless populations live so that they can provide necessary services accordingly. This article presents a satellite image tent-detection solution with three deep learning methods that utilize transfer learning from the ResNetV2, InceptionV3, and MobileNetV2 models, trained on ImageNet, attached to a unique architecture referred to as ""TentNet"". The performance of these models are first shown in detecting planes and ships within satellite imagery in previously defined datasets as a baseline. Then, a new dataset is created from a compilation of tents from the xView project to use for testing, along with another dataset of synthetic images from the generative adversarial networks StyleGAN2 and DCGAN for training. After training on a dataset containing only synthetic images for the tents class, the ResNetV2 architecture achieved the highest accuracy of 73.68% when testing on the real satellite imagery.",https://ieeexplore.ieee.org/document/9283377/,"2020 IEEE International Conference on Systems, Man, and Cybernetics (SMC)",11-14 Oct. 2020,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICARSC.2015.19,Testing a Fully Autonomous Robotic Salesman in Real Scenarios,IEEE,Conferences,"Over the past decades, the number of robots deployed in museums, trade shows and exhibitions have grown steadily. This new application domain has become a key research topic in the robotics community. Therefore, new robots are designed to interact with people in these domains, using natural and intuitive channels. Visual perception and speech processing have to be considered for these robots, as they should be able to detect people in their environment, recognize their degree of accessibility and engage them in social conversations. They also need to safely navigate around dynamic, uncontrolled environments. They must be equipped with planning and learning components, that allow them to adapt to different scenarios. Finally, they must attract the attention of the people, be kind and safe to interact with. In this paper, we describe our experience with Gualzru, a salesman robot endowed with the cognitive architecture RoboCog. This architecture synchronizes all previous processes in a social robot, using a common inner representation as the core of the system. The robot has been tested in crowded, public daily life environments, where it interacted with people that had never seen it before nor had a clue about its functionality. Experimental results presented in this paper demonstrate the capabilities of the robot and its limitations in these real scenarios, and define future improvement actions.",https://ieeexplore.ieee.org/document/7101621/,2015 IEEE International Conference on Autonomous Robot Systems and Competitions,8-10 April 2015,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.23919/CISTI.2018.8399260,The Doei mobile app,IEEE,Conferences,"Smartphones have changed the paradigm of how people get services, access information, and connect with friends and family. Mobile devices have transformed the global reality with the mobility and unlimited people connectivity. The purpose of this article is to present the use of a mobile application to facilitate the process of information flow between institutions that provide social assistance to children and adolescents within the Distrito Federal, providing communication with people who are likely to collaborate with donations. For the application development were used the Challenge Based Learning and Scrum methodologies. The Doei app was developed in iOS and is available for free download at the App Store. It is concluded that the awareness of people to engage in the social good is the great challenge of this application, considering that the technology only allows to mediate the interaction of who needs with those who have items to be donated.",https://ieeexplore.ieee.org/document/8399260/,2018 13th Iberian Conference on Information Systems and Technologies (CISTI),13-16 June 2018,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SNPD.2008.103,"The Implementation and Execution Framework of a Role Model Based Language, EpsilonJ",IEEE,Conferences,"In the social reality, objects communicate with each other by means of assuming roles to establish collaboration, and then can adaptively change their roles to obtain other interaction possibilities. To achieve the goal of supporting and realizing such object collaboration and adaptation in the object-oriented technology, especially in Java, a new adaptive role-based model Epsilon and a corresponding language EpsilonJ have been proposed. In this paper, we present the background of adaptive role-based models, and then focus on the design of this Epsilon model and its language. A program written in EpsilonJ must be translated into executable code to execute. We propose a translation scheme of mapping EpsilonJ syntax to the standard Java. With this translation scheme, we implemented a practical syntax translator as a preprocessor of EpsilonJ program, through lexical analysis and parsing. To utilize this translator, we also propose an interactive framework prototype for EpsilonJ program development and execution, and developed as a web-based application tool, by deploying this EpsilonJ translator as a core component. Evaluation shows that our translator can effectively perform transformation in high accuracy, and translated programs can be executed more efficiently than the existing implementation of EpsilonJ.",https://ieeexplore.ieee.org/document/4617382/,"2008 Ninth ACIS International Conference on Software Engineering, Artificial Intelligence, Networking, and Parallel/Distributed Computing",6-8 Aug. 2008,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/WCICA.2000.863203,The advanced control and optimization in tobacco strips processing,IEEE,Conferences,"The paper introduces briefly the tobacco strips processing procedure, pointing out the economical and social benefits. It lists the theories of the online adaptive optimization and the generalized predictive control algorithm. The control and optimization schemes are implemented on an advanced adaptive control and optimization workstation. Through the system reconstruction and software configuration, the workstation can be integrated into the existing computer monitoring system. The schemes aims at increasing the filling density of tobacco strips and controlling the water content percentage on the set-point within the permitted bound.",https://ieeexplore.ieee.org/document/863203/,Proceedings of the 3rd World Congress on Intelligent Control and Automation (Cat. No.00EX393),26 June-2 July 2000,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.23919/CISTI.2018.8399251,The scratch pilot experience in 1st cycle of basic education schools the students perspective,IEEE,Conferences,"The use of technological tools in the teaching and learning process allows students the digital skills essential to their inclusion in active and social life. The integration of programming languages, such as Scratch, contributes to the acquisition of these skills and to this inclusion. The objectives of this communication are to describe the pilot experience of Programming Languages in the 1st CEB, in two classes of the 3rd and 4th years, of the 1st Cycle of Basic Education, in Portugal and present a study that shows the perspective of the students who participated in this experiment.",https://ieeexplore.ieee.org/document/8399251/,2018 13th Iberian Conference on Information Systems and Technologies (CISTI),13-16 June 2018,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CSEET.2011.5876103,The virtual agile enterprise: Making the most of a software engineering course,IEEE,Conferences,"Software engineers are expected to possess a variety of technical, social and personal competencies to be well prepared for real world working environments. At the German University in Cairo (GUC), we were able to guide large groups of students into becoming self managing and self learning communities whose members work together as a team to build large scale software. The students were able to experience many of the challenges in software engineering and develop a variety of related skills in a short period of time. This report describes our detailed experience in reaching such results using agile development practices in a simulated enterprise environment. With an aim to enable educators realise the same success, this report serves as a guide for educators who wish to replicate the process. The resulting successes and the concerns from this unique experience are discussed along with future recommendations.",https://ieeexplore.ieee.org/document/5876103/,2011 24th IEEE-CS Conference on Software Engineering Education and Training (CSEE&T),22-24 May 2011,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ASONAM.2014.6921589,Time-aware reciprocity prediction in trust network,IEEE,Conferences,"Study of reciprocity helps to find influential factors for users building relationships, which greatly facilitates the social behavior understanding in trust networks. In the previous literature, the dynamics of both network structure and user generated content are rarely considered. Our investigation of the available timing information from a real-world network demonstrates that time delay has significant impact on reciprocity formation. In particular, we find structural factors possess greater effect on short-term reciprocity while factors based on user generated content become more important for long-term reciprocity. Based on the empirical analysis, we redefine the reciprocity prediction problem as a learning task specific to each pair of users with different reciprocal delays. Evaluations show that our time-aware framework eventually outperforms the conventional classifiers that ignore the temporal information. Meanwhile, we tackle the problem of concept drift through fitting the evolving trend of features for Naive Bayes and performing periodic retraining for Logistic Regression classifiers, respectively.",https://ieeexplore.ieee.org/document/6921589/,2014 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining (ASONAM 2014),17-20 Aug. 2014,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICSE-NIER55298.2022.9793533,Toward the Analysis of Graph Neural Networks,IEEE,Conferences,"Graph Neural Networks (GNNs) have recently emerged as an effective framework for representing and analyzing graph-structured data. GNNs have been applied to many real-world problems such as knowledge graph analysis, social networks recommendation, and even COVID-19 detection and vaccine development. However, unlike other deep neural networks such as Feedforward Neural Networks (FFNNs), few verification and property inference techniques exist for GNNs. This is potentially due to dynamic behaviors of GNNs, which can take arbitrary graphs as input, whereas FFNNs which only take fixed size numerical vectors as inputs. This paper proposes GNN-Infer, an approach to analyze and infer properties of GNNs by extracting influential structures of the GNNs and then converting them into FFNNs. This allows us to leverage existing powerful FFNNs analyses to obtain results for the original GNNs. We discuss various designs of CNN-lnfer to ensure the scalability and accuracy of the conversions. We also illustrate CNN-Infer on a study case of node classification. We believe that CNN-Infer opens new research directions for understanding and analyzing GNNs. ACM Reference Format: Thanh-Dat Nguyen, Thanh Le-Cong, ThanhVu H. Nguyen, Xuan-Bach D. Le, and Quyet-Thang Huynh. 2022. Toward the Analysis of Graph Neural Networks. In New Ideas and Emerging Results (ICSE-NIER&#x2019;22), May 21-29, 2022, Pittsburgh, PA, USA. ACM, New York, NY, USA, 5 pages. https://doi.org/10.1145/3510455.3512780",https://ieeexplore.ieee.org/document/9793533/,2022 IEEE/ACM 44th International Conference on Software Engineering: New Ideas and Emerging Results (ICSE-NIER),22-24 May 2022,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/WI-IAT.2013.63,Towards Adaptive Multi-agent Planning in Cyber Physical Space,IEEE,Conferences,"Cyber physical space is potentially hosting innumerable spatio-temporal data streams due to increasing use of social networking platforms as real-time information dissemination system and world-wide deployment of sensors for continuous monitoring of physical phenomena. In this paper we address the problem of how cyber physical space can be used for sensing and responding to global calamities such as earthquake. The paper proposes a novel approach of generating online adaptive response in assisting search-and-rescue operations using situation awareness built from real-time heterogeneous spatio-temporal data streams. Online adaptive response is achieved by using agent-based cooperative task sharing and modeling agent decision making as self-organized emergent behavior based on concepts of complex adaptive system. An implemented simulation platform use concepts of situation modeling, domain task network, contract net protocol based negotiation and complex adaptive system to generate adaptive plans. Preliminary simulation results are promising as we have been able to demonstrate a repertoire of self-organized emergent behaviors.",https://ieeexplore.ieee.org/document/6690049/,2013 IEEE/WIC/ACM International Joint Conferences on Web Intelligence (WI) and Intelligent Agent Technologies (IAT),17-20 Nov. 2013,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.23919/FRUCT52173.2021.9435451,Towards Automatic Modelling of Thematic Domains of a National Literature: Technical Issues in the Case of Russian,IEEE,Conferences,"A significant part of modern technologies associated with the development of artificial intelligence systems and digital analytics of diverse data relies on methods of computer text processing (NLP, speech technologies). However, NLP methods are applied primarily to specialized texts, such as scientific literature, technical documentation, news, etc., or social media discourse. Fiction texts are usually left out of the focus of NLP practitioners as the fictional world seems to be of less significance or less “information value” from a practical point of view. Moreover, due to the poetic and metaphorical nature of literary texts, the use of some NLP methods (e.g., topic modelling) for fiction analysis turned out to be more complicated. At the same time, the influence of literature both on the consciousness of individuals and on the formation of social values can hardly be overestimated. Besides, making computers “understand” fiction in a similar way as humans do would be a real challenge for artificial intelligence. The article puts forward the idea of modelling thematic areas of literature on a national scale, which should reveal the main thematic domains of national literature as a whole. It will allow a better understanding of the cultural traits of the national consciousness in a given historical period and contribute to either literary studies or practical tasks. Methodological approaches to determining and modelling themes of literary works are considered, technical difficulties arising in the process are described, and the ways to solve them are suggested. The proposed methodology has been implemented in the design of the corpus of Russian short stories of 1900-1930s and can be applied in the development of artificial intelligence systems that process large volumes of literary texts in any language.",https://ieeexplore.ieee.org/document/9435451/,2021 29th Conference of Open Innovations Association (FRUCT),12-14 May 2021,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/RO-MAN50785.2021.9515348,Towards Out-of-Sight Predictive Tracking for Long-Term Indoor Navigation of Non-Holonomic Person Following Robot,IEEE,Conferences,"The ability to predict the movements of the target person allows a person following robot (PFR) to coexist with the person while still complying with the social norms. In human-robot collaboration, this is an essential requisite for long-term time-dependent navigation and not losing sight of the person during momentary occlusions that may arise from a crowd due to static or dynamic obstacles, other human beings, or intersections in the local surrounding. The PFR must not only traverse to the previously unknown goal position but also relocate the target person after the miss, and resume following. In this paper, we try to solve this as a coupled motion-planning and control problem by formulating a model predictive control (MPC) controller with non-linear constraints for a wheeled differential-drive robot. And, using a human motion prediction strategy based on the recorded pose and trajectory information of both the moving target person and the PFR, add additional constraints to the same MPC, to recompute the optimal controls to the wheels. We make comparisons with RNNs like LSTM and Early Relocation for learning the best-predicted reference path.MPC is best suited for complex constrained problems because it allows the PFR to periodically update the tracking information, as well as to adapt to the moving person’s stride. We show the results using a simulated indoor environment and lay the foundation for its implementation on a real robot. Our proposed method offers a robust person following behaviour without the explicit need for policy learning or offline computation, allowing us to design a generalized framework.",https://ieeexplore.ieee.org/document/9515348/,2021 30th IEEE International Conference on Robot & Human Interactive Communication (RO-MAN),8-12 Aug. 2021,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICAECT49130.2021.9392389,Towards Prediction of Academic School of Students for Real-Time,IEEE,Conferences,"This study presented the five predictive models to predict the student's study school based on their demographics, personal, social, and grade features. Using the Orange stable version 3.23.0 machine learning tool, the authors performed numerous experiments to identify the study place (school) of students. The dataset stored the records and attributes of two senior secondary schools in Portugal. A binary classification problem has solved using the five machine learning algorithms. Also, the training time and testing time of each classifier have been compared. After the preprocessing, 875 instances and 31 attributes were practically analyzed in the Orange workflow environment. This experiential study's findings presented five predicted models and proved that Artificial Neural Network outperformed others with providing the most accuracy of 84.7%. The second winner classifier Random Forest attained an accuracy of 83.7% with 10-folds. Increasing holdout ratios and folds significantly improved the prediction accuracies of each classifier. Also, full cross-validation (leave one out) is recommended for this dataset compared to others. Based on the testing time of 8.2 seconds, the Random Forest classifier has been suggested, and the Neural Network is recommended owing to the highest prediction accuracy. The school predictive models supported identifying the school to be aware of student situations in their academic, personal, and social life.",https://ieeexplore.ieee.org/document/9392389/,"2021 International Conference on Advances in Electrical, Computing, Communication and Sustainable Technologies (ICAECT)",19-20 Feb. 2021,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISAMSR53229.2021.9567829,Towards Software Engineering Perspective for BDI Agent,IEEE,Conferences,"BDI agent is a cognitive architecture to model human reasoning based on a philosophical stream. Based on the survey, the BDI agent is widely adopted in agent based social simulation to model a complex and realistic human behaviour. To support BDI agent development, Prometheus and tactic development framework (extended Prometheus are introduced. Although Prometheus and TDF are introduced to model BDI agent, the modelling and simulating BDI agent is not a straightforward process. As simulation is one of the pillars in IR4.0, it is undoubtedly that there will be a higher demand and usage of agent simulation in real world problem. Hence, there is a need to call for software engineering perspective for BDI agent. Inline with the works on Software Engineering for AI application development, this paper introduces an alternative method to model BDI agent through a software engineering perspective. The requirement model, design and implementation of BDI based on software engineering approach are covered in this paper. A walkthrough example of BDI based a bush fire evacuation simulation has revealed the potential usage of the method.",https://ieeexplore.ieee.org/document/9567829/,"2021 4th International Symposium on Agents, Multi-Agent Systems and Robotics (ISAMSR)",6-8 Sept. 2021,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IISA.2017.8316400,Towards hierarchical cooperative analytics architecture in law enforcement agencies,IEEE,Conferences,"The vast growth in populations and the widespread of metropolitan areas hinder the efforts of crime investigation and inhibit tracking the risk of threats to safety and national security. Information technology has been helping in tracing criminals and terrorists, but with the outspread of social networks and the emergence of the Internet of Things, new challenges have arisen. The Internet is becoming a major part of everyone's daily life. Data representing trustful and suspicious activities is continuously generated and stored everywhere at an unprecedented scale. This extremely large data, which is known as Big Data, may contain critical and helpful information that can be used in detecting, and in many cases, preventing illegal activities, crimes and more importantly terror attacks. Unfortunately, the limitations on the communication between administrative zones, the huge amount of data and the intrinsically unstructured nature of such data makes leveraging its usefulness impractical. In this paper, we propose a hierarchical cooperative analytics architecture that utilizes Big Data Analytics, Ontologies, and Metamodels to facilitate the analysis of data collected from different sources to aid law enforcement agencies' efforts in establishing law and maintaining security.",https://ieeexplore.ieee.org/document/8316400/,"2017 8th International Conference on Information, Intelligence, Systems & Applications (IISA)",27-30 Aug. 2017,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/I2CT.2018.8529342,Troll-Detection Systems Limitations of Troll Detection Systems and AI/ML Anti-Trolling Solution,IEEE,Conferences,"Trolling is a modern-day vice which has manifested itself in the burgeoning virtual world. As bullying shifts from playgrounds to social media, the need of the hour is having anti-trolling softwares in place to combat this malpractice. Many internet companies have taken steps in this direction by creating softwares or applications that can prevent trolling but none have been completely successful. The word anti-trolling has existed as a concept for a long time. It is necessary to make it a concrete reality. Most of the softwares identify abusive words and simply block them but trolls have found ways to circumvent this obstacle by using clever ways. This paper explores the currently existing “Anti-trolling systems”, their methodologies and technological challenges. In particular, we consider machine learning and existing expert systems that detect and prevent trolls. The paper concludes with a discussion of the issues faced by this technology, the current functioning and a few suggestions along with a modified architecture and the vision of a trolling-free internet.",https://ieeexplore.ieee.org/document/8529342/,2018 3rd International Conference for Convergence in Technology (I2CT),6-8 April 2018,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICC40277.2020.9148767,Trust Computational Heuristic for Social Internet of Things: A Machine Learning-based Approach,IEEE,Conferences,"The Internet of Things (IoT) is an evolving network of billions of interconnected physical objects, such as, numerous sensors, smartphones, wearables, and embedded devices. These physical objects, generally referred to as the smart objects, when deployed in real-world aggregates useful information from their surrounding environment. As-of-late, this notion of IoT has been extended to incorporate the social networking facets which have led to the promising paradigm of the `Social Internet of Things' (SIoT). In SIoT, the devices operate as an autonomous agent and provide an exchange of information and services discovery in an intelligent manner by establishing social relationships among them with respect to their owners. Trust plays an important role in establishing trustworthy relationships among the physical objects and reduces probable risks in the decision making process. In this paper, a trust computational model is proposed to extract individual trust features in a SIoT environment. Furthermore, a machine learning-based heuristic is used to aggregate all the trust features in order to ascertain an aggregate trust score. Simulation results illustrate that the proposed trust-based model isolates the trustworthy and untrustworthy nodes within the network in an efficient manner.",https://ieeexplore.ieee.org/document/9148767/,ICC 2020 - 2020 IEEE International Conference on Communications (ICC),7-11 June 2020,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CIG.2011.6031981,Tutorials: Cultural Algorithms: Incorporating social intelligence into virtual worlds,IEEE,Conferences,These tutorials discuss the following: cultural algorithms: incorporating social intelligence into virtual worlds; automating the analysis of evolved agents; experience-driven procedural content generation; and experimentation in CI-affected games research.,https://ieeexplore.ieee.org/document/6031981/,2011 IEEE Conference on Computational Intelligence and Games (CIG'11),31 Aug.-3 Sept. 2011,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/UBMK.2017.8093420,Twitter fake account detection,IEEE,Conferences,"Social networking sites such as Twitter and Facebook attracts millions of users across the world and their interaction with social networking has affected their life. This popularity in social networking has led to different problems including the possibility of exposing incorrect information to their users through fake accounts which results to the spread of malicious content. This situation can result to a huge damage in the real world to the society. In our study, we present a classification method for detecting the fake accounts on Twitter. We have preprocessed our dataset using a supervised discretization technique named Entropy Minimization Discretization (EMD) on numerical features and analyzed the results of the Naïve Bayes algorithm.",https://ieeexplore.ieee.org/document/8093420/,2017 International Conference on Computer Science and Engineering (UBMK),5-8 Oct. 2017,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICIMTech53080.2021.9534945,Twitter-Based Text Classification Using SVM for Weather Information System,IEEE,Conferences,The popularity of social media amplified the amount of text data that is used to enrich text classification research with machine learning approach. The modernization in weather information system is needed to produce real-time weather information using the involvement of Twitter users. GetOldTweets was used to cover data acquisition from Twitter. Support Vector Machine (SVM) was deployed as a machine learning model for text classification. The result was delivered in Esri Maps features. It facilitated a social sensing for Indonesia weather information system by using Twitter posts from Indonesian users with 93% accuracy.,https://ieeexplore.ieee.org/document/9534945/,2021 International Conference on Information Management and Technology (ICIMTech),19-20 Aug. 2021,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCIDS.2017.8272631,Twitter-user recommender system using tweets: A content-based approach,IEEE,Conferences,"With the advent of the internet into our everyday lives, online social networks such as Facebook and Twitter have taken up a major role in networking, information deployment and entertainment. As of 2017, Twitter's outreach is over 317M monthly active users generating more than 320M tweets every day, thus making it one of the fastest information deployment mediums of this era. In order to aid data distribution without causing a glut of information to the users, we develop a recommender system focusing on a vital aspect of social media - relationships among users, addressing a popular problem of users - who to follow/befriend? By choosing the right accounts and users to follow, the sources of information can be controlled as desired. The information collected from the most recent tweets of a user is used to find other users whose recent tweets contain similar information, ensuring there is at least one mutual friend among users. By making use of the continuous and real time updating of data on social networks, we develop a method to ensure our training sets consist of relevant information for classification, thus preserving accuracy while reducing training set sizes for probabilistic learning models. We use two algorithms to detect tweets of common topics, namely a Noun Phrase detector and a Naïve Bayes Text (Topic) Classifier and further compare their complexity and accuracy. The Naive Bayes Classifier, despite being probabilistic, functioned well with a relatively small training set. This is only with the exception of Twitter as it is a real-time updating framework. Exact matches were hard to obtain with the Noun phrase detector, as we are going only one level deep due to limited compute. However, when matches were found, it is upto 90% accurate. Experiments on tweets of random public users have found that Naive Bayes Classifier with a small but recent training data set can work as well as or better than a Collaborative filter without the assumptions of the Collaborative model.",https://ieeexplore.ieee.org/document/8272631/,2017 International Conference on Computational Intelligence in Data Science(ICCIDS),2-3 June 2017,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/KSE.2015.63,Uniform Detection in Social Image Streams,IEEE,Conferences,"Social media mining from Internet has been an emerging research topic. The problem is challenging because of massive data contents from various sources, especially image data from user upload. In recent years, dictionary learning based image classification has been widely studied and gained significant success. In this paper, we propose a framework for automatic detection of interested uniforms in image streams from social networks. The systems is composed of a powerful feature extraction module based on dense SIFT feature and a state-of-the-art discriminative dictionary learning approach. Beside that, a parallel implementation of feature extraction is deployed to make the system work real time. An extensive set of experiments has been conducted on four real-life datasets. The experimental results show that we can obtain the detection rate up to 100% on some datasets. We also get real time performance with a speed of image stream of about 40 images per second. The framework can be applied to emerging applications such as uniform detection, automated image tagging, content base image retrieval or online advertisement based on image content.",https://ieeexplore.ieee.org/document/7371779/,2015 Seventh International Conference on Knowledge and Systems Engineering (KSE),8-10 Oct. 2015,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/WACV51458.2022.00391,Unveiling Real-Life Effects of Online Photo Sharing,IEEE,Conferences,"Social networks give free access to their services in exchange for the right to exploit their users&#x2019; data. Data sharing is done in an initial context which is chosen by the users. However, data are used by social networks and third parties in different contexts which are often not transparent. In order to unveil such usages, we propose an approach which focuses on the effects of data sharing in impactful real-life situations. Focus is put on visual content because of its strong influence in shaping online user profiles. The approach relies on three components: (1) a set of visual objects with associated situation impact ratings obtained by crowdsourcing, (2) a corresponding set of object detectors for mining users&#x2019; photos and (3) a ground truth dataset made of 500 visual user profiles which are manually rated per situation. These components are combined in LERV UP, a method which learns to rate visual user profiles in each situation. LERV UP exploits a new image descriptor which aggregates object ratings and object detections at user level and an attention mechanism which boosts highly-rated objects to prevent them from being overwhelmed by low-rated ones. Performance is evaluated per situation by measuring the correlation between the automatic ranking of profile ratings and a manual ground truth. Results indicate that LERV UP is effective since a strong correlation of the two rankings is obtained. A practical implementation of the approach in a mobile app which raises user awareness about shared data usage is also discussed.",https://ieeexplore.ieee.org/document/9706741/,2022 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV),3-8 Jan. 2022,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICTAI.2017.00043,User Interest Propagation and Its Application in Recommender System,IEEE,Conferences,"User interest prediction plays an important role in online services, such as electronic commerce, social network, and online advertising. This information is not always explicitly available for online systems. To identify the interests, existing studies merely focused on modeling the relationships between user generated contents and user interests. However, the interests of a user should not only be inferred by user generated contents, but also the relationships between users which might imply the user interests. In this paper, our goal is to unveil the true interests of users based on user generated contents as well as relationships between users. We built a probabilistic user interests model and proposed a user interests propagation algorithm (UIP) to tackle this problem. A factor graph-based approach is utilized to estimate the distribution of the interests of users. We conducted experiments on real-world datasets to validate the effectiveness of the model. Furthermore, we integrated our UIP algorithm with the classical matrix factorization algorithm to deal with the rating prediction task. Experimental studies confirm the superiority of the proposed approach.",https://ieeexplore.ieee.org/document/8371946/,2017 IEEE 29th International Conference on Tools with Artificial Intelligence (ICTAI),6-8 Nov. 2017,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AIVR46125.2019.00048,Using CNNs For Users Segmentation In Video See-Through Augmented Virtuality,IEEE,Conferences,"In this paper, we present preliminary results on the use of deep learning techniques to integrate the user's self-body and other participants into a head-mounted video see-through augmented virtuality scenario. It has been previously shown that seeing user's bodies in such simulations may improve the feeling of both self and social presence in the virtual environment, as well as user performance. We propose to use a convolutional neural network for real time semantic segmentation of users' bodies in the stereoscopic RGB video streams acquired from the perspective of the user. We describe design issues as well as implementation details of the system and demonstrate the feasibility of using such neural networks for merging users' bodies in an augmented virtuality simulation.",https://ieeexplore.ieee.org/document/8942263/,2019 IEEE International Conference on Artificial Intelligence and Virtual Reality (AIVR),9-11 Dec. 2019,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICTAI.2017.00035,Using Deep Learning for Community Discovery in Social Networks,IEEE,Conferences,"Community detection is an important task in social network analysis. Existing methods typically use the topological information alone, and ignore the rich information available in the content data. Recently, some researchers have noticed that user profiles can also benefit to community detection, and hence the combination of topology and node contents has become a new hot topic. Some methods using both topology and content have been proposed. However, they often suffer from two drawbacks: 1) they cannot extract a potential deep representation of the network; 2) they cannot automatically weight different information sources with adequate balance parameters. To overcome these issues, we propose a deep integration representation (DIR) algorithm via deep joint reconstruction, which is motivated by the similarity between deep feedforward auto-encoders and spectral clustering in terms of matrix reconstruction. Thanks to spectral clustering which is one of the best community detection methods, the proposed new method is also good at community discovery task. In addition, DIR has further benefit because it not only provides a nonlinear and deep representation of the network, but also learns the most suitable balance between different components automatically. We compare the proposed new approach with nine state-of-the-art community detection methods on eight real relatively large networks. The experimental results show the definite superiority of this new approach.",https://ieeexplore.ieee.org/document/8371938/,2017 IEEE 29th International Conference on Tools with Artificial Intelligence (ICTAI),6-8 Nov. 2017,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IPDPSW.2014.52,Using Physical Stigmergy in Decentralized Optimization under Multiple Non-separable Constraints: Formal Methods and an Intelligent Lighting Example,IEEE,Conferences,"In this paper, a distributed asynchronous algorithm for intelligent lighting is presented that minimizes collective power use while meeting multiple user lighting constraints simultaneously and requires very little communication among agents participating in the distributed computation. Consequently, the approach is arbitrarily scalable, adapts to exogenous disturbances, and is robust to failures of individual agents. This algorithm is an example of a decentralized primal-space algorithm for constrained non-linear optimization that achieves coordination between agents using stigmergic memory cues present in the physical system as opposed to explicit communication and synchronization. Not only does this work make of stigmergy, a property first used to describe decentralized decision making in eusocial insects, but details of the algorithm are inspired by classic social foraging theory and more recent results in eusocial-insect macronutrient regulation. This theoretical analysis in this paper guarantees that the decentralized stigmergically coupled system converges to within a finite neighborhood of the optimal resource allocation. These results are validated using a hardware implementation of the algorithm in a small-scale intelligent lighting scenario. There are other real-time distributed resource allocation applications that are amenable to these methods, like distributed power generation, in general, this paper means to provide proof of concept that physical variables in cyberphysical systems can be leveraged to reduce the communication burden of algorithms.",https://ieeexplore.ieee.org/document/6969416/,2014 IEEE International Parallel & Distributed Processing Symposium Workshops,19-23 May 2014,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISPDC.2016.21,Using Social Media and the Mobile Cloud to Enhance Emergency and Risk Management,IEEE,Conferences,"All emergency events require fast response and decisions based on first-hand information. This paper presents a mobile cloud service that makes the best use of social media applications, such as Twitter, in emergency and risk management. Risk and emergency teams can receive in matter of seconds data that can inform their decisions when an emergency has affected areas under their management. The proposed system allows users to provide on-the-ground information regarding such an event, as well as early notification to people who are in the vicinity of the location of an emergency situation. The system matches users' requests to a set of pre-defined labels that will help rescuers to understand the situation more clearly. The service is implemented and tested with Android devices and a cloud-computing instance hosted on an Amazon platform. A cloud-based tool is also provided for risk and emergency management teams to interact with users' requests. The experimental results show that the system enhances the early detection of emergencies.",https://ieeexplore.ieee.org/document/7904275/,2016 15th International Symposium on Parallel and Distributed Computing (ISPDC),8-10 July 2016,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/EDUNINE.2018.8450969,Using text mining to evaluate student interaction in virtual learning environments,IEEE,Conferences,"The field of education has been affected by globalization and the constant increase of online courses. The high number of students enrolled in these learning environments and their constant interaction with platforms generate a large amount of data that is difficult to handle with traditional methods of data analysis. The permanence of students in these courses poses challenges aimed at raising their level of commitment and motivation. Several articles with this approach have been identified in the literature analyzed in this work. Some of them are related to the application of text mining techniques aimed at analyzing the interaction of students in these environments. This interaction is based on entries included in discussion forums, emails or interaction in social networks. In this article, we explore the interaction of students through text mining techniques in different student interaction environments in a massive open online course (MOOC). The research focuses on the calculation and analysis of the frequency of terms, the analysis of concordances and groupings in n-grams.",https://ieeexplore.ieee.org/document/8450969/,2018 IEEE World Engineering Education Conference (EDUNINE),11-14 March 2018,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ZINC.2018.8448645,VR Job Interview Simulator: Where Virtual Reality Meets Artificial Intelligence for Education,IEEE,Conferences,"Nowadays, people have to face many challenges when going to an interview: introversion, insecurity, lack of technical or social skills. Training becomes highly recommended in order to improve interview performances. The current paper presents VR Job, an application which proposes an innovative way of training for an interview. By combining the advantages of various technologies, such as virtual reality and chatbots, our application creates an interactive way of helping software engineers train for their interviews. Emotion recognition techniques are also included, helping provide accurate feedback for the user.",https://ieeexplore.ieee.org/document/8448645/,2018 Zooming Innovation in Consumer Technologies Conference (ZINC),30-31 May 2018,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CVIDL51233.2020.00-85,Vehicle automatic driving system based on embedded and machine learning,IEEE,Conferences,"With the rapid development of Chinnes highway transportation industry, the problem of road traffic safety has become increasingly prominent. Highway passenger transport accidents are generally fatal and fatal accidents. Traffic accidents not only cause enormous economic losses to transport enterprises, but also have a very bad social impact on local highway transport management departments, which has even become a new social instability factor. This is mainly because there are many problems in autopilot technology, such as low recognition accuracy, poor real-time performance, weak anti-interference ability and so on. However, embedded technology and machine learning can solve these problems well, so autopilot technology will become the mainstream in the future. Firstly, this paper analyses the importance of autopilot technology. Then this paper analyses the machine learning target recognition, vehicle automatic driving system model and vehicle automatic driving system flow. Finally, this paper designs the function of autopilot system.",https://ieeexplore.ieee.org/document/9270523/,"2020 International Conference on Computer Vision, Image and Deep Learning (CVIDL)",10-12 July 2020,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AVSS.2007.4425279,Verbal aggression detection in complex social environments,IEEE,Conferences,"The paper presents a knowledge-based system designed to detect evidence of aggression by means of audio analysis. The detection is based on the way sounds are analyzed and how they attract attention in the human auditory system. The performance achieved is comparable to human performance in complex social environments. The SIgard system has been deployed in a number of different real-life situations and was tested extensively in the inner city of Groningen. Experienced police observers have annotated ~1400 recordings with various degrees of shouting, which were used for optimization. All essential events and a small number of nonessential aggressive events were detected. The system produces only a few false alarms (non-shouts) per microphone per year and misses no incidents. This makes it the first successful detection system for a non-trivial target in an unconstrained environment.",https://ieeexplore.ieee.org/document/4425279/,2007 IEEE Conference on Advanced Video and Signal Based Surveillance,5-7 Sept. 2007,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AIIoT54504.2022.9817374,Violence Detection Using Computer Vision Approaches,IEEE,Conferences,"Violent crime has always been a major social problem. The rise of violent behavior in public areas can be attributed to a variety of factors. Greed, frustration, and hostility among individuals, as well as social and economic anxieties, are the primary causes of increased violence. It is critical to protect our possessions, as well as our lives, from threats such as robbery or homicide. It is impossible to prevent crime and violent acts unless brain signals are studied and a certain pattern deduced from criminal ideas is detected in real-time. Due to its technological viability, it has yet to be realized. However, We can identify violent activity in public spaces by using the concepts of computer vision (a subfield of deep learning) technology. The goal of this project is to build a real-time violent activity monitoring system that will be capable of detecting violence very quickly and efficiently. The public of any city can benefit from it, as it will allow the people of the law enforcement department to take necessary actions to prevent violent activities. When the system is implemented, it will be able to detect the speed of the movements of people and their distances from other people walking in public places by using cameras. The system will mainly detect the speed of hand and leg movements of a person who will be very close to another person. If anyone is identified as a violent maker, the server-side of the system will notify the people who will be responsible for preventing violence in a very short time. The system was built using the concepts of computer vision and neural networks. The system has been developed and tested initially on the personal computing devices of the system developers. This system is very easy to design and develop, making it very easy to use for any kind of public area surveillance. At the same time, the system gives its desired output due to its high accuracy.",https://ieeexplore.ieee.org/document/9817374/,2022 IEEE World AI IoT Congress (AIIoT),6-9 June 2022,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICAC54203.2021.9671133,Virtual Makeover and Makeup Recommendation Based on Personal Trait Analysis,IEEE,Conferences,"The utilization of facial makeup is an important attribute in modern society as a means of self-expression, and as a method to feel more confident during social interactions. A makeover has become a necessity when attending divergent functions, and makeup used on diverse occasions varies in style. Choosing the perfect makeup that best suits a person is challenging unless they have years of expertise with cosmetics. This paper proposes a “Virtual Makeover and Makeup Recommendation System” to eliminate the need to be concerned about the appearance after applying makeup. The proposed system enables real-time makeup simulation in an Augmented Reality (AR) environment and recommends makeup styles considering skin tone, colour of clothing and hair, type of clothing and occasion to be attended with makeup. The personal traits of a user would be automatically detected and processed to generate recommendations for makeup products, namely lipstick, foundation and eyeshadow. Complications of wasting makeup products and time, and cleaning makeup can be mitigated by using a real-time makeup simulation system. Recommendations generated by the application assist the users to decide on makeup styles and provides a better user experience. The proposed system is developed with the aid of Deep Learning (DL) algorithms.",https://ieeexplore.ieee.org/document/9671133/,2021 3rd International Conference on Advancements in Computing (ICAC),9-11 Dec. 2021,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICAC51239.2020.9357281,VirtualPT: Virtual Reality based Home Care Physiotherapy Rehabilitation for Elderly,IEEE,Conferences,"This paper describes the development of Personal computer based Virtual Reality home-care Physiotherapy system aimed for rehabilitating full body function in elders. VirtualPT is a true virtual reality platform where the environment is completely replaced by a virtual reality platform based on the mental condition of the person at the time. While doing the home-based prescribed physiotherapy exercises, the key health metrics are continuously monitored and tracked by combining the immersive Virtual Reality with the wearable VirtualPT Sensor kit. Virtual Reality combined with 3D motion capture lets real time movements to be accurately translated onto the virtual reality avatar that can be viewed in a virtual environment to assist physiotherapist to add exercises to the system easily. This ultimate virtual reality Physiotherapy assistant avatar is used to provide guidance to elders at home, to demonstrate and assist elders in adhering to the prescribed exercises. As a significant aspect of social interactions, mirroring of movements has been added to focus on whether the elder is able to accurately follow the movements of avatar. Furthermore, the insightful dashboard offers the elders and physiotherapists an interactive platform through virtual reality capabilities. VirtualPT physiotherapy system is cost effective and makes recovery and more convenient to elders at home while the participatory and immersive nature of Virtual Reality offers a unique realistic quality that is not generally existing in clinical-based physiotherapy. When looking at the broader concept of VirtualPT; continuity of care, integration of services, quality of life and access are equally important criteria which add more value.",https://ieeexplore.ieee.org/document/9357281/,2020 2nd International Conference on Advancements in Computing (ICAC),10-11 Dec. 2020,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISDFS.2016.7473536,Visualization of the social bot's fingerprints,IEEE,Conferences,"As the number of social media users increases for platforms such as Twitter, Facebook, and Instagram, so does the number of bot or spam accounts on these platforms. Typically, these bots or spam accounts are automated programmatically using the social media site's API and attempt to convey or spread a particular message. Some bots are designed for marketers trying to sell products or attract users to new sites. Other types of bots are much more malicious and disseminate misinformation that harms or tricks users. Such bots (fake accounts) may lead to serious consequences, as people's social network has become one of the determining factors in their general decision making. Therefore, these accounts have the potential to influence people's opinions drastically and hence real life events as well. Through different machine learning techniques, researchers have now begun to investigate ways to detect these types of malicious accounts automatically. To successfully differentiate between real accounts and bot accounts, a comprehensive analysis of the behavioral patterns of both types of accounts is required. In this paper, we investigate ways to select the best features from a data set for automated classification of different types of social media accounts (ex. bot versus real account) via visualization. To help select better feature combinations, we try to visualize which features may be more effective for classification using self-organizing maps.",https://ieeexplore.ieee.org/document/7473536/,2016 4th International Symposium on Digital Forensic and Security (ISDFS),25-27 April 2016,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICAC51239.2020.9357146,Water Quality Index Based Prediction of Ground Water Properties for Safe Consumption,IEEE,Conferences,"Water as one crucial element for the survival of human beings is necessary to be handled with care. With a 59% of the Sri Lankan population depending themselves on this indispensable element, the authorities and the people must heed the importance of safe consumption to avoid severe consequences like Chronic Kidney Disease (CKD). In an attempt to address this social predicament, a smart device was fashioned as an initiative to predict safe consumption of groundwater and, to oblige and upskill the users to identify the quality of a groundwater sample in real-time. With the inclusion of machine learning techniques, the implementation was done by predicting the Water Quality Index (WQI) which is a single numeric index that mirrors the overall quality of any water sample, with an accuracy of 97.82%. In addition, two more serviceable functionalities to predict possibilities of CKD outbreak and forecasting water quality parameters were also implemented with accuracies of 76.99% and 92% respectively. The sole of this research relies on the hardware device that embeds a set of sensors which accompanies the individual functionalities. The readings and outputs will be displayed through the mobile application which is real-time and of high performance with a friendly user-interface.",https://ieeexplore.ieee.org/document/9357146/,2020 2nd International Conference on Advancements in Computing (ICAC),10-11 Dec. 2020,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICME51207.2021.9428255,Weakly-Supervised Online Hashing,IEEE,Conferences,"With the rapid development of social websites, recent years have witnessed an explosive growth of social images with user-provided tags. Most existing hashing methods for social image retrieval are batch-based which may violate the nature of social images, i.e., social images are usually generated periodically or collected in a stream fashion. Although there exist many online hashing methods, they either adopt unsupervised learning which ignore the relevant tags, or are designed in the supervised manner which needs high-quality labels. In this paper, to overcome the above limitations, we propose a new method named Weakly-supervised Online Hashing (WOH). In order to learn high-quality hash codes, WOH exploits the weak supervision, i.e., tags, by considering the semantics of tags and removing the noise. Besides, we develop a discrete online optimization algorithm, which is efficient and scalable. Extensive experiments conducted on two real-world datasets demonstrate the superiority of WOH.",https://ieeexplore.ieee.org/document/9428255/,2021 IEEE International Conference on Multimedia and Expo (ICME),5-9 July 2021,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1049/ic:19960648,What agents aren't: a discussion paper,IET,Conferences,"A standard criticism of artificial intelligence is that it does not direct itself to real problems: and yet, we argue that AI research on intelligent agents, and in particular, work on agent architectures and agent oriented programming, is aimed at developing software systems that have precisely these attributes. Specifically, the term agent is used to refer to a system that is: autonomous in that it operates independently; reactive; pro-active in that it is capable of taking the initiative, and is not driven solely by events; and social in that it can communicate, cooperate, and negotiate with other agents in order to achieve its tasks. A particularly interesting tradition in intelligent agent research is that of viewing agents as rational systems. We argue that this notion of agents is of importance to software engineering researchers, who require tools to design and implement particularly complex computer systems.",https://ieeexplore.ieee.org/document/573230/,IEE Colloquium on Intelligent Agents and Their Applications (Digest No: 1996/101),30-30 April 1996,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/FG.2019.8756520,When Computers Decode your Social Intention,IEEE,Conferences,"In this demo session, we will propose our framework that is based on our paper [1] . In real time, we proposed to analyze the trajectories of the human arm to predict social intention (personal or social intention). The trajectories of different 3D markers acquired by Mocap system are defined in shape spaces of open curves, thus analyze in a Riemannian manifold. The results obtained in the experiments on a new dataset show an average recognition of about 68% for the proposed method, which is comparable with the average score produced by human evaluation. The experimental results show also that the classification rate could be used to improve social communication between human and virtual agents. To the best of our knowledge, this is the first demo in real time, which uses computer vision techniques to analyze the effect of social intention on motor action for improving the social communication between human and avatar. The main goal is to categorize the user intention among two classes denote {personal, social}. This experimentation contains 3 parts: a) data acquisitions and a learning step; b) classification; c) Kinematic analysis of the evolution of subjects to interact with the avatar. To successfully drive our study, all the using scripts are writing under Matlab and C/C++. Then the using equipments are: 1) Qualisys motion capture camera (qualisys system). The qualisys system is delivered with a desk computer with 8 GB, a processor Intel core i7-4770k (8 CPUs) at 3.5 GHz. The frequency of those cameras can varies from 100 to 500 Hz. A black glove equipped with infrared reflective markers, all those equipments are also provided by qualisys system. 2) A Matlab software (version R2014a) installed on a desk computer (qualisys system); the Qualisys system provide a specific driver that allow to couple all the Matlab scripts with their system. Thus, it is possible to command all the cameras directly from Matlab for real time analysis, see Fig. 1 .",https://ieeexplore.ieee.org/document/8756520/,2019 14th IEEE International Conference on Automatic Face & Gesture Recognition (FG 2019),14-18 May 2019,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICED.2016.7804679,Wide scope and fast websites phishing detection using URLs lexical features,IEEE,Conferences,"Phishing is a considerable problem differs from the other security threats such as intrusions and Malware which are based on the technical security holes of the network systems. The weakness point of any network system is its Users. Phishing attacks are targeting these users depending on the trikes of social engineering. Despite there are several ways to carry out these attacks, unfortunately the current phishing detection techniques cover some attack vectors like email and fake websites. Therefore, building a specific limited scope detection system will not provide complete protection from the wide phishing attack vectors. This paper develops detection system with a wide protection scope using URL features only which is relying on the fact that users directly deal with URLs to surf the internet and provides a good approach to detect malicious URLs as proved by previous studies. Additionally, Anti-phishing solutions can be positioned at different levels of attack flow where most researchers are focusing on client side solutions which turn to add more processing overhead at the client side and lead to losing the trust and satisfaction of the users. Nowadays many organizations make centralized protection of spam filtering. This paper proposes a system which can be integrated into such process in order to increase the detection performance in a real time. The simulation results of the proposed system showed a phishing URLs detection accuracy with 93% and provided online process of a single URL in average time of 0.12 second.",https://ieeexplore.ieee.org/document/7804679/,2016 3rd International Conference on Electronic Design (ICED),11-12 Aug. 2016,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICSECS52883.2021.00105,Word Embedding based Event Identification and Labeling of Connected Events from Tweets,IEEE,Conferences,"Events conceived as facts which are fine grained entities that happen around us such as completing graduation, birthday celebration, and death of an individual. Events are regarded as happening in a certain place, during a particular interval of time which can be occurred planned or unplanned way. Social media is a platform where users share their attending events with others. In this paper, we present a novel machine learning based approach to identify events from social media, i.e., Twitter, by using Bidirectional Encoder Representations from Transformers (BERT) based word embedding technique. Events might be connected with each other which might have real life implications such as identifying causal-effect, investigating criminal activities, etc. We also demonstrate a mechanism which can organize relevant events into a cluster based on their spatiotemporal properties. Later, we develop an unsupervised connected event labeling technique by using BERT word embedding approach by exploiting its semantic strength from the content of tweets. Our model shows an outstanding performance which has an accuracy of 91%. We also compare our approach with two competitive baseline techniques (i.e., word2vec and tf-idf) to identify events and our model shows better performance (on an average 5% better accuracy) than that of those baseline models.",https://ieeexplore.ieee.org/document/9537046/,2021 International Conference on Software Engineering & Computer Systems and 4th International Conference on Computational Science and Information Management (ICSECS-ICOCSIM),24-26 Aug. 2021,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.23919/iLRN47897.2020.9155122,Work-in-Progress—Virtual Agents in Teaching: A Study of Human Aspects,IEEE,Conferences,"Students require human intelligence and social interaction in the form of academic assistance at different times of their study period. Their desire to get and find academic assistance varies and is dependent on many factors such as attendance mode, personal situation, semester timetables, and assessment due dates. Providing students with access to this expertise when it is needed and to large numbers of students is problematic. Virtual Agents (VAs) seek to provide a technology-enabled social element to encourage and provide timely support to aid students' learning. We have implemented 4 unit-specific VIRtual Teaching Assistants (VIRTAs) across 2 universities to provide support to answer student's questions about various aspects of the unit. In this work-in-progress paper, we present the usage patterns of students to show how many questions were asked by students and at what point of time in the semester the questions were asked addressing the desire to find assistance when required from VIRTA.",https://ieeexplore.ieee.org/document/9155122/,2020 6th International Conference of the Immersive Learning Research Network (iLRN),21-25 June 2020,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/BigMM.2019.00-22,XRA-Net Framework for Visual Sentiments Analysis,IEEE,Conferences,"The exponential growth of social media has motivated people to express themselves in various forms. Visual media is one of the most effective and popular ways of conveying sentiments or opinions on the web as people keeps on uploading millions of photos on famous social networking sites. Hence, Visual Sentiment Analysis is instrumental in monitoring an overview of the broader public consensus behind a specific topic or issue. This work proposes a deep learning-based architecture XRA-Net (Xception Residual Attention based Network) for visual sentiment analysis. Moreover, the performance of the XRA-Net architecture is evaluated on the publicly available real-world Twitter I dataset, which is further composed of three subsets of the dataset: 3-agree, 4-agree, and 5-agree. The accuracy achieved on these datasets are: 79.2%, 81.2%, and 86.4% respectively, which shows that the proposed architecture has outperformed the state-of-the-art results on all the three subsets of Twitter I dataset as it can focus on the most informative features in an input image, which boosts the visual sentiment analysis process.",https://ieeexplore.ieee.org/document/8919315/,2019 IEEE Fifth International Conference on Multimedia Big Data (BigMM),11-13 Sept. 2019,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ComPE53109.2021.9752229,YOLOv3 based Real Time Social Distance Violation Detection in Public Places,IEEE,Conferences,"The prevalent COVID 19 pandemic is incessantly taking toll on the lives of people throughout the world. Moreover, the dearth of effectual remedies has caused an expeditious rise in the total COVID 19 cases. Though vaccines have been developed, the enormous task of vaccinating a large population is still challenging. Also, as new variants emanate, the resilience from infections conceivably decreases. Hence, it&#x2019;s most unlikely that we&#x2019;ll achieve herd immunity globally so soon. Thus, since the transmission of COVID causing coronavirus roots mainly to social proximity between people, it is necessary to stringently comply to the non pharmaceutical preventive measures of wearing masks and maintaining physical distancing. Howbeit, it has evidently been found that people are being lethargically ignorant to the social distancing norms with passing time. Hence, an autonomous mechanism intended at social distancing violation detection through monitoring of people is needed to be introduced at an authority level. In this paper, the implementation of YOLO Object detection transfer learning process has been used for accomplishing this aim of real time detection of social distancing violation. Our social distance prediction approach uses a pre-trained YOLOv3 object tracking algorithm for identifying people in an input video stream. A Distance estimation algorithm is further used, that works by computing euclidean distance between the centroids of each pair of detected people. This approach highlights the people violating the social distancing criteria as well as calculates the number of times social distancing gets violated as any two people get closer than a set threshold value of minimum permissible distance. A number of experiments on various pre-recorded video streams has been conducted in order to estimate the viability of this method. Through experimental outcomes, it has been found that this YOLO based object detection method with the proposed social distance prediction algorithm produces favourable results for tracking social distancing in public spaces.",https://ieeexplore.ieee.org/document/9752229/,2021 International Conference on Computational Performance Evaluation (ComPE),1-3 Dec. 2021,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/VRW50115.2020.00132,"[DC] Quality, Presence, and Emotions in Virtual Reality Communications",IEEE,Conferences,"This doctoral thesis looks for the identification and evaluation of the factors that allow to improve the QoE of a remote client in telepresence and virtual reality scenarios. Specifically, quality and socioemotional concepts such as social and spatial presence, empathy, and emotions of being in a completely different place, as well as communicate and interact with people who are in that place. The main goals of my research are the analysis of the methodologies to evaluate video quality and socioemotional concepts, the implementation of additional tools using ML techniques to improve the QoE, and finally, experiments in real use cases.",https://ieeexplore.ieee.org/document/9090552/,2020 IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops (VRW),22-26 March 2020,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/INES.2015.7329762,[Front cover],IEEE,Conferences,The following topics are dealt with: learning; Web; urban water-supply system; IP Core; DCI approach; real-time sensor network; linked open data source; process mining; image coding; deep neural network architecture; human computer interaction; social human-robot interaction; VANET; authorized V2V communication; MIMO system; surgical robotics; ontologies; genetic algorithm; image reconstruction; mobile robot; artificial neural network; fuzzy reasoning; heat exchanger; fuzzy controller design; mobile device; human machine interface design; decision support system; data mining technique; discrete-time SISO system; augmented reality; visual analysis; content management system; Androids; nonlinear MPC; collaborative filtering; recommendation; wireless sensor networks;; humidity control; temperature control and stability.,https://ieeexplore.ieee.org/document/7329762/,2015 IEEE 19th International Conference on Intelligent Engineering Systems (INES),3-5 Sept. 2015,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICIMU.2011.6122726,[Title page],IEEE,Conferences,The following topics are dealt with: e-services; e-commerce; e-government; e-leaming; mobile-commerce; wireless networks; ad hoc networks; sensor networks; high speed networks; Internet; Web applications; multimedia networking; information governance; information ethic; information content security; data compression; cloud computing; grid computing; green IT computing; peer-to-peer social networks; data mining; computer forensics; computer security; network security; information management; data management; real-time systems; Internet modeling; anti-cyberterτorism; soft computing techniques; computational intelligence; ubiquitous computing; fuzzy system; neural network systems; signal processing; pattern recognition; user interfaces; visualization; sensor network management; Web services architecture; self-organizing networks; networked systems; indexing; query processing; knowledge discovery; cryptography; data protection; image processing; multimedia computing; software engineering; biometrics technologies; wireless communications and semantic Web.,https://ieeexplore.ieee.org/document/6122726/,ICIMU 2011 : Proceedings of the 5th international Conference on Information Technology & Multimedia,14-16 Nov. 2011,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CloudCom.2011.21,iHadoop: Asynchronous Iterations for MapReduce,IEEE,Conferences,"MapReduce is a distributed programming framework designed to ease the development of scalable data-intensive applications for large clusters of commodity machines. Most machine learning and data mining applications involve iterative computations over large datasets, such as the Web hyperlink structures and social network graphs. Yet, the MapReduce model does not efficiently support this important class of applications. The architecture of MapReduce, most critically its dataflow techniques and task scheduling, is completely unaware of the nature of iterative applications, tasks are scheduled according to a policy that optimizes the execution for a single iteration which wastes bandwidth, I/O, and CPU cycles when compared with an optimal execution for a consecutive set of iterations. This work presents iHadoop, a modified MapReduce model, and an associated implementation, optimized for iterative computations. The iHadoop model schedules iterations asynchronously. It connects the output of one iteration to the next, allowing both to process their data concurrently. iHadoop's task scheduler exploits inter-iteration data locality by scheduling tasks that exhibit a producer/consumer relation on the same physical machine allowing a fast local data transfer. For those iterative applications that require satisfying certain criteria before termination, iHadoop runs the check concurrently during the execution of the subsequent iteration to further reduce the application's latency. This paper also describes our implementation of the iHadoop model, and evaluates its performance against Hadoop, the widely used open source implementation of MapReduce. Experiments using different data analysis applications over real-world and synthetic datasets show that iHadoop performs better than Hadoop for iterative algorithms, reducing execution time of iterative applications by 25% on average. Furthermore, integrating iHadoop with HaLoop, a variant Hadoop implementation that caches invariant data between iterations, reduces execution time by 38% on average.",https://ieeexplore.ieee.org/document/6133130/,2011 IEEE Third International Conference on Cloud Computing Technology and Science,29 Nov.-1 Dec. 2011,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SNPD.2012.99,Evaluation of Realism of Dynamic Sound Space Using a Virtual Auditory Display,IEEE,Conferences,"We can perceive a sound position from binaural signals using mainly head-related transfer functions (HRTFs). Using the theorem presented herein, we can display a sound image to a specific position in virtual auditory space by HRTFs. However, HRTF is defined commonly in a free-field, and a virtual sound image is perceived as a dry source without reflection, reverberation, or ambient noise. Therefore, the virtual sound space might be unnatural. The authors developed a software-based virtual auditory display (VAD) that outputs audio signals for a set of headphones with a three-dimensional position sensor. The VAD software can display a dynamic virtual auditory space that is responsive to a listener's head movement. Subjective evaluations were conducted to clarify the relation between the perceived reality of virtual sound space and ambient sound. Evaluation results of the reality of the virtual sound space displayed by the VAD software are introduced.",https://ieeexplore.ieee.org/document/6299338/,"2012 13th ACIS International Conference on Software Engineering, Artificial Intelligence, Networking and Parallel/Distributed Computing",8-10 Aug. 2012,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/DCOSS.2013.49,Event Prediction and Modeling of Variable Rate Sampled Data Using Dynamic Bayesian Networks,IEEE,Conferences,"Event detection is an important issue in sensor networks for a variety of real-world applications. Many events in real world are often correlated on a complex spatio-temporal level whereby they are manifested via observations over time and space proximities. In order to predict events in these spatiotemporal observations, the prediction model should be capable of modeling codependencies between data observed at various locations. In this paper, we propose a Dynamic Bayesian Network (DBN) with such spatio-temporal event prediction capability in sensor networks deployed for sensing environmental data. More specifically, we develop a DBN model with mixture distribution and a novel learning algorithm, for water level data prediction for different canals, using rainfall data at multiple locations. Experiments on real data demonstrates that our model and training method can provide accurate event prediction in real time for spatio-temporal sensor networks.",https://ieeexplore.ieee.org/document/6569444/,2013 IEEE International Conference on Distributed Computing in Sensor Systems,20-23 May 2013,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IJCNN.2019.8852384,Evidence Transfer for Improving Clustering Tasks Using External Categorical Evidence,IEEE,Conferences,"In this paper we introduce evidence transfer for clustering, a deep learning method that can incrementally manipulate the latent representations of an autoencoder, according to external categorical evidence, in order to improve a clustering outcome. By evidence transfer we define the process by which the categorical outcome of an external, auxiliary task is exploited to improve a primary task, in this case representation learning for clustering. Our proposed method makes no assumptions regarding the categorical evidence presented, nor the structure of the latent space. We compare our method, against the baseline solution by performing k-means clustering before and after its deployment. Experiments with three different kinds of evidence show that our method effectively manipulates the latent representations when introduced with real corresponding evidence, while remaining robust when presented with low quality evidence.",https://ieeexplore.ieee.org/document/8852384/,2019 International Joint Conference on Neural Networks (IJCNN),14-19 July 2019,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/DeSE.2011.109,Evolutionary Environmental Modelling in Self-Managing Software Systems,IEEE,Conferences,"The promise of robust software that can self-manage significant aspects of its operation, including the ability to self-configure, self-heal, self-optimise and self-protect through having the requisite functionality to respond and adapt to changes in its operational environment is both seductive and compelling. There are a growing number of examples of partial implementations appearing in the literature and continued development across a number of areas can be expected in the future.One of the less travelled areas of research concerns the problem of developing an accurate and current model of the environment in which such adaptive systems will operate. It would seem a compelling argument that holding a current model of both the environment and the current capability of the system allowing the system to ""know itself"" are desirable additions to any adaptive system. As such they have a view of the complex space within which they can adapt and that without these properties the system could only be considered as purely reactive.Here, the use of Learning Classifier Systems and geneticalgorithms to provide the modelling element required of effective adaptive software systems is presented and evaluated. The work uses the virtual world platform of ""Second Life"" to represent anappropriate experimental environment. One outcome of this work is the restatement of some classical cybernetic principles to reflect the need for constant evolution.",https://ieeexplore.ieee.org/document/6150008/,2011 Developments in E-systems Engineering,6-8 Dec. 2011,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CIG.2016.7860426,Evolutionary deckbuilding in hearthstone,IEEE,Conferences,"One of the most notable features of collectible card games is deckbuilding, that is, defining a personalized deck before the real game. Deckbuilding is a challenge that involves a big and rugged search space, with different and unpredictable behaviour after simple card changes and even hidden information. In this paper, we explore the possibility of automated deckbuilding: a genetic algorithm is applied to the task, with the evaluation delegated to a game simulator that tests every potential deck against a varied and representative range of human-made decks. In these preliminary experiments, the approach has proven able to create quite effective decks, a promising result that proves that, even in this challenging environment, evolutionary algorithms can find good solutions.",https://ieeexplore.ieee.org/document/7860426/,2016 IEEE Conference on Computational Intelligence and Games (CIG),20-23 Sept. 2016,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICDM.2005.59,Example-based robust outlier detection in high dimensional datasets,IEEE,Conferences,"Detecting outliers is an important problem. Most of its applications typically possess high dimensional datasets. In high dimensional space, the data becomes sparse which implies that every object can be regarded as an outlier from the point of view of similarity. Furthermore, a fundamental issue is that the notion of which objects are outliers typically varies between users, problem domains or, even, datasets. In this paper, we present a novel robust solution which detects high dimensional outliers based on user examples and tolerates incorrect inputs. It studies the behavior of projections of such a few examples, to discover further objects that are outstanding in the projection where many examples are outlying. Our experiments on both real and synthetic datasets demonstrate the ability of the proposed method to detect outliers corresponding to the user examples.",https://ieeexplore.ieee.org/document/1565793/,Fifth IEEE International Conference on Data Mining (ICDM'05),27-30 Nov. 2005,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IJCNN.1991.155235,Experimental demonstration of large-scale holographic optical neural network,IEEE,Conferences,"The authors present an experimental implementation of the N/sup 4/ holographic optical neural network and demonstrations of the potential capabilities of large-scale and high-speed pattern recognition applications of this architecture. The extremely large space-bandwidth-product of the holographic materials makes it possible to construct large-scale static holographic optical neural networks (HONNs). A 1024-neuron HONN has been experimentally demonstrated for pattern recognition operations. A 32*32 optical neural network was trained to recognize different airplanes and to give the correct names at the output detector array. The computational time for training the four pairs of patterns took only a few minutes on a 386 microcomputer. It is estimated that with proper selection, over 200 pairs of heteroassociative patterns can be stored in the neural network of 1024 neurons for target recognition using the interpattern association (IPA) neural network model (T. Lu et al., 1990). By recording more holograms on a holographic plate, a 128*128 neural network and eventually a 256*256 neural network can be constructed, thus a high-speed real-time recognition of an enemy's aircraft can be performed.<>",https://ieeexplore.ieee.org/document/155235/,IJCNN-91-Seattle International Joint Conference on Neural Networks,8-12 July 1991,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IWCSN.2017.8276520,Expert recommendation in oss projects based on knowledge embedding,IEEE,Conferences,"Modern Open Source Software (OSS) projects depend on the globally-distributed and synchronized software development. The online collaboration promotes more and more developers to join in OSS projects, while on the other hand, integrating new developers with teams is challenging and pivotal to the success of a project. In this paper, we propose a novel expert recommendation method, based on knowledge embedding, that realizes real-time recommendation for working developers. To capture structural information of source files in call graph, we use node2vec algorithm to convert file entities within projects into knowledge mappings within low-dimensional space, based on which we further propose four features to capture the work status and social relationship of developers. We then design a recommender system using random forest method to recommend appropriate experts for the developers. Experiments on 20 Apache OSS projects show that, compared with the baseline methods, our approach behaves significantly better in terms of a series of performance metrics.",https://ieeexplore.ieee.org/document/8276520/,2017 International Workshop on Complex Systems and Networks (IWCSN),8-10 Dec. 2017,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CVPR.2019.00965,Explore-Exploit Graph Traversal for Image Retrieval,IEEE,Conferences,"We propose a novel graph-based approach for image retrieval. Given a nearest neighbor graph produced by the global descriptor model, we traverse it by alternating between exploit and explore steps. The exploit step maximally utilizes the immediate neighborhood of each vertex, while the explore step traverses vertices that are farther away in the descriptor space. By combining these two steps we can better capture the underlying image manifold, and successfully retrieve relevant images that are visually dissimilar to the query. Our traversal algorithm is conceptually simple, has few tunable parameters and can be implemented with basic data structures. This enables fast real-time inference for previously unseen queries with minimal memory overhead. Despite relative simplicity, we show highly competitive results on multiple public benchmarks, including the largest image retrieval dataset that is currently publicly available. Full code for this work is available here: https://github.com/layer6ai-labs/egt.",https://ieeexplore.ieee.org/document/8954266/,2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),15-20 June 2019,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SIEDS49339.2020.9106581,"Explorer51 – Indoor Mapping, Discovery, and Navigation for an Autonomous Mobile Robot",IEEE,Conferences,"The nexus of robotics, autonomous systems, and artificial intelligence (AI) has the potential to change the nature of human guided exploration of indoor and outdoor spaces. Such autonomous mobile robots can be incorporated into a variety of applications, ranging from logistics and maintenance, to intelligence gathering, surveillance, and reconnaissance (ISR). One such example is that of a tele-operator using the robot to generate a map of the inside of a building while discovering and tagging the objects of interest. During this process, the tele-operator can also assign an area for the robot to navigate autonomously or return to a previously marked area/object of interest. Search and rescue and ISR abilities could be immensely improved with such capabilities. The goal of this research is to prototype and demonstrate the above autonomous capabilities in a mobile ground robot called Explorer51. Objectives include: (i) enabling an operator to drive the robot non-line of sight to explore a space by incorporating a first-person view (FPV) system to stream data from the robot to the base station; (ii) implementing automatic collision avoidance to prevent the operator from running the robot into obstacles; (iii) creating and saving 2D and 3D maps of the space in real time by using a 2D laser scanner, tracking, and depth/RGB cameras; (iv) locating and tagging objects of interest as waypoints within the map; (v) autonomously navigate within the map to reach a chosen waypoint.To accomplish these goals, we are using the AION Robotics R1 Unmanned Ground Vehicle (UGV) rover as the platform for Explorer51 to demonstrate the autonomous features. The rover runs the Robot Operating System (ROS) onboard an NVIDIA Jetson TX2 board, connected to a Pixhawk controller. Sensors include a 2D scanning LiDAR, depth camera, tracking camera, and an IMU. Using existing ROS packages such as Cartographer and TEB planner, we plan to implement ROS nodes for accomplishing these tasks. We plan to extend the mapping ability of the rover using Visual Inertial Odometry (VIO) using the cameras. In addition, we will explore the implementation of additional features such as autonomous target identification, waypoint marking, collision avoidance, and iterative trajectory optimization. The project will culminate in a series of demonstrations to showcase the autonomous navigation, and tele-operation abilities of the robot. Success will be evaluated based on ease of use by the tele-operator, collision avoidance ability, autonomous waypoint navigation accuracy, and robust map creation at high driving speeds.",https://ieeexplore.ieee.org/document/9106581/,2020 Systems and Information Engineering Design Symposium (SIEDS),24-24 April 2020,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICTAI.2009.24,Exploring Software Quality Classification with a Wrapper-Based Feature Ranking Technique,IEEE,Conferences,"Feature selection is a process of selecting a subset of relevant features for building learning models. It is an important activity for data preprocessing used in software quality modeling and other data mining problems. Feature selection algorithms can be divided into two categories, feature ranking and feature subset selection. Feature ranking orders the features by a criterion and a user selects some of the features that are appropriate for a given scenario. Feature subset selection techniques search the space of possible feature subsets and evaluate the suitability of each. This paper investigates performance metric based feature ranking techniques by using the multilayer perceptron (MLP) learner with nine different performance metrics. The nine performance metrics include overall accuracy (OA), default F-measure (DFM), default geometric mean (DGM), default arithmetic mean (DAM), area under ROC (AUC), area under PRC (PRC), best F-measure (BFM), best geometric mean (BGM) and best arithmetic mean (BAM). The goal of the paper is to study the effect of the different performance metrics on the feature ranking results, which in turn influences the classification performance. We assessed the performance of the classification models constructed on those selected feature subsets through an empirical case study that was carried out on six data sets of real-world software systems. The results demonstrate that AUC, PRC, BFM, BGM and BAM as performance metrics for feature ranking outperformed the other performance metrics, OA, DFM, DGMand DAM, unanimously across all the data sets and therefore are recommended based on this study. In addition, the performances of the classification models were maintained or even improved when over 85 percent of the features were eliminated from the original data sets.",https://ieeexplore.ieee.org/document/5364717/,2009 21st IEEE International Conference on Tools with Artificial Intelligence,2-4 Nov. 2009,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IMIS.2012.137,Exploring Urban Dynamics Based on Pervasive Sensing: Correlation Analysis of Traffic Density and Air Quality,IEEE,Conferences,"Modern cities, with large population and complicated infrastructures, are complex entities with non-linear and dynamic properties that challenge the city management. Therefore, as the first step towards the goal of thorough understanding of the phenomena, pervasive urban sensing have become a cornerstone of future smart city that enhance the interplay between the cyber space and the physical world. We introduce a taxi-based pervasive urban sensing system and its key algorithm, aiming at the quantitative study of the correlation between human activities and environmental changes. Our contributions are twofold. First, we propose an urban crowd-sourcing framework that take automobiles as participatory mobile agents to the sensing tasks, and implemented a prototype in Beijing. Second, we design a Spatial-Temporal Manifold Learning (STML) algorithm to analyse the correlation between physical processes. Based on noisy and partially labelled dataset that are collected by pervasive urban sensor networks, we evaluate STML's performance by analysing correlation between the traffic density and the air quality. The results show great potential of STML for future urban sensing applications.",https://ieeexplore.ieee.org/document/6296824/,2012 Sixth International Conference on Innovative Mobile and Internet Services in Ubiquitous Computing,4-6 July 2012,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA48506.2021.9561040,Extendable Navigation Network based Reinforcement Learning for Indoor Robot Exploration,IEEE,Conferences,This paper presents a navigation network based deep reinforcement learning framework for autonomous indoor robot exploration. The presented method features a pattern cognitive non-myopic exploration strategy that can better reflect universal preferences for structure. We propose the Extendable Navigation Network (ENN) to encode the partially observed high-dimensional indoor Euclidean space to a sparse graph representation. The robot&#x2019;s motion is generated by a learned Q-network whose input is the ENN. The proposed framework is applied to a robot equipped with a 2D LIDAR sensor in the GAZEBO simulation where floor plans of real buildings are implemented. The experiments demonstrate the efficiency of the framework in terms of exploration time.,https://ieeexplore.ieee.org/document/9561040/,2021 IEEE International Conference on Robotics and Automation (ICRA),30 May-5 June 2021,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICMLA51294.2020.00011,Extending SpArSe: Automatic Gesture Recognition Architectures for Embedded Devices,IEEE,Conferences,"Neural Architecture Search (NAS), which allows for automatically developing neural networks, has been mostly devoted to performance on a single metric, usually accuracy. New approaches have added more objectives, such as model size, in order to find networks suitable for resource-constrained platforms. SpArSe [1] is a multi-objective Bayesian optimization framework for automatically developing image classification convolutional neural networks (CNNs) for micro-controller units (MCUs). In this work, we first implement SpArSe and modify it to reduce search time, obtaining similar results regarding accuracy, model size, and maximum working memory but in less optimization time. Moreover, we extend the search space to include recurrent neural networks (RNNs) and add an inference latency objective for time-constrained tasks. Finally, we test our implementation in a gesture recognition task obtaining better results than previous manually tuned approaches for size and performance metrics, which validates the approach and its utility.",https://ieeexplore.ieee.org/document/9356164/,2020 19th IEEE International Conference on Machine Learning and Applications (ICMLA),14-17 Dec. 2020,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SOCA.2009.5410453,Extending service model to build an effective service composition framework for cyber-physical systems,IEEE,Conferences,"Cyber-Physical Systems (CPSs) are combinations of physical entities controlled by software systems to accomplish specified tasks under stringent real-time and physical entity constraints. As more and more physical entities are equipped with embedded computers, they are becoming more and more intelligent. However, the problem of effectively composing the services provided by cyber and physical entities to achieve specific goals still remains a challenge. Traditional service-oriented models and composition techniques are insufficient for CPS. In this paper, we present a novel Physical-Entity (PE) service-oriented model to address the problem, including the concepts of PE-ontology and PE-SOA specifications. Based on the model, we develop a two-level compositional reasoning approach, which divides the process into abstract and physical levels to expedite the composition process. With the assistance of the PE-ontology and PE-SOA, abstract level reasoning is performed by hiding the physical level details. This separation greatly reduces the search space for both levels through a divide-and-conquer technique. The model and the composition approach are illustrated using a simplified emergency response case study system.",https://ieeexplore.ieee.org/document/5410453/,2009 IEEE International Conference on Service-Oriented Computing and Applications (SOCA),14-15 Jan. 2009,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SIEDS52267.2021.9483745,Extensions and Application of the Robust Shared Response Model to Electroencephalography Data for Enhancing Brain-Computer Interface Systems,IEEE,Conferences,"Brain Computer Interfaces (BCI) decode electroencephalography (EEG) data collected from the human brain to predict subsequent behavior. While this technology has promising applications, successfully implementing a model is challenging. The typical BCI control application requires many hours of training data from each individual to make predictions of intended activity specific to that individual. Moreover, there are individual differences in the organization of brain activity and low signal-to-noise ratios in noninvasive measurement techniques such as EEG. There is a fundamental bias-variance trade-off between developing a single model for all human brains vs. an individual model for each specific human brain. The Robust Shared Response Model (RSRM) attempts to resolve this tradeoff by leveraging both the homogeneity and heterogeneity of brain signals across people. RSRM extracts components that are common and shared across individual brains, while simultaneously learning unique representations between individual brains. By learning a latent shared space in conjunction with subject-specific representations, RSRM tends to result in better predictive performance on functional magnetic resonance imaging (fMRI) data relative to other common dimension reduction techniques. To our knowledge, we are the first research team attempting to expand the domain of RSRM by applying this technique to controlled experimental EEG data in a BCI setting. Using the openly available Motor Movement/ Imagery dataset, the decoding accuracy of RSRM exceeded models whose input was reduced by Principal Component Analysis (PCA), Independent Component Analysis (ICA), and subject-specific PCA. The results of our experiments suggest that RSRM can recover distributed latent brain signals and improve decoding accuracy of BCI tasks when dimension reduction is implemented as a feature engineering step. Future directions of this work include augmenting state-of-the art BCI with efficient reduced representations extracted by RSRM. This could enhance the utility of BCI technology in the real world. Furthermore, RSRM could have wide-ranging applications across other machine-learning applications that require classification of naturalistic data using reduced representations.",https://ieeexplore.ieee.org/document/9483745/,2021 Systems and Information Engineering Design Symposium (SIEDS),29-30 April 2021,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IJCNN.1999.833481,Extracting knowledge from temporal clusters for real-time clustering,IEEE,Conferences,"Real-time anomaly detection is an increasing need as the desire for data analysis grows along with the amount of data stored on computers. Increasing the number of features used as input and the number of clusters created also increases the time needed to classify new inputs using clustering methods. Without intelligent reduction of the search space, the need to cluster in real-time forces the user to either limit the number of input features or reduce the number of total clusters created. However, the intelligent reduction of the search space can result in real-time anomaly detection without the loss of accuracy in classification. A hybrid neural network technique that allows for clustering of sequences, the extraction of regular grammars, and a method for using the grammars for real-time classification has been developed and is presented in this paper.",https://ieeexplore.ieee.org/document/833481/,IJCNN'99. International Joint Conference on Neural Networks. Proceedings (Cat. No.99CH36339),10-16 July 1999,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICIT.2011.5754405,"FPGA based intelligent condition monitoring of induction motors: Detection, diagnosis, and prognosis",IEEE,Conferences,"This paper presents three intelligent methods for condition monitoring of induction motors in real-time. A structured neural network has been designed to prognosis of instantaneous faults. The inputs of neural network are the standard deviation and mean of feature signal obtained by Hilbert transform of one phase current signal. The stator related faults have been diagnosed by designing fuzzy logic. The amplitudes of three phase currents have been given to fuzzy logic and the condition of stator has been diagnosed. The last algorithm uses the phase space of the Hilbert transform of one phase current and detects broken rotor bar faults using negative selection algorithm. The contribution of the algorithm is the development of synchronously worked algorithms, optimized for low-cost Field Programmable Gate Array (FPGA) implementation. Extensive simulations were applied to test the performance of each algorithm, and the results show that the algorithms give high accuracy in detecting whether a possible fault has occurred in any component of the motor. The average detection time of the faults is above within 2 milliseconds or less.",https://ieeexplore.ieee.org/document/5754405/,2011 IEEE International Conference on Industrial Technology,14-16 March 2011,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SCORED.2009.5443323,FPGA implementation of a space-time trellis decoder,IEEE,Conferences,This paper describes the real-time implementation of a space-time trellis encoder and decoder using the Xilinx Virtex-4¿FX12 FPGA. The code uses a generator matrix designed for 4-state space-time trellis (STT) that uses Quadrature Phase Shift Keying (QPSK) modulation scheme. The decoding process was done using Maximum Likelihood (ML) through the Viterbi Algorithm. The results show that the STT decoder can successfully decipher the encoded symbols from the STT encoder and that it can fully recover the original data in the absence of noise. The data rate of the decoder was 6.25 Msymbols/s. It was shown that 14% of the logic elements in Virtex 4 FPGA were used in implementing an encoder-decoder system.,https://ieeexplore.ieee.org/document/5443323/,2009 IEEE Student Conference on Research and Development (SCOReD),16-18 Nov. 2009,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ComPE53109.2021.9752237,Face Mask Detection for Preventing the Spread of Covid-19 using Knowledge Distillation,IEEE,Conferences,"The coronavirus pandemic (COVID-19) has unfolded hastily throughout the entire world. This pandemic disease can spread through droplets and can be airborne. Hence, the use of face masks in public places is crucial to stop its spread. The present study aims to develop a system that can identify masked or non-masked faces; whether it is a normal mask, transparent mask, or a face alike mask. The face mask detection system is developed with the help of Convolutional Neural Networks (CNN). The model compression technique of Knowledge Distillation has been used to make the machine lesser computation and memory intensive so that it is simple to install the model on a few embedded gadgets and cell computing platforms. Using the model compression technique and GPU systems will help boom the calculation velocity of the model and drop the storage space required for calculations. The experimental outcomes show that the developed detector is capable to classify diverse types of masks. Also, it can classify video images in real-time. Using the Knowledge Distillation on the baseline model can improve the testing accuracy from 88.79% to 90.13%. The proposed unique system can be implemented to assist in the prevention of COVID-19 spread and detect various mask types.",https://ieeexplore.ieee.org/document/9752237/,2021 International Conference on Computational Performance Evaluation (ComPE),1-3 Dec. 2021,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICoICT.2015.7231422,Face image-based gender recognition using complex-valued neural network,IEEE,Conferences,"Automatic gender recognition is an emerging problem in computer visions. An accurate gender recognition system can be used to reduce the search space in face recognition system for about half. However, since there is no definitive features of sexual dimorphism on human face that can be applied to all kind of face shapes from any race and age, it needs more studies to optimize the recognition system. Thus, this research investigates the implementation of complex-valued neural network as a classifier to recognize human gender which is based on face image. The experiment is also aimed to study the comparison between complex-valued and real-valued neural network. The methods proposed in this paper include image processing, feature extraction, and classification. After the face image processed by using local binary pattern to accentuate face texture and gradient filter to define face outline, the features of the face image is extracted by using histogram of oriented gradient. Then, the dimension of the resulted vectors is reduced by using principal component analysis. The final feature vectors is then used in neural network training and neural network testing processes. This paper shows investigation results in implementing the methods for some measurement parameters. The accuracy level of real-valued neural network system is a bit lower than the average accuracy level of complex-valued one, with 78.2% and 80.2% average accuracy rate, respectively. The results also show that complex-valued neural network system can achieve convergency rate four times faster than the real-valued neural network, which implies on the training process time.",https://ieeexplore.ieee.org/document/7231422/,2015 3rd International Conference on Information and Communication Technology (ICoICT),27-29 May 2015,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SSCI47803.2020.9308337,Fake-Face Image Classification using Improved Quantum-Inspired Evolutionary-based Feature Selection Method,IEEE,Conferences,"Deep learning models have been quite successful in discriminating synthesized or edited fake-face images. However, in the case of small training data, transfer-learning is rather preferable. This is a complex process for high dimensional feature space due to the curse of dimensionality. To mitigate the same, this paper proposes a new feature selection method for the classification of manually created fake-face images. In the proposed method, a pre-trained deep learning model is used to extract features of an image. Next, an optimal feature subset is selected from the extracted features through an improved quantum-inspired evolutionary algorithm. Lastly, the elicited features are considered to perform the classification. Experiments are conducted on a publicly available manually created fake-face image dataset, namely Real and Fake Face Detection by Yonsei University. The performance of the proposed method is compared with two methods in terms of classification accuracy and the number of selected features. The experimental comparisons exhibit that the proposed method achieves promising results among the considered methods.",https://ieeexplore.ieee.org/document/9308337/,2020 IEEE Symposium Series on Computational Intelligence (SSCI),1-4 Dec. 2020,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ITSC48978.2021.9564641,Fast Collision Prediction for Autonomous Vehicles using a Stochastic Dynamics Model,IEEE,Conferences,"Autonomous Vehicles (AVs) have the potential to save millions of lives by reducing traffic deaths and accidents. However, despite recent advances, AVs have not met safety standard expectations for a variety of reasons, key among them being the difficulty in certifying AV safety. The development of model-based methods is essential for achieving more explainable tools that provide better safety assurances, in contrast to popular data-dependent end-to-end learning methods. This paper introduces a model-based collision prediction method that uses discretized Gaussian processes for future vehicle position estimation. It can incorporate road layout information, statistical agent dynamics, and be coupled with any trajectory prediction module. The discretization of the space together with a single Normal random variable for each vehicle trajectory allows fast and efficient computation for real-time deployment and computational intensive applications, such as simulation and training of Deep Learning and Reinforcement Learning models. The method can be applied to various scenarios by the adjustment of the model parameters that control dynamics uncertainty. Two scenarios extracted from real data are used as case studies.",https://ieeexplore.ieee.org/document/9564641/,2021 IEEE International Intelligent Transportation Systems Conference (ITSC),19-22 Sept. 2021,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/INFOCOM41043.2020.9155456,Fast Network Alignment via Graph Meta-Learning,IEEE,Conferences,"Network alignment (NA) - i.e., linking entities from different networks (also known as identity linkage) - is a fundamental problem in many application domains. Recent advances in deep graph learning have inspired various auspicious approaches for tackling the NA problem. However, most of the existing works suffer from efficiency and generalization, due to complexities and redundant computations.We approach the NA from a different perspective, tackling it via meta-learning in a semi-supervised manner, and propose an effective and efficient approach called Meta-NA - a novel, conceptually simple, flexible, and general framework. Specifically, we reformulate NA as a one-shot classification problem and address it with a graph meta-learning framework. Meta-NA exploits the meta-metric learning from known anchor nodes to obtain latent priors for linking unknown anchor nodes. It contains multiple sub-networks corresponding to multiple graphs, learning a unified metric space, where one can easily link entities across different graphs. In addition to the performance lift, Meta-NA greatly improves the anchor linking generalization, significantly reduces the computational overheads, and is easily extendable to multi-network alignment scenarios. Extensive experiments conducted on three real-world datasets demonstrate the superiority of Meta-NA over several state-of-the-art baselines in terms of both alignment accuracy and learning efficiency.",https://ieeexplore.ieee.org/document/9155456/,IEEE INFOCOM 2020 - IEEE Conference on Computer Communications,6-9 July 2020,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICMLC.2004.1380499,Fast training of SVM for color-based image segmentation,IEEE,Conferences,"A novel method based on support vector machine (SVM) for color image segmentation is presented. Considering image segmentation is a two-class problem, a two-class SVM to classify pixels in color space is proposed. In order to speed up training and optimize parameters of SVM, the color quantization and sample selection approaches are presented to reduce the size of training set and to make the reduced set separable. Training with reduced and separable dataset, minimizing the number of support vectors is regard as an estimation criterion of generalization performance to kernel parameter optimization. The classifier can be trained on-line and implemented in real-time. The new algorithm has been used to color-based image segmentation, and it brings robust performance in practice.",https://ieeexplore.ieee.org/document/1380499/,Proceedings of 2004 International Conference on Machine Learning and Cybernetics (IEEE Cat. No.04EX826),26-29 Aug. 2004,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/WiSPNET51692.2021.9419438,Faster Training of Edge-attention Aided 6D Pose Estimation Model using Transfer Learning and Small Customized Dataset,IEEE,Conferences,"Computer Vision is the field of machine learning that deals with computers gaining knowledge from digital images/videos and performing tasks that human vision is capable of doing. It is widely used in the field of robotics for designing guidance systems where objects in the robot's field of view are identified and located. This research work is an application-specific project enabling a half-humanoid to find the 6D pose and bounding boxes of its hand and other objects within its field of view. We add an edge prediction head to the NOCS (Normalised Object Coordinate Space) model, which predicts the edges of each object from the predicted instance maps. An additional edge-agreement-loss found from the predicted edges is added to the total loss. This increases the attention to the edges and improves the accuracy of prediction of the instance masks. This edge-attention aided model is initialized with pre-trained weights of CAMERA and REAL dataset using transfer learning. The backbone layers of the model are frozen and the head layers alone are trained using a synthetic dataset (HAND dataset) we created using a software called blender. The model gives promising results when tested with objects kept in varying lighting conditions and at different distances from the camera. The use of transfer learning in models as large as the NOCS model allows us to train the model for a new class by only training the top few layers with a significantly small dataset.",https://ieeexplore.ieee.org/document/9419438/,"2021 Sixth International Conference on Wireless Communications, Signal Processing and Networking (WiSPNET)",25-27 March 2021,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/TFSA.1994.467254,Fault detection and identification using real-time wavelet feature extraction,IEEE,Conferences,"Development of real-time fault detection and identification technologies will allow a migration, in the respective theater of operation, from expensive scheduled based maintenance to the more efficient, less costly alternative of condition based maintenance. This paper presents successful initial results applying continuous wavelet transforms coupled with conventional neural networks to the development of a real-time fault detection and classification systems. The approach taken results in a general methodology which is shown to work equally well on fault-seeded, helicopter gear-box data and operational data from Navy shipboard pumps. The family of wavelet basis functions are specifically engineered to allow for real-time implementation. The wavelet basis functions have a time-scale decomposition mathematically inspired from biological systems and provides a clustering in feature space which allows for the development of simplified neural network classifiers. Application to various classes of fault data (helicopter and shipboard pump data) resulted in perfect detection, no false alarms with only modest deferral rates.<>",https://ieeexplore.ieee.org/document/467254/,Proceedings of IEEE-SP International Symposium on Time- Frequency and Time-Scale Analysis,25-28 Oct. 1994,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CEC.2011.5949774,Fault identification with binary adaptive fireflies in parallel and distributed systems,IEEE,Conferences,"The efficient identification of hardware and software faults in parallel and distributed systems still remains a serious challenge in today's most prolific decentralized environments. System-level fault diagnosis is concerned with the detection of all faulty nodes in a set of interconnected units. This is accomplished by thoroughly examining the collection of outcomes of all tests carried out by the nodes under a particular test model. Such task has non-polynomial complexity and can be posed as a combinatorial optimization problem, whose optimal solution has been sought through bio-inspired methods like genetic algorithms, ant colonies and artificial immune systems. In this paper, we employ a swarm of artificial fireflies to quickly and reliably navigate across the search space of all feasible sets of faulty units under the invalidation and comparison test models. Our approach uses a binary encoding of the potential solutions (fireflies), an adaptive light absorption coefficient to accelerate the search and problem-specific knowledge to handle infeasible solutions. The empirical analysis confirms that the proposed algorithm outperforms existing techniques in terms of convergence speed and memory requirements, thus becoming a viable approach for real-time fault diagnosis in large-size systems.",https://ieeexplore.ieee.org/document/5949774/,2011 IEEE Congress of Evolutionary Computation (CEC),5-8 June 2011,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICMLA.2007.10,Feature Extraction from Microarray Expression Data by Integration of Semantic Knowledge,IEEE,Conferences,"Microarray techniques give biologists first peek into the molecular states of living tissues. Previous studies have proven that it is feasible to build sample classifiers using the gene expressional profiles. To build an effective sample classifier, dimension reduction process is necessary since classic pattern recognition algorithms do not work well in high dimensional space. In this paper, we present a novel feature extraction algorithm based on the concept of virtual genes by integrating microarray expression data sets with domain knowledge embedded in gene ontology (GO) annotations. We define semantic similarity to measure the functional associations between two genes using the annotation on each GO term. We then identify the groups of genes, called virtual genes, that potentially interact with each other for a biological function. The correlation in gene expression levels of virtual genes can be used to build a sample classifier. For a colon cancer data set, the integration of microarray expression data with GO annotations significantly improves the accuracy of sample classification by more than 10%.",https://ieeexplore.ieee.org/document/4457296/,Sixth International Conference on Machine Learning and Applications (ICMLA 2007),13-15 Dec. 2007,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CyberSA52016.2021.9478199,Feature Vulnerability and Robustness Assessment against Adversarial Machine Learning Attacks,IEEE,Conferences,"Whilst machine learning has been widely adopted for various domains, it is important to consider how such techniques may be susceptible to malicious users through adversarial attacks. Given a trained classifier, a malicious attack may attempt to craft a data observation whereby the data features purposefully trigger the classifier to yield incorrect responses. This has been observed in various image classification tasks, including falsifying road sign detection and facial recognition, which could have severe consequences in real-world deployment. In this work, we investigate how these attacks could impact on network traffic analysis, and how a system could perform misclassification of common network attacks such as DDoS attacks. Using the CICIDS2017 data, we examine how vulnerable the data features used for intrusion detection are to perturbation attacks using FGSM adversarial examples. As a result, our method provides a defensive approach for assessing feature robustness that seeks to balance between classification accuracy whilst minimising the attack surface of the feature space.",https://ieeexplore.ieee.org/document/9478199/,"2021 International Conference on Cyber Situational Awareness, Data Analytics and Assessment (CyberSA)",14-18 June 2021,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICMLC.2013.6890416,Feature extraction based on discriminant analysis with penalty constraint for hyperspectral image classification,IEEE,Conferences,"The main issue of hyperspectral image data (HSI) is its high dimensionality which conducts challenge in high dimensional data analysis community. Popular linear approaches can work effectively when the data is unimodal Gaussian class conditional independently distributions. Yet, they usually fail when applied to HSI data since the distribution of HSI data is usually unknown in reality. Locality preserving projection (LPP) addresses this problem approvingly, where the neighborhood information can be preserved in the reduced space. Based on typical behaviors of Fisher's linear discriminant analysis (LDA), a novel discriminant analysis framework under penalty constraint(PFDA), which extends the ideas of LDA and LPP, is developed in this paper. Benefiting from different construction of affinity matrix, our method can also preserve the locality embedding information effectively in the reduced space. Four types of PFDA are analyzed in this paper and the efficiency and effectiveness of proposed methods under penalty framework are demonstrated by both synthesis data and real hyperspectral remote sensing image data set.",https://ieeexplore.ieee.org/document/6890416/,2013 International Conference on Machine Learning and Cybernetics,14-17 July 2013,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICTAI50040.2020.00019,Feature-Aware Attentive Variational Auto-Encoder for Top-N Recommendation,IEEE,Conferences,"Personalized recommendation has become increasingly pervasive due to its great commercial value in business. Deep neural networks can automatically exvacate the behavior patterns from the historical interaction records, which has achieved excellent results in related tasks. Among them, the variational auto-encoders have been shown to be superior for learning to rank and recommendation on massive data. However, prior work neglects the association between user behavior and side information, which affects the quality of recommendation services to some extent. In this paper, we propose a feature-aware attentive variational auto-encoder for top-N recommendation. The attention mechanism is utilized to capture the relationship between user's representation and side information through a sub network, balancing the fusion weight of attributes in the main network. In addition, this method tries to construct combination of features in the high-dimensional embedding space, helping mining the promotion of side information at a finer scale. Experiments conducted on real-world datasets demonstrate the effectiveness over the state-of-art methods.",https://ieeexplore.ieee.org/document/9288316/,2020 IEEE 32nd International Conference on Tools with Artificial Intelligence (ICTAI),9-11 Nov. 2020,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IJCNN.2019.8852314,Feature-Dependent Graph Convolutional Autoencoders with Adversarial Training Methods,IEEE,Conferences,"Graphs are ubiquitous for describing and modeling complicated data structures, and graph embedding is an effective solution to learn a mapping from a graph to a low-dimensional vector space while preserving relevant graph characteristics. Most existing graph embedding approaches either embed the topological information and node features separately or learn one regularized embedding with both sources of information, however, they mostly overlook the interdependency between structural characteristics and node features when processing the graph data into the models. Moreover, existing methods only reconstruct the structural characteristics, which are unable to fully leverage the interaction between the topology and the features associated with its nodes during the encoding-decoding procedure. To address the problem, we propose a framework using autoencoder for graph embedding (GED) and its variational version (VEGD). The contribution of our work is two-fold: 1) the proposed frameworks exploit a feature-dependent graph matrix (FGM) to naturally merge the structural characteristics and node features according to their interdependency; and 2) the Graph Convolutional Network (GCN) decoder of the proposed framework reconstructs both structural characteristics and node features, which naturally possesses the interaction between these two sources of information while learning the embedding. We conducted the experiments on three real-world graph datasets such as Cora, Citeseer and PubMed to evaluate our framework and algorithms, and the results outperform baseline methods on both link prediction and graph clustering tasks.",https://ieeexplore.ieee.org/document/8852314/,2019 International Joint Conference on Neural Networks (IJCNN),14-19 July 2019,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISIE45552.2021.9576369,Feature-based Egocentric Grasp Pose Classification for Expanding Human-Object Interactions,IEEE,Conferences,"This paper presents a framework for classifying human hand pose, especially in grasping object intuitively. First, we propose a system based on the stereo infra-red image as a sensor that can produce hand coordinates in 3-dimensional space. We use egocentric vision because it can get uniform and natural data with only a single sensor module. Second, we transformed the position to get the angle information for each joint on the finger. Third, we designed an intelligent system based on Multi-Layer Perceptron (MLP) to process angular data to obtain classification results according to the Cutkosky grasp taxonomy. Finally, we compared the results on several similar objects and evaluated their classification accuracy. In the validation phase, the results yielded an accuracy of 16 grasp pose classification is 89,60%. In real-time testing, the results yielded an accuracy of 81.93%. This result shows feature-based learning can reduce the complexity and training time of the MLP. Furthermore, a small amount of training data is sufficient for the training and implementation.",https://ieeexplore.ieee.org/document/9576369/,2021 IEEE 30th International Symposium on Industrial Electronics (ISIE),20-23 June 2021,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICTAI.2011.39,Filtering by ULP Maximum,IEEE,Conferences,"Constraint solving over floating-point numbers is an emerging topic that found interesting applications in software analysis and testing. Even for IEEE-754 compliant programs, correct reasoning over floating-point computations is challenging and requires dedicated constraint solving approaches to be developed. Recent advances indicate that numerical properties of floating-point numbers can be used to efficiently prune the search space. In this paper, we reformulate the Marre and Michel property over floating-point addition/subtraction constraint to ease its implementation in real-world floating-point constraint solvers. We also generalize the property to the case of multiplication/division in order to benefit from its improvements in more cases.",https://ieeexplore.ieee.org/document/6103329/,2011 IEEE 23rd International Conference on Tools with Artificial Intelligence,7-9 Nov. 2011,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICTAI.2009.60,FlockStream: A Bio-Inspired Algorithm for Clustering Evolving Data Streams,IEEE,Conferences,"Existing density-based data stream clustering algorithms use a two-phase scheme approach consisting of an online phase, in which raw data is processed to gather summary statistics, and an offline phase that generates the clusters by using the summary data. In this paper we propose a data stream clustering method based on a multi-agent system that uses a decentralized bottom-up self-organizing strategy to group similar data points. Data points are associated with agents and deployed onto a 2D space, to work simultaneously by applying a heuristic strategy based on a bio-inspired model, known as flocking model. Agents move onto the space for a fixed time and, when they encounter other agents into a predefined visibility range, they can decide to form a flock if they are similar. Flocks can join to form swarms of similar groups. This strategy allows to merge the two phases of density-based approaches and thus to avoid the offline cluster computation, since a swarm represents a cluster. Experimental results show the capability of the bio-inspired approach to obtain very good results on real and synthetic data sets.",https://ieeexplore.ieee.org/document/5364942/,2009 21st IEEE International Conference on Tools with Artificial Intelligence,2-4 Nov. 2009,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SYSCON.2019.8836965,Fog Computing for Real-Time Accident Identification and Related Congestion Control,IEEE,Conferences,"This paper focuses on developing (i) a benchmark application for Real-Time traffic incidence identification and related traffic management, using Real-Time congestion-aware navigation of smart vehicles (Edge nodes) with video feeds, (ii) an image database for Deep Learning used for recognition and classification of traffic incidences such as accidents and congestions, (iii) the System Level Software (or Middleware) required for Distributed Computing in such a heterogeneous Real-Time constrained system with Rapid Mobility - today's Internet-of-Everything (IoE), and (iv) a hardware prototype of the distributed computing and storage infrastructure. The video bandwidth requirement of 10-100 GigaBytes of data per minute per vehicular camera makes it a Big Data problem. With millions of smart vehicles projected to be deployed within the next 5 years, BigData from a single vehicle, multiplied with the large number of vehicles, presents a Big-Squared-Data computing space which will easily overwhelm any Cloud infrastructure with its Real-Time or near Real-Time demands. Hence the need for a Fog tier between the Edge nodes and the Cloud to bring distributed computation (servers) and storage closer to the Edge nodes. Such a Fog consists of multiple Fog instances, each one of which services cells or Virtual Clusters of Edge nodes. Results show that Fog-Cloud computing framework outperforms a Cloud-only platform by 79.7% reduction in total latency or response time.",https://ieeexplore.ieee.org/document/8836965/,2019 IEEE International Systems Conference (SysCon),8-11 April 2019,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AIVR.2018.00046,FoodChangeLens: CNN-Based Food Transformation on HoloLens,IEEE,Conferences,"In this demonstration, we implemented food category transformation in mixed reality using both image generation and HoloLens. Our system overlays transformed food images to food objects in the AR space, so that it is possible to convert in consideration of real shape. This system has the potential to make meals more enjoyable. In this work, we use the Conditional CycleGAN trained with a large-scale food image data collected from the Twitter Stream for food category transformation which can transform among ten kinds of foods mutually keeping the shape of a given food. We show the virtual meal experience which is food category transformation among ten kinds of typical Japanese foods: ramen noodle, curry rice, fried rice, beef rice bowl, chilled noodle, spaghetti with meat source, white rice, eel bowl, and fried noodle. Note that additional results including demo videos can be see at https://negi111111.github.io/FoodChangeLensProjectHP/.",https://ieeexplore.ieee.org/document/8613665/,2018 IEEE International Conference on Artificial Intelligence and Virtual Reality (AIVR),10-12 Dec. 2018,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/HUMANOIDS.2014.7041373,Footstep planning on uneven terrain with mixed-integer convex optimization,IEEE,Conferences,"We present a new method for planning footstep placements for a robot walking on uneven terrain with obstacles, using a mixed-integer quadratically-constrained quadratic program (MIQCQP). Our approach is unique in that it handles obstacle avoidance, kinematic reachability, and rotation of footstep placements, which typically have required non-convex constraints, in a single mixed-integer optimization that can be efficiently solved to its global optimum. Reachability is enforced through a convex inner approximation of the reachable space for the robot's feet. Rotation of the footsteps is handled by a piecewise linear approximation of sine and cosine, designed to ensure that the approximation never overestimates the robot's reachability. Obstacle avoidance is ensured by decomposing the environment into convex regions of obstacle-free configuration space and assigning each footstep to one such safe region. We demonstrate this technique in simple 2D and 3D environments and with real environments sensed by a humanoid robot. We also discuss computational performance of the algorithm, which is currently capable of planning short sequences of a few steps in under one second or longer sequences of 10-30 footsteps in tens of seconds to minutes on common laptop computer hardware. Our implementation is available within the Drake MATLAB toolbox [1].",https://ieeexplore.ieee.org/document/7041373/,2014 IEEE-RAS International Conference on Humanoid Robots,18-20 Nov. 2014,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ROMAN.2008.4600635,Force skill training with a hybrid trainer model,IEEE,Conferences,"In this work, we present novel VR training strategies that incorporate a hybrid trainer model to train force. For modeling the trainer skill, weighted K-means algorithm in parameter space with LS optimization is implemented. The efficiency of the training strategies is verified via user tests in frame of a bone drilling training application. An objective evaluation method based on n dimensional Euclidean distances is introduced to assess user tests results. It is shown that the proposed strategies improve the student skill and accelerate force learning.",https://ieeexplore.ieee.org/document/4600635/,RO-MAN 2008 - The 17th IEEE International Symposium on Robot and Human Interactive Communication,1-3 Aug. 2008,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/GLOCOM.2017.8255034,FreeCount: Device-Free Crowd Counting with Commodity WiFi,IEEE,Conferences,"In the era of Internet of Things, crowd counting, which estimates the number of people within a region, becomes the underpinning for many emerging applications, such as occupancy estimation in smart building and queuing management and product placement in shopping center. Existing vision based crowd counting schemes require favorable lighting conditions and also raise privacy concerns. RF based approaches rely on specialized sensors and require users to carry RF devices. Thus, an accurate, reliable and non-intrusive crowd counting scheme is still desired. In this paper, we propose FreeCount, a device-free crowd counting scheme that is able to precisely estimate the number of people within a region using only commodity WiFi routers. To this end, the channel state information (CSI) data in PHY layer is obtained directly by upgrading the router's software. We propose an information theory based feature selection scheme to select the most representative features that are sensitive to human motion. To build a classifier that is robust to temporal and environmental disparities, we adopt transfer kernel learning, which minimizes the difference between the source and target distributions in the reproducing kernel Hilbert space, is adopted to process the real-time CSI feature data. Experiments were conducted in moderate sized rooms and the results demonstrated that FreeCount is able to accurately estimate the number of people with 96% crowd counting accuracy consistently over temporal and environmental variation.",https://ieeexplore.ieee.org/document/8255034/,GLOBECOM 2017 - 2017 IEEE Global Communications Conference,4-8 Dec. 2017,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AICI.2009.369,Frequent Items Mining on Data Stream Based on Time Fading Factor,IEEE,Conferences,"Most of the existing algorithms for mining frequent items on data stream do not emphasis the importance of the recent data items. We present an algorithm using a fading factor to detect the data items with frequency counts exceeding a user-specified threshold. Our algorithm can detect ¿-approximate frequent data items on data stream using O(¿-1) memory space and the processing time for each data item and a query is O(¿-1). Experimental results on several artificial datasets and real datasets show our algorithm has higher precision, requires less memory and consumes less computation time than other similar methods.",https://ieeexplore.ieee.org/document/5376330/,2009 International Conference on Artificial Intelligence and Computational Intelligence,7-8 Nov. 2009,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS40897.2019.8967568,From Pixels to Buildings: End-to-end Probabilistic Deep Networks for Large-scale Semantic Mapping,IEEE,Conferences,"We introduce TopoNets, end-to-end probabilistic deep networks for modeling semantic maps with structure reflecting the topology of large-scale environments. TopoNets build a unified deep network spanning multiple levels of abstraction and spatial scales, from pixels representing geometry of local places to high-level descriptions of semantics of buildings. To this end, TopoNets leverage complex spatial relations expressed in terms of arbitrary, dynamic graphs. We demonstrate how TopoNets can be used to perform end-to-end semantic mapping from partial sensory observations and noisy topological relations discovered by a robot exploring large-scale office spaces. Thanks to their probabilistic nature and generative properties, TopoNets extend the problem of semantic mapping beyond classification. We show that TopoNets successfully perform uncertain reasoning about yet unexplored space and detect novel and incongruent environment configurations unknown to the robot. Our implementation of TopoNets achieves real-time, tractable and exact inference, which makes these new deep models a promising, practical solution to mobile robot spatial understanding at scale.",https://ieeexplore.ieee.org/document/8967568/,2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),3-8 Nov. 2019,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS.2016.7759701,From indoor GIS maps to path planning for autonomous wheelchairs,IEEE,Conferences,"This work focuses on how to compute trajectories for an autonomous wheelchair based on indoor GIS maps, in particular on IndoorGML maps, which set the standard in this context. Good wheelchair trajectories are safe and comfortable for the user and the people sharing the space with him, turn gently, are high legible, and smooth (at least G2 continuos). We derive a navigation graph from a given IndoorGML map. We define and solve an optimization problem to find the desired path: given a succession of cells to traverse, the path corresponds to the best composite Bézier trajectory for the wheelchair. We discuss a related multi-objective path planning problem. Experimental results and an implementation on real robots show the planner performance.",https://ieeexplore.ieee.org/document/7759701/,2016 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),9-14 Oct. 2016,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IJCNN.2001.939561,Fuzzy clusters identification in the feature space using neural networks,IEEE,Conferences,Deals with the development of ARTMAP-like neural networks to analyze feature space for classification purposes. The proposed tool provides information about the value of membership functions of the unknown input vector to each class of interest. The designed ARTMAP-like system is called MF-ARTMAP based on the fact that membership functions are calculated. The functions shape is predefined as Gaussian with adaptation of mean value and variance in each feature space dimension during the training procedure. The parallel version of this approach is designed and implemented too. The parallel MF ARTMAP have some advantages over regular MF ARTMAP. The usefulness of this approach is presented on the benchmark classification problems e.g. circle in the square and spiral and on real-world data from satellite images over Slovakia. Classification accuracy is calculated using the contingency tables approach on actual and predicted classes of interest.,https://ieeexplore.ieee.org/document/939561/,IJCNN'01. International Joint Conference on Neural Networks. Proceedings (Cat. No.01CH37222),15-19 July 2001,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/FUZZ-IEEE.2016.7737911,Fuzzyfication of principle component analysis for data dimensionalty reduction,IEEE,Conferences,"Principal component analysis (PCA) extracts small uncorrelated data from original high dimensional data space and is widely used for data analysis. The methodology of classical PCA is based on orthogonal projection defined in convex vector space. Thus, a norm between two projected vectors is unavoidably smaller than the norm between any two objects before implementation of PCA. Due to this, in some cases when the PCA cannot capture the data structure, its implementation does not necessarily confirm the real similarity of data in the higher dimensional space, making the results unacceptable. In order to avoid this problem for the purposes of high dimensional data clustering, we propose new Fuzzy PCA (FPCA) algorithm. The novelty is in the extracted similarity structures of objects in high-dimensional space and dissimilarities between objects based on their cluster structure and dimension when the algorithm is implemented in conjunction with known fuzzy clustering algorithms as FCM, GK or GG algorithms. This is done by using the fuzzy membership functions and modification of the classical PCA approach by considering the similarity structures during the construction of projections in smaller dimensional space. The effectiveness of the proposed algorithm is tested on several benchmark data sets. We also evaluate the clustering efficiency by implementing validation measures.",https://ieeexplore.ieee.org/document/7737911/,2016 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE),24-29 July 2016,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/INDIN.2009.5195905,GPS and sonar based area mapping and navigation by mobile robots,IEEE,Conferences,"In this paper, we have presented a GPS and sonar based area mapping and navigation scheme for a mobile robot. A mapping is achieved between the GPS space and the world coordinates of the mobile robot which enables us to generate direct motion commands for it. This mapping enables the robot to navigate among different GPS locations within the mapped area. The GPS data is extracted online to get the latitude and longitude information of a particular location. In the training phase, a 2-D axis transformation is used to relate local robot frame with the robot world coordinates and then the actual world coordinates are mapped from the GPS data using a RBFN (radial basis function network) based Neural Network. In the second phase, direct GPS data is used to get the mapping into the world coordinates of mobile robot using the trained network and the motion commands are generated accordingly. The physical placement of sonar devices, their ranging limits and beam opening angles are considered during navigation for possible collision detection and obstacle avoidance. This scheme is successfully implemented in real time with Pioneer mobile robot from ActivMedia Robotics and GPS receiver. The scheme is also tested in the simulation to justify its application in the real world.",https://ieeexplore.ieee.org/document/5195905/,2009 7th IEEE International Conference on Industrial Informatics,23-26 June 2009,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICSE-Companion55297.2022.9793764,Gallery D.C.: Auto-created GUI Component Gallery for Design Search and Knowledge Discovery,IEEE,Conferences,"GUI design is an integral part of software development. The process of designing a mobile application typically starts with the ideation and inspiration search from existing designs. However, existing information-retrieval based, and database-query based methods cannot e&#xFB03;ciently gain inspirations in three requirements: design practicality, design granularity and design knowledge discovery. In this paper we propose a web application, called Gallery D.C. that aims to facilitate the process of user interface design through real world GUI component search. Gallery D.C. indexes GUI compo-nent designs using reverse engineering and deep learning based computer vision techniques on millions of real world applications. To perform an advanced design search and knowledge discovery, our approach extracts information about size, color, component type, and text information to help designers explore multi-faceted design space and distill higher-order of design knowledge. Gallery D.C. is well received via an informal evaluation with 7 professional designers.Web Link: http://mui-collection.herokuapp.com/.Demo Video Link: https://youtu.be/zVmsz_wY5OQ.",https://ieeexplore.ieee.org/document/9793764/,2022 IEEE/ACM 44th International Conference on Software Engineering: Companion Proceedings (ICSE-Companion),22-24 May 2022,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/INES52918.2021.9512921,Game Feature Validation of a Real-Time Game Space with an eXtended Classifier System,IEEE,Conferences,"The objective of this paper is the proposal of a new approach for the game feature validation of a game space with the eXtended Classifier System (XCS) algorithm. For initial ""proof-of-concept"" evaluation we used the game space of the Tic-Tac-Toe game, which was placed in a context of real-time characteristics. Evaluation was done with the XCS algorithm without pre-processing internal knowledge programmed (online mode). Evaluation data results were acquired under real-time constraints. No-loss-strategy was implemented for the game, with various scenarios tested to find out if the algorithm has the capability of finding invalid game feature design flaws. Results indicated that the XCS algorithm is able to deliver stable validation of game feature testing results, can be a powerful tool and, therefore, worthy of further research in the Gaming domain. Confirmation of this core concept testing with this type of algorithm (and similar ones) is necessary, since it paves the way for further research in more complex game environments, where the dimension size, the number of aspects, game states and game actions rises intensely.",https://ieeexplore.ieee.org/document/9512921/,2021 IEEE 25th International Conference on Intelligent Engineering Systems (INES),7-9 July 2021,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/BioCAS.2015.7348397,General-purpose LSM learning processor architecture and theoretically guided design space exploration,IEEE,Conferences,"This paper presents a general-purpose liquid state machine based neuromorphic learning processor with integrated training and recognition for real world pattern recognition problems. The proposed architecture consists of a generic preprocessor and one or multiple task processors. The pre-processor, or the reservoir, consists of a recurrent spiking neural network with fixed synaptic weights. Task processors are light weight and comprise a set of readout spiking neurons with plastic weights, which are tuned by a biologically plausible supervised learning rule. Importantly, we leverage the unique computational structure of the reservoir for highly efficient implementation of multiple tasks on the same learning processor. A novel theoretical measure of computational power, which is strongly correlated with the true learning performance, is proposed to facilitate fast design space exploration of the recurrent reservoir. We demonstrate the application of our processor architecture by mapping four recognition tasks onto a reconfigurable FPGA processor platform.",https://ieeexplore.ieee.org/document/7348397/,2015 IEEE Biomedical Circuits and Systems Conference (BioCAS),22-24 Oct. 2015,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/PRDC50213.2020.00018,Generative Deep Learning for Internet of Things Network Traffic Generation,IEEE,Conferences,"The rapid development of the Internet of Things (IoT) has prompted a recent interest into realistic IoT network traffic generation. Security practitioners need IoT network traffic data to develop and assess network-based intrusion detection systems (NIDS). Emulating realistic network traffic will avoid the costly physical deployment of thousands of smart devices. From an attacker's perspective, generating network traffic that mimics the legitimate behavior of a device can be useful to evade NIDS. As network traffic data consist of sequences of packets, the problem is similar to the generation of sequences of categorical data, like word by word text generation. Many solutions in the field of natural language processing have been proposed to adapt a Generative Adversarial Network (GAN) to generate sequences of categorical data. In this paper, we propose to combine an autoencoder with a GAN to generate sequences of packet sizes that correspond to bidirectional flows. First, the autoencoder is trained to learn a latent representation of the real sequences of packet sizes. A GAN is then trained on the latent space, to learn to generate latent vectors that can be decoded into realistic sequences. For experimental purposes, bidirectional flows produced by a Google Home Mini are used, and the autoencoder is combined with a Wassertein GAN. Comparison of different network characteristics shows that our proposed approach is able to generate sequences of packet sizes that behave closely to real bidirectional flows. We also show that the synthetic bidirectional flows are close enough to the real ones that they can fool anomaly detectors into labeling them as legitimate.",https://ieeexplore.ieee.org/document/9320384/,2020 IEEE 25th Pacific Rim International Symposium on Dependable Computing (PRDC),1-4 Dec. 2020,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SAMOS.2016.7818338,Genesys: Automatically generating representative training sets for predictive benchmarking,IEEE,Conferences,"Fast and efficient design space exploration is a critical requirement for designing computer systems, however, the growing complexity of hardware/software systems and significantly long run-times of detailed simulators often makes it challenging. Machine learning (ML) models have been proposed as popular alternatives that enable fast exploratory studies. The accuracy of any ML model depends heavily on the re-presentativeness of applications used for training the predictive models. While prior studies have used standard benchmarks or hand-tuned micro-benchmarks to train their predictive models, in this paper, we argue that it is often sub-optimal because of their limited coverage of the program state-space and their inability to be representative of the larger suite of real-world applications. In order to overcome challenges in creating representative training sets, we propose Genesys, an automatic workload generation methodology and framework, which builds upon key low-level application characteristics and enables systematic generation of applications covering a broad range of program behavior state-space without increasing the training time. We demonstrate that the automatically generated training sets improve upon the state-space coverage provided by applications from popular benchmarking suites like SPEC-CPU2006, MiBench, Media Bench, TPC-H by over llx and improve the accuracy of two machine learning based power and performance prediction systems by over 2.5x and 3.6x respectively.",https://ieeexplore.ieee.org/document/7818338/,"2016 International Conference on Embedded Computer Systems: Architectures, Modeling and Simulation (SAMOS)",17-21 July 2016,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICNN.1993.298714,Genetic algorithm based input selection for a neural network function approximator with applications to SSME health monitoring,IEEE,Conferences,"A genetic algorithm is used to select the inputs to a neural network function approximator. In the application considered, modeling critical parameters of the Space Shuttle main engine, the functional relationships among measured parameters if unknown and complex and the number of possible input parameters is quite large. Due to the optimization and space searching capabilities of genetic algorithms, they are employed to systematize the input selection process. The results suggest that the genetic algorithm can generate parameter lists of high quality without the explicit use of problem domain knowledge. Suggestions for improving the performance of the input selection process are provided.<>",https://ieeexplore.ieee.org/document/298714/,IEEE International Conference on Neural Networks,28 March-1 April 1993,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/KES.2000.885785,Genetic algorithm driven clustering for toxicity prediction,IEEE,Conferences,The pace of technological advancement in today's society has generated an enormous demand for methods facilitating the intelligent testing of the toxicity of new chemicals. Until now it was common use to make predictions based on 'real' tests. Recent investigations support the general assumption that macroscopic properties like toxicity and ecotoxicity strongly depend on microscopic features and the structure of the molecule. The authors have developed a computationally intelligent method for supervised training of regression systems. Their method selects those features needed to predict and calculate the toxicity. The proposed methodology relies on supervised clustering with genetic algorithms and local learning. Different molecular descriptors are computed and the correlation behaviour of the different descriptors in the descriptor space is studied.,https://ieeexplore.ieee.org/document/885785/,KES'2000. Fourth International Conference on Knowledge-Based Intelligent Engineering Systems and Allied Technologies. Proceedings (Cat. No.00TH8516),30 Aug.-1 Sept. 2000,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISSPIT.2011.6151580,Genetic algorithm implementation of multi-user detection in SDMA-OFDM systems,IEEE,Conferences,Number of supported users in the orthogonal frequency division multiplexing (OFDM) systems can be increased considerably using powerful multi-user detector (MUD) combined with space division multiple access (SDMA) techniques. This paper presents the results of implementing MUD in SDMA-OFDM systems based on an advanced genetic-algorithm (GA) optimization tool. The hardware implementation is performed using Field Programmable Gate array (FPGA) devices which allow the real time performance of the proposed tool. Results show that the GA scheme enhances the performance and provides BER near to that attained using maximum likelihood (ML) detector at considerably lower computation complexity. Investigation of the GA population size is presented and FPGA implementation is described based on the shared memory approach.,https://ieeexplore.ieee.org/document/6151580/,2011 IEEE International Symposium on Signal Processing and Information Technology (ISSPIT),14-17 Dec. 2011,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/COGANN.1992.273945,Genetic sparse distributed memory,IEEE,Conferences,"Kanerva's 'sparse distributed memory' (SDM) is a type of self-organizing neural network which is able to extract a statistical summary from large volumes of data as it is being processed online. Genetic algorithms have been used to optimize the 'location address space' which corresponds to the mapping from the input layer to the hidden units in the neural network implementation of the sparse distributed memory. If treated as a global optimization problem, the genetic algorithm will attempt to optimize the sparse distributed memory so as to extract a single best statistical predictor. However, the real objective is to obtain not just a single global optimum, but to extract information about as many local optima as possible, since each local optimum in this particular definition of the search space represents a different and distinct data pattern that correlates with some output in which we may be interested. The implementation details of a genetic sparse distributed memory as well as modified algorithm designed to deal better with multiple data patterns are presented.<>",https://ieeexplore.ieee.org/document/273945/,[Proceedings] COGANN-92: International Workshop on Combinations of Genetic Algorithms and Neural Networks,6-6 June 1992,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISMAR-Adjunct51615.2020.00030,Ginput: a tool for fast hi-fi prototyping of gestural interactions in virtual reality,IEEE,Conferences,"Gestural interfaces in virtual reality (VR) expand the design space for user interaction, allowing spatial metaphors with the environment and more natural and immersive experiences. Typically, machine learning approaches recognize gestures with models that rely on a large number of samples for the training phase, which is an obstacle for rapidly prototyping gestural interactions. In this paper, we propose a solution designed for hi-fi prototyping of gestures within a virtual reality environment through a high-level Domain-Specific Language (DSL), as a subset of the natural language. The proposed DSL allows non-programmer users to intuitively describe a broad domain of poses and connect them for compound gestures. Our DSL was designed to be general enough for multiple input classes, such as body tracking, hand tracking, head movement, motion controllers, and buttons. We tested our solution for wands with VR designers and developers. Results showed that the tool gives non-programmers the ability to prototype gestures with ease and refine its recognition within a few minutes.",https://ieeexplore.ieee.org/document/9288432/,2020 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct),9-13 Nov. 2020,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ROBIO54168.2021.9739622,Globally Learnable Point Set Registration Between 3D CT and Multi-view 2D X-ray Images of Hip Phantom,IEEE,Conferences,"2D-3D registration is a crucial step in Image-Guided Intervention, such as spine surgery, total hip re-placement, and kinematic analysis. To find the information in common between pre-operative 3D CT images and intra-operative X-ray 2D images is vital to plan and navigate. In a nutshell, the goal is to find the movement and rotation of the 3D body&#x2019;s volume to make them reorient with the patient body in the 2D image space. Due to the loss of dimensionality and different sources of images, efficient and fast registration is challenging. To this end, we propose a novel approach to incorporate a point set Neural Network to combine the information from different views, which enjoys the robustness of the traditional method and the geometrical information extraction ability. The pre-trained Deep BlindPnP captures the global information and local connectivity, and each implementation of view-independent Deep BlindPnP in different view pairs will select top-priority pairs candidates. The transformation of different viewpoints into the same coordinate will accumulate the correspondence. Finally, a POSEST-based module will output the final 6 DoF pose. Extensive experiments on a real-world clinical dataset show the effectiveness of the proposed framework compared to the single view. The accuracy and computation speed are improved by incorporating the point set neural network.",https://ieeexplore.ieee.org/document/9739622/,2021 IEEE International Conference on Robotics and Biomimetics (ROBIO),27-31 Dec. 2021,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/FUZZY.2006.1681714,Granular Auto-regressive Moving Average (grARMA) Model for Predicting a Distribution from Other Distributions. Real-world Applications,IEEE,Conferences,"Industrial products are often output in batches at discrete times. A batch gives rise to distributions of measurements, one distribution per variable of interest. There may be a need for modeling to predict a distribution from other distributions. This work represents a distribution by a fuzzy interval number (FIN) interpreted as an information granule. Based on vector lattice theory it is shown that the lattice F+ of positive FINs is a cone in a non-linearly tunable, metric, linear space. In conclusion, a multivariate granular autoregressive moving average (grARMA) model is proposed for predicting a distribution from other distributions. A recursive neural network implementation is shown. We report preliminary results regarding two real-world applications including, first, industrial fertilizer production and, second, environmental pollution monitoring along seashore in northern Greece. The far-reaching potential of novel techniques is discussed.",https://ieeexplore.ieee.org/document/1681714/,2006 IEEE International Conference on Fuzzy Systems,16-21 July 2006,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICME51207.2021.9428448,Graph Attention Neural Network for Image Restoration,IEEE,Conferences,"Self-similarity underpins modern non-local attention mechanism, which has been verified to be an effective prior for image restoration. However, most existing non-local attention restorers are implemented based on pixels, which tend to be biased due to image degeneration. Furthermore, most non-local methods for image restoration are restricted to construct fully-connected correlations in a regular Euclidean space so that all features within the search region have to participate in the feature aggregation process, no matter how similar the key feature is to the query feature. To rectify these weaknesses, in this paper, we propose a novel graph attention network for image restoration, dubbed GATIR, which establishes the non-local attention based on feature patches and utilizes the graph convolution to perform feature aggregation selectively in a non-Euclidean space. Experimental results demonstrate that our GATIR can achieve state-of-the-art performance on synthetic image denoising, real image denoising, image demosaicing, and compression artifact reduction tasks.",https://ieeexplore.ieee.org/document/9428448/,2021 IEEE International Conference on Multimedia and Expo (ICME),5-9 July 2021,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IEEECONF51394.2020.9443359,Graph Diffusion Kernel LMS using Random Fourier Features,IEEE,Conferences,"This work introduces kernel adaptive graph filters that operate in the reproducing kernel Hilbert space. We propose a centralized graph kernel least mean squares (GKLMS) approach for identifying the nonlinear graph filters. The principles of coherence-check and random Fourier features (RFF) are used to reduce the dictionary size. Additionally, we leverage the graph structure to derive the graph diffusion KLMS (GDKLMS). The proposed GDKLMS requires only single-hop communication during successive time instants, making it viable for real-time network-based applications. In the distributed implementation, usage of RFF avoids the requirement of a centralized pre-trained dictionary in the case of coherence-check. Finally, the performance of the proposed algorithms is demonstrated in modeling a nonlinear graph filter via numerical examples. The results show that centralized and distributed implementations effectively model the nonlinear graph filters, whereas the random-feature-based solutions are shown to outperform coherence-check based solutions.",https://ieeexplore.ieee.org/document/9443359/,"2020 54th Asilomar Conference on Signals, Systems, and Computers",1-4 Nov. 2020,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICSMC.2008.4811307,Graph-based semi-supervised learning with manifold preprocessing for image classification,IEEE,Conferences,"In real worlds applications, some former research papers have shown that manifold learning tries to discover the non-linear low-dimensional data manifold from a high-dimensional space. Many natural images and the face images are believed to be sampled from a manifold. In this paper, we try to investigate whether discovering such manifold can aid the semi-supervised learning algorithms. We propose a novel graph-based learning algorithm locality preserving graph-based semi-supervised method (LLGSM), which firstly use both labeled and unlabeled examples as unlabeled to discover the manifolds of the data samples and then use the projected labeled examples together with projected unlabeled ones to do classification. Experiments performed on some public image data sets have demonstrated the effectiveness of our algorithm.",https://ieeexplore.ieee.org/document/4811307/,"2008 IEEE International Conference on Systems, Man and Cybernetics",12-15 Oct. 2008,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/BigData.2018.8622381,Guiding the Data Learning Process with Physical Model in Air Pollution Inference,IEEE,Conferences,"The surveillance of air pollution is becoming a highly concerned issue for city residents and urban administrators. Fixed air quality stations as well as mobile gas sensors have been deployed for air quality monitoring but with sparse observations over the entire temporal-spatial space. Therefore, an inference algorithm is essential for comprehensive fine-grained air pollution sensing. Conventional physically-based models can hardly be applied to all the scenarios, while pure data-driven methods suffer from sampling bias and overfitting problems. This paper presents a hybrid algorithm for air pollution inference by guiding the data learning process with physical model. The quantitative combination of knowledge from observed dataset and a discretized convective-diffusion model is performed within a multi-task learning scheme. Evaluations show that, benefited from physical guidance, our hybrid method obtains higher extrapolation ability and more robustness, achieving the same performance with 1/8 sample amount and obtaining 31.9% less error in noisy synthesized environment. In a real-world deployment in Tianjin, our algorithm outperforms the pure data-driven model with 9.69% less inference error over a 9-day PM2.5 data collection.",https://ieeexplore.ieee.org/document/8622381/,2018 IEEE International Conference on Big Data (Big Data),10-13 Dec. 2018,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICAML54311.2021.00056,Gymnasium Simulation Design and Implementation Based on 3D Virtual Building,IEEE,Conferences,"Based on the three-dimensional virtual building-simulation design and implementation method of the gymnasium, when the building space is subjected to virtual reality and simulation design, based on the constructed building space coordinate system and scale, the position and parameters of the building are used to construct a mathematical model of the building space component., The mathematical model of each component is integrated to build a mathematical model of the overall building space. OpenGL virtual reality technology is used to expand the target building based on the mathematical model of the target building space. The target building is given materials and texture characteristics to obtain the ideal 3D virtual view of the building space. The 3D virtual view of the building space is 3D rendered and displayed. The vivid 3D virtual renderings of the building space are animated using animation design technology. The experimental results show that the proposed method has good point-line rendering and overall rendering effect, can obtain more realistic 3D virtual building based gym simulation design results, and has high interactivity and practicability.",https://ieeexplore.ieee.org/document/9712032/,2021 3rd International Conference on Applied Machine Learning (ICAML),23-25 July 2021,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ASP-DAC47756.2020.9045442,HL-Pow: A Learning-Based Power Modeling Framework for High-Level Synthesis,IEEE,Conferences,"High-level synthesis (HLS) enables designers to customize hardware designs efficiently. However, it is still challenging to foresee the correlation between power consumption and HLS-based applications at an early design stage. To overcome this problem, we introduce HL-Pow, a power modeling framework for FPGA HLS based on state-of-the-art machine learning techniques. HL-Pow incorporates an automated feature construction flow to efficiently identify and extract features that exert a major influence on power consumption, simply based upon HLS results, and a modeling flow that can build an accurate and generic power model applicable to a variety of designs with HLS. By using HL-Pow, the power evaluation process for FPGA designs can be significantly expedited because the power inference of HL-Pow is established on HLS instead of the time-consuming register-transfer level (RTL) implementation flow. Experimental results demonstrate that HL-Pow can achieve accurate power modeling that is only 4.67% (24.02 mW) away from onboard power measurement. To further facilitate power-oriented optimizations, we describe a novel design space exploration (DSE) algorithm built on top of HL-Pow to trade off between latency and power consumption. This algorithm can reach a close approximation of the real Pareto frontier while only requiring running HLS flow for 20% of design points in the entire design space.",https://ieeexplore.ieee.org/document/9045442/,2020 25th Asia and South Pacific Design Automation Conference (ASP-DAC),13-16 Jan. 2020,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICDMW.2009.69,HOCT: A Highly Scalable Algorithm for Training Linear CRF on Modern Hardware,IEEE,Conferences,"Conditional Random Fields (CRFs) are widely used in machine learning and natural language processing fields. A number of methods have been developed for CRF training. However, even with state-of-the-art algorithms, the training of CRF is still very time and space consuming. This make it infeasible to use CRFs in large-scale data analysis tasks. This paper proposes an efficient algorithm, HOCT, for CRF training on modern computer architectures. First, software prefetching techniques are utilized to hide cache miss latency. Second, we exploit SIMD to process data in parallel. Third, when dealing with large data sets, we let HOCT instead of operating system to manage swapping operations. Our experiments on various real data sets show that HOCT yields a fourfold speedup when the data can fit in memory, and over a 30-fold speedup when the memory requirement exceeds the physical memory.",https://ieeexplore.ieee.org/document/5360418/,2009 IEEE International Conference on Data Mining Workshops,6-6 Dec. 2009,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IJCNN.2018.8489438,Half-precision Floating Point on Spiking Neural Networks Simulations in FPGA,IEEE,Conferences,"The use of half-precision floating-point numbers (hFP) in simulations of spiking neural networks (SNN) was investigated. The hFP format is used successfully in computer graphics and video games for storage and data transfer. The IEEE 754-2008 standard settles that arithmetic operations must occur at least on single-precision floating-point format (sFP). This means that it is necessary to convert hFP to sFP for arithmetical operations and reconvert the results to hFP before storing it. The influence of successive conversions when simulating SNN is the main concern of this article. Three methods were used to evaluate the impact of hFP on SNNs: (i) F-I curve, (ii) subthreshold regime, and (iii) the time for the next spike. We have tested the leaky integrate-and-fire and the Izhikevich's neuron model; both presented similar results. The data show that SNNs simulated with sFP present equivalent results when compared to the ones simulated with hFP with identical topology. Such results are important because hFP requires half of the memory space, simpler buses, and lower bandwidth for transferring data. We may infer they require lower clock frequency consequently lower power consumption. These are essential factors for real-time simulation of SNN on embedded electronics. The sFP to hFP conversion circuits, and vice versa, may be implemented using few logical blocks in a field-programmable gate arrays (FPGA) with no relevant Iatency. We conclude that data in the hFP format are suitable for SNNs synthesized in FPGAs, even though such implementations require conversion circuits.",https://ieeexplore.ieee.org/document/8489438/,2018 International Joint Conference on Neural Networks (IJCNN),8-13 July 2018,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IJCNN.2006.247206,Handwritten Signature Authentication using Artificial Neural Networks,IEEE,Conferences,"The main goal of this paper is to describe our research and implementation of a handwritten signature authentication system based on artificial neural networks. In this system the authentication process occurs in the following way: firstly, the users' signatures are read using a pen tablet device and then stored; after that some adjustments in position and scale are accomplished; representative signature features are extracted; the input space dimensionality is reduced using principal component analysis; and finally, the users' signatures are classified as authentic or not, through the use of a neural network. Several experiments were accomplished using a 2440 real signatures database, and the obtained results were very satisfactory.",https://ieeexplore.ieee.org/document/1716797/,The 2006 IEEE International Joint Conference on Neural Network Proceedings,16-21 July 2006,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISSC.2018.8585376,Hello & Goodbye: Conversation Boundary Identification Using Text Classification,IEEE,Conferences,"One of the main challenges in discourse analysis is the process of segmenting text into meaningful topic segments. While this problem has been studied over the past thirty years, previous topic segmentation studies ignore crucial elements of a conversation: an opening and closing remark. Our motivation to revisit this problem space is the rise of instant message usage. We consider the problem of topic segmentation as a machine learning classification one. Using both enterprise and open source datasets, we address the question as to whether a machine learning algorithm can be trained to identify salutations and valedictions within multi-party real-time chat conversations. Our results show that both Naïve Bayes (NB) and Support Vector Machine (SVM) algorithms provide a reasonable degree of precision(mean F1 score: 0.58).",https://ieeexplore.ieee.org/document/8585376/,2018 29th Irish Signals and Systems Conference (ISSC),21-22 June 2018,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICDM.2014.147,Heterogeneous Metric Learning with Content-Based Regularization for Software Artifact Retrieval,IEEE,Conferences,"The problem of software artifact retrieval has the goal to effectively locate software artifacts, such as a piece of source code, in a large code repository. This problem has been traditionally addressed through the textual query. In other words, information retrieval techniques will be exploited based on the textual similarity between queries and textual representation of software artifacts, which is generated by collecting words from comments, identifiers, and descriptions of programs. However, in addition to these semantic information, there are rich information embedded in source codes themselves. These source codes, if analyzed properly, can be a rich source for enhancing the efforts of software artifact retrieval. To this end, in this paper, we develop a feature extraction method on source codes. Specifically, this method can capture both the inherent information in the source codes and the semantic information hidden in the comments, descriptions, and identifiers of the source codes. Moreover, we design a heterogeneous metric learning approach, which allows to integrate code features and text features into the same latent semantic space. This, in turn, can help to measure the artifact similarity by exploiting the joint power of both code and text features. Finally, extensive experiments on real-world data show that the proposed method can help to improve the performances of software artifact retrieval with a significant margin.",https://ieeexplore.ieee.org/document/7023378/,2014 IEEE International Conference on Data Mining,14-17 Dec. 2014,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICTAI.2019.00146,Heterogeneous Transfer Clustering for Partial Co-occurrence Data,IEEE,Conferences,"Heterogeneous transfer clustering can translate knowledge from some related heterogeneous source domains to the target domain without any supervision. Existing works usually use a large amount of complete co-occurrence data to learn the projection functions mapping heterogeneous data to a common latent feature subspace. However, in many real applications, it is not practical to collect abundant co-occurrence data, while the available co-occurrence data are always incomplete. Another commonly encountered problem is that the complex structure of real heterogeneous data may result in substantial degeneration in clustering performance. To address these issues, we propose a heterogeneous transfer clustering method specifically designed for partial co-occurrence data (HTCPC). It is superior to the existing methods in three facets. First, HTCPC fully uses the partial co-occurrence data in both source and target domains to learn a latent space, maximally extracting useful knowledge for clustering from limited information. Second, it incorporates multi-layer hidden representations, accurately preserving the complex hierarchical structure of data. Third, it enforces approximately orthogonal constraint in representations, effectively characterizing the latent subspace with minimal redundancy. An efficient algorithm has been derived and implemented to realize the proposed HTCPC. A series of experiments on the real datasets have illustrated the advantage of the proposed approach compared with state-of-the-art methods.",https://ieeexplore.ieee.org/document/8995324/,2019 IEEE 31st International Conference on Tools with Artificial Intelligence (ICTAI),4-6 Nov. 2019,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISCID.2017.172,Heterogeneous Transfer with Deep Latent Correlation for Sentiment Analysis,IEEE,Conferences,"Most traditional methods of image sentiment analysis focus on the design of visual features, and the usefulness of texts associated to the images have not been sufficiently investigated. Heterogeneous transfer learning has recently gained much attention as a new machine learning paradigm in which knowledge can be transferred from source domain feature space to target domain feature space. This paper proposes a novel approach that exploits deep latent correlation between visual and textual modalities. In our proposed method, we build a latent embedding space for symmetric heterogeneous feature transfer. The latent space is able to generate domain-specific and maximally correlative cross-domain features which are regarded as the semantic-intensive visual feature representation and used to train sentiment polarity classifiers. The results of experiments conducted on real-world data sets show that the proposed approach can achieve better sentiment classification accuracy by using multi-layer neural network to capture deeper internal relations.",https://ieeexplore.ieee.org/document/8283267/,2017 10th International Symposium on Computational Intelligence and Design (ISCID),9-10 Dec. 2017,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ROBOT.1993.292013,Hidden Markov model approach to skill learning and its application in telerobotics,IEEE,Conferences,"The problem of how human skill can be represented as a parametric model using a hidden Markov (HMM), and how an HMM-based skill model can be used to learn human skill, is discussed. The HMM is feasible for characterizing two stochastic processes, measurable action and immeasurable mental states that are involved in the skill learning. Based on the most likely performance criterion, the best action sequence can be selected from previously measured action data by modeling the skill as an HMM. This selection process can be updated in real-time by feeding new action data and modifying HMM parameters. The implementation of the proposed method in a teleoperation-controlled space robot is discussed. The results demonstrate the feasibility of the method.<>",https://ieeexplore.ieee.org/document/292013/,[1993] Proceedings IEEE International Conference on Robotics and Automation,2-6 May 1993,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SMC.2018.00176,Hierarchical Control Architecture Regulating Competition between Model-Based and Context-Dependent Model-Free Reinforcement Learning Strategies,IEEE,Conferences,"Recent evidence in neuroscience and psychology suggests that a single reinforcement learning (RL) algorithm only accounts for less than 60% of the variance of human choice behavior in an uncertain and dynamic environment, where the amount of uncertainty in state-action-state transitions drift over time. The prediction performance further decreases when the size of the state space increases. We proposed a hierarchical context-dependent RL control framework that dynamically exerted control weights on model-based (MB) and multiple model-free (MF) RL strategies associated with different task goals. To properly assess the validity of the proposed method, we considered a two-stage Markov decision task (MDT) in which the three different types of context changed over time. We trained 57 different RL control models on a Caltech MDT data set; then, we assessed their prediction performance using a Bayesian model comparison. This large-scale computer simulation analysis revealed that the model providing the most accurate prediction was the version that implemented the competition between the MB and multiple goal-dependent MF RL strategies. The present study demonstrates the applicability of the goal-driven RL control to a variety of real-world human-robot interaction scenarios.",https://ieeexplore.ieee.org/document/8616172/,"2018 IEEE International Conference on Systems, Man, and Cybernetics (SMC)",7-10 Oct. 2018,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISIC.1992.225127,Hierarchical architecture for multi-sensor robot cell operation,IEEE,Conferences,"The authors describe a hierarchical architecture designed to carry out experiments in multisensor integration and sensor-based control in robotics. The hierarchical model is composed of three major levels: a high-level information processing and planning structure at the top, a logic-branching control structure at the intermediate level, and a real-time continuous sensory feedback loop at the bottom level. The two lower control structures are addressed. The principal submodules of the intermediate structure are described, with particular emphasis on communication issues and on the available software mechanisms for configuration and online maintenance of the robot cell. The architecture of the real-time continuous control structure that composes the bottom level is also described. The application of the adaptive self-tuning scheme in controlling position and force, specified in task-space coordinates, is discussed. Practical issues and experimental results are summarized.<>",https://ieeexplore.ieee.org/document/225127/,Proceedings of the 1992 IEEE International Symposium on Intelligent Control,11-13 Aug. 1992,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/BigData.2013.6691727,Hierarchical feature learning from sensorial data by spherical clustering,IEEE,Conferences,"Surveillance sensors are a major source of unstructured Big Data. Discovering and recognizing spatiotemporal objects (e.g., events) in such data is of paramount importance to the security and safety of facilities and individuals. What kind of computational model is necessary for discovering spatiotemporal objects at the level of abstraction they occur? Hierarchical invariant feature learning is the crux to the problems of discovery and recognition in Big Data. We present a multilayered convergent neural architecture for storing repeating spatially and temporally coincident patterns in data at multiple levels of abstraction. A node is the canonical computational unit consisting of neurons. Neurons are connected in and across nodes via bottom-up, top-down and lateral connections. The bottom-up weights are learned to encode a hierarchy of overcomplete and sparse feature dictionaries from space- and time-varying sensorial data by recursive layer-by-layer spherical clustering. The model scales to full-sized high-dimensional input data and also to an arbitrary number of layers thereby having the capability to capture features at any level of abstraction. The model is fully-learnable with only two manually tunable parameters. The model is generalpurpose (i.e., there is no modality-specific assumption for any spatiotemporal data), unsupervised and online. We use the learning algorithm, without any alteration, to learn meaningful feature hierarchies from images and videos which can then be used for recognition. Besides being online, operations in each layer of the model can be implemented in parallelized hardware, making it very efficient for real world Big Data applications.",https://ieeexplore.ieee.org/document/6691727/,2013 IEEE International Conference on Big Data,6-9 Oct. 2013,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/NAECON46414.2019.9057909,High Speed Approximate Cognitive Domain Ontologies for Constrained Asset Allocation based on Spiking Neurons,IEEE,Conferences,"Cognitive agents are typically utilized in autonomous systems for automated decision making. These systems interact at real time with their environment and are generally heavily power constrained. Thus, there is a strong need for a real time agent running on a low power platform. The agent examined is the Cognitively Enhanced Complex Event Processing (CECEP) architecture. This is an autonomous decision support tool that reasons like humans and enables enhanced agent-based decision-making. It has applications in a large variety of domains including autonomous systems, operations research, intelligence analysis, and data mining. One of the most time consuming and key components of CECEP is the mining of knowledge from a repository described as a Cognitive Domain Ontology (CDO). One problem that is often tasked to CDOs is asset allocation. Given the number of possible solutions in this allocation problem, determining the optimal solution via CDO can be very time and energy consuming. A grid of isolated spiking neurons is capable of generating solutions to this problem very quickly, although some degree approximation is required to achieve the speedup. The approximate spiking approach presented in this work was able to complete nearly all allocation simulations with greater than 98% accuracy. Our results in this work show that constraining the possible solution space by creating specific rules for a scenario can alter the quality of the allocation result. We present a study compares allocation score and computation time for three different constraint implementation cases. Given the vast increase in speed, as well as the reduction computational requirements, the presented algorithm is ideal for moving asset allocation to low power embedded hardware.",https://ieeexplore.ieee.org/document/9057909/,2019 IEEE National Aerospace and Electronics Conference (NAECON),15-19 July 2019,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISUVR.2010.25,High-Performance Real-Time Face-Detection Architecture for HCI Applications,IEEE,Conferences,"This paper proposes a novel hardware structure and FPGA implementation method for real-time detection of multiple human faces with robustness against illumination variations and Rotated faces. These are designed to greatly improve face detection in various environments, using the Adaboost learning algorithm and MCT techniques, Rotation Transformation, which is robust against variable illumination and rotated faces. The overall structure of proposed hardware is composed of a Color Space Converter, Noise Filter, Memory Controller Interface, Rotation Transformation, MCT (Modified Census Transform), Candidate Detector/Confidence Mapper, Position Resizer, Data Grouper, Overlay Processor and Color Overlay Processor. The experiment was conducted in various environments using a QVGA Camera, LCD Display and Virtext5 XC5VLX330 FF1760 FPGA, made by Xilinx. Implementation and verification results showed that it is possible to detect at least 32 faces in a wide variety of sizes at a maximum speed of 149 frames per second in real time.",https://ieeexplore.ieee.org/document/5557933/,2010 International Symposium on Ubiquitous Virtual Reality,7-10 July 2010,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS.2008.4651150,High-dimensional underactuated motion planning via task space control,IEEE,Conferences,"Kinodynamic planning algorithms have the potential to find feasible control trajectories which accomplish a task even in very nonlinear or constrained dynamical systems. Underactuation represents a particular form of a dynamic constraint, inherently present in many machines of interest (e.g., walking robots), and necessitates planning for long-term control solutions. A major limitation in motion planning techniques, especially for real-time implementation, is that they are only practical for relatively low degree-of-freedom problems. Here we present a model-based dimensionality reduction technique based on an extension of partial feedback linearization control into a task-space framework. This allows one to plan motions for a complex underactuated robot directly in a low-dimensional task-space, and to resolve redundancy with lower-priority tasks. We illustrate the potential of this approach with an extremely simple motion planning system which solves the swing-up problem for multi-link underactuated pendula, and discuss extensions to the control of walking.",https://ieeexplore.ieee.org/document/4651150/,2008 IEEE/RSJ International Conference on Intelligent Robots and Systems,22-26 Sept. 2008,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICAECT49130.2021.9392459,Highly Accurate Real time human Counter with Minimum Computation Cost,IEEE,Conferences,"During the ongoing Covid-19 Pandemic when we need to operate any public facility like museum, shopping mall, restaurants, or public dealing organizations, we not only need to keep the operations going but also have to ensure precautionary measures to ascertain their safety. As per all SOPs (Standard Operating Procedures) it is advisable to restrict number of visitors inside these enclosed spaces which are most likely to be weather controlled. Automatic safety compliance thus becomes imperative in such situations. Even though absolute compliance and alert signalling will require scrutiny and cross-checking at several levels, a beginning towards automation of compliance monitoring seems mandatory in the neo-normal era. Hence In this project we have designed a low cost rapidly implementable design to monitor the number of visitors inside the self-contained hall. The system will give signal once the maximum permissible visitor population is reached at a given time. Monitoring the optimal population and the density and enforcing visitor to wear mask even within the space manually is tantamount to imposing health hazards to the person who will physically have to monitor and it may as well render the visitors vulnerable. Here we have used Artificial Intelligence based model person detection and tracking. Real time tracking with accuracy is still an important area in computer vision. There are some commercial solution available for the problem but all of them either implemented considering ideal situation or need huge cost and infrastructure. But as a part of museums community we are passing through a financial crises as due to pandemic we closed for visitors. Hence neither we can afford costly system nor a system designed with ideal condition. This motivates us to develop a new system according to our criteria. Here we have modified the available solution for implementation in real world environment using very minimum hardware infrastructure requirements to work on real time with maximum possible efficiency. This system is not only useful for COVID-19 Situation but also its use can be extended beyond the boundary of museums for visitor density monitoring system for large public establishment with minimum computation cost.",https://ieeexplore.ieee.org/document/9392459/,"2021 International Conference on Advances in Electrical, Computing, Communication and Sustainable Technologies (ICAECT)",19-20 Feb. 2021,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ALLERTON.2009.5394528,Highly parallel decoding of space-time codes on graphics processing units,IEEE,Conferences,Graphics processing units (GPUs) with a few hundred extremely simple processors represent a paradigm shift for highly parallel computations. We use this emergent GPU architecture to provide a first demonstration of the feasibility of real time ML decoding (in software) of a high rate space-time block code that is representative of codes incorporated in 4th generation wireless standards such as WiMAX and LTE. The decoding algorithm is conditional optimization which reduces to a parallel calculation that is a natural fit to the architecture of low cost GPUs. Experimental results demonstrate that asymptotically the GPU implementation is more than 700 times faster than a standard serial implementation. These results suggest that GPU architectures have the potential to improve the cost / performance tradeoff of 4th generation wireless base stations. Additional benefits might include reducing the time required for system development and the time required for configuration and testing of wireless base stations.,https://ieeexplore.ieee.org/document/5394528/,"2009 47th Annual Allerton Conference on Communication, Control, and Computing (Allerton)",30 Sept.-2 Oct. 2009,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/6GSUMMIT49458.2020.9083875,Histograms to Quantify Dataset Shift for Spectrum Data Analytics: A SoC Based Device Perspective,IEEE,Conferences,"Cloud/software-based wireless resource controllers have been recently proposed to exploit radio frequency (RF) data analytics for a network control, configuration and management. For efficient resource controller design, tracking the right metrics in real-time (analytics) and making realistic predictions (deep learning) will play an important role to increase its efficiency. This factor becomes particularly critical as radio environments are generally dynamic, and the data sets collected may exhibit shift in distribution over time and/or space. When a trained model is deployed at the controller without taking into account dataset shift, a large amount of prediction errors may take place. This paper quantifies dataset shift in real wireless physical layer data by using a statistical distance method called earth mover's distance (EMD). It utilizes an FPGA to process in real-time the inphase and quadrature (IQ) samples to obtain useful information, such as histograms of wireless channel utilization (CU). We have prototyped the data processing modules on a Xilinx System on Chip (SoC) board using Vivado, Vivado HLS, SDK and MATLAB tools. The histograms are sent as low-overhead analytics to the resource controller server where they are processed to evaluate dataset shift. The presented results provide insight into dataset shift in real wireless CU data collected over multiple weeks in the University of Oulu using the implemented modules on SoC devices. The results can be used to design approaches that can prevent failures due to datashift in deep learning models for wireless networks.",https://ieeexplore.ieee.org/document/9083875/,2020 2nd 6G Wireless Summit (6G SUMMIT),17-20 March 2020,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SECON.1991.147712,Hough transform system enhancements resulting from neural network implementation,IEEE,Conferences,A neural network implementation of a Hough transform system has been suggested to make real-time operation possible. That and other developments are summarized and combined to suggest a complete system. Convergence times under 30 mu s are obtainable with general-purpose operational amplifiers for the processing nodes or neurons. Enhancements are possible with an artificial neural network system such as multiple target detection with a simple hardware addition to the basic system. Approximation of the peak magnitude in parameter space is available from the multiple peak detection circuitry. The magnitude of the peak indirectly yields the length of the line segments in image space. The system can adapt to varying image intensities. This feature can be used to locate weak targets and avoid neuron amplifier saturation. A signal perturbation technique can be used to enhance resolution or reduce hardware requirements.<>,https://ieeexplore.ieee.org/document/147712/,IEEE Proceedings of the SOUTHEASTCON '91,7-10 April 1991,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IWQoS54832.2022.9812885,How Asynchronous can Federated Learning Be?,IEEE,Conferences,"As a practical paradigm designed to involve large numbers of edge devices in distributed training of deep learning models, federated learning has witnessed a significant amount of research attention in the recent years. Yet, most existing mechanisms on federated learning assumed either fully synchronous or asynchronous communication strategies between clients and the federated learning server. Existing designs that were partially asynchronous in their communication were simple heuristics, and were evaluated using the number of communication rounds or updates required for convergence, rather than the wall-clock time in practice.In this paper, we seek to explore the entire design space between fully synchronous and asynchronous mechanisms of communication. Based on insights from our exploration, we propose Port, a new partially asynchronous mechanism designed to allow fast clients to aggregate asynchronously, yet without waiting excessively for the slower ones. In addition, Port is designed to adjust the aggregation weights based on both the staleness and divergence of model updates, with provable convergence guarantees. We have implemented Port and its leading competitors in Plato, an open-source scalable federated learning research framework designed from the ground up to emulate real-world scenarios. With respect to the wall-clock time it takes for converging to the target accuracy, Port outperformed its closest competitor, FedBuff, by up to 40% in our experiments.",https://ieeexplore.ieee.org/document/9812885/,2022 IEEE/ACM 30th International Symposium on Quality of Service (IWQoS),10-12 June 2022,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ACSOS49614.2020.00036,How far should I watch? Quantifying the effect of various observational capabilities on long-range situational awareness in multi-robot teams,IEEE,Conferences,"In our previous work, we showed that individual robots within a multi-robot team can gain long-distance situational awareness from passive observations of a single nearby neighbor without any explicit robot-to-robot communication. However, that prior work was developed only in simulation, and performance was not measured for real robot teams in physical space with realistic hardware limitations. Toward this end, we studied the performance of these methods in real robot scenarios with methods using more sophisticated techniques in machine learning to mitigate practical implementation problems. In this study, we further extend that work by characterizing the effects of changing history length and sensor range. Rather than finding that increasing history length and sensor range always yield better estimation performance, we find that the optimal history length and sensor range varies depending on the distance between the estimating robot and the robot being estimated. For estimation problems where the estimation target is nearby, longer histories actually degrade performance, and so sensor ranges could be increased instead. Conversely, for farther targets, history length is as valuable or more valuable than sensor range. Thus, just as optimal shutter speed varies with light availability and speed of the subject, passive situational awareness in multi-robot teams is best achieved with different strategies depending on proximity to locations of interest. All studies use the teams of Thymio II physical, two-wheeled robots in laboratory environments 1.1Data and models used are available at https://github.com/PavlicLab/ACSOS2020_ReTLo_Extension.git.",https://ieeexplore.ieee.org/document/9196255/,2020 IEEE International Conference on Autonomic Computing and Self-Organizing Systems (ACSOS),17-21 Aug. 2020,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCSE.2018.8468759,How to Enhance Chinese Word Segmentation Using Knowledge Graphs,IEEE,Conferences,"Chinese word segmentation is a very important problem for Chinese information processing. Chinese word segmentation results are the basis for computers to understand natural language. However, unlike most Western languages, Chinese words do not have fixed symbols like white space as word segmentation marks. Moreover, Chinese has a very complex grammar, and the word segmentation criteria are varied with the contexts. Therefore, Chinese word segmentation is a very difficult task. Many existing works have proposed many algorithms to solve this problem. However, to our best knowledge, none of them could outperform all the other methods. In this paper, we develop a novel algorithm based on semantics and contexts. We propose a semantic-based word similarity measure using the concept hierarchy in knowledge graphs, and use this measure to prune the different results which are generated by several state-of-the-art Chinese word segmentation methods. The idea is to respectively compute the concept similarity of these words to other words in the text, and choose the word with the highest concept similarity score. To evaluate the effectiveness of the proposed approach, we conduct a series of experiment on two real datasets. The results show that our method outperforms all the state-of-the-art algorithms by filtering out wrong results and retaining correct ones.",https://ieeexplore.ieee.org/document/8468759/,2018 13th International Conference on Computer Science & Education (ICCSE),8-11 Aug. 2018,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IoTDI.2015.19,"Human SLAM, Indoor Localisation of Devices and Users",IEEE,Conferences,"The indoor localisation problem is more complex than just finding whereabouts of users. Finding positions of users relative to the devices of a smart space is even more important. Unfortunately, configuring such systems manually is a tedious process, requires expert knowledge, and is sensitive to changes in the environment. Moreover, many existing solutions do not take user privacy into account. We propose a new system, called Simultaneous Localisation and Configuration (SLAC), to address the problem of locating devices and users relative to those devices, and combine this problem into a single estimation problem. The SLAC algorithm, based on FastSLAM, is able to locate devices using the received signal strength indicator (RSSI) of devices and motion data from users. Simulations have been used to show the performance in a controlled environment and the effect of the amount of RSSI updates on the localisation error. Live tests in non-trivial environments showed that we can achieve room level accuracy and that the localisation can be performed in real time. This is all done locally, i.e. running on a user's device, with respect for privacy and without using any prior information of the environment or device locations. Although promising, more work is required to increase accuracy in larger environments and to make the algorithm more robust for environment noise caused by walls and other objects. Existing techniques, e.g. map fusing, can alleviate these problems.",https://ieeexplore.ieee.org/document/7471364/,2016 IEEE First International Conference on Internet-of-Things Design and Implementation (IoTDI),4-8 April 2016,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ROBIO.2011.6181717,Human-like gradual multi-agent Q-learning using the concept of behavior-based robotics for autonomous exploration,IEEE,Conferences,"In the last few years, the field of mobile robotics has made lots of advancements. These advancements are due to the extensive application of mobile robots for autonomous exploration. Mobile robots are being popularly used for applications in space, underwater explorations, underground coal mines monitoring, inspection in chemical/toxic/ nuclear factories etc. But if these environments are unknown/unpredictable, conventional/ classical robotics may not serve the purpose. In such cases robot learning is the best option. Learning from the past experiences, is one such way for real time application of robots for completely unknown environments. Reinforcement learning is one of the best learning methods for robots using a constant system-environment interaction. Both single and multi-agent concepts are available for implementation of learning. The current research work describes a multi-agent based reinforcement learning using the concept of behaviour-based robotics for autonomous exploration of mobile robots. The concept has also been tested both in indoor and outdoor environments using real-time robots.",https://ieeexplore.ieee.org/document/6181717/,2011 IEEE International Conference on Robotics and Biomimetics,7-11 Dec. 2011,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ETFA45728.2021.9613547,Hybrid Feature Selection for High-Dimensional Manufacturing Data,IEEE,Conferences,"In manufacturing environment, hundreds of input parameters are related to product quality. To build an accurate machine learning model for quality prediction, it is necessary to find major input parameters which have a big influence in quality prediction. The procedure of identifying major factors out of original high-dimensional input parameters is called to be feature selection. This paper proposes a hybrid method for feature selection, which effectively reduces the searching space by leveraging feature subset chosen by Fast Correlation Based Filter (FCBF) and Relief-based feature selection. The computational complexity is proved to be quadratic in feature number, while most of the existing methods suffer from exponential computation complexity. This improvement is crucial especially when we deal with high-dimensional input parameters because it dramatically reduces the computational time. Further, the proposed method outperforms in prediction accuracy as well when it compares with the benchmarking method. It has been demonstrated by the implementation of our method into real-world manufacturing data sets and open source benchmarking data set.",https://ieeexplore.ieee.org/document/9613547/,2021 26th IEEE International Conference on Emerging Technologies and Factory Automation (ETFA ),7-10 Sept. 2021,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/TAI.1998.744843,IBHYS: a new approach to learn users habits,IEEE,Conferences,"Learning interface agents search regularities in the user behavior and use them to predict user's actions. We propose a new inductive concept learning approach, called IBHYS, to learn such regularities. This approach limits the hypothesis search to a small portion of the hypothesis space by letting each training example build a local approximation of the global target function. It allows to simultaneously search several hypothesis spaces and to simultaneously handle hypotheses described in different languages. This approach is particularly suited for learning interface agents because it provides an incremental algorithm with low training time and decision time, which does not require the designer of the interface agent to describe in advance and quite carefully the repetitive patterns searched. We illustrate our approach with two autonomous software agents, the Apprentice and the Assistant, devoted to assist users of interactive programming environments and implemented in Objectworks Smalltalk-80. The Apprentice learns user's work habits using an IBHYS algorithm and the Assistant, based on what has been learnt, proposes to the programmer sequences of actions the user might want to redo. We show, with experimental results on real data, that IBHYS outperforms ID3 both in computing time and predictive accuracy.",https://ieeexplore.ieee.org/document/744843/,Proceedings Tenth IEEE International Conference on Tools with Artificial Intelligence (Cat. No.98CH36294),10-12 Nov. 1998,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/MDM48529.2020.00023,IFLoc: Indoor Height Estimation by Telco Data,IEEE,Conferences,"Understanding the fine-grained distribution of telecommunication (Telco) signals in terms of a three-dimensional (3D) space is important for Telco operators to manage, operate and optimize Telco networks. It is particularly true in nowadays urban cities with a large number of high buildings. One of the key tasks is to infer the location height of mobile devices, e.g., the floor within a high building where mobile devices are located. However, precise height estimation is challenging due to complex Telco signal propagation within an indoor 3D space, sparse cell tower deployment and scarce training samples. To tackle these issues, in this paper, we propose an indoor MR height estimation framework, namely IFLoc, via a machine learning model. IFLoc first builds a training MR database via a pre-processing step to comfortably tag raw MR samples by precisely inferred height from auxiliary data such as GPS and barometer readings. Next, IFLoc trains a regression model for height estimation by a set of developed techniques including 3D space division, post-processing techniques, feature augmentation and an improved SVR (Supported Vector Regression) model. Our evaluation on eight real datasets collected within five representative high buildings in Shanghai validates that IFLoc outperforms state-of-the-art counterparts in particularly with scarce training data.",https://ieeexplore.ieee.org/document/9162333/,2020 21st IEEE International Conference on Mobile Data Management (MDM),30 June-3 July 2020,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/GIOTS.2017.8016227,Identifying parking spaces & detecting occupancy using vision-based IoT devices,IEEE,Conferences,"The increasing number of vehicles in high density, urban areas is leading to significant parking space shortages. While systems have been developed to enable visibility into parking space vacancies for drivers, most rely on costly, dedicated sensor devices that require high installation costs. The proliferation of inexpensive Internet of Things (IoT) devices enables the use of compute platforms with integrated cameras that could be used to monitor parking space occupancy. However, even with camera-captured images, manual specification of parking space locations is required before such devices can be used by drivers after device installation. In this paper, we leverage machine learning techniques to develop a method to dynamically identify parking space topologies based on parked vehicle positions. More specifically, we designed and evaluated an occupation detection model to identify vacant parking spaces. We built a prototype implementation of the whole system using a Raspberry Pi and evaluated it on a real-world urban street near the University of Washington campus. The results show that our clustering-based learning technique coupled with our occupation detection pipeline is able to correctly identify parking spaces and determine occupancy without manual specication of parking space locations with an accuracy of 91%. By dynamically aggregating identied parking spaces from multiple IoT devices using Amazon Cloud Services, we demonstrated how a complete, city-wide parking management system can be quickly deployed at low cost.",https://ieeexplore.ieee.org/document/8016227/,2017 Global Internet of Things Summit (GIoTS),6-9 June 2017,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CCWC47524.2020.9031165,Image Classification on NXP i.MX RT1060 using Ultra-thin MobileNet DNN,IEEE,Conferences,"Deep Neural Networks play a very significant role in computer vision applications like image classification, object recognition and detection. They have achieved great success in this field but the main obstacles for deploying a DNN model into an Autonomous Driver Assisted System (ADAS) platform are limited memory, constrained resources, and limited power. MobileNet is a very efficient and light DNN model which was developed mainly for embedded and computer vision applications, but researchers still faced many constraints and challenges to deploy the model into resource-constrained microprocessor units. Design Space Exploration of such CNN models can make them more memory efficient and less computationally intensive. We have used the Design Space Exploration technique to modify the baseline MobileNet V1 model and develop an improved version of it. This paper proposes seven modifications on the existing baseline architecture to develop a new and more efficient model. We use Separable Convolution layers, the width multiplier hyperparamater, alter the channel depth and eliminate the layers with the same output shape to reduce the size of the model. We achieve a good overall accuracy by using the Swish activation function, Random Erasing technique and a choosing good optimizer. We call the new model as Ultra-thin MobileNet which has a much smaller size, lesser number of parameters, less average computation time per epoch and negligible overfitting, with a little higher accuracy as compared to the baseline MobileNet V1. Generally, when an attempt is made to make an existing model more compact, the accuracy decreases. But here, there is no trade off between the accuracy and the model size. The proposed model is developed with the intent to make it deployable in a realtime autonomous development platform with limited memory and power and, keeping the size of the model within 5 MB. It could be successfully deployed into NXP i.MX RT1060 ADAS platform due to its small model size of 3.9 MB. It classifies images of different classes in real-time, with an accuracy of more than 90% when it is run on the above-mentioned ADAS platform. We have trained and tested the proposed architecture from scratch on the CIFAR-10 dataset.",https://ieeexplore.ieee.org/document/9031165/,2020 10th Annual Computing and Communication Workshop and Conference (CCWC),6-8 Jan. 2020,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/GLOBECOM42002.2020.9348202,Image Download and Rate Allocation of Internet-of-Things Analytics at Gateways in Smart Cities,IEEE,Conferences,"Internet-of-Things (IoT) devices are connected to the Internet through a gateway, which can host IoT analytics encapsulated in containers to convert raw sensor data into more condensed processed data. In this paper, we study two research problems to maximize the overall Quality-of-Service (QoS) level of all IoT analytics that run on both data center servers and gateways. The first problem is to select additional IoT analytics to deploy on a gateway to save upload bandwidth due to transmitting raw sensor data. The second problem is to allocate the residue upload bandwidth among all IoT analytics to maximize the overall QoS level. We propose several algorithms to solve these two research problems. We have implemented real testbeds to evaluate our proposed system and algorithms. Our experiment results reveal that the proposed algorithms: (i) capitalize the download bandwidth and storage space of the gateway for saving the upload bandwidth consumption and (ii) achieve high QoS levels without overloading the network and gateway.",https://ieeexplore.ieee.org/document/9348202/,GLOBECOM 2020 - 2020 IEEE Global Communications Conference,7-11 Dec. 2020,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICM52667.2021.9664950,Image Inpainting and Classification Agent Training Based on Reinforcement Learning and Generative Models with Attention Mechanism,IEEE,Conferences,"What distinguishes the field of artificial intelligence (AI) from others is to develop fully independent agents that learn optimal behavior, change, and evolve solely through the communication of trial and error with the surrounding environment. Reinforcement learning (RL) can be seen in multiple aspects of Machine Learning (ML), provided the environment, reward, actions, the state will be defined. Agent training in previous years is seen to only relate to robotics, games, and self-driving cars. While trying to divert the focus of researchers from the view of self-driving cars, games, robots, etc. Here, we investigated using reinforcement learning in the aspect of task completion. We deployed our architecture in an inpainting task where the agent generates the distorted or missing image content into an eminent fidelity completed the image by using reinforcement learning to influence the generative model utilized. The Generative Adversary Network (GAN) problem of not being steady and challenging to train was overwhelmed by utilizing latent space representation. The dimension is reduced compared to the distorted or corrupted image in training the GAN. Then reinforcement learning was deployed to pick the correct GAN input to get the image’s latent space representation that is most suitable for the current input of the missing or distorted image region. In this paper, we also learned that the trained agent enhances the accuracy in a classification task of images with missing data. We successfully examined the classification enhancement on images missing 30%, 50%, and 70%.",https://ieeexplore.ieee.org/document/9664950/,2021 International Conference on Microelectronics (ICM),19-22 Dec. 2021,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ELECTR.1991.718282,Imaging And Controls For Mars Robots With Neural Networks,IEEE,Conferences,"Two aspects of the design of space robots is covered implemented by neural networks and by hybrid approach with artificial intelligence. One is a neurocontroller for a real-time autonomous system. An optical control system developed saves the time for the image processing that analyzes an image sensor through the environment and induces a transformation over the sensor array. A prototype of the neurocontroller is able to learn and control by itself. The second aspect deals with the design of a Servo Control System for a Robot with the capability of ""learning in Unanticipated Situations"" incorporated in the system. The robot is assumed to be employed to perform useful tasks in an alien evironment. The model developed is shown to provide the robot with the capability to recover from unanticipated situations that can lead to the disruption of its normal operation, and to learn to avoid such situations in the future. These two aspects will be integrated for a design of a very intelligent autonomous space robot.",https://ieeexplore.ieee.org/document/718282/,"Electro International, 1991",16-18 April 1991,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.23919/ITUK50268.2020.9303205,Immersive Technologies for Development: An Analysis of Agriculture,IEEE,Conferences,"Agricultural development is key to any economic development. Immersive technology plays a catalytic role and offers smart and sustainable choices to farmers who want to improve on agricultural productivity, and to agricultural training institutes that seek to use modern technology to advance pedagogy and reduce fatalities and operating costs in the learning space, among others. Currently, there are limited descriptive literature reviews in the area of immersive technology in agriculture, hereafter referred to as AVR-Agric. This paper presents a systematic literature review (SLR), which offers a structured, methodical, and rigorous approach to the understanding of the trend of research in AVR-Agric, and the least and most researched issues. This study explores and examines the current trends in the immersive technology-based agriculture areas, and provides a credible intellectual guide for future research direction. The SLR was limited to existing applications and peer-reviewed conference and journal articles published from 2006 to 2020. The results showed that virtual reality was implemented in 41% of the papers reviewed, augmented reality was found in 53%, while only 6% considered mixed-reality applications. The study also showed that developments that incorporate IoT, blockchain, and machine-learning technologies are still at their stage of exploration and advancement.",https://ieeexplore.ieee.org/document/9303205/,2020 ITU Kaleidoscope: Industry-Driven Digital Transformation (ITU K),7-11 Dec. 2020,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/GCCE46687.2019.9015335,Implementation of 3D Drawing to Interactive Display System,IEEE,Conferences,"Our laboratory has been developing an entertainment system called the “light crayon”. This system can draw on a screen with light pointer. In this paper, we propose a new system that can draw on 3D(three-dimensional) space with Mixed Reality technology.",https://ieeexplore.ieee.org/document/9015335/,2019 IEEE 8th Global Conference on Consumer Electronics (GCCE),15-18 Oct. 2019,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/I-SMAC52330.2021.9640863,Implementation of Machine Learning Classifier for DTN Routing,IEEE,Conferences,"This paper presents, better routing method in Delay Tolerant Network using Machine learning. Delay Tolerant Network is a wireless network, in which nodes are changing its positions dynamically in an unexpected way due to that Round trip time and error rates are very high. Examples are Disaster area, under the sea, Space communication, etc. In the proposed method neighbouring nodes are predicted by machine learning classifiers. These nodes use message history delivery information to deliver the message on destination. With the help of Bundle protocol implementation IBR-DTN [3], collects network traffic status and real-world location trace. These information uses to emulate DTN environment by Common Open Research Emulator (CORE) [2]. The new application is used to predict the results, preparation for the network history data, analysis and classification-based routing.",https://ieeexplore.ieee.org/document/9640863/,"2021 Fifth International Conference on I-SMAC (IoT in Social, Mobile, Analytics and Cloud) (I-SMAC)",11-13 Nov. 2021,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/FUZZ.2001.1007312,Implementation of a general real-time visual anomaly detection system via soft computing,IEEE,Conferences,"The intelligent visual system detects anomalies or defects in real time under normal lighting operating conditions. The application is basically a learning machine that integrates fuzzy logic (FL), artificial neural network (ANN), and genetic algorithm (GA) schemes to process the image, run the learning process, and finally detect the anomalies or defects. The system acquires the image, performs segmentation to separate the object being tested from the background, preprocesses, segments, and retrieves regions. FL provides a powerful framework for knowledge representation and overcomes uncertainty and vagueness typically found in image analysis. An application prototype currently runs on a regular PC under Windows NT, and preliminary work has been performed to build an embedded version with multiple image processors. The application prototype is being tested at the Kennedy Space Center to detect anomalies along slide basket cables utilized by the astronauts to evacuate the Shuttle launch pad in an emergency.",https://ieeexplore.ieee.org/document/1007312/,10th IEEE International Conference on Fuzzy Systems. (Cat. No.01CH37297),2-5 Dec. 2001,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IJCNN.2008.4633972,Implementation of a neural network based visual motor control algorithm for A 7 DOF redundant manipulator,IEEE,Conferences,"This paper deals with visual-motor coordination of a 7 dof robot manipulator for pick and place applications. Three issues are dealt with in this paper - finding a feasible inverse kinematic solution without using any orientation information, resolving redundancy at position level and finally maintaining the fidelity of information during clustering process thereby increasing accuracy of inverse kinematic solution. A 3-dimensional KSOM lattice is used to locally linearize the inverse kinematic relationship. The joint angle vector is divided into two groups and their effect on end-effector position is decoupled using a concept called function decomposition. It is shown that function decomposition leads to significant improvement in accuracy of inverse kinematic solution. However, this method yields a unique inverse kinematic solution for a given target point. A concept called sub-clustering in configuration space is suggested to preserve redundancy during learning process and redundancy is resolved at position level using several criteria. Even though the training is carried out off-line, the trained network is used online to compute the required joint angle vector in only one step. The accuracy attained is better than the current state of art. The experiment is implemented in real-time and the results are found to corroborate theoretical findings.",https://ieeexplore.ieee.org/document/4633972/,2008 IEEE International Joint Conference on Neural Networks (IEEE World Congress on Computational Intelligence),1-8 June 2008,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CCAAW.2017.8001607,Implementation of a space communications cognitive engine,IEEE,Conferences,"Although communications-based cognitive engines have been proposed, very few have been implemented in a full system, especially in a space communications system. In this paper, we detail the implementation of a multi-objective reinforcement-learning algorithm and deep artificial neural networks for the use as a radio-resource-allocation controller. The modular software architecture presented encourages re-use and easy modification for trying different algorithms. Various trade studies involved with the system implementation and integration are discussed. These include the choice of software libraries that provide platform flexibility and promote reusability, choices regarding the deployment of this cognitive engine within a system architecture using the DVB-S2 standard and commercial hardware, and constraints placed on the cognitive engine caused by real-world radio constraints. The implemented radio-resource-allocation-management controller was then integrated with the larger space-ground system developed by NASA Glenn Research Center (GRC).",https://ieeexplore.ieee.org/document/8001607/,2017 Cognitive Communications for Aerospace Applications Workshop (CCAA),27-28 June 2017,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/GeoInformatics.2011.5980945,Implementation of a vector-based cellular automata model for simulating land-use changes,IEEE,Conferences,"In the last few years, cellular automata (CA) models have been increasingly used to simulate the complex land-use changes. However, the traditional regular raster-based CA models are sensitive to the cell size and the neighborhood configuration used in the models, which restrict its ability to simulate the real world. By representing space as irregular shape and size geographic objects and defining artificial neural network as transition rule, a vector-based CA model is constructed and applied to the simulation and prediction of land-use changes. By taking the north branch of Yangtze River estuary as an example, studies illustrated that the vector-based CA model can make full use of artificial neural network to obtain the variable space parameters and simplify the land-use transfer rule. It is concluded that the vector-based CA model produces more realistic spatial patterns than those generated by a raster-based CA model.",https://ieeexplore.ieee.org/document/5980945/,2011 19th International Conference on Geoinformatics,24-26 June 2011,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IJCNN.2005.1556443,Implementation of an MLP-based DOA system using a reduced number of MM-wave antenna elements,IEEE,Conferences,"It is required to know the direction of arrival (DOA) of a signal in many applications, such as car tracking or reception optimization for satellite antenna. However, several reflections of different intensities highly affect the sensor outputs and the estimation quality. The system presented here is divided into three parts: a pair of three-element antenna arrays which receives the main beam and the reflected one, a radio frequency combiner which generates power signals and multilayer perceptron neural networks used to invert the mapping between the DOA space and the combiner output space. Simulations are carried out including a model of a simple reflection over a road. They are validated by experimental dataset provided by real antenna array outputs coming from tests using an asphalt reflector in an anechoic chamber.",https://ieeexplore.ieee.org/document/1556443/,"Proceedings. 2005 IEEE International Joint Conference on Neural Networks, 2005.",31 July-4 Aug. 2005,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CAIPT.2017.8320692,Implementation of real-time static hand gesture recognition using artificial neural network,IEEE,Conferences,"Sign language is a language that requires the combination of hand gesture, orientation, movement of the hands, arms, body, and facial to simultaneously express the thoughts of the speaker. This paper implements static hand gesture recognition in recognizing the alphabetical sign from “A” to “Z”, number from “0” to “9”, and additional punctuation mark such as “Period”, “Question Mark”, and “Space”in Sistem Isyarat Bahasa Indonesia (SIBI). Hand gestures are obtained by evaluating the contour representation from image segmentation of the glove wore by user and then is classified using Artificial Neural Network based on the training model previously built from 100 images for each gesture. The accuracy rate of hand gesture translation is calculated to be 90%. Speech translation recognized NATO phonetic letter as the speech input for translation.",https://ieeexplore.ieee.org/document/8320692/,2017 4th International Conference on Computer Applications and Information Processing Technology (CAIPT),8-10 Aug. 2017,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/FPT.2018.00015,Implementing NEF Neural Networks on Embedded FPGAs,IEEE,Conferences,"Low-power, high-speed neural networks are critical for providing deployable embedded AI applications at the edge. We describe an FPGA implementation of Neural Engineering Framework (NEF) networks with online learning that outperforms mobile GPU implementations by an order of magnitude or more. Specifically, we provide an embedded Python-capable PYNQ FPGA implementation supported with a High-Level Synthesis (HLS) workflow that allows sub-millisecond implementation of adaptive neural networks with low-latency, direct I/O access to the physical world. We tune the precision of the different intermediate variables in the code to achieve competitive absolute accuracy against slower and larger floating-point reference designs. The online learning component of the neural network exploits immediate feedback to adjust the network weights to best support a given arithmetic precision. As the space of possible design configurations of such networks is vast and is subject to a target accuracy constraint, we use the Hyperopt hyper-parameter tuning tool instead of manual search to find Pareto optimal designs. Specifically, we are able to generate the optimized designs in under 500 iterations of Vivado HLS before running the complete Vivado place-and-route phase on that subset. For neural network populations of 64-4096 neurons and 1-8 representational dimensions our optimized FPGA implementation generated by Hyperopt has a speedup of 10-484× over a competing cuBLAS implementation on the Jetson TX1 GPU while using 2.4-9.5× less power. Our speedups are a result of HLS-specific reformulation (15× improvement), precision adaptation (4× improvement), and low-latency direct I/O access (1000× improvement).",https://ieeexplore.ieee.org/document/8742303/,2018 International Conference on Field-Programmable Technology (FPT),10-14 Dec. 2018,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IAMA.2009.5228045,Improved classification association rule mining,IEEE,Conferences,"Classification aims to define an abstract model of a set of classes, called classifier, which is built from a set of labeled data, the training set. However, in large or correlated data sets, association rule mining may yield huge rule sets. Hence several pruning techniques have been proposed to select a small subset of high-quality rules. Since the availability of a ldquorichrdquo rule set may improve the accuracy of the classifier, we argue that rule pruning should be reduced to a minimum. A small subset of high-quality rules is first considered. When this set is not able to classify the data, a larger rule set is exploited. This second set includes rules usually discarded by previous approaches. To cope with the need of mining large rule sets and to efficiently use them for classification, a compact form is proposed to represent a complete rule set in a space-efficient way and without information loss. An extensive experimental evaluation on real and synthetic data sets shows that improves the classification accuracy with respect to previous approaches.",https://ieeexplore.ieee.org/document/5228045/,2009 International Conference on Intelligent Agent & Multi-Agent Systems,22-24 July 2009,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/NSSMIC.1994.474587,Improved resolution via 3D iterative reconstruction for PET volume imaging,IEEE,Conferences,"The authors have implemented iterative filtered backprojection (IFBP) and maximum likelihood by expectation maximization (ML-EM) algorithms in 3D space and applied them to phantom and real PET data. Transaxial resolution improves /spl ap/50% and axial resolution improves /spl ap/15% for IFBP at 15 iterations without a sieve compared to FBP. With a sieve, the improvements are reduced to /spl ap/6%. 3D ML-EM reconstruction shows similar resolution improvement with a much slower convergence rate compared to IFBP. The improvements in resolution from both IFBP and ML-EM are apparent in 3D FDG brain data.<>",https://ieeexplore.ieee.org/document/474587/,Proceedings of 1994 IEEE Nuclear Science Symposium - NSS'94,30 Oct.-5 Nov. 1994,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1145/3238147.3238206,Improving Automatic Source Code Summarization via Deep Reinforcement Learning,IEEE,Conferences,"Code summarization provides a high level natural language description of the function performed by code, as it can benefit the software maintenance, code categorization and retrieval. To the best of our knowledge, most state-of-the-art approaches follow an encoder-decoder framework which encodes the code into a hidden space and then decode it into natural language space, suffering from two major drawbacks: a) Their encoders only consider the sequential content of code, ignoring the tree structure which is also critical for the task of code summarization; b) Their decoders are typically trained to predict the next word by maximizing the likelihood of next ground-truth word with previous ground-truth word given. However, it is expected to generate the entire sequence from scratch at test time. This discrepancy can cause an exposure bias issue, making the learnt decoder suboptimal. In this paper, we incorporate an abstract syntax tree structure as well as sequential content of code snippets into a deep reinforcement learning framework (i.e., actor-critic network). The actor network provides the confidence of predicting the next word according to current state. On the other hand, the critic network evaluates the reward value of all possible extensions of the current state and can provide global guidance for explorations. We employ an advantage reward composed of BLEU metric to train both networks. Comprehensive experiments on a real-world dataset show the effectiveness of our proposed model when compared with some state-of-the-art methods.",https://ieeexplore.ieee.org/document/9000003/,2018 33rd IEEE/ACM International Conference on Automated Software Engineering (ASE),3-7 Sept. 2018,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS51168.2021.9636239,Improving Competence via Iterative State Space Refinement,IEEE,Conferences,"Despite considerable efforts by human designers, accounting for every unique situation that an autonomous robotic system deployed in the real world could face is often an infeasible task. As a result, many such deployed systems still rely on human assistance in various capacities to complete certain tasks while staying safe. Competence-aware systems (CAS) is a recently proposed model for reducing such reliance on human assistance while in turn optimizing the system’s global autonomous operation by learning its own competence. However, such systems are limited by a fixed model of their environment and may perform poorly if their a priori planning model does not include certain features that emerge as important over the course of the system’s deployment. In this paper, we propose a method for improving the competence of a CAS over time by identifying important state features missing from the system’s model and incorporating them into its state representation, thereby refining its state space. Our approach exploits information that exists in the standard CAS model and adds no extra work to the human. The result is an agent that better predicts human involvement, improving its competence, reliability, and overall performance.",https://ieeexplore.ieee.org/document/9636239/,2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),27 Sept.-1 Oct. 2021,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/HPDC.2006.1652187,Improving Resource Matching Through Estimation of Actual Job Requirements,IEEE,Conferences,"Heterogeneous clusters and grid infrastructures are becoming increasingly popular. In these computing infrastructures, machines have different resources (e.g., memory sizes, disk space, and installed software packages). These differences give rise to a problem of over-provisioning, that is, sub-optimal utilization of a cluster due to users requesting resource capacities greater than what their jobs actually need. Our analysis of a real workload file (LANL CM 5) revealed differences of up to two orders of magnitude between requested memory capacity and actual memory usage. The problem of over-provisioning has received very little attention so far. We discuss different approaches for applying machine learning methods to estimate the actual resource capacities used by jobs. These approaches are independent of the scheduling policies and the dynamic resource-matching schemes used. Our simulations show that these methods can yield an improvement of over 50% in utilization (throughput) of heterogeneous clusters",https://ieeexplore.ieee.org/document/1652187/,2006 15th IEEE International Conference on High Performance Distributed Computing,19-23 June 2006,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/HPCA.2018.00018,In-Situ AI: Towards Autonomous and Incremental Deep Learning for IoT Systems,IEEE,Conferences,"Recent years have seen an exploration of data volumes from a myriad of IoT devices, such as various sensors and ubiquitous cameras. The deluge of IoT data creates enormous opportunities for us to explore the physical world, especially with the help of deep learning techniques. Traditionally, the Cloud is the option for deploying deep learning based applications. However, the challenges of Cloud-centric IoT systems are increasing due to significant data movement overhead, escalating energy needs, and privacy issues. Rather than constantly moving a tremendous amount of raw data to the Cloud, it would be beneficial to leverage the emerging powerful IoT devices to perform the inference task. Nevertheless, the statically trained model could not efficiently handle the dynamic data in the real in-situ environments, which leads to low accuracy. Moreover, the big raw IoT data challenges the traditional supervised training method in the Cloud. To tackle the above challenges, we propose In-situ AI, the first Autonomous and Incremental computing framework and architecture for deep learning based IoT applications. We equip deep learning based IoT system with autonomous IoT data diagnosis (minimize data movement), and incremental and unsupervised training method (tackle the big raw IoT data generated in ever-changing in-situ environments). To provide efficient architectural support for this new computing paradigm, we first characterize the two In-situ AI tasks (i.e. inference and diagnosis tasks) on two popular IoT devices (i.e. mobile GPU and FPGA) and explore the design space and tradeoffs. Based on the characterization results, we propose two working modes for the In-situ AI tasks, including Single-running and Co-running modes. Moreover, we craft analytical models for these two modes to guide the best configuration selection. We also develop a novel two-level weight shared In-situ AI architecture to efficiently deploy In-situ tasks to IoT node. Compared with traditional IoT systems, our In-situ AI can reduce data movement by 28-71%, which further yields 1.4X-3.3X speedup on model update and contributes to 30-70% energy saving.",https://ieeexplore.ieee.org/document/8327001/,2018 IEEE International Symposium on High Performance Computer Architecture (HPCA),24-28 Feb. 2018,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/STPEC52385.2021.9718693,Incipient Faults Detection in Induction Motor using MLP-NN and RBF-NN-based Fault Classifier,IEEE,Conferences,"Stator winding inter-turn faults (SITFs) diagnosis has enormously exploited motor current signatures, and eccentricity faults (EFs) detection has significantly investigated vibration and current signals. However, the motor-current signature analysis sometimes may need other diagnostics techniques for indicating the faulty events. The measurement of vibration signals using an accelerometer is an expensive task. The researchers appreciably implemented artificial intelligence (AI) systems for incipient fault detection in induction motors (IMs). State of the art mainly signifies fault detection techniques for IMs based on the statistical parameters as an input frame to the neural networks (NN). However, in this paper, the proposed neural networks are equipped with the twelve real input parameters to meet the desired fault identification and classification. The NN is trained by deliberately creating the SITFs, EFs, and both faults simultaneously. Multilayer perceptron (MLP) and radial basis function (RBF) based NN models are designed and verified for optimal performance in fault detection techniques. The experimental data set of a three-phase, 420 V, 4 pole squirrel cage induction motor (SCIM) is harnessed to develop the proposed NN. The proposed fault classifier has proven to have reduced mean square error (MSE) and better classification accuracy as compared to classifiers with statistical parameters used as an input feature space.",https://ieeexplore.ieee.org/document/9718693/,"2021 IEEE 2nd International Conference on Smart Technologies for Power, Energy and Control (STPEC)",19-22 Dec. 2021,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICTAI.2019.00226,Incorporating Domain Knowledge in Learning Word Embedding,IEEE,Conferences,"Word embedding is a Natural Language Processing (NLP) technique that automatically maps words from a vocabulary to vectors of real numbers in an embedding space. It has been widely used in recent years to boost the performance of a variety of NLP tasks such as named entity recognition, syntactic parsing and sentiment analysis. Classic word embedding methods such as Word2Vec and GloVe work well when they are given a large text corpus. When the input texts are sparse as in many specialized domains (e.g., cybersecurity), these methods often fail to produce high-quality vectors. In this paper, we describe a novel method, called Annotation Word Embedding (AWE), to train domain-specific word embeddings from sparse texts. Our method is generic and can leverage diverse types of domain knowledge such as domain vocabulary, semantic relations and attribute specifications. Specifically, our method encodes diverse types of domain knowledge as text annotations and incorporates the annotations in word embedding. We have evaluated AWE in two cybersecurity applications: identifying malware aliases and identifying relevant Common Vulnerabilities and Exposures (CVEs). Our evaluation results have demonstrated the effectiveness of our method over state-of-the-art baselines.",https://ieeexplore.ieee.org/document/8995379/,2019 IEEE 31st International Conference on Tools with Artificial Intelligence (ICTAI),4-6 Nov. 2019,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICMSAO.2013.6552635,Incremental Bayesian network structure learning in high dimensional domains,IEEE,Conferences,"The recent advances in hardware and software has led to development of applications generating a large amount of data in real-time. To keep abreast with latest trends, learning algorithms need to incorporate novel data continuously. One of the efficient ways is revising the existing knowledge so as to save time and memory. In this paper, we proposed an incremental algorithm for Bayesian network structure learning. It could deal with high dimensional domains, where whole dataset is not completely available, but grows continuously. Our algorithm learns local models by limiting search space and performs a constrained greedy hill-climbing search to obtain a global model. We evaluated our method on different datasets having several hundreds of variables, in terms of performance and accuracy. The empirical evaluation shows that our method is significantly better than existing state of the art methods and justifies its effectiveness for incremental use.",https://ieeexplore.ieee.org/document/6552635/,"2013 5th International Conference on Modeling, Simulation and Applied Optimization (ICMSAO)",28-30 April 2013,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/DEVLRN.2014.6983001,Incremental training of Restricted Boltzmann Machines using information driven saccades,IEEE,Conferences,"In the context of developmental robotics, a robot has to cope with complex sensorimotor spaces by reducing their dimensionality. In the case of sensor space reduction, classical approaches for pattern recognition use either hardcoded feature detection or supervised learning. We believe supervised learning and hard-coded feature extraction must be extended with unsupervised learning of feature representations. In this paper, we present an approach to learn representations using space-variant images and saccades. The saccades are driven by a measure of quantity of information in the visual scene, emerging from the activations of Restricted Boltzmann Machines (RBMs). The RBM, a generative model, is trained incrementally on locations where the system saccades. Our approach is implemented using real data captured by a NAO robot in indoor conditions.",https://ieeexplore.ieee.org/document/6983001/,4th International Conference on Development and Learning and on Epigenetic Robotics,13-16 Oct. 2014,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ASIANCON51346.2021.9544615,Indian Sign language Recognition Using Color Space Model and Thresholding,IEEE,Conferences,"Sign language is for deaf and mute people. Population as large as India's provides for an enormous section of people using Indian sign language to communicate. Unfortunately, a major chunk of the population does not understand sign language which limits communication between those with disabilities and wider population. We are proposing a system to harness this communication chasm. First the images are taken from webcam in RGB color space. Preprocessing and semantic segmentation are applied on the input image. By employing a simple background, the RGB segmented image is transformed to gray scale and background noise is removed. Otsu's segmentation method is used to segment the image. Convolutional Neural Networks were employed, utilizing skin segmented hand images as the input. On the training data, a classification accuracy of 99.33% was attained for 36 static hand gestures from Indian Sign Language. The above mentioned model performed significantly well in real time implementation.",https://ieeexplore.ieee.org/document/9544615/,2021 Asian Conference on Innovation in Technology (ASIANCON),27-29 Aug. 2021,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/BWCCA.2011.52,Indoor Location Fingerprinting Based on Data Reduction,IEEE,Conferences,"Agent localization in indoor wireless environments is a challenging issue. Numerous techniques have been developed. Location fingerprinting, which is based on received signal strength measurements, is a frequently used approach for indoor applications. In this paper, we examine the possibility to obtain the location fingerprinting method characterized with more accurate mapping between the signal-space and the physical-space. An implemented well-known weighted k-nearest neighbor (WkNN) method is enhanced by two steps: a) pre-processing by the unsupervised learning technique during radio map building and b) post-processing of initial estimates obtained by the WkNN localization method. In this post-processing step signal-space and physical-space are transformed and mapped using two techniques of the dimension reduction: principal component analysis and multidimensional scaling. The aim of this transformation step is to de-correlate and refine initially obtained location estimates. Parameters such as number of access points and number of nearest reference nodes are examined for their impact on accuracy of the presented localization techniques. Performances are examined and verified through the experiments with real environment data.",https://ieeexplore.ieee.org/document/6103053/,"2011 International Conference on Broadband and Wireless Computing, Communication and Applications",26-28 Oct. 2011,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SMC.2013.183,Indoor Positioning with Virtual Fingerprint Mapping by Using Linear and Exponential Taper Functions,IEEE,Conferences,"A 2D localization system is constructed by using Wireless Sensor Nodes (WSN) to create a Virtual Fingerprint map. Linear and exponential taper functions are utilized with the received signal strength distributions between the fingerprint nodes to generate virtual fingerprint maps. Thus, a real and virtual combined fingerprint map is generated across the test area. k-NN and k-NN weighted algorithms have been implemented on virtual fingerprint maps to find the coordinates of the unknown objects. The system Localization accuracies of less than a grid space are obtained in calculations.",https://ieeexplore.ieee.org/document/6721936/,"2013 IEEE International Conference on Systems, Man, and Cybernetics",13-16 Oct. 2013,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ITAIC.2019.8785597,Induction motor control system based on FOC algorithm,IEEE,Conferences,"In today's increasingly severe energy crisis, new energy vehicles have zero emissions, high efficiency, environmentally friendly features are sought after by people, influenced by policies and future market trends, the development of a highly efficient, versatile motor control It is particularly important. In this paper, the mathematical model of the high-power asynchronous motor of new energy vehicles is constructed. Analysis of its control principle, By analyzing the mathematical model of the three-wire AC asynchronous motor under the three-phase static coordinates, the three-phase coordinate system of the motor is decoupled into a two-phase rotating coordinate system similar to the DC motor by using the Clark transform Park variation mathematical formula. By detecting the phase current, the torque component and the excitation component of the motor are used in the above method, and the magnetic flux observer of the motor is designed by the method of magnetic field orientation. Based on the FOC field oriented control algorithm, TI's TMS320F28035 high-performance digital signal processor has developed an efficient and versatile induction motor control system with a speed loop and a current loop double closed loop. Controls the A/D to sample the voltage and current of the controller and build its current model. The real-time speed of the motor is detected using the incremental photoelectric encoder M/T method. The DSP-enhanced PWM (EPWM) module is used to generate high-precision flexible space vector pulse width modulation (SVPWM) signals, which greatly improves the control efficiency and speed range of the motor. By downloading the prepared control software to the controller hardware platform, the signal output test and the motor load test show that the controlled motor runs smoothly, the speed is stable, the corresponding speed is fast, and the parameters can be easily modified to adapt different motors, which saves the cost and development cycle for the enterprise.",https://ieeexplore.ieee.org/document/8785597/,2019 IEEE 8th Joint International Information Technology and Artificial Intelligence Conference (ITAIC),24-26 May 2019,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/GIOTS49054.2020.9119497,Industrial IoT and Digital Twins for a Smart Factory : An open source toolkit for application design and benchmarking,IEEE,Conferences,"The rapid evolution of digital technology and designed intelligence, such as the Internet of Things (IoT), Big data analytics, Artificial Intelligence (AI), Cyber Physical Systems (CPS), has been a catalyst for the 4th industrial revolution (known as industry 4.0). Among other, the two key state-of-the-art concepts in Industry 4.0, are Industrial IoT (IIoT) and digital twins. IIoT facilitates real-time data acquisition, processing and analytics over large amount of sensor data streams produced by sensors installed within a smart factory, while the ‘digital twin’ concept aims to enable smart factories via the digital replication or representation of physical machines, processes, people in cyber-space. This paper explores the capability of present-state open-source platforms to collectively achieve digital twin capabilities, including IoT real-time data acquisition, virtual representation, analytics, and visualisation. The aim of this work is to ‘close the gap’ between research and implementation, through a collective open source IoT and Digital Twin architecture. The performance of the open-source architecture in this work, is demonstrated in a use-case utilising industry ‘open data’, and is bench-marked with universal testing tools.",https://ieeexplore.ieee.org/document/9119497/,2020 Global Internet of Things Summit (GIoTS),3-3 June 2020,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.23919/ACC45564.2020.9147268,Inferential Methods for Additive Manufacturing Feedback,IEEE,Conferences,"Adaptive manufacturing has revolutionized desktop prototyping and the production of physical models for non-load bearing or stress inducing applications. Many extrusion-based printers are available for purchase by entrepreneurial enthusiasts or businesses with manufacturing space limitations. These low-cost printers allow for quick prototyping but are not designed or intended for high quality production or high-cycle production, requiring extensive user tuning and upkeep to maintain the printer in usable condition. In a quest to apply modern deep learning and reinforcement learning based models, this work focuses on the development of control systems and infrastructure needed to resolve many of these intrinsic limitations of desktop 3D printers. A series of real-time agents were designed and deployed to actively monitor the printing of every layer and make continuous corrections in the printing parameters and G-code commands to reduce the variance in the tensile strength of homogeneous parts printed in a large batch.",https://ieeexplore.ieee.org/document/9147268/,2020 American Control Conference (ACC),1-3 July 2020,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/MLSP.2014.6958856,Inferring clinical depression from speech and spoken utterances,IEEE,Conferences,"In this paper, we investigate the problem of detecting depression from recordings of subjects' speech using speech processing and machine learning. There has been considerable interest in this problem in recent years due to the potential for developing objective assessments from real-world behaviors, which may provide valuable supplementary clinical information or may be useful in screening. The cues for depression may be present in “what is said” (content) and “how it is said” (prosody). Given the limited amounts of text data, even in this relatively large study, it is difficult to employ standard method of learning models from n-gram features. Instead, we learn models using word representations in an alternative feature space of valence and arousal. This is akin to embedding words into a real vector space albeit with manual ratings instead of those learned with deep neural networks [1]. For extracting prosody, we employ standard feature extractors such as those implemented in openSMILE and compare them with features extracted from harmonic models that we have been developing in recent years. Our experiments show that our features from harmonic model improve the performance of detecting depression from spoken utterances than other alternatives. The context features provide additional improvements to achieve an accuracy of about 74%, sufficient to be useful in screening applications.",https://ieeexplore.ieee.org/document/6958856/,2014 IEEE International Workshop on Machine Learning for Signal Processing (MLSP),21-24 Sept. 2014,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/NeuroNT55429.2022.9805531,Information Threat Recognition Method Using a Neural Network,IEEE,Conferences,"Proposed a method of continuous processing of network data, which increases the speed and efficiency of response to network infrastructure attacks. The paper implements a method of data preparation, identifies the main characteristics that allow to frame the attack space. Proposed neural network architecture, consists of two recurrent layers and one fully connected layer. The process of training and analyzing the effectiveness of network threat recognition on a deferred sample of data was formed. The result of the work is a software module capable of classifying the current security state of network traffic and informing the user or the protection system about the detection of a threat.",https://ieeexplore.ieee.org/document/9805531/,2022 III International Conference on Neural Networks and Neurotechnologies (NeuroNT),16-16 June 2022,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICMLC.2008.4620601,Infrared passenger flow collection system based on RBF neural net,IEEE,Conferences,"As the current level of the people-counting, on the consideration of the cost of the acquisition equipment and the acquisition accuracy, the customer-counting system is constructed based on RBF neural net using technology of infrared photoelectric sensor. For better differential count, extant data segmentation and the method of feature extraction is improved based on the feature of passenger-counting data continuous space-time sequence. Compared with traditional customer-counting using sensor, the accuracy of real-time customer-counting is improved in this method and the situation of customers who entry at the same time can be identified with lower error rate. It is helpful for both principle research and actual application.",https://ieeexplore.ieee.org/document/4620601/,2008 International Conference on Machine Learning and Cybernetics,12-15 July 2008,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SPW53761.2021.00016,Innocent Until Proven Guilty (IUPG): Building Deep Learning Models with Embedded Robustness to Out-Of-Distribution Content,IEEE,Conferences,"Deep Neural Network classifiers trained with the conventional Categorical Cross-Entropy loss face problems in real-world environments such as a tendency to produce overly confident posterior distributions on out-of-distribution inputs, sensitivity to adversarial noise, and lost performance due to distributional shift. We hypothesize that a central shortcoming -an inability to effectively process out-of-distribution content within inputs-exacerbates each of these setbacks. In response, we propose a novel learning framework called Innocent Until Proven Guilty which prototypes training data clusters or classes within the input space while uniquely leveraging noise and inherently random classes to discover noise-resistant, uniquely identifiable features of the modeled classes. In evaluation, we leverage both academic computer vision datasets and realworld JavaScript and URL datasets for malware classification. Across these interdisciplinary settings, we observe favorable classification performance on test data, decreased loss of performance due to recency bias, decreased false-positive responses on noise samples, and decreased vulnerability in several noisebased attack simulations when compared to a baseline network of equal topology trained with Categorical Cross-Entropy. To the best of our knowledge, ours is the first work that demonstrates significantly decreased vulnerability to blackbox append attacks on malware. By applying the well-known FastGradient Sign Method, we show the potential to combine our framework with existing adversarial learning techniques and discover favorable performance by a significant margin. Our framework is general enough for use with any network topology that could otherwise be trained with Categorical Cross-Entropy.",https://ieeexplore.ieee.org/document/9474279/,2021 IEEE Security and Privacy Workshops (SPW),27-27 May 2021,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICMLC.2007.4370315,Integrating Incremental Feature Weighting into NaÏve Bayes Text Classifier,IEEE,Conferences,"In the real-world operational environment, text classification systems should handle the problem of incomplete training set and no prior knowledge of feature space. In this regard, the most appropriate algorithm for operational text classification is the Naïve Bayes since it is easy to incrementally update its pre-learned classification model and feature space. Our work mainly focuses on improving Naïve Bayes classifier through feature weighting strategy. The basic idea is that parameter estimation of Naïve Bayes can consider the degree of feature importance as well as feature distribution. In addition, we have extended a conventional algorithm for incremental feature update for developing a dynamic feature space in operational environment. Through experiments using the Reuters-21578 and the 20Newsgroup benchmark collections, we show that the traditional multinomial Naïve Bayes classifier can be significantly improved by Χ2-statistic based feature weighting.",https://ieeexplore.ieee.org/document/4370315/,2007 International Conference on Machine Learning and Cybernetics,19-22 Aug. 2007,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AERO.1997.577990,Integrating autonomy technologies into an embedded spacecraft system-flight software system engineering for new millennium,IEEE,Conferences,"Deep Space 1 (DS1) is the first deep-space mission of NASA's New Millennium technology validation program. The DS1 flight software will validate five autonomy technologies: 1) Planner/Scheduler, which receives ground or on-board requests for spacecraft activities and schedules them to resolve any resource conflicts or timing constraints; 2) Smart Executive, which expands planned activities into lower-level commands, deduces required hardware configurations or other actions, and provides detection and avoidance of constraint violations; 3) Mode Identification and Reconfiguration engine, which incorporates models of hardware and software behavior, detects discrepancies due to hardware or software failures, and requests recovery actions via the Smart Executive. 4) Autonomous Navigation, which determines the spacecraft trajectory from images of asteroids against the celestial sphere, and autonomously adjusts the trajectory to reach the target asteroid or comet. 5) Beacon Monitoring, which uses radio carrier modification and telemetry summarization to simplify ground monitoring of spacecraft health. Integration of these technologies into the spacecraft flight software architecture has presented a number of system engineering challenges, Some of these technologies were developed in a research-oriented, non-real-time, artificial intelligence organizational culture while spacecraft software is typically developed in a strong real-time, algorithmically-oriented culture. The Navigation technology has been developed in a ground-based environment. Integration of these different cultures and mutual education of the software team has been achieved. An early rapid prototype of an existing spacecraft design proved very valuable in educating the team members and in working out the development process.",https://ieeexplore.ieee.org/document/577990/,1997 IEEE Aerospace Conference,13-13 Feb. 1997,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCMC53470.2022.9753872,Intelligent Application of Virtual Reality Technology in the Design of Elderly Service Station,IEEE,Conferences,"This article is mainly based on virtual reality technology to carry out related researches on the architectural design of the retirement post in the old community, which is mainly divided into three parts. First, the traditional design and shortcomings of the old-age service station building are introduced, and then the virtual reality technology and circuit system are designed to complete the transformation of the old-age facilities service from extensive to refined. Through an in-depth analysis of the functional composition and service aspects of the elderly care service stations in the functional types and corresponding design strategies of the elderly care service stations adapted to different levels of care and individual needs are proposed, and the space design basis for the refined services of the terminal elderly care facilities is improved. 7.3 %.",https://ieeexplore.ieee.org/document/9753872/,2022 6th International Conference on Computing Methodologies and Communication (ICCMC),29-31 March 2022,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SSCI50451.2021.9659994,Intelligent Strategies to Combine Move Heuristics in Selection Hyper-heuristics for Real-World Fibre Network Design Optimisation,IEEE,Conferences,"Increasing competition in today's telecommunication industry drives the need for more cost effective services. In order to reduce the cost of designing a fibre network with low capital expenditure, automation and optimisation of network design has become crucial. British Telecom's network design software, BT NetDesign, has been developed for the purpose of network design and optimisation using a rich set of network/graph-based heuristics and the simulated annealing (SA) search method. Although NetDesign provides several different ways of navigating the search space via different move heuristics, the existing search method (SA) does not consistently reach the near-global optimum as the size of network increases. To deal with larger networks, this study utilises an intelligent approach based on the well-known Luby sequence to combine move heuristics, using two separate learning schemes: frequency based and bigram statistics. These two strategies are rigorously evaluated on network instances of different sizes. Experimental results on real-world case studies indicate that a bigram scheme with a longer warm-up period to learn heuristic combinations can reach high quality solutions for large networks.",https://ieeexplore.ieee.org/document/9659994/,2021 IEEE Symposium Series on Computational Intelligence (SSCI),5-7 Dec. 2021,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IST.2013.6729736,Intelligent Tracking Teaching System based on monocular active vision,IEEE,Conferences,"Teacher detection and tracking is the most important and fundamental functionality in the implementation of the Intelligent Tracking Teaching System (ITTS). In order to track teacher's movement in real-time, face tracking in rostrum region is initiated by the normalized size face adaboost detection followed with the Expectation Maximization (EM) algorithm based on HSV color space and prediction of face position. The split line and position-based visual servo were adopted to realize the tracking strategy, which is trying to keep the teacher in the middle of image by controlling pan/tilt/zoom monocular camera in either rostrum region or classroom region. Furthermore, the student camera will adaptive pan/tilt during the interaction process between teacher and students, and a real-time display on the GUI window is given. The experiment results demonstrated fast and extremely smooth pursuit for teacher's motion despite time-varying variation in illumination, with sustained robustness to the change of pose (e.g., from frontal face to nearly back of head).",https://ieeexplore.ieee.org/document/6729736/,2013 IEEE International Conference on Imaging Systems and Techniques (IST),22-23 Oct. 2013,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ITNEC52019.2021.9587115,Interactive Augmented Reality Application Design based on Mobile Terminal,IEEE,Conferences,"Augmented Reality could realize the integration of virtuality and reality and bring users an enhanced information space feeling. In this context, the design of interactive Augmented Reality application based on mobile terminal is described. Under the integrated development environment of Unity3D, ARFoundation and TensorFlowSharp are adopted to integrate virtual information with markers, and an application with functions of command interaction, plane recognition, image recognition and object recognition are implemented. The design scheme is introduced, and the realization processes are discussed in detail. The test shows that the Augmented Reality application provide rich information and natural way of interacting in three-dimensional virtual-real fusion mode, which improves the interactive experience and brings novelty and immersion to users. The immersive and interactive space improves the interactive experience and strengthens the effect of information transmission, which has certain application significance.",https://ieeexplore.ieee.org/document/9587115/,"2021 IEEE 5th Information Technology,Networking,Electronic and Automation Control Conference (ITNEC)",15-17 Oct. 2021,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SNPD.2018.8441031,Interactive Distance Media Learning Collaborative Based on Virtual Reality with Solar System Subject,IEEE,Conferences,"The emergence of the Internet and various computer devices has revolutionized the learning process in various schools and colleges, both in terms of media and interaction methods. E-learning and distance learning is one of the revolutions of how education can be passed well through cyberspace media that are connected, organized, and integrated with each other. In this research will be developed distance learning media using virtual reality technology, where teachers and students can make communication and made an immersive learning process although they stayed in different places. However, they can have the same face-to-face conversation, sitting in one table, and made virtual meeting with their avatars like meet in the real world in space. As an example learning subject, this project will present solar system learning with interactive virtual reality media.",https://ieeexplore.ieee.org/document/8441031/,"2018 19th IEEE/ACIS International Conference on Software Engineering, Artificial Intelligence, Networking and Parallel/Distributed Computing (SNPD)",27-29 June 2018,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCPS54341.2022.00027,Interpretable Detection of Distribution Shifts in Learning Enabled Cyber-Physical Systems,IEEE,Conferences,"The use of learning based components in cyber-physical systems (CPS) has created a gamut of possible avenues to use high dimensional real world signals generated from sensors like camera and LiDAR. The ability to process such signals can be largely attributed to the adoption of high-capacity function approximators like deep neural networks. However, this does not come without its potential perils. The pitfalls arise from possible over-fitting, and subsequent unsafe behavior when exposed to unknown environments. One challenge is that, in high dimensional input spaces it is almost impossible to experience enough training data in the design phase. What is required here, is an efficient way to flag out-of-distribution (OOD) samples that is precise enough to not raise too many false alarms. In addition, the system needs to be able to detect these in a computationally efficient manner at runtime. In this paper, our proposal is to build good representations for in-distribution data. We introduce the idea of a memory bank to store prototypical samples from the input space. We use these memories to compute probability density estimates using kernel density estimation techniques. We evaluate our technique on two challenging scenarios : a self-driving car setting implemented inside the simulator CARLA with image inputs, and an autonomous racing car navigation setting, with LiDAR inputs. In both settings, it was observed that a deviation from indistribution setting can potentially lead to deviation from safe behavior. An added benefit of using training samples as memories to detect out-of-distribution inputs is that the system is interpretable to a human operator. Explanation of this nature is generally hard to obtain from pure deep learning based alter-natives. Our code for reproducing the experiments is available at https://github.com/yangy96/interpretable_ood_detection.git",https://ieeexplore.ieee.org/document/9797620/,2022 ACM/IEEE 13th International Conference on Cyber-Physical Systems (ICCPS),4-6 May 2022,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/STFSSD.2009.29,Intra Media Synchronization for Actual Feeling Video Model,IEEE,Conferences,"The video clips only have simple information that these are just recorded data. There is a need to make the video clips actual sensation and look real to satisfy users for resolve the needs. We suggest new video model, called AFVM (Actual Feeling Video Model), that is based on ontology concept. The purpose of this study is to provide AFVM and new format of space where containing divided videos for actual image expression.",https://ieeexplore.ieee.org/document/4804590/,2009 Software Technologies for Future Dependable Distributed Systems,17-17 March 2009,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SP40000.2020.00073,Intriguing Properties of Adversarial ML Attacks in the Problem Space,IEEE,Conferences,"Recent research efforts on adversarial ML have investigated problem-space attacks, focusing on the generation of real evasive objects in domains where, unlike images, there is no clear inverse mapping to the feature space (e.g., software). However, the design, comparison, and real-world implications of problem-space attacks remain underexplored.This paper makes two major contributions. First, we propose a novel formalization for adversarial ML evasion attacks in the problem-space, which includes the definition of a comprehensive set of constraints on available transformations, preserved semantics, robustness to preprocessing, and plausibility. We shed light on the relationship between feature space and problem space, and we introduce the concept of side-effect features as the byproduct of the inverse feature-mapping problem. This enables us to define and prove necessary and sufficient conditions for the existence of problem-space attacks. We further demonstrate the expressive power of our formalization by using it to describe several attacks from related literature across different domains.Second, building on our formalization, we propose a novel problem-space attack on Android malware that overcomes past limitations. Experiments on a dataset with 170K Android apps from 2017 and 2018 show the practical feasibility of evading a state-of-the-art malware classifier along with its hardened version. Our results demonstrate that ""adversarial-malware as a service"" is a realistic threat, as we automatically generate thousands of realistic and inconspicuous adversarial applications at scale, where on average it takes only a few minutes to generate an adversarial app. Yet, out of the 1600+ papers on adversarial ML published in the past six years, roughly 40 focus on malware [15]-and many remain only in the feature space.Our formalization of problem-space attacks paves the way to more principled research in this domain. We responsibly release the code and dataset of our novel attack to other researchers, to encourage future work on defenses in the problem space.",https://ieeexplore.ieee.org/document/9152781/,2020 IEEE Symposium on Security and Privacy (SP),18-21 May 2020,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CEC.2000.870315,Introducing performance landscapes and a generic framework for evolutionary search algorithms,IEEE,Conferences,"Introduces a generic framework for evolutionary search algorithms (ESAs) and shows how we can use it to imagine the space of all possible ESAs. The idea of the performance landscape for a given search problem is then introduced in relation to this space of an possible ESAs. The author has previously (1999) argued that search problems can be usefully classified by the most effective ESA on that problem, which translates into the highest peak of the performance landscape. The reason for introducing performance landscapes is to get a better feel for the implications of the ""no-free lunch"" theorem, together with the notion of real-world problems. These are discussed in relation to performance landscapes. Finally, using the generic framework, a simple generic ESA is described and then used to glean a view of the performance landscape of some NK fitness landscapes (S. Kauffman, 1993), with N=20 and K=2, 5 or 10, and a small real-world problem. With these results, a simple classification of each search problem is made.",https://ieeexplore.ieee.org/document/870315/,Proceedings of the 2000 Congress on Evolutionary Computation. CEC00 (Cat. No.00TH8512),16-19 July 2000,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCAIRO.2018.00041,Iterative Algorithm for Solving a Split Common Null Point Problem for Demicontractive Operators,IEEE,Conferences,"In this paper, first we introduce an iterative algorithm which does not require prior knowledge of operator norm and prove strong convergence theorem for approximating a solution of split common null point problem of demicontractive mappings in a real Hilbert space. Widely known the computation of algorithms involving the operator norm for solving split common null point problem may be difficult and for this reason, authors have recently started constructing iterative algorithms with a way of selecting the step-sizes such that the implementation of the algorithm does not require the calculation or estimation of the operator norm. We introduce a new algorithm for solving the split common null point problem for demicontractive mappings with a way of selecting the step-sizes such that the implementation of the algorithm does not require the calculation or estimation of the operator norm and then prove strong convergence of the sequence in real Hilbert spaces. Finally, we give some numerical examples to illustrate our main result.",https://ieeexplore.ieee.org/document/8698385/,"2018 International Conference on Control, Artificial Intelligence, Robotics & Optimization (ICCAIRO)",19-21 May 2018,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SAHCN.2015.7338346,JICE: Joint data compression and encryption for wireless energy auditing networks,IEEE,Conferences,"Fine-grained real-time metering is a fundamental service of wireless energy auditing networks, where metering data is transmitted from embedded power meters to gateways for centralized processing, storage, and forwarding. Due to limited meter capability and wireless bandwidth, the increasing sampling rates and network scales needed to support new energy auditing applications pose significant challenges to metering data fidelity and secrecy. This paper exploits the compression and encryption properties of compressive sensing (CS) to design a joint data compression and encryption (JICE) approach that addresses these two challenges simultaneously. Compared with a conventional signal processing pipeline that compresses and encrypts data sequentially, JICE reduces computation and storage complexities due to its simple design. It thus leaves more processor time and available buffer space for handling lossy wireless transmissions. Moreover, JICE features a machine-learning-based reconfiguration mechanism that adapts its signal representation basis to changing power patterns autonomously. On a smart plug platform, we implemented JICE and several baseline approaches including downsampling, lossless compression, and the pipeline approach. Extensive testbed experiments show that JICE achieves higher data delivery ratios and lower recovery distortions under a range of realistic settings. In particular, JICE increases the number of meters supported by a gateway by 50%, compared with the pipeline approach, while keeping a distortion rate lower than 5%.",https://ieeexplore.ieee.org/document/7338346/,"2015 12th Annual IEEE International Conference on Sensing, Communication, and Networking (SECON)",22-25 June 2015,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS.2018.8594049,Joint 3D Proposal Generation and Object Detection from View Aggregation,IEEE,Conferences,"We present AVOD, an Aggregate View Object Detection network for autonomous driving scenarios. The proposed neural network architecture uses LIDAR point clouds and RGB images to generate features that are shared by two subnetworks: a region proposal network (RPN) and a second stage detector network. The proposed RPN uses a novel architecture capable of performing multimodal feature fusion on high resolution feature maps to generate reliable 3D object proposals for multiple object classes in road scenes. Using these proposals, the second stage detection network performs accurate oriented 3D bounding box regression and category classification to predict the extents, orientation, and classification of objects in 3D space. Our proposed architecture is shown to produce state of the art results on the KITTI 3D object detection benchmark [1] while running in real time with a low memory footprint, making it a suitable candidate for deployment on autonomous vehicles. Code is available at: https://github.com/kujason/avod.",https://ieeexplore.ieee.org/document/8594049/,2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),1-5 Oct. 2018,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICMLA.2018.00037,Joint Adversarial Domain Adaptation for Resilient WiFi-Enabled Device-Free Gesture Recognition,IEEE,Conferences,"Human gesture recognition plays a critical role in numerous applications of human-computer interaction. By analyzing how gesture alters the WiFi propagation among WiFi-enabled IoT devices to identify the gestures in a device-free manner could be a promising solution. However, existing methods require tedious data collection and labeling process each time being implemented in a new environment. The classifier constructed by SVM or random forest is vulnerable to spatial dynamics. In this paper, we proposed JADA, a novel unsupervised Joint adversarial domain adaptation (JADA) scheme that realizes accurate and resilient WiFi-enabled device-free gesture recognition without collecting and labeling training data in new environments. After constructing a source encoder and a source classifier in the source domain by convolutional neural network, JADA trains a target encoder and also fine-tunes the source encoder through adversarial learning to map both unlabeled target data and labeled source data to a domain-invariant feature space such that a domain discriminator cannot distinguish the domain labels of the data. After training a shared classifier with the labeled source data while fixing the parameters of the source encoder, we employ the trained target encoder to embed the test target samples into the domain-invariant feature space and infer its class using the shared classifier. We develop a novel Channel State Information (CSI) enabled IoT platform that could obtain fine-grained CSI time series data directly from IoT devices and transform them into CSI frames. Real-world experiments with COTS WiFi routers were conducted in 2 indoor environments. The experimental results demonstrate that JADA achieves 98.75% gesture recognition accuracy in the original environment. Moreover, when the environmental scenario is altered, it is able to reduce the domain discrepancy across domains without collecting any labeled data in the new context.",https://ieeexplore.ieee.org/document/8614062/,2018 17th IEEE International Conference on Machine Learning and Applications (ICMLA),17-20 Dec. 2018,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IVCNZ.2009.5378371,Kernel PCA of HOG features for posture detection,IEEE,Conferences,"Motivated by the non-linear manifold learning ability of the kernel principal component analysis (KPCA), we propose in this paper a method for detecting human postures from single images by employing KPCA to learn the manifold span of a set of HOG features that can effectively represent the postures. The main contribution of this paper is to apply the KPCA as a non-linear learning and open-set classification tool, which implicitly learns a smooth manifold from noisy data that scatter over the feature space. For a new instance of HOG feature, its distance to the manifold that is measured by its reconstruction error when mapping into the kernel space serves as a criterion for detection. And by combining with a newly developed KPCA approximation technique, the detector can achieve almost real-time speed with neglectable loss of performance. Experimental results have shown that the proposed method can achieve promising detection rate with relatively small size of positive training dataset.",https://ieeexplore.ieee.org/document/5378371/,2009 24th International Conference Image and Vision Computing New Zealand,23-25 Nov. 2009,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICIIECS.2015.7193182,Kernel centric machine learning classifiers for anomaly detection with real bank datasets,IEEE,Conferences,"The machine learning is more effective today in anomaly detection to improve the classification accuracy. The use of powerful kernel based learning is very practical in current trends may expose accurate results in real time database applications. In this context, we need to use the new and adorned machine learning classifiers. In this paper we have given very successful and emerged kernels SVM (Support Vector Machines) which uses the marginal hyperplane uniquely determine the classes by mapping of data and KPCA (Kernel Principal Component Analysis) is an extension to PCA. Both used to classify the data and detecting anomalies by transforming input space into high dimensional feature space. The SVM kernel is use non-linear mapping function and inner product replace with kernel ingredients. KPCA extract principal components from set of corresponding eigenvectors and used as threshold with reference to kernel width. The SVM and KPCA are implemented by taking one real-time bank dataset and other from UCI machine learning repository sets. Finally performance compared with non-kernel techniques (CART, k-NN, PLSDA, PCA) applied on same datasets using training and test set combinations.",https://ieeexplore.ieee.org/document/7193182/,"2015 International Conference on Innovations in Information, Embedded and Communication Systems (ICIIECS)",19-20 March 2015,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/FMEC49853.2020.9144828,Keynote speech 3: Big data Computing and Machine Learning for Intelligent Transportation and Connected Vehicles,IEEE,Conferences,"We are developing machine learning algorithms and software to fuse real-time feeds from video cameras and traffic sensor data to generate real-time detection, classification, and space-time trajectories of individual vehicles and pedestrians. This information is then transmitted to a cloud-based system and then synthesized to create a real-time city-wide traffic palette. I will discuss our research on: Smart intersections: Space-time trajectories are used to understand and improve the safety and efficiency of the intersection. Using conflict points of the vehicle-pedestrian trajectories, we identify potential collisions, or &#x201C;near-misses,&#x201D; and how they are related to the state of the signal cycle (transition from green to yellow, from yellow to red, etc.) and the presence of other vehicles and pedestrians. &#x2022; Smart system: We are developing efficient signal re-timing for different corridors by time of day and day of the week to reflect the changes in network demand. We are also developing machine learning techniques for real-time detection of incidents and accidents on arterial networks. &#x2022; Smart interactions with connected and autonomous vehicles: We have developed signalized intersection control strategies and sensor fusion algorithms for jointly optimizing vehicle trajectories and signal control for a mixture of autonomous vehicles and traditional vehicles at every intersection",https://ieeexplore.ieee.org/document/9144828/,2020 Fifth International Conference on Fog and Mobile Edge Computing (FMEC),20-23 April 2020,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/PerComWorkshops51409.2021.9431114,Keynote: Explainable-by-design Deep Learning,IEEE,Conferences,"Summary form only given, as follows. The complete presentation was not made available for publication as part of the conference proceedings. MACHINE and AI justifiably attract the attention and interest not only of the wider scientific community and industry, but also society and policy makers. However, even the most powerful (in terms of accuracy) algorithms such as deep learning (DL) can give a wrong output, which may be fatal. Due to the opaque and cumbersome model structure used by DL, some authors started to talk about a dystopian &#x201C;black box&#x201D; society. Despite the success in this area, the way computers learn is still principally different from the way people acquire new knowledge, recognise objects and make decisions. People do not need a huge amount of annotated data. They learn by example, using similarities to previously acquired prototypes, not by using parametric analytical models. Current ML approaches are focused primarily on accuracy and overlook explainability, the semantic meaning of the internal model representation, reasoning and its link with the problem domain. They also overlook the efforts to collect and label training data and rely on assumptions about the data distribution that are often not satisfied. The ability to detect the unseen and unexpected and start learning this new class/es in real time with no or very little supervision is critically important and is something that no currently existing classifier can offer. The challenge is to fill this gap between high level of accuracy and the semantically meaningful solutions. The most efficient algorithms that have fuelled interest towards ML and AI recently are also computationally very hungry - they require specific hardware accelerators such as GPU, huge amounts of labeled data and time. They produce parametrised models with hundreds of millions of coefficients, which are also impossible to interpret or be manipulated by a human. Once trained, such models are inflexible to new knowledge. They cannot dynamically evolve their internal structure to start recognising new classes. They are good only for what they were originally trained for. They also lack robustness, formal guarantees about their behaviour and explanatory and normative transparency. This makes problematic use of such algorithms in high stake complex problems such as aviation, health, bailing from jail, etc. where the clear rationale for a particular decision is very important and the errors are very costly. All these challenges and identified gaps require a dramatic paradigm shift and a radical new approach. In this talk the speaker will present such a new approach towards the next generation of computationally lean ML and AI algorithms that can learn in real-time using normal CPUs on computers, laptops, smartphones or even be implemented on chip that will change dramatically the way these new technologies are being applied. It is explainable-by-design. It focuses on addressing the open research challenge of developing highly efficient, accurate ML algorithms and AI models that are transparent, interpretable, explainable and fair by design. Such systems are able to self-learn lifelong, and continuously improve without the need for complete retraining, can start learning from few training data samples, explore the data space, detect and learn from unseen data patterns, collaborate with humans or other such algorithms seamlessly.",https://ieeexplore.ieee.org/document/9431114/,2021 IEEE International Conference on Pervasive Computing and Communications Workshops and other Affiliated Events (PerCom Workshops),22-26 March 2021,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AIMSEC.2011.6009874,Kinematics simulation of upper limb rehabilitant robot based on virtual reality techniques,IEEE,Conferences,"The wearable exoskeletal robot for upper extremity rehabilitation is taken as the research object. According to D-H method, an accurate three-dimensional mechanism model for the robot system is established by SolidWorks software. The virtual set was generated in Simulink/VRML to carry out dynamic simulation. The variable parameters were set based on robotic practical joint range movement. The simulation of all joints and terminal trajectory and space motion area provided theoretical basis for position control, remote control and trajectory planning, realizing the rehabilitation robot visualizations and system interaction.",https://ieeexplore.ieee.org/document/6009874/,"2011 2nd International Conference on Artificial Intelligence, Management Science and Electronic Commerce (AIMSEC)",8-10 Aug. 2011,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA46639.2022.9812352,KinoJGM: A framework for efficient and accurate quadrotor trajectory generation and tracking in dynamic environments,IEEE,Conferences,"Unmapped areas and aerodynamic disturbances render autonomous navigation with quadrotors extremely challenging. To fly safely and efficiently, trajectory planners and trackers must be able to navigate unknown environments with unpredictable aerodynamic effects in real-time. When encountering aerodynamic effects such as strong winds, most current approaches to quadrotor trajectory planning and tracking will not attempt to deviate from a determined plan, even if it is risky, in the hope that any aerodynamic disturbances can be resisted by a robust controller. This paper presents a novel systematic trajectory planning and tracking framework for autonomous quadrotors. We propose a Kinodynamic Jump Space Search (Kino-JSS) to generate a safe and efficient route in unknown environments with aerodynamic disturbances. A real-time Gaussian Process is employed to model the errors caused by aerodynamic disturbances, which we then integrate with a Model Predictive Controller to achieve efficient and accurate trajectory optimization and tracking. We demonstrate our system to improve the efficiency of trajectory generation in unknown environments by up to 75&#x0025; in the cases tested, compared with recent state-of-the-art. We also show that our system improves the accuracy of tracking in selected environments with unpredictable aerodynamic effects. Our implementation is available in an open source package<sup>1</sup><sup>1</sup>https://github.com/Alex-yanranwang/Imperial-KinoJGM.",https://ieeexplore.ieee.org/document/9812352/,2022 International Conference on Robotics and Automation (ICRA),23-27 May 2022,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CVPR.2019.00655,L3-Net: Towards Learning Based LiDAR Localization for Autonomous Driving,IEEE,Conferences,"We present L3-Net - a novel learning-based LiDAR localization system that achieves centimeter-level localization accuracy, comparable to prior state-of-the-art systems with hand-crafted pipelines. Rather than relying on these hand-crafted modules, we innovatively implement the use of various deep neural network structures to establish a learning-based approach. L3-Net learns local descriptors specifically optimized for matching in different real-world driving scenarios. 3D convolutions over a cost volume built in the solution space significantly boosts the localization accuracy. RNNs are demonstrated to be effective in modeling the vehicle's dynamics, yielding better temporal smoothness and accuracy. We comprehensively validate the effectiveness of our approach using freshly collected datasets. Multiple trials of repetitive data collection over the same road and areas make our dataset ideal for testing localization systems. The SunnyvaleBigLoop sequences, with a year's time interval between the collected mapping and testing data, made it quite challenging, but the low localization error of our method in these datasets demonstrates its maturity for real industrial implementation.",https://ieeexplore.ieee.org/document/8954371/,2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),15-20 June 2019,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IJCNN.2010.5596809,Label propagation through neuronal synchrony,IEEE,Conferences,"Semi-Supervised Learning (SSL) is a machine learning research area aiming the development of techniques which are able to take advantage from both labeled and unlabeled samples. Additionally, most of the times where SSL techniques can be deployed, only a small portion of samples in the data set is labeled. To deal with such situations in a straightforward fashion, in this paper we introduce a semi-supervised learning approach based on neuronal synchrony in a network of coupled integrate-and-fire neurons. For that, we represent the input data set as a graph and model each of its nodes by an integrate-and-fire neuron. Thereafter, we propagate the class labels from the seed samples to unlabeled samples through the graph by means of the emerging synchronization dynamics. Experimentations on synthetic and real data show that the introduced technique achieves good classification results regardless the feature space distribution or geometrical shape.",https://ieeexplore.ieee.org/document/5596809/,The 2010 International Joint Conference on Neural Networks (IJCNN),18-23 July 2010,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/INFOCOM48880.2022.9796737,Landing Reinforcement Learning onto Smart Scanning of The Internet of Things,IEEE,Conferences,"Cyber search engines, such as Shodan and Censys, have gained popularity due to their strong capability of indexing the Internet of Things (IoT). They actively scan and fingerprint IoT devices for unearthing IP-device mapping. Because of the large address space of the Internet and the mapping&#x2019;s mutative nature, efficiently tracking the evolution of IP-device mapping with a limited budget of scans is essential for building timely cyber search engines. An intuitive solution is to use reinforcement learning to schedule more scans to networks with high churn rates of IP-device mapping. However, such an intuitive solution has never been systematically studied. In this paper, we take the first step toward demystifying this problem based on our experiences in maintaining a global IoT scanning platform. Inspired by the measurement study of large-scale real-world IoT scan records, we land reinforcement learning onto a system capable of smartly scanning IoT devices in a principled way. We disclose key parameters affecting the effectiveness of different scanning strategies, and find that our system would achieve growing advantages with the proliferation of IoT devices.",https://ieeexplore.ieee.org/document/9796737/,IEEE INFOCOM 2022 - IEEE Conference on Computer Communications,2-5 May 2022,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CVPR.2008.4587406,Large margin pursuit for a Conic Section classifier,IEEE,Conferences,"Learning a discriminant becomes substantially more difficult when the datasets are high-dimensional and the available samples are few. This is often the case in computer vision and medical diagnosis applications. A novel conic section classifier (CSC) was recently introduced in the literature to handle such datasets, wherein each class was represented by a conic section parameterized by its focus, directrix and eccentricity. The discriminant boundary was the locus of all points that are equi-eccentric relative to each class-representative conic section. Simpler boundaries were preferred for the sake of generalizability. In this paper, we improve the performance of the two-class classifier via a large margin pursuit. When formulated as a non-linear optimization problem, the margin computation is demonstrated to be hard, especially due to the high dimensionality of the data. Instead, we present a geometric algorithm to compute the distance of a point to the non-linear discriminant boundary generated by the CSC in the input space. We then introduce a large margin pursuit in the learning phase so as to enhance the generalization capacity of the classifier. We validate the algorithm on real datasets and show favorable classification rates in comparison to many existing state-of-the-art binary classifiers as well as the CSC without margin pursuit.",https://ieeexplore.ieee.org/document/4587406/,2008 IEEE Conference on Computer Vision and Pattern Recognition,23-28 June 2008,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/FOAN.2017.8215259,Large-scale location-aware services in access: Hierarchical building/floor classification and location estimation using Wi-Fi fingerprinting based on deep neural networks,IEEE,Conferences,"One of key technologies for future large-scale location-aware services in access is a scalable indoor localization technique. In this paper, we report preliminary results from our investigation on the use of deep neural networks (DNNs) for hierarchical building/floor classification and floor-level location estimation based on Wi-Fi fingerprinting, which we carried out as part of a feasibility study project on Xi'an Jiaotong-Liverpool University (XJTLU) Campus Information and Visitor Service System. To take into account the hierarchical nature of the building/floor classification problem, we propose a new DNN architecture based on a stacked autoencoder for the reduction of feature space dimension and a feed-forward classifier for multi-label classification with argmax functions to convert multi-label classification results into multi-class classification ones. We also describe the demonstration of a prototype DNN-based indoor localization system for floor-level location estimation using real received signal strength (RSS) data collected at one of the buildings on the XJTLU campus. The preliminary results for both building/floor classification and floor-level location estimation clearly show the strengths of DNN-based approaches, which can provide near state-of-the-art performance with less parameter tuning and higher scalability.",https://ieeexplore.ieee.org/document/8215259/,2017 International Workshop on Fiber Optics in Access Network (FOAN),6-8 Nov. 2017,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS45743.2020.9341460,Latent Replay for Real-Time Continual Learning,IEEE,Conferences,"Training deep neural networks at the edge on light computational devices, embedded systems and robotic platforms is nowadays very challenging. Continual learning techniques, where complex models are incrementally trained on small batches of new data, can make the learning problem tractable even for CPU-only embedded devices enabling remarkable levels of adaptiveness and autonomy. However, a number of practical problems need to be solved: catastrophic forgetting before anything else. In this paper we introduce an original technique named ""Latent Replay"" where, instead of storing a portion of past data in the input space, we store activations volumes at some intermediate layer. This can significantly reduce the computation and storage required by native rehearsal. To keep the representation stable and the stored activations valid we propose to slow-down learning at all the layers below the latent replay one, leaving the layers above free to learn at full pace. In our experiments we show that Latent Replay, combined with existing continual learning techniques, achieves state-of-the-art performance on complex video benchmarks such as CORe50 NICv2 (with nearly 400 small and highly non-i.i.d. batches) and OpenLORIS. Finally, we demonstrate the feasibility of nearly real-time continual learning on the edge through the deployment of the proposed technique on a smartphone device.",https://ieeexplore.ieee.org/document/9341460/,2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),24 Oct.-24 Jan. 2021,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/WACV51458.2022.00373,Latent to Latent: A Learned Mapper for Identity Preserving Editing of Multiple Face Attributes in StyleGAN-generated Images,IEEE,Conferences,"Several recent papers introduced techniques to adjust the attributes of human faces generated by unconditional GANs such as StyleGAN. Despite efforts to disentangle the attributes, a request to change one attribute often triggers unwanted changes to other attributes as well. More importantly, in some cases, a human observer would not recognize the edited face to belong to the same person. We propose an approach where a neural network takes as input the latent encoding of a face and the desired attribute changes and outputs the latent space encoding of the edited image. The network is trained offline using unsupervised data, with training labels generated by an off-the-shelf attribute classifier. The desired attribute changes and conservation laws, such as identity maintenance, are encoded in the training loss. The number of attributes the mapper can simultaneously modify is only limited by the attributes available to the classifier &#x2013; we trained a network that handles 35 attributes, more than any previous approach. As no optimization is performed at deployment time, the computation time is negligible, allowing real-time attribute editing. Qualitative and quantitative comparisons with the current state-of-the-art show our method is better at conserving the identity of the face and restricting changes to the requested attributes.",https://ieeexplore.ieee.org/document/9706683/,2022 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV),3-8 Jan. 2022,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CoDIT55151.2022.9804101,Lateral Control of a Vehicle using Reinforcement Learning,IEEE,Conferences,"In the current state of the art, autonomous driving is implemented by controlling the longitudinal and lateral dynamics of a vehicle based on a predefined or online calculated reference trajectory. In this context, the longitudinal and lateral controllers of the vehicle are mostly independent of each other and are based on standard controller approaches such as nonlinear adaptive controllers, cascaded control loops or LQ controllers. The application of these controllers on the real vehicle is a laborious and imprecise process due to the simplification of the vehicle models used. In this paper, lateral control of a vehicle is implemented using reinforcement learning. Here, the longitudinal motion of the vehicle is given by a trajectory-dependent velocity profile. The Soft Actor Critic is an off-policy reinforcement learning algorithm which we used in this paper. It features a continuous action space of the agent, which is the case with a steering angle of a vehicle. In the training process, the agent learns to control the lateral dynamics of the vehicle in a performant manner according to the reward function of the environment which is an easier process for the development and calibration of the controllers. We show that the reinforcement learning agent achieves a slightly improved performance with respect to the relevant error variables for the lateral control compared to a standard controller approach. These error variables are the lateral distance to the reference trajectory and the yaw angle error to the reference trajectory.",https://ieeexplore.ieee.org/document/9804101/,"2022 8th International Conference on Control, Decision and Information Technologies (CoDIT)",17-20 May 2022,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.23919/ECC.2001.7076545,Lateral auto-pilot design for an agile missile using dynamic fuzzy neural networks,IEEE,Conferences,"This paper presents a new approach, which exploits the recently developed Dynamic Fuzzy Neural Networks (DFNN) learning algorithm. The DFNN is based on extended Radial Basis Function (RBF) neural networks, which are functionally equivalent to Takagi-Sugeno-Kang (TSK) fuzzy systems. The algorithm comprises 4 parts: (1) Criteria of rules generation; (2) Allocation of premise parameters; (3) Determination of consequent parameters and (4) Pruning technology. The salient characteristics of the approach are: (1) A hierarchical on-line self-organizing learning paradigm is employed so that not only parameters can be adjusted, but also the determination of structure can be self-adaptive without partitioning the input space a priori; (2) Fast learning speed can be achieved so that the system can be implemented in real time. The application of the proposed approach is demonstrated in application to a demanding, highly nonlinear, missile control design task. Scheduling on instantaneous incidence (a rapidly varying quantity) is well known to lead to considerable difficulties with classical gain-scheduling methods. It is shown that the methods proposed here can, however, be used to successfully design an effective intelligent controller.",https://ieeexplore.ieee.org/document/7076545/,2001 European Control Conference (ECC),4-7 Sept. 2001,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/FIE49875.2021.9637402,Learning Autonomous Driving in Tangible Practice: Development and On-Road Applications of a 1/10-Scale Autonomous Vehicle,IEEE,Conferences,"This Innovative Practice Work-In-Progress Paper presents a case of learning autonomous driving in tangible practice. As technology sustainably enhances the quality of life, intelligent systems continue to contribute solutions to some of the biggest challenges faced by humans. Autonomous vehicles offer humans the opportunity to increase transportation safety by reducing human errors on the road, preventing accidents, improving human productivity by reducing commuting time, and possibly mitigating air pollution. There is a critical shortage of educational and training programs in autonomous vehicles due to the high cost of full-size vehicles, computing and sensor equipment, and big lab space needed. To address this problem, we develop a 1/10-scale autonomous vehicle powered by pre-collision detection, lane tracking, and road sign recognition systems. The pre-collision system is built using ultrasonic sensors, and the Proportional-Integral-Derivative (PID) control is implemented to manipulate the vehicle's safety response. The Open-Source Computer Vision Library (OpenCV) is exploited to detect and process real-time on-road streaming video to enable lane-tracking and road sign recognition. AI techniques are utilized for the model training. Preliminary results of this work are presented and analyzed. We also discuss the future directions of this study.",https://ieeexplore.ieee.org/document/9637402/,2021 IEEE Frontiers in Education Conference (FIE),13-16 Oct. 2021,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS51168.2021.9636547,Learning Contact-Rich Assembly Skills Using Residual Admittance Policy,IEEE,Conferences,"Contact-rich assembly tasks may result in large and unpredictable forces and torques when the locations of the contacting parts are uncertain. The ability to correct the trajectory in response to haptic feedback and accomplish the task despite location uncertainties is an important skill. We hypothesize that this skill would facilitate generalization and support direct transfer from simulations to real world. To reduce sample complexity, we propose to learn a residual admittance policy (RAP). RAP is learned to correct the movements generated by a baseline policy in the framework of dynamic movement primitives. Given the reference trajectories generated by the baseline policy, the action space of RAP is limited to the admittance parameters. Using deep reinforcement learning, a deep neural network is trained to map task specifications to proper admittance parameters. We demonstrate that RAP handles uncertainties in board location, generalizes well over space, size and shape, and facilitates quick transfer learning. Most impressively, we demonstrate that the policy learned in simulations achieves similar robustness to uncertainties, generalization and performance when deployed on an industrial robot (UR5e) without further training. See accompanying video for demonstrations.",https://ieeexplore.ieee.org/document/9636547/,2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),27 Sept.-1 Oct. 2021,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA46639.2022.9812011,Learning Crowd-Aware Robot Navigation from Challenging Environments via Distributed Deep Reinforcement Learning,IEEE,Conferences,"This paper presents a deep reinforcement learning (DRL) sframework for safe and efficient navigation in crowded environments. Here, the robot learns cooperative behavior using a new reward function that penalizes robot actions interfering with the pedestrian&#x0027;s movement. Also, we propose a simulated pedestrian policy reflecting data from actual pedestrian movements. Furthermore, we introduce a collision detection that considers the pedestrian&#x0027;s personal space to generate affinity robot behavior. To efficiently explore this simulation environment, we propose distributed learning using Ape-X [1]. We deployed the robot in a real environment and verified its crowd-aware navigation performance compared with an actual human in terms of path length, travel time, and the number of abrupt avoidances.",https://ieeexplore.ieee.org/document/9812011/,2022 International Conference on Robotics and Automation (ICRA),23-27 May 2022,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICPR48806.2021.9413196,Learning Defects in Old Movies from Manually Assisted Restoration,IEEE,Conferences,"We propose to detect defects in old movies, as the first step of a larger framework of old movies restoration by inpainting techniques. The specificity of our work is to learn a film restorer's expertise from a pair of sequences, composed of a movie with defects, and the same movie which was semiautomatically restored with the help of a specialized software. In order to detect those defects with minimal human interaction and further reduce the time spent for a restoration, we feed a U-Net with consecutive defective frames as input to detect the unexpected variations of pixel intensity over space and time. Since the output of the network is a mask of defect location, we first have to create the dataset of mask frames on the basis of restored frames from the software used by the film restorer, instead of classical synthetic ground truth, which is not available. These masks are estimated by computing the absolute difference between restored frames and defectuous frames, combined with thresholding and morphological closing. Our network succeeds in automatically detecting real defects with more precision than the manual selection with an all-encompassing shape, including some the expert restorer could have missed for lack of time.",https://ieeexplore.ieee.org/document/9413196/,2020 25th International Conference on Pattern Recognition (ICPR),10-15 Jan. 2021,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA46639.2022.9811624,Learning Design and Construction with Varying-Sized Materials via Prioritized Memory Resets,IEEE,Conferences,"Can a robot autonomously learn to design and construct a bridge from varying-sized blocks without a blueprint? It is a challenging task with long horizon and sparse reward - the robot has to figure out physically stable design schemes and feasible actions to manipulate and transport blocks. Due to diverse block sizes, the state space and action trajectories are vast to explore. In this paper, we propose a hierarchical approach for this problem. It consists of a reinforcement-learning designer to propose high-level building instructions and a motion-planning-based action generator to manipulate blocks at the low level. For high-level learning, we develop a novel technique, prioritized memory resetting (PMR) to improve exploration. PMR adaptively resets the state to those most critical configurations from a replay buffer so that the robot can resume training on partial architectures instead of from scratch. Furthermore, we augment PMR with auxiliary training objectives and fine-tune the designer with the locomotion generator. Our experiments in simulation and on a real deployed robotic system demonstrate that it is able to effectively construct bridges with blocks of varying sizes at a high success rate. Demos can be found at https://sites.google.com/view/bridge-pmr.",https://ieeexplore.ieee.org/document/9811624/,2022 International Conference on Robotics and Automation (ICRA),23-27 May 2022,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.23919/CNSM46954.2019.9012710,Learning From Evolving Network Data for Dependable Botnet Detection,IEEE,Conferences,"This work presents an emerging problem in real-world applications of machine learning (ML) in cybersecurity, particularly in botnet detection, where the dynamics and the evolution in the deployment environments may render the ML solutions inadequate. We propose an approach to tackle this challenge using Genetic Programming (GP) - an evolutionary computation based approach. Preliminary results show that GP is able to evolve pre-trained classifiers to work under evolved (expanded) feature space conditions. This indicates the potential use of such an approach for botnet detection under non-stationary environments, where much less data and training time are required to obtain a reliable classifier as new network conditions arise.",https://ieeexplore.ieee.org/document/9012710/,2019 15th International Conference on Network and Service Management (CNSM),21-25 Oct. 2019,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IJCNN.1993.716991,Learning goal-directed navigation as attractor dynamics for a sensory motor system. (An experiment by the mobile robot YAMABICO),IEEE,Conferences,"This paper describes experimental results based on the authors' prior-proposed scheme: learning of sensory-based, goal-directed behavior. The scheme was implemented on the mobile robot ""YAMABICO"" and learning of a set of goal-directed navigations were conducted. The experiment assumed that the robot receives no global information such as position nor prior environment model. Instead, the robot was trained to learn adequate maneuvering in the adopted workspace by building a correct mapping between a spatio-temporal sequence of sensory inputs and maneuvering outputs on a neural structure. The experimental results showed that sufficient training generated rigid dynamical structure of a fixed point and limit cycling in the sensory-based state space, which realized robust navigations of homing and cyclic routing even against certain changes of environment as well as miscellaneous noises in the real world.",https://ieeexplore.ieee.org/document/716991/,"Proceedings of 1993 International Conference on Neural Networks (IJCNN-93-Nagoya, Japan)",25-29 Oct. 1993,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CEC.2017.7969562,Learning of a tracker model from multi-radar data for performance prediction of air surveillance system,IEEE,Conferences,"A valid model of the air surveillance system performance is highly valued when making decisions related to the optimal control of the system. We formulate a model for a multi-radar tracker system by combining a radar performance model with a tracker performance model. A tracker as a complex software system is hard to model mathematically and physically. Our novel approach is to utilize machine learning to create a tracker model based on measurement data from which the input and target output for the model are calculated. The measured data comprises the time series of 3D coordinates of cooperative aircraft flights, the corresponding target detection recordings from multiple radars, and the related multi-radar track recordings. The collected data is used to calculate performance measures for the radars and the tracker at specific locations in the air space. We apply genetic programming to learning such rules from radar performance measures that explain tracker performance. The easily interpretable rules are intended to reveal the real behavior of the system providing comprehension for its control and further development. The learned rules allow predicting tracker performance level for the system control in all radar geometries, modes, and conditions at any location. In the experiments, we show the feasibility of our approach to learning a tracker model and compare our rule learner with two tree classifiers, another rule learner, a neural network, and an instance-based classifier using the real air surveillance data. The tracker model created by our rule learner outperforms the models by the other methods except for the neural network whose prediction performance is equal.",https://ieeexplore.ieee.org/document/7969562/,2017 IEEE Congress on Evolutionary Computation (CEC),5-8 June 2017,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICPR.2016.7899802,Learning opposites using neural networks,IEEE,Conferences,"Many research works have successfully extended algorithms such as evolutionary algorithms, reinforcement agents and neural networks using “opposition-based learning” (OBL). Two types of the “opposites” have been defined in the literature, namely type-I and type-II. The former are linear in nature and applicable to the variable space, hence easy to calculate. On the other hand, type-II opposites capture the “oppositeness” in the output space. In fact, type-I opposites are considered a special case of type-II opposites where inputs and outputs have a linear relationship. However, in many real-world problems, inputs and outputs do in fact exhibit a nonlinear relationship. Therefore, type-II opposites are expected to be better in capturing the sense of “opposition” in terms of the input-output relation. In the absence of any knowledge about the problem at hand, there seems to be no intuitive way to calculate the type-II opposites. In this paper, we introduce an approach to learn type-II opposites from the given inputs and their outputs using the artificial neural networks (ANNs). We first perform opposition mining on the sample data, and then use the mined data to learn the relationship between input x and its opposite x̌. We have validated our algorithm using various benchmark functions to compare it against an evolving fuzzy inference approach that has been recently introduced. The results show the better performance of a neural approach to learn the opposites. This will create new possibilities for integrating oppositional schemes within existing algorithms promising a potential increase in convergence speed and/or accuracy.",https://ieeexplore.ieee.org/document/7899802/,2016 23rd International Conference on Pattern Recognition (ICPR),4-8 Dec. 2016,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICTAI.2019.00220,Learning to Drive via Apprenticeship Learning and Deep Reinforcement Learning,IEEE,Conferences,"With the implementation of reinforcement learning (RL) algorithms, current state-of-art autonomous vehicle technology have the potential to get closer to full automation. However, most of the applications have been limited to game domains or discrete action space which are far from the real world driving. Moreover, it is very tough to tune the parameters of reward mechanism since the driving styles vary a lot among the different users. For instance, an aggressive driver may prefer driving with high acceleration whereas some conservative drivers prefer a safer driving style. Therefore, we propose an apprenticeship learning in combination with deep reinforcement learning approach that allows the agent to learn the driving and stopping behaviors with continuous actions. We use gradient inverse reinforcement learning (GIRL) algorithm to recover the unknown reward function and employ REINFORCE as well as Deep Deterministic Policy Gradient algorithm (DDPG) to learn the optimal policy. The performance of our method is evaluated in simulation-based scenario and the results demonstrate that the agent performs human like driving and even better in some aspects after training.",https://ieeexplore.ieee.org/document/8995417/,2019 IEEE 31st International Conference on Tools with Artificial Intelligence (ICTAI),4-6 Nov. 2019,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CSSE.2008.720,Learning to Rank with Bayesian Evidence Framework,IEEE,Conferences,"The problem of ranking has recently gained attention in data learning. The goal ranking is to learn a real-valued ranking function that induces a ranking or ordering over an instance space. In this paper, we apply popular Bayesian techniques on ranking support vector machine. We propose a novel differentiable loss function called trigonometric loss function with the desirable characteristic of natural normalization in the likelihood function, and then set up a Bayesian framework. In this framework, Bayesian inference is used to implement model adaptation, while keeping the merits of ranking SVM. Experimental results on data sets indicate the usefulness of this approach.",https://ieeexplore.ieee.org/document/4722718/,2008 International Conference on Computer Science and Software Engineering,12-14 Dec. 2008,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.23919/DATE.2019.8714959,Learning to infer: RL-based search for DNN primitive selection on Heterogeneous Embedded Systems,IEEE,Conferences,"Deep Learning is increasingly being adopted by industry for computer vision applications running on embedded devices. While Convolutional Neural Networks' accuracy has achieved a mature and remarkable state, inference latency and throughput are a major concern especially when targeting low-cost and low-power embedded platforms. CNNs' inference latency may become a bottleneck for Deep Learning adoption by industry, as it is a crucial specification for many real-time processes. Furthermore, deployment of CNNs across heterogeneous platforms presents major compatibility issues due to vendor-specific technology and acceleration libraries.In this work, we present QS-DNN, a fully automatic search based on Reinforcement Learning which, combined with an inference engine optimizer, efficiently explores through the design space and empirically finds the optimal combinations of libraries and primitives to speed up the inference of CNNs on heterogeneous embedded devices. We show that, an optimized combination can achieve 45x speedup in inference latency on CPU compared to a dependency-free baseline and 2x on average on GPGPU compared to the best vendor library. Further, we demonstrate that, the quality of results and time ""to-solution"" is much better than with Random Search and achieves up to 15x better results for a short-time search.",https://ieeexplore.ieee.org/document/8714959/,"2019 Design, Automation & Test in Europe Conference & Exhibition (DATE)",25-29 March 2019,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCPS.2018.00048,Learning-Based Control Design for Deep Brain Stimulation,IEEE,Conferences,"By employing low-voltage electrical stimulation of the basal ganglia (BG) regions of the brain, deep brain stimulation (DBS) devices are used to alleviate the symptoms of several neurological disorders, including Parkinson's disease (PD). Recently, we have developed a Basal Ganglia Model (BGM) that can be utilized for design and evaluation of DBS devices. In this work, we focus on the use of a hardware (FPGA) implementation of the BGM platform to facilitate development of new control policies. Specifically, we introduce a design-time framework that allows for development of suitable control policies, in the form of electrical pulses with variable temporal patterns, while supporting tradeoffs between energy efficiency and efficacy (i.e., Quality-of-Control) of the therapy. The developed framework exploits machine learning and optimization based methods for design-space exploration where predictive behavior for any control configuration (i.e., temporal pattern) is obtained using the BGM platform that simulates physiological response to the considered control in real-time. To illustrate the use of the developed framework, in our demonstration we present how the BGM can be utilized for physiologically relevant BG modeling and design-state exploration for DBS controllers, as well as show the effectiveness of obtained controllers that significantly outperform conventional DBS controllers.",https://ieeexplore.ieee.org/document/8443755/,2018 ACM/IEEE 9th International Conference on Cyber-Physical Systems (ICCPS),11-13 April 2018,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/GLOBECOM42002.2020.9322310,Learning-Based Massive Beamforming,IEEE,Conferences,"Developing resource allocation algorithms with strong real-time and high efficiency has been an imperative topic in wireless networks. Conventional optimization-based iterative resource allocation algorithms often suffer from slow convergence, especially for massive multiple-input-multiple-output (MIMO) beamforming problems. This paper studies learning-based efficient massive beamforming methods for multi-user MIMO networks. The considered massive beamforming problem is challenging in two aspects. First, the beamforming matrix to be learned is quite high-dimensional in case with a massive number of antennas. Second, the objective is often time-varying and the solution space is not fixed due to some communication requirements. All these challenges make learning representation for massive beamforming an extremely difficult task. In this paper, by exploiting the structure of the most popular WMMSE beamforming solution, we propose convolutional massive beamforming neural networks (CMBNN) using both supervised and unsupervised learning schemes with particular design of network structure and input/output. Numerical results demonstrate the efficacy of the proposed CMBNN in terms of running time and system throuuhput,",https://ieeexplore.ieee.org/document/9322310/,GLOBECOM 2020 - 2020 IEEE Global Communications Conference,7-11 Dec. 2020,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SYNCHROINFO.2019.8814156,Lightweight Machine Learning Classifiers of IoT Traffic Flows,IEEE,Conferences,"IoT traffic flows have different from traditional devices statistics and their classification become an important task because of the exponentially growing number of smart devices. Conventional Deep Packet Inspection systems that rely on inspection of open fields in TLS and DNS packets, and the trend of encrypting the open fields makes machine learning based systems the only viable option for future networks. Moreover, computational complexity of models becomes crucial for large-scale operations. In this work, we investigated whether simple models, such as Logistic Regression, SVM with linear kernel, and a Decision Tree, have suitable for real-world deployments performance of multiclass classification of IoT traces, given thoughtful features engineering. We introduced a new flow feature of categorical type that describes a set of TCP-flag fields within a flow. In addition, removal of correlated features and feature space transformation via PCA method showed their usefulness in terms of prediction complexity reduction. In order to account for online classification mode, we limited the maximal number of packets within a flow to 10. Moreover, to estimate the upper-bound performance with given features, we compared the simple algorithms with Random Forest, Gradient Boosting and a feed-forward neural network. We performed 4-fold cross-validation of models by metrics Accuracy and F1-measure. The test results demonstrated that the introduced feature increases F1-measure for logistic regression from 99.1% in the base case to 99.6%, thus closely approaching more computationally expensive models. Overall, the evaluation results demonstrated feasibility of a lightweight model for IoT flow classification task with the suitable for a practical deployment performance.",https://ieeexplore.ieee.org/document/8814156/,"2019 Systems of Signal Synchronization, Generating and Processing in Telecommunications (SYNCHROINFO)",1-3 July 2019,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ETFA.2018.8502485,Linear Classification of Badly Conditioned Data,IEEE,Conferences,"We present a method for the fast and robust linear classification of badly conditioned data. In our considerations, badly conditioned data are such data which are numerically difficult to handle. Due to, e.g. a large number of features or a large number of objects representing classes as well as noise, outliers or incompleteness, the common software computation of the discriminating linear combination of features between classes fails or is extremely time consuming. The theoretical foundations of our approach are based on the single feature ranking, which allows fast calculation of the approximative initial classification boundary. For the increasing of classification accuracy of this boundary, the refinement is performed in the lower dimensional space. Our approach is tested on several datasets from UCI Reposi-tiory. Experimental results indicate high classification accuracy of the approach. For the modern real industrial applications such a method is especially suitable in the Cyber-Physical-System environments and provides a part of the workflow for the automated classifier design.",https://ieeexplore.ieee.org/document/8502485/,2018 IEEE 23rd International Conference on Emerging Technologies and Factory Automation (ETFA),4-7 Sept. 2018,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AERO.2016.7500514,Link performance analysis of multi-user detection techniques for W-band multi-beam satellites,IEEE,Conferences,"The existing literature about broadband satellite communications clearly indicates the exploitation of Extremely High Frequency (EHF) bandwidth portions and the massive employment of multi-beam satellites as key enabling technologies for “terabit connectivity” in the Space. In particular, increasing the reuse factor of frequency sub-bands, it is possible to boost multi-beam satellite capacity, provided that co-channel interference is conveniently reduced at the terminal side. To this aim, suitable multi-user detection techniques are required. In this paper, we shall analyze the link performance of basic theoretical multi-user detection techniques, namely: optimum Maximum-Likelihood (ML) and sub-optimum Minimum Mean Squared Error (MMSE) detection, in the framework of a W-band (70-110 GHz) multi-gigabit geostationary satellite system. At our best knowledge, some partial analyses have been proposed in literature, mostly in terms of reachable capacity and, anyway, not considering the multi-gigabit W-band case. The results shown in this paper will be the basis for the practical implementation of multi-user detection techniques for real-world multi-beam terminals that will have to consider stringent constraints in terms of affordable computational complexity.",https://ieeexplore.ieee.org/document/7500514/,2016 IEEE Aerospace Conference,5-12 March 2016,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISCAS51556.2021.9401231,Live Demonstration: Real-Time Multi-Hand Segmentation on Exhibition,IEEE,Conferences,"In this paper, we proposed a multi-hand segmentation on exhibition. In exhibition there are many objects with similar color such as skin clothes and the decoration close to skin color. First we made a lot of virtual image to make the datasets closed to the exhibition, and combined the palm and back of hand into same picture. Secondly we proposed a robustness neural network call ""Unet- Encoder Network (Unet-EN)"" to train this datasets. We use pruning method to reduce the parameter and increase the speed. We implemented on NVIDIA® Jetson™ TX2. As a result, it can be implemented on some skin color space and supported multi-hand segmentation.",https://ieeexplore.ieee.org/document/9401231/,2021 IEEE International Symposium on Circuits and Systems (ISCAS),22-28 May 2021,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AVSS.2018.8639111,Local Computation with Adaptive Spatial Clustering for Multi-Size Motion Patch Proposals in WAMI,IEEE,Conferences,"Near real-time moving object detection in Wide Area Motion Imagery (WAMI) can support applications in many fields. However, since the targets&#x2019; search space are extremely large, current state-of-the-art methods usually suffers high computational cost. It&#x2019;s crucial to recommend candidate regions for detection first. Different from most research that extracts candidate blobs as proposals, this paper attempts to offer high-quality patches, which can provide background info and be taken as many algorithms&#x2019; input (convolutional neural network, optical flow, block matching, etc.). First by the idea that local errors are tolerable, neighborhood frame differencing with local computation is applied to roughly obtain irregular blobs. After that, an adaptive spatial clustering algorithm which utilizes grid and density reachable, is proposed to generate multi-size motion patches quickly. Compared with traditional clustering, advantages of this algorithm include parameter-free, saving of time and widely applications. Experimental results show that the proposed method is competitive especially in dense traffic regions.",https://ieeexplore.ieee.org/document/8639111/,2018 15th IEEE International Conference on Advanced Video and Signal Based Surveillance (AVSS),27-30 Nov. 2018,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/RADAR.2017.7944428,Localized random projections for space-time adaptive processing,IEEE,Conferences,"High-dimensional multi-sensor radar data suffers from the well known curse of dimensionality. For example, in radar space time adaptive processing (STAP), training data from neighboring range cells is limited, since the statistical properties vary significantly over range and azimuth. Therefore, precluding straightforward implementation of standard detectors, for example, the whitening minimum variance distortionless response filter. Using random projections, we can reduce the dimension of the radar problem by random sampling, i.e. by projecting the data into a random d-dimensional subspace. The Johnson-Lindenstrauss (JL) theorem provides theoretical guarantees which explicitly states that the low dimensional data after random projections is only very slightly perturbed when compared to the data from the original problem in an l2 norm sense. Random projections offers significant computational savings permitting possible real time solutions, however, at the cost of reducing the clairvoyant SINR for radar STAP. To alleviate this issue of SINR loss, we use localized random projections where the random projection matrix incorporates the look angle information, thereby minimizing the noise and interference effects from other angles, and increasing the SINR. We show that the resulting detector is CFAR, and the transformation matrix satisfies all the necessary conditions for the the JL theorem to hold.",https://ieeexplore.ieee.org/document/7944428/,2017 IEEE Radar Conference (RadarConf),8-12 May 2017,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IJCNN.2014.6889658,Long-term learning behavior in a recurrent neural network for sound recognition,IEEE,Conferences,"In this paper, the long-term learning properties of an artificial neural network model, designed for sound recognition and computational auditory scene analysis in general, are investigated. The model is designed to run for long periods of time (weeks to months) on low-cost hardware, used in a noise monitoring network, and builds upon previous work by the same authors. It consists of three neural layers, connected to each other by feedforward and feedback excitatory connections. It is shown that the different mechanisms that drive auditory attention emerge naturally from the way in which neural activation and intra-layer inhibitory connections are implemented in the model. Training of the artificial neural network is done following the Hebb principle, dictating that ""Cells that fire together, wire together"", with some important modifications, compared to standard Hebbian learning. As the model is designed to be on-line for extended periods of time, also learning mechanisms need to be adapted to this. The learning needs to be strongly attention- and saliency-driven, in order not to waste available memory space for sounds that are of no interest to the human listener. The model also implements plasticity, in order to deal with new or changing input over time, without catastrophically forgetting what it already learned. On top of that, it is shown that also the implementation of short-term memory plays an important role in the long-term learning properties of the model. The above properties are investigated and demonstrated by training on real urban sound recordings.",https://ieeexplore.ieee.org/document/6889658/,2014 International Joint Conference on Neural Networks (IJCNN),6-11 July 2014,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISQED.2018.8357325,Low cost and power CNN/deep learning solution for automated driving,IEEE,Conferences,"Automated driving functions, like highway driving and parking assist, are increasingly getting deployed in high-end cars with the ultimate goal of realizing self-driving car using Deep learning techniques like convolution neural network (CNN). For mass-market deployment, the embedded solution is required to address the right cost and performance envelope along with security and safety. In the case of automated driving, one of the key functionality is “finding drivable free space”, which is addressed using deep learning techniques like CNN. These CNN networks pose huge computing requirements in terms of hundreds of GOPS/TOPS (Giga or Tera operations per second), which seems beyond the capability of today's embedded SoC. This paper covers various techniques consisting of fixed-point conversion, sparse multiplication, fusing of layers and network pruning, for tailoring on the embedded solution. These techniques are implemented on the device by means of optimized Deep learning library for inference. The paper concludes by demonstrating the results of a CNN network running in real time on TI's TDA2X embedded platform producing a high-quality drivable space output for automated driving.",https://ieeexplore.ieee.org/document/8357325/,2018 19th International Symposium on Quality Electronic Design (ISQED),13-14 March 2018,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICASSP39728.2021.9414107,Low-Complexity Parameter Learning for OTFS Modulation Based Automotive Radar,IEEE,Conferences,"Orthogonal time frequency space (OTFS) as an emerging modulation technique in the 5G and beyond era exploits full time-frequency diversity and is robust against doubly-selective channels in high mobility scenarios. In this work, we consider an OTFS modulation based automotive joint radar-communication system and focus on the design of low-complexity parameter estimation algorithm for radar targets. It is well known that target parameter estimation in OTFS radar is computationally much more expensive than the orthogonal frequency division multiplex based platform, which hampers low-cost and real-time implementation. In this context, an efficient Bayesian learning scheme is proposed for OTFS automotive radars, which leverages the structural sparsity of radar channel in the delay-Doppler domain. We also reduce the dimension of the measurement matrix by incorporating the prior knowledge on the motion parameter limit of the true targets. Numerical simulation results are presented to demonstrate the superior performance of the proposed method in comparison with the state-of-the-art.",https://ieeexplore.ieee.org/document/9414107/,"ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",6-11 June 2021,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/TENCON.2018.8650452,ML-Based Approach to Detect DDoS Attack in V2I Communication Under SDN Architecture,IEEE,Conferences,"The need for Internet-based services is increasing at a tremendous pace in smart cities. The driver and occupants of the vehicle access Internet, and different intelligent transportation system (ITS) related services such as real-time traffic information, parking space availability, downloading the map, etc., in a vehicle to infrastructure communication (V2I) mode. In a highly dynamic network environment like vehicular network, software-defined networking (SDN) promises to be an ideal solution. However, it also opens doors for various distributed denial of service (DDoS) attacks. An attacker can easily flood short-lived spoofed flows and exhaust network resources. This motivates us to find a solution to detect the attacks in a V2I communication under SDN. In this paper, we propose a machine learning (ML) based DDoS attack detection. The proposed system uses various ML schemes, and few of them found to be accurate with a high detection rate and a relatively low false alarm rate.",https://ieeexplore.ieee.org/document/8650452/,TENCON 2018 - 2018 IEEE Region 10 Conference,28-31 Oct. 2018,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/NOMS54207.2022.9789916,ML-based Performance Prediction of SDN using Simulated Data from Real and Synthetic Networks,IEEE,Conferences,"With increasing digitization and the emergence of the Internet of Things, more and more devices communicate with each other, resulting in a drastic growth of communication networks. Consequently, managing these networks, too, becomes harder and harder. Thus, Software-defined Networking (SDN) is employed, simplifying the management and configuration of networks by introducing a central controlling entity, which makes the network programmable via software and ultimately more flexible. As the SDN controller may impose scalability and elasticity issues, distributed controller architectures are utilized to combat this potential performance bottleneck. However, these distributed architectures introduce the need for constant synchronization to keep a centralized network view, and controller instances need to be placed in appropriate locations. As a result, thoroughly designing SDN-enabled networks with respect to a multitude of performance metrics, e. g., latency and induced traffic, is a challenging task. To assist in this process, we train a performance prediction model based on properties which are available during the network planning phase. We utilize a simulation-based approach for data collection to cover a large parameter space, simulating a variety of networks and controller placements for two opposing SDN architectures. On basis of this dataset, we apply Machine Learning (ML) to solve the performance prediction as a regression problem.",https://ieeexplore.ieee.org/document/9789916/,NOMS 2022-2022 IEEE/IFIP Network Operations and Management Symposium,25-29 April 2022,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/WACV.2009.5403083,ML-fusion based multi-model human detection and tracking for robust human-robot interfaces,IEEE,Conferences,"A novel stereo vision system for real-time human detection and tracking on a mobile service robot is presented in this paper. The system integrates the individually enhanced stereo-based human detection, HOG-based human detection, color-based tracking, and motion estimation for the robust detection and tracking of humans with large appearance and scale variations in real-world environments. A new framework of maximum likelihood based multi-model fusion is proposed to fuse these four human detection and tracking models according to the detection-track associations in 3D space, which is robust to the possible missed detections, false detections, and duplicated responses from the individual models. Multi-person tracking is implemented in a sequential near-to-far way, which well alleviates the difficulties caused by human-over-human occlusions. Extensive experimental results demonstrate the robustness of the proposed system under real-world scenarios with large variations in lighting conditions, cluttered backgrounds, human clothes and postures, and complex occlusion situations. Significant improvements in human detection and tracking have been achieved. The system has been deployed on six robot butlers to serve drinks, and showed encouraging performance in open ceremony events.",https://ieeexplore.ieee.org/document/5403083/,2009 Workshop on Applications of Computer Vision (WACV),7-8 Dec. 2009,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICDE51399.2021.00146,MLCask: Efficient Management of Component Evolution in Collaborative Data Analytics Pipelines,IEEE,Conferences,"With the ever-increasing adoption of machine learning for data analytics, maintaining a machine learning pipeline is becoming more complex as both the datasets and trained models evolve with time. In a collaborative environment, the changes and updates due to pipeline evolution often cause cumbersome coordination and maintenance work, raising the costs and making it hard to use. Existing solutions, unfortunately, do not address the version evolution problem, especially in a collaborative environment where non-linear version control semantics are necessary to isolate operations made by different user roles. The lack of version control semantics also incurs unnecessary storage consumption and lowers efficiency due to data duplication and repeated data pre-processing, which are avoidable.In this paper, we identify two main challenges that arise during the deployment of machine learning pipelines, and address them with the design of versioning for an end-to-end analytics system MLCask. The system supports multiple user roles with the ability to perform Git-like branching and merging operations in the context of the machine learning pipelines. We define and accelerate the metric-driven merge operation by pruning the pipeline search tree using reusable history records and pipeline compatibility information. Further, we design and implement the prioritized pipeline search, which gives preference to the pipelines that probably yield better performance. The effectiveness of MLCask is evaluated through an extensive study over several real-world deployment cases. The performance evaluation shows that the proposed merge operation is up to 7.8x faster and saves up to 11.9x storage space than the baseline method that does not utilize history records.",https://ieeexplore.ieee.org/document/9458924/,2021 IEEE 37th International Conference on Data Engineering (ICDE),19-22 April 2021,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICMLA.2018.00161,Machine Cognition of Violence in Videos Using Novel Outlier-Resistant VLAD,IEEE,Conferences,"Understanding highly accurate and real-time violent actions from surveillance videos is a demanding challenge. Our primary contribution of this work is divided into two parts. Firstly, we propose a computationally efficient Bag-of-Words (BoW) pipeline along with improved accuracy of violent videos classification. The novel pipeline's feature extraction stage is implemented with densely sampled Histogram of Oriented Gradients (HOG) and Histogram of Optical Flow (HOF) descriptors rather than Space-Time Interest Point (STIP) based extraction. Secondly, in encoding stage, we propose Outlier-Resistant VLAD (OR-VLAD), a novel higher order statistics-based feature encoding, to improve the original VLAD performance. In classification, efficient Linear Support Vector Machine (LSVM) is employed. The performance of the proposed pipeline is evaluated with three popular violent action datasets. On comparison, our pipeline achieved near perfect classification accuracies over three standard video datasets, outperforming most state-of-the-art approaches and having very low number of vocabulary size compared to previous BoW Models.",https://ieeexplore.ieee.org/document/8614186/,2018 17th IEEE International Conference on Machine Learning and Applications (ICMLA),17-20 Dec. 2018,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/I3DA48870.2021.9610915,Machine Learning-Based Room Classification for Selecting Binaural Room Impulse Responses in Augmented Reality Applications,IEEE,Conferences,"A key attribute of augmented reality (AR) applications is the matching reverberation of virtual sounds to the room acoustics of the real environment. However, especially in real-time scenarios where the properties of rapidly changing surroundings are unknown, creating a persistently coherent sound field synthesis within a real space is a challenging problem. While AR devices and their sensors can usually provide depth information within the field of view of the user, retrieving a complete geometric model requires significant time and user activity. Prior acoustic measurements or scans of the deployment area also severely limit many use cases, especially in the consumer sector. In this paper, we propose an automatic system that provides a fast selection of room categories and their corresponding binaural reverberation using only monoscopic images as input information. The proposed system combines existing approaches of machine learning (ML) based room classification and parametric synthesis of binaural room impulse responses (BRIRs) to provide room reverberation for arbitrary indoor environments. As a proof of concept, we present a demonstrator developed in Cycling&#x2019;74s Max linked to a python-based ML model. For the ML model, we use the convolutional neural network (CNN) GoogLeNet architecture trained on a subset of the Places365 data set. This subset contains 20 custom indoor room categories which are composed of the original categories that share similar acoustic properties. The demonstrator captures images and automatically selects binaural reverberation based on the predictions of the ML classifier. Monophonic stimuli are reverberated and presented using dynamic headphone-based binauralization.",https://ieeexplore.ieee.org/document/9610915/,2021 Immersive and 3D Audio: from Architecture to Automotive (I3DA),8-10 Sept. 2021,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISPASS.2017.7975264,Machine learning for performance and power modeling/prediction,IEEE,Conferences,"Effective design space exploration relies on fast and accurate pre-silicon performance and power models. Simulation is commonly used for understanding architectural tradeoffs, however many emerging workloads cannot even run on many full-system simulators. Even if you manage to run an emerging workload, it may be a tiny part of the workload, because detailed simulators are prohibitively slow. This talk presents some examples of how machine learning can be used to solve some of the problems haunting the performance evaluation field. An application for machine learning is in cross-platform performance and power prediction. If one model is slow to run real-world benchmarks/workloads, is it possible to predict/estimate its performance/power by using runs on another platform? Are there correlations that can be exploited using machine learning to make cross-platform performance and power predictions? A methodology to perform cross-platform performance/power predictions will be presented in this talk. Another application illustrating the use of machine learning to calibrate analytical power estimation models will be discussed. Yet another application for machine learning has been to create max power stressmarks. Manually developing and tuning so called stressmarks is extremely tedious and time-consuming while requiring an intimate understanding of the processor. In our past research, we created a framework that uses machine learning for the automated generation of stressmarks. In this talk, the methodology of the creation of automatic stressmarks will be explained. Experiments on multiple platforms validating the proposed approach will also be described.",https://ieeexplore.ieee.org/document/7975264/,2017 IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS),24-25 April 2017,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/TAI.2002.1180806,Maintenance scheduling of oil storage tanks using tabu-based genetic algorithm,IEEE,Conferences,"Due to the entry of Taiwan into WTO and the recently liberalized Petroleum Management Law, the oil market in Taiwan is liberalized and thus is becoming more competitive. However, the space limitation and the residents' increasing awareness of environmental protection issues in the island make international vendors unavoidably have to rent tanks from domestic oil companies. In order to help the leaseholder maximize revenue by increasing the availability of tanks, an efficient maintenance scheduling is needed. This paper introduces a tabu-based genetic algorithm (TGA) and its implementation for solving a real-world maintenance scheduling problem of oil storage tanks. TGA incorporates a tabu list to prevent inbreeding and utilizes an aspiration criterion to supply moderate selection pressure so that the selection efficiency is improved, and the population diversity is maintained. The experimental results validate that TGA outperform GA in terms of solution quality and convergence efficiency. Keywords: Tabu-based genetic algorithm, maintenance scheduling, tabu search, genetic algorithm.",https://ieeexplore.ieee.org/document/1180806/,"14th IEEE International Conference on Tools with Artificial Intelligence, 2002. (ICTAI 2002). Proceedings.",4-6 Nov. 2002,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AIKE.2018.00051,Management of Subdivided Dynamic Indoor Environments by Autonomous Scanning System,IEEE,Conferences,"With the development of sensing technologies, various spatial applications have been expanding into indoor spaces. For smooth spatial services, grasping indoor space information is most essential task. However, the indoor spaces is not only becoming increasingly complex, but also frequently changed than outdoor spaces. This makes it hard to provide an accurate location based service in an indoor space. This paper propose a way of managing a dynamic indoor environment by defining a multi-layered indoor model in terms of an object mobility. It allows an indoor space to be managed more elaborate and realistic than up-to-date indoor models which only consider an indoor floor plan. We firstly define a classification of indoor objects based on their characteristic to frequently change location, and propose three-layers indoor model followed by the classified objects with its mobility. Secondly, we design and implement an autonomous scanning system to understand changes of indoor situation quickly and automatically. The system is made up of a combination of IoT devices, including a programmable robot, lidar scanner and single-board computer. Finally, we demonstrate an implementation of the system with constructing the proposed model from a real indoor environment.",https://ieeexplore.ieee.org/document/8527483/,2018 IEEE First International Conference on Artificial Intelligence and Knowledge Engineering (AIKE),26-28 Sept. 2018,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SiPS.2018.8598454,Mapping Systolic Arrays onto 3D Circuit Structures: Accelerating Convolutional Neural Network Inference,IEEE,Conferences,"In recent years, numerous designs have used systolic arrays to accelerate convolutional neural network (CNN) inference. In this work, we demonstrate that we can further speed up CNN inference and lower its power consumption by mapping systolic arrays onto 3D circuit structures as opposed to conventional 2D structures. Specifically, by operating in 3D space, a wide systolic array consisting of a number of subarrays can efficiently implement wide convolutional layers prevalent in state of the art CNNs. Additionally, by accumulating intermediate results along the third dimension, systolic arrays can process partitioned data channels in parallel with reduced data skew for lowered inference latency. We present a building block design using through-silicon vias (TSVs) for the 3D realization of systolic subarrays. We validate the 3D scheme using a 2.5D FPGA design and demonstrate that when mapped onto 3D structures wide systolic arrays can scale up in size without increasing wiring length in interconnecting subarrays. Further, by taking full advantage of 3D structures, we are able to pipeline inference across multiple layers of a CNN over a series of systolic arrays, dramatically reducing the inference time per input sample. These improvements lead to significantly reduced inference latency, which is especially important for real-time applications where it is common to process samples one at a time.",https://ieeexplore.ieee.org/document/8598454/,2018 IEEE International Workshop on Signal Processing Systems (SiPS),21-24 Oct. 2018,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICIP.2009.5414267,Margin and domain integrated classification,IEEE,Conferences,"Multi-category classification is an ongoing research topic with numerous applications. In this paper, a novel approach called margin and domain integrated classifier (MDIC) is addressed. It handles multi-class problems as a combination of several target classes plus outliers. The basic idea behind the proposed approach is that target classes possess structured characteristics while outliers scatter around in the feature space. In our approach the domain description and large-margin discrimination are adjustable and therefore higher classification accuracy leads to better performance. The properties of MDIC are analyzed and the performance comparisons using synthetic and real data are presented.",https://ieeexplore.ieee.org/document/5414267/,2009 16th IEEE International Conference on Image Processing (ICIP),7-10 Nov. 2009,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CCIS53392.2021.9754642,Mask Image to Real Image Generation Based on Semantic Control Context Encoder,IEEE,Conferences,"In the field of image inpainting, there are some deep learning schemes, but the pixel inpainting of these schemes generally does not consider the semantics of the image. In this paper, the Semantic Control Context Encoder(SCCE) is proposed, which combines the confrontation network of text-generated images with traditional image restoration to form a comprehensive image restoration method. In this method, a context encoder is used as the generator, and a picture generated from the text is compared with the restored pictures. At the same time, the difference between the text itself and the restored picture mapped to the same space is regarded as the loss to judge the restored result, thus introducing the semantic meaning represented by the picture generated by the text on the basis of the original context encoder, and increasing the rationality of the generated picture. Experimental results on the open data set show that the proposed algorithm is superior to the traditional context encoder algorithms and the edge first algorithms.",https://ieeexplore.ieee.org/document/9754642/,2021 IEEE 7th International Conference on Cloud Computing and Intelligent Systems (CCIS),7-8 Nov. 2021,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/DSAA.2019.00059,Maximum Relevance and Minimum Redundancy Feature Selection Methods for a Marketing Machine Learning Platform,IEEE,Conferences,"In machine learning applications for online product offerings and marketing strategies, there are often hundreds or thousands of features available to build such models. Feature selection is one essential method in such applications for multiple objectives: improving the prediction accuracy by eliminating irrelevant features, accelerating the model training and prediction speed, reducing the monitoring and maintenance workload for feature data pipeline, and providing better model interpretation and diagnosis capability. However, selecting an optimal feature subset from a large feature space is considered as an NP-complete problem. The mRMR (Minimum Redundancy and Maximum Relevance) feature selection framework solves this problem by selecting the relevant features while controlling for the redundancy within the selected features. This paper describes the approach to extend, evaluate, and implement the mRMR feature selection methods for classification problem in a marketing machine learning platform at Uber that automates creation and deployment of targeting and personalization models at scale. This study first extends the existing mRMR methods by introducing a non-linear feature redundancy measure and a model-based feature relevance measure. Then an extensive empirical evaluation is performed for eight different feature selection methods, using one synthetic dataset and three real-world marketing datasets at Uber to cover different use cases. Based on the empirical results, the selected mRMR method is implemented in production for the marketing machine learning platform. A description of the production implementation is provided and an online experiment deployed through the platform is discussed.",https://ieeexplore.ieee.org/document/8964172/,2019 IEEE International Conference on Data Science and Advanced Analytics (DSAA),5-8 Oct. 2019,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICMLA.2016.0118,Area-Specific Crime Prediction Models,IEEE,Conferences,"The convergence of public data and statistical modeling has created opportunities for public safety officials to prioritize the deployment of scarce resources on the basis of predicted crime patterns. Current crime prediction methods are trained using observed crime and information describing various criminogenic factors. Researchers have favored global models (e.g., of entire cities) due to a lack of observations at finer resolutions (e.g., ZIP codes). These global models and their assumptions are at odds with evidence that the relationship between crime and criminogenic factors is not homogeneous across space. In response to this gap, we present area-specific crime prediction models based on hierarchical and multi-task statistical learning. Our models mitigate sparseness by sharing information across ZIP codes, yet they retain the advantages of localized models in addressing non-homogeneous crime patterns. Out-of-sample testing on real crime data indicates predictive advantages over multiple state-of-the-art global models.",https://ieeexplore.ieee.org/document/7838222/,2016 15th IEEE International Conference on Machine Learning and Applications (ICMLA),18-20 Dec. 2016,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/EMBC46164.2021.9630252,Arousal-Valence Classification from Peripheral Physiological Signals Using Long Short-Term Memory Networks,IEEE,Conferences,"The automated recognition of human emotions plays an important role in developing machines with emotional intelligence. However, most of the affective computing models are based on images, audio, videos and brain signals. There is a lack of prior studies that focus on utilizing only peripheral physiological signals for emotion recognition, which can ideally be implemented in daily life settings using wearables, e.g., smartwatches. Here, an emotion classification method using peripheral physiological signals, obtained by wearable devices that enable continuous monitoring of emotional states, is presented. A Long Short-Term Memory neural network-based classification model is proposed to accurately predict emotions in real-time into binary levels and quadrants of the arousal-valence space. The peripheral sensored data used here were collected from 20 participants, who engaged in a naturalistic debate. Different annotation schemes were adopted and their impact on the classification performance was explored. Evaluation results demonstrate the capability of our method with a measured accuracy of >93% and >89% for binary levels and quad classes, respectively. This paves the way for enhancing the role of wearable devices in emotional state recognition in everyday life.",https://ieeexplore.ieee.org/document/9630252/,2021 43rd Annual International Conference of the IEEE Engineering in Medicine & Biology Society (EMBC),1-5 Nov. 2021,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CIEEC50170.2021.9510594,Artificial Intelligence Assistant Decision-Making Method for Main &#x0026; Distribution Power Grid Integration Based on Deep Deterministic Network,IEEE,Conferences,"This paper studies the technology of generating DDPG (deep deterministic policy gradient) by using the deep dual network and experience pool network structure, and puts forward the sampling strategy gradient algorithm to randomly select actions according to the learned strategies (action distribution) in the continuous action space, based on the dispatching control system of the power dispatching control center of a super city power grid, According to the actual characteristics and operation needs of urban power grid, The developed refined artificial intelligence on-line security analysis and emergency response plan intelligent generation function realize the emergency response auxiliary decision-making intelligent generation function. According to the hidden danger of overload and overload found in the online safety analysis, the relevant load lines of the equipment are searched automatically. Through the topology automatic analysis, the load transfer mode is searched to eliminate or reduce the overload or overload of the equipment. For a variety of load transfer modes, the evaluation index of the scheme is established, and the optimal load transfer mode is intelligently selected. Based on the D5000 system of Metropolitan power grid, a multi-objective and multi resource coordinated security risk decision-making assistant system is implemented, which provides integrated security early warning and decision support for the main network and distribution network of city power grid. The intelligent level of power grid dispatching management and dispatching operation is improved. The state reality network can analyze the joint state observations from the action reality network, and the state estimation network uses the actor action as the input. In the continuous action space task, DDPG is better than dqn and its convergence speed is faster.",https://ieeexplore.ieee.org/document/9510594/,2021 IEEE 4th International Electrical and Energy Conference (CIEEC),28-30 May 2021,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/RAST.2019.8767447,Artificial Intelligence Implementation on Voice Command and Sensor Anomaly Detection for Enhancing Human Habitation in Space Mission,IEEE,Conferences,"The work in this paper describes implementation of Artificial Neural Network (ANN) on space processor LEON3. The ANN has been tested for training voice signal and for detecting anomaly signal on multiple analog sensors. The build-in radiation hardened UART 115200 interface of standard Space Hardware was utilized to receive compressed data from Artificial Intelligence (AI) Kit. The AI Kit was built to acquire human voice (as voice command) or to process input signals from multiple sensors concurrently. The Kit enables voice intuitively by pressing a training button and selecting proper command type from the keypad. The experiment results show that voice commands were detected successfully with accuracy of more than 95%. The second experiment was carried out by using analog sensor signals mixer to allow AI to learn and determine type of sensor data anomalies when some failures occur. The anomalies types were generated by adding unexpected stimulus signals to AI Kit analog input terminals. The result shows that the anomalies can be detected with accuracy of 80%. The size of AI Kit is relatively small and it was built with commercial components that enable replacement with space radiation hardened components. The space computer platform is based on LEON3 processor core and synthesized on Xilinx Virtex-5QV Field Programmable Logic Arrays (FPGA). The core runs at 100 MHz. The feed forward artificial neural networks Algorithm was implemented on Real-Time Executive for Multiprocessor Systems (RTEMS) operating system. The AI Kit consists of audio signal pre-amplifier, automatic gain control circuit, analog signal buffer circuit, high pass filter circuit (HPF), analog to digital converter ADC which is integrated in the 8 bit microcontroller. The modified FFT algorithm that runs on microcontroller is used for data compression and for increasing uniqueness of the data acquired. A DC/DC converter for battery usage is included, when 5V voltage supply is not available.",https://ieeexplore.ieee.org/document/8767447/,2019 9th International Conference on Recent Advances in Space Technologies (RAST),11-14 June 2019,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IPAPS49326.2019.9069391,Attributes of Big Data Analytics for Data-Driven Decision Making in Cyber-Physical Power Systems,IEEE,Conferences,"Big data analytics is a virtually new term in power system terminology. This concept delves into the way a massive volume of data is acquired, processed, analyzed to extract insight from available data. In particular, big data analytics alludes to applications of artificial intelligence, machine learning techniques, data mining techniques, time-series forecasting methods. Decision-makers in power systems have been long plagued by incapability and weakness of classical methods in dealing with large-scale real practical cases due to the existence of thousands or millions of variables, being time-consuming, the requirement of a high computation burden, divergence of results, unjustifiable errors, and poor accuracy of the model. Big data analytics is an ongoing topic, which pinpoints how to extract insights from these large data sets. The extant article has enumerated the applications of big data analytics in future power systems through several layers from grid-scale to local-scale. Big data analytics has many applications in the areas of smart grid implementation, electricity markets, execution of collaborative operation schemes, enhancement of microgrid operation autonomy, management of electric vehicle operations in smart grids, active distribution network control, district hub system management, multi-agent energy systems, electricity theft detection, stability and security assessment by PMUs, and better exploitation of renewable energy sources. The employment of big data analytics entails some prerequisites, such as the proliferation of IoT-enabled devices, easily-accessible cloud space, blockchain, etc. This paper has comprehensively conducted an extensive review of the applications of big data analytics along with the prevailing challenges and solutions.",https://ieeexplore.ieee.org/document/9069391/,2020 14th International Conference on Protection and Automation of Power Systems (IPAPS),31 Dec.-1 Jan. 2020,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IC3.2019.8844913,Augmentation of Images through DCGANs,IEEE,Conferences,"Now-a-days, extending images has become a challenging task to implement. Many of algorithms like convolution neural networks (CNN), Generative Adversarial Network (GAN) are used to fill out the image spaces. But the challenge arrives to guess or make appropriate assumption of image extended borders. We used Deep Convolution Generative Adversarial Network(DCGAN) over GAN and CNN to implement it.[1], [2] GAN's are implemented to guess the space in image by generating fake examples using generator and decides using discriminator which determines how much the image is real. DCGAN is improved version of GAN which generates spatial correlations [5].",https://ieeexplore.ieee.org/document/8844913/,2019 Twelfth International Conference on Contemporary Computing (IC3),8-10 Aug. 2019,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/EMBC46164.2021.9629804,Augmented Reality Assisted Surgical Navigation System for Epidural Needle Intervention,IEEE,Conferences,"An augmented reality (AR)-assisted surgical navigation system was developed for epidural needle intervention. The system includes three components: a virtual reality-based surgical planning software, a patient and tool tracking system, and an AR-based surgical navigation system. A three-dimensional (3D) path plan for the epidural needle was established on the preoperative computed tomography (CT) image. The plan is then registered to the intraoperative space by 3D models of the target vertebrae using skin markers and real-time tracking information. In the procedure, the plan and tracking information are transmitted to the head-mounted display (HMD) through a wireless network such that the device directly visualizes the plan onto the back surface of the patient. The physician determines the entry point and inserts the needle into the target based on the direct visual guidance of the system. An experiment was conducted to validate the system using two torso phantoms that mimic human respiration. The experimental results demonstrated that the time and the number of X-rays required for needle insertion were significantly decreased by the proposed method (43.6&#x00B1;20.55sec, 2.9&#x00B1;1.3times) compared to those of the conventional fluoroscopy-guided approach (124.5 &#x00B1; 46.7s, 9.3&#x00B1;2.4times), whereas the average targeting errors were similar in both cases. The proposed system may potentially decrease ionizing radiation exposure not only to the patient but also to the medical team.",https://ieeexplore.ieee.org/document/9629804/,2021 43rd Annual International Conference of the IEEE Engineering in Medicine & Biology Society (EMBC),1-5 Nov. 2021,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICC.2019.8761821,Authentication Scheme Based on Hashchain for Space-Air-Ground Integrated Network,IEEE,Conferences,"With the development of artificial intelligence and self-driving, vehicular ad-hoc network (VANET) has become an irreplaceable part of the Intelligent Transportation Systems (ITSs). However, the traditional network of the ground cannot meet the requirements of transmission, processing, and storage among vehicles. Under this circumstance, integrating space and air nodes into the whole network can provide comprehensive traffic information and reduce the transmission delay. The high mobility and low latency in the Space-Air-Ground Integrated Network (SAGIN) put forward higher requirements for security issues such as identity authentication, privacy protection and data security. This paper simplifies the Blockchain and proposes an identity authentication and privacy protection scheme based on the Hashchain in the SAGIN. The scheme focuses on the characteristics of the wireless signal to identify and authenticate the nodes. The verification and backup of the records on the block are implemented with the distributed streaming platform, Kafka algorithm, instead of the consensus. Furthermore, this paper analyzes the security of this scheme. Afterward, the experimental results reveal the delay brought by the scheme using the simulation of SUMO, OMNeT++, and Veins.",https://ieeexplore.ieee.org/document/8761821/,ICC 2019 - 2019 IEEE International Conference on Communications (ICC),20-24 May 2019,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1145/3238147.3238175,AutoConfig: Automatic Configuration Tuning for Distributed Message Systems,IEEE,Conferences,"Distributed message systems (DMSs) serve as the communication backbone for many real-time streaming data processing applications. To support the vast diversity of such applications, DMSs provide a large number of parameters to configure. However, It overwhelms for most users to configure these parameters well for better performance. Although many automatic configuration approaches have been proposed to address this issue, critical challenges still remain: 1) to train a better and robust performance prediction model using a limited number of samples, and 2) to search for a high-dimensional parameter space efficiently within a time constraint. In this paper, we propose AutoConfig - an automatic configuration system that can optimize producer-side throughput on DMSs. AutoConfig constructs a novel comparison-based model (CBM) that is more robust that the prediction-based model (PBM) used by previous learning-based approaches. Furthermore, AutoConfig uses a weighted Latin hypercube sampling (wLHS) approach to select a set of samples that can provide a better coverage over the high-dimensional parameter space. wLHS allows AutoConfig to search for more promising configurations using the trained CBM. We have implemented AutoConfig on the Kafka platform, and evaluated it using eight different testing scenarios deployed on a public cloud. Experimental results show that our CBM can obtain better results than that of PBM under the same random forests based model. Furthermore, AutoConfig outperforms default configurations by 215.40% on average, and five state-of-the-art configuration algorithms by 7.21%-64.56%.",https://ieeexplore.ieee.org/document/9000079/,2018 33rd IEEE/ACM International Conference on Automated Software Engineering (ASE),3-7 Sept. 2018,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/RCAR49640.2020.9303043,Autobot for Effective Design Space Exploration and Agile Generation of RBFNN Hardware Accelerator in Embedded Real-time Computing,IEEE,Conferences,"This paper presents a method of employing Auto-bot to replace humans in the task of efficient hardware design for radial basis function neural network (RBFNN) in real-time computing applications. Autobot applies quick iterations using hardware generation and supports various number systems such as floating-point, half-floating point, and mixed-precision and hardware architectures to perform possible design space exploration, enabling an agile analysis for those requests. We have implemented and employed Autobot to successfully test with the applications of RBFNN-based Mackey-Glass chaotic time series prediction, servo motor control, and data classification. Analysis of these results shows that Autobot is able to deliver the hardware accelerator with less execution time than previous works, which also shortens the design time from days to minutes. Therefore, the proposed methodology is a useful alternative for agile real-time hardware development on FPGA.",https://ieeexplore.ieee.org/document/9303043/,2020 IEEE International Conference on Real-time Computing and Robotics (RCAR),28-29 Sept. 2020,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/BigMM52142.2021.00015,Automated Detection of Corona Cavities from SDO Images with YOLO,IEEE,Conferences,"Solar eruptions, such as filament eruptions and Corona Mass Ejections (CMEs), are primary drivers of hazardous space weather that impacts the critical infrastructure on the Earth. Researchers now believe that corona cavities serve as launch pads for CMEs. In this study, we proposed, implemented, and evaluated a method for automated corona cavity detection using YOLOv5 framework. We applied our method to the data observed by the Solar Dynamics Observatory (SDO) during the period from 2010 to 2013. With 18, 512 x 512 images, we were able to build a model that achieved a precision rate of 0.964, a recall rate of 0.883, and a mAP@0.5 of 0.898. With GPU freely available on the Google Colaboratory, training the model takes about 40 minutes and the inference time per image is about 9 milliseconds. Experiments showed the proposed method is promising in real-time corona cavity detection, and potentially tracking CMEs and predicting space weather.",https://ieeexplore.ieee.org/document/9643198/,2021 IEEE Seventh International Conference on Multimedia Big Data (BigMM),15-17 Nov. 2021,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/DICTA.2012.6411741,Automated Detection of Root Crowns Using Gaussian Mixture Model and Bayes Classification,IEEE,Conferences,"In this paper a method for automatic detection of root crowns in root images, are designed, implemented and quantitatively compared. The approach is based on the theory of statistical learning. The root images are preprocessed with algorithms for intensity normalization, segmentation, edge detection and scale space corner detection. The features used in the experiments are the Zernike moments of the bi-level image patch centered around high curvature detections. Zernike moments are orthogonal and thus can be rightly assumed to be independent. The densities of the feature vectors for different classes are modelled with Gaussian mixture model (GMM), with a diagonal covariance matrix. The parameters for the feature's distribution densities for different classes are learnt by expectation maximization. Bayes rule and Neymann-Pearson criteria is used to design the classification method. We experiment with different orders of Zernike moments and different number of Gaussians in the GMM. The experiments are done on a real dataset with images of rice, corn, and grass roots. Pattern classification results are quantitatively analyzed using Receiver Operating Characteristic (ROC) curves and area under the ROC curves. We quantitatively compare the results of the proposed method with that of support vector machine (SVM) which is another very popular statistical learning method for pattern classification.",https://ieeexplore.ieee.org/document/6411741/,2012 International Conference on Digital Image Computing Techniques and Applications (DICTA),3-5 Dec. 2012,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.23919/OCEANS44145.2021.9705933,Automated Synthetic Aperture Sonar Image Segmentation using Spatially Coherent Clustering,IEEE,Conferences,"Seabed image segmentation is an important product for a variety of fields, including, habitat mapping, geological surveys, mine counter measures, and naval route planning. Developing a clustering algorithm that can both accurately segment high resolution imagery and generalize well over large areas is challenging. In this paper we will evaluate the performance of a new unsupervised image segmentation algorithm. The method utilizes imagery derived features (intensity and texture) to identify clusters (different seabed types) in feature space while also encouraging local homogeneity. In this paper we will demonstrate how our spatially coherent k-means clustering algorithm can efficiently and accurately segment Synthetic Aperture Sonar (SAS) images. Our experiments show that our spatially coherent clustering algorithm can increase segmentation accuracy up to 15 and 20 percent relative to OpenCV k-means and ArcGIS Pro ISO clustering, respectively.",https://ieeexplore.ieee.org/document/9705933/,OCEANS 2021: San Diego – Porto,20-23 Sept. 2021,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SMC.2018.00655,Automated Training Plan Generation for Athletes,IEEE,Conferences,"In sports, athletes need detailed and individualised training plans for maintaining and improving their skills in order to achieve their best performance in competitions. This presents a considerable workload for coaches, who besides setting objectives have to formulate extremely detailed training plans. Automated Planning, which has already been successfully deployed in many real-world applications such as space exploration, robotics, and manufacturing processes, embodies a useful mechanism that can be exploited for generating training plans for athletes. In this paper, we propose the use of Automated Planning techniques for generating individual training plans, which consist of exercises the athlete has to perform during training, given the athlete's current performance, period of time, and target performance that should be achieved. Our experimental analysis, which considers general training of kickboxers, shows that apart of considerable less planning time, training plans automatically generated by the proposed approach are more detailed and individualised than plans prepared manually by an expert coach.",https://ieeexplore.ieee.org/document/8616652/,"2018 IEEE International Conference on Systems, Man, and Cybernetics (SMC)",7-10 Oct. 2018,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/EMBC.2016.7592132,Automated classification of pathological gait after stroke using ubiquitous sensing technology,IEEE,Conferences,"This study uses machine learning methods to distinguish between healthy and pathological gait. Examples of multi-dimensional pathological and normal gait sequences were collected from post-stroke and healthy individuals in a real clinical setting and with two Kinect sensors. The trajectories of rotational angle and global velocity of selected body joints (hips, spine, shoulders, neck, knees and ankles) over time formed the gait sequences. The combination of k nearest neighbor (kNN) and dynamic time warping (DTW) was used for classification. Leave one subject out cross validation was implemented to evaluate the performance of the binary classifier in terms of F1-score in the original feature space, and also in a reduced dimensional feature space using PCA. The pair of k = 1 in kNN and the warping window size 25% of gait sequences in DTW achieved maximum F1-score. Using PCA, pathological gait sequences were discriminated from healthy sequences with the F1-score = 96%.",https://ieeexplore.ieee.org/document/7592132/,2016 38th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC),16-20 Aug. 2016,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1145/3061639.3062207,Automated systolic array architecture synthesis for high throughput CNN inference on FPGAs,IEEE,Conferences,"Convolutional neural networks (CNNs) have been widely applied in many deep learning applications. In recent years, the FPGA implementation for CNNs has attracted much attention because of its high performance and energy efficiency. However, existing implementations have difficulty to fully leverage the computation power of the latest FPGAs. In this paper we implement CNN on an FPGA using a systolic array architecture, which can achieve high clock frequency under high resource utilization. We provide an analytical model for performance and resource utilization and develop an automatic design space exploration framework, as well as source-to-source code transformation from a C program to a CNN implementation using systolic array. The experimental results show that our framework is able to generate the accelerator for real-life CNN models, achieving up to 461 GFlops for floating point data type and 1.2 Tops for 8–16 bit fixed point.",https://ieeexplore.ieee.org/document/8060313/,2017 54th ACM/EDAC/IEEE Design Automation Conference (DAC),18-22 June 2017,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISSREW.2019.00102,Automatic Cause Detection of Performance Problems in Web Applications,IEEE,Conferences,"The execution of similar units can be compared by their internal behaviors to determine the causes of their potential performance issues. For instance, by examining the internal behaviors of different fast or slow web requests more closely, and by clustering and comparing their internal executions, one can determine what causes some requests to run slowly or behave in unexpected ways. In this paper, we propose a method of extracting the internal behavior of web requests as well as introduce a pipeline that detects performance issues in web requests and provides insights into their root causes. First, low-level and fine-grained information regarding each request is gathered by tracing both the user space and the kernel space. Second, further information is extracted and fed into an outlier detector. Finally, these outliers are then clustered by their behavior, and each group is analyzed separately. Experiments revealed that this pipeline is indeed able to detect slow web requests and provide additional insights into their true root causes. Notably, we were able to identify a real PHP cache contention issue using the proposed approach.",https://ieeexplore.ieee.org/document/8990337/,2019 IEEE International Symposium on Software Reliability Engineering Workshops (ISSREW),27-30 Oct. 2019,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CYBER.2017.8446080,Automatic Safety Helmet Wearing Detection,IEEE,Conferences,"Surveillance is very essential for the safety of power substation. The detection of whether wearing safety helmets or not for perambulatory workers is the key component of overall intelligent surveillance system in power substation. In this paper, a novel and practical safety helmet detection framework based on computer vision, machine learning and image processing is proposed. In order to ascertain motion objects in power substation, the ViBe background modelling algorithm is employed. Moreover, based on the result of motion objects segmentation, real-time human classification framework C4 is applied to locate pedestrian in power substation accurately and quickly. Finally, according to the result of pedestrian detection, the safety helmet wearing detection is implemented using the head location, the color space transformation and the color feature discrimination. Extensive compelling experimental results in power substation illustrate the efficiency and effectiveness of the proposed framework.",https://ieeexplore.ieee.org/document/8446080/,"2017 IEEE 7th Annual International Conference on CYBER Technology in Automation, Control, and Intelligent Systems (CYBER)",31 July-4 Aug. 2017,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICSE43902.2021.00048,Automatic Web Testing Using Curiosity-Driven Reinforcement Learning,IEEE,Conferences,"Web testing has long been recognized as a notoriously difficult task. Even nowadays, web testing still mainly relies on manual efforts in many cases while automated web testing is still far from achieving human-level performance. Key challenges include dynamic content update and deep bugs hiding under complicated user interactions and specific input values, which can only be triggered by certain action sequences in the huge space of all possible sequences. In this paper, we propose WebExplor, an automatic end-to-end web testing framework, to achieve an adaptive exploration of web applications. WebExplor adopts a curiosity-driven reinforcement learning to generate high-quality action sequences (test cases) with temporal logical relations. Besides, WebExplor incrementally builds an automaton during the online testing process, which acts as the high-level guidance to further improve the testing efficiency. We have conducted comprehensive evaluations on six real-world projects, a commercial SaaS web application, and performed an in-the-wild study of the top 50 web applications in the world. The results demonstrate that in most cases WebExplor can achieve significantly higher failure detection rate, code coverage and efficiency than existing state-of-the-art web testing techniques. WebExplor also detected 12 previously unknown failures in the commercial web application, which have been confirmed and fixed by the developers. Furthermore, our in-the-wild study further uncovered 3,466 exceptions and errors.",https://ieeexplore.ieee.org/document/9402046/,2021 IEEE/ACM 43rd International Conference on Software Engineering (ICSE),22-30 May 2021,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCChina.2014.7008377,Automatic collecting of indoor localization fingerprints: An crowd-based approach,IEEE,Conferences,"For typical indoor positioning systems employing a training/positioning model based on Wi-Fi fingerprints, significant training costs extremely restrict this kind of indoor localization system to be widely deployed and implemented with real location based applications. In this paper, we present a crowd-based approach to solve this problem, which automatically collects and constructs fingerprints database for anonymous buildings through common crowd customers with their smart-phones. However, such a crowd-based approach also introduces an accuracy degradation problem as crowd customers are not professional trained and equipped. So in this approach we employ fixed and hint landmarks to do error resetting. In our practical system, common corridor crossing points will serve as fixed landmarks and cross points between different crowd paths serve as hint landmarks. Machine-learning techniques are utilized for short range approximation around fixed landmarks and fuzzy logic decision technology is applied for searching hint landmarks in crowd traces space. We test this crowd-based automatic collecting approach on a dataset of about 5.09km walking in four corridors and the rooms besides. Experimental results indicate that this automatic collecting approach successfully construct indoor fingerprint radio map with rather high accuracy.",https://ieeexplore.ieee.org/document/7008377/,2014 IEEE/CIC International Conference on Communications in China (ICCC),13-15 Oct. 2014,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISIE.1999.796864,Automatic edge detection of DNA bands in autoradiograph images,IEEE,Conferences,"Scale space analysis is an efficient solution to the edge detection of objects in low to high contrast images. However, this approach is time consuming and computationally expensive. The parallel processing properties of a neural network provide an ideal solution to managing the large amounts of data processed in image analysis, however their application to multiscale analysis is still in its infancy. This paper reports on a new approach to detecting 2-dimensional and 3-dimensional objects in low to high contrast images. The novel idea is based on combining neural network arbitration and scale space analysis to automatically select one optimum scale for the entire image at which scale space edge detection can be applied. Thus, introducing new measures to solve many of the problems existing in the discipline of image processing, such as poor edge detection in low contrast images, speed of recognition and high computational cost. This new approach to edge detection is formalised in the Automatic Edge Detection Scheme (AEDS). The AEDS is implemented on a real-life application namely, the detection of bands within low contrast DNA autoradiograph images. Results are presented to show the AEDS overcoming the aforementioned common problems with image processing techniques.",https://ieeexplore.ieee.org/document/796864/,ISIE '99. Proceedings of the IEEE International Symposium on Industrial Electronics (Cat. No.99TH8465),12-16 July 1999,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/TAI.1996.560799,Automatic scale selection as a pre-processing stage to interpreting real-world data,IEEE,Conferences,"Summary form only given. We perceive objects in the world as meaningful entities only over certain ranges of scale. This fact that objects in the world appear in different ways depending on the scale of observation has important implications if one aims at describing them. It shows that the notion of scale is of utmost importance when processing unknown measurement data by automatic methods. In their seminal works, Witkin (1983) and Koenderink (1984) proposed to approach this problem by representing image structures at different scales in a so-called scale-space representation. Traditional scale-space theory building on this work, however, does not address the problem of how to select local appropriate scales for further analysis. After a brief review of the main ideas behind a scale-space representation, I describe a systematic methodology for generating hypotheses about interesting scale levels in image data based on a general principle stating that local extrema over scales of different combinations of normalized derivatives are likely candidates to correspond to interesting image structures. Specifically, I show how this idea can be used for formulating feature detectors which automatically adapt their local scales of processing to the local image structure. I show how the scale selection approach applies to various types of feature detection problems in early vision. In many computer vision applications, the poor performance of the low-level vision modules constitutes a major bottleneck.",https://ieeexplore.ieee.org/document/560799/,Proceedings Eighth IEEE International Conference on Tools with Artificial Intelligence,16-19 Nov. 1996,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS45743.2020.9341657,Autonomous Exploration Under Uncertainty via Deep Reinforcement Learning on Graphs,IEEE,Conferences,"We consider an autonomous exploration problem in which a range-sensing mobile robot is tasked with accurately mapping the landmarks in an a priori unknown environment efficiently in real-time; it must choose sensing actions that both curb localization uncertainty and achieve information gain. For this problem, belief space planning methods that forward- simulate robot sensing and estimation may often fail in real-time implementation, scaling poorly with increasing size of the state, belief and action spaces. We propose a novel approach that uses graph neural networks (GNNs) in conjunction with deep reinforcement learning (DRL), enabling decision-making over graphs containing exploration information to predict a robot's optimal sensing action in belief space. The policy, which is trained in different random environments without human intervention, offers a real-time, scalable decision-making process whose high-performance exploratory sensing actions yield accurate maps and high rates of information gain.",https://ieeexplore.ieee.org/document/9341657/,2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),24 Oct.-24 Jan. 2021,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SIMPAR.2016.7862403,Autonomous exploration by expected information gain from probabilistic occupancy grid mapping,IEEE,Conferences,"Occupancy grid maps are spatial representations of environments, where the space of interest is decomposed into a number of cells that are considered either occupied or free. This paper focuses on exploring occupancy grid maps by predicting the uncertainty of the map. Based on recent improvements in computing occupancy probability, this paper presents a novel approach for selecting robot poses designed to maximize expected map information gain represented by the change in entropy. This result is simplified with several approximations to develop an algorithm suitable for real-time implementation. The predicted information gain proposed in this paper governs an effective autonomous exploration strategy when applied in conjunction with an existing motion planner to avoid obstacles, which is illustrated by numerical examples.",https://ieeexplore.ieee.org/document/7862403/,"2016 IEEE International Conference on Simulation, Modeling, and Programming for Autonomous Robots (SIMPAR)",13-16 Dec. 2016,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SmartWorld-UIC-ATC-SCALCOM-IOP-SCI.2019.00201,BDMF: A Biased Deep Matrix Factorization Model for Recommendation,IEEE,Conferences,"As a representative collaborative filtering method, matrix factorization has been widely used in personalized recommendation. Recently, deep matrix factorization model, which utilizes deep neural networks to project users and items into a latent structured space, has received increased attention. In this paper, inspired by the idea of BiasedSVD that introduces bias to both users and items, we propose a novel matrix factorization model with neural network architecture, named BDMF, short for Biased Deep Matrix Factorization. Specifically, we first construct a user-item interaction matrix with explicit ratings and implicit feedback, and randomly sample users and items as the input. Next, we feed this input to the proposed BDMF model to learn latent factors of both users and items, and then use them to predict the ratings for personalized ranking. We also formally show that BDMF works on the same principle as BiasedSVD, which means that BDMF can be viewed as a deep neural network implementation of BiasedSVD. Finally, extensive experiments on real-world datasets are conducted and the results verify the superiority of our model over other state-of-the-art.",https://ieeexplore.ieee.org/document/9060377/,"2019 IEEE SmartWorld, Ubiquitous Intelligence & Computing, Advanced & Trusted Computing, Scalable Computing & Communications, Cloud & Big Data Computing, Internet of People and Smart City Innovation (SmartWorld/SCALCOM/UIC/ATC/CBDCom/IOP/SCI)",19-23 Aug. 2019,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IES53407.2021.9594013,Ball Position Transformation with Artificial Intelligence Based on Tensorflow Libraries,IEEE,Conferences,Research on wheeled soccer robots has been carried out by several researchers. This is due to the existence of national and international competitions. Previous research was to create a ball position transformation system with a modified method of neural network architecture. This research was developed by building an intelligent transformation system with the Tensorflow library. This transformation system aims to be able to directly measure the distance of objects in real terms without first changing the environmental image from an omni field to a flat plane with conventional camera calibration techniques. This process can replace manual calibration with a variety of field size changes The system can transform with mean error 0.0000026 on epoch 10000 using “conda-tensorflowneural network” libraries. It can transform the position of the ball from the omni space to the cartesian space. This system was implemented on wheeled soccer robot as keeper.,https://ieeexplore.ieee.org/document/9594013/,2021 International Electronics Symposium (IES),29-30 Sept. 2021,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICDL-EpiRob48136.2020.9278071,Bayesian Optimization for Developmental Robotics with Meta-Learning by Parameters Bounds Reduction,IEEE,Conferences,"In robotics, methods and softwares usually require optimizations of hyperparameters in order to be efficient for specific tasks, for instance industrial bin-picking from homogeneous heaps of different objects. We present a developmental framework based on long-term memory and reasoning modules (Bayesian Optimisation, visual similarity and parameters bounds reduction) allowing a robot to use meta-learning mechanism increasing the efficiency of such continuous and constrained parameters optimizations. The new optimization, viewed as a learning for the robot, can take advantage of past experiences (stored in the episodic and procedural memories) to shrink the search space by using reduced parameters bounds computed from the best optimizations realized by the robot with similar tasks of the new one (e.g. bin-picking from an homogenous heap of a similar object, based on visual similarity of objects stored in the semantic memory). As example, we have confronted the system to the constrained optimizations of 9 continuous hyperparameters for a professional software (Kamido) in industrial robotic arm bin-picking tasks, a step that is needed each time to handle correctly new object. We used a simulator to create bin-picking tasks for 8 different objects (7 in simulation and one with real setup, without and with meta-learning with experiences coming from other similar objects) achieving goods results despite a very small optimization budget, with a better performance reached when meta-learning is used (84.3 % vs 78.9 % of success overall, with a small budget of 30 iterations for each optimization) for every object tested (p-value=0.036).",https://ieeexplore.ieee.org/document/9278071/,2020 Joint IEEE 10th International Conference on Development and Learning and Epigenetic Robotics (ICDL-EpiRob),26-30 Oct. 2020,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/DSC.2016.48,Behavior Analysis Based SMS Spammer Detection in Mobile Communication Networks,IEEE,Conferences,"In a communication network, automatic short message service (SMS) spammer detection is a big challenge for a telecommunication operator nowadays, especially with the development of the rich communication services (RCS). Three main problems exist in the areas of research and real practice. They are (1) the whole-volume content based SMS spam detection techniques cannot be easily used on the side of network due to the issue of user privacy, (2) traditional ways to filter the spam according to the combination of key words and sending frequency can be easily bypassed by adding the interference words, (3) Most of them result in a great deal of manual review after the automatic filtering due to a low precision rate. To make up the aforementioned gaps, we study the user behavior characteristics. A two-dimensional visualized result indicates that any combination of two user behavior attributes cannot distinguish the abnormal users from the whole set by splitting the 2-dimensional space. Thus, the integration of multiple user behavior attributes is exploited to train the classifier in a labeled set by machine learning algorithms, respectively, including decision tree, random forest, supported vector machine (SVM), logistic regression, and self-organized feature mapping (SOM). The performance comparison indicates that random forest is a good choice to balance the tradeoff of the precision rate and the recall rate, and in an acceptable time. The experimental result shows the proposed method without the knowledge of SMS content has a significant improvement in terms of precision rate and recall rate compared with the traditional method using the combination of key words and sending frequency used in most of existing networks.",https://ieeexplore.ieee.org/document/7866183/,2016 IEEE First International Conference on Data Science in Cyberspace (DSC),13-16 June 2016,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CONECCT52877.2021.9622586,Benchmarking Transformer-Based Transcription on Embedded GPUs for Space Applications,IEEE,Conferences,"Speech transcription is a necessary tool for backend applications commonly found in voice assistants. Transcription is typically performed using cloud-based servers or custom hardware, but those resources are not always amenable to space environments due to size, weight, power, and cost constraints. Therefore, it is important to determine the performance of and optimal conditions for running transcription on hardware that is feasible for deployment in a space application. This research investigates and evaluates the performance of the wav2vec2 speech transcription engine, the current state-of-the-art model for this domain with and without optimizations. The target hardware, the NVIDIA Xavier NX Jetson embedded GPU, was chosen for its modern GPU architecture and small form factor. In addition to examining the input scaling behavior, we evaluate the hyperparameters of the clustered attention optimization, and average power and energy for inference relative to the operating power mode of the device. The clustered attention model outperformed the improved-clustered model for large input sizes, but the wav2vec2 model without clustering performed better for small input sizes. The clustered model energy per inference (13.90 J) was less than energy per inference of the improved-cluster model (15.03 J) and the vanilla softmax model (15.85 J). All models meet real-time speech processing requirements necessary to perform onboard inference entirely on a space system.",https://ieeexplore.ieee.org/document/9622586/,"2021 IEEE International Conference on Electronics, Computing and Communication Technologies (CONECCT)",9-11 July 2021,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/DEST.2007.372004,Benefits of Ontologies in Real Time Data Access,IEEE,Conferences,"Next generation Business Intelligence systems requires flexible and on-time access to enterprise's data. This paper highlights the importance of a data integration layer in a Business Intelligence system and the benefits that the use of an ontology as data description formalism and query interface, can bring to the system. In particular we focus on the aspects of data mapping and ontology enrichment, giving a general overview of the problem. We introduce also BT Data Foundry platform that has been implemented by Intelligent Systems Research Centre of BT Exact (British Telecom) as a method to provide the Business Intelligence system with a common data access layer. We provide also an outline of the solution to data mapping and ontology enrichment problems, based on semantic bridges and formal concept analysis (FCA). We use a normalization layer based on heuristic to reduce the terms space.",https://ieeexplore.ieee.org/document/4233738/,2007 Inaugural IEEE-IES Digital EcoSystems and Technologies Conference,21-23 Feb. 2007,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICARCV.2018.8581174,Bi-Manual Articulated Robot Teleoperation using an External RGB-D Range Sensor,IEEE,Conferences,"In this paper, we present an implementation of a bi-manual teleoperation system, controlled by a human through three-dimensional (3D) skeleton extraction. The input data is given from a cheap RGB-D range sensor, such as the ASUS Xtion PRO. To achieve this, we have implemented a 3D version of the impressive OpenPose package, which was recently developed. The first stage of our method contains the execution of the OpenPose Convolutional Neural Network (CNN), using a sequence of RGB images as input. The extracted human skeleton pose localisation in two-dimensions (2D) is followed by the mapping of the extracted joint location estimations into their 3D pose in the camera frame. The output of this process is then used as input to drive the end-pose of the robotic hands relative to the human hand movements, through a whole-body inverse kinematics process in the Cartesian space. Finally, we implement the method as a ROS wrapper package and we test it on the centaur-like CENTAURO robot. Our demonstrated task is of a box and lever manipulation in real-time, as a result of a human task demonstration.",https://ieeexplore.ieee.org/document/8581174/,"2018 15th International Conference on Control, Automation, Robotics and Vision (ICARCV)",18-21 Nov. 2018,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CVPR46437.2021.00937,Binary Graph Neural Networks,IEEE,Conferences,"Graph Neural Networks (GNNs) have emerged as a powerful and flexible framework for representation learning on irregular data. As they generalize the operations of classical CNNs on grids to arbitrary topologies, GNNs also bring much of the implementation challenges of their Euclidean counterparts. Model size, memory footprint, and energy consumption are common concerns for many real-world applications. Network binarization allocates a single bit to parameters and activations, thus dramatically reducing the memory requirements (up to 32x compared to single-precision floating-point numbers) and maximizing the benefits of fast SIMD instructions on modern hardware for measurable speedups. However, in spite of the large body of work on binarization for classical CNNs, this area remains largely unexplored in geometric deep learning. In this paper, we present and evaluate different strategies for the binarization of graph neural networks. We show that through careful design of the models, and control of the training process, binary graph neural networks can be trained at only a moderate cost in accuracy on challenging benchmarks. In particular, we present the first dynamic graph neural network in Hamming space, able to leverage efficient k-NN search on binary vectors to speed-up the construction of the dynamic graph. We further verify that the binary models offer significant savings on embedded devices. Our code is publicly available on Github1.",https://ieeexplore.ieee.org/document/9578443/,2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),20-25 June 2021,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICSIDP47821.2019.9173414,Binary Separable Convolutional: An Efficient Fast Image Classification Method,IEEE,Conferences,"The training and running of neural network require large computational space and memory space, which makes it difficult to deploy on a resource-constrained embedded sys-tems. To address this limitation, we introduce a two-stage pipeline: Depth separable, local binary. Our method is divided into two steps. Firstly, a deep separable convolution is applied to each input channel. Then, point-by-point convolution is applied to the feature map obtained by the filter. In the second step, we use local binarization method to initialize the filter corresponding to the input channel into sparse binary. In network training, the sparse binary filter remains fixed and only needs to train convolution of 1×1 size. Our experimental results show that, on the basis of approximate accuracy with the original network, we have reduced the number of convolution parameters by 9x to 10x, and reduced the training time and testing time to by 2x. Our compression method helps to deploy complex neural networks on resource-constrained embedded platform.",https://ieeexplore.ieee.org/document/9173414/,"2019 IEEE International Conference on Signal, Information and Data Processing (ICSIDP)",11-13 Dec. 2019,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IDAACS53288.2021.9660834,Biometric Identification via Oculomotor System Based on the Volterra Model,IEEE,Conferences,"In recent years, there has been an increase in interest in biometrics research involving the use of brain characteristics commonly known as behavioral traits. Human eyes contain a rich source of idiosyncratic information which may be used for the recognition of an individual's identity. This article implements an innovative experiment and a new approach to processing human eye movements, ultimately aimed at biometric identification of individuals. In our experiment, the subjects observe special test visual stimuli, which are generated on the computer monitor screen. The eye movements are tracked in dynamics providing information for constructing a nonparametric nonlinear dynamic model (Volterra model) of a human's oculomotor system (OMS) in the form of multivariate transient functions. The implemented method treats eye trajectories as 2-D distributions of points on the “Coordinate-Time” plane. The efficiency of dynamic characteristics for personality identification is confirmed by examples of models built on the basis of data from real experiments. The resulting OMS models are a source of information for the selection of informative features, in the space of which the decisive rule of optimal identification of individuals is determined using machine learning methods. Promising results at the task of identification according to behavioral characteristics of an individual have been obtained - recognition accuracy is higher than 97%.",https://ieeexplore.ieee.org/document/9660834/,2021 11th IEEE International Conference on Intelligent Data Acquisition and Advanced Computing Systems: Technology and Applications (IDAACS),22-25 Sept. 2021,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/KESE.2009.47,Bleeding Simulation Based Particle System for Surgical Simulator,IEEE,Conferences,"Bleeding simulation is important components of a surgical simulator. However, realistic simulation of bleeding is a challenging problem. There are several fluid flow models described in the literature, but they are computationally slow and do not satisfy real-time requirement of a surgical simulator. The paper introduce a bleeding simulation algorithm based on the particle system and implement arterial squirting blood, blood splashes in free space, blood drops running down a surface. This algorithm calculates how many blood particles are needed to model a particular instance of bleeding, and assigns a velocity to each particle in a gravitational field so that particles flow out of the injured region at the proper rate. The key characteristics of our algorithm are a visually realistic display and real-time computational performance. Experiment results show that proposed method is feasible and effective. In addition, this algorithm can also be used to simulate water irrigation and smoke effect in virtual surgery simulator.",https://ieeexplore.ieee.org/document/5383596/,2009 Pacific-Asia Conference on Knowledge Engineering and Software Engineering,19-20 Dec. 2009,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IJCNN52387.2021.9534340,Born-Again Decision Boundary: Unsupervised Concept Drift Detection by Inspector Neural Network,IEEE,Conferences,"In the actual operation of machine learning models, differences between training and operational data distributions are a problem referred to as concept drifts. Most existing detection approaches assume that labels can be made readily available, but it is unrealistic to acquire all labels instantaneously. As concept drift is often unpredictable, the deployed machine learning model is not always amenable to concept drift detection. In this paper, we propose the Born-Again Decision Boundary algorithm, which is a novel unsupervised concept drift detection method for inspecting a deployed black box model without labels. The proposed method builds an inspector model that recreates the decision boundaries from the deployed model using black-box rule extraction techniques. This model calculates the distance from the decision boundary to the data point and monitors a region of uncertainty in the input space. Our method removes the dependency on machine learning algorithms, which has hindered the existing methods in practical applications and can be widely applied regardless of the classification algorithm. Experimental results on synthetic and real-world data show that the proposed method is capable of detecting concept drift on unlabeled data.",https://ieeexplore.ieee.org/document/9534340/,2021 International Joint Conference on Neural Networks (IJCNN),18-22 July 2021,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICNNB.2005.1614779,Bp Neural Network Implementation On Real-time Reconfigurable FPGA System For A Soft-sensing Process,IEEE,Conferences,"In this paper, the algorithm and structure of BP NN (Backpropagation neural network) and its training process used for a soft-sensing process are described and its implementation on real-time reconfigurable FPGA (Field Programmable Field Array) system is introduced. The whole system is totally controlled by a microprocessor chip which can completely manage to in system reconfigure FPGA between the two models: BP neural network model and its training model. That is, only one single FPGA is configured with multifunction. This technique can be widely applied into the field such as measurement of human-body internal state, space traffic equipment, deep-sea exploration, where small size of measurement equipment is required and only one microcomputer system used for multifunction is allowed to be installed.",https://ieeexplore.ieee.org/document/1614779/,2005 International Conference on Neural Networks and Brain,13-15 Oct. 2005,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AIVR50618.2020.00052,Breast3D: An Augmented Reality System for Breast CT and MRI,IEEE,Conferences,"Adoption of Virtual Reality (VR), Augmented Reality (AR) and Mixed Reality (MR) - known collectively as Extended Reality (XR) devices has been rapidly increasing over recent years. However, the focus of XR research has shown a lack of diversity in solutions to the problems within medicine, with it being predominantly focused in augmenting surgical procedures. Whilst important, XR applied to aiding medical diagnosis and surgical planning is relatively unexplored. In this paper we present a fully functional mammographic image analysis system, Breast3D, that can reconstruct MRI and CT scan data in XR. With breast cancer Breast Imaging-Reporting and Data System (BI-RADS) risk lexicon, early detection and clinical workflow such as Multi-disciplinary team (MDT) meetings for cancer in mind, our new mammography visualization system reconstructs CT and MRI volumes in a real 3D space. Breast3D is built upon the past literature and inspired from research for diagnosis and surgical planning. In addition to visualising the models in MR using the Microsoft HoloLens, Breast3D is versatile and portable to different XR head-mounted displays such as HTC Vive. Breast3D demonstrates the early potential for XR within diagnostics of 3D mammographic modalities, an application that has been proposed but until now has not been implemented.",https://ieeexplore.ieee.org/document/9319058/,2020 IEEE International Conference on Artificial Intelligence and Virtual Reality (AIVR),14-18 Dec. 2020,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SiPS.2015.7345005,Bridge deep learning to the physical world: An efficient method to quantize network,IEEE,Conferences,"As better performance is achieved by deep convolutional network with more and more layers, the increasing number of weighting and bias parameters makes it only possible to be implemented on servers in cyber space but infeasible to be deployed in physical-world embedded systems because of huge storage and memory bandwidth requirements. In this paper, we proposed an efficient method to quantize the model parameters. Instead of taking the quantization process as a negative effect on precision, we regarded it as a regularize problem to prevent overfitting, and a two-stage quantization technique including soft- and hard-quantization is developed. With the help of our quantization method, not only 93.75% of the parameter memory size can be reduced by replacing the word length from 32-bit to 2-bit, but the testing accuracy after quantization is also better than previous approaches in some dataset, and the additional training overhead is only 3% of the ordinary one.",https://ieeexplore.ieee.org/document/7345005/,2015 IEEE Workshop on Signal Processing Systems (SiPS),14-16 Oct. 2015,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISNCC.2018.8530988,Building an Intelligent and Efficient Smart Space to Detect Human Behavior in Common Areas,IEEE,Conferences,"Smart spaces have become an integral part of our daily routines to improve quality of life for many different groups of people. The use of embedded systems to build these smart spaces, in combination with data analytics, can provide real-time information about the environment and how it interacts with the people in it. In this paper, we demonstrate how one embedded system that acquires data based on a 2-dimensional positional-grid, movement, temperature and vibration is used to build a smart and pervasive space. Data collected from these sensors is used for real time localization in conjunction with machine learning mechanisms to analyze human activities. We evaluate five machine learning algorithms, namely Logistic Regression, Support Vector Machine, Decision Tree, Random Forest, Naive Bayes and Artificial Neural Network applied on a dataset collected in our lab. Results show high classification performance for all methods giving up-to 99.95% classification accuracy. These patterns provide useful information about occupancy patterns, movement patterns, etc., which will be later used to allocate computational resources in the smart space accordingly. Furthermore, our implementation does not use any camera or microphone deployment, hence addressing potential privacy issues.",https://ieeexplore.ieee.org/document/8530988/,"2018 International Symposium on Networks, Computers and Communications (ISNCC)",19-21 June 2018,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IISWC53511.2021.00026,Cactus: Top-Down GPU-Compute Benchmarking using Real-Life Applications,IEEE,Conferences,"Benchmarking is the de facto standard for evaluating hardware architectures in academia and industry. While several benchmark suites targeting different application domains have been developed for CPU processors over many decades, benchmarking GPU architectures is not as mature. Since the introduction of GPUs for general-purpose computing, the purpose has been to accelerate (a) specific part(s) of the code, called (a) kernel(s). The initial GPU-compute benchmark suites, which are still widely used today, hence consist of relatively simple workloads that are composed of one or few kernels with specific unambiguous execution characteristics. In contrast, we find that modern-day real-life GPU-compute applications are much more complex consisting of many more kernels with differing characteristics. A fundamental question can hence be raised: are current benchmark suites still representative for modern real-life applications? In this paper, we introduce Cactus, a collection of widely used real-life open-source GPU-compute applications. The aim of this work is to offer a new perspective on GPU-compute benchmarking: while existing benchmark suites are designed in a bottom-up fashion (i.e., starting from kernels that are likely to perform well on GPUs), we perform GPU-compute benchmarking in a top-down fashion, starting from complex real-life applications that are composed of multiple kernels. We characterize the Cactus benchmarks by quantifying their kernel execution time distribution, by analyzing the workloads using the roofline model, by performing a performance metrics correlation analysis, and by classifying their constituent kernels through multi-dimensional data analysis. The overall conclusion is that the Cactus workloads execute many more kernels, include more diverse and more complex execution behavior, and cover a broader range of the workload space compared to the prevalently used benchmark suites. We hence believe that Cactus is a useful complement to the existing GPU-compute benchmarking toolbox.",https://ieeexplore.ieee.org/document/9668300/,2021 IEEE International Symposium on Workload Characterization (IISWC),7-9 Nov. 2021,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/MLSP52302.2021.9596414,Caesynth: Real-Time Timbre Interpolation and Pitch Control with Conditional Autoencoders,IEEE,Conferences,"In this paper, we present a novel audio synthesizer, CAESynth, based on a conditional autoencoder. CAESynth synthesizes timbre in real-time by interpolating the reference sounds in their shared latent feature space, while controlling a pitch independently. We show that training a conditional autoen-coder based on accuracy in timbre classification together with adversarial regularization of pitch content allows timbre distribution in latent space to be more effective and stable for timbre interpolation and pitch conditioning. The proposed method is applicable not only to creation of musical cues but also to exploration of audio affordance in mixed reality based on novel timbre mixtures with environmental sounds. We demonstrate by experiments that CAESynth achieves smooth and high-fidelity audio synthesis in real-time through timbre interpolation and independent yet accurate pitch control for musical cues as well as for audio affordance with environmental sound. A Python implementation along with some generated samples are shared online.",https://ieeexplore.ieee.org/document/9596414/,2021 IEEE 31st International Workshop on Machine Learning for Signal Processing (MLSP),25-28 Oct. 2021,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCE-Berlin50680.2020.9352201,Camera-LIDAR Object Detection and Distance Estimation with Application in Collision Avoidance System,IEEE,Conferences,"Nowadays we are aware of accelerated development of automotive software. Numerous of ADAS (Advanced Driver Assistance Systems) systems are being developed these days. One such system is the forward CAS (Collision Avoidance System). In order to implement such a system, this paper presents one solution for detecting an object located directly in front of the vehicle and estimating its distance. The solution is based on the use of camera and LIDAR (Light Detection and Ranging) sensor fusion. The camera was used for object detection and classification, while 3D data obtained from LIDAR sensor were used for distance estimation. In order to map the 3D data from the LIDAR to the 2D image space, a spatial calibration was used. The solution was developed as a prototype using the ROS (Robot Operating System) based Autoware open source platform. This platform is essentially a framework intended for the development and testing of automotive software. ROS as the framework on which the Autoware platform is based, provides a library for the Python and C++ programming languages, intended for creating new applications. For the reason that this is a prototype project, and it is popular for application in machine learning, we decided to use the Python programming language. The solution was tested inside the CARLA simulator, where the estimation of the obstacle distance obtained at the output of our algorithm was compared with the ground truth values obtained from the simulator itself. Measurements were performed under different weather conditions, where this algorithm showed satisfactory results, with real-time processing.",https://ieeexplore.ieee.org/document/9352201/,2020 IEEE 10th International Conference on Consumer Electronics (ICCE-Berlin),9-11 Nov. 2020,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/INFOCOM42981.2021.9488865,Can You Fix My Neural Network? Real-Time Adaptive Waveform Synthesis for Resilient Wireless Signal Classification,IEEE,Conferences,"Due to the sheer scale of the Internet of Things (IoT) and 5G, the wireless spectrum is becoming severely congested. For this reason, wireless devices will need to continuously adapt to current spectrum conditions by changing their communication parameters in real-time. Therefore, wireless signal classification (WSC) will become a compelling necessity to decode fast-changing signals from dynamic transmitters. Thanks to its capability of classifying complex phenomena without explicit mathematical modeling, deep learning (DL) has been demonstrated to be a key enabler of WSC. Although DL can achieve a very high accuracy under certain conditions, recent research has unveiled that the wireless channel can disrupt the features learned by the DL model during training, thus drastically reducing the classification performance in real-world live settings. Since retraining classifiers is cumbersome after deployment, existing work has leveraged the usage of carefully-tailored Finite Impulse Response (FIR) filters that, when applied at the transmitter&#x2019;s side, can restore the features that are lost because of the the channel actions, i.e., waveform synthesis. However, these approaches compute FIRs using offline optimization strategies, which limits their efficacy in highly-dynamic channel settings. In this paper, we improve the state of the art by proposing Chares, a Deep Reinforcement Learning (DRL)-based framework for channel-resilient adaptive waveform synthesis. Chares adapts to new and unseen channel conditions by optimally computing through DRL the FIRs in real time. Chares is a DRL agent whose architecture is based upon the Twin Delayed Deep Deterministic Policy Gradients (TD3), which requires minimal feedback from the receiver and explores a continuous action space for best performance. Chares has been extensively evaluated on two well-known datasets with an extensive number of channels. We have also evaluated the real-time latency of Chares with an implementation on field-programmable gate array (FPGA). Results show that Chares increases the accuracy up to 4.1x when no waveform synthesis is performed, by 1.9x with respect to existing work, and can compute new actions within 41&#x00B5;s.",https://ieeexplore.ieee.org/document/9488865/,IEEE INFOCOM 2021 - IEEE Conference on Computer Communications,10-13 May 2021,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CAIA.1990.89198,Case study of a knowledge-based system which plans molecular genetic experiments,IEEE,Conferences,"A logic-based approach to planning recombinant DNA experiments is presented. Plans are constructed by attempting to match lists of constraints with existing molecules, with failure resulting in rule-directed decomposition of these constraints to create subproblems. Several AI techniques which are not traditionally associated with planning but which find application for this problem domain are used. These include an object-oriented paradigm to help reduce the size of the search space and to afford an approach to efficient machine learning and the use of simulation to solve aspects of the frame problem and other real-world difficulties idiosyncratic to molecular genetics. The resulting system is capable of solving tasks which could not be addressed by previous attempts at planning in this problem domain.<>",https://ieeexplore.ieee.org/document/89198/,Sixth Conference on Artificial Intelligence for Applications,5-9 May 1990,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/URS.2007.371826,Cellular automata urban growth model calibration with genetic algorithms,IEEE,Conferences,"Last few decades witness a dramatic increase in city population worldwide associated with excessive urbanization rates. This raises the necessity to understand the dynamics of urban growth process for sustainable distribution of available resources. Cellular automata, an artificial intelligence technique composed of pixels, states, neighborhood and transition rules, is being widely implemented to model the urban growth process due to its ability to fit such complex spatial nature using simple and effective rules. The main objective of our work is to use genetic algorithms to effectively calibrate, i.e., identify transition rule values, a cellular automata urban growth model that is designed as a function of multitemporal satellite imagery and population density. Transition rules in our model identify the required neighborhood urbanization level for a test pixel to develop. Calibration is performed spatially to find best rule values per township. Genetic algorithms calibration model, through proper design of their parameters, including objective function, initial population, selection, crossover and mutation, is prepared to fit the cellular automata model. Genetic algorithms start processing the initial solution space, through sequential implementation of the parameters, to identify the best rule values using a predefined criterion over the maximum number of iterations. Minimum objective function, representing the total modeling errors, is used to identify the optimal rule values. Each rule set is evaluated in term of urban level and pattern match with reality. Calibration with genetic algorithms proves to be effective in producing the optimal rule values in a time effective manner at an early generation. Proposed calibration algorithm is implemented to model the historical urban growth of Indianapolis-IN, USA. Urban growth results show a close match for both urban count and pattern with reality.",https://ieeexplore.ieee.org/document/4234425/,2007 Urban Remote Sensing Joint Event,11-13 April 2007,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ROBOT.1993.292250,Cellular robotics: simulation and HW implementation,IEEE,Conferences,"Aspects of self-organization are presented in this paper. Computer simulations as well as a real prototypical implementation are used to illustrate the proposed approach. Results of simulations are presented to compare different strategies of self-organization enabling a system of autonomous robots to form a chain between two landmarks in a completely unknown environment. This chain implicitly represents a path between any two points of the environment without an explicit representation of free space (no single robot has a global map of the environment). The experimental part, even if restricted to a few robots, demonstrates that the set of stimuli-action processes used in the simulations are indeed feasible on real systems.<>",https://ieeexplore.ieee.org/document/292250/,[1993] Proceedings IEEE International Conference on Robotics and Automation,2-6 May 1993,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ENC.2005.14,Cervical cancer detection using colposcopic images: a temporal approach,IEEE,Conferences,"In the present work, we propose a methodology analysis of the colposcopic images to help the expert to make a more robust diagnosis of precursor lesions of cervical cancer. Although some others approaches have been used to assess cervical lesion, a complete methodology to evaluate temporal changes of tissue color is still missing. The different processes involved in the analysis are described. The image registration was implemented using the phase correlation method followed by a locally applied algorithm based on the normalized cross-correlation. During the parameterization process, each time series obtained from the image sequences was represented as a parabola in a parameter space. A supervised Bayesian learning approach is proposed to classify the features in the parameter space according to the classification made by the colposcopist. Then those labels are used as a criterion to categorize the tissue and perform the image segmentation. Some preliminary results are shown using unsupervised learning with real data.",https://ieeexplore.ieee.org/document/1592214/,Sixth Mexican International Conference on Computer Science (ENC'05),26-30 Sept. 2005,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/C2I454156.2021.9689446,Challenges in Design and Development of Electron Beam Weld Predictor Software,IEEE,Conferences,"Electron beam welding (EBW) is high power density welding process used predominantly for fabrication of launch vehicles and spacecrafts made of aerospace grade materials. EBW process at LPSC-ISRO is a space qualified process and the cost and time involved in imbibing quality into the EBW process amounts to 65% of the total fabrication costs and 80 % of the total welding time. The weld quality is ensured by evaluating a pre-weld WTS (Weld Test Specimen), each time a weld is done.. The LPSC-ISRO-Bangalore, houses many variants of EBW machines and in order to tap, utilise the huge potential of empirical data available in-house, this software has been designed and developed to predict the EBW machine parameters for a given material and geometry details. The software developed is based on machine learning of the empirical data. The accuracy of prediction results have been validated using EBW experiments and presently the first version of the software is available for deployment in ISRO and continual improvement of prediction accuracies is ensured by enhancing the database with additional reference points. The software aids in reduction of total manufacturing cost and lead time as the WTS (Weld Test Specimen) requirements have been substantially reduced with aid of this software. Unlike software development for electrical and electronic systems, the challenges in design and development of software for mechanical systems (EBW machines in this design) commences with limited data availability in assessable form; data collection and assimilation are humongous as it is based on almost 20 nos of parameters related to EBW machine and weld quality and not all parameters are in measurable or assessable format. Despite this limitation, the software has been developed in two phases including feature extraction studies in phase2 thus enhancing the prediction accuracies achieved in phase1(a primitive and draft software design). Also the other challenges are input and output parameters for the software are in analog format &amp; knowledge based (interpretation needs skill); the empirical data is predominantly dependent on physics of EBW such as geometry, material properties and software accuracy to be validated by real-time physical EBW experiments. The challenges have been overcome by ensuring precision in empirical data collection, assimilation and cleansing and also feature extraction studies indicated the relevance in selection of input parameters. The selection of machine learning algorithm is also based on the applicability of the selected algorithm to address all the above challenges endured. Also the experimental results validated the prediction accuracies achieved via the Phase2 developed software. Despite the challenges endured, the indigenously designed, developed and validated EBW predictor software can predict EBW machine parameters (beam current and focus current) without any need to weld a WTS and hence, the software developed aids in reduction of the total manufacturing cost and also the lead time involved in realisation of launch vehicles and spacecrafts for various ISRO missions have been substantially reduced with induction of this EBW Predictor software.",https://ieeexplore.ieee.org/document/9689446/,"2021 2nd International Conference on Communication, Computing and Industry 4.0 (C2I4)",16-17 Dec. 2021,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IEMBS.2007.4353509,Channel and Feature Selection in Multifunction Myoelectric Control,IEEE,Conferences,"Real time controlling devices based on myoelectric singles (MES) is one of the challenging research problems. This paper presents a new approach to reduce the computational cost of real time systems driven by myoelectric signals (MES) (a.k.a Electromyography-EMG). The new approach evaluates the significance of feature/channel selection on MES pattern recognition. Particle Swarm Optimization (PSO), an evolutionary computational technique, is employed to search the feature/channel space for important subsets. These important subsets will be evaluated using a multilayer perceptron trained with back propagation neural network (BPNN). Practical results acquired from tests done on six subject's datasets of MES signals measured in a noninvasive manner using surface electrodes are presented. It is proved that minimum error rates can be achieved by considering the correct combination of features/channels, thus providing a feasible system for practical implementation purpose for rehabilitation of patients.",https://ieeexplore.ieee.org/document/4353509/,2007 29th Annual International Conference of the IEEE Engineering in Medicine and Biology Society,22-26 Aug. 2007,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICNP.2016.7784407,Characterizing industrial control system devices on the Internet,IEEE,Conferences,"Industrial control system (ICS) devices with IP addresses are accessible on the Internet and play a crucial role for critical infrastructures like power grid. However, there is a lack of deep understanding of these devices' characteristics in the cyberspace. In this paper, we take a first step in this direction by investigating these accessible industrial devices on the Internet. Because of critical nature of industrial control systems, the detection of online ICS devices should be done in a real-time and non-intrusive manner. Thus, we first analyze 17 industrial protocols widely used in industrial control systems, and train a probability model through the learning algorithm to improve detection accuracy. Then, we discover online ICS devices in the IPv4 space while reducing the noise of industrial honeypots. To observe the dynamics of ICS devices in a relatively long run, we have deployed our discovery system on Amazon EC2 and detected online ICS devices in the whole IPv4 space for eight times from August 2015 to March 2016. Based on the ICS device data collection, we conduct a comprehensive data analysis to characterize the usage of ICS devices, especially in the answer to the following three questions: (1) what are the distribution features of ICS devices, (2) who use these ICS devices, and (3) what are the functions of these ICS devices.",https://ieeexplore.ieee.org/document/7784407/,2016 IEEE 24th International Conference on Network Protocols (ICNP),8-11 Nov. 2016,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCSS53909.2021.9721962,Chebyshev Polynomial Broad Learning System,IEEE,Conferences,"The broad learning system (BLS) has been attracting more and more attention due to its excellent property in the field of machine learning. A great deal of variants and hybrid structures of BLS have also been designed and developed for better performance in some specialized tasks. In this paper, the Chebyshev polynomials are introduced into the BLS to take advantage of their powerful approximation capability, where the feature windows are replaced by a set of Chebyshev polynomials. This new variant, named Chebyshev polynomial BLS (CPBLS), has a light structure with a reduction in computational complexity since the sparse autoencoder is removed. Instead, the dimension of each input sample is expended by n + 1 Chebyshev polynomials, mapping the original feature into a new feature space with higher dimension, which helps to classify the patterns in training. The proposed CPBLS is evaluated by some popular datasets from UCI and KEEL repositories, and it outperforms some representative neural networks and neuro-fuzzy models in terms of classification accuracy. The CPBLS also show some advantages over the recent developed compact fuzzy BLS (CFBLS) which indicates its great potential in future research and real-world applications.",https://ieeexplore.ieee.org/document/9721962/,"2021 8th International Conference on Information, Cybernetics, and Computational Social Systems (ICCSS)",10-12 Dec. 2021,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CNNA.1998.685324,Classification systems based on neural networks,IEEE,Conferences,"Classification is a problem that appears in many real life applications. We describe the general case of multi-class classification, where the task of the classification system is to map an input vector x to one of K>2 given classes. This problem is split in many two-class classification problems, each of them describing a part of the whole problem. These are solved by neural networks, producing an intermediate output in a reference space, which is then decoded to the solution of the original problem. The methods described here are then applied to the handwritten character recognition problem to produce the results described later in the article. It is suspected that they also may be applied successfully in the context of the CNN paradigm and be implemented on a CNN-Universal Machine.",https://ieeexplore.ieee.org/document/685324/,1998 Fifth IEEE International Workshop on Cellular Neural Networks and their Applications. Proceedings (Cat. No.98TH8359),14-17 April 1998,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/INES.2013.6632823,Cloud security monitoring and vulnerability management,IEEE,Conferences,"Cloud infrastructure becomes the primary business environment for all types of enterprises during recent years. In cloud computing security is a fundamental concern, loss of control and potential lack of trust prevent large set of potential customers to immerse in the cloud world. One of the major key problem is how one can test, monitor or measure the underlying Cloud infrastructure from user/customer space. We have developed a solution which is able to examine the infrastructure, from security point-of-views. We offer a clear, adaptable, concise and easy-to-extend framework to assess the underlying cloud infrastructure. Our developed solution is generic and multipurpose it can act as a vulnerability scanner, and performance benchmarking tool at the same time. It is virtualized, it is agent based and collects assessment information by the decentralized Security Monitor and it archives the results received from the components and visualize them via a web interface for the tester/administrators. In this paper we present our virtualized cloud security monitor and assessment solution, we describe its functionalities and provide some examples of its results captured in real systems.",https://ieeexplore.ieee.org/document/6632823/,2013 IEEE 17th International Conference on Intelligent Engineering Systems (INES),19-21 June 2013,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/EMBC.2018.8513597,Clustering Based Kernel Reinforcement Learning for Neural Adaptation in Brain-Machine Interfaces,IEEE,Conferences,"Reinforcement learning (RL) interprets subject's movement intention in Brain Machine Interfaces (BMIs) through trial-and-error with the advantage that it does not need the real limb movements. When the subjects try to control the external devices purely using brain signals without actual movements (brain control), they adjust the neural firing patterns to adapt to device control, which expands the state-action space for the RL decoder to explore. The challenge is to quickly explore the new knowledge in the sizeable state-action space and maintain good performance. Recently quantized attention-gated kernel reinforcement learning (QAGKRL) was proposed to quickly explore the global optimum in Reproducing Kernel Hilbert Space (RKHS). However, its network size will grow large when the new input comes, which makes it computationally inefficient. In addition, the output is generated using the whole input structure without being sensitive to the new knowledge. In this paper, we propose a new kernel based reinforcement learning algorithm that utilizes the clustering technique in the input domain. The similar neural inputs are grouped, and a new input only activates its nearest cluster, which either utilizes an existing sub-network or forms a new one. In this way, we can build the sub-feature space instead of the global mapping to calculate the output, which transfers the old knowledge effectively and also consequently reduces the computational complexity. To evaluate our algorithm, we test on the synthetic spike data, where the subject's task mode switches between manual control and brain control. Compared with QAGKRL, the simulation results show that our algorithm can achieve a faster learning curve, less computational time, and more accuracy. This indicates our algorithm to be a promising method for the online implementation of BMIs.",https://ieeexplore.ieee.org/document/8513597/,2018 40th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC),18-21 July 2018,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ASP-DAC47756.2020.9045595,Co-Exploring Neural Architecture and Network-on-Chip Design for Real-Time Artificial Intelligence,IEEE,Conferences,"Hardware-aware Neural Architecture Search (NAS), which automatically finds an architecture that works best on a given hardware design, has prevailed in response to the ever-growing demand for real-time Artificial Intelligence (AI). However, in many situations, the underlying hardware is not pre-determined. We argue that simply assuming an arbitrary yet fixed hardware design will lead to inferior solutions, and it is best to co-explore neural architecture space and hardware design space for the best pair of neural architecture and hardware design. To demonstrate this, we employ Network-on-Chip (NoC) as the infrastructure and propose a novel framework, namely NANDS, to co-explore NAS space and NoC Design Search (NDS) space with the objective to maximize accuracy and throughput. Since two metrics are tightly coupled, we develop a multi-phase manager to guide NANDS to gradually converge to solutions with the best accuracy-throughput tradeoff. On top of it, we propose techniques to detect and alleviate timing performance bottleneck, which allows better and more efficient exploration of NDS space. Experimental results on common datasets, CIFAR10, CIFAR-100 and STL-10, show that compared with state-of-the-art hardware-aware NAS, NANDS can achieve 42.99% higher throughput along with 1.58% accuracy improvement. There are cases where hardware-aware NAS cannot find any feasible solutions while NANDS can.",https://ieeexplore.ieee.org/document/9045595/,2020 25th Asia and South Pacific Design Automation Conference (ASP-DAC),13-16 Jan. 2020,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/C5.2005.8,Co-creation system and human-computer interaction,IEEE,Conferences,"The purpose of this research is to realize a ""co-creation system"". Co-creation means co-emergence of real-time coordination by sharing subjective space and time between different persons. Human communication with emergent reality like this is essential to improve communicability in social systems, and we assume such communication needs two kinds of information process at the same time. One is explicit communication such as the transmission of messages and the other is implicit embodied interaction such as the sympathy and direct experience. The conventional IT system mainly covers the former process but we have been pointing out the importance of the latter process. Especially, this implicit process is related with rhythmic interaction between humans, such as entrainment of body motion. From this background, using these implicit and explicit processing complementarily, we are developing co-creative man-machine interfaces and communication media. We think this dual-processing based new technology will be effective for recovering human linkage and mutual-reliability that has been weakened in modern IT society.",https://ieeexplore.ieee.org/document/1419802/,"Third International Conference on Creating, Connecting and Collaborating through Computing (C5'05)",28-29 Jan. 2005,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ECC.2014.6862630,Co-design of hardware and algorithms for real-time optimization,IEEE,Conferences,"It is difficult or impossible to separate the performance of an optimization solver from the architecture of the computing system on which the algorithm is implemented. This is particularly true if measurements from a physical system are used to update and solve a sequence of mathematical optimization problems in real-time, such as in control, automation, signal processing and machine learning. In these real-time optimization applications the designer has to trade off computing time, space and energy against each other, while satisfying constraints on the performance and robustness of the resulting cyber-physical system. This paper is an informal introduction to the issues involved when designing the computing hardware and a real-time optimization algorithm at the same time, which can result in systems with efficiencies and performances that are unachievable when designing the sub-systems independently. The co-design process can, in principle, be formulated as a sequence of uncertain and non-smooth optimization problems. In other words, optimizers might be used to design optimizers. Before this can become a reality, new systems theory and numerical methods will have to be developed to solve these co-design problems effectively and reliably.",https://ieeexplore.ieee.org/document/6862630/,2014 European Control Conference (ECC),24-27 June 2014,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICECCME52200.2021.9591113,Cobots for FinTech,IEEE,Conferences,"Embedded devices enabling payments transaction processing in Financial Services industry cannot have any margin for error. These devices need to be tested & validated by replicating production like environment to the extent possible. This means literally handling payments related events like swiping a credit card, tapping a mobile phone or pressing buttons amongst many other things like in real world. Embedded Software development is time consuming as it involves multiple man-machine interactions and dependencies such as managing and handling embedded devices, operating devices (Push buttons, interpret display panels, read receipt printouts etc.) and sharing devices for collaboration within team. During the current pandemic, it was impossible for software teams to travel to office, share devices or even procure necessary devices on time for project related tasks. This caused delay to project delivery and increased Time to market. The paper describes how the team used Capgemini's flexible Robotics as a Service (RaaS) platform that helped during pandemic to automate feasible man-machine interactions using Robotic arms. The paper provides details of the work done by the team that involves internet of things (IoT), Artificial Intelligence (AI) to remotely handle and operate hardware and devices thereby completing embedded software development life cycles faster and well within budget while ensuring superior product quality and importantly ensuring team's health and safety. This is novel in Financial Services space.",https://ieeexplore.ieee.org/document/9591113/,"2021 International Conference on Electrical, Computer, Communications and Mechatronics Engineering (ICECCME)",7-8 Oct. 2021,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ITSC.2017.8317627,Cognitive map-based model: Toward a developmental framework for self-driving cars,IEEE,Conferences,"End-to-end learning and multi-sensor fusion-based methods are two major frameworks used for self-driving cars. To enable these intelligence vehicles to acquire driving skills at a level comparable to that of human drivers, long short-term memory of previous self-driving processes is necessary, but is difficult to introduce into the above-mentioned frameworks. In this paper, we propose a model for self-driving cars called the cognitive map-based neural network (CMNN). Our framework consists of three parts: a convolutional neural network that can perceive the environment in the manner that the human visual cortex does, a cognitive map to describe the locations of objects in a complex traffic scene and the relationships among them, and a recurrent neural network to process long short-term memory from the cognitive map, which is updated in real time. The proposed model is built to simultaneously handle three tasks: i) detecting free space and lane boundaries, ii) estimating vehicle pose and obstacle distance, and iii) learning to plan and control based on the behaviors of a human driver. More significantly, our approach introduces external instructions during an end-to-end driving process. To test it, we created a large-scale road vehicle dataset (RVD) containing more than 50,000 labeled road images captured by three cameras. We implemented the proposed model on an embedded system.",https://ieeexplore.ieee.org/document/8317627/,2017 IEEE 20th International Conference on Intelligent Transportation Systems (ITSC),16-19 Oct. 2017,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICECE48499.2019.9058541,Collaborative Filtering Based on Factorization and Distance Metric Learning,IEEE,Conferences,"Collaborative filtering predicts user preferences by modeling the user's historical behavior. Matrix factorization plays an important role in collaborative filtering. Matrix factorization use dot product to predicts rating. Nevertheless, the dot product does not satisfy the triangle inequality, which may limit their expressiveness and lead to sub-optimal solutions. This paper combines metric learning and distance factorization. Firstly, convert the scoring matrix to a distance matrix. Then the distance matrix is factorization by metric learning to obtain the position of users and items in low-dimensional Euclidean space. User preferences and distance are negatively correlated, the user is closer to the favorite item and farther away from the unloved item, that is, the user's preference is reflected by the distance. Experiments conducted on real-world data sets have shown that compared with classical algorithm, our method significantly improves the accuracy of recommendations.",https://ieeexplore.ieee.org/document/9058541/,2019 IEEE 2nd International Conference on Electronics and Communication Engineering (ICECE),9-11 Dec. 2019,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IRI-05.2005.1506442,Collaborative knowledge management by integrating knowledge modeling and workflow modeling,IEEE,Conferences,"Recently both industrial and academic researches show great interest in knowledge management. However, users still find it hard to obtain a suitable knowledge management tool that fits for their needs. Although business requirements always change, current software systems still cannot be adapted to these changes quickly. In this paper, a framework is put forward to facilitate the design of an adaptive knowledge management system. In this framework, the structural knowledge modeling is combined with processes, which are used for ensuring the quality of knowledge acquisition in the framework. Two kinds of spaces, knowledge space and process space, and a knowledge state model are introduced. Finally, application systems based on this framework, which are being used in three real business enterprises for controlling life cycle of knowledge management in China, are discussed.",https://ieeexplore.ieee.org/document/1506442/,"IRI -2005 IEEE International Conference on Information Reuse and Integration, Conf, 2005.",15-17 Aug. 2005,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/EEEI.2010.5662183,Collision-based computing and Event Calculus to represent and control complex engineering systems,IEEE,Conferences,"Software is always more a strategic asset to sustain complexity of human societies. The requirement to control an increasing number of continuously changing processes is the cause of a transition from centralized to distributed and cooperative organizations. The consequence is a computer systems design integrated in the process of define/improve system models with a progressive approach too. To reach the described research objective we have developed a new approach to design a computer system. In our design approach we mapped the physical structure of a system to a multidimensional Cellular Automata Space (CAS). CAS collects data from sensors that are at the specific coordinates of cells and it is the substrata for energy, materials and information flow representation. Events and Actions in a CAS are described using Event Calculus and the flow of fluents is interpreted on the basis of Collision-based computing paradigm. Trajectories approaching a collision site represent input values, and trajectories of localizations traveling away from a collision site represent output values. At this point we have a model of a real system as dynamic networks of fluents. This is represented using the language of flow pattern, inspired by F. Niscida's work, a semi-symbolic representation of the patterns in CAS states space. The flow patterns are used in a qualitative analysis to monitor and interpret the result of CAS computation.",https://ieeexplore.ieee.org/document/5662183/,2010 IEEE 26-th Convention of Electrical and Electronics Engineers in Israel,17-20 Nov. 2010,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SNPD.2018.8441141,Color Classification based on Pixel Intensity Values,IEEE,Conferences,"What is this color? This is an easy question but sometimes difficult to answer. Because there are millions of colors but a few color names available. In this paper, we propose a method for naming colors based on pixel intensity values. Color naming is important and can be applied to many real world applications such as colorblind assistance, image segmentation, and image retrieval. Our goal here is to map color pixel intensity values to the twelve basic color names: pink, purple, red, orange, yellow, green, cyan, blue, brown, white, grey, and black. We consider only twelve categories, because basic color names were found to be shared between languages. To do so, our method firstly uses the hue value of HSV color space to categorize colors into a predefined number of ranges. Then, we apply a nearest neighbor classification on RGB color space to get color names, where only training colors containing in the same range as testing colors will be considered. Through experiments, we show that the first step will help filter out some incorrect answers, so that color names obtained from the nearest neighbor classification are more accurately. In addition, the evaluation results demonstrate that the proposed method is promising.",https://ieeexplore.ieee.org/document/8441141/,"2018 19th IEEE/ACIS International Conference on Software Engineering, Artificial Intelligence, Networking and Parallel/Distributed Computing (SNPD)",27-29 June 2018,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SIS.2007.368020,Combined Training of Recurrent Neural Networks with Particle Swarm Optimization and Backpropagation Algorithms for Impedance Identification,IEEE,Conferences,"A recurrent neural network (RNN) trained with a combination of particle swarm optimization (PSO) and backpropagation (BP) algorithms is proposed in this paper. The network is used as a dynamic system modeling tool to identify the frequency-dependent impedances of power electronic systems such as rectifiers, inverters, and DC-DC converters. As a category of supervised learning methods, the various backpropagation training algorithms developed for recurrent neural networks use gradient descent information to guide their search for optimal weights solutions that minimize the output errors. While they prove to be very robust and effective in training many types of network structures, they suffer from some serious drawbacks such as slow convergence and being trapped at local minima. In this paper, a modified particle swarm optimization technique is used in combination with the backpropagation algorithm to traverse in a much larger search space for the optimal solution. The combined method preserves the advantages of both techniques and avoids their drawbacks. The method is implemented to train a RNN that successfully identifies the impedance characteristics of a three-phase inverter system. The performance of the proposed method is compared to those of both BP and PSO when used separately to solve the problem, demonstrating its superiority",https://ieeexplore.ieee.org/document/4223149/,2007 IEEE Swarm Intelligence Symposium,1-5 April 2007,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCSA54496.2021.00016,Combining Weighted Total Variation and Deep Image Prior for natural and medical image restoration via ADMM,IEEE,Conferences,"In the last decades, unsupervised deep learning based methods have caught researchers&#x2019; attention, since in many real applications, such as medical imaging, collecting a large amount of training examples is not always feasible. Moreover, the construction of a good training set is time consuming and hard because the selected data have to be enough representative for the task. In this paper, we focus on the Deep Image Prior (DIP) framework and we propose to combine it with a space-variant Total Variation regularizer with an automatic estimation of the local regularization parameters. Differently from other existing approaches, we solve the arising minimization problem via the flexible Alternating Direction Method of Multipliers (ADMM). Furthermore, we provide a specific implementation also for the standard isotropic Total Variation. The promising performances of the proposed approach, in terms of PSNR and SSIM values, are addressed through several experiments on simulated as well as real natural and medical corrupted images.",https://ieeexplore.ieee.org/document/9732379/,2021 21st International Conference on Computational Science and Its Applications (ICCSA),13-16 Sept. 2021,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCA.2016.7505247,Communication of spatial expressions on multi-agent systems using the qualitative Ego-Sphere,IEEE,Conferences,"The need for spatial representations and spatial reasoning is omnipresent in various real world applications of autonomous systems. The task of the qualitative spatial reasoning sub-field of Artificial Intelligence is to provide formalisms allowing a machine to represent and make inferences about spatial entities. In this work we make use of one such formalism for representing qualitative location, named Qualitative Ego-Sphere (QES), that discretises the world around a visual agent into sectors, as well as with respect to the relative distance of objects from the observer's point of view. QES was used in this paper as a means for communicating spatial expressions between pairs of agents. Four situations were proposed and implemented in order to address interactions between pairs of artificial agents and between an artificial agent and a human. Tests with human volunteers suggested that the human description of space in sectors agrees with the qualitative discretisation provided by QES. However, no similar agreement rates were obtained regarding the description of space related to the distance between objects. This was arguably due to the fact that qualitative distance judgements imply the existence of some relative external reference frame (not taken into account in the Qualitative Ego-Sphere formalism).",https://ieeexplore.ieee.org/document/7505247/,2016 12th IEEE International Conference on Control and Automation (ICCA),1-3 June 2016,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IJCNN.2007.4371111,Compact hardware for real-time speech recognition using a Liquid State Machine,IEEE,Conferences,"Hardware implementations of Spiking Neural Networks are numerous because they are well suited for implementation in digital and analog hardware, and outperform classic neural networks. This work presents an application driven digital hardware exploration where we implement realtime, isolated digit speech recognition using a Liquid State Machine (a recurrent neural network of spiking neurons where only the output layer is trained). First we test two existing hardware architectures, but they appear to be too fast and thus area consuming for this application. Then we present a scalable, serialised architecture that allows a very compact implementation of spiking neural networks that is still fast enough for real-time processing. This work shows that there is actually a large hardware design space of Spiking Neural Network hardware that can be explored. Existing architectures only spanned part of it.",https://ieeexplore.ieee.org/document/4371111/,2007 International Joint Conference on Neural Networks,12-17 Aug. 2007,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/PIC.2018.8706141,Comparison and Analysis on Typical Network Representation Learning Algorithms,IEEE,Conferences,"Large-scale complex networks show complex nonlinear relationships among objects, such as the social relationships in the real world, the citation relationship among papers and the interactions among proteins in biology. The analysis of complex network systems make it possible to reveal network structures, information disseminating laws, and communication patterns. Network representation learning (NRL) algorithms focus on mapping the original network structure information to a low-dimensional vector space through a series of operations under the premise of maximally retaining the network structure. In order to analyze current representative NRL algorithms effectively to provide valuable references for other researchers, we built an experimental platform to perform and test the NRL algorithms based on matrix factorization, the NRL algorithms based on shallow neural network and the NRL algorithms based on deep neural network, with datasets on Collaboration Network, Social Network and Citation Network. We implemented a series of comprehensive experiments, based on metrics include precision@k, micro-F1 and macro-F1. Our experiments include network reconstruction, vertex classification, and link prediction, and show readers principles, performances and applications of typical NRL algorithms.",https://ieeexplore.ieee.org/document/8706141/,2018 IEEE International Conference on Progress in Informatics and Computing (PIC),14-16 Dec. 2018,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ITNT52450.2021.9649145,Comparison of Reinforcement Learning Algorithms for Motion Control of an Autonomous Robot in Gazebo Simulator,IEEE,Conferences,"This article compares various implementations of deep Q learning as it is one of the most efficient reinforcement learning algorithms for discrete action space systems. The efficiency of the implementations for the classical Cartpole problem ported to the Gazebo environment is investigated. Then, these algorithms are compared for a self-created bipedal robot problem. Since the creation and configuration of a real robotic system is a laborious process, the initial debugging of the robot can be performed using the appropriate software that simulates the real environment. In our case, the Gazebo simulator was used. Using the simulator allows you to conduct research without having a real robotic system. In this case, it is possible to transfer the results from the simulator to the real system. The result of the study is the conclusion about the greatest efficiency of deep Q-learning with the experience reproduction mechanism. Also, the conclusion is that even for a robot with two degrees of freedom, Q-learning algorithms are not effective enough, and a comparative study with other families of reinforcement learning algorithms is needed.",https://ieeexplore.ieee.org/document/9649145/,2021 International Conference on Information Technology and Nanotechnology (ITNT),20-24 Sept. 2021,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/RCAR52367.2021.9517443,Complementary Multi-Branch CNNs Towards Real-World 3D Point Classification,IEEE,Conferences,"Autonomous driving requires precise and efficient point clouds processing techniques, where deep learning has shown great potential. Nonetheless, most of the existing works achieve high accuracy in synthetic data while performing unsatisfactorily in real-world datasets. On the other hand, though existing multi-view-based, volumetric-based, and point-based methods have achieved promising results, they still suffer from high computational complexity or low memory utilization efficiency. For example, the computational time and memory consumption of multi-view-based and volumetric-based methods increase at most cubically with the increasing of the input resolution, while half of the point-based methods&#x0027;s runtime is wasted due to their irregular memory access. To address these issues, we propose a novel method to fuse complementary CNNs, dubbed as Multi-Branch Convolutional Neural Networks (MBCNN), which achieves the current state-of-the-art performance while maintaining high efficiency. The underlying design of MBCNN is to utilize the point-based convolution as a local-feature branch and spherical convolution as a global-feature branch, to capture patterns in space and exploit the merit of the pointwise MLP, which is robust to the perturbation. Besides, we design an efficient voxel-based indexing technique to prevent the feature-extracting phase from being blocked, thus significantly accelerating the procedure of finding a point&#x0027;s neighbor. Experimental results show that MBCNN achieves state-of-the-art performance on the real-world 3D point clouds dataset while reducing 30&#x0025; of runtime in neighbor indexing.",https://ieeexplore.ieee.org/document/9517443/,2021 IEEE International Conference on Real-time Computing and Robotics (RCAR),15-19 July 2021,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AMFG.2003.1240833,Component-based LDA method for face recognition with one training sample,IEEE,Conferences,"Many face recognition algorithms/systems have been developed in the last decade and excellent performances are also reported when there is sufficient number of representative training samples. In many real-life applications, only one training sample is available. Under this situation, the performance of existing algorithms will be degraded dramatically or the formulation is incorrect, which in turn, the algorithm cannot be implemented. We propose a component-based linear discriminant analysis (LDA) method to solve the one training sample problem. The basic idea of the proposed method is to construct local facial feature component bunches by moving each local feature region in four directions. In this way, we not only generate more samples, but also consider the face detection localization error while training. After that, we employ a sub-space LDA method, which is tailor-made for small number of training samples, for the local feature projection to maximize the discrimination power. Finally, combining the contributions of each local feature draws the recognition decision. FERET database is used for evaluating the proposed method and results are encouraging.",https://ieeexplore.ieee.org/document/1240833/,2003 IEEE International SOI Conference. Proceedings (Cat. No.03CH37443),17-17 Oct. 2003,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/DCC.2009.75,Compressed Kernel Perceptrons,IEEE,Conferences,"Kernel machines are a popular class of machine learning algorithms that achieve state of the art accuracies on many real-life classification problems. Kernel perceptrons are among the most popular online kernel machines that are known to achieve high-quality classification despite their simplicity. They are represented by a set of B prototype examples, called support vectors, and their associated weights. To obtain a classification, a new example is compared to the support vectors. Both space to store a prediction model and time to provide a single classification scale as O(B). A problem with kernel perceptrons is that on noisy data the number of support vectors tends to grow without bounds with the number of training examples. To reduce the strain at computational resources, budget kernel perceptrons have been developed by upper bounding the number of support vectors. In this work, we propose a new budget algorithm that upper bounds the number of bits needed to store kernel perceptron. Setting the bitlength constraint could facilitate development of hardware and software implementations of kernel perceptrons on resource-limited devices such as microcontrollers. The proposed compressed kernel perceptron algorithm decides on the optimal tradeoff between number of support vectors and their bit precision. The algorithm was evaluated on several benchmark data sets and the results indicate that it can train highly accurate classifiers even when the available memory budget is below 1Kbit. This promising result points to a possibility of implementing powerful learning algorithms even on the most resource-constrained computational devices.",https://ieeexplore.ieee.org/document/4976459/,2009 Data Compression Conference,16-18 March 2009,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/TENSYMP46218.2019.8971326,Computational Color Naming for Human-Machine Interaction,IEEE,Conferences,"In this paper, we present two simple methods for automatic color naming, the first one learns the pixel-wise color name annotations from the color-chips and later method learns the image-wise color names from the real-world weakly labelled images. Color information is an important feature for many computer vision applications. Color name as a descriptor finds it's applications in many real-world tasks such as Color Blind assistance, image retrieval and scene understanding. Color naming in images is a challenging problem due to shadows, view angles, illumination conditions and surface reflections. Manual labelling of color names for real-world images in applications like search engines, fashion parsing and tracking is a tedious task and time consuming. The proposed systems for color naming automates the process and avoids human labelling. These methods are based on superpixels in the CIELAB color space. We trained Random Forests classifier with color-chip dataset for pixel-wise color naming and for image-wise color naming it is trained on weakly color labelled image dataset. Both models are tested on real-world images for color name judgments. Experimental results shows that color names learned through these proposed systems have advantages in terms of implementation costs, speed of execution and can be used in real-time applications with lowcost hardware.",https://ieeexplore.ieee.org/document/8971326/,2019 IEEE Region 10 Symposium (TENSYMP),7-9 June 2019,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/MSR.2017.20,Concept-Based Classification of Software Defect Reports,IEEE,Conferences,"Automatic identification of the defect type from the textual description of a software defect can significantly speed-up as well as improve the software defect management life-cycle. This has been recognized in the research community and multiple solutions based on supervised learning approach have been proposed in the recent literature. However, these approaches need significant amount of labeled training data for use in real-life projects. In this paper, we propose to use Explicit Semantic Analysis (ESA) to carry out concept-based classification of software defect reports. We compute the ""semantic similarity"" between the defect type labels and the defect report in a concept space spanned by Wikipedia articles and then, assign the defect type which has the highest similarity with the defect report. This approach helps us to circumvent the problem of dependence on labeled training data. Experimental results show that using concept-based classification is a promising approach for software defect classification to avoid the expensive process of creating labeled training data and yet get accuracy comparable to the traditional supervised learning approaches. To the best of our knowledge, this is the first use of Wikipedia and ESA for software defect classification problem.",https://ieeexplore.ieee.org/document/7962367/,2017 IEEE/ACM 14th International Conference on Mining Software Repositories (MSR),20-21 May 2017,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/RTSS52674.2021.00026,Concurrent Order Dispatch for Instant Delivery with Time-Constrained Actor-Critic Reinforcement Learning,IEEE,Conferences,"Instant delivery has developed rapidly in recent years and significantly changed the lifestyle of people due to its timeliness and convenience. In instant delivery, the order dispatch process is concurrent. Couriers take new orders continuously and deliver multiple orders in a delivery trip (i.e., a batch). The delivery time of orders in a batch is often overlapped and interlinked with each other. The pickup and delivery sequence of the existing orders in a batch changes dynamically due to time constraints and real-time overdue possibility (i.e., the rate of deliveries that are not finished in promised time). Most of existing order dispatch mechanisms are designed for independent order dispatch or concurrent delivery without strict time constraints, hence are incapable of handling real-time concurrent dispatch with strict time constraints in on-demand instant delivery.To address the challenge, we propose a Time-Constrained Actor-Critic Reinforcement learning based concurrent dispatch system called TCAC-Dispatch to enhance the long-term overall revenue and reduce the overdue rate. Specifically, we design a deep matching network (DMN) with a variable action space, which integrates the state embedding (including route behaviors encoding) and actions embedding features into a long-term matching value. Then the Actor-Critic model tackles the concurrent order dispatch problem considering strict time constraints and stochastic demand-supply in instant delivery. An estimated-time based action pruning module is designed to ensure time constraints guarantee and accelerate the training as well as dispatching processes. We evaluate the TCAC-Dispatch with one-month data involved with 36.48 million orders and 42,000 couriers collected from one of the largest instant delivery companies in China, i.e., Eleme. Empirical experiments are conducted on a data-driven emulator deployed on the development environment of Eleme and results show that our method achieves 22% of the increase in total revenue and reduces the overdue rate by 21.6%.",https://ieeexplore.ieee.org/document/9622337/,2021 IEEE Real-Time Systems Symposium (RTSS),7-10 Dec 2021,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS.2009.5354270,Consideration on robotic giant-swing motion generated by reinforcement learning,IEEE,Conferences,"This study attempts to make a compact humanoid robot acquire a giant-swing motion without any robotic models by using reinforcement learning; only the interaction with environment is available. Generally, it is widely said that this type of learning method is not appropriated to obtain dynamic motions because Markov property is not necessarily guaranteed during the dynamic task. However, in this study, we try to avoid this problem by embedding the dynamic information in the robotic state space; the applicability of the proposed method is considered using both the real robot and dynamic simulator. This paper, in particular, discusses how the robot with 5-DOF, in which the Q-Learning algorithm is implemented, acquires a giant-swing motion. Further, we describe the reward effects on the Q-Learning. Finally, this paper demonstrates that the application of the Q-Learning enable the robot to perform a very attractive giant-swing motion.",https://ieeexplore.ieee.org/document/5354270/,2009 IEEE/RSJ International Conference on Intelligent Robots and Systems,10-15 Oct. 2009,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AERO50100.2021.9438171,Considerations in the Deployment of Machine Learning Algorithms on Spaceflight Hardware,IEEE,Conferences,"Recent advances in artificial intelligence (AI) and machine learning (ML) have revolutionized many fields. ML has many potential applications in the space domain. Next generation space instruments are producing data at rates that exceed the capabilities of current spacecraft to store or transmit to ground stations. Deployment of ML algorithms onboard future spacecraft could perform processing of sensor data as it is gathered, reducing data volume and providing a dramatic increase in throughput of meaningful data. ML techniques may also be used to enhance the autonomy of space missions. However ML techniques have not yet been widely deployed in space environments, primarily due to limitations on the computational capabilities of spaceflight hardware. The need to verify that high-performance computational hardware can reliably operate in this environment delays the adoption of these technologies. Nevertheless, the availability of advanced processing capabilities onboard spacecraft is increasing. These platforms may not provide the processing power of terrestrial equivalents, but they do provide the resources necessary for deploying real-time execution of ML algorithms. In this paper, we present results exploring the implementation of ML techniques on computationally-constrained, high-reliability spacecraft hardware. We show two ML algorithms utilizing deep learning techniques which illustrate the utility of these approaches for space applications. We describe implementation considerations when tailoring these algorithms for execution on computationally-constrained hardware and present a workflow for performing these optimizations. We also present initial results on characterizing the trade space between algorithm accuracy, throughput, and reliability on a variety of hardware platforms with current and anticipated paths to spaceflight.",https://ieeexplore.ieee.org/document/9438171/,2021 IEEE Aerospace Conference (50100),6-13 March 2021,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IECON.1994.398074,Constrained Hopfield neural network for real-time predictive control,IEEE,Conferences,The hardware implementation of an optimization network with restrictions to perform real-time generalized predictive control (GPC) is described. The use of a space-efficient stochastic architecture allows a realization on a programmable logic device. As a result a programmable neural chip coprocessor that solves optimization problems subject to restrictions has been developed. Expressions for network parameters are provided to implement GPC. An adaptive controller is achieved using RAM memories to store the network parameters. Experimental results from a simple implementation of the controller are included.<>,https://ieeexplore.ieee.org/document/398074/,Proceedings of IECON'94 - 20th Annual Conference of IEEE Industrial Electronics,5-9 Sept. 1994,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/DSAA.2019.00051,Constrained Multi-Objective Optimization for Automated Machine Learning,IEEE,Conferences,"Automated machine learning has gained a lot of attention recently. Building and selecting the right machine learning models is often a multi-objective optimization problem. General purpose machine learning software that simultaneously supports multiple objectives and constraints is scant, though the potential benefits are great. In this work, we present a framework called Autotune that effectively handles multiple objectives and constraints that arise in machine learning problems. Autotune is built on a suite of derivative-free optimization methods, and utilizes multi-level parallelism in a distributed computing environment for automatically training, scoring, and selecting good models. Incorporation of multiple objectives and constraints in the model exploration and selection process provides the flexibility needed to satisfy trade-offs necessary in practical machine learning applications. Experimental results from standard multi-objective optimization benchmark problems show that Autotune is very efficient in capturing Pareto fronts. These benchmark results also show how adding constraints can guide the search to more promising regions of the solution space, ultimately producing more desirable Pareto fronts. Results from two real-world case studies demonstrate the effectiveness of the constrained multi-objective optimization capability offered by Autotune.",https://ieeexplore.ieee.org/document/8964219/,2019 IEEE International Conference on Data Science and Advanced Analytics (DSAA),5-8 Oct. 2019,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICDM.2010.15,Constraint Based Dimension Correlation and Distance Divergence for Clustering High-Dimensional Data,IEEE,Conferences,"Clusters are hidden in subspaces of high dimensional data, i.e., only a subset of features is relevant for each cluster. Subspace clustering is challenging since the search for the relevant features of each cluster and the detection of the final clusters are circular dependent and should be solved simultaneously. In this paper, we point out that feature correlation and distance divergence are important to subspace clustering, but both have not been considered in previous works. Feature correlation groups correlated features independently thus helps to reduce the search space for the relevant features search problem. Distance divergence distinguishes distances on different dimensions and helps to find the final clusters accurately. We tackle the two problems with the aid of a small amount domain knowledge in the form of must-links and cannot-links. We then devise a semi-supervised subspace clustering algorithm CDCDD. CDCDD integrates our solutions of the feature correlation and distance divergence problems, and uses an adaptive dimension voting scheme, which is derived from a previous unsupervised subspace clustering algorithm FINDIT. Experimental results on both synthetic data sets and real data sets show that the proposed CDCDD algorithm outperforms FINDIT in terms of accuracy, and outperforms the other constraint based algorithm SCMINER in terms of both accuracy and efficiency.",https://ieeexplore.ieee.org/document/5694017/,2010 IEEE International Conference on Data Mining,13-17 Dec. 2010,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AITest.2019.00015,Constraint-Based Testing of An Industrial Multi-Robot Navigation System,IEEE,Conferences,"Intelligent multi-robot systems get more and more deployed in industrial settings to solve complex and repetitive tasks. Due to safety and economic reasons they need to operate dependably. To ensure a high degree of dependability, testing the deployed system has to be done in a rigorous way. Advanced multi-robot systems show a rich set of complex behaviors. Thus, these systems are difficult to test manually. Moreover, the space of potential environments and tasks for such systems is enormous. Therefore, methods that are able to explore this space in a structured way are needed. One way to address these issues is through model-based testing. In this paper we present an approach for testing the navigation system of a fleet of industrial transport robots. We show how all potential environments and navigation behaviors as well as requirements and restrictions can be represented in a formal constraint-based model. Moreover, we present the concept of coverage criteria in order to handle the potentially infinite space of test cases. Finally, we show how test cases can be derived from this model in an efficient way. In order to show the feasibility of the proposed approach we present an empirical evaluation of a prototype implementation using a real industrial use case.",https://ieeexplore.ieee.org/document/8718216/,2019 IEEE International Conference On Artificial Intelligence Testing (AITest),4-9 April 2019,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IJCNN52387.2021.9533996,Constraint-Guided Reinforcement Learning: Augmenting the Agent-Environment-Interaction,IEEE,Conferences,"Reinforcement Learning (RL) agents have great successes in solving tasks with large observation and action spaces from limited feedback. Still, training the agents is data-intensive and there are no guarantees that the learned behavior is safe and does not violate rules of the environment, which has limitations for the practical deployment in real-world scenarios. This paper discusses the engineering of reliable agents via the integration of deep RL with constraint-based augmentation models to guide the RL agent towards safe behavior. Within the constraints set, the RL agent is free to adapt and explore, such that its effectiveness to solve the given problem is not hindered. However, once the RL agent leaves the space defined by the constraints, the outside models can provide guidance to still work reliably. We discuss integration points for constraint guidance within the RL process and perform experiments on two case studies: a strictly constrained card game and a grid world environment with additional combinatorial subgoals. Our results show that constraint-guidance does both provide reliability improvements and safer behavior, as well as accelerated training.",https://ieeexplore.ieee.org/document/9533996/,2021 International Joint Conference on Neural Networks (IJCNN),18-22 July 2021,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SMC42975.2020.9282816,Container Terminal Liner Berthing Time Prediction with Computational Logistics and Deep Learning,IEEE,Conferences,"The quayside running conditions play a key role in container terminal logistics systems, and the terminal liner berthing time (LBT) is the central index of quayside service efficiency that is also the important evidence and guidance to task scheduling and resource allocation at container terminals. The computational logistics and deep learning are combined to discuss the prediction of LBT by the generalization, unification and integration of the essence and connotation of computation. It is supposed to integrate the deep neural networks learning computation and logistics generalized computation for container terminals (LGC-CT) cross the boundaries between information space and physical world. A deep learning model is designed and executed to predict and evaluate LBT at a typical container terminal in China based on its LBT data for the past four years, which is also intended to lay a good foundation for the configuration, deployment and execution of LGC-CT. The deep neural networks are designed and implemented by the fusion of long short-term memory network, gated recurrent unit one, Gaussian noise one and dense one with TensorFlow 2.3, which demonstrates the feasibility and credibility of the proposed compound computing architecture and paradigms preliminarily.",https://ieeexplore.ieee.org/document/9282816/,"2020 IEEE International Conference on Systems, Man, and Cybernetics (SMC)",11-14 Oct. 2020,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/RO-MAN47096.2020.9223341,Context Dependent Trajectory Generation using Sequence-to-Sequence Models for Robotic Toilet Cleaning,IEEE,Conferences,"A robust, easy-to-deploy robot for service tasks in a real environment is difficult to construct. Record-and-playback (R&P) is a method used to teach motor-skills to robots for performing service tasks. However, R&P methods do not scale to challenging tasks where even slight changes in the environment, such as localization errors, would either require trajectory modification or a new demonstration. In this paper, we propose a Sequence-to-Sequence (Seq2Seq) based neural network model to generate robot trajectories in configuration space given a context variable based on real-world measurements in Cartesian space. We use the offset between a target pose and the actual pose after localization as the context variable. The model is trained using a few expert demonstrations collected using teleoperation. We apply our proposed method to the task of toilet cleaning where the robot has to clean the surface of a toilet bowl using a compliant end-effector in a constrained toilet setting. In the experiments, the model is given a novel offset context and it generates a modified robot trajectory for that context. We demonstrate that our proposed model is able to generate trajectories for unseen setups and the executed trajectory results in cleaning of the toilet bowl.",https://ieeexplore.ieee.org/document/9223341/,2020 29th IEEE International Conference on Robot and Human Interactive Communication (RO-MAN),31 Aug.-4 Sept. 2020,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICDCS47774.2020.00101,Context-Aware Deep Model Compression for Edge Cloud Computing,IEEE,Conferences,"While deep neural networks (DNNs) have led to a paradigm shift, its exorbitant computational requirement has always been a roadblock in its deployment to the edge, such as wearable devices and smartphones. Hence a hybrid edge-cloud computational framework is proposed to transfer part of the computation to the cloud, by naively partitioning the DNN operations under the constant network condition assumption. However, real-world network state varies greatly depending on the context, and DNN partitioning only has limited strategy space. In this paper, we explore the structural flexibility of DNN to fit the edge model to varying network contexts and different deployment platforms. Specifically, we designed a reinforcement learning-based decision engine to search for model transformation strategies in response to a combined objective of model accuracy and computation latency. The engine generates a context-aware model tree so that the DNN can decide the model branch to switch to at runtime. By the emulation and field experimental results, our approach enjoys a 30% − 50% latency reduction while retaining the model accuracy.",https://ieeexplore.ieee.org/document/9355722/,2020 IEEE 40th International Conference on Distributed Computing Systems (ICDCS),29 Nov.-1 Dec. 2020,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/DICTA.2009.81,Context-Based Appearance Descriptor for 3D Human Pose Estimation from Monocular Images,IEEE,Conferences,"In this paper we propose a novel appearance descriptor for 3D human pose estimation from monocular images using a learning-based technique. Our image-descriptor is based on the intermediate local appearance descriptors that we design to encapsulate local appearance context and to be resilient to noise. We encode the image by the histogram of such local appearance context descriptors computed in an image to obtain the final image-descriptor for pose estimation. We name the final image-descriptor the histogram of local appearance context (HLAC). We then use relevance vector machine (RVM) regression to learn the direct mapping between the proposed HLAC image-descriptor space and the 3D pose space. Given a test image, we first compute the HLAC descriptor and then input it to the trained regressor to obtain the final output pose in real time. We compared our approach with other methods using a synchronized video and 3D motion dataset. We compared our proposed HLAC image-descriptor with the Histogram of shape context and histogram of SIFT like descriptors. The evaluation results show that HLAC descriptor outperforms both of them in the context of 3D Human pose estimation.",https://ieeexplore.ieee.org/document/5384910/,2009 Digital Image Computing: Techniques and Applications,1-3 Dec. 2009,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/QRS.2015.17,Cross-Project Aging Related Bug Prediction,IEEE,Conferences,"In a long running system, software tends to encounter performance degradation and increasing failure rate during execution, which is called software aging. The bugs contributing to the phenomenon of software aging are defined as Aging Related Bugs (ARBs). Lots of manpower and economic costs will be saved if ARBs can be found in the testing phase. However, due to the low presence probability and reproducing difficulty of ARBs, it is usually hard to predict ARBs within a project. In this paper, we study whether and how ARBs can be located through cross-project prediction. We propose a transfer learning based aging related bug prediction approach (TLAP), which takes advantage of transfer learning to reduce the distribution difference between training sets and testing sets while preserving their data variance. Furthermore, in order to mitigate the severe class imbalance, class imbalance learning is conducted on the transferred latent space. Finally, we employ machine learning methods to handle the bug prediction tasks. The effectiveness of our approach is validated and evaluated by experiments on two real software systems. It indicates that after the processing of TLAP, the performance of ARB bug prediction can be dramatically improved.",https://ieeexplore.ieee.org/document/7272913/,"2015 IEEE International Conference on Software Quality, Reliability and Security",3-5 Aug. 2015,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CEC45853.2021.9504779,Cultural Weight-Based Fish School Search: A Flexible Optimization Algorithm For Engineering,IEEE,Conferences,"Many real-life engineering applications are optimization problems. To find the best configuration of variables to minimize costs and maximize efficiency are typically used engineering software as CAD, CAE and CAM. In this context, Machine Learning can be used to automate and improve this type of application. This despite, those searches not seldomly evoke unreliable areas and suggest risky solutions. Because of inaccuracies, volatility, unfeasibility, and specificities of real environments, the easy incorporation of cultural practices (i.e. normative, situational, domain and historic knowledge) and as well as the production of multiple acceptable solutions for a problem is always welcome, especially in Engineering. The present article put forward a hybridization of a multi-modal algorithm (Weight-Based Fish School Search - wFSS) with Cultural Algorithms' belief space. New cwFSS is able to guide the optimization also considering normative knowledge from experts, technical literature and problem domain readily available knowledge to prevent the incorporation of constraints into the fitness function. We also evaluated the use of temporal knowledge to guide the simulation. The proposed method was tested in a thermal power plant efficiency optimization and compared with standard wFSS and the Niching Migratory Multi-Swarm Optimizer (NMMSO), winner of CEC'2015 niching competition. As results, cwFSS has outperformed at times NMMSO about time, fitness and variability, as well as traditional wFSS about time, stability, safeness and variability of the multimodal solutions. By avoiding penalties, the appropriation of a priori search directly into the search can effectively and elegantly help better support for engineering decisions.",https://ieeexplore.ieee.org/document/9504779/,2021 IEEE Congress on Evolutionary Computation (CEC),28 June-1 July 2021,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ADFSP.1998.685683,D-FANNS (dynamical functional artificial neural networks)-a new avenue for intelligent analog signal processing,IEEE,Conferences,"Summary form only given. Intelligent signal processing may be defined as the process of mapping a signal x into a binary vector or matrix y, so that y enables the detection, classification, or interpretation of an event present in x. (In the case of an interpretation in an appropriate language, y would represent a digitally coded relational structure.) We denote by f the input-output map of such an intelligent signal processing filter. In a number of applications, it is possible to naturally implement the nonlinear filter map f by an artificial neural network (ANN). We consider the case in which x is an analog signal (waveform) belonging to L2(I), where I is an appropriate interval of the real line R1 (i.e., L2(I) is the space of square integrable functions on I), and propose the realization of f by an artificial neural network in which the synaptic weight actions of the first layer are implemented by a filter bank. We call such a network a dynamical functional artificial neural network (D-FANN) to distinguish it from a conventional functional artificial neural network (FANN), where a synaptic weight action is implemented by a scalar product (integration) in L2(I), between the incoming waveform x and a ""distributed"" functional weight. Compared with conventional FANNs, D-FANNs permit simple and meaningful causal realizations of intelligent analog signal processors. A novel element in the present paper is the introduction of a ""D-FANN gain equation"", in a way analogous to that in Kalman filtering. Applications of D-FANNs to real and simulated data are now in progress and these results are discussed.",https://ieeexplore.ieee.org/document/685683/,1998 IEEE Symposium on Advances in Digital Filtering and Signal Processing. Symposium Proceedings (Cat. No.98EX185),5-6 June 1998,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.23919/SpliTech.2019.8783027,DANN: Digital Audio Neural Network,IEEE,Conferences,"Neural networks have been widely used in computer audio to deal with many synthesis and processing parameters. Experiments have also led to direct synthesis of audio using dynamic neural networks that exhibit oscillating behaviour, as they are clocked at audio rates. There is clearly great potential in convolutional networks as they directly implement complex resonances enabling acoustic interaction. It thus transpires that many different neural network types were used for direct audio synthesis and processing. The proposed innovation extends a fully-connected network of computational cells to facilitate long feedback delays (needed for waveguides) intertwined with zero delays (needed for polynomials). Another improvement is the network computing algorithm that allows for randomisation of delays by adjusting the order of calculations. The DANN model is a specification for network of cells and links that allows direct translation from digital audio synthesis and processing schemes. Taking basic examples such as exponential decay, logistic chaos and biquad filter it transpires that there are exact DANN equivalents to these basic building blocks. It is therefore suggested that complex DANN schemes can reproduce myriad synthesis and processing techniques in a transparent fashion. As a matter of proving the concept a simple snare drum synthesizer is translated to a DANN scheme in order to explore the parameter space in this domain. Thanks to the large amount of training schemes there is great potential be found in training networks for velocity sensitive synthesis to include complex physical interaction sensors. Audio signal processing applications may include modelling of analogue equipment and symbolic acoustic instrument processing. The simplicity of the network grants easy implementation on fast architectures to facilitate near real-time processing and higher sample rates. Beyond the practical uses proposed it is further suggested that network simplification procedures may lead to yet unknown compact schemes for complex signal processing problems in the audio domain, potentially reverberation and pitch-shifting.",https://ieeexplore.ieee.org/document/8783027/,2019 4th International Conference on Smart and Sustainable Technologies (SpliTech),18-21 June 2019,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1145/3240765.3240801,DNNBuilder: an Automated Tool for Building High-Performance DNN Hardware Accelerators for FPGAs,IEEE,Conferences,"Building a high-performance FPGA accelerator for Deep Neural Networks (DNNs) often requires RTL programming, hardware verification, and precise resource allocation, all of which can be time-consuming and challenging to perform even for seasoned FPGA developers. To bridge the gap between fast DNN construction in software (e.g., Caffe, TensorFlow) and slow hardware implementation, we propose DNNBuilder for building high-performance DNN hardware accelerators on FPGAs automatically. Novel techniques are developed to meet the throughput and latency requirements for both cloud- and edge-devices. A number of novel techniques including high-quality RTL neural network components, a fine-grained layer-based pipeline architecture, and a column-based cache scheme are developed to boost throughput, reduce latency, and save FPGA on-chip memory. To address the limited resource challenge, we design an automatic design space exploration tool to generate optimized parallelism guidelines by considering external memory access bandwidth, data reuse behaviors, FPGA resource availability, and DNN complexity. DNNBuilder is demonstrated on four DNNs (Alexnet, ZF, VGG16, and YOLO) on two FPGAs (XC7Z045 and KU115) corresponding to the edge- and cloud-computing, respectively. The fine-grained layer-based pipeline architecture and the column-based cache scheme contribute to 7.7x and 43x reduction of the latency and BRAM utilization compared to conventional designs. We achieve the best performance (up to 5.15x faster) and efficiency (up to 5.88x more efficient) compared to published FPGA-based classification-oriented DNN accelerators for both edge and cloud computing cases. We reach 4218 GOPS for running object detection DNN which is the highest throughput reported to the best of our knowledge. DNNBuilder can provide millisecond-scale real-time performance for processing HD video input and deliver higher efficiency (up to 4.35x) than the GPU-based solutions.",https://ieeexplore.ieee.org/document/8587697/,2018 IEEE/ACM International Conference on Computer-Aided Design (ICCAD),5-8 Nov. 2018,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/DAC18074.2021.9586295,Dancing along Battery: Enabling Transformer with Run-time Reconfigurability on Mobile Devices,IEEE,Conferences,"A pruning-based AutoML framework for run-time reconfigurability, namely RT3, is proposed in this work. This enables Transformer-based large Natural Language Processing (NLP) models to be efficiently executed on resource-constrained mobile devices and reconfigured (i.e., switching models for dynamic hardware conditions) at run-time. Such reconfigurability is the key to save energy for battery-powered mobile devices, which widely use dynamic voltage and frequency scaling (DVFS) technique for hardware reconfiguration to prolong battery life. In this work, we creatively explore a hybrid block-structured pruning (BP) and pattern pruning (PP) for Transformer-based models and first attempt to combine hardware and software reconfiguration to maximally save energy for battery-powered mobile devices. Specifically, RT3 integrates two-level optimizations: First, it utilizes an efficient BP as the first-step compression for resource-constrained mobile devices; then, RT3 heuristically generates a shrunken search space based on the first level optimization and searches multiple pattern sets with diverse sparsity for PP via reinforcement learning to support lightweight software reconfiguration, which corresponds to available frequency levels of DVFS (i.e., hardware reconfiguration). At run-time, RT3 can switch the lightweight pattern sets within 45ms to guarantee the required real-time constraint at different frequency levels. Results further show that RT3 can prolong battery life over $ 4\times$ improvement with less than 1% accuracy loss for Transformer and 1.5% score decrease for DistilBERT.",https://ieeexplore.ieee.org/document/9586295/,2021 58th ACM/IEEE Design Automation Conference (DAC),5-9 Dec. 2021,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.23919/IRS.2019.8768102,Data Analytics and Machine Learning in Wide Area Surveillance Systems,IEEE,Conferences,"Modern surveillance networks are able to provide trajectories of all kind of vessels and aircrafts within worldwide or at least extended environment. Most widely used are Automatic Dependent Surveillance - Broadcast (ADS-B) and (Satellite-) Automatic Identification System (AIS) used within air and maritime surveillance. Both of them are cooperative systems. Besides these systems, sensor networks based on ground installations or mounted on airborne and space-based platforms deliver object trajectories independent of any cooperation. Examples include GMTI radar-based systems operating on UAV platforms and coastal or air traffic control sensor network installations. These surveillance systems provide mid- and long-term trajectories. The challenging part is the related situational awareness and the estimation of the intent of the tracked objects. New technologies include activity-based intelligence and the determination of patterns of life. An approach for these technologies can be found in the advanced analysis of those trajectories, which are extracted by the mentioned surveillance systems. Trajectories are partitioned into specific segments of interest using cluster algorithms. This helps to decode their pattern of life based on unsupervised machine learning. Trajectories are aggregated into different routes with dedicated representatives. Calculated probabilities indicate the frequentation of these routes. This allows predictive analytics and the identification of anomalous behaviour. Finally, these new data analytic techniques have to be integrated in existing near real time surveillance systems. This requires specific system architectures as well as a completely new software and hardware landscape. So, trajectory-based Machine Learning is embedded in local or global clouds and uses dedicated mechanisms for distributed and parallel processing.",https://ieeexplore.ieee.org/document/8768102/,2019 20th International Radar Symposium (IRS),26-28 June 2019,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.23919/IRS48640.2020.9253816,"Data Analytics, Machine Learning and Risk Assessment for Surveillance and Situation Awareness",IEEE,Conferences,"Modern surveillance networks are able to provide trajectories of all kinds for aircrafts and vessels worldwide or at least in extended areas of the airspace or earth surface. Best known are Automatic Dependent Surveillance - Broadcast (ADS-B) and (Satellite-) Automatic Identification System (AIS) used in air and maritime surveillance. Both of them are cooperative systems. Besides these sources, sensors based on ground installations or mounted on airborne and space-based platforms deliver object trajectories independently of any transponders. This is done by advanced tracking and fusion algorithms generating trajectories out of sensor measurements. Examples include GMTI radar-based systems operating on UAV platforms or imaging systems based on high altitude pseudo satellites (HAPS) and satellites. These surveillance systems enable a continuous extraction of mid- and long-term trajectories of objects. Besides the trajectory generation, the challenge will be to place them into the right context and to provide situational awareness. This includes the estimation of the intents of the tracked objects, activity-based intelligence, and the determination of patterns of life. Otherwise, even modern surveillance systems are not able to take a real advantage of the gathered data. Therefore, trajectories are further processed by data analytics and machine learning. Unsupervised machine learning offers techniques to cluster and to partition trajectories, extract highly frequented routes and points of interest, predict object movement and identify anomalous behaviour. On the other hand, transponder and broadcast systems provide additional attributes of the tracked trajectories. These labels pave the way for numerous supervised machine learning methods. The derived predictors realise the determination of object types and activities. Finally, these new data analytic techniques have to be integrated in existing near real time surveillance systems. This requires specific system architectures as well as a completely new software and hardware landscape. In summary, trajectory-based data analytics, machine learning and risk assessment are embedded on local or global clouds and use dedicated mechanisms for distributed and parallel processing.",https://ieeexplore.ieee.org/document/9253816/,2020 21st International Radar Symposium (IRS),5-8 Oct. 2020,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCCNT51525.2021.9579976,"Data Processing in IoT, Sensor to Cloud: Survey",IEEE,Conferences,"IoT is connecting Things over the Internet and the realization of the environment through smart things to create a responsive space. Many surveys predicted the growth of IoT devices is going to be around 50 billion and an average of 7 devices per person. IoT has shown promising future with its applications like smart city, connected factories, buildings, roadways, smart health and many more. To make the promise a reality IoT has to overcome many hurdles like scalability, connectivity, architectural, big data, analysis, security, and privacy. In this literature survey, an attempt has been made to identify current challenges faced by IoT implementation and possible solutions, future opportunities, and research openings. Further, the processing of sensed data at IoT device, edge/fog layer, and the cloud is discussed in detail. Keywords- IoT, IoT architecture, Machine learning, Deep",https://ieeexplore.ieee.org/document/9579976/,2021 12th International Conference on Computing Communication and Networking Technologies (ICCCNT),6-8 July 2021,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/TAI.2002.1180785,Data mining using cultural algorithms and regional schemata,IEEE,Conferences,"In the paper we demonstrate how evolutionary search for functional optima can be used as a vehicle for data mining, that is, in the process of searching for optima in a multi-dimensional space we can keep track of the constraints that must be placed on related variables in order to move towards the optima. Thus, a side effect of evolutionary search can be the mining of constraints for related variables. We use a cultural algorithm framework to embed the search and store the results in regional schemata. An application to a large-scale real world archaeological data set is presented.",https://ieeexplore.ieee.org/document/1180785/,"14th IEEE International Conference on Tools with Artificial Intelligence, 2002. (ICTAI 2002). Proceedings.",4-6 Nov. 2002,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICAS49788.2021.9551175,Data-Driven Kalman-Based Velocity Estimation for Autonomous Racing,IEEE,Conferences,"Real-time velocity estimation is a core task in autonomous driving, which is carried out based on available raw sensors such as wheel odometry and motor currents. When the system dynamics and observations can be modeled together as a fully known linear Gaussian state space (SS) model, the celebrated Kalman filter (KF) is a low complexity optimal solution. However, both linearity of the underlying SS model and accurate knowledge of it are often not encountered in practice. This work proposes to estimate the velocity using a hybrid data-driven (DD) implementation of the KF for non-linear systems, coined KalmanNet. KalmanNet integrates a compact recurrent neural network in the flow of the classical KF, retaining low computational complexity, high data efficiency, and interpretability, while enabling operation in non-linear SS models with partial information. We apply KalmanNet on an autonomous racing car as part of the Formula Student (FS) Driverless competition. Our results demonstrate the ability of KalmanNet to outperform a state-of-the-art implementation of the KF that uses a postulated SS model, while being applicable on the vehicle control unit used by the car.",https://ieeexplore.ieee.org/document/9551175/,2021 IEEE International Conference on Autonomous Systems (ICAS),11-13 Aug. 2021,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICBBE.2008.696,Decoding Hand Kinematics and Neural States Using Gaussian Process Model,IEEE,Conferences,"Probabilistic modeling of correlated neural population firing activity is central to understanding the neural code and building practical decoding algorithms, the accurate reconstruction of a continuous motion signal is necessary for the control of devices such as computer cursors, robots, or a patient's own paralyzed limbs. For such applications we developed a realtime system that uses Gaussian process techniques to estimate hand motion from the firing rates of multiple neurons. Gaussian Processes for Machine Learning presents one of the most important Bayesian machine learning approaches based on a particularly effective method for placing a prior distribution over the space of functions. Decoding was performed using Gaussian processes model which gives an efficient method for Bayesian inference when the firing rates and hand kinematics are nonlinear. Gaussian processes provide a principled, practical, probabilistic approach to learning in noisy measurements. In off-line experiments, the Gaussian processes model reconstructions of hand trajectory were more accurate than previously reported results. The resulting decoding algorithm provides a principled probabilistic model of motor-cortical coding, decodes hand motion in real time, provides an estimate of uncertainty, and is straight to implement. Additionally the formulation unifies and extends previous models of neural coding while providing insights into the motor-cortical code.",https://ieeexplore.ieee.org/document/4535576/,2008 2nd International Conference on Bioinformatics and Biomedical Engineering,16-18 May 2008,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IJCNN52387.2021.9534268,Deep Embedded Clustering of Urban Communities Using Federated Learning,IEEE,Conferences,"Deep clustering utilizes representation learning to learn features in an unsupervised setting. Although successful, the current models rely on the assumption of the centralized dataset, which due to the privacy concerns is becoming less realistic. To address this challenge, we propose a federated deep convolutional embedded clustering framework. Our framework relies on a federated server to orchestrate the training between workers where each participant individually trains the model with the objective of decreasing clustering loss using Kullback&#x2013;Leibler divergence. To avoid feature space being distorted by the clustering loss, each worker maintains their own local decoder which for privacy reasons is not shared with the federated server. Empirical results with both IID and non-IID client data on benchmark datasets demonstrates the feasibility of our federated training when compared to the centralized counterpart. We also evaluate our model on a real world application of community detection using GPS traces and measure the computational complexity and energy consumption on a smartphone.",https://ieeexplore.ieee.org/document/9534268/,2021 International Joint Conference on Neural Networks (IJCNN),18-22 July 2021,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CCDC49329.2020.9164231,Deep Generative Model for Malware Detection,IEEE,Conferences,"Malware detection play different roles in a computer system and exhibit high degrees of importance with respect to system security. Malware detection is the process of attempting to infer the reputation score of the files via the applications. However, malware detection approaches are challenged by the large, dynamic, and heterogeneous space of benign binaries that they must track. In this research, we use deep generative models to develop two a semi-supervised Bayesian models for malware detection, in which we model the data generating process to be dependent on a Gaussian mixture. Furthermore, we propose the efficient stochastic gradient optimization technique used in deep generative models makes our model suitable for large data sets. Extensive experimental results on one real-world dataset demonstrate that our model is the effectiveness. Moreover, the semi-supervised deep generative scheme achieves comparable or even better results in malware detection when compared with classic and alternative machine learning models. This demonstrates the feasibility of deep generative model and presents a promising new approach to malware detection.",https://ieeexplore.ieee.org/document/9164231/,2020 Chinese Control And Decision Conference (CCDC),22-24 Aug. 2020,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/TENSYMP52854.2021.9550904,Deep Learning based Smart Parking for a Metropolitan Area,IEEE,Conferences,"In this study, we have introduced a method for utilizing the maximum parking space available for a metropolitan city. This will result in much lesser traffic congestion due to street-side parking. Furthermore, it will also decrease the hassle drivers face when they have to leave their vehicles on the side of the road to do other activities. The method introduces a Deep Learning based system where parking spaces are detected using Data Capturing Units (DCU). These DCUs feed data into our database which can be accessed by the users from our mobile application. The users can book parking spaces accordingly. All these data are saved in real-time and can be accessed through the mobile application. A vehicle classification system has also been designed that achieves an accuracy of 77% from multiple vehicle classes. Furthermore, a number plate recognition system has been used for the identification and safety protocols of the vehicles in parking sites. The number plate identification system is very precise and achieves an accuracy of over 90% for each digit. To the best of our knowledge, no other system of this kind has been implemented for the city of Dhaka before this. On top of that, successful implementation in a hectic city like Dhaka implies that it can be applied anywhere in the world. We believe this system can have a huge impact in reducing traffic congestions and can save an endless measure of time and money for citizens in a metropolitan area.",https://ieeexplore.ieee.org/document/9550904/,2021 IEEE Region 10 Symposium (TENSYMP),23-25 Aug. 2021,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICDMW.2015.121,Deep Learning for Image Retrieval: What Works and What Doesn't,IEEE,Conferences,"To build an industrial content-based image retrieval system (CBIRs), it is highly recommended that feature extraction, feature processing and feature indexing need to be fully considered. Although research that bloomed in the past years suggest that the convolutional neural network (CNN) be in a leading position on feature extraction & representation for CBIRs, there are less instructions on the deep analysis of feature related topics, for example the kind of feature representation that has the best performance among the candidates provided by CNN, the extracted features generalization ability, the relationship between the dimensional reduction and the accuracy loss in CBIRs, the best distance measure technique in CBIRs and the benefit of the coding techniques in improving the efficiency of CBIRs, etc. Therefore, several practicing studies were conducted and a thorough analysis was made in this research attempting to answer the above questions. The results in the study on both ImageNet-2012 and an industrial dataset provided by Sogou demonstrate that fc4096a and fc4096b perform the best on the datasets from unseen categories. Several interesting and practicing conclusions are drawn, for instance, fc4096a and fc4096b are found to have a better generalization ability than other features of CNN and could be considered as the first choice for industrial CBIRs. Furthermore, a novel feature binarization approach is presented in this paper for better efficiency of CBIRs. More specifically, the binarization is capable of reducing 31/32 space usage of original data. To sum up, the conclusions seem to provide practical instructions on real industrial CBIRs.",https://ieeexplore.ieee.org/document/7395863/,2015 IEEE International Conference on Data Mining Workshop (ICDMW),14-17 Nov. 2015,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICMIM48759.2020.9299052,Deep Open Space Segmentation using Automotive Radar,IEEE,Conferences,"In this work, we propose the use of radar with advanced deep segmentation models to identify open space in parking scenarios. A publically available dataset of radar observations called SCORP was collected. Deep models are evaluated with various radar input representations. Our proposed approach achieves low memory usage and real-time processing speeds, and is thus very well suited for embedded deployment.",https://ieeexplore.ieee.org/document/9299052/,2020 IEEE MTT-S International Conference on Microwaves for Intelligent Mobility (ICMIM),23-23 Nov. 2020,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA48506.2021.9561145,Deep Reinforcement Learning Framework for Underwater Locomotion of Soft Robot,IEEE,Conferences,"Soft robotics is an emerging technology with excellent application prospects. However, due to the inherent compliance of the materials used to build soft robots, it is extremely complicated to control soft robots accurately. In this paper, we introduce a data-based control framework for solving the soft robot underwater locomotion problem using deep reinforcement learning (DRL). We first built a soft robot that can swim based on the dielectric elastomer actuator (DEA). We then modeled it in a simulation for the purpose of training the neural network and tested the performance of the control framework through real experiments on the robot. The framework includes the following: a simulation method for the soft robot that can be used to collect data for training the neural network, the neural network controller of the swimming robot trained in the simulation environment, and the computer vision method to collect the observation space from the real robot using a camera. We confirmed the effectiveness of the learning method for the soft swimming robot in the simulation environment by allowing the robot to learn how to move from a random initial state to a specific direction. After obtaining the trained neural network through the simulation, we deployed it on the real robot and tested the performance of the control framework. The soft robot successfully achieved the goal of moving in a straight line in disturbed water. The experimental results suggest the potential of using deep reinforcement learning to improve the locomotion ability of mobile soft robots.",https://ieeexplore.ieee.org/document/9561145/,2021 IEEE International Conference on Robotics and Automation (ICRA),30 May-5 June 2021,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.23919/SoftCOM50211.2020.9238313,Deep Semantic Image Segmentation for UAV-UGV Cooperative Path Planning: A Car Park Use Case,IEEE,Conferences,"Navigation of Unmanned Ground Vehicles (UGV) in unknown environments is an active area of research for mobile robotics. A main hindering factor for UGV navigation is the limited range of the on-board sensors that process only restricted areas of the environment at a time. In addition, most existing approaches process sensor information under the assumption of a static environment. This restrains the exploration capability of the UGV especially in time-critical applications such as search and rescue. The cooperation with an Unmanned Aerial Vehicle (UAV) can provide the UGV with an extended perspective of the environment which enables a better-suited path planning solution that can be adjusted on demand. In this work, we propose a UAV-UGV cooperative path planning approach for dynamic environments by performing semantic segmentation on images acquired from the UAV’s view via a deep neural network. The approach is evaluated in a car park scenario, with the goal of providing a path plan to an empty parking space for a ground-based vehicle. The experiments were performed on a created dataset of real-world car park images located in Croatia and Germany, in addition to images from a simulated environment. The segmentation results demonstrate the viability of the proposed approach in producing maps of the dynamic environment on demand and accordingly generating path plans for ground-based vehicles.",https://ieeexplore.ieee.org/document/9238313/,"2020 International Conference on Software, Telecommunications and Computer Networks (SoftCOM)",17-19 Sept. 2020,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICASSP.2017.7952155,Deep attractor network for single-microphone speaker separation,IEEE,Conferences,"Despite the overwhelming success of deep learning in various speech processing tasks, the problem of separating simultaneous speakers in a mixture remains challenging. Two major difficulties in such systems are the arbitrary source permutation and unknown number of sources in the mixture. We propose a novel deep learning framework for single channel speech separation by creating attractor points in high dimensional embedding space of the acoustic signals which pull together the time-frequency bins corresponding to each source. Attractor points in this study are created by finding the centroids of the sources in the embedding space, which are subsequently used to determine the similarity of each bin in the mixture to each source. The network is then trained to minimize the reconstruction error of each source by optimizing the embeddings. The proposed model is different from prior works in that it implements an end-to-end training, and it does not depend on the number of sources in the mixture. Two strategies are explored in the test time, K-means and fixed attractor points, where the latter requires no post-processing and can be implemented in real-time. We evaluated our system on Wall Street Journal dataset and show 5.49% improvement over the previous state-of-the-art methods.",https://ieeexplore.ieee.org/document/7952155/,"2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",5-9 March 2017,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SAI.2017.8252172,Deep learning teachology for the prediction of solar flares from GOES data,IEEE,Conferences,"Predicting solar storms from real-time satellites data is extremely important for the protection of variousaviation, power and communication infrastructures. In this paper deep learning technologyis applied, for the first time, tothe real-time prediction of solar flares by analyzing the x-ray flux (1-minute cadence) time-series data from the satellite GOES (Geostationary Operational Environmental Satellite). The prediction system introduced here consists of 2units. The first converts GOES data to Markov Transition Field (MTF) images. An unsupervised feature learning algorithm and the prediction, flare or no-flare, are implemented by a Convolutional Neural Network in the second unit. Severalevaluation metrics, as required by space weather specialists, are applied to evaluate the performance of the system.",https://ieeexplore.ieee.org/document/8252172/,2017 Computing Conference,18-20 July 2017,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CNS48642.2020.9162219,DeepBLOC: A Framework for Securing CPS through Deep Reinforcement Learning on Stochastic Games,IEEE,Conferences,"One important aspect in protecting Cyber Physical System (CPS) is ensuring that the proper control and measurement signals are propagated within the control loop. The CPS research community has been developing a large set of check blocks that can be integrated within the control loop to check signals against various types of attacks (e.g., false data injection attacks). Unfortunately, it is not possible to integrate all these “checks” within the control loop as the overhead introduced when checking signals may violate the delay constraints of the control loop. Moreover, these blocks do not completely operate in isolation of each other as dependencies exist among them in terms of their effectiveness against detecting a subset of attacks. Thus, it becomes a challenging and complex problem to assign the proper checks, especially with the presence of a rational adversary who can observe the check blocks assigned and optimizes her own attack strategies accordingly. This paper tackles the inherent state-action space explosion that arises in securing CPS through developing DeepBLOC (DB)-a framework in which Deep Reinforcement Learning algorithms are utilized to provide optimal/sub-optimal assignments of check blocks to signals. The framework models stochastic games between the adversary and the CPS defender and derives mixed strategies for assigning check blocks to ensure the integrity of the propagated signals while abiding to the real-time constraints dictated by the control loop. Through extensive simulation experiments and a real implementation on a water purification system, we show that DB achieves assignment strategies that outperform other strategies and heuristics.",https://ieeexplore.ieee.org/document/9162219/,2020 IEEE Conference on Communications and Network Security (CNS),29 June-1 July 2020,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SANER.2019.8668044,DeepCT: Tomographic Combinatorial Testing for Deep Learning Systems,IEEE,Conferences,"Deep learning (DL) has achieved remarkable progress over the past decade and has been widely applied to many industry domains. However, the robustness of DL systems recently becomes great concerns, where minor perturbation on the input might cause the DL malfunction. These robustness issues could potentially result in severe consequences when a DL system is deployed to safety-critical applications and hinder the real-world deployment of DL systems. Testing techniques enable the robustness evaluation and vulnerable issue detection of a DL system at an early stage. The main challenge of testing a DL system attributes to the high dimensionality of its inputs and large internal latent feature space, which makes testing each state almost impossible. For traditional software, combinatorial testing (CT) is an effective testing technique to balance the testing exploration effort and defect detection capabilities. In this paper, we perform an exploratory study of CT on DL systems. We propose a set of combinatorial testing criteria specialized for DL systems, as well as a CT coverage guided test generation technique. Our evaluation demonstrates that CT provides a promising avenue for testing DL systems.",https://ieeexplore.ieee.org/document/8668044/,"2019 IEEE 26th International Conference on Software Analysis, Evolution and Reengineering (SANER)",24-27 Feb. 2019,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/BigData.2018.8622372,DeepFP: A Deep Learning Framework For User Fingerprinting via Mobile Motion Sensors,IEEE,Conferences,"In this paper, we propose a deep learning framework for user fingerprinting via mobile motion sensors, DeepFP, which can identify and track users based on their behavioral patterns while interacting with the smartphone. Existing machine learning techniques for user identification are classification-oriented and thus are not amenable easily to large-scale, real world deployment. They need to be trained on all the users whom they want to identify. DeepFP exploits metric learning techniques and deep neural networks to address the challenges of current user identification techniques. We leverage feature embedding to directly extract informative features and map input samples to a discriminative lower-dimensional space, where recurrent neural networks are used to model the temporal information of data. DeepFP does not need to re-train to identify new users which makes it feasible to be used in real world scenarios with a huge number of users, without needing a large number of training samples. Experiments on a publicly available mobile sensors dataset and comparison with other embedding methods depict the effectiveness of DeepFP.",https://ieeexplore.ieee.org/document/8622372/,2018 IEEE International Conference on Big Data (Big Data),10-13 Dec. 2018,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1145/3240765.3240791,DeepFense: Online Accelerated Defense Against Adversarial Deep Learning,IEEE,Conferences,"Recent advances in adversarial Deep Learning (DL) have opened up a largely unexplored surface for malicious attacks jeopardizing the integrity of autonomous DL systems. With the wide-spread usage of DL in critical and time-sensitive applications, including unmanned vehicles, drones, and video surveillance systems, online detection of malicious inputs is of utmost importance. We propose DeepFense, the first end-to-end automated framework that simultaneously enables efficient and safe execution of DL models. DeepFense formalizes the goal of thwarting adversarial attacks as an optimization problem that minimizes the rarely observed regions in the latent feature space spanned by a DL network. To solve the aforementioned minimization problem, a set of complementary but disjoint modular redundancies are trained to validate the legitimacy of the input samples in parallel with the victim DL model. DeepFense leverages hardware/software/algorithm co-design and customized acceleration to achieve just-in-time performance in resource-constrained settings. The proposed countermeasure is unsupervised, meaning that no adversarial sample is leveraged to train modular redundancies. We further provide an accompanying API to reduce the non-recurring engineering cost and ensure automated adaptation to various platforms. Extensive evaluations on FPGAs and GPUs demonstrate up to two orders of magnitude performance improvement while enabling online adversarial sample detection.",https://ieeexplore.ieee.org/document/8587702/,2018 IEEE/ACM International Conference on Computer-Aided Design (ICCAD),5-8 Nov. 2018,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/DASC-PICom-DataCom-CyberSciTec.2017.200,DeepSim: Cluster Level Behavioral Simulation Model for Deep Learning,IEEE,Conferences,"We are witnessing an explosion of AI based use cases driving the computer industry, and especially datacenter and server architectures. As Intel faces fierce competition in this emerging technology space, it is important that architecture definitions and directions are driven with data from proper tools and methodologies, and insights are drawn from end-to-end holistic analysis at the datacenter levels. In this paper, we introduce DeepSim, a cluster-level behavioral simulation model for deep learning. DeepSim, which is based on the Intel CoFluent simulation framework, uses timed behavioral models to simulate complex interworking between compute nodes, networking, and storage at the datacenter level, providing a realistic performance model of a real-world image recognition applications based on the popular Deep Learning Framework Caffe. The end-to-end simulation data from DeepSim provides insight which can be used for architecture analysis driving future datacenter architecture directions. DeepSim enables scalable system design, deployment, and capacity planning through accurate performance insights. Results from preliminary scaling studies (e.g. node scaling and network scaling) and what-if analyses (e.g., Xeon with HBM and Xeon Phi with dual OPA) are presented in this paper. The simulation results are correlated well with empirical measurements, achieving an accuracy of 95%.",https://ieeexplore.ieee.org/document/8328544/,"2017 IEEE 15th Intl Conf on Dependable, Autonomic and Secure Computing, 15th Intl Conf on Pervasive Intelligence and Computing, 3rd Intl Conf on Big Data Intelligence and Computing and Cyber Science and Technology Congress(DASC/PiCom/DataCom/CyberSciTech)",6-10 Nov. 2017,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICMLC.2008.4620589,Defending against tcp syn flooding with a new kind of syn-agent,IEEE,Conferences,"TCP-based flooding attack is a common form of Denial-of-Service (DoS) attacks which abuses network resources and may bring serious threats to the network. The SYN flood attack is a DoS method affecting hosts to retain the half-open state and exhaust its memory resources. This attack is hard to be filtered by the routers in case that the source IP address is always spoofed. There are some common ways to defend against this attach, but all of them either requires a high-performance firewall or trade time for space. In this paper, we proposed a method to build a new kind of syn-agent which uses the TCP header reserved flag bits to notify the server a complete three-way TCP handshake. First the syn-agent instead of the real server answer the client with ACK after received a SYN packet from the client. Then if it is a syn-attack, there should be no further ACKs after this. After a given short period, the half-open TCP sock should be deleted from the agent. If it is a really connection request, after the third time handshake packet arrived, the agent set the reserved bit in the TCP header to be ‘1’ and route the packet to the real server. When the server received a packet with the reserved bits set to be ‘1’, it directly allocates memory for the connection and begins to communicate.",https://ieeexplore.ieee.org/document/4620589/,2008 International Conference on Machine Learning and Cybernetics,12-15 July 2008,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AHS.2018.8541460,Delay Tolerant Network Routing as a Machine Learning Classification Problem,IEEE,Conferences,"This paper discusses a machine learning-based approach to routing for delay tolerant networks (DTNs) [1]. DTNs are networks which experience frequent disconnections between nodes, uncertainty of an end-to-end path, long one-way trip times, and may have high error rates and asymmetric links. Such networks exist in deep space satellite networks, very rural environments, disaster areas and underwater environments. In this work, we use machine learning classifiers to predict a set of neighboring nodes which are the most likely to deliver a message to a desired location based on message history delivery information. We use the Common Open Research Emulator (CORE) [2] to emulate the DTN environment based on real-world location traces and collect network traffic statistics from the Bundle Protocol implementation IBR-DTN [3]. The software architecture for classification-based routing, analysis and preparation of the network history data and prediction results are discussed.",https://ieeexplore.ieee.org/document/8541460/,2018 NASA/ESA Conference on Adaptive Hardware and Systems (AHS),6-9 Aug. 2018,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CAIA.1990.89189,Demonstrating artificial intelligence for space systems-integration and project management issues,IEEE,Conferences,"As part of its systems autonomy demonstration project (SADP), the National Aeronautics and Space Administration (NASA) has recently demonstrated the Thermal Expert System (TEXSYS). Advanced real-time expert system and human interface technology was successfully developed and integrated with conventional controllers of prototype space hardware to provide intelligent fault detection, isolation and recovery capability. Many specialized skills were required, and responsibility for the various phases of the project therefore spanned multiple NASA centers, internal departments and contractor organizations. The test environment required communication among many types of hardware and software as well as between many people. The integration, testing, and configuration management tools and methodologies which were applied to the TEXSYS project to assure its safe and successful completion are detailed. The project demonstrated that artificial intelligence technology, including model-based reasoning, is capable of the monitoring and control of a large, complex system in real time.<>",https://ieeexplore.ieee.org/document/89189/,Sixth Conference on Artificial Intelligence for Applications,5-9 May 1990,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCAD51958.2021.9643473,Demystifying the Characteristics of High Bandwidth Memory for Real-Time Systems,IEEE,Conferences,"The number of functionalities controlled by software on every critical real-time product is on the rise in domains like automotive, avionics and space. To implement these advanced functionalities, software applications increasingly adopt artificial intelligence algorithms that manage massive amounts of data transmitted from various sensors. This translates into unprecedented memory performance requirements in critical systems that the commonly used DRAM memories struggle to provide. High-Bandwidth Memory (HBM) can satisfy these requirements offering high bandwidth, low power and high-integration capacity features. However, it remains unclear whether the predictability and isolation properties of HBM are compatible with the requirements of critical embedded systems. In this work, we perform to our knowledge the first timing analysis of HBM. We show the unique structural and timing characteristics of HBM with respect to DRAM memories and how they can be exploited for better time predictability, with emphasis on increased isolation among tasks and reduced worst-case memory latency.",https://ieeexplore.ieee.org/document/9643473/,2021 IEEE/ACM International Conference On Computer Aided Design (ICCAD),1-4 Nov. 2021,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA.2016.7487691,Denoising auto-encoders for learning of objects and tools affordances in continuous space,IEEE,Conferences,"The concept of affordances facilitates the encoding of relations between actions and effects in an environment centered around the agent. Such an interpretation has important impacts on several cognitive capabilities and manifestations of intelligence, such as prediction and planning. In this paper, a new framework based on denoising Auto-encoders (dA) is proposed which allows an agent to explore its environment and actively learn the affordances of objects and tools by observing the consequences of acting on them. The dA serves as a unified framework to fuse multi-modal data and retrieve an entire missing modality or a feature within a modality given information about other modalities. This work has two major contributions. First, since training the dA is done in continuous space, there will be no need to discretize the dataset and higher accuracies in inference can be achieved with respect to approaches in which data discretization is required (e.g. Bayesian networks). Second, by fixing the structure of the dA, knowledge can be added incrementally making the architecture particularly useful in online learning scenarios. Evaluation scores of real and simulated robotic experiments show improvements over previous approaches while the new model can be applied in a wider range of domains.",https://ieeexplore.ieee.org/document/7487691/,2016 IEEE International Conference on Robotics and Automation (ICRA),16-21 May 2016,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CEC48606.2020.9185611,DenseDisp: Resource-Aware Disparity Map Estimation by Compressing Siamese Neural Architecture,IEEE,Conferences,"Stereo vision cameras are flexible sensors due to providing heterogeneous information such as color, luminance, disparity map (depth), and shape of the objects. Today, Convolutional Neural Networks (CNNs) present the highest accuracy for the disparity map estimation [1]. However, CNNs require considerable computing capacity to process billions of floating-point operations in a real-time fashion. Besides, commercial stereo cameras produce huge size images (e.g., 10 Megapixels [2]), which impose a new computational cost to the system. The problem will be pronounced if we target resource-limited hardware for the implementation. In this paper, we propose DenseDisp, an automatic framework that designs a Siamese neural architecture for disparity map estimation in a reasonable time. DenseDisp leverages a meta-heuristic multi-objective exploration to discover hardware-friendly architectures by considering accuracy and network FLOPS as the optimization objectives. We explore the design space with four different fitness functions to improve the accuracy-FLOPS trade-off and convergency time of the DenseDisp. According to the experimental results, DenseDisp provides up to 39. 1x compression rate while losing around 5% accuracy compared to the state-of-the-art results.",https://ieeexplore.ieee.org/document/9185611/,2020 IEEE Congress on Evolutionary Computation (CEC),19-24 July 2020,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/MLBDBI54094.2021.00084,Density Peaks Clustering Algorithm for Large-scale Data Based on Divide-and-Conquer Strategy,IEEE,Conferences,"Density peaks clustering algorithm is a simple but effective clustering method, which requires fewer parameters and iteration, and can determine the number of clusters. But this algorithm has high complexity of time and space which makes it is unsuitable to cluster large-scale data. So that this paper proposes a density peaks clustering algorithm based on the divide-and-conquer strategy. Firstly, divides the large-scale data into a series of data blocks consistent with the original data distribution, then performs density peaks clustering on a randomly selected block to get the clustering center of the data block. Since the data block has the same distribution with the original large-scale data, the clustering center can be used as the clustering center of the original data. Finally, allocates the remaining data blocks to the corresponding clustering center to obtain the final clustering result. Experimental results on real and synthetic datasets verify the effectiveness of the proposed algorithm.",https://ieeexplore.ieee.org/document/9731032/,"2021 3rd International Conference on Machine Learning, Big Data and Business Intelligence (MLBDBI)",3-5 Dec. 2021,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IES53407.2021.9594053,Design And Development of Human Anatomy Learning Platform for Medical Students Based On Augmented Intelligence Technology,IEEE,Conferences,"Augmented Intelligence technology was introduced for the task of helping improve human work in various fields, one of which is education. Several problems in the learning process, which are currently completely virtual, raise new problems, especially related to practicums which require teaching modules as guidelines for practicum implementation but still maintain the impression of interactive learning. For this reason, Augmented Reality technology is applied as a solution to build a practical human anatomy module, then called AIVE Platform embedded in smartphones to provide informative and immersive learning that can be run indoors or outdoors so that it is not limited by space and time. This platform can run on Android and iOS which is built on the AR Foundation framework to work across platforms. This module has followed the rules of the anatomical atlas that include labels on each part, there is also a login system to store student usage history, as well as the choice of learning mode. This module has been licensed to operate from a teaching doctor in anatomy to be used as a teaching module. The PIECES framework used to analyze the importance and satisfaction level of the platform gives score 4.085 out of 5 on and 4.081 out of 5 respectively.",https://ieeexplore.ieee.org/document/9594053/,2021 International Electronics Symposium (IES),29-30 Sept. 2021,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CVIDLICCEA56201.2022.9825229,Design and Deployment of Space Surveillance System to Monitor Geosynchronous Orbit,IEEE,Conferences,"This paper proposes a space surveillance system deployment method for detecting geosynchronous orbit targets. To analyze the combined influence of major factors on the node deployment, this research focuses on the influence of node deployment in different latitude bands on the tilt angles of phased array systems, scan coverage, maximum detectable distance and accuracy of target detection in geosynchronous orbit. So, considering these three evaluation criteria, a multi-objective optimization mathematical model is established for the estimation of node deployment reliability in this study. Then a particle swarm optimization algorithm (PSO) is used to solve this model. And then, the surveillance station deployment optimization is conducted with reference to optimization value, which can meet the needs of the detection in geosynchronous (GEO) orbit with better coverage and take into account the optimal power space surveillance system, and also ensure the accuracy of the target position. Finally, a quasi-real-time simulation for space surveillance is established by using STK and MATLAB software. This research is greatly useful to conduct the deployment of the space surveillance system.",https://ieeexplore.ieee.org/document/9825229/,"2022 3rd International Conference on Computer Vision, Image and Deep Learning & International Conference on Computer Engineering and Applications (CVIDL & ICCEA)",20-22 May 2022,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CAC53003.2021.9727460,Design and Implementation of Automatic Transport Control System for Warehouse with High Spatiotemporal Coupling Constraints,IEEE,Conferences,"In a logistics warehouse with high storage density, conflicts between items often occur during warehouse management due to the coupling of space between transportations and storages. This paper designs and implements a warehouse automatic transport control system (ATCS). Besides, a special path planning strategy based on Reverse Escort Algorithm (REA) is applied in the target warehouse. Based on the realistic warehouse model, the algorithm is improved to be more convenient for practical implementation. Simulation results prove the effectiveness and optimization effect of the proposed path planning algorithm.",https://ieeexplore.ieee.org/document/9727460/,2021 China Automation Congress (CAC),22-24 Oct. 2021,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISKE47853.2019.9170453,Design and Implementation of On-orbit Valuable Image Extraction for the TianZhi-1 Satellite,IEEE,Conferences,"Recently, software-defined satellite has become a research hotspot in the aerospace. Based on an advanced computing platform with open system architecture, researchers can upload software for specific tasks even the satellite has been launched into space. This paper we have designed an on-orbit application for China's first software-defined satellite TianZhi-1, which use Android smartphone as a system platform. Two main tasks are focused on our work, one is to reduce data redundancy and the other is to compress the size of the software. First, a light-weight and extensible framework is designed to support different image processing algorithms. Following this, we propose a three-step approach for on-orbit valuable image extraction, include image denoising, stitching, and salient object extraction. Experiments on the real satellite achieve outstanding results.",https://ieeexplore.ieee.org/document/9170453/,2019 IEEE 14th International Conference on Intelligent Systems and Knowledge Engineering (ISKE),14-16 Nov. 2019,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AINIT54228.2021.00134,Design and Implementation of Virtual Animation Based on Unity3D,IEEE,Conferences,"This project uses Unity3D engine to make VR virtual reality experiment, uses UG NX to model the experiment model, uses Shadow SDK plug-in to make the glasses show binocular effects, particle system can add special effects to the experiment, it is the second rendering of the three-dimensional space Three-dimensional images are mainly used to render different experimental requirements for effects such as smoke, lire, water droplets, fallen leaves, etc., through the use of dotween plug-in in unity3D to make the experiment, the experiment is presented in VR glasses, which greatly promotes VR Teaching development.",https://ieeexplore.ieee.org/document/9725101/,"2021 2nd International Seminar on Artificial Intelligence, Networking and Information Technology (AINIT)",15-17 Oct. 2021,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ETFA.2015.7301549,Design and implementation for multiple-robot deployment in intelligent space,IEEE,Conferences,"This paper presents the problem of robot deployment for a number of scattered tasks. We aim to minimize the duration it takes for all robots to reach their assigned task locations. In previous work, we have proposed a team composed of one carrier robot (CR) and several servant robots to accomplish the mission. Then we have suggested an algorithm that determines a path of the CR for an efficient deployment under a few constraints, which is verified by simulations. Assuming that the servant robots are unmanned aerial vehicles (UAVs), the present paper extends the discussion to a real robot experiment. We design and implement a deployment system in intelligent space. The feasibility of the study is demonstrated through an experiment.",https://ieeexplore.ieee.org/document/7301549/,2015 IEEE 20th Conference on Emerging Technologies & Factory Automation (ETFA),8-11 Sept. 2015,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICBAIE52039.2021.9389950,Design of Cloud Computing Platform Based Accurate Measurement for Structure Monitoring Using Fiber Bragg Grating Sensors,IEEE,Conferences,"The efficient integration of distributed fiber Bragg grating (FBG) sensors and cloud computing platform is used to achieve accurate measurement and evaluation of physical quantities, which solves the problems of traditional fiber Bragg grating sensing technology for health structure monitoring system, such as the cost and space constraints, it is difficult to deploy enough servers to deal with data collection, transmission and storage in real time. The cloud platform using fiber Bragg grating sensors adopts the structure of erbium-doped fiber cascaded Bragg grating, reasonably configures the FBG demodulator acquisition and analysis software, deploys the health monitoring system in the cloud, constructs the cloud platform of high-efficiency health monitoring optical fiber sensor network, improves the scalability of the system, flexibly deploys applications and services, and ensures the security and reliability of various real-time monitoring data and professional data. It can meet the needs of some specific or wide application fields for the automation technology, structural mechanics, computer technology, Internet architecture, cloud deployment and interdisciplinary practical comprehensive valuable application research.",https://ieeexplore.ieee.org/document/9389950/,"2021 IEEE 2nd International Conference on Big Data, Artificial Intelligence and Internet of Things Engineering (ICBAIE)",26-28 March 2021,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AIEA53260.2021.00013,Design of a Real-time Robot Control System oriented for Human-Robot Cooperation,IEEE,Conferences,"An open real-time control system based on the EtherCAT fieldbus communication technology is proposed to fulfill the high real-time requirement of the human-robot cooperation controller in this paper. An open source real-time kernel of Xenomai is employed as the real-time software platform of the robot control system. Based on this, four-layer interfaces architecture are accomplished, which are human-machine cooperation control layer, motion control layer, robot axis control layer and hardware abstraction layer, through the corresponding four real-time tasks to meet the demand of human-robot cooperation operations. In addition, the scheduling task is developed to manage the 4 real-time tasks. The dual buffer communication mechanisms and priority-based scheduling strategy between layers was exploited to synchronize these real-time tasks. The underlying hardware abstract interface and the human-robot collaborative control algorithm interface are opened in the control system as the quadric exploitation interfaces to meet the need of developing application tasks in real-time space. Experiment results which are conducted on a self-developed 6-DOF collaborative robot show that the proposed control system is effective in real-time control applications of human-robot cooperative control at the control cycle of 5 milliseconds.",https://ieeexplore.ieee.org/document/9525600/,2021 International Conference on Artificial Intelligence and Electromechanical Automation (AIEA),14-16 May 2021,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCS.2008.4737145,Design of new minimum decoding complexity quasi-orthogonal Space-Time Block Code for four transmit antennas,IEEE,Conferences,"A new Space-Time Block Code (STBC) achieving full rate and full diversity for general QAM and four transmit antennas is proposed. This code also possesses a quasi-orthogonal (QO) property like the conventional Minimum Decoding Complexity QO-STBC (MDC-QO-STBC), leading to joint ML detection of only two real symbols. The proposed code is shown to exhibit the identical error performance with the existing MDC-QO-STBC. However, the proposed code has an advantage in the transceiver implementation since this code can be modified so that the increase of PAPR occurs at only two transmit antennas, but the MDC-QO-STBC at all of transmit antennas.",https://ieeexplore.ieee.org/document/4737145/,2008 11th IEEE Singapore International Conference on Communication Systems,19-21 Nov. 2008,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ITAIC.2019.8785568,Design of on-line monitoring system for catenary compensation device,IEEE,Conferences,"The safe operation of high-speed railway is directly affected by the reliability of catenary. Manual inspection is time/labor consumptive, bad real-time and etc. To solve these problems, a non-contact intelligent remote control monitoring system based on laser sensor is devised in this paper which can effectively monitor the catenary status and send alarm messages in time. On the respect of performance improvement, GPRS Network Communication Technology is applied to reduce the difficulty and cost of the access to the Internet of the intelligent terminal. A SD card is used to expand the store space to enhance the capacity of batch data processing. In order to decrease the consumption of outdoor electrical energy, a relay is utilized in circuit to control the turn-on and off of the solar power. At the same time, the control software is capable to analyze and store massive data with database technology. It turns out that the system has high reliability and real-time performance in functions such as acquiring accurate data and alarming timely by means of the spot test.",https://ieeexplore.ieee.org/document/8785568/,2019 IEEE 8th Joint International Information Technology and Artificial Intelligence Conference (ITAIC),24-26 May 2019,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IJCNN.2012.6252851,Design of the Self-Constructing Fuzzy Neural Network controller for a sliding door system,IEEE,Conferences,"In this paper, the Self-Constructing Fuzzy Neural Network (SCFNN) controller suitable for real-time control of the speed control of the slide door is presented to track reference model. The structure and parameter learning can be done automatically and online. The structure learning is accordance with the partition of input space (error and change of error), and the parameter learning is based on the supervised gradient decent method. In this paper, the weights of SCFNN are generated from functional-link-based neural network (FLNN). The SCFNN adopted the FLNN, generating complex nonlinear combinations of input space to the weights of the SCFNN with FLNN. Finally, a slide door speed control system is implemented in this paper to verify the effectiveness of the proposed SCFNN with FLNN.",https://ieeexplore.ieee.org/document/6252851/,The 2012 International Joint Conference on Neural Networks (IJCNN),10-15 June 2012,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ASPDAC.2016.7428073,Design space exploration of FPGA-based Deep Convolutional Neural Networks,IEEE,Conferences,"Deep Convolutional Neural Networks (DCNN) have proven to be very effective in many pattern recognition applications, such as image classification and speech recognition. Due to their computational complexity, DCNNs demand implementations that utilize custom hardware accelerators to meet performance and energy-efficiency constraints. In this paper we propose an FPGA-based accelerator architecture which leverages all sources of parallelism in DCNNs. We develop analytical feasibility and performance estimation models that take into account various design and platform parameters. We also present a design space exploration algorithm for obtaining the implementation with the highest performance on a given platform. Simulation results with a real-life DCNN demonstrate that our accelerator outperforms other competing approaches, which disregard some sources of parallelism in the application. Most notably, our accelerator runs 1.9× faster than the state-of-the-art DCNN accelerator on the same FPGA device.",https://ieeexplore.ieee.org/document/7428073/,2016 21st Asia and South Pacific Design Automation Conference (ASP-DAC),25-28 Jan. 2016,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISCAS.2018.8351685,Design-Space Exploration of Pareto-Optimal Architectures for Deep Learning with DVFS,IEEE,Conferences,"Specialized computing engines are required to accelerate the execution of Deep Learning (DL) algorithms in an energy-efficient way. To adapt the processing throughput of these accelerators to the workload requirements while saving power, Dynamic Voltage and Frequency Scaling (DVFS) seems the natural solution. However, DL workloads need to frequently access the off-chip memory, which tends to make the performance of these accelerators memory-bound rather than computation-bound, hence reducing the effectiveness of DVFS. In this work we use a performance-power analytical model fitted on a parametrized implementation of a DL accelerator in a 28-nm FDSOI technology to explore a large design space and to obtain the Pareto points that maximize the effectiveness of DVFS in the sub-space of throughput and energy efficiency. In our model we consider the impact on performance and power of the off-chip memory using real data of a commercial low-power DRAM.",https://ieeexplore.ieee.org/document/8351685/,2018 IEEE International Symposium on Circuits and Systems (ISCAS),27-30 May 2018,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/COMPSAC.2008.76,Designing Simulated Context-Aware Telephone in Pervasive Spaces,IEEE,Conferences,"Personal context information is important to enable context-aware applications in pervasive spaces. As privacy, many investigators have pointed out that it must be in the controllable state by using personal privacy policy. The long term objective of this research was to extract privacy policy from personal context information database with machine learning method. A context-aware telephone system was designed to collect data, learn policy and examine the accuracy of policy. In the step of PDA simulation, the results showed that the method was possible to make suitable decision for telephone communication in consideration of privacy protection. In order to further validate the proposed privacy protection method, the pervasive space was designed and implemented to get more real data about using context-aware telephone. At the step of pervasive space simulation, we design and code the prototype of context-aware telephone system from the view of software engineering.",https://ieeexplore.ieee.org/document/4591762/,2008 32nd Annual IEEE International Computer Software and Applications Conference,28 July-1 Aug. 2008,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AERO.2017.7943637,Designing and implementing Machine Learning Algorithms for advanced communications using FPGAs,IEEE,Conferences,"Communications systems can obtain substantial benefits from increased intelligence. Improvements to communications include increased spectral situational awareness, spectral optimization, and robust operation in dynamic and demanding communications environments. Furthermore, complex communication systems require a high degree of autonomous intelligence to optimize performance under such varying conditions. Machine Learning Algorithms provide a means to increase the intrinsic intelligence of wideband communication systems. This paper considers the use of Machine Learning Algorithms to increase the intelligence of communication systems. Specifically, the focus of this paper is to sense and learn the communication environment in real-time and optimize system parameters to maximize end-to-end performance. Communications systems have existing adaptive capabilities in many subsystems such as equalization. The focus in this paper is top level system intelligence by learning from the environment, and based on the system capabilities determine an optimal mode in the solution space in real-time. Furthermore, the goal of this paper is to consider implementation of Machine Learning Algorithms using FPGAs. Design data for implementing Machine Learning Algorithms using FPGAs is provided in the paper as well as reference circuits for implementation. Finally, an example implementation of a Machine Learning Algorithm for intelligent communications is provided based on implementation in a Xilinx UltraScale FPGA.",https://ieeexplore.ieee.org/document/7943637/,2017 IEEE Aerospace Conference,4-11 March 2017,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/EMCTECH49634.2020.9261546,Designing of a Classifier for the Unstructured Text Formalization Model Based on Word Embedding,IEEE,Conferences,"The active use of artificial intelligence technologies has a direct positive impact on the development of society in various areas of human life. The article describes the developed model of processing and formalization of textual unstructured information in the form of a continuous flow of text information taken from the news feed of news agencies. A method for preprocessing text to reduce the execution time of the algorithm and save CPU resources is given. A method for representing words as a real vector is formed using various algorithms for training artificial neural networks and their properties. A model of the first stage of the text information processing system as a subsystem for classifying the subject of a news article text based on a vector representation of words, including a description of the word vectorization algorithm, an example of the type of word structure with a corresponding numeric vector, and a metric that determines the proximity of vectors to each other in space. The results of the experiment are obtained and a method for setting a decision criterion for the implemented classifier is proposed. The area of use of the proposed classifier is the sphere of information security. The results of the experiment can be indicators of the suitability of using the classifier as a definition of the subject of a news article.",https://ieeexplore.ieee.org/document/9261546/,2020 International Conference on Engineering Management of Communication and Technology (EMCTECH),20-22 Oct. 2020,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SAINT.2012.14,Detecting Malicious Websites by Learning IP Address Features,IEEE,Conferences,"Web-based malware attacks have become one of the most serious threats that need to be addressed urgently. Several approaches that have attracted attention as promising ways of detecting such malware include employing various blacklists. However, these conventional approaches often fail to detect new attacks owing to the versatility of malicious websites. Thus, it is difficult to maintain up-to-date blacklists with information regarding new malicious websites. To tackle this problem, we propose a new method for detecting malicious websites using the characteristics of IP addresses. Our approach leverages the empirical observation that IP addresses are more stable than other metrics such as URL and DNS. While the strings that form URLs or domain names are highly variable, IP addresses are less variable, i.e., IPv4 address space is mapped onto 4-bytes strings. We develop a lightweight and scalable detection scheme based on the machine learning technique. The aim of this study is not to provide a single solution that effectively detects web-based malware but to develop a technique that compensates the drawbacks of existing approaches. We validate the effectiveness of our approach by using real IP address data from existing blacklists and real traffic data on a campus network. The results demonstrate that our method can expand the coverage/accuracy of existing blacklists and also detect unknown malicious websites that are not covered by conventional approaches.",https://ieeexplore.ieee.org/document/6305258/,2012 IEEE/IPSJ 12th International Symposium on Applications and the Internet,16-20 July 2012,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/MECO.2018.8405951,Detecting and counteracting benign faults and malicious attacks in cyber physical systems,IEEE,Conferences,"The use of cyber-physical system (CPS) is rapidly expanding and many of their applications require a highly reliable and secure implementation as they control critical infrastructures or even life-critical devices. Unfortunately, current techniques for achieving high reliability and security incur high overheads. In particular, integrating countermeasures against security attacks is problematic as security threats are often not well defined, evolve continuously, and as a result, many CPSs often remain vulnerable. We propose to exploits the physical plant state information to enhance both reliability and security. Our approach, which monitors the controlled plant state trajectory, allows for tunable fault-tolerance as well as detection of malicious attacks, and it achieves these at a low overhead. The plant state space consists of safe and marginal state subspaces. In the safe subspace the CPS will continue its safe operation even if the worst case control signal is applied. In contrast, any erroneous control applied when the plant state is marginal, may lead to a catastrophic system failure. Such an erroneous control output may be due to either a benign fault or a malicious security attack. As most of the time the plant will be deep within its safe subspace, we can avoid using expensive redundancy techniques and thus, reduce the computational load while still guaranteeing safe operation. When a marginal state of the plant is detected, it will signal the potential presence of a “natural” fault or malicious attack. Our scheme will counter this by switching to a critical mode involving higher levels of redundancy to combat natural failures as well as alternative mechanisms to defeat malicious attacks. A major challenge in our approach is to determine, in real-time, whether the current state of the physical plant is deep within its safe sub-space or is marginal. We have used various machine learning techniques for classifying the state and our results indicate that with a reasonable number of entries in a lookup table and with a short execution time, the required classification can be performed efficiently.",https://ieeexplore.ieee.org/document/8405951/,2018 7th Mediterranean Conference on Embedded Computing (MECO),10-14 June 2018,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/LCN.2008.4664303,Detection of anomalous network packets using lightweight stateless payload inspection,IEEE,Conferences,"A real-time packet-level anomaly detection approach for high-speed network intrusion prevention is described. The approach is suitable for small and fast hardware implementation and was designed to be embedded in network appliances. Each network packet is characterized using a novel technique that efficiently maps the payload histogram onto a simple pair of features using hypercube hash functions, which were chosen for their implementation efficiency in both hardware and software. This two-dimensional feature space is quantized into a binary bitmap representing the normal and anomalous feature regions. The potential loss of accuracy due to the reduction in feature space is countered by the ability of the bitmaps to capture nearly arbitrary shaped regions in the feature space. These bitmaps are used as the classifiers for real-time detection. The proposed method is extremely efficient in both the offline machine learning and real-time detection components. Results using the 1999 DARPA Intrusion Detection Evaluation Data Set yield a 100% detection of all applicable attacks, with extremely low false positive rate. The approach is also evaluated on real traffic captures.",https://ieeexplore.ieee.org/document/4664303/,2008 33rd IEEE Conference on Local Computer Networks (LCN),14-17 Oct. 2008,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CAMSAP.2007.4497964,Determining the number of propagation paths from broadband mimo measurements via bootstrapped likelihoods and the false discovery rate criterion – part i: methodology,IEEE,Conferences,"In this paper, we propose a multiple hypotheses test for determining the number of propagation paths from broadband MIMO channel measurements. For this test, maximum– likelihood (ML) estimates for propagation delay, direction of arrival, direction of departure, and Doppler shifts are required for each potential number of propagation paths. The ML-estimator is implemented via a variant of the space alternating generalized expectation-maximization (SAGE) algorithm. The proposed test is based on the Benjamini– Hochberg procedure for guaranteeing a false discovery rate and employs the simple bootstrap approach for approximating the required p-values for the multiple test. In a companion paper, we apply the proposed test to real broadband MIMO antenna array measurements and discuss its performance.",https://ieeexplore.ieee.org/document/4497964/,2007 2nd IEEE International Workshop on Computational Advances in Multi-Sensor Adaptive Processing,12-14 Dec. 2007,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/COMPSAC.2018.10238,Development of Bicycle Simulator with Tilt Angle Control Tilt Angle,IEEE,Conferences,"Bicycles are a healthy and eco-friendly form of travel. Information about dangerous points (hazards) on roads is important for bicycle-riding safety. We proposed a method to detect road hazards using sensors attached to a bicycle in a prior conventional study. The conventional approach needs training data for machine learning, requiring the bicycle to travel over hazardous areas repeatedly. Not only is this is dangerous, it is also difficult to collect sufficiently large amounts of data. A bicycle simulator is a potential solution to this problem. Commercial bicycle simulators have been developed in Japan. However, the bicycle body is fixed, thus making the ride feel unnatural. This type of bicycle simulator is not suitable for gathering hazard data. We therefore propose a bicycle simulator capable of tilt angle control that runs in a three-dimensional (3D) virtual space. The built-in sensors send the speed and front-wheel angle information to the control unit. A new tilt angle is then calculated by the control unit and the information is sent to an AC servomotor to achieve this new tilt angle. Simultaneously, a 3D view of a virtual course is generated using the calculated results. By using our previously proposed rotation center tracking method, left and right steering actions can be observed by the system. This system allows for dangerous situations to be easily and repeatedly created with no danger to the rider.",https://ieeexplore.ieee.org/document/8377866/,2018 IEEE 42nd Annual Computer Software and Applications Conference (COMPSAC),23-27 July 2018,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRAIE51050.2020.9358310,Development of a Neural Network Library for Resource Constrained Speech Synthesis,IEEE,Conferences,"Machine learning frameworks, like Tensorflow and PyTorch, use GPU hardware acceleration to deliver the needed performance. Since GPUs require a lot of power (and space) to operate, typical use cases involve high-performance servers, with the final deployment available as a cloud service. To address limitations of this approach, AI Accelerators have been proposed. In this context, we have designed and implemented a library of neural network algorithms, to efficiently run on “edge devices”, with AI Accelerators. Moreover, a unified interface has been provided, to allow easy experimentation with various neural networks applied to the same dataset. Here, let us stress that we do not propose new algorithms, but port known ones to, resource restricted, edge devices. The context is provided by a speech synthesis application for edge devices that is deployed on an NVIDIA Jetson Nano. This application is to be used by social robots for real-time off-cloud text-to-speech processing.",https://ieeexplore.ieee.org/document/9358310/,2020 5th IEEE International Conference on Recent Advances and Innovations in Engineering (ICRAIE),1-3 Dec. 2020,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/BIYOMUT.2010.5479790,Diffusion tensor fiber tracking based on unsupervised learning,IEEE,Conferences,"Using Hebbian learning rule and its special case Self-Organizing Map (SOM) as unsupervised learning, a solution is proposed for defining the fiber paths which is a critical problem in diffusion tensor literature, and synthetic diffusion patterns are analyzed by artificial neural network (ANN) approach. Unsupervised learning in training neural networks is a method, where network classification rules are self developed and which does not require any knowledge about the desired output. Only input data is presented to the ANN in the learning process of the network, in other words the input space of the unsupervised learning ANN is the diffusion tensor eigenvector data of each imaging matrix. The network then adjusts the weightings to determine patterns having similar characteristics and classification is done in that way. The resulting classification represents the principal diffusion direction and the weighted diffusion distribution tracked by the fibers. Verification of the application on synthetic data enabled the implementation of the method on real brain images. The aim of the proposed method is to accomplish brain fiber tracking based on learning algorithms according to the modeling studies accepted in artificial neural network literature. Implementing SOM for fiber path discrimination purposes was successful and future work relies in 3D diffusion tensor tractography.",https://ieeexplore.ieee.org/document/5479790/,2010 15th National Biomedical Engineering Meeting,21-24 April 2010,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/EMCTECH49634.2020.9261512,Digital Socio-Political Communication and its Transformation in the Technological Evolution of Artificial Intelligence and Neural Network Algorithms,IEEE,Conferences,"The study aims to analyze the specifics of determining the subjects of digital social and political communication in the context of the development of artificial intelligence technologies and neural network algorithms. The work uses a case-study design. As a research methodology, the method of critical analysis of the digital communications practice in the socio-political sphere, as well as discourse analysis of modern scientific research in the field of the development of artificial intelligence and neural network algorithms, are used. It is concluded that the implementation of technological solutions based on artificial intelligence and neural network algorithms into the processes of socio-political communications creates a problem of defining the subject of communication acts in the socio-political sphere. Society may face such communication practices in which hybrid subjectness is realized. In the conditions of hybrid subjectness, both real subjects and programmed neural network algorithms acting as real subjects (but only imitating own subjectivity) interact in common communication space. The originality of the work lies in the formulation of the author's hypothesis about the emergence of the phenomenon of hybrid subjectness in the space of modern socio-political communications and its potential in the aspect of influencing the mass consciousness of citizens.",https://ieeexplore.ieee.org/document/9261512/,2020 International Conference on Engineering Management of Communication and Technology (EMCTECH),20-22 Oct. 2020,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCV.2009.5459193,Dimensionality reduction and principal surfaces via Kernel Map Manifolds,IEEE,Conferences,"We present a manifold learning approach to dimensionality reduction that explicitly models the manifold as a mapping from low to high dimensional space. The manifold is represented as a parametrized surface represented by a set of parameters that are defined on the input samples. The representation also provides a natural mapping from high to low dimensional space, and a concatenation of these two mappings induces a projection operator onto the manifold. The explicit projection operator allows for a clearly defined objective function in terms of projection distance and reconstruction error. A formulation of the mappings in terms of kernel regression permits a direct optimization of the objective function and the extremal points converge to principal surfaces as the number of data to learn from increases. Principal surfaces have the desirable property that they, informally speaking, pass through the middle of a distribution. We provide a proof on the convergence to principal surfaces and illustrate the effectiveness of the proposed approach on synthetic and real data sets.",https://ieeexplore.ieee.org/document/5459193/,2009 IEEE 12th International Conference on Computer Vision,29 Sept.-2 Oct. 2009,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICMLA.2009.121,Discriminative Multi-stream Discrete Hidden Markov Models,IEEE,Conferences,"We propose a modified discrete HMM that handles multimodalities. We assume that the feature space is partitioned into subspaces generated by different sources of information. To combine these heteregoneous modalities we propose a multi-stream discrete HMM that assigns a relevance weight to each subspace. The relevance weights are set local and depend on the symbols and the states. In particular, we associate a partial probability with each symbol in each subspace. The overall observation state probability is then computed as an aggregation of the partial probabilities and their objective relevance weights based on a linear combination. The minimum classification error (MCE) objective based on the gradient probabilistic descent (GPD) optimization algorithm is reformulated to derive the update equations for the relevance weights and the partial state probabilities. The proposed approach is validated using synthetic and real data sets. The results are shown to outperform the baseline discrete HMM that treats all streams equally important.",https://ieeexplore.ieee.org/document/5381827/,2009 International Conference on Machine Learning and Applications,13-15 Dec. 2009,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICSE.2019.00112,Distance-Based Sampling of Software Configuration Spaces,IEEE,Conferences,"Configurable software systems provide a multitude of configuration options to adjust and optimize their functional and non-functional properties. For instance, to find the fastest configuration for a given setting, a brute-force strategy measures the performance of all configurations, which is typically intractable. Addressing this challenge, state-of-the-art strategies rely on machine learning, analyzing only a few configurations (i.e., a sample set) to predict the performance of other configurations. However, to obtain accurate performance predictions, a representative sample set of configurations is required. Addressing this task, different sampling strategies have been proposed, which come with different advantages (e.g., covering the configuration space systematically) and disadvantages (e.g., the need to enumerate all configurations). In our experiments, we found that most sampling strategies do not achieve a good coverage of the configuration space with respect to covering relevant performance values. That is, they miss important configurations with distinct performance behavior. Based on this observation, we devise a new sampling strategy, called distance-based sampling, that is based on a distance metric and a probability distribution to spread the configurations of the sample set according to a given probability distribution across the configuration space. This way, we cover different kinds of interactions among configuration options in the sample set. To demonstrate the merits of distance-based sampling, we compare it to state-of-the-art sampling strategies, such as t-wise sampling, on 10 real-world configurable software systems. Our results show that distance-based sampling leads to more accurate performance models for medium to large sample sets.",https://ieeexplore.ieee.org/document/8812049/,2019 IEEE/ACM 41st International Conference on Software Engineering (ICSE),25-31 May 2019,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.23919/MIPRO.2019.8756937,Distillation of a CNN for a high accuracy mobile face recognition system,IEEE,Conferences,"In face recognition systems, the use of convolutional neural networks (CNNs) permits to achieve good accuracy performances, which derive largely from a huge number of well-trained parameters. While using online services any mobile device can suffice for an accurate identification, in the offline scenario, implemented on a wearable mobile hardware, it is difficult to achieve both real-time responsiveness and high accuracy. In this paper we present a solution to replace a large open source face recognizer network (provided as part of the dlib libraries), distilling its learned knowledge into a less demanding CNN. The former is used as an expert oracle that provides the targets, while the latter is trained on the same input image, following a regression approach. In addition to lightness, our CNN is trained to use smaller input images, naturally allowing the recognition of identities in a wider distance range and with a reduced amount of computation. This eventually permits the porting of the network into a dedicated mobile accelerating hardware. The hypothesis we want to demonstrate is that since the feature space topology has been deeply explored during the training of the expert network, and due to the fact that no information is created during the up sampling of a tiny face to the input size of the expert oracle, the smaller network can provide the same accuracy at a reduced computational cost.",https://ieeexplore.ieee.org/document/8756937/,"2019 42nd International Convention on Information and Communication Technology, Electronics and Microelectronics (MIPRO)",20-24 May 2019,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCCNT49239.2020.9225309,Distributed Artificial Neural Network Model for Neutron Flux Mapping in Nuclear Reactors,IEEE,Conferences,"Neutron flux distribution inside the core of large size nuclear reactors is a function of space and time. An online Flux Mapping System (FMS) is needed to monitor the core during the reactor operation. FMS estimates the core flux distribution from the measurements of few in-core detectors using an appropriate algorithm. Here, a Distributed Artificial Neural Network (D-ANN) model is developed using parallel-forward multi-layer perceptron architecture to capture the spatial core flux variation in a nuclear reactor. The proposed D-ANN model is tested with simulated test case data of Advanced Heavy Water Reactor (AHWR) for multiple operating conditions of the reactor. The model estimates the neutron flux in all horizontal mesh locations (2-D) from the multiple networks distributed spatially across AHWR core. Estimation error using the proposed D-ANN model is found to be significantly lower than that with lumped ANN model. Validation exercises establish that this D-ANN model could effectively capture the spatial variations in the reactor core and therefore could be utilized for efficient flux mapping. The real time implementation of D-ANN based flux mapping method is also proposed.",https://ieeexplore.ieee.org/document/9225309/,"2020 11th International Conference on Computing, Communication and Networking Technologies (ICCCNT)",1-3 July 2020,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICMLA.2013.17,Distributed Kernel Matrix Approximation and Implementation Using Message Passing Interface,IEEE,Conferences,"We propose a distributed method to compute similarity (also known as kernel and Gram) matrices used in various kernel-based machine learning algorithms. Current methods for computing similarity matrices have quadratic time and space complexities, which make them not scalable to large-scale data sets. To reduce these quadratic complexities, the proposed method first partitions the data into smaller subsets using various families of locality sensitive hashing, including random project and spectral hashing. Then, the method computes the similarity values among points in the smaller subsets to result in approximated similarity matrices. We analytically show that the time and space complexities of the proposed method are sub quadratic. We implemented the proposed method using the Message Passing Interface (MPI) framework and ran it on a cluster. Our results with real large-scale data sets show that the proposed method does not significantly impact the accuracy of the computed similarity matrices and it achieves substantial savings in running time and memory requirements.",https://ieeexplore.ieee.org/document/6784587/,2013 12th International Conference on Machine Learning and Applications,4-7 Dec. 2013,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICTON.2019.8840514,Distributed and Centralized Options for Self-Learning,IEEE,Conferences,"In general, the availability of enough real data from real fog computing scenarios to produce accurate Machine Learning (ML) models is rarely ensured since new equipment, techniques, etc., are continuously being deployed in the field. Although an option is to generate data from simulation and lab experiments, such data could not cover the whole features space, which would translate into ML models inaccuracies. In this paper, we propose a self-learning approach to facilitate ML deployment in real scenarios. A dataset for ML training can be initially populated based on the results from simulation and lab experiments and once ML models are generated, ML re-training can be performed after inaccuracies are detected to improve their precision. Illustrative numerical results show the benefits from the proposed self-learning approach for two general use cases of regression and classification.",https://ieeexplore.ieee.org/document/8840514/,2019 21st International Conference on Transparent Optical Networks (ICTON),9-13 July 2019,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ITSC.2010.5624970,Distributed evolutionary estimation of dynamic traffic origin/destination,IEEE,Conferences,"This paper focuses on updating time varying demand matrices using real-time information. An Artificial Intelligence technique based on Distributed Evolutionary Algorithms (DEA), which is capable to exploit the use of grid computing, is developed. This EA-based demand estimation framework is implemented into a model that we call DynODE (Dynyamic O/D Estimator). DynODE provides a direct way of fusing information of varying types, with different levels of accuracy and from different sensors/sources. DynODE is integrated with an existing Dynamic Traffic Assignment platform (i.e. Dynasmart-P) and is evaluated on a medium size network for various search space sizes and for different quality of the apriori matrix. The obtained results, in terms of replicating observed vehicle counts and the closeness to the real demand, are promising and point to the robustness of the gradient-free framework and its high performance irrespective of the quality of the apriori travel information. The use of Distributed EA is also shown to provide good results within fast computing speeds.",https://ieeexplore.ieee.org/document/5624970/,13th International IEEE Conference on Intelligent Transportation Systems,19-22 Sept. 2010,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.23919/ACC.2017.7963641,Distributed mean-field-type filter for vehicle tracking,IEEE,Conferences,"Particle filter is an effective tool for vehicle tracking. However, we need to maintain a large number of particles to keep a reasonable tracking accuracy for multi-target tracking in large scale state space. This paper proposes a new distributed mean-field-type filter to handle those noisy, partial-observed and high-dimensional data. The state space is decomposed and the particles are deployed locally and updated independently in the simplified subspaces. The filtering framework contains four operations: sampling, prediction, decomposition and correction. A mean-field term is included in the system dynamic so that the prediction is based on the previous state as well as its statistic distribution, which is estimated by a multi-frame learning procedure. The experiment on real data shows that our approach can achieve accurate tracking results with a small number of particles.",https://ieeexplore.ieee.org/document/7963641/,2017 American Control Conference (ACC),24-26 May 2017,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICASSP.2019.8683856,Diving Deep onto Discriminative Ensemble of Histological Hashing & Class-Specific Manifold Learning for Multi-class Breast Carcinoma Taxonomy,IEEE,Conferences,"Histopathological images (HI) encrypt resolution dependent heterogeneous textures & diverse color distribution variability, manifesting in micro-structural surface tissue convolutions & inherently high coherency of cancerous cells posing significant challenges to breast cancer (BC) multi-classification. As such, multi-class stratification is sparsely explored & prior work mainly focus on benign & malignant tissue characterization only, which forestalls further quantitative analysis of subordinate classes like adenosis, mucinous carcinoma & fibroadenoma etc, for diagnostic competence. In this work, a fully-automated, near-real-time & computationally inexpensive robust multi-classification deep framework from HI is presented.The proposed scheme employs deep neural network (DNN) aided discriminative ensemble of holistic class-specific manifold learning (CSML) for underlying HI sub-space embedding & HI hashing based local shallow signatures. The model achieves 95.8% accuracy pertinent to multi-classification, an 2.8% overall performance improvement & 38.2% enhancement for Lobular carcinoma (LC) sub-class recognition rate as compared to the existing state-of-the-art on well known BreakHis dataset is achieved. Also, 99.3% recognition rate at 200 & a sensitivity of 100% for binary grading at all magnification validates its suitability for clinical deployment in hand-held smart devices.",https://ieeexplore.ieee.org/document/8683856/,"ICASSP 2019 - 2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",12-17 May 2019,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SmartGridComm.2019.8909793,Domain-Adversarial Transfer Learning for Robust Intrusion Detection in the Smart Grid,IEEE,Conferences,"The smart grid faces growing cyber-physical attack threats aimed at the critical systems and processes communicating over the complex cyber-infrastructure. Thanks to the increasing availability of high-quality data and the success of deep learning algorithms, machine learning (ML)-based detection and classification have been increasingly effective and adopted against sophisticated attacks. However, many of these techniques rely on the assumptions that the training and testing datasets share the same distribution and the same class labels in a stationary environment. As such assumptions may fail to hold when the system dynamics shift and new threat variants emerge in a non-stationary environment, the capability of trained ML models to adapt in complex operating scenarios will be critical to their deployment in real-world smart grid communications. To this aim, this paper proposes a domain-adversarial transfer learning framework for robust intrusion detection against smart grid attacks. The framework introduces domain-adversarial training to create a mapping between the labeled source domain and the unlabeled target domain so that the classifiers can learn in a new feature space against unknown threats. The proposed framework with different baseline classifiers was evaluated using a smart grid cyber-attack dataset collected over a realistic hardware-in-the- loop security testbed. The results have demonstrated effective performance improvements of trained classifiers against unseen threats of different types and locations.",https://ieeexplore.ieee.org/document/8909793/,"2019 IEEE International Conference on Communications, Control, and Computing Technologies for Smart Grids (SmartGridComm)",21-23 Oct. 2019,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SCCC.2001.972633,Domain-dependent option policies in autonomous robot learning,IEEE,Conferences,"In control-related applications such as robotics, determination of optimal solutions is made very difficult for many reasons. Among these stands the difficulty in finding out an appropriate model of the domain, as defined by the control agent (robot), environment where it acts and their interaction. Reinforcement learning is a theory which defines a collection of algorithms for determination of control actions under model-free assumptions, which allows control agents to learn optimal actions in an autonomous way. In reinforcement learning, a cost functional to be optimised is determined in advance. The agent then learns how to perform this optimisation via trial and error on its environment. A trial corresponds to execution of actions chosen by the agent, and the error is the immediate result (a real-valued reinforcement) of this action. In the work reported, we consider trials by a learning robotic agent which are not based on low level actions, but instead on sequences of actions (options or macro-operators). We analysed the performance both in terms of learning speed and quality of learned control-for options that correspond to mappings from states to action policies (O/sub /spl Pi// options). Experimental results show that careful (domain-dependent) selection of options (via methods such as discretised potential fields) produce much faster learning for option-based robots when compared to their action-based counterparts. Of critical importance, however, is the option mapping in regions of the state space where the options are not assumed to be necessary: as performance of reinforcement learning algorithms is strongly dependent on sufficient exploration of the state space, even in such regions a careful, ad-hoc selection of actions is of foremost importance.",https://ieeexplore.ieee.org/document/972633/,SCCC 2001. 21st International Conference of the Chilean Computer Science Society,9-9 Nov. 2001,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCAR.2017.7942676,Door and cabinet recognition using Convolutional Neural Nets and real-time method fusion for handle detection and grasping,IEEE,Conferences,"In this paper we present a new method that robustly identifies doors, cabinets and their respective handles, with special emphasis on extracting useful features from handles to be then manipulated. The novelty of this system relies on the combination of a Convolutional Neural Net (CNN), as a form of reducing the search space, several methods to extract point cloud data and a mobile robot to interact with the objects. The framework consists of the following components: The implementation of a CNN to extract a Region of Interest (ROI) from an image corresponding to a door or cabinet. Several vision based techniques to detect handles inside the ROI and its 3D positioning. A complementary plane segmentation method to differentiate door/cabinet from the handle. An algorithm to fuse both approaches robustly and extract essential information from the handle for robotic grasping (i.e. handle point cloud, door plane model, grasping locations, turning orientation, orthogonal vector to door). A mobile robot for grasping the handle. The system assumes no prior knowledge of the environment.",https://ieeexplore.ieee.org/document/7942676/,"2017 3rd International Conference on Control, Automation and Robotics (ICCAR)",24-26 April 2017,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SNPD.2007.475,Double Unmanned Aerial Vehicle's Path Planning for Scout via Cross-Entropy Method,IEEE,Conferences,"Cross-entropy has been recently applied to combinatorial optimization problems with satisfying results. This paper introduce the cross-entropy method theory, the way of model making, real time and robust, and the application, in military scout , Unmanned Aerial Vehicle(UAV) is a special tool, with so many advantage in real time and global space. A new method, based on cross-entropy method, was used for the optimal way of double UAV's path planning.",https://ieeexplore.ieee.org/document/4287760/,"Eighth ACIS International Conference on Software Engineering, Artificial Intelligence, Networking, and Parallel/Distributed Computing (SNPD 2007)",30 July-1 Aug. 2007,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ASONAM.2018.8508284,DroidEye: Fortifying Security of Learning-Based Classifier Against Adversarial Android Malware Attacks,IEEE,Conferences,"To combat the evolving Android malware attacks, systems using machine learning techniques have been successfully deployed for Android malware detection. In these systems, based on different feature representations, various kinds of classifiers are constructed to detect Android malware. Unfortunately, as classifiers become more widely deployed, the incentive for defeating them increases. In this paper, we first extract a set of features from the Android applications (apps) and represent them as binary feature vectors; with these inputs, we then explore the security of a generic learning-based classifier for Android malware detection in the presence of adversaries. To harden the evasion, we first present count featurization to transform the binary feature space into continuous probabilities encoding the distribution in each class (either benign or malicious). To improve the system security while not compromising the detection accuracy, we further introduce softmax function with adversarial parameter to find the best trade-off between security and accuracy for the classifier. Accordingly, we develop a system named DroidEye which integrates our proposed method for Android malware detection. Comprehensive experiments on the real sample collection from Comodo Cloud Security Center are conducted to validate the effectiveness of DroidEye against adversarial Android malware attacks. Our proposed secure-learning paradigm is also applicable for other detection tasks, such as spammer detection in social media.",https://ieeexplore.ieee.org/document/8508284/,2018 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining (ASONAM),28-31 Aug. 2018,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/VDAT50263.2020.9190415,DynRP- Non-Intrusive Profiler for Dynamic Reconfigurability,IEEE,Conferences,"Emerging technological areas such as machine learning, speech recognition, computer vision, autonomous robots, AI, bioinformatics involving big data, require implementation in complex heterogeneous accelerator platforms, to be able to handle data explosion with higher efficiency, lower power, and better performance. Dynamic reconfiguration in such platforms can help in run-time optimization to meet the design goals. The required optimal platform configuration can be achieved by a flexible design space exploration and appropriate task partitioning obtained through profiling computation and communication of processes in application code. This paper focuses on profiling, it being the key to the success of obtaining optimal platform configurations. It points to existing profiling techniques, their pros and cons vis-à-vis dynamic reconfigurable architectures, and the challenges in their design for obtaining optimal profiling performance. It further outlines desirable specifications for a profiler to allow dynamic real-time profiling for effective use of dynamic reconfiguration. DynRP, a non-intrusive hardware profiler for dynamic reconfiguration is proposed based on the desirable specifications, followed by its design and implementation details.",https://ieeexplore.ieee.org/document/9190415/,2020 24th International Symposium on VLSI Design and Test (VDAT),23-25 July 2020,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRAE50850.2020.9310850,Dynamic Occlusion Handling for Real Time Object Perception,IEEE,Conferences,"An RGB-D based occlusion-handling camera position computation method for proper object perception has been designed and implemented. This proposal is an improved alternative to our previous optimisation-based approach where the contribution is twofold: this new method is geometric-based and it is also able to handle dynamic occlusions. This approach makes extensive use of a ray-projection model where a key aspect is that the solution space is defined within a sphere surface around the object. The method has been designed with a view to robotic applications and therefore provides robust and versatile features. Therefore, it does not require training nor prior knowledge of the scene, making it suitable for diverse applications and scenarios. Satisfactory results have been obtained with real time experiments.",https://ieeexplore.ieee.org/document/9310850/,2020 5th International Conference on Robotics and Automation Engineering (ICRAE),20-22 Nov. 2020,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IUCC-CIT-DSCI-SmartCNS55181.2021.00057,Dynamic Social Network Embedding Based on Triadic Closure Pattern Analysis,IEEE,Conferences,"Social network representation learning aims to convert the vertices in the social network to a low-dimensional vector space while preserving the inherent structural properties of the original network. The effectiveness of social network analysis benefits from a highly accurate embedding model. However, most of the existing social network representation learning methods mainly focus on static networks while ignoring the dynamic characteristics of real-world networks. This paper constructs a novel dynamic network representation learning model for online social networks to preserve the structural information and evolution characteristics of a given network. We focus on one of the most critical evolutionary characteristics, usually called triads. The process of closed triads develop from open triads in dynamic networks is investigated by combining three factors: user popularity, user similarity, and community structure. The triadic closure process is a fundamental mechanism in the evolution of dynamic networks, which may act to the macroscopic network structure changes at different time steps. This study utilizes the community structure further to impose constraints on the local proximity of vertices and learn more discriminative vertex representations. The triadic closure evolution concerning the vertices in the feature-reinforced representation can be profoundly optimized. Extensive experiments on several online social networks demonstrate superiority over the baseline methods.",https://ieeexplore.ieee.org/document/9719639/,2021 20th International Conference on Ubiquitous Computing and Communications (IUCC/CIT/DSCI/SmartCNS),20-22 Dec. 2021,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CloudNet.2014.6968974,Dynamic allocation and efficient distribution of data among multiple clouds using network coding,IEEE,Conferences,"Distributed storage has attracted large interest lately from both industry and researchers as a flexible, cost-efficient, high performance, and potentially secure solution for geographically distributed data centers, edge caching or sharing storage among users. This paper studies the benefits of random linear network coding to exploit multiple commercially available cloud storage providers simultaneously with the possibility to constantly adapt to changing cloud performance in order to optimize data retrieval times. The main contribution of this paper is a new data distribution mechanisms that cleverly stores and moves data among different clouds in order to optimize performance. Furthermore, we investigate the trade-offs among storage space, reliability and data retrieval speed for our proposed scheme. By means of real-world implementation and measurements using well-known and publicly accessible cloud service providers, we can show close to 9x less network use for the adaptation compared to more conventional dense recoding approaches, while maintaining similar download time performance and the same reliability.",https://ieeexplore.ieee.org/document/6968974/,2014 IEEE 3rd International Conference on Cloud Networking (CloudNet),8-10 Oct. 2014,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/WCICA.2016.7578244,Dynamic ensemble selection methods for heterogeneous data mining,IEEE,Conferences,"Big Data is often collected from multiple sources with possibly different features, representations and granularity and hence is defined as heterogeneous data. Such multiple datasets need to be fused together in some ways for further analysis. Data fusion at feature level requires domain knowledge and can be time-consuming and ineffective, but it could be avoided if decision-level fusion is applied properly. Ensemble methods appear to be an appropriate paradigm to do just that as each subset of heterogeneous data sources can be separately used to induce models independently and their decisions are then aggregated by a decision fusion function in an ensemble. This study investigates how heterogeneous data can be used to generate more diverse classifiers to build more accurate ensembles. A Dynamic Ensemble Selection Optimisation (DESO) framework is proposed, using the local feature space of heterogeneous data to increase diversity among classifiers and Simulated Annealing for optimisation. An implementation example of DESO - BaggingDES is provided with Bagging as a base platform of DESO, to test its performance and also explore the relationship between diversity and accuracy. Experiments are carried out with some heterogeneous datasets derived from real-world benchmark datasets. The statistical analyses of the results show that BaggingDES performed significantly better than the baseline method - decision tree, and reasonably better than the classic Bagging.",https://ieeexplore.ieee.org/document/7578244/,2016 12th World Congress on Intelligent Control and Automation (WCICA),12-15 June 2016,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICSMC.1995.537949,Dynamic path planning,IEEE,Conferences,"Path planning is dynamic when the path is continually recomputed as more information becomes available. A computational framework for dynamic path planning is proposed which has the ability to provide navigational directions during the computation of the plan. Path planning is performed using a potential field approach. We use a specific type of potential function-a harmonic function-which has no local minima. The implementation is parallel and consists of a collection of communicating processes, across a network of SPARC & SGI workstations using a message passing software package called PVM. The computation of the plan is performed independently of the execution of the plan. A hierarchical coarse-to-fine procedure is used to guarantee a correct control strategy at the expense of accuracy. We have successfully navigated a Nomad robot around our lab space with no a priori map in real-time. The result of the described approach is a parallel implementation which permits dynamic path planning using available processor resources.",https://ieeexplore.ieee.org/document/537949/,"1995 IEEE International Conference on Systems, Man and Cybernetics. Intelligent Systems for the 21st Century",22-25 Oct. 1995,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS.2015.7353649,Dynamically Pruned A for re-planning in navigation meshes,IEEE,Conferences,"Modern simulations feature crowds of AI-controlled agents moving through dynamic environments, with obstacles appearing or disappearing at run-time. A dynamic navigation mesh can represent the traversable space of such environments. The A* algorithm computes optimal paths through the dual graph of this mesh. When an obstacle is inserted or deleted, the mesh changes and agents should re-plan their paths. Many existing re-planning algorithms are too memory-intensive for crowds, or they cannot easily be used on graphs where vertices and edges are added or removed. In this paper, we present Dynamically Pruned A* (DPA*), an extension of A* for re-planning optimal paths in dynamic navigation meshes. DPA* has similarities to adaptive algorithms that make the A* heuristic more informed based on previous queries. However, DPA* prunes the search using only the previous path and its relation to the dynamic event. We describe the four re-planning scenarios that can occur; DPA* uses different rules in each scenario. Our algorithm is memory-friendly and robust against structural changes in the graph, which makes it suitable for crowds in dynamic navigation meshes. Experiments show that DPA* performs particularly well in complex environments and when the dynamic event is visible to the agent. We integrate the algorithm into crowd simulation software to model large crowds in dynamic environments in real-time.",https://ieeexplore.ieee.org/document/7353649/,2015 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),28 Sept.-2 Oct. 2015,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SECON52354.2021.9491609,EFCam: Configuration-Adaptive Fog-Assisted Wireless Cameras with Reinforcement Learning,IEEE,Conferences,"Visual sensing has been increasingly employed in industrial processes. This paper presents the design and implementation of an industrial wireless camera system, namely, EFCam, which uses low-power wireless communications and edge-fog computing to achieve cordless and energy-efficient visual sensing. The camera performs image pre-processing (i.e., compression or feature extraction) and transmits the data to a resourceful fog node for advanced processing using deep models. EFCam admits dynamic configurations of several parameters that form a configuration space. It aims to adapt the configuration to maintain desired visual sensing performance of the deep model at the fog node with minimum energy consumption of the camera in image capture, pre-processing, and data communications, under dynamic variations of application requirement and wireless channel conditions. However, the adaptation is challenging due primarily to the complex relationships among the involved factors. To address the complexity, we apply deep reinforcement learning to learn the optimal adaptation policy. Extensive evaluation based on trace-driven simulations and experiments show that EFCam complies with the accuracy and latency requirements with lower energy consumption for a real industrial product object tracking application, compared with four baseline approaches incorporating hysteresis-based adaptation.",https://ieeexplore.ieee.org/document/9491609/,"2021 18th Annual IEEE International Conference on Sensing, Communication, and Networking (SECON)",6-9 July 2021,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/BSN.2015.7299402,"EMI Spy: Harnessing electromagnetic interference for low-cost, rapid prototyping of proxemic interaction",IEEE,Conferences,"We present a wearable system that uses ambient electromagnetic interference (EMI) as a signature to identify electronic devices and support proxemic interaction. We designed a low cost tool, called EMI Spy, and a software environment for rapid deployment and evaluation of ambient EMI-based interactive infrastructure. EMI Spy captures electromagnetic interference and delivers the signal to a user's mobile device or PC through either the device's wired audio input or wirelessly using Bluetooth. The wireless version can be worn on the wrist, communicating with the user;s mobile device in their pocket. Users are able to train the system in less than 1 second to uniquely identify displays in a 2-m radius around them, as well as to detect pointing at a distance and touching gestures on the displays in real-time. The combination of a low cost EMI logger and an open source machine learning tool kit allows developers to quickly prototype proxemic, touch-to-connect, and gestural interaction. We demonstrate the feasibility of mobile, EMI-based device and gesture recognition with preliminary user studies in 3 scenarios, achieving 96% classification accuracy at close range for 6 digital signage displays distributed throughout a building, and 90% accuracy in classifying pointing gestures at neighboring desktop LCD displays. We were able to distinguish 1- and 2-finger touching with perfect accuracy and show indications of a way to determine power consumption of a device via touch. Our system is particularly well-suited to temporary use in a public space, where the sensors could be distributed to support a popup interactive environment anywhere with electronic devices. By designing for low cost, mobile, flexible, and infrastructure-free deployment, we aim to enable a host of new proxemic interfaces to existing appliances and displays",https://ieeexplore.ieee.org/document/7299402/,2015 IEEE 12th International Conference on Wearable and Implantable Body Sensor Networks (BSN),9-12 June 2015,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AERO.1997.574422,ESL: a language for supporting robust plan execution in embedded autonomous agents,IEEE,Conferences,"ESL (Execution Support Language) is a language for encoding execution knowledge in embedded autonomous agents. It is similar in spirit to RAPs (1989), RS (1983), and RPL Reactive Plan Language, and its design owes much to these systems. Unlike its predecessors, ESL aims for a more utilitarian point in the design space. ESL was designed primarily to be a powerful and easy-to-use tool, not to serve as a representation for automated reasoning or formal analysis (although nothing precludes its use for these purposes). ESL consists of several sets of loosely coupled features that can be composed in arbitrary ways. It is currently implemented as a set of extensions to Common Lisp, and is being used to build the executive component of a control architecture for an autonomous spacecraft.",https://ieeexplore.ieee.org/document/574422/,1997 IEEE Aerospace Conference,13-13 Feb. 1997,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/DDECS54261.2022.9770144,Early Performance Estimation of Embedded Software on RISC-V Processor using Linear Regression,IEEE,Conferences,"RISC-V-based embedded systems are becoming more and more popular in recent years. Performance estimation of embedded software at an early stage of the design process plays an important role in efficient design space exploration and reducing time-to-market constraints. Although several cycle-accurate RISC-V simulators at different levels of abstraction have been proposed, they have an inherently high cost, both for the development of the simulation setting and for obtaining the software performance in terms of the number of cycles through simulation. This results in a significant burden on designers to perform design space exploration.In this paper, we present a novel ML-based approach, enabling designers to fast and accurately estimate the performance of a given embedded software implemented on the RISC-V processor at the early stage of the design process. The proposed approach is evaluated against a real-world cycle-accurate RISC-V Virtual Prototype (VP) using a set of standard benchmarks. Our experiments demonstrate that our approach allows obtaining highly-accurate performance estimation results in a short execution time. In comparison to the cycle-accurate RISC-V VP model, the proposed approach achieves up to more than 5 x faster simulation speed and less than 2.5% prediction error on average.",https://ieeexplore.ieee.org/document/9770144/,2022 25th International Symposium on Design and Diagnostics of Electronic Circuits and Systems (DDECS),6-8 April 2022,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ROMAN.2011.6005223,Effect of human guidance and state space size on Interactive Reinforcement Learning,IEEE,Conferences,"The Interactive Reinforcement Learning algorithm enables a human user to train a robot by providing rewards in response to past actions and anticipatory guidance to guide the selection of future actions. Past work with software agents has shown that incorporating user guidance into the policy learning process through Interactive Reinforcement Learning significantly improves the policy learning time by reducing the number of states the agent explores. We present the first study of Interactive Reinforcement Learning in real-world robotic systems. We report on four experiments that study the effects that teacher guidance and state space size have on policy learning performance. We discuss modifications made to apply Interactive Reinforcement Learning to a real-world system and show that guidance significantly reduces the learning rate, and that its positive effects increase with state space size.",https://ieeexplore.ieee.org/document/6005223/,2011 RO-MAN,31 July-3 Aug. 2011,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IPCCC50635.2020.9391551,Efficient Architecture Paradigm for Deep Learning Inference as a Service,IEEE,Conferences,"Deep learning (DL) inference has been broadly used and shown excellent performance in many intelligent applications. Unfortunately, the high resource consumption and training efforts of sophisticated models present obstacles for regular users to enjoy it. Thus, Deep Learning Inference as a Service (DIaaS), offering online inference services on cloud, has earned great popularity among cloud tenants who can send their DIaaS inputs via RPCs across the internal network. However, such detached architecture paradigm is inappropriate to DIaaS because the high-dimensional inputs of DIaaS consume a lot of precious internal bandwidth and the service latency of DIaaS has to be low and stable. We therefore propose a novel architecture paradigm on cloud for DIaaS in order to address the above two problems without giving up the security and maintenance benefits. We first leverage the SGX technology, a strongly-protected user space enclave, to bring DIaaS computation to its input source as close as possible, i.e. co-locating a cloud tenant and its subscribed DIaaS in the same virtual machine. When the GPU acceleration is needed, we migrate this virtual machine to any available GPU host and transparently utilize the GPU via our backend computing stack installed on it. In this way the majority of internal bandwidth is saved compared to traditional paradigm. Furthermore, we greatly improve the efficiency of the proposed architecture paradigm, from the computation and I/O perspectives, by making the entire data flow more DL-oriented. Finally, we implement a prototype system and evaluate it in real-world scenarios. The experiments show that our locality-aware architecture achieves the average single CPU (GPU) based deep learning inference time 2.84X (4.87X) less than the traditional detached architecture on average.",https://ieeexplore.ieee.org/document/9391551/,2020 IEEE 39th International Performance Computing and Communications Conference (IPCCC),6-8 Nov. 2020,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CVPR.2019.00790,Efficient Decision-Based Black-Box Adversarial Attacks on Face Recognition,IEEE,Conferences,"Face recognition has obtained remarkable progress in recent years due to the great improvement of deep convolutional neural networks (CNNs). However, deep CNNs are vulnerable to adversarial examples, which can cause fateful consequences in real-world face recognition applications with security-sensitive purposes. Adversarial attacks are widely studied as they can identify the vulnerability of the models before they are deployed. In this paper, we evaluate the robustness of state-of-the-art face recognition models in the decision-based black-box attack setting, where the attackers have no access to the model parameters and gradients, but can only acquire hard-label predictions by sending queries to the target model. This attack setting is more practical in real-world face recognition systems. To improve the efficiency of previous methods, we propose an evolutionary attack algorithm, which can model the local geometry of the search directions and reduce the dimension of the search space. Extensive experiments demonstrate the effectiveness of the proposed method that induces a minimum perturbation to an input face image with fewer queries. We also apply the proposed method to attack a real-world face recognition system successfully.",https://ieeexplore.ieee.org/document/8953400/,2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),15-20 June 2019,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1145/3510003.3510188,Efficient Online Testing for DNN-Enabled Systems using Surrogate-Assisted and Many-Objective Optimization,IEEE,Conferences,"With the recent advances of Deep Neural Networks (DNNs) in real-world applications, such as Automated Driving Systems (ADS) for self-driving cars, ensuring the reliability and safety of such DNN-enabled Systems emerges as a fundamental topic in software testing. One of the essential testing phases of such DNN-enabled systems is online testing, where the system under test is embedded into a specific and often simulated application environment (e.g., a driving environment) and tested in a closed-loop mode in interaction with the environment. However, despite the importance of online testing for detecting safety violations, automatically generating new and diverse test data that lead to safety violations presents the following challenges: (1) there can be many safety requirements to be considered at the same time, (2) running a high-fidelity simulator is often very computationally-intensive, and (3) the space of all possible test data that may trigger safety violations is too large to be exhaustively explored. In this paper, we address the challenges by proposing a novel approach, called SAMOTA (Surrogate-Assisted Many-Objective Testing Approach), extending existing many-objective search algorithms for test suite generation to efficiently utilize surrogate models that mimic the simulator, but are much less expensive to run. Empirical evaluation results on Pylot, an advanced ADS composed of multiple DNNs, using CARLA, a high-fidelity driving simulator, show that SAMOTA is significantly more effective and efficient at detecting unknown safety requirement violations than state-of-the-art many-objective test suite generation algorithms and random search. In other words, SAMOTA appears to be a key enabler technology for online testing in practice.",https://ieeexplore.ieee.org/document/9794023/,2022 IEEE/ACM 44th International Conference on Software Engineering (ICSE),25-27 May 2022,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/DAC.2005.193910,Efficient SAT solving: beyond supercubes,IEEE,Conferences,"SAT (Boolean satisfiability) has become the primary Boolean reasoning engine for many EDA applications, so the efficiency of SAT solving is of great practical importance. Recently, Goldberg et al introduced supercubing, a different approach to search-space pruning, based on a theory that unifies many existing methods. Their implementation reduced the number of decisions, but no speedup was obtained. In this paper, we generalize beyond supercubes, creating a theory we call B-cubing, and show how to implement B-cubing in a practical solver. On extensive benchmark runs, using both real problems and synthetic benchmarks, the new technique is competitive on average with the newest version of ZChaff, is much faster in some cases, and is more robust.",https://ieeexplore.ieee.org/document/1510430/,"Proceedings. 42nd Design Automation Conference, 2005.",13-17 June 2005,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/BDCloud.2015.51,Efficient k-Nearest Neighbors Search in High Dimensions Using MapReduce,IEEE,Conferences,"Finding the k-Nearest Neighbors (kNN) of a query object for a given dataset S is a primitive operation in many application domains. kNN search is very costly, especially many applications witness a quick increase in the amount and dimension of data to be processed. Locality sensitive hashing (LSH) has become a very popular method for this problem. However, most such methods can't obtain good performance in terms of search quality, search efficiency and space cost at the same time, such as RankReduce, which gains good search efficiency at the sacrifice of the search quality. Motivated by these, we propose a novel LSH-based inverted index scheme and design an efficient search algorithm, called H-c2kNN, which enables fast high-dimensional kNN search with excellent quality and low space cost. For efficiency and scalability concerns, we implemented our proposed approach to solve the kNN search in high dimensional space using MapReduce, which is a well-known framework for data-intensive applications and conducted extensive experiments to evaluate our proposed approach using both synthetic and real datasets. The results show that our proposed approach outperforms baseline methods in high dimensional space.",https://ieeexplore.ieee.org/document/7310711/,2015 IEEE Fifth International Conference on Big Data and Cloud Computing,26-28 Aug. 2015,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/NLPKE.2003.1275966,Efficient mining of textual associations,IEEE,Conferences,"We describe an efficient implementation for mining textual associations from text corpora. In order to tackle real world applications, efficient algorithms and data structures are needed to manage, in reasonable time and space, the overgrowing volume of text data. For that purpose, we introduce a global architecture based on masks, suffix arrays and multidimensional arrays to implement the SENTA extractor (Dias, 2002). In particular, SENTA has shown great flexibility and accuracy for mining textual associations such as collocations, cognates, morphemes and chunks. Our solution shows O(h(F) N log N) time complexity and O(N) space complexity where N is the size of the corpus and h(F) is a function of the context window size.",https://ieeexplore.ieee.org/document/1275966/,"International Conference on Natural Language Processing and Knowledge Engineering, 2003. Proceedings. 2003",26-29 Oct. 2003,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IJCNN.1999.831156,Efficient training techniques for classification with vast input space,IEEE,Conferences,"Strategies to efficiently train a neural network for an aerospace problem with a large multidimensional input space are developed and demonstrated. The neural network provides classification for over 100,000,000 data points. A query-based strategy is used that initiates training using a small input set, and then augments the set in multiple stages to include important data around the network decision boundary. Neural network inversion and oracle query are used to generate the additional data, jitter is added to the query data to improve the results, and an extended Kalman filter algorithm is used for training. A causality index is discussed as a means to reduce the dimensionality of the problem based on the relative importance of the inputs.",https://ieeexplore.ieee.org/document/831156/,IJCNN'99. International Joint Conference on Neural Networks. Proceedings (Cat. No.99CH36339),10-16 July 1999,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICPR.2000.902928,Eigensnakes for vessel segmentation in angiography,IEEE,Conferences,"We introduce a new deformable model, called eigensnake, for segmentation of elongated structures in a probabilistic framework. Instead of snake attraction by specific image features extracted independently of the snake, our eigensnake learns an optimal object description and searches for such image feature in the target image. This is achieved applying principal component analysis on image responses of a bank of Gaussian derivative filters. Therefore, attraction by eigensnakes is defined in terms of classification of image features. The potential energy for the snake is defined in terms of likelihood in the feature space and incorporated into a new energy minimising scheme. Hence, the snake deforms to minimise the mahalanobis distance in the feature space. A real application of segmenting and tracking coronary vessels in angiography is considered and the results are very encouraging.",https://ieeexplore.ieee.org/document/902928/,Proceedings 15th International Conference on Pattern Recognition. ICPR-2000,3-7 Sept. 2000,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/TAI.1993.633958,Elastic version space: a knowledge acquisition method with background knowledge adjustment,IEEE,Conferences,"Similarity based learning (SBL) is efficient in knowledge acquisition process, and it uses training examples to generate rules and refine them. Training examples collected in the real world are very often constructed with numerical attributes. In order to deal with these training examples, SBL needs background knowledge. Especially segments which specify value ranges of numerical attributes are discussed in the background knowledge. Elastic version space method is proposed here which integrates the version space method with the functions of segments adjustment. By defining the segments structure using margin segments in the background knowledge, the version space method itself adjusts the segments. In consequence, this method expands the region of application of version space. Empirical results applying to the man-power allocation problem are presented which shows that the elastic version space method is an effective SBL in the knowledge acquisition process.",https://ieeexplore.ieee.org/document/633958/,Proceedings of 1993 IEEE Conference on Tools with Al (TAI-93),8-11 Nov. 1993,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCCNT45670.2019.8944554,Elucidating Farmers towards Smart Agricultural Farm Building through Cloud Model,IEEE,Conferences,"Agriculture is the main source of food and is undeniably the key reason for the survival of the beings in this planet. With the growing population, the precision-centric, informed, and optimized production of various food items is essential. There are new technologies, methodologies and best practices being conceived and conceptualized by the experts and researchers in the agriculture field. The various advancements in the information technology (IT) domain are showing some positive signs towards efficient and intelligent agriculture. The widely discussed innovations and disruptions in the IT space are bound to impact the agriculture activities in a strategically sound manner. A variety of changes in the way our farmers are toiling in the agriculture land parcels are being expected. With the emergence of software-defined cloud environments, big and real-time data analytics, the faster adoption of artificial intelligence (AI) advancements, knowledge visualization tools, the growing array of Smartphone applications are to bring in a raft of transformations in the lives of farmers in the days ahead. This paper is to describe the various ways and means of empowering our farmers with realtime information and insights through the cloud technology.",https://ieeexplore.ieee.org/document/8944554/,"2019 10th International Conference on Computing, Communication and Networking Technologies (ICCCNT)",6-8 July 2019,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICNC.2010.5584314,Emotion generation for virtual human using Cognitive Map,IEEE,Conferences,"Affective computing is an indispensable aspect in harmonious human-computer interaction and artificial intelligence. Making computers have the ability of generating emotions is a challenging task of affective computing. The paper first introduces the basic affective elements, and the representation of affections in a computer. An emotion generation model using Cognitive Map is proposed. The model is used to generate emotion based on the evaluation of the overall influences of the mood, the personality, the previous emotion and the external stimulations. Then the paper describes a method to build a mapping from the emotion space to the facial expression space with a competitive network, and the implementation of facial expression animation. Finally, the paper constructs an intelligent virtual human system with facial expression, voice and vision communication. Experimental results show that the emotion generation model using Cognitive Map can produce realistic emotions similar to those of a real human.",https://ieeexplore.ieee.org/document/5584314/,2010 Sixth International Conference on Natural Computation,10-12 Aug. 2010,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/MILCOM.2017.8170755,Empirical statistical inference attack against PHY-layer key extraction in real environments,IEEE,Conferences,"Traditional cryptographic secret key establishment mechanisms are facing challenges with the fast growth of high-performance computing, and can be very costly in many settings, e.g. in wireless ad-hoc networks, since they consume scarce resources such as bandwidth and battery power. As an alternative, link-signature-based (LSB) secret key extraction techniques have received many interests in recent years. It is believed that these mechanisms are secure, based on the fundamental assumption that wireless signals received at two locations separated by more than half a wavelength apart are uncorrelated. However, recently it has been observed that in some circumstances this assumption does not hold, rendering LSB key extraction mechanisms vulnerable to attacks. This paper studies empirical statistical inference attacks (SIA) to LSB key extraction, whereby an attacker infers the signature of a target link, and henceforce recovers the secret key extracted from that signature, by observing the surrounding links. Different from prior work that assumes a theoretical link-correlation model for the inference, our study does not make any assumption on link correlation. Instead, ours is taking a machine learning method for link inference based on empirically measured link data. Machine learning (ML) algorithms are developed to launch SIA under various realistic scenarios. Our experiment results show that even without making assumptions on link correlation, the proposed inference algorithms are still quite effective, and can reduce the key search space by many orders of magnitudes compared to brutal force search.",https://ieeexplore.ieee.org/document/8170755/,MILCOM 2017 - 2017 IEEE Military Communications Conference (MILCOM),23-25 Oct. 2017,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ECCTD.2017.8093280,Emulating CNN with template learning on FPGA,IEEE,Conferences,"A 2-D Cellular Neural Network structure with space invariant neural weights is widely used in image processing applications. Recent advances VLSI technology appears to be very promising to use discrete time CNNs for real time vision applications. In this paper, a system-on-chip implementation which consists of a new CNN emulator design and a processor which performs template learning algorithm is shown. SoC design is programmed to perform a sequential CNN operations on different input and state images with different templates. Furthermore, the presented SoC design allows that templates can be updated by a learning algoritm in run time. SoC design is realised on a target FPGA. Test results on FPGA and MATLAB are presented and compared with structural similarity map.",https://ieeexplore.ieee.org/document/8093280/,2017 European Conference on Circuit Theory and Design (ECCTD),4-6 Sept. 2017,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICIP.2017.8296489,Encyclopedia enhanced semantic embedding for zero-shot learning,IEEE,Conferences,"There are tremendous object categories in the real world besides those in image datasets. Zero-shot learning aims to recognize image categories which are unseen in the training set. A large number of previous zero-shot learning models use word vectors of the class labels directly as category prototypes in the semantic embedding space. But word vectors cannot obtain the global knowledge of an image category sufficiently. In this paper, we propose a new encyclopedia enhanced semantic embedding model to promote the discriminative capability of word vector prototypes with the global knowledge of each image category. The proposed model extracts the TF-IDF key words from encyclopedia articles to acquire the global knowledge of each category. The convex combination of the key words' word vectors acts as the prototypes of the object categories. The prototypes of seen and unseen classes build up the embedding space where the nearest neighbour search is implemented to recognize the unseen images. The experiments show that the proposed method achieves the state-of-the-art performance on the challenging ImageNet Fall 2011 1k2hop dataset.",https://ieeexplore.ieee.org/document/8296489/,2017 IEEE International Conference on Image Processing (ICIP),17-20 Sept. 2017,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CVPR42600.2020.01463,End-to-End Adversarial-Attention Network for Multi-Modal Clustering,IEEE,Conferences,"Multi-modal clustering aims to cluster data into different groups by exploring complementary information from multiple modalities or views. Little work learns the deep fused representations and simutaneously discovers the cluster structure with a discriminative loss. In this paper, we present an End-to-end Adversarial-attention network for Multi-modal Clustering (EAMC), where adversarial learning and attention mechanism are leveraged to align the latent feature distributions and quantify the importance of modalities respectively. To benefit from the joint training, we introducea divergence-based clustering objective that not only encourages the separation and compactness of the clusters but also enjoy a clear cluster structure by embedding the simplex geometry of the output space into the loss. The proposed network consists of modality-specific feature learning, modality fusion and cluster assignment three modules. It can be trained from scratch with batch-mode based optimization and avoid an autoencoder pretraining stage. Comprehensive experiments conducted on five real-world datasets show the superiority and effectiveness of the proposed clustering method.",https://ieeexplore.ieee.org/document/9156700/,2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),13-19 June 2020,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CISIS.2011.82,Engineering SLS Algorithms for Statistical Relational Models,IEEE,Conferences,"We present high performing SLS algorithms for learning and inference in Markov Logic Networks (MLNs). MLNs are a state-of-the-art representation formalism that integrates first-order logic and probability. Learning MLNs structure is hard due to the combinatorial space of candidates caused by the expressive power of first-order logic. We present current work on the development of algorithms for learning MLNs, based on the Iterated Local Search (ILS) metaheuristic. Experiments in real-world domains show that the proposed approach improves accuracy and learning time over the existing state-of-the-art algorithms. Moreover, MAP and conditional inference in MLNs are hard computational tasks too. This paper presents two algorithms for these tasks based on the Iterated Robust Tabu Search (IRoTS) schema. The first algorithm performs MAP inference by performing a RoTS search within a ILS iteration. Extensive experiments show that it improves over the state-of the-art algorithm in terms of solution quality and inference times. The second algorithm combines IRoTS with simulated annealing for conditional inference and we show through experiments that it is faster than the current state-of-the-art algorithm maintaining the same inference quality.",https://ieeexplore.ieee.org/document/5989060/,"2011 International Conference on Complex, Intelligent, and Software Intensive Systems",30 June-2 July 2011,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IADCC.2015.7154739,Enhanced SMOTE algorithm for classification of imbalanced big-data using Random Forest,IEEE,Conferences,"In the era of big data, the applications generating tremendous amount of data are becoming the main focus of attention as the wide increment of data generation and storage that has taken place in the last few years. This scenario is challenging for data mining techniques which are not arrogated to the new space and time requirements. In many of the real world applications, classification of imbalanced data-sets is the point of attraction. Most of the classification methods focused on two-class imbalanced problem. So, it is necessary to solve multi-class imbalanced problem, which exist in real-world domains. In the proposed work, we introduced a methodology for classification of multi-class imbalanced data. This methodology consists of two steps: In first step we used Binarization techniques (OVA and OVO) for decomposing original dataset into subsets of binary classes. In second step, the SMOTE algorithm is applied against each subset of imbalanced binary class in order to get balanced data. Finally, to achieve classification goal Random Forest (RF) classifier is used. Specifically, oversampling technique is adapted to big data using MapReduce so that this technique is able to handle as large data-set as needed. An experimental study is carried out to evaluate the performance of proposed method. For experimental analysis, we have used different datasets from UCI repository and the proposed system is implemented on Apache Hadoop and Apache Spark platform. The results obtained shows that proposed method outperforms over other methods.",https://ieeexplore.ieee.org/document/7154739/,2015 IEEE International Advance Computing Conference (IACC),12-13 June 2015,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA40945.2020.9197510,Episodic Koopman Learning of Nonlinear Robot Dynamics with Application to Fast Multirotor Landing,IEEE,Conferences,"This paper presents a novel episodic method to learn a robot's nonlinear dynamics model and an increasingly optimal control sequence for a set of tasks. The method is based on the Koopman operator approach to nonlinear dynamical systems analysis, which models the flow of observables in a function space, rather than a flow in a state space. Practically, this method estimates a nonlinear diffeomorphism that lifts the dynamics to a higher dimensional space where they are linear. Efficient Model Predictive Control methods can then be applied to the lifted model. This approach allows for real time implementation in on-board hardware, with rigorous incorporation of both input and state constraints during learning. We demonstrate the method in a real-time implementation of fast multirotor landing, where the nonlinear ground effect is learned and used to improve landing speed and quality.",https://ieeexplore.ieee.org/document/9197510/,2020 IEEE International Conference on Robotics and Automation (ICRA),31 May-31 Aug. 2020,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/WCICA.2000.863468,Estimated force emulation for space robot using neural networks,IEEE,Conferences,"This paper introduces the telerobotic system estimated force emulation using neural networks. A delay-compensating 3D stereo-graphic simulator is implemented in SGI ONYX/4 RE/sup 2/. The estimated force emulation can protect the real robot in time from being damaged in collision. The neural network is used to learn the mapping between the contact force error and the accommodated position command to the controller of the space robot. Finally, the controller can feel the emulated force with a two-hand 6-DOF master arm using the force feedback interface.",https://ieeexplore.ieee.org/document/863468/,Proceedings of the 3rd World Congress on Intelligent Control and Automation (Cat. No.00EX393),26 June-2 July 2000,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CVPRW50498.2020.00337,Evading Deepfake-Image Detectors with White- and Black-Box Attacks,IEEE,Conferences,"It is now possible to synthesize highly realistic images of people who do not exist. Such content has, for example, been implicated in the creation of fraudulent socialmedia profiles responsible for dis-information campaigns. Significant efforts are, therefore, being deployed to detect synthetically-generated content. One popular forensic approach trains a neural network to distinguish real from synthetic content.We show that such forensic classifiers are vulnerable to a range of attacks that reduce the classifier to near- 0% accuracy. We develop five attack case studies on a state- of-the-art classifier that achieves an area under the ROC curve (AUC) of 0.95 on almost all existing image generators, when only trained on one generator. With full access to the classifier, we can flip the lowest bit of each pixel in an image to reduce the classifier's AUC to 0.0005; perturb 1% of the image area to reduce the classifier's AUC to 0.08; or add a single noise pattern in the synthesizer's latent space to reduce the classifier's AUC to 0.17. We also develop a black-box attack that, with no access to the target classifier, reduces the AUC to 0.22. These attacks reveal significant vulnerabilities of certain image-forensic classifiers.",https://ieeexplore.ieee.org/document/9150604/,2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW),14-19 June 2020,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICDMW53433.2021.00135,Evaluating Time Series Predictability via Transition Graph Analysis,IEEE,Conferences,"This study is focused on exploring time series intrinsic predictability using transition graph analysis. The goal is to find out whether a special graph that reproduces system transition along its trajectory in the state space is useful for distinguishing time series of &#x201C;good&#x201D; and &#x201C;bad&#x201D; predictability. We perform a state space clustering to construct a weighted and directed transition graph and then to calculate different graph characteristics. We train several predictive models (in particular, the well-known auto-regression, singular spectrum analyses, artificial neural network and specific dynamic models of local approximation and maximal similarity) for time series and apply k-means algorithm to divide the set of the series into two parts using the properties of the corresponding transition graph. As a result we have artificial and real-world datasets divided into two clusters in one of which the mean forecasting error is much less than in the other. The F<inf>1</inf>-score value (&#x2248; 0.87) for this clustering shows that our approach performs better than those in some related works. We also train several classification models on a set of artificial series so that they are able to distinguish real-world time series of &#x201C;good&#x201D; and &#x201C;bad&#x201D; predictability. The results of this work can be used for data engineering in time series forecasting tasks and for predictive model design and evaluation. The datasets, the framework implementation and the results related to our study are publicly available on GitHub.",https://ieeexplore.ieee.org/document/9679876/,2021 International Conference on Data Mining Workshops (ICDMW),7-10 Dec. 2021,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/WiSEE44079.2020.9262700,Evaluating the Cognitive Network Controller with an SNN on FPGA,IEEE,Conferences,"The cognitive network controller (CNC) defines a learning agent that can adapt online and in near real-time the routing decisions needed for bundle transmissions in a space delay-tolerant network and other challenged networks. The agent uses a spiking neural network (SNN) as the learning element in a reinforcement learning loop, which incrementally optimizes the outbound link selection for each bundle based on its estimated routing cost. In this paper, a digital hardware implementation of the SNN element of the CNC is proposed, which helps to accelerate the routing decision-making process to faster-than-real-time levels. The design was tested on a Zynq Z7020 (PYNQ-Z2) SoC/FPGA board. A distributed implementation of the CNC is also proposed, which allows offloading the SNN execution from a DTN gateway to a remote device that hosts the FPGA implementation of the SNN. The methods were validated using an emulated satellite network testbed.",https://ieeexplore.ieee.org/document/9262700/,2020 IEEE International Conference on Wireless for Space and Extreme Environments (WiSEE),12-14 Oct. 2020,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/OCEANS.2014.7003088,Evaluation of Q-learning for search and inspect missions using underwater vehicles,IEEE,Conferences,"An application for offline Reinforcement Learning in the underwater domain is proposed. We present and evaluate the integration of the Q-learning algorithm into an Autonomous Underwater Vehicle (AUV) for learning the action-value function in simulation. Three separate experiments are presented. The first compares two search policies: the ε - least visited, and random action, with respect to convergence time. The second experiment presents the effect of the learning discount factor, gamma, on the convergence time of the ε - least visited search policy. The final experiment is to validate the use of a policy learnt offline on a real AUV. This learning phase occurs offline within the continuous simulation environment which had been discretized into a grid-world learning problem. Presented results show the system's convergence to a global optimal solution whilst following both sub-optimal policies during simulation. Future work is introduced, after discussion of our results, to enable the system to be used in a real world application. The results presented, therefore, form the basis for future comparative analysis of the necessary improvements such as function approximation of the state space.",https://ieeexplore.ieee.org/document/7003088/,2014 Oceans - St. John's,14-19 Sept. 2014,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICDE48307.2020.00144,Maxson: Reduce Duplicate Parsing Overhead on Raw Data,IEEE,Conferences,"JSON is a very popular data format in many applications in Web and enterprise. Recently, many data analytical systems support the loading and querying JSON data. However, JSON parsing can be costly, which dominates the execution time of querying JSON data. Many previous studies focus on building efficient parsers to reduce this parsing cost, and little work has been done on how to reduce the occurrences of parsing. In this paper, we start with a study with a real production workload in Alibaba, which consists of over 3 million queries on JSON. Our study reveals significant temporal and spatial correlations among those queries, which result in massive redundant parsing operations among queries. Instead of repetitively parsing the JSON data, we propose to develop a cache system named Maxson for caching the JSON query results (the values evaluated from JSONPath) for reuse. Specifically, we develop effective machine learning-based predictor with combining LSTM (long shortterm memory) and CRF (conditional random field) to determine the JSONPaths to cache given the space budget. We have implemented Maxson on top of SparkSQL. We experimentally evaluate Maxson and show that 1) Maxson is able to eliminate the most of duplicate JSON parsing overhead, 2) Maxson improves end-to-end workload performance by 1.5&#x2013;6.5&#x00D7;.",https://ieeexplore.ieee.org/document/9101499/,2020 IEEE 36th International Conference on Data Engineering (ICDE),20-24 April 2020,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IVCNZ51579.2020.9290736,Melanoma and Nevi Classification using Convolution Neural Networks,IEEE,Conferences,"Early identification of melanoma skin cancer is vital for the improvement of patients' prospects of five year disease free survival. The majority of malignant skin lesions present at a general practice level where a diagnosis is based on a clinical decision algorithm. As a false negative diagnosis is an unacceptable outcome, clinical caution tends to result in a low positive predictive value of as low at 8%. There has been a large burden of surgical excisions that retrospectively prove to have been unnecessary.This paper proposes a method to identify melanomas in dermoscopic images using a convolution neural network (CNN). The proposed method implements transfer learning based on the ResNet50 CNN, pretrained using the ImageNet dataset. Datasets from the ISIC Archive were implemented during training, validation and testing. Further tests were performed on a smaller dataset of images taken from the Dermnet NZ website and from recent clinical cases still awaiting histological results to indicate the trained network's ability to generalise to real cases. The 86% test accuracy achieved with the proposed method was comparable to the results of prior studies but required significantly less pre-processing actions to classify a lesion and was not dependant on consistent image scaling or the presence of a scale on the image. This method also improved on past research by making use of all of the information present in an image as opposed to focusing on geometric and colour-space based aspects independently.",https://ieeexplore.ieee.org/document/9290736/,2020 35th International Conference on Image and Vision Computing New Zealand (IVCNZ),25-27 Nov. 2020,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IIAI-AAI.2015.290,Memetic Self-Configuring Genetic Programming for Solving Machine Learning Problems,IEEE,Conferences,"A hybridization of self-configuring genetic programming algorithms (SelfCGPs) with a local search in the space of trees is fulfilled to improve their performance for symbolic regression problem solving and artificial neural network automated design. The local search is implemented with two neighborhood systems (1-level and 2-level neighborhoods), three strategies of a tree scanning (""full"", ""incomplete"" and ""truncated"") and two ways of a movement between adjacent trees (transition by the first improvement and the steepest descent). The Lamarckian local search is applied on each generation to ten percent of best individuals. The performance of all developed memetic algorithms is estimated on a representative set of test problems of the functions approximation as well as on real-world machine learning problems. It is shown that developed memetic algorithms require comparable amount of computational efforts but outperform the original SelfCGPs both for the symbolic regression and neural network design. The best variant of the local search always uses the steepest descent but different tree scanning strategies, namely, full scanning for the solving of symbolic regression problems and incomplete scanning for the neural network automated design. Additional advantage of the approach proposed is a possibility of the automated features selection.",https://ieeexplore.ieee.org/document/7373977/,2015 IIAI 4th International Congress on Advanced Applied Informatics,12-16 July 2015,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA40945.2020.9196540,Meta Reinforcement Learning for Sim-to-real Domain Adaptation,IEEE,Conferences,"Modern reinforcement learning methods suffer from low sample efficiency and unsafe exploration, making it infeasible to train robotic policies entirely on real hardware. In this work, we propose to address the problem of sim-to-real domain transfer by using meta learning to train a policy that can adapt to a variety of dynamic conditions, and using a task-specific trajectory generation model to provide an action space that facilitates quick exploration. We evaluate the method by performing domain adaptation in simulation and analyzing the structure of the latent space during adaptation. We then deploy this policy on a KUKA LBR 4+ robot and evaluate its performance on a task of hitting a hockey puck to a target. Our method shows more consistent and stable domain adaptation than the baseline, resulting in better overall performance.",https://ieeexplore.ieee.org/document/9196540/,2020 IEEE International Conference on Robotics and Automation (ICRA),31 May-31 Aug. 2020,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICASSP39728.2021.9413978,Meta-Learning for Cross-Channel Speaker Verification,IEEE,Conferences,"Automatic speaker verification (ASV) has been successfully deployed for identity recognition. With increasing use of ASV technology in real-world applications, channel mismatch caused by the recording devices and environments severely degrade its performance, especially in the case of unseen channels. To this end, we propose a meta speaker embedding network (MSEN) via meta-learning to generate channel-invariant utterance embeddings. Specifically, we optimize the differences between the embeddings of a support set and a query set in order to learn a channel-invariant embedding space for utterances. Furthermore, we incorporate distribution optimization (DO) to stabilize the performance of MSEN. To quantitatively measure the effect of MSEN on unseen channels, we specially design the generalized cross-channel (GCC) evaluation. The experimental results on the HI-MIA corpus demonstrate that the proposed MSEN reduce considerably the impact of channel mismatch, while significantly outperforms other state-of-the-art methods.",https://ieeexplore.ieee.org/document/9413978/,"ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",6-11 June 2021,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SOFTWAREMINING.2017.8100847,Mining temporal intervals from real-time system traces,IEEE,Conferences,"We introduce a novel algorithm for mining temporal intervals from real-time system traces with linear complexity using passive, black-box learning. Our interest is in mining nfer specifications from spacecraft telemetry to improve human and machine comprehension. Nfer is a recently proposed formalism for inferring event stream abstractions with a rule notation based on Allen Logic. The problem of mining Allen's relations from a multivariate interval series is well studied, but little attention has been paid to generating such a series from symbolic time sequences such as system traces. We propose a method to automatically generate an interval series from real-time system traces so that they may be used as inputs to existing algorithms to mine nfer rules. Our algorithm has linear runtime and constant space complexity in the length of the trace and can mine infrequent intervals of arbitrary length from incomplete traces. The paper includes results from case studies using logs from the Curiosity rover on Mars and two other realistic datasets.",https://ieeexplore.ieee.org/document/8100847/,2017 6th International Workshop on Software Mining (SoftwareMining),3-3 Nov. 2017,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AMS.2008.128,Misuse Intrusion Detection Using a Fuzzy-Metaheuristic Approach,IEEE,Conferences,"In this paper, we use simulated annealing heuristics for constructing an intrusion detection system (IDS). The proposed IDS combines the learning ability of simulated annealing heuristics with the approximate reasoning method of fuzzy systems. The use of simulated annealing is an effort to effectively explore the large search space related to intrusion detection problems, and find the optimum set of fuzzy if-then rules. The aim of this paper is to present the capability of simulated annealing based fuzzy system to deal with intrusion detection classification problem as a new real-world application area. Experiments were performed with KDD-Cup99 intrusion detection benchmark data set.",https://ieeexplore.ieee.org/document/4530516/,2008 Second Asia International Conference on Modelling & Simulation (AMS),13-15 May 2008,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/DSAA49011.2020.00024,Mix2Vec: Unsupervised Mixed Data Representation,IEEE,Conferences,"Unsupervised representation learning on mixed data is highly challenging but rarely explored. It has to tackle significant challenges related to common issues in real-life mixed data, including sparsity, dynamics and heterogeneity of attributes and values. This work introduces an effective and efficient unsupervised deep representer called Mix2Vec to automatically learn a universal representation of dynamic mixed data with the above complex characteristics. Mix2Vec is empowered with three effective mechanisms: random shuffling prediction, prior distribution matching, and structural informativeness maximization, to tackle the aforementioned challenges. These mechanisms are implemented as an unsupervised deep neural representer Mix2Vec. Mix2Vec converts complex mixed data into vector space-based representations that are universal and comparable to all data objects and transparent and reusable for both unsupervised and supervised learning tasks. Extensive experiments on four large mixed datasets demonstrate that Mix2Vec performs significantly better than state-of-the-art deep representation methods. We also empirically verify the designed mechanisms in terms of representation quality, visualization and capability of enabling better performance of downstream tasks.",https://ieeexplore.ieee.org/document/9260035/,2020 IEEE 7th International Conference on Data Science and Advanced Analytics (DSAA),6-9 Oct. 2020,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CW.2019.00011,Mixed Reality User Interface for Astronauts Procedure Viewer,IEEE,Conferences,"This paper introduces a Proof-of-Concept (PoC) for Mixed Reality (MR) system to support an astronaut's manual work, the system is called MobiPV4Hololens. It has been developed in the European Space Agency's (ESA) project called ""MobiPV4Hololens - Prototype a Media Helmet for MobiPV Implemented Using Microsoft (MS) HoloLens"". The MS HoloLens mixed reality platform was integrated as the hands-free user interface to the ESA Mobile Procedure Viewer system called MobiPV. Based on the user evaluation most of the users believe that the MobiPV4Hololens is beneficial in supporting procedure execution.",https://ieeexplore.ieee.org/document/8919243/,2019 International Conference on Cyberworlds (CW),2-4 Oct. 2019,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICSGRC.2017.8070573,Mobile outdoor parking space detection application,IEEE,Conferences,"Finding a vacant parking space in outdoor parking lots is a daily concern of most vehicle drivers during rush hours, especially in the urban context. In this paper, an outdoor parking space vacancy detection system is proposed, using mobile devices to improve parking space searching experience for vehicle drivers by providing them with the location and occupancy information of parking spaces. The system uses state-of-the-art image recognition algorithm, namely Convolutional Neural Network with a Raspberry Pi to identify vacant parking spaces from a parking lot image retrieved in real time via an IP camera. A university parking lot has been chosen as the test bed to deploy the proposed system for real time parking space vacancy detection. An Android smartphone application called Driver App is developed to enable ubiquitous visualization of real time outdoor parking spaces occupancy information for vehicle drivers. Evaluation outcomes based on the responses to System Usability Scale (SUS) questionnaire revealed high usability of the Driver App as a tool that provides smart parking service to assist vehicle drivers in searching for a vacant parking space.",https://ieeexplore.ieee.org/document/8070573/,2017 IEEE 8th Control and System Graduate Research Colloquium (ICSGRC),4-5 Aug. 2017,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ROBOT.1998.681416,Mobile robot exploration and map-building with continuous localization,IEEE,Conferences,"Our research addresses how to integrate exploration and localization for mobile robots. A robot exploring and mapping an unknown environment needs to know its own location, but it may need a map in order to determine that location. In order to solve this problem, we have developed ARIEL, a mobile robot system that combines frontier based exploration with continuous localization. ARIEL explores by navigating to frontiers, regions on the boundary between unexplored space and space that is known to be open. ARIEL finds these regions in the occupancy grid map that it builds as it explores the world. ARIEL localizes by matching its recent perceptions with the information stored in the occupancy grid. We have implemented ARIEL on a real mobile robot and tested ARIEL in a real-world office environment. We present quantitative results that demonstrate that ARIEL can localize accurately while exploring, and thereby build accurate maps of its environment.",https://ieeexplore.ieee.org/document/681416/,Proceedings. 1998 IEEE International Conference on Robotics and Automation (Cat. No.98CH36146),20-20 May 1998,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SAMI.2015.7061869,Model Predictive Control of a Ball and Plate laboratory model,IEEE,Conferences,"The papers presents an implementation of the predictive state space control algorithm, called Model Predictive Control (MPC). This control algorithm is verified on the Ball and Plate laboratory model, called B&P_KYB, for the reference trajectory tracking. The control algorithm is first verified using the derived nonlinear simulation model in Matlab/Simulink. Since simulation results are acceptable, an experiment is realized on the real laboratory model. The results of the experiment are demonstrated as the time response of the ball position and the voltage.",https://ieeexplore.ieee.org/document/7061869/,2015 IEEE 13th International Symposium on Applied Machine Intelligence and Informatics (SAMI),22-24 Jan. 2015,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ROBOT.1992.220046,Model-driven pose correction,IEEE,Conferences,"Pose determination for robot navigation is discussed. The problem is to maintain the system's instantaneous precept of its position and orientation in space for performing various tasks. The authors describe a system in which models were used to guide the sensory interpretation and to correct expectations. In this system, simulated images were used to analyze the real images and to correct the pose parameters. The reported techniques have been implemented and experiments with real images in a real environment have been performed.<>",https://ieeexplore.ieee.org/document/220046/,Proceedings 1992 IEEE International Conference on Robotics and Automation,12-14 May 1992,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICTAI.2006.86,Modeling Workflow Systems with Genetic Planner and Scheduler,IEEE,Conferences,"Workflow systems have been widely employed by organizations in general; however, the generation of process models is still an area to be explored. Some works are concentrated in the use of planning techniques to solve problems; however, one of the difficulties in applying such techniques in workflow problems is the size of the search space required for the real world problems. An alternative is the use of evolutionary computing techniques, particularly genetic algorithms that in general are more suitable for these problems. In this context, we will present an architecture based on the use of a genetic planner in order to allow the automatic generation of process modeling. A simulation environment is also proposed by using scheduling techniques based on the use of genetic algorithms to identify the most suitable process model",https://ieeexplore.ieee.org/document/4031922/,2006 18th IEEE International Conference on Tools with Artificial Intelligence (ICTAI'06),13-15 Nov. 2006,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/POWERI.2016.8077385,Modelling and design of a modified neuro-fuzzy control-based IM drive via feedback linearization,IEEE,Conferences,"This research paper presents a design of a simplified structured adaptive neuro-fuzzy controller (NFC) technique with an intuitive feedback linearization controlled induction motor (IM) model for extensive torque and speed ripple minimization with better performance enhancement of IM drive. The non-linear dynamics of IM is modeled and simulated based on state space linearization technique in the stationary reference frame. The proposed simplified adaptive NFC is the fusion approach of fuzzy logic and neural network with one input as torque error unlike conventional two-input NFC as torque error and change in torque error. It also improves the computational efficiency by making the structure very simple and robust as compared to the conventional NFC, thereby easy to apply in a realistic environment. The effectiveness and execution of the proposed control technique based linearized IM drive is investigated in MATLAB environment in various operating conditions and is contrasted with the conventional two-input NFC as well as PI-controller to analyze the superior performance of IM drive. The system is also implemented in real-time system using DSP 2812 to validate the different control strategies.",https://ieeexplore.ieee.org/document/8077385/,2016 IEEE 7th Power India International Conference (PIICON),25-27 Nov. 2016,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICIINFS.2017.8300340,Modelling and simulation analysis of the genetic-fuzzy controller for speed regulation of a sensored BLDC motor using MATLAB/SIMULINK,IEEE,Conferences,"This paper presents the speed regulation of a Sensored BLDC (Brushless Direct Current) Motor through a Genetic-Fuzzy controller, where the Sensored BLDC motor was modeled in MATLAB Simulink environment according to State-Space analysis approach. When designing Fuzzy Logic controllers (FLCs) there is no generalized defined approach and these controllers are mainly based on linguistically defined variables which are non-linear elements, that are impossible to model accurately. Our test results shows that the fuzzy controller's output highly depends on the fuzzy rules. In some situations, very experienced and a skillful expert's solutions (fuzzy rules) even may not satisfy the desired output. In many cases FLCs rule bases have been designed according to trial-and-error method which makes the optimization of the solution very difficult. As a solution, FLC of the Sensored BLDC motor was tuned through a stochastic search optimization technique which is based on GA (Genetic Algorithm) and the GA parameters (Crossover, Mutation rates etc.) adapted through another TSK-FLC (Takagi-Sugeno-Kang type FLC) in real-time. The optimization stochastic search process was implemented using a fitness function index (i.e. a predefined threshold level) which is calculated from the population (randomly generated solutions by the GA) based on the E (Error), MAE (Mean Absolute Error) and the RMSE (Root Mean Square Error). The simulated test results shows the proposed control technique has effectively reduced the maximum overshoot, settling time, steady state error and the rise time by 12%, 15%, 11% and 1% respectively. But further research is needed to optimize the search algorithm to increase the Genetic-Fuzzy controller's efficiency and the stability to withstand external disturbances while increasing the frequency for various desired input signal wave pattern trajectories.",https://ieeexplore.ieee.org/document/8300340/,2017 IEEE International Conference on Industrial and Information Systems (ICIIS),15-16 Dec. 2017,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IS3C.2014.69,Monitoring System of Coal Electrical Haulage Shearer Based on Data Fusion Theory,IEEE,Conferences,"The electrical haulage coal shearer is very important cutting equipment in underground digging coal working space and plays a vital role for the coal mining production. But the coal mining environments are very harsher than ground, the shearer is often broken and stopping work, so its working states must be monitored. In this paper, a novel coal real-time monitoring system for the electrical haulage coal shearer is proposed based on multi-sensor data fusion theory. The four different kinds of sensors are used to supervise temperature, smog thickness, motor voltage and current for the key parts of the shearer. Fuzzy logic characteristic fusion algorithm and the quantum neural network fusion algorithm are adopted for fault diagnosis. The ARM12 is used as the master chip to carry out the on-line monitoring running parameters, and the Lab VIEW software is used to real-time display the running state of the shearer via CAN bus. The theory analysis and simulation results show that the system has higher reliable and more practical.",https://ieeexplore.ieee.org/document/6845861/,"2014 International Symposium on Computer, Consumer and Control",10-12 June 2014,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/NCVPRIPG.2013.6776185,Monitoring a large surveillance space through distributed face matching,IEEE,Conferences,Large space with many cameras require huge storage and computational power to process these data for surveillance applications. In this paper we propose a distributed camera and processing based face detection and recognition system which can generate information for finding spatiotemporal movement pattern of individuals over a large monitored space. The system is built upon Hadoop Distributed File System using map reduce programming model. A novel key generation scheme using distance based hashing technique has been used for distribution of the face matching task. Experimental results have established effectiveness of the technique.,https://ieeexplore.ieee.org/document/6776185/,"2013 Fourth National Conference on Computer Vision, Pattern Recognition, Image Processing and Graphics (NCVPRIPG)",18-21 Dec. 2013,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICMA.2019.8816292,Monitoring and Warning for Digital Twin-driven Mountain Geological Disaster,IEEE,Conferences,"With the unprecedented increase in the complexity of human space activities, geological disasters are increasing. In recent years, with the development of information technology and artificial intelligence, the monitoring and prediction of geological disasters requires real-time, dynamic and the early warning process to be more intelligent. A digital twin can better combine physical space and information space. Through the fusion of these two spaces, a digital disaster monitoring and early warning system based on digital twin driving is established. Firstly, the system uses Internet of Things, data-driven technology, and so on, combined with the BP neural network algorithm for dynamic prediction of geological disasters. Secondly, the disaster information is used to alert nearby residents through the Internet and base stations. On this basis, the operational mechanism, key technologies and implementation methods of geological disaster monitoring and early warning based on digital twin driving are discussed.",https://ieeexplore.ieee.org/document/8816292/,2019 IEEE International Conference on Mechatronics and Automation (ICMA),4-7 Aug. 2019,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICASSP40776.2020.9053581,Multi-Conditioning and Data Augmentation Using Generative Noise Model for Speech Emotion Recognition in Noisy Conditions,IEEE,Conferences,"Degradation due to additive noise is a significant road block in the real-life deployment of Speech Emotion Recognition (SER) systems. Most of the previous work in this field dealt with the noise degradation either at the signal or at the feature level. In this paper, to address the robustness aspect of the SER in additive noise scenarios, we propose multi-conditioning and data augmentation using an utterance level parametric Generative noise model. The Generative noise model is designed to generate noise types which can span the entire noise space in the mel-filterbank energy domain. This characteristic of the model renders the system robust against unseen noise conditions. The generated noise types can be used to create multiconditioned data for training the SER systems. Multi-conditioning approach can also be used to increase the training data by many folds where such data is limited. We report the performance of the proposed method on two datasets, namely EmoDB and IEMOCAP. We also explore multi-conditioning and data augmentation using noise samples from NOISEX-92 database.",https://ieeexplore.ieee.org/document/9053581/,"ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",4-8 May 2020,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SMC52423.2021.9659268,Multi-Exemplar Learning Particle Swarm Optimization for Regional Traffic Signal Timing Optimization with Multi-Intersections,IEEE,Conferences,"Traffic congestion has become one of the major problems of smart travel. The application of evolutionary computation (EC) for traffic signal timing optimization (TSTO) can effectively alleviate traffic congestion at a single intersection. However, while in more complicated regional traffic signal timing optimization (RTSTO) problems, the canonical EC algorithm such as particle swarm optimization (PSO) still has limitation due to population prematurity. In this paper, a multi-exemplar learning (MEL) strategy is adopted to improve the diversity of the population, so that the particles can have more opportunities to explore the search space. Furthermore, multiple traffic indicators are used in this paper to measure the comprehensive performance of the solution. Moreover, the microsimulation software is adopted to evaluate the solution to simulate the real-world intersection, making the obtained solution more practical in real-world application. Experiments are conducted to investigate the effectiveness and efficiency of MEL-PSO. The results show that the MEL-PSO algorithm is more effective and efficient than the compared algorithms on RTSTO problems.",https://ieeexplore.ieee.org/document/9659268/,"2021 IEEE International Conference on Systems, Man, and Cybernetics (SMC)",17-20 Oct. 2021,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ITAIC54216.2022.9836717,Multi-MEC cooperative transmission scheme based on real-time access requirement of power multi-service,IEEE,Conferences,"In view of the wide distribution points of different types of power services, randomness of time and space of business flows, and the inability of local edge cloud resources to meet the requirements of the minimum tolerance index of services due to massive flexible access, this paper provides a cross-MEC resource management method that takes into account the dynamic demands of power services. By aggregating the resource units of multiple MECs and allocating them to different services on demand, the collaboration among MECs can improve the service access demand satisfaction rate and solve the problem of massive multi-service flexible access demand in the power industry.",https://ieeexplore.ieee.org/document/9836717/,2022 IEEE 10th Joint International Information Technology and Artificial Intelligence Conference (ITAIC),17-19 June 2022,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICTAI.2011.89,Multi-agent Simulation Design Driven by Real Observations and Clustering Techniques,IEEE,Conferences,"The multi-agent simulation consists in using a set of interacting agents to reproduce the dynamics and the evolution of the phenomena that we seek to simulate. It is considered now as an alternative to classical simulations based on analytical models. But, its implementation remains difficult, particularly in terms of behaviors extraction and agents modelling. This task is usually performed by the designer who has some expertise and available observation data on the process. In this paper, we propose a novel way to make use of the observations of real world agents to model simulated agents. The modelling is based on clustering techniques. Our approach is illustrated through an example in which the behaviors of agents are extracted as trajectories and destinations from video sequences analysis. This methodology is investigated with the aim to apply it, in particular, in a retail space simulation for the evaluation of marketing strategies. This paper presents experiments of our methodology in the context of a public area modelling.",https://ieeexplore.ieee.org/document/6103379/,2011 IEEE 23rd International Conference on Tools with Artificial Intelligence,7-9 Nov. 2011,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/BigDataSecurity-HPSC-IDS49724.2020.00042,Multi-label Classification for Clinical Text with Feature-level Attention,IEEE,Conferences,"Multi-label text classification, which tags a given plain text with the most relevant labels from a label space, is an important task in the natural language process. To diagnose diseases, clinical researchers use a machine-learning algorithm to do multi-label clinical text classification. However, conventional machine learning methods can neither capture deep semantic information nor the context of words strictly. Diagnostic information from the EHRs (Electronic Health Records) is mainly constructed by unstructured clinical free text which is an obstacle for clinical feature extraction. Moreover, feature engineering is time-consuming and labor-intensive. With the rapid development of deep learning, we apply neural network models to resolve this problem mentioned above. To favor multi-label classification on EHRs, we propose FAMLC-BERT (Feature-level Attention for Multi-label classification on BERT) to capture semantic features from different layers. The model uses feature-level attention with BERT to recognize the labels of EHRs. We empirically compared our model with other state-of-the-art models on real-world documents collected from the hospital. Experiments show that our model achieved significant improvements compared to other selected benchmarks.",https://ieeexplore.ieee.org/document/9123057/,"2020 IEEE 6th Intl Conference on Big Data Security on Cloud (BigDataSecurity), IEEE Intl Conference on High Performance and Smart Computing, (HPSC) and IEEE Intl Conference on Intelligent Data and Security (IDS)",25-27 May 2020,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCP.2013.6646076,Multi-objective DSE algorithms' evaluations on processor optimization,IEEE,Conferences,"Very complex micro-architectures, like complex superscalar/SMT or multicore systems, have lots of configurations. Exploring this huge design space and trying to optimize multiple objectives, like performance, power consumption and hardware complexity is a real challenge. In this paper, using the multi-objective design space exploration tool FADSE, we tried to optimize the hardware parameters of the complex superscalar Grid ALU Processor. We compared how different heuristic algorithms handle the DSE optimization. Three of these algorithms are taken from the jMetal library (NSGAII, SPEA2 and SMPSO) while the other two, CNSGAII and MOHC were implemented by us. We show that in this huge design space the differences between the best found individuals by every algorithm are very small, only the time in which they got to these solutions differs. In order to accelerate the DSE process we also did a feature selection through machine learning techniques and ran all DSE algorithms again with a smaller number of input parameters.",https://ieeexplore.ieee.org/document/6646076/,2013 IEEE 9th International Conference on Intelligent Computer Communication and Processing (ICCP),5-7 Sept. 2013,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1049/ic:19970129,Multi-objective genetic algorithm partitioning for hierarchical learning of high dimensional spaces,IET,Conferences,Complex pattern recognition problems of high dimensionality are best addressed through a 'divide-and-conquer' approach rather than monolithically. We introduce a novel approach to partitioning the pattern space into hyperspheres using a multiobjective genetic algorithm for subsequent mapping onto a hierarchical neural network for subspace learning. In our technique clusters are generated on the basis of 'fitness for purpose'-they are explicitly optimised for their subsequent mapping onto the hierarchical classifier-rather than emerging as some implicit property of the clustering algorithm. Multi-objective genetic algorithms perform optimisation on a vector space of objectives and are able to explore the NP-complete search space for a set of equally viable partitions of the pattern space. The rationale behind this strategy is set-out and the objectives used for (near-) optimum partitioning of feature spaces for hierarchical learning are identified. Implementation details are described in brief and results presented for both high dimensional synthetic and real data.,https://ieeexplore.ieee.org/document/598541/,IEE Colloquium on Pattern Recognition (Digest No. 1997/018),26-26 Feb. 1997,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SMCIA.2005.1466971,Multiobjective route selection for car navigation system using genetic algorithm,IEEE,Conferences,"Route planning is an important problem for a car navigation system. Given a set of origin-destination pair, there could be many possible routes for a driver. Search for shortest route from one point to another on a weighted graph is a well known problem and has several solutions like Dijkstra algorithm, Bellman-Ford algorithm etc. But in case of car navigation systems the shortest path may not be the best one from the point of view of driver's satisfaction. So, for a practical car navigation system in dynamical environment, we need to specify multiple and separate good (near optimal) choices according to multiple criteria which make the search space too large to find out the solution in real time by deterministic algorithms. Genetic algorithms (GA) are now widely used to solve search problems with applications in practical routing and optimization problems. GA includes a variety of quasi optimal solutions, which can be obtained in a given time. In this work we propose a GA based algorithm to find out simultaneously several alternate routes depending on different criterion according to driver's choice such as shortest path by distance, path which contains minimum number of turns, path passing through mountains or by the side of a river etc. The proposed algorithm has been evaluated by simulation experiment using real road map compared to other existing GA based algorithms. It has been found that the proposed algorithm is quite efficient in finding alternate non overlapping routes with different characteristics.",https://ieeexplore.ieee.org/document/1466971/,"Proceedings of the 2005 IEEE Midnight-Summer Workshop on Soft Computing in Industrial Applications, 2005. SMCia/05.",28-30 June 2005,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CEC.2007.4424774,Multiple sensors data integration using MFAM for mobile robot navigation,IEEE,Conferences,"The mobile robot navigation with complex environment needs more input space to match the environmental data into robot outputs in order to perform realistic task. At the same time, the number of rules at the rule base needs to be optimized to reduce the computing time and to provide the possibilities for real time operation. In this paper, the optimization of fuzzy rules using a modified fuzzy associative memory (MFAM) is designed and implemented. MFAM provides good flexibility to use multiple input space and reduction of rule base for robot navigation. This paper presents the MFAM model to generate the rule base for robot navigation. The behavior rules obtained from MFAM model are tested using simulation and real world experiments, and the results are discussed in the paper and compared with the existing methods.",https://ieeexplore.ieee.org/document/4424774/,2007 IEEE Congress on Evolutionary Computation,25-28 Sept. 2007,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/WCICA.2000.862545,Multipurpose virtual-reality-based motion simulator,IEEE,Conferences,"Public security has become an important issue everywhere. Especially, the safe manipulation and control of various machines and vehicles has gained special attention such that the authorities keep emphasizing the strict training and censoring of human operators. Currently, such training and censoring process usually relies on the actual machines, equipment, or vehicles in the real sites. This not only has high demands in space, time and cost, but also causes another public security problem. In this connection, the world-wide trend is to tackle the above dilemma by using virtual reality (VR). However, the current researches or products on VR are more matured in the software display part of VR. How to combine 3D VR display with motion platform to achieve the aforementioned training and censoring purposes is an important research issue. This paper focuses on this research issue, and the goal is to develop a multipurpose virtual-reality-based motion simulation system to meet the requirements of public security in training and censoring of human operators.",https://ieeexplore.ieee.org/document/862545/,Proceedings of the 3rd World Congress on Intelligent Control and Automation (Cat. No.00EX393),26 June-2 July 2000,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IJCNN.2017.7966020,Multiscale Hebbian neural network for cyber threat detection,IEEE,Conferences,"The recent blaze in cyber espionage has posed unprecedented challenges to the cutting edge network intrusion detection systems in terms of accurate and precise classification of dynamically evolving threats. Along with the traditional signature based detection, the supervised and unsupervised machine learning algorithms are also being deployed to detect advance anomalies. However, due to the class overlap between the threat and legitimate data over feature space, satisfactory detection results cannot be obtained. This necessitates the introduction of cognition in the domain of cyber-security. In this paper, a wavelet based multiscale Hebbian learning approach in neural networks is introduced to address the challenge of class overlap. Contrary to inherently linear single scale Hebbian learning, the proposed methodology is able to distinguish non-linear and overlapping classification boundaries sufficiently well. A comparison of presented techniques with fundamental gradient descent based neural network shows promising results. Experimental results on simulated and real-world UNSW-NB15 dataset have been presented to support the claim.",https://ieeexplore.ieee.org/document/7966020/,2017 International Joint Conference on Neural Networks (IJCNN),14-19 May 2017,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/KIMAS.2003.1245070,Multisensor image fusion & mining: from neural systems to COTS software,IEEE,Conferences,"We summarize our methods for the fusion of multisensor imagery based on concepts derived from neural models of visual processing and pattern learning and recognition. These methods have been applied to real-time fusion of night vision sensors in the field, airborne multispectral and hyperspectral imaging systems, and space-based multiplatform multimodality sensors. The methods enable color fused 3D visualization, as well as interactive exploitation and data mining in the form of human-guided machine learning and search for targets and cultural features. Over the last year we have developed a user-friendly system integrated into a COTS exploitation environment known as ERDAS Imagine. We demonstrate fusion and interactive mining of low-light Visible/SWIR/MWIR/LWIR night imagery, and IKONOS multispectral imagery. We also demonstrate how target learning and search can be enabled over extended operating conditions by allowing training over multiple scenes. This is illustrated for detecting small boats in coastal waters using fused Visible/MWIR/LWIR imagery.",https://ieeexplore.ieee.org/document/1245070/,IEMC '03 Proceedings. Managing Technologically Driven Organizations: The Human Side of Innovation and Change (IEEE Cat. No.03CH37502),30 Sept.-4 Oct. 2003,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/WARSD.2003.1295180,Multisensor image fusion and mining: from neural systems to COTS software with application to remote sensing AFE,IEEE,Conferences,"We summarize our methods for the fusion of multisensor/spectral imagery based on concepts derived from neural models of visual processing (adaptive contrast enhancement, opponent-color contrast, multi-scale contour completion, and multi-scale texture enhancement) and semi-supervised pattern learning and recognition. These methods have been applied to the problem of aided feature extraction (AFE) from remote sensing airborne multispectral and hyperspectral imaging systems, and space-based multi-platform multi-modality imaging sensors. The methods enable color fused 3D visualization, as well as interactive exploitation and data mining in the form of human-guided machine learning and search for objects, landcover, and cultural features. This technology has been evaluated on space-based imagery for the National Imagery and Mapping Agency, and real-time implementation has also been demonstrated for terrestrial fused-color night imaging. We have recently incorporated these methods into a commercial software platform (ERDAS Imagine) for imagery exploitation. We describe the approach and user interfaces, and show results for a variety of sensor systems with application to remote sensing feature extraction including EO/IR/MSI/SAR imagery from Landsat and Radarsat, multispectral Ikonos imagery, and Hyperion and HyMap hyperspectral imagery.",https://ieeexplore.ieee.org/document/1295180/,"IEEE Workshop on Advances in Techniques for Analysis of Remotely Sensed Data, 2003",27-28 Oct. 2003,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CCA.1993.348267,Multivariable neural network vibration control based on output feedback,IEEE,Conferences,"This paper presents a multivariable direct adaptive control concept for vibration suppression in flexible space structures. The adaptive controller is implemented by a combination of forward neural networks. Tuning of the controller gains (neural network synaptic weights) takes place in real-time and is performed by a nonlinear least squares algorithm. The control scheme is based on output rather than state feedback, an approach motivated from the fact that, in most applications, the system state is not readily available. The results are demonstrated by simulation using a high fidelity 6-input 6-output dynamic model of the testbed at the JPL/USAF-PL Large Spacecraft Control Laboratory.<>",https://ieeexplore.ieee.org/document/348267/,Proceedings of IEEE International Conference on Control and Applications,13-16 Sept. 1993,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IJCNN.2018.8489481,NEGAN:Network Embedding based on Generative Adversarial Networks,IEEE,Conferences,"Network embedding, also known as graph representation, is a classical topic in data mining. It has been widely used in real-world network applications such as node classification and community detection. However, it remains open to find a method that is scalable and preserves both structure and content information. Based on generative adversarial networks, we propose an unsupervised network embedding framework NEGAN, which is featured by combining graph topology and node content. In NEGAN, network nodes are mapped to the target space in a highly flexible non-linear way, guided by the content of the nodes. This mapping is learned from the generator of the generative adversarial networks, and node adjacency in the input network is preserved. Experiments on real datasets show that NEGAN outperforms all the existing methods on many scenarios including node classification, visualization and community detection tasks.",https://ieeexplore.ieee.org/document/8489481/,2018 International Joint Conference on Neural Networks (IJCNN),8-13 July 2018,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/MICRO.2016.7783724,NEUTRAMS: Neural network transformation and co-design under neuromorphic hardware constraints,IEEE,Conferences,"With the recent reincarnations of neuromorphic computing comes the promise of a new computing paradigm, with a focus on the design and fabrication of neuromorphic chips. A key challenge in design, however, is that programming such chips is difficult. This paper proposes a systematic methodology with a set of tools to address this challenge. The proposed toolset is called NEUTRAMS (Neural network Transformation, Mapping and Simulation), and includes three key components: a neural network (NN) transformation algorithm, a configurable clock-driven simulator of neuromorphic chips and an optimized runtime tool that maps NNs onto the target hardware for better resource utilization. To address the challenges of hardware constraints on implementing NN models (such as the maximum fan-in/fan-out of a single neuron, limited precision, and various neuron models), the transformation algorithm divides an existing NN into a set of simple network units and retrains each unit iteratively, to transform the original one into its counterpart under such constraints. It can support both spiking neural networks (SNNs) and traditional artificial neural networks (ANNs), including convolutional neural networks (CNNs) and multilayer perceptrons (MLPs) and recurrent neural networks (RNNs). With the combination of these tools, we have explored the hardware/software co-design space of the correlation between network error-rates and hardware constraints and consumptions. Doing so provides insights which can support the design of future neuromorphic architectures. The usefulness of such a toolset has been demonstrated with two different designs: a real Complementary Metal-Oxide-Semiconductor (CMOS) neuromorphic chip for both SNNs and ANNs and a processing-in-memory architecture design for ANNs.",https://ieeexplore.ieee.org/document/7783724/,2016 49th Annual IEEE/ACM International Symposium on Microarchitecture (MICRO),15-19 Oct. 2016,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1145/3326285.3329056,NFVdeep: Adaptive Online Service Function Chain Deployment with Deep Reinforcement Learning,IEEE,Conferences,"With the evolution of network function virtualization (NFV), diverse network services can be flexibly offered as service function chains (SFCs) consisted of different virtual network functions (VNFs). However, network state and traffic typically exhibit unpredictable variations due to stochastically arriving requests with different quality of service (QoS) requirements. Thus, an adaptive online SFC deployment approach is needed to handle the real-time network variations and various service requests. In this paper, we firstly introduce a Markov decision process (MDP) model to capture the dynamic network state transitions. In order to jointly minimize the operation cost of NFV providers and maximize the total throughput of requests, we propose NFVdeep, an adaptive, online, deep reinforcement learning approach to automatically deploy SFCs for requests with different QoS requirements. Specifically, we use a serialization-and-backtracking method to effectively deal with large discrete action space. We also adopt a policy gradient based method to improve the training efficiency and convergence to optimality. Extensive experimental results demonstrate that NFVdeep converges fast in the training process and responds rapidly to arriving requests especially in large, frequently transferred network state space. Consequently, NFVdeep surpasses the state-of-the-art methods by 32.59% higher accepted throughput and 33.29% lower operation cost on average.",https://ieeexplore.ieee.org/document/9068634/,2019 IEEE/ACM 27th International Symposium on Quality of Service (IWQoS),24-25 June 2019,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IECON.2000.972604,NN controller of the constrained robot under unknown constraint,IEEE,Conferences,"In this paper, the problems faced in the constrained force control is studied (uncertainties in dynamic model and the unknown constraints). A neural network (NN) controller is proposed based on the derived dynamic model of robot in the task space. The feed-forward neural network is used to adaptively compensate for the uncertainties in the robot dynamics. Training signals are proposed for the feed-forward neural network controller. The NN weights are tuned online, with no off-line learning phase required. An online estimation algorithm is developed to estimate the local shape of the constraint surface by using measured data on the force and position of the end-effector. The suggested controller is simple in structure and can be implemented easily. Real-time experiments are conducted using the five-bar robot to demonstrate the effectiveness of the proposed controller.",https://ieeexplore.ieee.org/document/972604/,"2000 26th Annual Conference of the IEEE Industrial Electronics Society. IECON 2000. 2000 IEEE International Conference on Industrial Electronics, Control and Instrumentation. 21st Century Technologies",22-28 Oct. 2000,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CVPR46437.2021.01403,NPAS: A Compiler-aware Framework of Unified Network Pruning and Architecture Search for Beyond Real-Time Mobile Acceleration,IEEE,Conferences,"With the increasing demand to efficiently deploy DNNs on mobile edge devices, it becomes much more important to reduce unnecessary computation and increase the execution speed. Prior methods towards this goal, including model compression and network architecture search (NAS), are largely performed independently, and do not fully consider compiler-level optimizations which is a must-do for mobile acceleration. In this work, we first propose (i) a general category of fine-grained structured pruning applicable to various DNN layers, and (ii) a comprehensive, compiler automatic code generation framework supporting different DNNs and different pruning schemes, which bridge the gap of model compression and NAS. We further propose NPAS, a compiler-aware unified network pruning and architecture search. To deal with large search space, we propose a meta-modeling procedure based on reinforcement learning with fast evaluation and Bayesian optimization, ensuring the total number of training epochs comparable with representative NAS frameworks. Our framework achieves 6.7ms, 5.9ms, and 3.9ms ImageNet inference times with 78.2%, 75% (MobileNet-V3 level), and 71% (MobileNet-V2 level) Top-1 accuracy respectively on an off-the-shelf mobile phone, consistently outperforming prior work.",https://ieeexplore.ieee.org/document/9578043/,2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),20-25 June 2021,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICPPW.2009.6,Network Anomaly Detection Using Dissimilarity-Based One-Class SVM Classifier,IEEE,Conferences,"We present a new network anomaly detection system using dissimilarity-based one-class support vector machine( DSVMC). we transform the raw data into a dissimilarity space using Dissimilarity Representations (DR). DR describe objects by their dissimilarities to a set of target class. DSVMC are constructed on these DR. We propose a framework of anomaly detection using DSVMC. A new strategy of prototype selection has been proposed to obtain better DR. We not only offer a better approach in strategy to describe to distribution of large training dataset but also reduce the computational cost of prototype selection largely. In order to deploy the ADS in real-time detection application, we use Kernel Primary Component Analysis (KPCA) to reduce the dimension of transformed data. Evaluation has been made among traditional one-class classifiers, the dissimilarity-based one class SVM classifier without optimization of DR (WSVMC) and our DSVMC on KDDCUP' 99 dataset. The results show that DSVMC can achieve high detection rate than WSVMC and more robust performance than traditional one-class classifiers.",https://ieeexplore.ieee.org/document/5364550/,2009 International Conference on Parallel Processing Workshops,22-25 Sept. 2009,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IJCNN.2004.1380187,Neural network approach for user activity monitoring in computer networks,IEEE,Conferences,"A system is proposed for user activity monitoring in computer networks. The system is based on the use of neural networks and is implemented using agent approach. The monitoring system allows to detect anomalies in user activity, and consists of two components-on-line and off-line. On-line monitoring is carried out in real time and is used to predict the processes started by an user on the basis of previous ones. Off-line monitoring is carried out at the end of the day and is based on the analysis of statistical parameters of user behavior (user signature). Both on-line and off-line monitoring use neural network approach to detect anomalies in user behavior. Proposed system was verified on real data obtained in Intranet of Space Research Institute of NASU-NSAU and Institute of Physics and Technologies of National Technical University of Ukraine ""Kiev Polytechnic Institute"".",https://ieeexplore.ieee.org/document/1380187/,2004 IEEE International Joint Conference on Neural Networks (IEEE Cat. No.04CH37541),25-29 July 2004,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/NNSP.2003.1318072,Neural network classifiers for automated video surveillance,IEEE,Conferences,"In automated visual surveillance applications, detection of suspicious human behaviors is of great practical importance. However due to random nature of human movements, reliable classification of suspicious human movements can be very difficult. Artificial neural network (ANN) classifiers can perform well however their computational requirements can be very large for real time implementation. In this paper, a data-based modeling neural network such as modified probabilistic neural network (MPNN) is introduced which partitions the decision space nonlinearly in order to achieve reliable classification, however still with acceptable computations. The experiment shows that the compact MPNN attains good classification performance compared to that of other larger conventional neural network based classifiers such as multilayer perceptron (MLP) and self organising map (SOM).",https://ieeexplore.ieee.org/document/1318072/,2003 IEEE XIII Workshop on Neural Networks for Signal Processing (IEEE Cat. No.03TH8718),17-19 Sept. 2003,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CEC.1999.785521,Neural network training with constrained integer weights,IEEE,Conferences,"Presents neural network training algorithms which are based on the differential evolution (DE) strategies introduced by Storn and Price (J. of Global Optimization, vol. 11, pp. 341-59, 1997). These strategies are applied to train neural networks with small integer weights. Such neural networks are better suited for hardware implementation than the real weight ones. Furthermore, we constrain the weights and biases in the range [-2/sup k/+1, 2/sup k/-1], for k=3,4,5. Thus, they can be represented by just k bits. These algorithms have been designed keeping in mind that the resulting integer weights require less bits to be stored and the digital arithmetic operations between them are more easily implemented in hardware. Obviously, if the network is trained in a constrained weight space, smaller weights are found and less memory is required. On the other hand, the network training procedure can be more effective and efficient when large weights are allowed. Thus, for a given application, a trade-off between effectiveness and memory consumption has to be considered. We present the results of evolution algorithms for this difficult task. Based on the application of the proposed class of methods on classical neural network benchmarks, our experience is that these methods are effective and reliable.",https://ieeexplore.ieee.org/document/785521/,Proceedings of the 1999 Congress on Evolutionary Computation-CEC99 (Cat. No. 99TH8406),6-9 July 1999,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/MELCON.2000.879972,Neural networks arbitration for automatic edge detection of DNA bands in low-contrast images,IEEE,Conferences,"Low-contrast images, such as DNA autoradiograph images, provide a challenge for edge detection techniques, where the detection of the DNA bands within the images and locating their position is vital. In addition, the speed of recognition, high computational cost, and real-time implementation are also problems that haunt image processing. Thus, new measures are required to solve these problems. This paper reports on a new approach to solving the aforementioned problems. The novel idea is based on combining neural network arbitration and scale space analysis to automatically select one optimum scale for the entire image at which scale space edge detection can be applied. This approach to edge detection is formalised in the automatic edge detection scheme (AEDS). The AEDS is implemented on a real-life application namely, the detection of bands within low-contrast DNA autoradiograph images. An accurate comparison is drawn between the AEDS and the grammar-based multiscale analysis technique (GBMAT).",https://ieeexplore.ieee.org/document/879972/,2000 10th Mediterranean Electrotechnical Conference. Information Technology and Electrotechnology for the Mediterranean Countries. Proceedings. MeleCon 2000 (Cat. No.00CH37099),29-31 May 2000,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IPDPS.2014.59,Nitro: A Framework for Adaptive Code Variant Tuning,IEEE,Conferences,"Autotuning systems intelligently navigate a search space of possible implementations of a computation to find the implementation(s) that best meets a specific optimization criteria, usually performance. This paper describes Nitro, a programmer-directed auto tuning framework that facilitates tuning of code variants, or alternative implementations of the same computation. Nitro provides a library interface that permits programmers to express code variants along with meta-information that aids the system in selecting among the set of variants at run time. Machine learning is employed to build a model through training on this meta-information, so that when a new input is presented, Nitro can consult the model to select the appropriate variant. In experiments with five real-world irregular GPU benchmarks from sparse numerical methods, graph computations and sorting, Nitro-tuned variants achieve over 93% of the performance of variants selected through exhaustive search. Further, we describe optimizations and heuristics in Nitro that substantially reduce training time and other overheads.",https://ieeexplore.ieee.org/document/6877283/,2014 IEEE 28th International Parallel and Distributed Processing Symposium,19-23 May 2014,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IPIN.2019.8911773,No-Sweat Detective: No Effort Anomaly Detection for Wi-Fi-Based Localization,IEEE,Conferences,"At present, Wi-Fi localization is the main approach to estimating location indoors. However, age deterioration of the localization model due to dynamic environmental changes degrades its accuracy. Therefore, periodic model recalibration is inescapable. Existing methods for doing this use transfer learning and a small set of additional and supervised datasets. However, the reference points to obtain these datasets are determined either randomly or comprehensively. Such poor datasets catastrophically destabilize model recovery after recalibration because overfitting occurs. We propose a new approach that detects anomalous reference points to gain felicitous supervised datasets in order to prevent overfitting. Unsupervised datasets obtained from off-the-shelf mobile navigation applications, i.e., user logs uploaded from phones, are used. Our approach is implemented in a system we call ""No-Sweat Detective"". The results of an experiment in a controlled environment demonstrate that No-Sweat Detective can detect anomalies caused by environmental changes, and the results of a five-month experiment show that No-Sweat Detective has redundancy against a complex open-space environment in the real world. In addition, it could suppress model age deterioration by up to 10.9% compared to existing methods.",https://ieeexplore.ieee.org/document/8911773/,2019 International Conference on Indoor Positioning and Indoor Navigation (IPIN),30 Sept.-3 Oct. 2019,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IJCNN.2019.8851949,Noise-Aware Network Embedding for Multiplex Network,IEEE,Conferences,"Network embedding aims at learning the latent representations of nodes while preserving the complex structure of the underlying graph. Real-world networks are usually related with each other via common nodes, the so-called multiplex network. To make the data mining work on the multiplex network more actionable, it become urgent and essential to transform it into low-dimension vector space. Recently, several works have been proposed to leverage the complementary information for embedding. However, they suffer from sacrificing distinct properties of the counterparts in different layers, as they preserve much noise information into embedding vectors. In this paper, we propose a Noise-Aware Network Embedding approach for Multiplex Network, namely NANE. Unlike previous works, NANE considers the roles of an identical node in different layers, and adopts a more robust and flexible strategy to rationally integrate the cross-layer information while keeping the unique characteristic of each layer. We perform extensive evaluations on several real-world datasets. The experimental results demonstrate that our NANE can achieve better performance on link prediction task and significantly outperform previous methods especially in noisy multiplex network scenarios.",https://ieeexplore.ieee.org/document/8851949/,2019 International Joint Conference on Neural Networks (IJCNN),14-19 July 2019,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/UEMCON.2018.8796576,Non-Intrusive Activity Detection and Prediction in Smart Residential Spaces,IEEE,Conferences,"Non-intrusive human activity detection and prediction is an important and challenging problem in smart and pervasive spaces. The advantages of such a design are the reduced dependency on the users and fewer security/privacy concerns. However, these also make it difficult to effectively and accurately understand the activities in real-time. In residential spaces, this can be even a bigger challenge due to nonuniform space boundaries and multiple people sharing the space. In this paper, we present a system, that consists of both hardware and software components, capable of detecting and predicting human activities in a smart residential environment. Our system deploys a finite state machine-based activity detection with 96% accuracy in real-time. Afterwards, we use several machine learning methods to create an effective activity prediction framework. We demonstrate that we can achieve up to 98.5% activity prediction accuracy with 4ms delay, making it a perfect real-time system example. Since our smart and pervasive space implementation does not use any intrusive sensor or data acquisition unit (such as wearables, camera, or audio sources), we reduce the dependency to the user and potential security/privacy issues.",https://ieeexplore.ieee.org/document/8796576/,"2018 9th IEEE Annual Ubiquitous Computing, Electronics & Mobile Communication Conference (UEMCON)",8-10 Nov. 2018,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/MLSP.2011.6064611,Non-parametric co-clustering of large scale sparse bipartite networks on the GPU,IEEE,Conferences,"Co-clustering is a problem of both theoretical and practical importance, e.g., market basket analysis and collaborative filtering, and in web scale text processing. We state the co-clustering problem in terms of non-parametric generative models which can address the issue of estimating the number of row and column clusters from a hypothesis space of an infinite number of clusters. To reach large scale applications of co-clustering we exploit that parameter inference for co-clustering is well suited for parallel computing. We develop a generic GPU framework for efficient inference on large scale sparse bipartite networks and achieve a speedup of two orders of magnitude compared to estimation based on conventional CPUs. In terms of scalability we find for networks with more than 100 million links that reliable inference can be achieved in less than an hour on a single GPU. To efficiently manage memory consumption on the GPU we exploit the structure of the posterior likelihood to obtain a decomposition that easily allows model estimation of the co-clustering problem on arbitrary large networks as well as distributed estimation on multiple GPUs. Finally we evaluate the implementation on real-life large scale collaborative filtering data and web scale text corpora, demonstrating that latent mesoscale structures extracted by the co-clustering problem as formulated by the Infinite Relational Model (IRM) are consistent across consecutive runs with different initializations and also relevant for interpretation of the underlaying processes in such large scale networks.",https://ieeexplore.ieee.org/document/6064611/,2011 IEEE International Workshop on Machine Learning for Signal Processing,18-21 Sept. 2011,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IJCNN.2011.6033598,Nonlinear multi-model ensemble prediction using dynamic Neural Network with incremental learning,IEEE,Conferences,"This paper introduces several nonlinear multi-model ensemble techniques for multiple chaotic models in high-dimensional phase space by means of artificial neural networks. A chaotic model is built by way of the time-delayed phase space reconstruction of the time series from observables. Several predictive global and local models, including Multi-layered Perceptron Neural Network (MLP-NN), are constructed and a number of multi-model ensemble techniques are implemented to produce more accurate hybrid models. One of these techniques is the nonlinear multi-model ensemble using one kind of dynamic neural network so-called Focused Time Delay Neural Network (FTDNN) with batch and incremental learning algorithms. The proposed techniques were used and tested for predicting storm surge dynamics in the North Sea. The results showed that the accuracy of multi-model ensemble predictions is generally improved in comparison to the one by single models. An FTDNN with incremental learning is more desirable for real-time operation, however in our experiments it was less accurate than batch learning.",https://ieeexplore.ieee.org/document/6033598/,The 2011 International Joint Conference on Neural Networks,31 July-5 Aug. 2011,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/EESCO.2015.7253955,Notice of Removal: Hybrid intelligent trail to search engine answering machine: Squat appraisal on pedestal technology (hybrid search machine),IEEE,Conferences,"Arched type Swing in loom of information retrieval system is observed with record progression to information fetch, to knowledge data processing, to intelligent information progression. Subsequent processing machines like document retrieval, text summarization, search engines, rule based machines, expert systems have been developed. These machines have dedicated performance with retrieval measure in particular dimension. Machine learning methods have facilitated reasoning machine with ability like humans. Still a corner in research argues highly intelligent time constraint fact seeking real world information processing machine. Hybrid technology is integration of optimized approaches at various levels of information processing. We proposed a hybrid search answer machine with four techniques of optimization “question reformulation” (from user-intent, profile), “search method” (semantic concept, context, machine learning), “answer presentation” (ranking algorithm), “decision support ” (comparative analysis to choose best techniques to retrieve results). Data corpus is heart of any IR system large dataset facilitates good search which argue to distributed data and computing. Intelligence is reformation proceeds that excel our time and dataset. The machine is designed to facilitated updatable training dataset for fact seeking knowledge acquirement “it trains over data”. Muti-agent model distributed search methodology is proposed. In precise Hybrid extraction of “hybrid models” is performed. Semantic context (concept based) user profiled; best machine learning, decision supportive multiagent distributed search system is proposed. This paper gives underlying technologies overview, with examinations of 30 papers is done as with recent review of technology advancement. The review outcomes are orderly placed with 3 research query answering. The outputs of query structure a trail to search engine answering machine. We facilitate research done by scholars on technology perspective we integrate them to draw a sketch of hybrid search answering. In domain “a point of reference” concepts of research are studied, with comparative views on advance in IR. We identify the benchmark of research methods blueprint and explore space of research in area of intelligent machine implementation.",https://ieeexplore.ieee.org/document/7253955/,"2015 International Conference on Electrical, Electronics, Signals, Communication and Optimization (EESCO)",24-25 Jan. 2015,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/INDIN45523.2021.9557354,Novelty Detection for Iterative Learning of MIMO Fuzzy Systems,IEEE,Conferences,"This paper proposes a methodology for iterative learning of multi-input multi-output (MIMO) fuzzy models focusing on dynamic system identification. The first step of the proposed method is the learning of the antecedent part of the fuzzy system, which is learned iteratively, where fuzzy rules can be added or merged based on the presented novelty detection and similarity criteria defined by a recursive extension of the Gath-Geva clustering algorithm. Then, the consequent part consists in the direct implementation of a non-recursive fuzzy approach that uses global least squares, Observer Kalman Filter Identification (OKID) and the Eigensystem Realization Algorithm (ERA). The proposed method is validated using experimental data from a real quadrotor aerial robot, a nonlinear dynamic system. Using quantitative performance metrics, the proposed method is compared with Hammerstein-Wiener models (H.-W.), nonlinear autoregressive models with exogenous input (NARX), and state-space models using subspace method with time-domain data (N4SID), other MIMO system identification techniques. The proposed method achieved better results compared to other techniques, showing the importance and versatility of learning based on novelty detection for MIMO problems.",https://ieeexplore.ieee.org/document/9557354/,2021 IEEE 19th International Conference on Industrial Informatics (INDIN),21-23 July 2021,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ROBIO.2009.4913200,Object orientation recognition based on SIFT and SVM by using stereo camera,IEEE,Conferences,"The goal of this research is to recognize an object and its orientation in space by using stereo camera. The principle of object orientation recognition in this paper was based on the scale invariant feature transform (SIFT) and support vector machine (SVM). SIFT has been successfully implemented on object recognition but it had a problem recognizing the object orientation. For many autonomous robotics applications, such as using a vision-guided industrial robot to grab a product, not only correct object recognition will be needed in this process but also object orientation recognition is required. In this paper we used SVM to recognize object orientation. SVM has been known as a promising method for classification accuracy and its generalization ability. The stereo camera system adopted in this research provided more useful information compared to single camera one. The object orientation recognition technique was implemented on an industrial robot in a real application. The proposed camera system and recognition algorithms were used to recognize a specific object and its orientation and then guide the industrial robot to perform some alignment operations on the object.",https://ieeexplore.ieee.org/document/4913200/,2008 IEEE International Conference on Robotics and Biomimetics,22-25 Feb. 2009,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IEMBS.2007.4353406,Obstacle Avoidance for Power Wheelchair Using Bayesian Neural Network,IEEE,Conferences,"In this paper we present a real-time obstacle avoidance algorithm using a Bayesian neural network for a laser based wheelchair system. The raw laser data is modified to accommodate the wheelchair dimensions, allowing the free- space to be determined accurately in real-time. Data acquisition is performed to collect the patterns required for training the neural network. A Bayesian frame work is applied to determine the optimal neural network structure for the training data. This neural network is trained under the supervision of the Bayesian rule and the obstacle avoidance task is then implemented for the wheelchair system. Initial results suggest this approach provides an effective solution for autonomous tasks, suggesting Bayesian neural networks may be useful for wider assistive technology applications.",https://ieeexplore.ieee.org/document/4353406/,2007 29th Annual International Conference of the IEEE Engineering in Medicine and Biology Society,22-26 Aug. 2007,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IECON.2006.347441,Obstacle avoidance algorithm based on biological patterns for anthropomorphic robot manipulator,IEEE,Conferences,"This study addresses the problem of collision-free controlling of 3-DOF (degree of freedom) anthropomorphic manipulators with given a priori unrestricted trajectory. The robot constraints resulting from the physical robot's actuators are also taken into account during the robot movement. Obstacle avoidance algorithm is based on penalty function, which is minimized when collision is predicted. Mathematical construction of penalty function and minimization process allows modeling of variety behaviors of robot elusion moves. Implementation of artificial neural network (ANN) inside the control process gives the additional flexibility needed to remember most important robot behaviors based on biological pattern of human arm moves. Thanks to the fast collisions' detection, the presented algorithm appears to be applicable to the industrial real-time implementations. Numerical simulations of the anthropomorphic manipulator operating in three dimensional space with obstacles is also presented",https://ieeexplore.ieee.org/document/4152937/,IECON 2006 - 32nd Annual Conference on IEEE Industrial Electronics,6-10 Nov. 2006,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.23919/OCEANS44145.2021.9705808,Ocean current observations by infrared and visual Large Scale Particle Image Velocimetry (LSPIV),IEEE,Conferences,"The natural dynamics of tidal currents continuously change the appearance of the Wadden Sea. Especially coastal regions and bays with shallow waters are strongly influenced by tidal dynamics. Meanwhile, in offshore study areas with harsh environmental conditions as well as dynamic ocean currents and water levels, continuous in-situ investigations of the prevailing current conditions requires a high effort. In-situ measurement technology is thereby maintenance-intensive and partly limited in time or space by limited accessibility or restricted by the measurement method. In hence, remote sensing techniques based on video image information, such as Large Scale Particle Image Velocimetry (LSPIV) for sensing, and computer vision algorithms for long-term investigation of the prevailing dynamic near-surface ocean flow conditions, have become highly relevant. However, in some environmental situations there may be not sufficient or significant natural textures evaluable in the visual camera images, or only insufficient contrast ratio under varying ambient illumination at the water observation site, to perform image based velocimetry. Thus, we consider with this study an extended approach of visual and infrared video data analysis by an automated LSPIV measurement technique under real offshore deployment conditions. The treatise of this paper first introduces the reader to the subject area and technologies as well as the principles of the LSPIV measurement method. Followed by the depiction of the motivation for an extended approach to visual and infrared video data analysis by an automated LSPIV measurement method in real offshore applications. Subsequently, related research is discussed. Thereafter, the remote sensing setup and sensor-test-bed-system for offshore deployment on an observation platform is presented, therein we also addresses routines of necessary calibration procedures for camera sensors. We then depict details of our automated LSPIV measurement procedure. This is followed with an overview of the preceding validation procedures and LSPIV multispectral remote sensing results of long-term monitoring of horizontal flow dynamics over several days. Finally, we discuss uncertainties of the LSPIV velocity measurement method encountered in real applications and conclude with a brief outlook on further developments and applications.",https://ieeexplore.ieee.org/document/9705808/,OCEANS 2021: San Diego – Porto,20-23 Sept. 2021,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IAdCC.2013.6514374,Off-line signature verification using Neural Networks,IEEE,Conferences,"This paper proposes a signature verification system that can authenticate a signature to avoid forgery cases. In the real world environment, it is often very difficult for any verification system to handle a huge collection of data, and to detect the genuine signatures with relatively good accuracy. Consequently, some artificial intelligence technique are used that can learn from the huge data set, in its training phase and can respond accurately, in its application phase without consuming much storage memory space and computational time. In addition, it should also have the ability to continuously update its knowledge from real time experiences. One such adaptive machine learning technique called a Multi-Layered Neural Network Model (NN Model) is implemented for the purpose of this work. Initially, a huge set of data is generated by collecting the images of several genuine and forgery signatures. The quality of the images is improved by using image processing followed by further extracting certain unique standard statistical features in its feature extraction phase. This output is given as the input to the above proposed NN Model to further improve its decision making capabilities. The performance of the proposed model is evaluated by calculating the fault acceptance and rejection rates for a small set of data. Further possible developments of this model are also outlined.",https://ieeexplore.ieee.org/document/6514374/,2013 3rd IEEE International Advance Computing Conference (IACC),22-23 Feb. 2013,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/VETECS.2012.6239909,On the Effect of Gaussian Imperfect Channel Estimations on the Performance of Space Modulation Techniques,IEEE,Conferences,"Space modulation techniques, such as spatial modulation (SM) and space shift keying (SSK), are efficient low complexity implementation of multiple input multiple output (MIMO) systems. In such techniques, a single transmit-antenna is activated during each time instant and the activated antenna index is used to convey information. Due to the novel method of conveying information, a major criticism arises on the practicality of such techniques in the presence of real-time imperfections such as channel estimation errors. Therefore, the aim of this paper is to shed light on this issue. The performance of such systems are analyzed in the presence of Gaussian imperfect channel estimations. More specifically, the performance of SSK system consisting of Nt transmit and Nr receive antennas with maximum-likelihood (ML) detection and imperfect channel state information (CSI) at the receiver is studied. The exact average bit error probability (ABEP) over Rayleigh fading channels is obtained in closed-form for Nt = 2 and arbitrary Nr; while union upper bound is used to compute the ABEP when Nt >; 2 and arbitrary Nr. Furthermore, simple and general asymptotic expression for the ABEP is derived and analyzed. Besides, the effect of imperfect CSI on the performance of SM, Alamouti and SSK schemes considering different number of channel estimation pilots are studied and compared via numerical Monte Carlo simulations. It is shown that, on the contrary to the raised criticism, space modulation techniques are more robust to channel estimation errors than Alamouti since the probability of error is determined by the differences between channels associated with the different transmit antennas rather than the actual channel realization.",https://ieeexplore.ieee.org/document/6239909/,2012 IEEE 75th Vehicular Technology Conference (VTC Spring),6-9 May 2012,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICITACEE.2016.7892481,On the implementation of ZFS (Zettabyte File System) storage system,IEEE,Conferences,"Digital data storage is very critical in computer systems. Storage devices used to store data may at any time suffer from damage caused by its lifetime, resource failure or factory defects. Such damage may lead to loss of important data. The risk of data loss in the event of device damage can be minimized by building a storage system that supports redundancy. The design of storage based on ZFS (Zettabyte File System) aims at building a storage system that supports redundancy and data integrity without requiring additional RAID controllers. When the system fails on one of its hard drive, the stored data remains secure and data integrity is kept assured. In addition to providing redundancy, the ZFS-based storage system also supports data compression for savings on storage space. The results show that the ZFS with LZ4 compression has the highest read and write speed. For real benchmark, there is no significant difference in reading speed for a variety of different variables, whereas a significant increase in speed occurs when writing compressible files on the ZFS system with compression configuration.",https://ieeexplore.ieee.org/document/7892481/,"2016 3rd International Conference on Information Technology, Computer, and Electrical Engineering (ICITACEE)",19-20 Oct. 2016,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ACC.1995.532356,On the localization of feedforward networks,IEEE,Conferences,"Interference in neural networks occurs when learning in one area of the input space causes unlearning in another area. Networks that are less susceptible to interference are called spatially local networks. These networks are often used in neurocontrol, in online applications, where, because of the real time nature of the task, interference is often a problem. Although there are heuristics as to what makes a network local, there is no theoretical framework for measuring localization. This paper provides a formal definition of interference and localization that will allow measurement of a network's local properties. These definitions will be useful in developing learning algorithms that make networks more local. This may lead to faster learning over the entire input domain.",https://ieeexplore.ieee.org/document/532356/,Proceedings of 1995 American Control Conference - ACC'95,21-23 June 1995,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IRSEC48032.2019.9078164,On the use of Deep Learning Approaches for Occupancy prediction in Energy Efficient Buildings,IEEE,Conferences,"Occupancy forecasting is considered as a crucial input for improving the performance of predictive control strategies in energy efficient buildings. In fact, accurate occupancy forecast is the key enabler for context-drive control of active systems (e.g. heating, ventilation, and lighting). This paper focuses on forecasting occupants' number using real-time measurements of CO2 concentration and its forecasting values. The main aim is to evaluate the accuracy of forecasting occupants' number by applying the steady state model (1) [16] on the CO2 forecast using recent deep learning approaches. The LSTM, a recurrent neural network based deep learning algorithm, is deployed to forecast the CO2 level in a dedicated space, a testlab deployed in our university for conducting experiments and assess approaches for energy efficiency in buildings. Preliminary results show the effectiveness of LSTM in forecasting occupants' number, which reaches 70% in accuracy.",https://ieeexplore.ieee.org/document/9078164/,2019 7th International Renewable and Sustainable Energy Conference (IRSEC),27-30 Nov. 2019,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/NAECON.2018.8556744,Onboard Image Processing for Small Satellites,IEEE,Conferences,"In general, the computational ability of spacecraft and satellites has lagged behind terrestrial computers by several generations. Moore's Law turns the supercomputers of yesterday into the laptops of today, but space computing remains relatively underpowered due to the harsh radiation environment and low risk-tolerance of most space missions. Space missions are generally low risk because of the high cost of components and launch. However, launch costs are drastically decreasing and innovations such as CubeSats are changing the risk equation. By accepting more risk and utilizing commercial of the shelf (COTS) parts, it is possible to cheaply build and launch extremely capable computing platforms into space. High performance satellites will be required for advanced interplanetary exploration due to latency challenges. The long transmission times between planets means satellites or robotic explorers need onboard processing to perform tasks in real-time. This paper explores one possible application that could be hosted onboard the next generation of high performance satellites, performing object classification on satellite imagery. Automation of satellite imagery processing is currently performed by servers or workstations on Earth, but this paper will show that those algorithms can be moved onboard satellites by using COTS components. First traditional computer vision techniques such as edge detection and sliding windows are used to detect possible objects on the open ocean. Then a modern neural network architecture is used to classify the object as a ship or not. This application is implemented on a Nvidia Jetson TX2 and measurements of the application's power use confirm that it fits within the Size Weight and Power (SWAP) requirements of SmallSats and possibly even CubeSats.",https://ieeexplore.ieee.org/document/8556744/,NAECON 2018 - IEEE National Aerospace and Electronics Conference,23-26 July 2018,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CCDC.2018.8407436,One-dimensional data augmentation using a wasserstein generative adversarial network with supervised signal,IEEE,Conferences,"In recent years, Generative Adversarial Network (GAN) is widely applied in many domains; however, there is still some difficulties in training the network, which are mainly caused by mode collapse, vanishing gradient of generator and indirect assessment criteria of generated samples. In this paper, the supervised signal is introduced into Wasserstein Generative Adversarial Network (WGAN) on the application of one-dimensional data augmentation to alleviate this difficulty. In the proposed method, besides generating fake samples, a well trained generative model is implemented to reconstruct the real samples , whose input data are latent space samples obtained from autoencoder (AE). In addition, the mode collapse can be prevented by the new model through ensuring that the supervised signal grounded in all the available training data. The performance of our method is verified based on parameters of electronic equipment and stock index systematically and quantitatively, and the superiority of the algorithm is demonstrated by the experiment results both in convergence rate and the quality of samples compared with WGAN and VAEGAN.",https://ieeexplore.ieee.org/document/8407436/,2018 Chinese Control And Decision Conference (CCDC),9-11 June 2018,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICPR48806.2021.9412087,One-stage Multi-task Detector for 3D Cardiac MR Imaging,IEEE,Conferences,"Fast and accurate landmark location and bounding box detection are important steps in 3D medical imaging. In this paper, we propose a novel multi-task learning framework, for real-time, simultaneous landmark location and bounding box detection in 3D space. Our method extends the famous single-shot multibox detector (SSD) from single-task learning to multitask learning and from 2D to 3D. Furthermore, we propose a post-processing approach to refine the network landmark output, by averaging the candidate landmarks. Owing to these settings, the proposed framework is fast and accurate. For 3D cardiac magnetic resonance (MR) images with size 224×224×64, our framework runs ~128 volumes per second (VPS) on GPU and achieves 6.75mm average point-to-point distance error for landmark location, which outperforms both state-of-the-art and baseline methods. We also show that segmenting the 3D image cropped with the bounding box results in both improved performance and efficiency.",https://ieeexplore.ieee.org/document/9412087/,2020 25th International Conference on Pattern Recognition (ICPR),10-15 Jan. 2021,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.23919/INM.2017.7987462,Online learning and adaptation of network hypervisor performance models,IEEE,Conferences,"Software Defined Networking (SDN) paved the way for a logically centralized entity, the SDN controller, to excerpt near real-time control over the forwarding state of a network. Network hypervisors are an in-between layer to allow multiple SDN controllers to share this control by slicing the network and giving each controller the power over a part of the network. This makes network hypervisors a critical component in terms of reliability and performance. At the same time, compute virtualization is ubiquitous and may not guarantee statically assigned resources to the network hypervisors. It is therefore important to understand the performance of network hypervisors in environments with varying compute resources. In this paper we propose an online machine learning pipeline to synthesize a performance model of a running hypervisor instance in the face of varying resources. The performance model allows precise estimations of the current capacity in terms of control message throughput without time-intensive offline benchmarks. We evaluate the pipeline in a virtual testbed with a popular network hypervisor implementation. The results show that the proposed pipeline is able to estimate the capacity of a hypervisor instance with a low error and furthermore is able to quickly detect and adapt to a change in available resources. By exploring the parameter space of the learning pipeline, we discuss its characteristics in terms of estimation accuracy and convergence time for different parameter choices and use cases. Although we evaluate the approach with network hypervisors, our work can be generalized to other latency-sensitive applications with similar characteristics and requirements as network hypervisors.",https://ieeexplore.ieee.org/document/7987462/,2017 IFIP/IEEE Symposium on Integrated Network and Service Management (IM),8-12 May 2017,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS.2017.8202247,Online learning for human classification in 3D LiDAR-based tracking,IEEE,Conferences,"Human detection and tracking are essential aspects to be considered in service robotics, as the robot often shares its workspace and interacts closely with humans. This paper presents an online learning framework for human classification in 3D LiDAR scans, taking advantage of robust multi-target tracking to avoid the need for data annotation by a human expert. The system learns iteratively by retraining a classifier online with the samples collected by the robot over time. A novel aspect of our approach is that errors in training data can be corrected using the information provided by the 3D LiDAR-based tracking. In order to do this, an efficient 3D cluster detector of potential human targets has been implemented. We evaluate the framework using a new 3D LiDAR dataset of people moving in a large indoor public space, which is made available to the research community. The experiments analyse the real-time performance of the cluster detector and show that our online learned human classifier matches and in some cases outperforms its offline version.",https://ieeexplore.ieee.org/document/8202247/,2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),24-28 Sept. 2017,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/INDIN.2014.6945484,Online passive learning of timed automata for cyber-physical production systems,IEEE,Conferences,"Model-based approaches are very often used for diagnosis in production systems. And since the manual creation of behavior models is a tough task, many learning algorithms have been constructed for the automatic model identification. Most of them are tested and evaluated on artificial datasets on personal computers only. However, the implementation on cyber-physical production systems puts additional requirements on learning algorithms, for instance the real-time aspect or the usage of memory space. This paper analyzes the requirements on learning algorithms for cyber-physical production systems and presents an appropriate online learning algorithm, the Online Timed Automaton Learning Algorithm, OTALA. It is the first online passive learning algorithm for timed automata which in addition copes without negative learning examples. An analysis of the algorithm and comparison with offline learning algorithms completes this contribution.",https://ieeexplore.ieee.org/document/6945484/,2014 12th IEEE International Conference on Industrial Informatics (INDIN),27-30 July 2014,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/INDIN.2006.275808,"Ontology for Cognitics, Closed-Loop Agility Constraint, and Case Study - a Mobile Robot with Industrial-Grade Components",IEEE,Conferences,"The paper refers to intelligent industrial automation. The objective is to present key elements and methods for best practice, as well as some results obtained. The first part presents an ontology for automated cognition (cognitics), where, based on information and time, the main cognitive concepts, including those of complexity, knowledge, expertise, learning, intelligence abstraction, and concretization are rigorously defined, along with corresponding metrics and specific units. Among important conclusions at this point are the fact that reality is much too complex to be approached better than through much simplified models, in very restricted contexts. Another conclusion is the necessity to be focused on goal. Extensions are made here for group behavior. The second part briefly presents a basic law governing the choice of overall control architecture: achievable performance level of control system in terms of agility, relative to process dynamics, dictates the type of approaches which is suitable, in a spectrum which ranges from simple threshold-based switching, to classical closed-loop calculus (PID, state space multivariable systems, etc.), up to ""impossible"" cases where additional controllers must be considered, leading to cascaded, hierarchical control structures. For complex cases such as latter ones, new tools and methodologies must be designed, as is typical in O3NEIDA initiative, at least for software components. Finally, a large part of the paper presents a case study, a mobile robot, i.e. an embedded autonomous system with distributed, networked control, featuring industry-grade components, designed with the main goal of robust functionality. The case illustrates several of the concepts introduced earlier in the paper.",https://ieeexplore.ieee.org/document/4053562/,2006 4th IEEE International Conference on Industrial Informatics,16-18 Aug. 2006,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/iThings.2014.13,Ontology-Based Semantic Modeling and Evaluation for Internet of Things Applications,IEEE,Conferences,"Internet of Things (IoT) is a major paradigm shift from traditional Internet applications that connects things / objects to the Internet for automatic sensing, information processing, understanding, reasoning, decision and response to virtual and physical world. Due to its highly distributed and heterogeneous characteristics, it requires more semantic interactions between different scenarios. Therefore, interoperability is one of the most fundamental requirements for IoT systems. Currently, semantic modeling for IoT applications has significant deficiency in its reusability and interoperability. In this paper, we propose a semantic information model for IoT applications, named OntoIoT. This model has a two-level architecture including a set of general upper ontology and flexible interfaces for different application domain ontology extensions. We have preliminarily implemented the OntoIoT framework and domain extension in selected scenarios of smart space and healthcare. A data-driven and other ontology evaluation approaches are used to prove the scalability and interoperability of OntoIoT. Result demonstrates that, compared to other test ontologies, Onto IoT is more suitable for the given application scenario.",https://ieeexplore.ieee.org/document/7059638/,"2014 IEEE International Conference on Internet of Things (iThings), and IEEE Green Computing and Communications (GreenCom) and IEEE Cyber, Physical and Social Computing (CPSCom)",1-3 Sept. 2014,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IVS.2014.6856509,Ontology-based context awareness for driving assistance systems,IEEE,Conferences,"Within a vehicle driving space, different entities such as vehicles and vulnerable road users are in constant interaction which governs their behaviour. Whilst smart sensors provide information about the state of the perceived objects, considering the spatio-temporal relationships between them with respect to the subject vehicle remains a challenge. This paper proposes to fill this gap by using contextual information to infer how perceived entities are expected to behave, and thus what are the consequences of these behaviours on the subject vehicle. For this purpose, an ontology is formulated about the vehicle, perceived entities and context (map information) to provide a conceptual description of all road entities with their interaction. It allows for inferences of knowledge about the situation of the subject vehicle with respect to the environment in which it is navigating. The framework is applied to the navigation of a vehicle as it approaches road intersections, to demonstrate its applicability. Results from the real-time implementation on a vehicle operating under controlled conditions are included. They show that the proposed ontology allows for a coherent understanding of the interactions between the perceived entities and contextual data. Further, it can be used to improve the situation awareness of an ADAS (Advanced Driving Assistance System), by determining which entities are the most relevant for the subject vehicle navigation.",https://ieeexplore.ieee.org/document/6856509/,2014 IEEE Intelligent Vehicles Symposium Proceedings,8-11 June 2014,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/WI.2005.108,Ontology-based information integration in virtual learning environment,IEEE,Conferences,"A good virtual learning environment should deliver relevant learning materials to learners at the most appropriate time and locations to facilitate learners' acquisition of knowledge and skills. In this paper, we propose ontology-based information integration in virtual learning environment using ontology and Web services. Relevant concepts extracted from domain ontology provide ontology-based browsing space that allows users to browse and select relevant terms of interest and increases the degree of relevancy. By using Web services to integrate learning materials from heterogeneous public domain data sources, applications do not need to know the internal structure and working of public domain data sources, and reuse existing applications and recourses. We use gene ontology, PubMed eUtils and Google Web APIs to demonstrate our idea. The implementation involves techniques in image and video processing, database management, programming, and multimedia learning materials presentation.",https://ieeexplore.ieee.org/document/1517949/,The 2005 IEEE/WIC/ACM International Conference on Web Intelligence (WI'05),19-22 Sept. 2005,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISMAR52148.2021.00016,"OpenRDW: A Redirected Walking Library and Benchmark with Multi-User, Learning-based Functionalities and State-of-the-art Algorithms",IEEE,Conferences,"Redirected walking (RDW) is a locomotion technique that guides users on virtual paths, which might vary from the paths they physically walk in the real world. Thereby, RDW enables users to explore a virtual space that is larger than the physical counterpart with near-natural walking experiences. Several approaches have been proposed and developed; each using individual platforms and evaluated on a custom dataset, making it challenging to compare between methods. However, there are seldom public toolkits and recognized benchmarks in this field. In this paper, we introduce OpenRDW, an open-source library and benchmark for developing, deploying and evaluating a variety of methods for walking path redirection. The OpenRDW library provides application program interfaces to access the attributes of scenes, to customize the RDW controllers, to simulate and visualize the navigation process, to export multiple formats of the results, and to evaluate RDW techniques. It also supports the deployment of multi-user real walking, as well as reinforcement learning-based models exported from TensorFlow or PyTorch. The OpenRDW benchmark includes multiple testing conditions, such as walking in size varied tracking spaces or shape varied tracking spaces with obstacles, multiple user walking, etc. On the other hand, procedurally generated paths and walking paths collected from user experiments are provided for a comprehensive evaluation. It also contains several classic and state-of-the-art RDW techniques, which include the above mentioned functionalities.",https://ieeexplore.ieee.org/document/9583831/,2021 IEEE International Symposium on Mixed and Augmented Reality (ISMAR),4-8 Oct. 2021,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICPR.2008.4761060,Optimal feature weighting for the discrete HMM,IEEE,Conferences,"We propose a modified discrete HMM that includes a feature weighting discrimination component. We assume that the feature space is partitioned into subspaces and that the relevance weights of the different subspaces depends on the symbols and the states. In particular, we associate a partial probability with each symbol in each subspace. The overall observation state probability is then computed as an aggregation of the partial probabilities and their relevance weights. We consider two aggregation models. The first one is based on a linear combination, while the second one is based on a geometric combination. For both models, we reformulate the Baum-Welch learning algorithm and derive the update equations for the relevance weights and the partial state probabilities. The proposed approach is validated using synthetic and real data sets. The results are shown to outperform the baseline HMM.",https://ieeexplore.ieee.org/document/4761060/,2008 19th International Conference on Pattern Recognition,8-11 Dec. 2008,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SII.2012.6426933,Optimization of obstacle avoidance using reinforcement learning,IEEE,Conferences,Walking through narrow space for multi-legged robot is optimized using reinforcement learning in this paper. The walking is generated by the virtual repulsive force from the estimated obstacle position and the virtual impedance field. The resulted action depends on the parameter of the virtual impedance coefficients. The reinforcement learning is employed to find an optimal motion. The temporal walking through motion consists of each parameter optimized for a situation. Optimization of integrated walking through motion is finally achieved evaluating walking in compound encountering obstacle on simulator. The resulted motion is implemented to a real multi-legged robot and results show the effectiveness of the proposed method.,https://ieeexplore.ieee.org/document/6426933/,2012 IEEE/SICE International Symposium on System Integration (SII),16-18 Dec. 2012,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCVE45908.2019.8965211,Optimizing coverage of simulated driving scenarios for the autonomous vehicle,IEEE,Conferences,"Self-driving cars and advanced driver-assistance systems are perceived as a game-changer in the future of road transportation. However, their validation is mandatory before industrialization; testing every component should be assessed intensively in order to mitigate potential failures and avoid unwanted problems on the road. In order to cover all possible scenarios, virtual simulations are used to complement real-test driving and aid in the validation process. This paper focuses on the validation of the command law during realistic virtual simulations. Its aim is to detect the maximum amount of failures while exploring the input search space of the scenarios. A key industrial restriction, however, is to launch simulations as little as possible in order to minimize computing power needed. Thus, a reduced model based on a random forest model helps in decreasing the number of simulations launched. It accompanies the algorithm in detecting the maximum amount of faulty scenarios everywhere in the search space. The methodology is tested on a tracking vehicle use case, which produces highly effective results.",https://ieeexplore.ieee.org/document/8965211/,2019 IEEE International Conference on Connected Vehicles and Expo (ICCVE),4-8 Nov. 2019,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICSP51882.2021.9408823,Over-the-Air Radar Emitter Signal Classification Based on SDR,IEEE,Conferences,"At present, in the field of radar emitter classification, theoretical simulation is mostly used to carry out algorithm research. However, there are few schemes to study signal classification in real electromagnetic environment using actual hardware. Therefore, this paper proposes a radar emitter classification scheme based on HackRF Software Defined Radio (SDR) and deep learning to solve the problem of weak engineering practice. Firstly, the GNU Radio development environment is used to realize the integration design of real space signal transceiver and time-frequency analysis algorithm application on HackRF hardware platform. Then, a classification model with 11 layers network is constructed to automatically extract the deep features of intra-pulse signal time-frequency image. Finally, the classification performance of eight kinds of signals in real electromagnetic environment is tested. The total recognition accuracy of this scheme is more than 83% under 6dB low Signal-to-Noise Ratio (SNR), which proves the effectiveness of the scheme, and provides an important basis for practical engineering application in the future.",https://ieeexplore.ieee.org/document/9408823/,2021 6th International Conference on Intelligent Computing and Signal Processing (ICSP),9-11 April 2021,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/HPCC-SmartCity-DSS50907.2020.00081,Overcoming Memory Constraint for Improved Target Classification Performance on Embedded Deep Learning Systems,IEEE,Conferences,"Pattern recognition applications such as face recognition, detection of broken eggs, and classification of agricultural products are all using image classification in deep neural networks to improve the quality of services. However, traditional cloud inference models suffer from several problems such as network delay fluctuations and privacy leakage. In this regard, most real-time applications currently need to be deployed on edge computing devices. Constrained by the computing power and memory limitations of edge devices, the use of an efficient memory manager for model reasoning is the key to improving the quality of service. This study firstly explored the incremental loading strategy of model weights for the model reasoning. Next, the memory space at runtime is optimized through data layout reorganization from the spatial dimension. In particular, our proposed schemes are orthogonal and transparent to the model. Experimental results demonstrate that the proposed approach reduced the memory consumption by 43.74% on average without additional reasoning time overhead.",https://ieeexplore.ieee.org/document/9408027/,2020 IEEE 22nd International Conference on High Performance Computing and Communications; IEEE 18th International Conference on Smart City; IEEE 6th International Conference on Data Science and Systems (HPCC/SmartCity/DSS),14-16 Dec. 2020,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/BigData.2015.7363884,PAIRS: A scalable geo-spatial data analytics platform,IEEE,Conferences,"Geospatial data volume exceeds hundreds of Petabytes and is increasing exponentially mainly driven by images/videos/data generated by mobile devices and high resolution imaging systems. Fast data discovery on historical archives and/or real time datasets is currently limited by various data formats that have different projections and spatial resolution, requiring extensive data processing before analytics can be carried out. A new platform called Physical Analytics Integrated Repository and Services (PAIRS) is presented that enables rapid data discovery by automatically updating, joining, and homogenizing data layers in space and time. Built on top of open source big data software, PAIRS manages automatic data download, data curation, and scalable storage while being simultaneously a computational platform for running physical and statistical models on the curated datasets. By addressing data curation before data being uploaded to the platform, multi-layer queries and filtering can be performed in real time. In addition, PAIRS offers a foundation for developing custom analytics. Towards that end we present two examples with models which are running operationally: (1) high resolution evapo-transpiration and vegetation monitoring for agriculture and (2) hyperlocal weather forecasting driven by machine learning for renewable energy forecasting.",https://ieeexplore.ieee.org/document/7363884/,2015 IEEE International Conference on Big Data (Big Data),29 Oct.-1 Nov. 2015,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICISCE50968.2020.00212,PTR-MirrorGAN: Parallel Text Redescription module for advancing MirrorGAN,IEEE,Conferences,"During generating images from a given text description, two aspects of consistency are usually considered: visual consistency and semantic consistency. Although there has been a great improvement in the use of generative adversarial networks to generate images, it remains a challenging project to ensure that the semantics of the generated images are consistent with the semantics of the input text descriptions. This study proposed a Real-Fake Images Parallel Text Redescription Module (RFIPTRM) for advancing MirrorGAN based on the idea of mirroring and reconstructed the loss of the image generation model by adding the loss between the real text and the text from generated images to the genarators for balancing the textual semantic space. Intuitively, there may be inconsistency between the textual semantic space structure of our real text and the generated image, so we conducted adversarial learning between redescriptive text from the real images and the real text to improve the inconsistency of this text space structure. Comprehensive experiments conducted on the public benchmark datasets CUB and COCO have proved that PTR-MirrorGAN is superior to other methods and achieved a better generation effect.",https://ieeexplore.ieee.org/document/9532027/,2020 7th International Conference on Information Science and Control Engineering (ICISCE),18-20 Dec. 2020,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AISIG.1989.47322,PX1: a space shuttle mission operations knowledge-based system project,IEEE,Conferences,"A knowledge-based system (KBS) prototyping project is examined. The work entails reasoning about the impact of component failures in space shuttle orbiter subsystems. The effort is directed toward the development of a system that will recognize passive component failures as potential safety hazards, rather than toward an active failure identification or diagnostic tool. The system was designed to be integrated as a knowledge-based processing system utilizing input from a procedure-based processing system. Implementation of the consolidated system will occur when real-time telemetry data is available at the workstations. The system may be used standalone in the meantime. Flight controllers at the Johnson Space Center will use this prototype to help develop requirements for a space shuttle mission operations tool.<>",https://ieeexplore.ieee.org/document/47322/,[1989] Proceedings. The Annual AI Systems in Government Conference,27-31 March 1989,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICDM.2008.119,Paired Learners for Concept Drift,IEEE,Conferences,"To cope with concept drift, we paired a stable online learner with a reactive one. A stable learner predicts based on all of its experience, whereas are active learner predicts based on its experience over a short, recent window of time. The method of paired learning uses differences in accuracy between the two learners over this window to determine when to replace the current stable learner, since the stable learner performs worse than does there active learner when the target concept changes. While the method uses the reactive learner as an indicator of drift, it uses the stable learner to predict, since the stable learner performs better than does the reactive learner when acquiring target concept. Experimental results support these assertions. We evaluated the method by making direct comparisons to dynamic weighted majority, accuracy weighted ensemble, and streaming ensemble algorithm (SEA) using two synthetic problems, the Stagger concepts and the SEA concepts, and three real-world data sets: meeting scheduling, electricity prediction, and malware detection. Results suggest that, on these problems, paired learners outperformed or performed comparably to methods more costly in time and space.",https://ieeexplore.ieee.org/document/4781097/,2008 Eighth IEEE International Conference on Data Mining,15-19 Dec. 2008,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/BigData.2018.8622065,Parallel Large-Scale Neural Network Training For Online Advertising,IEEE,Conferences,"Neural networks have shown great successes in many fields. Due to the complexity of the training pipeline, however, using them in an industrial setting is challenging. In online advertising, the complexity arises from the immense size of the training data, and the dimensionality of the sparse feature space (both can be hundreds of billions). To tackle these challenges, we built TrainSparse (TS), a system that parallelizes the training of neural networks with a focus on efficiently handling large-scale sparse features. In this paper, we present the design and implementation of TS, and show the effectiveness of the system by applying it to predict the ad conversion rate (pCVR), one of the key problems in online advertising. We also compare several methods for dimensionality reduction on sparse features in the pCVR task. Experiments on real-world industry data show that TS achieves outstanding performance and scalability.",https://ieeexplore.ieee.org/document/8622065/,2018 IEEE International Conference on Big Data (Big Data),10-13 Dec. 2018,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICAPR.2009.40,Parallel Point Symmetry Based Clustering for Gene Microarray Data,IEEE,Conferences,"Point symmetry-based clustering is an important unsupervised learning tool for recognizing symmetrical convex or non-convex shaped clusters, even in the microarray datasets. To enable fast clustering of this large data, in this article, a distributed space and time-efficient scalable parallel approach for point symmetry-based K-means algorithm has been proposed. A natural basis for analyzing gene expression data using this symmetry-based algorithm, is to group together genes with similar symmetrical patterns of expression. This new parallel implementation satisfies the quadratic reduction in timing, as well as the space and communication overhead reduction without sacrificing the quality of clustering solution. The parallel point symmetry based K-means algorithm is compared with another newly implemented parallel symmetry-based K-means and existing parallel K-means over four artificial, real-life and benchmark microarray datasets, to demonstrate its superiority,both in timing and validity.",https://ieeexplore.ieee.org/document/4782807/,2009 Seventh International Conference on Advances in Pattern Recognition,4-6 Feb. 2009,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ROBOT.1991.131722,Parallel robot motion planning,IEEE,Conferences,"A fast, parallel method for computing configuration space maps is presented. The method is made possible by recognizing that one can compute a family of primitive maps which can be combined by superposition based on the distribution of real obstacles. This motion planner has been implemented for the first three degrees-of-freedom of a Puma robot in *Lisp on a Connection Machine with 8 K processors. A six degree-of-freedom version of the algorithm which performs a sequential search of the six-dimensional configuration space, building three-dimensional cross sections in parallel, has also been implemented.<>",https://ieeexplore.ieee.org/document/131722/,Proceedings. 1991 IEEE International Conference on Robotics and Automation,9-11 April 1991,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/TAI.1995.479619,Path consistency revisited,IEEE,Conferences,"One of the main factors limiting the use of path consistency algorithms in real life applications is their high space complexity. C. Han and C. Lee (1988) presented a path consistency algorithm, PC-4, with O(n/sup 3/a/sup 3/) space complexity, which makes it practicable only for small problems. The author presents a new path consistency algorithm, PC-5, which has an O(n/sup 3/a/sup 2/) space complexity while retaining the worst case time complexity of PC-4. Moreover, the new algorithm exhibits a much better average case time complexity. The new algorithm is based on the idea (due to C. Bessiere (1994)) that, at any time, only a minimal amount of support has to be found and recorded for a labeling to establish its viability; one has to look for at new support only if the current support is eliminated. The author also shows that PC-5 can be improved further to yield an algorithm, PC5++, with even better average case performance and the same space complexity.",https://ieeexplore.ieee.org/document/479619/,Proceedings of 7th IEEE International Conference on Tools with Artificial Intelligence,5-8 Nov. 1995,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCC51575.2020.9345104,Payload-based Anomaly Detection for Industrial Internet Using Encoder Assisted GAN,IEEE,Conferences,"Payload-based anomaly detection has been proved effective in discovering Internet misbehavior and potential intrusions, but highly relies on the unstructured feature engineering to generalize the distribution of normal payloads. This kind of generalization may not adapt well to the emerging industrial Internet, where the normal behaviors are more diverse and usually embedded in the raw payloads' local structures. In this paper, we tackle this generalization problem and propose a very different solution to payload-based anomaly detection without the need of feature engineering. Our basic idea is to learn the raw structures of normal payloads directly by a generative adversarial network (GAN), in which we have a generator (i.e., a reversed convolutional decoder) to sample raw payloads from a latent space as well as a discriminator (i.e., a convolutional classifier) to guide the generator produce raw payloads approximating the normal structures. We also deploy an assisted convolutional encoder to map the true payloads back to the latent space and combine with the GAN's decoder (i.e., generator) to reconstruct the payload structures. We consider anomalies appear in condition the re-constructed payloads are largely deviated from the true ones, since our encoder-decoder architecture is trained able to rebuild only the normal payload structures. We have evaluated our solution using extensive experiments on real-world industrial Internet datasets, and confirmed its effectiveness in detecting industrial Internet anomalies in the raw payloads.",https://ieeexplore.ieee.org/document/9345104/,2020 IEEE 6th International Conference on Computer and Communications (ICCC),11-14 Dec. 2020,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IISWC47752.2019.9042000,Performance Aware Convolutional Neural Network Channel Pruning for Embedded GPUs,IEEE,Conferences,"Convolutional Neural Networks (CNN) are becoming a common presence in many applications and services, due to their superior recognition accuracy. They are increasingly being used on mobile devices, many times just by porting large models designed for server space, although several model compression techniques have been considered. One model compression technique intended to reduce computations is channel pruning. Mobile and embedded systems now have GPUs which are ideal for the parallel computations of neural networks and for their lower energy cost per operation. Specialized libraries perform these neural network computations through highly optimized routines. As we find in our experiments, these libraries are optimized for the most common network shapes, making uninstructed channel pruning inefficient. We evaluate higher level libraries, which analyze the input characteristics of a convolutional layer, based on which they produce optimized OpenCL (Arm Compute Library and TVM) and CUDA (cuDNN) code. However, in reality, these characteristics and subsequent choices intended for optimization can have the opposite effect. We show that a reduction in the number of convolutional channels, pruning 12% of the initial size, is in some cases detrimental to performance, leading to 2× slowdown. On the other hand, we also find examples where performance-aware pruning achieves the intended results, with performance speedups of 3× with cuDNN and above 10× with Arm Compute Library and TVM. Our findings expose the need for hardware-instructed neural network pruning.",https://ieeexplore.ieee.org/document/9042000/,2019 IEEE International Symposium on Workload Characterization (IISWC),3-5 Nov. 2019,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICUFN.2016.7537070,Performance evaluation of in-memory computing on scale-up and scale-out cluster,IEEE,Conferences,"Apache Spark framework, which is the implementation of Resilient Distributed Datasets(RDD), is used instead of MapReduce on recent data processing models of Hadoop ecosystem. In this paper, we evaluated the performance and resource usage of real world workloads on scale-up and scale-out clusters using the in-memory caching feature of Spark framework. In our experiments, scale-up processed data more efficiently than scaleout in write intensive workloads such as Sort and Scan, whereas scale-out had strength in those utilizing iterative algorithms such as Join, Pagerank and KMeans. Considering the efficiency in physical factors including performance per watt and the physical space each occupies, we show that it is more advantages to use scale up cluster than scale out.",https://ieeexplore.ieee.org/document/7537070/,2016 Eighth International Conference on Ubiquitous and Future Networks (ICUFN),5-8 July 2016,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICME51207.2021.9428411,Person Retrieval in Physical World,IEEE,Conferences,"Person re-identification (re-ID) gains plenty of achievements as a retrieval problem in constrained camera networks. However, most of the researches are concentrated on visual appearance, they still suffer from the complicated environments in unconstrained urban/campus surveillance scenario due to unreliable visual representations with extremely challenging problems as lack of training samples, amounts of irrelevant crowds, etc. Besides, most of the existing person re-ID datasets neglect the physical truth of realistic investigation application: 1) investigators search only few suspects among amounts of crowds. Moreover, he may not go through by every camera in the surveillance area and may appear in the same camera several times; and 2) the corresponding characteristic in multi-space of the same ID can be verified with each other. Therefore, we propose a person retrieval in physical world (PRPW) dataset with large-scale unconstrained surveillance scenario. It contains over 1.4 million bounding boxes, including 20 labeled IDs and numerous irrelevant crowds captured by 86 cameras. Furthermore, over 30,000 records of 20 mobile trajectories are collected in this dataset, and the 20 mobile trajectories are partially overlapped while passing by 86 cameras. Finally, based on two common senses and a verification experiment, we provide a proposal to tackle with PRPW task on the basis of trajectory association which utilizes global optimization to compensate for the errors caused by visual expression on local observation points. The comparison experiments with two typical unsupervised person re-ID methods are implemented on the constructed dataset.",https://ieeexplore.ieee.org/document/9428411/,2021 IEEE International Conference on Multimedia and Expo (ICME),5-9 July 2021,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CVPR46437.2021.00216,Person30K: A Dual-Meta Generalization Network for Person Re-Identification,IEEE,Conferences,"Recently, person re-identification (ReID) has vastly benefited from the surging waves of data-driven methods. However, these methods are still not reliable enough for real-world deployments, due to the insufficient generalization capability of the models learned on existing benchmarks that have limitations in multiple aspects, including limited data scale, capture condition variations, and appearance diversities. To this end, we collect a new dataset named Person30K with the following distinct features: 1) a very large scale containing 1.38 million images of 30K identities, 2) a large capture system containing 6,497 cameras deployed at 89 different sites, 3) abundant sample diversities including varied backgrounds and diverse person poses. Furthermore, we propose a domain generalization ReID method, dual-meta generalization network (DMG-Net), to exploit the merits of meta-learning in both the training procedure and the metric space learning. Concretely, we design a ""learning then generalization evaluation"" metatraining procedure and a meta-discrimination loss to enhance model generalization and discrimination capabilities. Comprehensive experiments validate the effectiveness of our DMG-Net.",https://ieeexplore.ieee.org/document/9578872/,2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),20-25 June 2021,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/BigData.2017.8258065,Personalized travel mode detection with smartphone sensors,IEEE,Conferences,"Detecting the travel modes such as walking and driving a car is an important task for user behavior understanding as well as transportation planning and management. Existing solutions for this task mainly train a generic classifier for all users although the walking or driving behaviors may differ greatly from one user to another. In this paper, we propose to build a personalized travel mode detection method. In particular, the proposed method can be divided into two stages. First, for a given target user, it applies user similarity computation to borrow data from a set of pre-collected data for transfer learning. Second, it estimates the data distribution in feature space, and uses it to reweight the borrowed data so as to minimize the model loss with respect to the target user. Experimental evaluations on real travel data show that the proposed method outperforms the generic method and the transfer learning method with kernel mean matching in terms of prediction accuracy.",https://ieeexplore.ieee.org/document/8258065/,2017 IEEE International Conference on Big Data (Big Data),11-14 Dec. 2017,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICOS.2017.8280269,Phishing classification models: Issues and perspectives,IEEE,Conferences,"Never-ending phishing threats on cyberspace motivate researchers to develop more proficient phishing classification models to survive a supreme cyber-security with safe web services. However, such achievements remain incompetent in their performance against novel phish attacks. This is attributed to the induction factors of the classification model itself such as hybrid feature space, inactive learning on up-to-date data flows, and limited adaptation to the evolving phish attacks. In this light, this paper surveys the current achievements, studies their limitations, restates what induction factors need to boost for a successful real-time application. Consequently, future outlooks are recommended on how to devote well-performed anti-phishing scheme.",https://ieeexplore.ieee.org/document/8280269/,2017 IEEE Conference on Open Systems (ICOS),13-14 Nov. 2017,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ITEC53557.2022.9813908,Physically Inspired Neural Network for Modeling Induction Machine Nonlinear Magnetic Saturation,IEEE,Conferences,"In the automotive area highly utilized electric traction machines are used to achieve the maximum efficiency with minimum space and weight. Their special technical design results in a strongly nonlinear inductive behavior. Detailed information on the inductive parameters and therefore the saturation behavior are important to gain highly efficient control of induction machines. Usually, these information are determined by complex standardized identification methods or specified commissioning runs. In this paper, a feed-forward multi-layer-perceptron is proposed to learn the steady state saturation behavior of an induction machine. For this purpose, characteristic equations are used to develop a physically inspired neural network. The training is proceeded offline based on standard testbench data sets of automotive induction machines. The model is optimized for implementation on embedded systems for real-time application with limited calculation capacity. Special consideration is given to the trade-off between accuracy and computational effort. To address this issue different configurations are implemented, varying the networks depth and the activation functions. The performances of the models are compared and discussed to show that no additional parameter acquisition is needed for the proposed approach.",https://ieeexplore.ieee.org/document/9813908/,2022 IEEE Transportation Electrification Conference & Expo (ITEC),15-17 June 2022,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CNSC.2014.6906671,Pixelwise object class segmentation based on synthetic data using an optimized training strategy,IEEE,Conferences,"In this paper we present an approach for low-level body part segmentation based on RGB-D data. The RGB-D sensor is thereby placed at the ceiling and observes a shared workspace for human-robot collaboration in the industrial domain. The pixelwise information about certain body parts of the human worker is used by a cognitive system for the optimization of interaction and collaboration processes. In this context, for rational decision making and planning, the pixelwise predictions must be reliable despite the high variability of the appearance of the human worker. In our approach we treat the problem as a pixelwise classification task, where we train a random decision forest classifier on the information contained in depth frames produced by a synthetic representation of the human body and the ceiling sensor, in a virtual environment. As shown in similar approaches, the samples used for training need to cover a broad spectrum of the geometrical characteristics of the human, and possible transformations of the body in the scene. In order to reduce the number of training samples and the complexity of the classifier training, we therefore apply an elaborated and coupled strategy for randomized training data sampling and feature extraction. This allows us to reduce the training set size and training time, by decreasing the dimensionality of the sampling parameter space. In order to keep the creation of synthetic training samples and real-world ground truth data simple, we use a highly reduced virtual representation of the human body, in combination with KINECT skeleton tracking data from a calibrated multi-sensor setup. The optimized training and simplified sample creation allows us to deploy standard hardware for the realization of the presented approach, while yielding a reliable segmentation in real-time, and high performance scores in the evaluation.",https://ieeexplore.ieee.org/document/6906671/,2014 First International Conference on Networks & Soft Computing (ICNSC2014),19-20 Aug. 2014,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/Indo-TaiwanICAN48429.2020.9181341,Planogram Design Analytics using Image Processing,IEEE,Conferences,"A planogram is a tool for visual merchandising of retail stores. It displays a detailed view of the retail store with the major intent for product placement. The planogram is highly useful for examining the point of sale as it demonstrates the exact positioning of products. Two major benefits are there for building planograms while planning the store layout such as maximizing sales and space of the retail store.An important part of Planogram management is to read planogram drawings and images and convert these to representational data in CSV or database formats.In this part, Contour analysis of image using OpenCV along with combination of text detection and recognition is performed using Connectionist Text Proposal Network and Convolutional Recurrent Neural Network deep learning models respectively. The proposed planogram design analytics system is implemented using real data. The customer had tested the system as per different cases. The feedback obtained from them confirms that the system meets the requirements to their satisfaction.",https://ieeexplore.ieee.org/document/9181341/,"2020 Indo – Taiwan 2nd International Conference on Computing, Analytics and Networks (Indo-Taiwan ICAN)",7-15 Feb. 2020,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CIRA.2005.1554245,Plenary talk June 29; The 3rdGeneration of Robotics: Ubiquitous Robot,IEEE,Conferences,"This talk shows its possibility of implementation in real life through demonstrations using a Sobot, Rity: i) continuous interface between physical and virtual worlds ii) seamless transmission of Sobot between a PC and a Mobot, and iii) omnipresence of Sobot. Rity, developed at the Robot Intelligence Technology (RIT) Laboratory, KAIST, is a Sobot implemented as a 12 DOF artificial creature in the virtual 3D world created in a PC. It has virtual sensors to survive in the virtual world and physical sensors attached to the PC to interact with the real world. Based on sensor information it can express its emotion, and interact with human beings through a web camera in the real world. It can generate behaviors autonomously and has its own IP. This means that it can be accessed through a network at anywhere and anytime using any device. With this technique omnipresence of Sobot can be realized in a ubiquitous space. The eventual goal of this research is to integrate Sobot, Embot, and Mobot to build up a Ubibot so that ubiquitous services through it can be available in a ubiquitous era",https://ieeexplore.ieee.org/document/1554245/,2005 International Symposium on Computational Intelligence in Robotics and Automation,27-30 June 2005,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICDE48307.2020.00021,PoisonRec: An Adaptive Data Poisoning Framework for Attacking Black-box Recommender Systems,IEEE,Conferences,"Data-driven recommender systems that can help to predict users' preferences are deployed in many real online service platforms. Several studies show that they are vulnerable to data poisoning attacks, and attackers have the ability to mislead the system to perform as their desires. Considering the realistic scenario, where the recommender system is usually a black-box for attackers and complex algorithms may be deployed in them, how to learn effective attack strategies on such recommender systems is still an under-explored problem. In this paper, we propose an adaptive data poisoning framework, PoisonRec, which can automatically learn effective attack strategies on various recommender systems with very limited knowledge. PoisonRec leverages the reinforcement learning architecture, in which an attack agent actively injects fake data (user behaviors) into the recommender system, and then can improve its attack strategies through reward signals that are available under the strict black-box setting. Specifically, we model the attack behavior trajectory as the Markov Decision Process (MDP) in reinforcement learning. We also design a Biased Complete Binary Tree (BCBT) to reformulate the action space for better attack performance. We adopt 8 widely-used representative recommendation algorithms as our testbeds, and make extensive experiments on 4 different real-world datasets. The results show that PoisonRec has the ability to achieve good attack performance on various recommender systems with limited knowledge.",https://ieeexplore.ieee.org/document/9101655/,2020 IEEE 36th International Conference on Data Engineering (ICDE),20-24 April 2020,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/DSR.2011.6026835,Portality - The portal between virtuality and reality,IEEE,Conferences,"This paper proposes an innovative concept of community, called Mirror Reality, which is formed by the seamless interaction between users of physical space and virtual world. The Mirror Reality is relying on a portal to achieve the data translation and communication between virtual and reality worlds. This study employs the Ontology technology to analyze unified representation of objects and events in both virtual world and physical space. Furthermore, the research issues of creating a Mirror Reality are also identified. The portal that is implemented from the analysis is called Portality to emphasize its role between virtuality and reality. In other words, Portality takes care all of the data conversion and processing to permit the interoperability between two totally different worlds. Finally, the prototype of Mirror Reality for the university campus with its Portality is illustrated at the end.",https://ieeexplore.ieee.org/document/6026835/,2011 Defense Science Research Conference and Expo (DSR),3-5 Aug. 2011,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IJCNN.2017.7966215,Pose invariance through registration for hierarchical feature based pattern recognition systems,IEEE,Conferences,"One of the main challenges in pattern recognition is handling variations in pose, which has been addressed in the past using exhaustive training, increasingly complex neural network architectures, or state space transformations, but often with limits on pose variation. The solution presented here implements complete pose invariance by estimating affine transform parameters and then registering samples to archetypes. This solution is intended for hierarchical feature based systems that use global features to get into the vicinity of the solution and then use local features to achieve higher precision if required. Here the entire distribution of a set of local features represents a global feature, from which properties are extracted that describe its pose. The difference between these properties for a pair of distributions yields transform parameters. This solution was deployed as a stand-alone solution in a low-noise real-world system that automatically registers raster maps to GIS datasets, but to facilitate the reproducibility of experiments, the current paper presents results from randomly generated datasets.",https://ieeexplore.ieee.org/document/7966215/,2017 International Joint Conference on Neural Networks (IJCNN),14-19 May 2017,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/INFCOMW.2019.8845276,Poster Abstract: Deep Learning Workloads Scheduling with Reinforcement Learning on GPU Clusters,IEEE,Conferences,"With the recent widespread adoption of deep learning (DL) in academia and industry, more attention are attracted by DL platform, which can support research and development (R&D) of AI firms, institutes and universities. Towards an off-the-shelf distributed GPU cluster, prior work propose prediction-based schedulers to allocate resources for diverse DL workloads. However, the prediction-based schedulers have disadvantages on prediction accuracy and offline-profiling costs. In this paper, we propose a learning-based scheduler, which models the scheduling problem as a reinforcement learning problem, achieving minimum average job completion time and maximum system utilization. The scheduler contains the designs of state space, action space, reward function and update scheme. Furthermore, we will evaluate our proposed scheduler implemented as a plugin of Tensorflow on real cluster and large-scale simulation.",https://ieeexplore.ieee.org/document/8845276/,IEEE INFOCOM 2019 - IEEE Conference on Computer Communications Workshops (INFOCOM WKSHPS),29 April-2 May 2019,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISCE.2013.6570213,Power monitor v2: Novel power saving Android application,IEEE,Conferences,"This paper presents a novel scheme to derive power saving profiles based on the usage patterns of the Android devices. The entire architecture is developed as an Android app ""Power Monitor v2"" and is deployed to the smart devices. A monitoring module of the app periodically collects several data from the devices and stores them locally. A learning engine then operates on the raw data to generate multiple usage patterns over time and space, which characterizes the user contexts. The engine then processes the patterns to generate power saving profiles dynamically within the devices. The profiles contain several system settings of the smart devices and intelligently optimize power consumption. We also present a real life usage pattern and the power saving profile. The overall battery life for the device estimated to increase by 82%.",https://ieeexplore.ieee.org/document/6570213/,2013 IEEE International Symposium on Consumer Electronics (ISCE),3-6 June 2013,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCDS.2017.8120453,Power optimization using Markov decision process based on multi-parameter constraint modeling,IEEE,Conferences,"Power optimization based on intelligent algorithm draws more and more attention. This article presents a novel low power optimization strategy based on the high level software power management employing Markov Process for charactering the real running workload. This article formulates workload characterization and selection with stochastic process method, and solves the formula using dynamic voltage frequency scaling base on microprocessor. Based on Markov process, the multi-parameter constraints has been employed to exploit the optimization space. Comparing with existing power optimization algorithm, our proposed power optimization algorithm doesn't need any prior data and maintains a value function representing expected reward. As many hardware events can be effectively captured and modeled, this optimization technique is capable to explore an ideal tradeoff in the constraint space.",https://ieeexplore.ieee.org/document/8120453/,"2017 International Conference on Circuits, Devices and Systems (ICCDS)",5-8 Sept. 2017,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/MASCOTS.2019.00045,Practical Design Space Exploration,IEEE,Conferences,"Multi-objective optimization is a crucial matter in computer systems design space exploration because real-world applications often rely on a trade-off between several objectives. Derivatives are usually not available or impractical to compute and the feasibility of an experiment can not always be determined in advance. These problems are particularly difficult when the feasible region is relatively small, and it may be prohibitive to even find a feasible experiment, let alone an optimal one. We introduce a new methodology and corresponding software framework, HyperMapper 2.0, which handles multi-objective optimization, unknown feasibility constraints, and categorical/ordinal variables. This new methodology also supports injection of the user prior knowledge in the search when available. All of these features are common requirements in computer systems but rarely exposed in existing design space exploration systems. The proposed methodology follows a white-box model which is simple to understand and interpret (unlike, for example, neural networks) and can be used by the user to better understand the results of the automatic search. We apply and evaluate the new methodology to the automatic static tuning of hardware accelerators within the recently introduced Spatial programming language, with minimization of design run-time and compute logic under the constraint of the design fitting in a target field-programmable gate array chip. Our results show that HyperMapper 2.0 provides better Pareto fronts compared to state-of-the-art baselines, with better or competitive hypervolume indicator and with 8x improvement in sampling budget for most of the benchmarks explored.",https://ieeexplore.ieee.org/document/8843094/,"2019 IEEE 27th International Symposium on Modeling, Analysis, and Simulation of Computer and Telecommunication Systems (MASCOTS)",21-25 Oct 2019,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICNC.2010.5584403,Practical travel time prediction algorithms based on neural network and data fusion for urban expressway,IEEE,Conferences,"Current travel time prediction algorithms often need large amount of travel time data to identify the algorithms parameters. However, it is highly costly and time-consuming to obtain many travel time data. In this paper, we proposed an algorithm structure to calculate the travel time which utilize neural network to dynamically predict future speed and employ data fusion to integrate the speed data of different detectors in a urban expressway link. Based on the algorithm structure, two practical travel time prediction algorithms, called as Space Discretization Travel Time Calculation Algorithm (SDTCM) and Speed Integral Travel Time Calculation Method (SITCM), were developed by the discretization of the space and integral of the predicted speed. Vehicle plate recognition technology was used to collect the real travel time data in the test section on Beijing Third-Ring urban expressway to evaluate the algorithms. The obtained results show that the average prediction error of two algorithms are both less than 10% which can meet the requirement of the field applications and the algorithms are easy to be implemented as no travel time data collection are needed in advance. The two algorithms have some advantages and disadvantages over each other in accuracy and smoothness. Although SDTCM is more accurate than SITCM in general, the fluctuation of error of the SITCM is a little smoother than SDTCM.",https://ieeexplore.ieee.org/document/5584403/,2010 Sixth International Conference on Natural Computation,10-12 Aug. 2010,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1145/2656045.2656064,Precise piecewise affine models from input-output data,IEEE,Conferences,"Formal design and analysis of embedded control software relies on mathematical models of dynamical systems, and such models can be hard to obtain. In this paper, we focus on automatic construction of piecewise affine models from input-output data. Given a set of examples, where each example consists of a d-dimensional real-valued input vector mapped to a real-valued output, we want to compute a set of affine functions that covers all the data points up to a specified degree of accuracy, along with a disjoint partitioning of the space of all inputs defined using a Boolean combination of affine inequalities with one region for each of the learnt functions. While traditional machine learning algorithms such as linear regression can be adapted to learn the set of affine functions, we develop new techniques based on automatic construction of interpolants to derive precise guards defining the desired partitioning corresponding to these functions. We report on a prototype tool, MOSAIC, implemented in Matlab. We evaluate its performance using some synthetic data, and compare it against known techniques using data-sets modeling electronic placement process in pick-and-place machines.",https://ieeexplore.ieee.org/document/6986111/,2014 International Conference on Embedded Software (EMSOFT),12-17 Oct. 2014,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IJCNN.2018.8489412,Prediction of Spatial Spectrum in Cognitive Radio using Cellular Simultaneous Recurrent Networks,IEEE,Conferences,"In cognitive radio networks, it is desirable to determine radio spectrum usage in frequency, time, and spatial domains. Spectrum data improves cognitive radio network planning, sensing, routing, and security. Due to cost concerns, spectrum monitors are deployed sparsely in space, spectrum usage at nearby locations can be modeled for use in these applications. Previous work using neural networks for spatial spectrum prediction involved prior knowledge of transmitter locations as input to the models. In practical scenarios, the prior knowledge is not available. Hence, this work considers prediction of the spatial spectrum without knowing the transmitter location information. The prediction task is achieved by using a specialized recurrent neural network known as Cellular Simultaneous Recurrent Network (CSRN). Our investigation shows the proposed recurrent neural network operates in real-time and is generalized to offer spectrum estimations without further changes to the network, even when a transmitter location is changed. The experiments are conducted in a challenging indoor environment to assess the performance in a practical scenario. Our results suggest the CSRN can learn efficiently to predict signal across an indoor space while transmitters move to different locations. We perform a performance comparison of our proposed technique with an MLP based estimation method. Our analysis further suggests that the CSRN achieves comparable prediction accuracy to that of the MLP based method. The major advantage of the proposed CSRN based method is the ability to perform prediction from new radio configurations without retraining the network, and, hence is more suitable for a real-time practical environment.",https://ieeexplore.ieee.org/document/8489412/,2018 International Joint Conference on Neural Networks (IJCNN),8-13 July 2018,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IJCNN.2003.1224017,Prediction of pitch and yaw head movements via recurrent neural networks,IEEE,Conferences,"In virtual-environment (VE) applications, where virtual objects are presented in a head-mounted display, virtual images must be continuously stabilized in space against the user's head motion. Latencies in head-motion compensation cause virtual objects to swim around instead of being stable in space. This results in an unnatural feel, disorientation, and simulation sickness in addition to errors in fitting/matching of virtual and real objects. Visual update delays are a critical technical obstacle for implementation of head-mounted displays in a wide variety of applications. To address this problem, we propose to use machine learning techniques to define a forward model of head movement based on angular velocity information. In particular, we utilize recurrent neural network to capture the temporal pattern of pitch and yaw motion. Our results demonstrate an ability to predict head motion up to 40 ms. ahead thus eliminating the main source of latencies. The accuracy of the system is tested for conditions akin to those encountered in virtual environments. These results demonstrate successful generalization by the learning system.",https://ieeexplore.ieee.org/document/1224017/,"Proceedings of the International Joint Conference on Neural Networks, 2003.",20-24 July 2003,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/EMBC.2017.8037453,Predictive local receptive fields based respiratory motion tracking for motion-adaptive radiotherapy,IEEE,Conferences,"Extracranial robotic radiotherapy employs external markers and a correlation model to trace the tumor motion caused by the respiration. The real-time tracking of tumor motion however requires a prediction model to compensate the latencies induced by the software (image data acquisition and processing) and hardware (mechanical and kinematic) limitations of the treatment system. A new prediction algorithm based on local receptive fields extreme learning machines (pLRF-ELM) is proposed for respiratory motion prediction. All the existing respiratory motion prediction methods model the non-stationary respiratory motion traces directly to predict the future values. Unlike these existing methods, the pLRF-ELM performs prediction by modeling the higher-level features obtained by mapping the raw respiratory motion into the random feature space of ELM instead of directly modeling the raw respiratory motion. The developed method is evaluated using the dataset acquired from 31 patients for two horizons in-line with the latencies of treatment systems like CyberKnife. Results showed that pLRF-ELM is superior to that of existing prediction methods. Results further highlight that the abstracted higher-level features are suitable to approximate the nonlinear and non-stationary characteristics of respiratory motion for accurate prediction.",https://ieeexplore.ieee.org/document/8037453/,2017 39th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC),11-15 July 2017,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ANZIIS.1994.396989,Primitive concept formation,IEEE,Conferences,"Our goal is to demonstrate the feasibility of an autonomous learning agent by developing means to learn and employ concepts in a primitive machine intelligence which must operate in a real-time, uncertain (noisy) environment. The paper reports on the first steps towards such an agent: the development of an agent, Alice, who starts out with only a primitive set of concepts-corresponding to perceptible attributes of objects in the environment and to her own utility function-and who generates a conceptual structure using cognitively plausible rules of concept formation and refinement, abstracting from the immediate attributes of the mushrooms she finds and their longer-term impact on her utility, in a goal-driven manner. The concept formation rules we have developed are more conservative than such standard methods of concept formation as version space methods and ID3. We suggest that this caution offers a competitive advantage in difficult environments.<>",https://ieeexplore.ieee.org/document/396989/,Proceedings of ANZIIS '94 - Australian New Zealnd Intelligent Information Systems Conference,29 Nov.-2 Dec. 1994,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICDCS51616.2021.00077,Privacy-Preserving Neural Network Inference Framework via Homomorphic Encryption and SGX,IEEE,Conferences,"Edge computing is a promising paradigm that pushes computing, storage, and energy to the networks' edge. It utilizes the data nearby the users to provide real-time, energy-efficient, and reliable services. Neural network inference in edge computing is a powerful tool for various applications. However, edge server will collect more personal sensitive information of users inevitably. It is the most basic requirement for users to ensure their security and privacy while obtaining accurate inference results. Homomorphic encryption (HE) technology is confidential computing that directly performs mathematical computing on encrypted data. But it only can carry out limited addition and multiplication operation with very low efficiency. Intel software guard extension (SGX) can provide a trusted isolation space in the CPU to ensure the confidentiality and integrity of code and data executed. But several defects are hard to overcome due to hardware design limitations when applying SGX in inference services. This paper proposes a hybrid framework utilizing SGX to accelerate the HE-based convolutional neural network (CNN) inference, eliminating the approximation operations in HE to improve inference accuracy in theory. Besides, SGX is also taken as a built-in trusted third party to distribute keys, thereby improving our framework's scalability and flexibility. We have quantified the various CNN operations in the respective cases of HE and SGX to provide the foresight practice. Taking the connected and autonomous vehicles as a case study in edge computing, we implemented this hybrid framework in CNN to verify its feasibility and advantage.",https://ieeexplore.ieee.org/document/9546527/,2021 IEEE 41st International Conference on Distributed Computing Systems (ICDCS),7-10 July 2021,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ROBIO49542.2019.8961870,Probabilistic Inferences on Quadruped Robots: An Experimental Comparison,IEEE,Conferences,"Due to the reality gap, computer software cannot fully model the physical robot in its environment, with noise, ground friction, and energy consumption. Consequently, a limited number of researchers work on applying machine learning in real-world robots. In this paper, we use two intelligent black-box optimization algorithms, Bayesian Optimization (BO) and Covariance Matrix Adaptation Evolution Strategy (CMA-ES), to solve a quadruped robot gait's parametric search problem in 10 dimensions, and compare these two methods to find which one is more suitable for legged robots' controller parameters tuning. Our results show that both methods can find an optimal solution in 130 iterations. BO converges faster than CMA-ES within its constrained range, while CMA-ES finds the optimum in the continuous space. Compared with the specific controller parameters of two methods, we also find that for quadruped robot's oscillators, the angular amplitude is the most important parameter. Thus, it is very beneficial for the quick parametric search of legged robots&#x2019; controllers and avoids time-consuming manual tuning.",https://ieeexplore.ieee.org/document/8961870/,2019 IEEE International Conference on Robotics and Biomimetics (ROBIO),6-8 Dec. 2019,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SoutheastCon48659.2022.9764053,Profiling CPU Behavior for Detection of Android Ransomware,IEEE,Conferences,"Android devices continue to dominate the market for global smartphone users, thus making them an ideal target for malicious software developers. In the past, side-channel attacks have been used for malicious purposes where attackers monitor system data such as power consumption, electromagnetic emissions, and CPU timing to infer sensitive user information. Likewise, researchers have begun to use mathematical models and machine learning techniques on side-channel data as a means to detect malicious activity or malware. In this paper we look at an alternative method for malware detection using side-channel analysis of CPU frequency data on Android smartphones. We perform a case-study analysis to validate feasibility of the technique and use a nonlinear phase space analysis (NLPSA) for detecting anomalous behavior based on power variations. Our study looks at a single ransomware artifact (WannaLocker) evaluated on a single Android device and operating system. As a contribution, we are the first to utilize the Perfetto system tracing tool as a potential data provider, which now comes standard on platforms since Android 9 Pie OS. Our case study results show that our NLPSA approach can classify and detect execution of the ransomware sample on a real Android device with perfect prediction accuracy in training, thus giving an initial validation to the approach.",https://ieeexplore.ieee.org/document/9764053/,SoutheastCon 2022,26 March-3 April 2022,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CIG.2019.8848047,Project Thyia: A Forever Gameplayer,IEEE,Conferences,"The space of Artificial Intelligence entities is dominated by conversational bots. Some of them fit in our pockets and we take them everywhere we go, or allow them to be a part of human homes. Siri, Alexa, they are recognised as present in our world. But a lot of games research is restricted to existing in the separate realm of software. We enter different worlds when playing games, but those worlds cease to exist once we quit. Similarly, AI game-players are run once on a game (or maybe for longer periods of time, in the case of learning algorithms which need some, still limited, period for training), and they cease to exist once the game ends. But what if they didn't? What if there existed artificial game-players that continuously played games, learned from their experiences and kept getting better? What if they interacted with the real world and us, humans: live-streaming games, chatting with viewers, accepting suggestions for strategies or games to play, forming opinions on popular game titles? In this paper, we introduce the vision behind a new project called Thyia, which focuses around creating a present, continuous, `always-on', interactive game-player.",https://ieeexplore.ieee.org/document/8848047/,2019 IEEE Conference on Games (CoG),20-23 Aug. 2019,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCW.2019.8757132,QoE Aware Transcoding for Live Streaming in SDN-Based Cloud-Aided HetNets: An Actor-Critic Approach,IEEE,Conferences,"With the advances in hand-held devices (smart-phones and tablets, etc.) and high speed wireless networks, users have an explosive growth demand for live streaming service. Due to the diversity of user equipments (UEs), the live streaming has to be transcoded as different versions. However, transcoding is a computationally expensive and time consuming process. Since the shortage of computational resources and unstable of wireless networks, providing strict delay requirement and high quality live videos for wireless UEs is a big challenge. In this paper, we investigate user scheduling, transcoding decision, computational and wireless spectrum resources allocation problem in software-defined networking (SDN) based cloud-aided of heterogeneous networks (HetNets). Our research focuses on improving UEs' quality of experience (QoE) while guaranteeing time-delay requirement for live streaming services. Different from existing literature, to approach the real wireless environment, the available computational and wireless spectrum resources are modeled as random processes in our research. Considering dynamic characteristics of wireless networks and the available resources, the above problem is modeled as a Markov decision problem (MDP). Since the action space of the MDP is multi-dimensional continuous variables mixed with discrete variables, traditional learning algorithms are powerless. Therefore, an online actor critic algorithm is proposed to resolve the problem. Simulation results show the proposed algorithm has superior performances compared with the policy gradient algorithm and deep Q-learning network (DQN).",https://ieeexplore.ieee.org/document/8757132/,2019 IEEE International Conference on Communications Workshops (ICC Workshops),20-24 May 2019,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CCAAW.2019.8904903,Quantifying Degradations of Convolutional Neural Networks in Space Environments,IEEE,Conferences,"Advances in machine learning applications for image processing, natural language processing, and direct ingestion of radio frequency signals continue to accelerate. Less attention, however, has been paid to the resilience of these machine learning algorithms when implemented on real hardware and subjected to unintentional and/or malicious errors during execution, such as those occurring from space-based single event upsets (SEU). This paper presents a series of results quantifying the rate and level of performance degradation that occurs when convolutional neural nets (CNNs) are subjected to selected bit errors in single-precision number representations. This paper provides results that are conditioned upon ten different error case events to isolate the impacts showing that CNN performance can be gradually degraded or reduced to random guessing based on where errors arise. The degradations are then translated into expected operational lifetimes for each of four CNNs when deployed to space radiation environments. The discussion also provides a foundation for ongoing research that enhances the overall resilience of neural net architectures and implementations in space under both random and malicious error events, offering significant improvements over current implementations. Future work to extend these CNN resilience evaluations, conditioned upon architectural design elements and well-known error correction methods, is also introduced.",https://ieeexplore.ieee.org/document/8904903/,2019 IEEE Cognitive Communications for Aerospace Applications Workshop (CCAAW),25-26 June 2019,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/HPCA53966.2022.00057,QuantumNAS: Noise-Adaptive Search for Robust Quantum Circuits,IEEE,Conferences,"Quantum noise is the key challenge in Noisy Intermediate-Scale Quantum (NISQ) computers. Previous work for mitigating noise has primarily focused on gate-level or pulse-level noise-adaptive compilation. However, limited research has explored a higher level of optimization by making the quantum circuits themselves resilient to noise.In this paper, we propose QuantumNAS, a comprehensive framework for noise-adaptive co-search of the variational circuit and qubit mapping. Variational quantum circuits are a promising approach for constructing quantum neural networks for machine learning and variational ansatzes for quantum simulation. However, finding the best variational circuit and its optimal parameters is challenging due to the large design space and parameter training cost. We propose to decouple the circuit search from parameter training by introducing a novel SuperCircuit. The SuperCircuit is constructed with multiple layers of pre-defined parameterized gates (e.g., U3 and CU3) and trained by iteratively sampling and updating the parameter subsets (SubCircuits) of it. It provides an accurate estimation of SubCircuits performance trained from scratch. Then we perform an evolutionary co-search of SubCircuit and its qubit mapping. The SubCircuit performance is estimated with parameters inherited from SuperCircuit and simulated with real device noise models. Finally, we perform iterative gate pruning and finetuning to remove redundant gates in a fine-grained manner.Extensively evaluated with 12 quantum machine learning (QML) and variational quantum eigensolver (VQE) benchmarks on 14 quantum computers, QuantumNAS significantly outperforms noise-unaware search, human, random, and existing noise-adaptive qubit mapping baselines. For QML tasks, QuantumNAS is the first to demonstrate over 95% 2-class, 85% 4-class, and 32% 10-class classification accuracy on real quantum computers. It also achieves the lowest eigenvalue for VQE tasks on H<inf>2</inf>, H<inf>2</inf>O, LiH, CH<inf>4</inf>, BeH<inf>2</inf> compared with UCCSD baselines. We also open-source the TorchQuantum library for fast training of parameterized quantum circuits to facilitate future research.",https://ieeexplore.ieee.org/document/9773233/,2022 IEEE International Symposium on High-Performance Computer Architecture (HPCA),2-6 April 2022,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IJCNN.2012.6252422,Query based hybrid learning models for adaptively adjusting locality,IEEE,Conferences,"Local learning employs locality adjusting mechanisms to give local function estimation for each query, while global learning tries to capture the global distribution characteristics of the entire training set. When fitting well with local characteristics of each individual region, the locality parameter may help local learning to improve performance. However, the real data distribution is impossible to get for a real-world problem, and thus an optimal locality is hard to get for each query. In addition, it is quite time-consuming to build an independent local model for each query. To solve these problems, we present strategies for estimating and tuning locality according to local distribution. Based on local distribution estimation, global learning and local learning are combined to achieve a good compromise between capacity and locality. In addition, multi-objective learning principles for the combination are also given. In implementation, a unique global model is first built on the entire training set based on empirical minimization principle. For each query, it is measured that whether the global model can well fit the vicinity space of the query. When an uneven local distribution is found, the locality of the model is tuned, and a specific local model will be built on the local region. To investigate the performance of hybrid models, we apply them to a typical learning problem-spam filtering, in which data are always found to be unevenly distributed. Experiments were conducted on five real-world corpora, namely PU1, PU2, PU3, PUA, and TREC07. It is shown that the hybrid models can achieve a better compromise between capacity and locality, and hybrid models outperform both global learning and local learning.",https://ieeexplore.ieee.org/document/6252422/,The 2012 International Joint Conference on Neural Networks (IJCNN),10-15 June 2012,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/DS-RT.2016.9,RA2: Predicting Simulation Execution Time for Cloud-Based Design Space Explorations,IEEE,Conferences,"Design space exploration refers to the evaluation of implementation alternatives for many engineering and design problems. A popular exploration approach is to run a large number of simulations of the actual system with varying sets of configuration parameters to search for the optimal ones. Due to the potentially huge resource requirements, cloud-based simulation execution strategies should be considered in many cases. In this paper, we look at the issue of running large-scale simulation-based design space exploration problems on commercial Infrastructure-as-a-Service clouds, namely Amazon EC2, Microsoft Azure and Google Compute Engine. To efficiently manage cloud resources used for execution, the key problem would be to accurately predict the running time for each simulation instance in advance. This is not trivial due to the currently wide range of cloud resource types which offer varying levels of performance. In addition, the widespread use of virtualization techniques in most cloud providers often introduces unpredictable performance interference. In this paper, we propose a resource and application-aware (RA2) prediction approach to combat performance variability on clouds. In particular, we employ neural network based techniques coupled with non-intrusive monitoring of resource availability to obtain more accurate predictions. We conducted extensive experiments on commercial cloud platforms using an evacuation planning design problem over a month-long period. The results demonstrate that it is possible to predict simulation execution times in most cases with high accuracy. The experiments also provide some interesting insights on how we should run similar simulation problems on various commercially available clouds.",https://ieeexplore.ieee.org/document/7789881/,2016 IEEE/ACM 20th International Symposium on Distributed Simulation and Real Time Applications (DS-RT),21-23 Sept. 2016,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICMNN.1994.593723,RAN/sup 2/SOM: a reconfigurable neural network architecture based on bit stream arithmetic,IEEE,Conferences,"We introduce the RAN/sup 2/SOM (Reconfigurable Architecture Neural Networks with Serially Operating Multipliers) architecture, a neural net architecture with a reconfigurable interconnection scheme based on bit stream arithmetic. RAN/sup 2/SOM nets are implemented using field programmable gate array logic. By conducting the training phase in software and executing the actual application in hardware, conflicting demands can be met: training benefits from a fast edit-debug cycle, and once the design has stabilized a hardware implementation results in higher performance. While neural nets have been implemented in hardware in the past, larger digital nets have not been possible due to the real-estate requirements of single neutrons. We present a bit-serial encoding scheme and computation model, which allows space-efficient computation of the sum of weighted inputs, thereby facilitating the implementation of complex neural networks.",https://ieeexplore.ieee.org/document/593723/,Proceedings of the Fourth International Conference on Microelectronics for Neural Networks and Fuzzy Systems,26-28 Sept. 1994,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/FPA.1994.636106,RCS: a reference model architecture for intelligent control,IEEE,Conferences,"The Real-time Control System (RCS) is a reference model architecture for intelligent real time control systems. It partitions the control problem into four basic elements: task decomposition, world modeling, sensory processing, and value judgment. It clusters these elements into computational nodes that have responsibility for specific subsystems and arranges these nodes in hierarchical layers such that each layer has characteristic functionality and timing. The RCS architecture has a systematic regularity, and recursive structure that suggests a canonical form. Systems based on the RCS architecture have been implemented more or less for a wide variety of applications that include loading and unloading of parts and tools in machine tools, controlling machining workstations, performing robotic deburring and chamfering, and controlling space station telerobots, multiple autonomous undersea vehicles, unmanned land vehicles, coal mining automation systems. postal service mail handling systems, and submarine operational automation systems. Software developers accustomed to using RCS for building control systems have found it provides a structured design approach that makes it possible to reuse a great deal of software.",https://ieeexplore.ieee.org/document/636106/,Proceedings of PerAc '94. From Perception to Action,7-9 Sept. 1994,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CVPR.2019.00605,RL-GAN-Net: A Reinforcement Learning Agent Controlled GAN Network for Real-Time Point Cloud Shape Completion,IEEE,Conferences,"We present RL-GAN-Net, where a reinforcement learning (RL) agent provides fast and robust control of a generative adversarial network (GAN). Our framework is applied to point cloud shape completion that converts noisy, partial point cloud data into a high-fidelity completed shape by controlling the GAN. While a GAN is unstable and hard to train, we circumvent the problem by (1) training the GAN on the latent space representation whose dimension is reduced compared to the raw point cloud input and (2) using an RL agent to find the correct input to the GAN to generate the latent space representation of the shape that best fits the current input of incomplete point cloud. The suggested pipeline robustly completes point cloud with large missing regions. To the best of our knowledge, this is the first attempt to train an RL agent to control the GAN, which effectively learns the highly nonlinear mapping from the input noise of the GAN to the latent space of point cloud. The RL agent replaces the need for complex optimization and consequently makes our technique real time. Additionally, we demonstrate that our pipelines can be used to enhance the classification accuracy of point cloud with missing data.",https://ieeexplore.ieee.org/document/8953469/,2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),15-20 June 2019,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IPDPS53621.2022.00064,RLRP: High-Efficient Data Placement with Reinforcement Learning for Modern Distributed Storage Systems,IEEE,Conferences,"Modern distributed storage systems with massive data and storage nodes pose higher requirements to the data placement strategy. Furthermore, with emerged new storage devices, heterogeneous storage architecture has become increasingly common and popular. However, traditional strategies expose great limitations in the face of these requirements, especially do not well consider distinct characteristics of heterogeneous storage nodes yet, which will lead to suboptimal performance. In this paper, we present and evaluate the RLRP, a deep reinforcement learning (RL) based replica placement strategy. RLRP constructs placement and migration agents through the Deep-Q-Network (DQN) model to achieve fair distribution and adaptive data migration. Besides, RLRP provides optimal performance for heterogeneous environment by an attentional Long Short-term Memory (LSTM) model. Finally, RLRP adopts Stagewise Training and Model fine-tuning to accelerate the training of RL models with large-scale state and action space. RLRP is implemented on Park and the evaluation results indicate RLRP is a highly efficient data placement strategy for modern distributed storage systems. RLRP can reduce read latency by 10&#x0025;&#x223C;50&#x0025; in heterogeneous environment compared with existing strategies. In addition, RLRP is used in the real-world system Ceph, which improves the read performance of Ceph by 30&#x0025;&#x223C;40&#x0025;.",https://ieeexplore.ieee.org/document/9820675/,2022 IEEE International Parallel and Distributed Processing Symposium (IPDPS),30 May-3 June 2022,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IJCNN.2019.8851689,RPR-BP: A Deep Reinforcement Learning Method for Automatic Hyperparameter Optimization,IEEE,Conferences,"We introduce a new deep reinforcement learning architecture - RPR-BP to optimize hyperparameter for any machine learning model on a given data set. In this method, an agent constructed by a Long Short-Term Memory Network aims at maximizing the expected accuracy of a machine learning model on a validation set. At each iteration, it selects a set of hyperparameters and uses the accuracy of the model on the validation set as the reward signal to update its internal parameters. After multiple iterations, the agent learns how to improve its decisions. However, the computation of the reward requires significant time and leads to low sample efficiency. To speed up training, we employ a neural network to predict the reward. The training process for the agent and the prediction network is divided into three phases: Real-Predictive-Real (RPR). First, the agent and the prediction network are trained by the real experience; then, the agent is trained by the reward generated from the prediction network; finally, the agent is trained again by the real experience. In this way, we can speed up training and make the agent achieve a high accuracy. Besides, to reduce the variance, we propose a Bootstrap Pool (BP) to guide the exploration in the search space. The experiment was carried out by optimizing hyperparameters of two widely used machine learning models: Random Forest and XGBoost. Experimental results show that the proposed method outperforms random search, Bayesian optimization and Tree-structured Parzen Estimator in terms of accuracy, time efficiency and stability.",https://ieeexplore.ieee.org/document/8851689/,2019 International Joint Conference on Neural Networks (IJCNN),14-19 July 2019,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/GCAIoT51063.2020.9345877,RSSI Based Real-Time and Secure Smart Parking Management System,IEEE,Conferences,"Discovering an available parking space in an unsupervised parking area is one of the critical issues that vehicle owners face which consumes a considerable time and effort. In this paper, a Received Signal Strength Indicator (RSSI) based approach is proposed for detecting available parking spaces. The Non-Linear Least Square method has been used to minimize the effects from external interferences. Further, itemploys a lightweight and scalable Message Queuing Telemetry Transport (MQTT) communication protocol. The proposed method does not require sensors to be deployed on each parking slot, which is more frequently used technique in the existing approaches, and it can provide a real-time representation of available parking slots. Moreover, the vehicle owners can discover available parking spaces remotely using the developed mobile application; thus, it saves a significant amount of time. Therefore, it exhibits a great promise as a real-time, cost effective, highly scalable and secure solution.",https://ieeexplore.ieee.org/document/9345877/,2020 IEEE Global Conference on Artificial Intelligence and Internet of Things (GCAIoT),12-16 Dec. 2020,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/VIMS.2002.1009368,"ReMLab: a Java-based remote, didactic measurement laboratory",IEEE,Conferences,"The availability of didactic experimental laboratories is of utmost importance for technical schools, mainly the engineering faculties. Experimental laboratories, however, represent a great investment, both in terms of required money and space, that may become unbearable to the schools, especially when the number of students and the number of subjects that require lab classes increase. Remote laboratories are a very cost-effective solution to this problem, provided they allow the students to access real instruments from any site connected to the Internet without any need for buying commercial software. This paper shows how this goal has been achieved by implementing a Java-based client-server architecture that is presently used as subsidiary lab for the measurement subjects in the Electrical Engineering curriculum at the Politecnico di Milano.",https://ieeexplore.ieee.org/document/1009368/,2002 IEEE International Symposium on Virtual and Intelligent Measurement Systems (IEEE Cat. No.02EX545),19-20 May 2002,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CDC.2014.7039601,Reachability-based safe learning with Gaussian processes,IEEE,Conferences,"Reinforcement learning for robotic applications faces the challenge of constraint satisfaction, which currently impedes its application to safety critical systems. Recent approaches successfully introduce safety based on reachability analysis, determining a safe region of the state space where the system can operate. However, overly constraining the freedom of the system can negatively affect performance, while attempting to learn less conservative safety constraints might fail to preserve safety if the learned constraints are inaccurate. We propose a novel method that uses a principled approach to learn the system's unknown dynamics based on a Gaussian process model and iteratively approximates the maximal safe set. A modified control strategy based on real-time model validation preserves safety under weaker conditions than current approaches. Our framework further incorporates safety into the reinforcement learning performance metric, allowing a better integration of safety and learning. We demonstrate our algorithm on simulations of a cart-pole system and on an experimental quadrotor application and show how our proposed scheme succeeds in preserving safety where current approaches fail to avoid an unsafe condition.",https://ieeexplore.ieee.org/document/7039601/,53rd IEEE Conference on Decision and Control,15-17 Dec. 2014,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ULTSYM.2009.5441886,Real time adaptive parametric equalization of Ultrasonic Transducers,IEEE,Conferences,"Parametric equalization is often used to achieve a desired response from an audio transmitter, but is rarely applied to ultrasonic transducer systems. The ability of a broadband ultrasonic transmission and reception system to adapt its frequency and time domain response to changing acoustic conditions would be a distinct advantage in certain applications. Ultrasonic remote monitoring systems would benefit significantly from this ability, as signal levels could be minimized and consequentially the transmitter power consumption decreased. This work presents a real-time adaptive ultrasonic parametric equalizer using optimization driven Matlab code to control the coefficients of a switched capacitor filter network implemented in a Cypress PSOC (Programmable System On a Chip). In this work, adaptive parametric magnitude equalization of a through-transmission ultrasonic system using CUTs (Capacitive Ultrasonic Transducers) has been achieved in real time by tracking a desired SNR (signal to noise ratio) across the operational frequency spectrum. A Matlab general radial basis function (GRBF) artificial neural network (ANN) was developed to control the equalization filter coefficients based on the received frequency response data. The adaptive parametric equaliser adjusts the magnitude of the driving signal to maintain the desired SNR as closely as possible. The neural network was trained using PSO (Particle Swarm Optimization) back-propagation, based on a state space model of the system developed from frequency response data. The developed equalization circuitry, which is switched capacitor based and was fully implemented on the PSOC, is also described.",https://ieeexplore.ieee.org/document/5441886/,2009 IEEE International Ultrasonics Symposium,20-23 Sept. 2009,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICDSP.2002.1028228,Real time nonlinear ARMA model structure identification,IEEE,Conferences,"This paper addresses the nonlinear autoregressive moving average (NARMA) identification problem in connection with the choice of the model structure (order) and computation of the time varying system coefficients. We introduce an intelligent method that is based on the reformulation of the problem in the standard state space form and the subsequent implementation of a bank of extended Kalman filters, each fitting a different order model. The problem is reduced then to selecting the true model, using the well known multi-model partitioning theory. Simulations illustrate that the proposed method is selecting the correct model order and identifies the time varying model parameters in real time, while it is insensitive to the noise variations.",https://ieeexplore.ieee.org/document/1028228/,2002 14th International Conference on Digital Signal Processing Proceedings. DSP 2002 (Cat. No.02TH8628),1-3 July 2002,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CAC51589.2020.9327198,Real time production scheduling based on Asynchronous Advanced Actor Critic and composite dispatching rule,IEEE,Conferences,"In the era of smart manufacturing, the requirements of real-time, adaptability and long-term optimization of the semiconductor manufacturing system (SMS) are increased due to the further expanded, more complicated and unpredictable uncertainties. This paper addresses the real time production scheduling of SMS to maximize on productivity (PROD) and average daily movement (AvgMOV), and minimize mean cycle time (MCT). We propose an Asynchronous Advanced Actor Critic and composite dispatching rule based real time production scheduling (A3C-CR2) framework, which involves a scheduling knowledge training module and a deployment module. The action space is designed as a combination of the composite dispatching rule (CDR) based continuous scheduling actions. In terms of various performance indices over a long period, the proposed A3C-CR2 approach outperforms other dispatching rules.",https://ieeexplore.ieee.org/document/9327198/,2020 Chinese Automation Congress (CAC),6-8 Nov. 2020,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/TENCON.2018.8650524,Real-Time American Sign Language Recognition Using Skin Segmentation and Image Category Classification with Convolutional Neural Network and Deep Learning,IEEE,Conferences,A real-time sign language translator is an important milestone in facilitating communication between the deaf community and the general public. We hereby present the development and implementation of an American Sign Language (ASL) fingerspelling translator based on skin segmentation and machine learning algorithms. We present an automatic human skin segmentation algorithm based on color information. The YCbCr color space is employed because it is typically used in video coding and provides an effective use of chrominance information for modeling the human skin color. We model the skin-color distribution as a bivariate normal distribution in the CbCr plane. The performance of the algorithm is illustrated by simulations carried out on images depicting people of different ethnicity. Then Convolutional Neural Network (CNN) is used to extract features from the images and Deep Learning Method is used to train a classifier to recognize Sign Language.,https://ieeexplore.ieee.org/document/8650524/,TENCON 2018 - 2018 IEEE Region 10 Conference,28-31 Oct. 2018,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICASSP40776.2020.9053215,Real-Time Binaural Speech Separation with Preserved Spatial Cues,IEEE,Conferences,"Deep learning speech separation algorithms have achieved great success in improving the quality and intelligibility of separated speech from mixed audio. Most previous methods focused on generating a single-channel output for each of the target speakers, hence discarding the spatial cues needed for the localization of sound sources in space. However, preserving the spatial information is important in many applications that aim to accurately render the acoustic scene such as in hearing aids and augmented reality (AR). Here, we propose a speech separation algorithm that preserves the interaural cues of separated sound sources and can be implemented with low latency and high fidelity, therefore enabling a real-time modification of the acoustic scene. Based on the time-domain audio separation network (TasNet), a single-channel time-domain speech separation system that can be implemented in real-time, we propose a multi-input-multi-output (MIMO) end-to-end extension of TasNet that takes binaural mixed audio as input and simultaneously separates target speakers in both channels. Experimental results show that the proposed end-to-end MIMO system is able to significantly improve the separation performance and keep the perceived location of the modified sources intact in various acoustic scenes.",https://ieeexplore.ieee.org/document/9053215/,"ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",4-8 May 2020,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CISIS.2012.213,Real-Time Classification of Sports Movement Using Adaptive Clustering,IEEE,Conferences,"Computer-based instructional systems provide an ideal setting for learning certain types of sports. In particular, the sports that require premium space could leverage the widely available computing and Internet facilities to teach individual users anywhere and anytime. An e-learning tennis instruction system is currently being designed and developed. The Nintendo Wii Remote is selected as the input device for its low cost and racket-handle like shape. After the data from motion sensors are captured, they have to be cleansed, normalised clustered and classified. Data of three common swings, backhand, forehand, and overhand, have been recorded from fifty people of various levels of tennis skill. Experiments are carried out to identify the most suitable techniques to classify a tennis swing. The adaptive nature of a prototype system is also introduced.",https://ieeexplore.ieee.org/document/6245591/,"2012 Sixth International Conference on Complex, Intelligent, and Software Intensive Systems",4-6 July 2012,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICTAI.2008.143,Real-Time Classification of Streaming Sensor Data,IEEE,Conferences,"The last decade has seen a huge interest in classification of time series. Most of this work assumes that the data resides in main memory and is processed offline. However, recent advances in sensor technologies require resource-efficient algorithms that can be implemented directly on the sensors as real-time algorithms. We show how a recently introduced framework for time series classification, time series bitmaps, can be implemented as efficient classifiers which can be updated in constant time and space in the face of very high data arrival rates. We describe results from a case study of an important entomological problem, and further demonstrate the generality of our ideas with an example from robotics.",https://ieeexplore.ieee.org/document/4669683/,2008 20th IEEE International Conference on Tools with Artificial Intelligence,3-5 Nov. 2008,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/EDGE.2018.00025,Real-Time Human Detection as an Edge Service Enabled by a Lightweight CNN,IEEE,Conferences,"Edge computing allows more computing tasks to take place on the decentralized nodes at the edge of networks. Today many delay sensitive, mission-critical applications can leverage these edge devices to reduce the time delay or even to enable real-time, online decision making thanks to their onsite presence. Human objects detection, behavior recognition and prediction in smart surveillance fall into that category, where a transition of a huge volume of video streaming data can take valuable time and place heavy pressure on communication networks. It is widely recognized that video processing and object detection are computing intensive and too expensive to be handled by resource-limited edge devices. Inspired by the depthwise separable convolution and Single Shot Multi-Box Detector (SSD), a lightweight Convolutional Neural Network (L-CNN) is introduced in this paper. By narrowing down the classifier's searching space to focus on human objects in surveillance video frames, the proposed L-CNN algorithm is able to detect pedestrians with an affordable computation workload to an edge device. A prototype has been implemented on an edge node (Raspberry PI 3) using openCV libraries, and satisfactory performance is achieved using real-world surveillance video streams. The experimental study has validated the design of L-CNN and shown it is a promising approach to computing intensive applications at the edge.",https://ieeexplore.ieee.org/document/8473387/,2018 IEEE International Conference on Edge Computing (EDGE),2-7 July 2018,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CHASE.2016.72,Real-Time Tidal Volume Estimation Using Iso-surface Reconstruction,IEEE,Conferences,"Breathing volume measurement has long been an important physiological indication widely used for the diagnosis and treatment of pulmonary diseases. However, most of existing breathing volume monitoring techniques require either physical contact with the patient or are prohibitively expensive. In this paper we present an automated and inexpensive non-contact, vision-based method for monitoring an individual's tidal volume, which is extracted from a three-dimensional (3D) chest surface reconstruction from a single depth camera. In particular, formulating the respiration monitoring process as a 3D space-time volumetric representation, we introduce a real-time surface reconstruction algorithm to generate omni-direction deformation states of a patient's chest while breathing, which reflects the change in tidal volume over time. These deformation states are then used to estimate breathing volume through a per-patient correlation metric acquired through a Bayesian-network learning process. Through prototyping and implementation, our results indicate that we have achieved 92.2% to 94.19% accuracy in the tidal volume estimations through the experimentation based on the proposed vision-based method.",https://ieeexplore.ieee.org/document/7545835/,"2016 IEEE First International Conference on Connected Health: Applications, Systems and Engineering Technologies (CHASE)",27-29 June 2016,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/DASC/PiCom/DataCom/CyberSciTec.2018.000-6,Real-Time Traffic Estimation of Unmonitored Roads,IEEE,Conferences,"Due to growing cities, the real-time knowledge about the state of traffic is a critical problem for urban mobility and the road-segments traffic densities estimation influences the efficiency of fundamental smart city services as smart routing, smart planning for evacuations, planning of civil works on the city, etc. Nevertheless, the traffic-related data from navigator Apps (e.g., TomTom, Google, Bing) are too expensive to be acquired. Also, the traditional sensors for the traffic flow detection are very expensive, and they are usually not dense enough for a correct traffic monitoring. In order to overcome such problems, there is a space for low cost and fast solutions for dense traffic flow reconstruction. We propose a real-time visual self-adaptive solution to reconstruct the traffic density at every location of a wide urban area leveraging the detections from a few fixed traffic sensors deployed within the area of interest. A such method is based on fluid dynamic models to simulate macroscopic phenomena as shocks formation and propagation of waves backwards along roads. Such physical constraints are applied to a detailed street graph which is enriched by specific parameters representing a weight in terms of traffic road capacity. The weight's assignment has been estimated by a stochastic learning approach at each time slot of the day. The accuracy of the proposed model comes from the error between the reconstructed traffic density and the measured values at the sensor position by excluding each sensor iteratively and reconstructing the flow without it. The proposed reconstruction model has been created by exploiting open and real-time data in the context of Sii-Mobility research project by using Km4City infrastructure in the area of Florence, Italy, for its corresponding Smart City solution.",https://ieeexplore.ieee.org/document/8512000/,"2018 IEEE 16th Intl Conf on Dependable, Autonomic and Secure Computing, 16th Intl Conf on Pervasive Intelligence and Computing, 4th Intl Conf on Big Data Intelligence and Computing and Cyber Science and Technology Congress(DASC/PiCom/DataCom/CyberSciTech)",12-15 Aug. 2018,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SoutheastCon42311.2019.9020591,Real-Time Traffic Incidence dataset,IEEE,Conferences,"This paper focuses on developing (i) a benchmark dataset for identification of traffic incidences, (ii) a congestion aware navigation application which uses this dataset for real-time detection and classification of traffic incidents, (iii) the System Level Software (or Middleware) required for Distributed Computing in such a system with Rapid Mobility, and (iv) a hardware prototype of the distributed computing and storage infrastructure. The video bandwidth requirement of 10-100 GigaBytes of data per minute per vehicular camera makes it a Big Data problem. With millions of smart vehicles predicted to be deployed within the next 5 years, BigData from a single vehicle, multiplied with the large number of vehicles, presents a Big-Squared-Data computing space which will easily overwhelm any Cloud infrastructure with its Real-Time or near Real-Time demands. Hence the need for a Fog tier between the Edge nodes and the Cloud to bring distributed computation (servers) and storage closer to the Edge nodes. Such a Fog consists of multiple Fog instances, each one of which services cells or Virtual Clusters of Edge nodes. Results show that Fog-Cloud computing framework outperforms a Cloud-only platform by 55.8&#x0025; reduction in total latency or response time.",https://ieeexplore.ieee.org/document/9020591/,2019 SoutheastCon,11-14 April 2019,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IGARSS47720.2021.9554141,"Real-Time, Deep Synthetic Aperture Sonar (SAS) Autofocus",IEEE,Conferences,"Synthetic aperture sonar (SAS) requires precise time-of-flight measurements of the transmitted/received waveform to produce well-focused imagery. It is not uncommon for errors in these measurements to be present resulting in image defocusing. To overcome this, an autofocus algorithm is employed as a post-processing step after image reconstruction to improve image focus. A particular class of these algorithms can be framed as a sharpness/contrast metric-based optimization. To improve convergence, a hand-crafted weighting function to remove &#x201C;bad&#x201D; areas of the image is sometimes applied to the image-under-test before the optimization procedure. Additionally, dozens of iterations are necessary for convergence which is a large compute burden for low size, weight, and power (SWaP) systems. We propose a deep learning technique to overcome these limitations and implicitly learn the weighting function in a data-driven manner. Our proposed method, which we call Deep Autofocus, uses features from the single-look-complex (SLC) to estimate the phase correction which is applied in k-space. Furthermore, we train our algorithm on batches of training imagery so that during deployment, only a single iteration of our method is sufficient to autofocus. We show results demonstrating the robustness of our technique by comparing our results to four commonly used image sharpness metrics. Our results demonstrate Deep Autofocus can produce imagery perceptually better than common iterative techniques but at a lower computational cost. We conclude that Deep Autofocus can provide a more favorable cost-quality tradeoff than alternatives with significant potential of future research.",https://ieeexplore.ieee.org/document/9554141/,2021 IEEE International Geoscience and Remote Sensing Symposium IGARSS,11-16 July 2021,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IJCNN.1992.227307,Real-time 2D analog motion detector VLSI circuit,IEEE,Conferences,"An analog VLSI implementation of real-time motion detection in early vision using a hybrid of the Reichardt and Ullman-Marr models has been fabricated. It has a 5*5 array of photoreceptors, with a receptor fill factor of 0.42. The majority of this circuit operates in strong inversion. The photosensors have squared logarithmic and linear responses for low and high intensity illumination, respectively. A linear passive resistive grid is used to implement the desired difference of Gaussian impulse response transfer function of the edge detection circuit whose dynamic range is 3 V. These outputs are used to determine the velocities of the translating objects. Velocities up to 2 m/s on chip corresponding to 6 m/s in space have been measured in 2D. The array has been fabricated in a 2 mu m n-well CMOS process.<>",https://ieeexplore.ieee.org/document/227307/,[Proceedings 1992] IJCNN International Joint Conference on Neural Networks,7-11 June 1992,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICISS49785.2020.9315906,Real-time Monitoring and Comprehensive Analysis Framework for Rainwater and Soil Pollution based on Big Data Feature Extraction and IoT,IEEE,Conferences,"Real-time monitoring and comprehensive analysis framework for rainwater and soil pollution based on big data feature extraction and Internet of Things (IoT) is discussed in this article. The procedural operation technology of the dispatch automation system is proposed, and introduces the overall structure and implementation plan of the dispatch automation system. The novelty is summarized into the following core aspects. (1) By representing the reconstructed training samples with low rank, a robust discriminant feature space constraint term was designed to enable the learning of feature subspace to improve the data analytic effectiveness. (2) For the forward propagation and backward propagation, the direction of the traversal and the operations that must be performed are different. Based on this, the novel real-time analysis model is designed. The proposed model is simulated on the hardware devices. The results have shown that the proposed method is efficient.",https://ieeexplore.ieee.org/document/9315906/,2020 3rd International Conference on Intelligent Sustainable Systems (ICISS),3-5 Dec. 2020,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICARM52023.2021.9536118,Real-time Monocular 3D People Localization and Tracking on Embedded System,IEEE,Conferences,"Localizing people in 3D space, rather than in original 2D image plane, provides a more comprehensive understanding of the scene and brings up more potential applications. However, inferring 3D locations usually requires stereo camera or additional sensors since deriving depth information from single image is regarded as an ill-posed problem. With recent progress in deep learning methods, depth estimation neural network can provide convincing depth map by a single RGB image. This work develops a people localization and tracking method based on a monocular camera. Specifically, an efficient self-supervised monocular depth estimation method is adopted to generate pseudo depth map. Afterwards, 2D object detection results are adopted for finding accurate people location. Finally, a filter based tracking method is adopted to fuse temporal information and improve the accuracy. Aiming to provide a real time solution for people tracking on embedded system, our methods are deployed and tested on a NVIDIA Jetson Xavier NX develop kit. The proposed efficient localization and tracking method is validated by a group of field tests. The overall performance reaches 12 fps with an acceptable accuracy compared to ground truth.",https://ieeexplore.ieee.org/document/9536118/,2021 6th IEEE International Conference on Advanced Robotics and Mechatronics (ICARM),3-5 July 2021,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICPR48806.2021.9412741,Real-time Pedestrian Lane Detection for Assistive Navigation using Neural Architecture Search,IEEE,Conferences,"Pedestrian lane detection is a core component in many assistive and autonomous navigation systems. These systems are usually deployed in environments that require realtime processing. Many state-of-the-art deep neural networks only focus on detection accuracy but not inference speed. Without further modifications, they are not suitable for real-time applications. Furthermore, the task of designing a high-performing deep neural network is time-consuming and requires experience. To tackle these issues, we propose a neural architecture search algorithm that can find the best deep network for pedestrian lane detection automatically. The proposed method searches in a network-level space using the gradient descent algorithm. Evaluated on a dataset of 5,000 images, the deep network found by the proposed algorithm achieves comparable segmentation accuracy, while being significantly faster than other state-of-the-art methods. The proposed method has been successfully implemented as a real-time pedestrian lane detection tool.",https://ieeexplore.ieee.org/document/9412741/,2020 25th International Conference on Pattern Recognition (ICPR),10-15 Jan. 2021,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICMLC.2003.1260131,Real-time face detection based on skin-color model and morphology filters,IEEE,Conferences,"This paper presents a new real-time face detection method that uses the skin color model in the YCrCb chrominance space to remove the non-skin-color pixels from the image from which we then extract candidate human face regions. We use the mathematical morphological filter to remove noisy regions and fill holes in the skin-color region. To locate the face region, we compute the similarity between the human face features and the candidate face regions in the image. We have implemented this algorithm in our smart media systems and found it effective in a real environment.",https://ieeexplore.ieee.org/document/1260131/,Proceedings of the 2003 International Conference on Machine Learning and Cybernetics (IEEE Cat. No.03EX693),5-5 Nov. 2003,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICNN.1996.549131,Real-time image restoration with an artificial neural network,IEEE,Conferences,"We present a neural network that can be applied to image correction in a preprocessing unit. Blur, geometric distortion and unequal brightness distribution are typical for many scanning techniques and can lead to difficulties during further processing of an image. These and other effects of image degradation, the space-variant can be considered simultaneously by this approach. In order to calibrate the correcting system the weights of a neural network are trained. Using suitable training patterns and an appropriate optimization criterion for the degraded images, the dimensioned network represents a space-variant filter with a behavior similar to the well-known Wiener filter. The restoration result can be easily altered by the scheme of the learning data generation. Theoretical considerations and examples for 1D, 2D and 3D implementations in both software and hardware are given.",https://ieeexplore.ieee.org/document/549131/,Proceedings of International Conference on Neural Networks (ICNN'96),3-6 June 1996,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ROBOT.1993.291973,Real-time implementation of neural network learning control of a flexible Space manipulator,IEEE,Conferences,"A neural network approach to online learning control and real-time implementation for a flexible space robot manipulator is presented. An overview of the motivation and system development of the self-mobile space modulator (SM/sup 2/) is given. The neural network learns control by updating feedforward dynamics based on feedback control input. Implementation issues associated with online training strategies are addressed and a single stochastic training scheme is presented. A recurrent neural network architecture with improved performance is proposed. Using the proposed learning scheme, the manipulator tracking error is reduced by 85% compared to that of conventional proportional-integral-derivative (PID) control. The approach possesses a high degree of generality and adaptability to various applications. It will be a valuable learning control method for robots working in unconstructed environments.<>",https://ieeexplore.ieee.org/document/291973/,[1993] Proceedings IEEE International Conference on Robotics and Automation,2-6 May 1993,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/VR.2001.913773,Real-time input of 3D pose and gestures of a user's hand and its applications for HCI,IEEE,Conferences,"Introduces a method for tracking a user's hand in 3D and recognizing the hand's gesture in real time without the use of any invasive devices attached to the hand. Our method uses multiple cameras for determining the position and orientation of a user's hand moving freely in a 3D space. In addition, the method identifies pre-determined gestures in a fast and robust manner by using a neural network which has been properly trained beforehand. This paper also describes results of user study of our proposed method and several types of applications, including 3D object handling for a desktop system and a 3D walkthrough for a large immersive display system.",https://ieeexplore.ieee.org/document/913773/,Proceedings IEEE Virtual Reality 2001,13-17 March 2001,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1145/3061639.3062307,Real-time meets approximate computing: An elastic CNN inference accelerator with adaptive trade-off between QoS and QoR,IEEE,Conferences,"Due to the recent progress in deep learning and neural acceleration architectures, specialized deep neural network or convolutional neural network (CNNs) accelerators are expected to provide an energy-efficient solution for real-time vision/speech processing, recognition and a wide spectrum of approximate computing applications. In addition to their wide applicability scope, we also found that the fascinating feature of deterministic performance and high energy-efficiency, makes such deep learning (DL) accelerators ideal candidates as application-processor IPs in embedded SoCs concerned with real-time processing. However, unlike traditional accelerator designs, DL accelerators introduce a new aspect of design trade-off between real-time processing (QoS) and computation approximation (QoR) into embedded systems. This work proposes an elastic CNN acceleration architecture that automatically adapts to the hard QoS constraint by exploiting the error-resilience in typical approximate computing workloads. For the first time, the proposed design, including network tuning-and-mapping software and reconfigurable accelerator hardware, aims to reconcile the design constraint of QoS and Quality of Result (QoR), which are respectively the key concerns in real-time and approximate computing. It is shown in experiments that the proposed architecture enables the embedded system to work flexibly in an expanded operating space, significantly enhances its real-time ability, and maximizes the energy-efficiency of system within the user-specified QoS-QoR constraint through self-reconfiguration.",https://ieeexplore.ieee.org/document/8060406/,2017 54th ACM/EDAC/IEEE Design Automation Conference (DAC),18-22 June 2017,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICAR.1997.620222,Real-time navigation of a mobile robot using Kohonen's topology conserving neural network,IEEE,Conferences,"This paper proposes a real-time sensor based navigation method using Kohonen's topology conserving network for navigation of a mobile robot in any uncertain environment. The sensory information including target location with respect to current location of the mobile robot, have been discretely conserved using a two dimensional Kohonen lattice. Reinforcement learning based on a stochastic real valued technique have been implemented to compute the action space for this Kohonen lattice. The proposed scheme learns the input and output weight space of the Kohonen lattice which is generalized to any workspace. The effectiveness of the proposed scheme has been established by simulation where the complete domain of the input-space is quantized based on experience on sensory data encountered in real-time. The input-output mapping conserved by the Kohonen lattice during simulation was used to guide a mobile robot in a real-time environment. Successful navigation of the mobile robot without further training confirms the robustness of the proposed scheme.",https://ieeexplore.ieee.org/document/620222/,1997 8th International Conference on Advanced Robotics. Proceedings. ICAR'97,7-9 July 1997,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/PIC.2010.5687433,Real-time pedestrian tracking by visual attention and human knowledge learning,IEEE,Conferences,"In this paper, a novel model of pedestrian tracking by using object-based attention and human knowledge is presented. The selective units in the system are the objects and groupings which are space-driven as well as feature-driven. The factors of speed, motion direction and spatial location are used to cluster and form the groupings. Hierarchical selectivity of attention for objects in a grouping is implemented under the guide of human model knowledge with the help of a head detector. The motion cues are utilized to tackle the multi-person tracking through hierarchical selection of attention. The experimental results from outdoor environments are reported.",https://ieeexplore.ieee.org/document/5687433/,2010 IEEE International Conference on Progress in Informatics and Computing,10-12 Dec. 2010,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISCAS.2016.7539214,Real-time sensory information processing using the TrueNorth Neurosynaptic System,IEEE,Conferences,"Summary form only given. The IBM TrueNorth (TN) Neurosynaptic System, is a chip multi processor with a tightly coupled processor/memory architecture, that results in energy efficient neurocomputing and it is a significant milestone to over 30 years of neuromorphic engineering! It comprises of 4096 cores each core with 65K of local memory (6T SRAM)-synapses- and 256 arithmetic logic units - neurons-that operate on a unary number representation and compute by counting up to a maximum of 19 bits. The cores are event-driven using custom asynchronous and synchronous logic, and they are globally connected through an asynchronous packet switched mesh network on chip (NOC). The chip development board, includes a Zyng Xilinx FPGA that does the housekeeping and provides support for standard communication support through an Ethernet UDP interface. The asynchronous Addressed Event Representation (AER) in the NOC is al so exposed to the user for connection to AER based peripherals through a packet with bundled data full duplex interface. The unary data values represented on the system buses can take on a wide variety of spatial and temporal encoding schemes. Pulse density coding (the number of events Ne represents a number N), thermometer coding, time-slot encoding, and stochastic encoding are examples. Additional low level interfaces are available for communicating directly with the TrueNorth chip to aid programming and parameter setting. A hierarchical, compositional programming language, Corelet, is available to aid the development of TN applications. IBM provides support and a development system as well as “Compass” a scalable simulator. The software environment runs under standard Linux installations (Red Hat, CentOS and Ubuntu) and has standard interfaces to Matlab and to Caffe that is employed to train deep neural network models. The TN architecture can be interfaced using native AER to a number of bio-inspired sensory devices developed over many years of neuromorphic engineering (silicon retinas and silicon cochleas). In addition the architecture is well suited for implementing deep neural networks with many applications in computer vision, speech recognition and language processing. In a sensory information processing system architecture one desires both pattern processing in space and time to extract features in symbolic sub-spaces as well as natural language processing to provide contextual and semantic information in the form of priors. In this paper we discuss results from ongoing experimental work on real-time sensory information processing using the TN architecture in three different areas (i) spatial pattern processing -computer vision(ii) temporal pattern processing -speech processing and recognition(iii) natural language processing -word similarity-. A real-time demonstration will be done at ISCAS 2016 using the TN system and neuromorphic event based sensors for audition (silicon cochlea) and vision (silicon retina).",https://ieeexplore.ieee.org/document/7539214/,2016 IEEE International Symposium on Circuits and Systems (ISCAS),22-25 May 2016,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICUAS.2016.7502588,Real-time unmanned aerial vehicle 3D environment exploration in a mixed reality environment,IEEE,Conferences,"This paper presents a novel human robot interaction system that can be used for real-time 3D environment exploration with an unmanned aerial vehicle (UAV). The method creates a mixed reality environment, in which a user can interactively control a UAV and visualize the exploration data in real-time. The method uses a combination of affordable sensors, and transforms the control and viewing space from the UAV to the controller's perspective. Different hardware and software configurations are studied so that the system can be adjusted to meet different needs and environments. A prototype system is presented and test results are discussed.",https://ieeexplore.ieee.org/document/7502588/,2016 International Conference on Unmanned Aircraft Systems (ICUAS),7-10 June 2016,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/NEUREL.2008.4685618,Realization of control of pneumatic system for positioning of nozzle based on fuzzy logic,IEEE,Conferences,This paper presents control of nozzle position in three dimension space using pneumatic muscles as actuators. Adequate fuzzy algorithm is designed using Matlab Fuzzy Inference System and implemented using PLC (Programmable Logic Controller) unit and proportional pressure regulators. Algorithm is tested on real model.,https://ieeexplore.ieee.org/document/4685618/,2008 9th Symposium on Neural Network Applications in Electrical Engineering,25-27 Sept. 2008,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICASSP.2008.4517901,Realtime detection of salient moving object: A multi-core solution,IEEE,Conferences,"Detection of salient moving object has great potentials in activity recognition, scene understanding, etc. However techniques to characterizing the object in fine granularity have not been well developed in real applications due to the computational intensity. The emerging multi-core technology in hardware design provides an opportunity for the compute intensive algorithms to boost speed in parallel. This paper proposed a scalable approach to detecting salient moving object which is designed inherently for parallelization. To characterize the object in fine granularity, we extract color-texture homogenous regions as the basic processing unit by image segmentation. To identify salient object, we generate probabilistic template by learning the space-time context. The parallel algorithm is implemented using OpenMP. Evaluations have been carried out on sports, news, and home video data. For the CIF size image, we get processing speed of 51.1 frames per second and near linear speed up on an eight-core machine. It indicates that the algorithm parallelization is a promising solution for practical applications in the multimedia field.",https://ieeexplore.ieee.org/document/4517901/,"2008 IEEE International Conference on Acoustics, Speech and Signal Processing",31 March-4 April 2008,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/PHM2022-London52454.2022.00042,Recent Research and Applications in Variational Autoencoders for Industrial Prognosis and Health Management: A Survey,IEEE,Conferences,"Whether in the industrial, medical, or real-world domains, more and more data are being collected. The common particularity of all these application domains is that a great part of this data is mostly unlabeled. Thus, designing a learning model with a minimum of labeled data represents a major challenge in the coming years. A particular emphasis has recently been put on unsupervised learning methods based on the idea of autoencoding. The objective of these methods is twofold: to reduce the dimensionality of the input space and to reconstruct the original observation from this lower dimensional representation space. The variational form of these autoencoders, called the Variational Autoencoders (VAEs), is particularly successful in almost all application areas. This enthusiasm comes from the fact that VAEs allow to take advantage of the theoretical foundations of the Variational Bayesian methods and the learning capabilities of artificial neural networks. This review paper gives to the PHM community a synthesis of the latest publications in the PHM domain using the VAEs related to four topics: 1) Data-Driven Soft Sensors for missing values and data outliers, 2) reconstruction error for fault detection, 3) resampling approach for imbalanced data generation and minority class and 4) the variational embedding as PHM preprocessing pipelines and data transformations. After a review of the theoretical foundations and some practical tricks to succeed the implementation of the VAEs in industrial applications, the four main topics used to exploit the VAEs in the PHM domain are detailed. Finally, a global view of the research done at the research institute of Hydro-Qu&#x00E9;bec regarding the diagnosis and failure detection of hydro-generators with VAEs are presented.",https://ieeexplore.ieee.org/document/9808794/,2022 Prognostics and Health Management Conference (PHM-2022 London),27-29 May 2022,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/TPEC54980.2022.9750805,Reconfiguring Unbalanced Distribution Networks using Reinforcement Learning over Graphs,IEEE,Conferences,"The recent trend in distribution system intelligence necessitates the deployment of real-time, automated, and adaptable decision-making tools. Reconfiguring the distribution network by changing the status of switches can aid in loss minimization during normal operations and resilience enhancement during disruptive events. Traditional methods employed for solving the network reconfiguration problem are model-based and scenario-specific. Besides this, the scalability and computational efficiency also limit the utilization of such techniques for online control, which could be potentially addressed by neural network based models trained with reinforcement learning (RL). To this end, we formulate the reconfiguration problem as a Markov Decision Process where the optimal control policy is learned using the RL approach. Considering the relevance of topology in decision making and the interaction between the generation and demand at different buses, we model the power distribution network along with its state variables as a graph in the learning space. Consequently, we propose an RL over graphs where a Capsule-based graph neural network is used as the policy network. The developed model is validated on the modified IEEE 13 and 34 bus test networks.",https://ieeexplore.ieee.org/document/9750805/,2022 IEEE Texas Power and Energy Conference (TPEC),28 Feb.-1 March 2022,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AERO47225.2020.9172719,Recurrent Neural Network Based Prediction to Enhance Satellite Telemetry Compression,IEEE,Conferences,"Because of the gradually increasing number of remote measured low and/or high frequency sampled parameters in space applications, aerospace mission operators have to make hard choices on which parameters at which sampling rates should be downlinked. On-board aerospace applications are characterized by limited storage and communication budgets, while lossless data compression schemes should be sufficient enough to enhance transmission efficiency and hence the whole aerospace mission. In this paper, a proposed two-stage lossless compression method for telemetry data is presented. The proposed method consists of a decorrelation stage and an entropy coding one. The Long-Short-Term Memory (LSTM) Recurrent Neural Network (RNN) is implemented as a predictor in the decorrelation stage of the proposed method, and an illustrative method of applying LSTM network for telemetry data samples prediction is presented and figured out. In experiments, different entropy coders: Rice codes, arithmetic method and Huffman algorithm are separately implemented at the second stage. The proposed method is tested by different real telemetry data sets of FUNcube satellite in frames of data words of 8-,10-,16-bits widths. Experimental results show that the proposed method improved compression efficiency based on a single stage of entropy coder: Rice codes, arithmetic code, and Huffman algorithm by a ratio up to: 98%, 21%, and 1.6%, respectively.",https://ieeexplore.ieee.org/document/9172719/,2020 IEEE Aerospace Conference,7-14 March 2020,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICARM52023.2021.9536145,Reducing the Dimension of the Configuration Space with Self Organizing Neural Networks,IEEE,Conferences,"For robotics, especially industrial applications, it is crucial to reactively plan safe motions through efficient algorithms. Planning is more powerful in the configuration space than the task space. However, for robots with many degrees of freedom, this is challenging and computationally expensive. Sophisticated techniques for motion planning such as the Wavefront algorithm are limited by the high dimensionality of the configuration space, especially for robots with many degrees of freedom. For a neural implementation of the Wavefront algorithm in the configuration space, neurons represent discrete configurations and synapses are used for path planning. In order to decrease the complexity, we reduce the search space by pruning superfluous neurons and synapses. We present different models of self-organizing neural networks for this reduction. The approach takes real-life human motion data as input and creates a representation with reduced dimension. We compare six different neural network models and adapt the Wavefront algorithm to the different structures of the reduced output spaces. The method is backed up by an extensive evaluation of the reduced spaces, including their suitability for path planning by the Wavefront algorithm.",https://ieeexplore.ieee.org/document/9536145/,2021 6th IEEE International Conference on Advanced Robotics and Mechatronics (ICARM),3-5 July 2021,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICSME46990.2020.00074,Regression Testing of Massively Multiplayer Online Role-Playing Games,IEEE,Conferences,"Regression testing aims to check the functionality consistency during software evolution. Although general regression testing has been extensively studied, regression testing in the context of video games, especially Massively Multiplayer Online Role-Playing Games (MMORPGs), is largely untouched so far. One big challenge is that game testing requires a certain level of intelligence in generating suitable action sequences among the huge search space, to accomplish complex tasks in the MMORPG. Existing game testing mainly relies on either the manual playing or manual scripting, which are labor-intensive and time-consuming. Even worse, it is often unable to satisfy the frequent industrial game evolution. The recent process in machine learning brings new opportunities for automatic game playing and testing. In this paper, we propose a reinforcement learning-based regression testing technique that explores differential behaviors between multiple versions of an MMORPGs such that the potential regression bugs could be detected. The preliminary evaluation on real industrial MMORPGs demonstrates the promising of our technique.",https://ieeexplore.ieee.org/document/9240641/,2020 IEEE International Conference on Software Maintenance and Evolution (ICSME),28 Sept.-2 Oct. 2020,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SSCI47803.2020.9308514,Regression learning on patches,IEEE,Conferences,"Neural networks often do poorly at representing dis-continuous functions, or even just functions with rapid transitions in the response surface between closely-spaced points in feature space. However, such `edges' in the data can be a useful way to partition the feature space in order to train specialised learners for individual regions. This is particularly beneficial where these regions are relatively simple, and hence low-complexity learners can be used successfully on them. Another benefit of such an approach is that it is easily parallelisable: the specialised learners use independent partitions of the data, and so they can be trained in parallel, while output prediction is based on the output of just one network, so there is no need to combine predictions. We introduce an algorithm to partition the data that is inspired by Finite Element Tearing and Interconnecting. Using an implementation based on a decision tree with neural networks at the leaves, we demonstrate our approach for regression learning on patches of the feature space. We use both artificial and real-world datasets to show that, in some use cases, this method can outperform conventional neural networks that see the entire feature set in the original training.",https://ieeexplore.ieee.org/document/9308514/,2020 IEEE Symposium Series on Computational Intelligence (SSCI),1-4 Dec. 2020,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISGTAsia49270.2021.9715603,Reinforcement Learning Based EV Charging Scheduling: A Novel Action Space Representation,IEEE,Conferences,"In recent years, several optimization techniques have been proposed for electric vehicle (EV) charging scheduling. A common approach to intelligent scheduling is day-ahead planning, assuming full arrival time, departure time and energy demand knowledge or having them forecasted. However, the result from the day-ahead scheduling is limitedly applicable due to the uncertainties from the charging behaviors. With the deployment of the EV charging communication protocol defined in ISO 15118, it is realistic to assume that the EV will publish the departure time and the energy demand upon arrival. Thus, real-time scheduling, making decisions at each decision timeslot, can adapt to the new information and increase scheduling performance. Traditional model-based approaches like model predictive control (MPC) still require models, for example, for the future arrival times to solve the scheduling problem. Reinforcement learning (RL), a model-free approach, has also been successfully applied to real-time scheduling. RL can learn how to make decisions without relying on any system knowledge. This paper proposes a new action space construction method for an RL as proposed in a preceding work. The resulting action space size is significantly reduced compared to the original approach. Further, we compare the performance of a novel prioritized RL method to the original method. A publicly available charging session dataset is used for performance comparison in contrast to the original method. It is shown, that the prioritized RL performs better.",https://ieeexplore.ieee.org/document/9715603/,2021 IEEE PES Innovative Smart Grid Technologies - Asia (ISGT Asia),5-8 Dec. 2021,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CASE48305.2020.9249227,Reinforcement Learning with Converging Goal Space and Binary Reward Function,IEEE,Conferences,"Usage of a sparse and binary reward function remains one of the most challenging problems in reinforcement learning. In particular, when the environments wherein robotic agents learn are sufficiently vast, it is much more difficult to learn tasks because the probability of reaching the goal is minimal. A Hindsight Experience Replay algorithm was proposed to overcome these difficulties; however, problems persist that affect the learning speed and delay learning when a learning agent cannot receive proper rewards at the beginning of the learning process. In this paper, we present a simple method called Converging Goal Space and Binary Reward Function, which helps agents learn tasks easily and efficiently in large environments while providing a binary reward. At an early stage in training, a larger goal space margin facilitates the reward function for a more rapid policy learning. As the number of successes increases, the goal space is gradually reduced to the size used to the size used in the test. We apply this reward function to two different task experiments: sliding and throwing, which must be explored at a wider range than the reach of the robotic arms, and then compare the learning efficiency to that of experiments that only employ a sparse and binary reward function. We show that the proposed reward function performs better in large environments using physics simulation, and we demonstrate that the function is applicable to real world robotic arms.",https://ieeexplore.ieee.org/document/9249227/,2020 IEEE 16th International Conference on Automation Science and Engineering (CASE),20-21 Aug. 2020,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISCID.2018.10171,Representation Learning for Knowledge Graph with Dynamic Margin,IEEE,Conferences,"Representation learning aims to embed knowledge graphs into a low-dimensional, dense, and real-valued vector space. Existing methods such as TransE and TransH make use of a global loss function based on margin to learn embedding representation, but the margin in the loss function is fixed during the training process. Since the margin is used as optimal distance to distinguish valid and invalid triples, it is reasonable to permit the margin to evolve during the training process. Based on this idea, a dynamic margin (DM) translation principle is proposed in this paper, and by introducing this principle into the classical TransE model, a knowledge graph embedding model named TransE-DM is presented. Compared with the TransE model, a feature of TransE-DM is that the loss function can evolve during the training process. Experimental results show that TransE-DM has a certain improvement on the link prediction task.",https://ieeexplore.ieee.org/document/8695607/,2018 11th International Symposium on Computational Intelligence and Design (ISCID),8-9 Dec. 2018,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICORR.2017.8009451,Representing high-dimensional data to intelligent prostheses and other wearable assistive robots: A first comparison of tile coding and selective Kanerva coding,IEEE,Conferences,"Prosthetic devices have advanced in their capabilities and in the number and type of sensors included in their design. As the space of sensorimotor data available to a conventional or machine learning prosthetic control system increases in dimensionality and complexity, it becomes increasingly important that this data be represented in a useful and computationally efficient way. Well structured sensory data allows prosthetic control systems to make informed, appropriate control decisions. In this study, we explore the impact that increased sensorimotor information has on current machine learning prosthetic control approaches. Specifically, we examine the effect that high-dimensional sensory data has on the computation time and prediction performance of a true-online temporal-difference learning prediction method as embedded within a resource-limited upper-limb prosthesis control system. We present results comparing tile coding, the dominant linear representation for real-time prosthetic machine learning, with a newly proposed modification to Kanerva coding that we call selective Kanerva coding. In addition to showing promising results for selective Kanerva coding, our results confirm potential limitations to tile coding as the number of sensory input dimensions increases. To our knowledge, this study is the first to explicitly examine representations for realtime machine learning prosthetic devices in general terms. This work therefore provides an important step towards forming an efficient prosthesis-eye view of the world, wherein prompt and accurate representations of high-dimensional data may be provided to machine learning control systems within artificial limbs and other assistive rehabilitation technologies.",https://ieeexplore.ieee.org/document/8009451/,2017 International Conference on Rehabilitation Robotics (ICORR),17-20 July 2017,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICIBA52610.2021.9688007,Research and Implementation of User Behavior Simulation Technology Based on Power Industry Cyber Range,IEEE,Conferences,"The current cyber range of the electric power industry has background traffic simulation problems, prospect behavior generation problems, and target simulation diversity problems in large-scale network environments. In response to these problems, this paper proposes a user behavior simulation technology suitable for power industry cyber range. We perform large-scale, high-fidelity network user behavior simulation by combining real traffic playback, inject user behavior traffic in the real network into the virtual target network, then model the user behavior in multiple scenarios in the power network space and generate it based on the method of sequence prediction, and finally realize the diversity simulation of user behavior.",https://ieeexplore.ieee.org/document/9688007/,"2021 IEEE 2nd International Conference on Information Technology, Big Data and Artificial Intelligence (ICIBA)",17-19 Dec. 2021,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICMLC.2003.1260027,Research and implementation of a real time approach to lip detection in video sequences,IEEE,Conferences,"Locating the lip in video sequences is one of the primary steps of the automatic lipreading system. In this paper a new approach to lip detection, which is based on Red Exclusion and Fisher transform, is presented. In this approach, firstly, we locate face region with skin-color model and motion correlation, then trisect the face image and take into account the lowest part, in which the lip lies, for the next processing. Secondly, we exclude R-component in RGB color space, then use G-component and B-component as the Fisher transform vector to enhance the lip image. Finally, in the enhanced image, we adaptively set the threshold to separate the lip color and the skin color in the light of the normal distribution of the gray value histogram. The experimental results showed that this fast approach is very efficient in detecting the whole lip and not affected by illuminant and different speakers.",https://ieeexplore.ieee.org/document/1260027/,Proceedings of the 2003 International Conference on Machine Learning and Cybernetics (IEEE Cat. No.03EX693),5-5 Nov. 2003,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICMLC.2003.1260018,"Research and implementation of real-time face detection, tracking and protection",IEEE,Conferences,"Privacy protection in video images is becoming one of the research focuses in the field of remote collaborative system. In this paper, a method for face detection, tracking and privacy protection is presented. According to skin-color distribution in the color space, we developed a statistical skin-color model through interactive sample training. Using this model we convert the color image to binary image and then segment face candidate region. Then we use a facial feature matching scheme for further detection. The presence or absence of a face in each region is verified by means of mouth detector. Real time detection and tracking can be achieved by using this method in video images. In order to speed up tracking, we improve the traditional method by adding motion prediction, which works better when several disturbing objects appear simultaneously. Finally we make the tracking region blurring and transmit the frames to the remote collaborative sites to obtain the privacy protection. The level of privacy protection can be dynamically adjusted according to collaborators' requests and credibility of remote sites. The experiment results show the proposed method not only has high speed and efficiency, but also is robust to head rotation to some extent.",https://ieeexplore.ieee.org/document/1260018/,Proceedings of the 2003 International Conference on Machine Learning and Cybernetics (IEEE Cat. No.03EX693),5-5 Nov. 2003,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IEMBS.2007.4352483,Research on 3D Modeling for Head MRI Image Based on Immune Sphere-Shaped Support Vector Machine,IEEE,Conferences,"In head MRI image sequences, the boundary of each encephalic tissue is highly complicated and irregular. It is a real challenge to traditional 3D modeling algorithms. Support Vector Machine (SVM) based on statistical learning theory has solid theoretical foundation. Sphere-Shaped SVM (SSSVM) was originally developed for solving some special classification problems. In this paper, it is extended to image 3D modeling which tries to find the smallest hypersphere enclosing target data in high dimensional space by kernel function. However, selecting parameter is a complicated problem which directly affects modeling accuracy. Immune Algorithm (IA), mainly applied to optimization, is used to search optimal parameter for SSSVM. So, Immune SSSVM (ISSSVM) is proposed to construct the 3D models for encephalic tissues. As our experiment demonstrates, the models are constructed and reach satisfactory modeling accuracies. Theory and experiment indicate ISSSVM exhibits its great potential in image 3D modeling.",https://ieeexplore.ieee.org/document/4352483/,2007 29th Annual International Conference of the IEEE Engineering in Medicine and Biology Society,22-26 Aug. 2007,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICTC55111.2022.9778652,Research on Autonomous Decision-Making of UCAV Based on Deep Reinforcement Learning,IEEE,Conferences,"In order to improve the intelligence level of training opponents in UCAV air combat simulation and the realism and immersion of air combat simulation in 3D space, this paper proposes a deep reinforcement learning algorithm for UCAV autonomous control based on virtual reality technology. A combination of reinforcement learning and Unity3D is used to train UCAV agents to achieve air combat tasks in 3D virtual reality space, and imitation learning is added to improve the efficiency of policy generation. Multiple perceptrons are used to simplify the agent&#x2019;s acquisition of environmental state data, and reward functions are designed by integrating UCAV angle, speed, and altitude considerations to visualize the entire 3D visualization process of reinforcement learning training UCAV agents to interact with the environment.",https://ieeexplore.ieee.org/document/9778652/,2022 3rd Information Communication Technologies Conference (ICTC),6-8 May 2022,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICDSCA53499.2021.9650252,Research on Behavior Mathematical Modeling of CGF Based on Reinforcement Learning under Complex Electromagnetic Environment,IEEE,Conferences,"This paper focuses on the modeling of agent behavior in complex electromagnetic environment. By using reinforcement learning technology, The traditional Markov decision process and dynamic programming algorithm are improved. The electromagnetic field intensity function is integrated with Markov decision process and dynamic programming algorithm, and high precision virtual battlefield environment is evaluated based on system simulation, in order to facilitate the modeling of the agent’s behavior, this paper meshes the complex electromagnetic battlefield environment and sets up the simulation task scene of agent’s maneuvering task using simulation and evaluation system. Several simulation tests were implemented to collect relative test data. The comparison was made with the traditional Markov decision algorithm from three dimensions, including time, space and working condition of electronic devices inside equipment. The results show that the improved Markov decision algorithm and dynamic programming algorithm proposed in this paper show better adaptability to adapt complex electromagnetic environment than traditional Markov algorithm, it will provide a strong support for efficient completion of combat tasks and improvement of combat effectiveness in complex electromagnetic simulation environment, and also lays a foundation for the subsequent research on the CGF behavior modeling method.",https://ieeexplore.ieee.org/document/9650252/,2021 IEEE International Conference on Data Science and Computer Application (ICDSCA),29-31 Oct. 2021,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AUTEEE52864.2021.9668793,Research on Intelligent Acceleration Algorithm for Big Data Mining in Communication Network Based on Support Vector Machine,IEEE,Conferences,"As a budding technology, big data’s technical implementation and commercial application are in the exploratory stage. With the increasing development of network and communication technology, a large amount of information is pouring in. How to effectively select the required information has become a more and more prominent problem. Data mining is a data processing technology developed to meet this need. Support vector machine is a new technology in data mining. It is a new tool to solve machine learning problems with the help of optimization methods. Among them, it focuses on the support vector machine, including the development history and present situation of support vector machine, the main basic concepts and research contents. On this basis, it studies various training algorithms of support vector machine which are relatively common at present, and compares their advantages and disadvantages. Big data has various data types, forming a data stream with various attributes. As we all know, data source classification based on batch processing can improve the query speed, but it still can’t meet the demand of real-time query. Therefore, feature selection mechanism is usually introduced in the process of data mining modeling to reduce its load. However, when faced with the query of high-dimensional data, the query space grows exponentially, which is difficult to realize. Therefore, this paper proposes the efficiency of an intelligent acceleration algorithm for big data mining based on vector machine communication network.",https://ieeexplore.ieee.org/document/9668793/,"2021 IEEE 4th International Conference on Automation, Electronics and Electrical Engineering (AUTEEE)",19-21 Nov. 2021,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICSESS47205.2019.9040787,Research on Real-time Learning Prediction Method Based on Spark,IEEE,Conferences,"Based on the research of real-time prediction and big data processing platform, an effective solution is proposed to solve the shortcomings of current real-time learning prediction in engineering application. By analyzing learners' learning behaviors related to a certain course, learners' learning behaviors can be divided into three categories in terms of time and space: online learning behaviors, offline learning behaviors and performance of relevant basic courses. Based on the parallel computation and binary logistic regression algorithm in Spark framework, the off-line learning prediction model is created. In the real-time environment, large scale real-time learning prediction can be realized based on Spark Streaming and kafka. With the increase of learning behaviors data, the scalability problem of prediction scheme can be solved by expanding Spark cluster nodes. The advantages of the proposed scheme have been verified in the practical application of smart campus.",https://ieeexplore.ieee.org/document/9040787/,2019 IEEE 10th International Conference on Software Engineering and Service Science (ICSESS),18-20 Oct. 2019,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ITAIC.2019.8785778,Research on Security Protection of Network Based on Address Layout Randomization from the Perspective of Attackers,IEEE,Conferences,"At present, the network architecture is based on the TCP/IP protocol and node communications are achieved by the IP address and identifier of the node. The IP address in the network remains basically unchanged, so it is more likely to be attacked by network intruder. To this end, it is important to make periodic dynamic hopping in a specific address space possible, so that an intruder fails to obtain the internal network address and grid topological structure in real time and to continue to perform infiltration by the building of a new address space layout randomization system on the basis of SDN from the perspective of an attacker.",https://ieeexplore.ieee.org/document/8785778/,2019 IEEE 8th Joint International Information Technology and Artificial Intelligence Conference (ITAIC),24-26 May 2019,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICPCA.2008.4783569,Research on Software Architecture for Expert System in Practice,IEEE,Conferences,"Advanced applications in fields such as expert system, real-time process control, problem solving, machine learning require the construction, efficient access and management of large, shared knowledge bases. Such knowledge bases cannot be built using existed tools such as expert system shells, because these do not scale up, nor can be built in terms of existing database technology, because such technology does not support the rich representational structure and inference mechanisms required for knowledge-based systems. This paper proposes a generic architecture for a knowledge base management system intended for such applications. The software architecture assumes quotient space structure which analyses identic problem in different granularity space. A new software architecture model for knowledge base system based on quotient space is given. The atomic component, quotient component, connector and configuration are defined. As an example, a knowledge base system framework for agro-meteorology is described. Some components and a configuration about agro-meteorology are given. All above work sets up basic theory of software architecture for knowledge base system based on quotient space and provides new way for further research.",https://ieeexplore.ieee.org/document/4783569/,2008 Third International Conference on Pervasive Computing and Applications,6-8 Oct. 2008,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CISE.2009.5363406,Research on Text Classification Algorithm by Combining Statistical and Ontology Methods,IEEE,Conferences,"Traditional statistics based text classification methods almost construct their characteristic vectors with some key terms, and they consider terms are independent of each other and there are no semantic relations among them. However, in the real world, words used to have semantic relationships, such as synonym, hyponymy and so on. Therefore, classification methods based on statistics do not conform to the fact and the classification results also do not satisfying. To draw this problem, there is a need to obtain characteristic semantic information by taking advantage of ontology. With the help of the features of ontology class hierarchical structure and property constraint, one can match the terms with domain ontology concepts and build up the concept vector space model. Using ontology method for text classification alone will lack scientific and stringency of the statistics. Taking all the above into consideration, this paper takes a combination of the two classification methods. Firstly, we choose the characteristics with statistics method and based on this, add in the ontology and form the concept vector space. Besides, we improve the KNN algorithm from two aspects. Finally, we implement a module for text classification of telecom domain. In the end, we make an analysis and comparison of the results of both statistics-only based (without improving the KNN algorithm) and the combination of two classification methods (with improved KNN).",https://ieeexplore.ieee.org/document/5363406/,2009 International Conference on Computational Intelligence and Software Engineering,11-13 Dec. 2009,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICBASE51474.2020.00054,Research on Unconstrained Face Recognition Based on Deep Learning,IEEE,Conferences,"The emergence of deep learning has greatly promoted the development of the field of face recognition. The accuracy of face recognition in real scenes is affected by many factors. Among them, the problem of multiple poses is still an external factor that is difficult to overcome in face recognition. For the identification process in the Central African people face extreme attitude with the state led to the problem of low recognition accuracy, this paper proposes a gesture of deep space based correction feature to improve the recognition accuracy of multi-pose, first proposed in 2019 to use Google's lightweight network MobileNet for attitude correction in deep space, additionally employed ResNet18 verify and compare recognition results. This paper uses the VggFace2 dataset to train the two models in an end-to-end manner, and then test them on the CFP dataset, IJB-A dataset, and LFW dataset. The results show that the two backbone models proposed in the article are not much different from ResNet50 on the CFP. The face recognition on the IJB-A dataset is around 96%. The average recognition on the public data set LFW is about 96%. From the results of the test set, the model in the article is better than other methods. In addition, MobileNetV3 has a better recognition accuracy than ResNet18, and the amount of calculation is smaller.",https://ieeexplore.ieee.org/document/9403774/,2020 International Conference on Big Data & Artificial Intelligence & Software Engineering (ICBASE),30 Oct.-1 Nov. 2020,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICMA.2019.8816557,Research on V-SLAM Methods,IEEE,Conferences,"With the development of intelligent mobile robots, SLAM, especially V-SLAM, as the basic technology of robot localization and navigation, has the advantages of strong adaptability, high precision and strong intelligence compared with the traditional localization technology. It is widely used in smart devices such as unmanned aerial vehicle, automatic driving and sweeping robots. According to different implementation methods, the visual SLAM is divided into: filter V-SLAM based on probability model, key frame BA-based V-SLAM using nonlinear optimization theory, direct tracking of V-SLAM under the assumption of luminosity invariance, space occupying V-SLAM that focuses on building three-dimensional dense maps. This paper focuses on representative systems of various V-SLAMs and gives their respective applicable scenarios and characteristics. Finally, this article forecasts the development of V-SLAM combining with multi-information fusion technology, semantic deep and learning technology.",https://ieeexplore.ieee.org/document/8816557/,2019 IEEE International Conference on Mechatronics and Automation (ICMA),4-7 Aug. 2019,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICAICA50127.2020.9182662,Research on feedback Cognitive Method of Insulator Self-blast State Based on Multi-scale Convolutional Network,IEEE,Conferences,"In view of the lack of the existing insulator self-blast state detection method and the scale defects of deep neural network structure, imitated the human cognitive model, and learn from the closed-loop control theory, this paper explores a feedback recognition method of insulator self-blast state with multi-scale convolutional neural network. Firstly, for the pre-processed insulator images, based on ResNet-18, branches with different network structure are added to improve the network ability to adapt to different resolutions. At the same time, the multi-scale information fusion module is added at the end of the network. Secondly, the multi-scale feature vector is sent to stochastic configuration networks (SCN) with universal approximation ability to establish the classification criterion of the self-blast state of insulator images with strong generalization ability. Finally, an imitation of human thinking patterns is employed that exhibits repeated deliberation and comparison. Consequently, based on generalized error and entropy theories, the performance index is defined to evaluate the uncertain results of the insulator self-blast states in real time. Then, the regulation mechanism is given to realize the self-optimizing of fixed feature space and the reconstruction of classification criteria, which renders the insulator self-blast states is re-recognized with feedback mechanism. Experimental results show that, compared with other open-loop and closed-loop algorithms, the proposed method enhances the generalization ability and improves the recognition accuracy of the model.",https://ieeexplore.ieee.org/document/9182662/,2020 IEEE International Conference on Artificial Intelligence and Computer Applications (ICAICA),27-29 June 2020,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/RCAR47638.2019.9044114,Research on omnidirectional mobile robot motion control based on integration of traction and steering wheel,IEEE,Conferences,"In order to solve the automatic transportation of heavy materials under the limited working space of production workshops and warehouses, two sets of heavy-duty omnidirectional mobile robot motion control systems with steering wheel drive units were designed. The steering wheel combination drive unit of the “walking + steering” set is used to build the mobile robot chassis, and the mechatronics servo system and mathematical model of multi-motor coordinated motion are constructed. The communication between the controller and the steering wheel combination drive unit is established through the CAN bus. The specific implementation is to capture and analyze the control signal through the controller to obtain the desired motion mode, to obtain the motion of each set of steering wheel unit through the mathematical model, and to realize the desired motion through the synthesis of each set of steering wheel unit motion. It has been verified by experiments that the two sets of steering wheel unit-driven mobile robot control system realizes the zero turning radius, 360-degree omnidirectional movement of the robot and rotation during the movement. It can be used for flexible work in tight spaces.",https://ieeexplore.ieee.org/document/9044114/,2019 IEEE International Conference on Real-time Computing and Robotics (RCAR),4-9 Aug. 2019,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICIEA.2012.6360740,Research on resistor array non-uniformity correction with neural network,IEEE,Conferences,"In order to make the imagery respond just as the real-world scenes do, nonuniformity correction of resistor array is necessary. Most of all, this paper proposed a new neural network algorithm for nonuniformity correction. It has simple neural network model, at the same time, not only can well compensated nonuniformity, also can fit the characteristic curves. In addition, we adopted piecewise linear method to approach the correspondence curve between the actual input voltage and the theoretical input voltage for the first time. This approach greatly reduces the storage space and makes the algorithm easily realized in hardware. Theoretical analysis of the improved neural network algorithm, the algorithm's implementation steps, piecewise linear approach of the corresponding curve, simulation results and considerations are given in the paper.",https://ieeexplore.ieee.org/document/6360740/,2012 7th IEEE Conference on Industrial Electronics and Applications (ICIEA),18-20 July 2012,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AICI.2009.91,Research on the Ontology-Based Complex Event Processing Engine of RFID Technology for Agricultural Products,IEEE,Conferences,"In order to improve the discriminating capacity of the RFID tags for agricultural products traceability data, a new handling engine ORFID-CEP for complex events of agricultural products RFID Tag based on ontology, is proposed by introducing the event ontology model into the field of the agricultural products quality safety administration. In the new engine, the ontology model of RFID event, semantic space and the rules of the event ontology are defined. In the RFID practical application instances, the number of the fringe real-time events is so large that those methods used in current systems can not handle them in time. Thus, many significant events are lost. Aiming to overcome the problem, the transform system of work flow for the complex event ontology and the optimized strategies for event exploration, event operation and restrictive conditions of event appearance are established. Experimental results show that the new engine can mine more complex events with semantic information than the conventional Esper-CEP engine. Additionally, the new engine is steadier than the Esper-CEP engine. With increasing number of the events, the growth rate of the new engine for mining complex events raises more quickly than Esper-CEP, which indicates that the engine has the good ability of information process.",https://ieeexplore.ieee.org/document/5376207/,2009 International Conference on Artificial Intelligence and Computational Intelligence,7-8 Nov. 2009,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IEMBS.2007.4353720,Research on the Segmentation of MRI Image Based on Multi-Classification Support Vector Machine,IEEE,Conferences,"In head MRI image, the boundary of each encephalic tissue is highly complicated and irregular. It is a real challenge to traditional segmentation algorithms. As a new kind of machine learning, Support Vector Machine (SVM) based on Statistical Learning Theory (SLT) has high generalization ability, especially for dataset with small number of samples in high dimensional space. SVM was originally developed for two-class classification. It is extended to solve multi-class classification problem. In this paper, 57 dimensional feature vectors for MRI image are selected as input for SVM. The segmentation of MRI image based on the Multi-Classification SVM (MCSVM) is investigated. As our experiment demonstrates, the boundaries of 7 kinds of encephalic tissues are extracted successfully, and it can reach satisfactory generalization accuracy. Thus, SVM exhibits its great potential in image segmentation.",https://ieeexplore.ieee.org/document/4353720/,2007 29th Annual International Conference of the IEEE Engineering in Medicine and Biology Society,22-26 Aug. 2007,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/DSAA.2016.32,Reserve Price Optimization at Scale,IEEE,Conferences,"Online advertising is a multi-billion dollar industry largely responsible for keeping most online content free and content creators (""publishers"") in business. In one aspect of advertising sales, impressions are auctioned off in second price auctions on an auction-by-auction basis through what is known as real-time bidding (RTB). An important mechanism through which publishers can influence how much revenue they earn is reserve pricing in RTB auctions. The optimal reserve price problem is well studied in both applied and academic literatures. However, few solutions are suited to RTB, where billions of auctions for ad space on millions of different sites and Internet users are conducted each day among bidders with heterogenous valuations. In particular, existing solutions are not robust to violations of assumptions common in auction theory and do not scale to processing terabytes of data each hour, a high dimensional feature space, and a fast changing demand landscape. In this paper, we describe a scalable, online, real-time, incrementally updated reserve price optimizer for RTB that is currently implemented as part of the AppNexus Publisher Suite. Our solution applies an online learning approach, maximizing a custom cost function suited to reserve price optimization. We demonstrate the scalability and feasibility with the results from the reserve price optimizer deployed in a production environment. In the production deployed optimizer, the average revenue lift was 34.4% with 95% confidence intervals (33.2%, 35.6%) from more than 8 billion auctions over 46 days, a substantial increase over non-optimized and often manually set rule based reserve prices.",https://ieeexplore.ieee.org/document/7796939/,2016 IEEE International Conference on Data Science and Advanced Analytics (DSAA),17-19 Oct. 2016,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICSE43902.2021.00028,Resource-Guided Configuration Space Reduction for Deep Learning Models,IEEE,Conferences,"Deep learning models, like traditional software systems, provide a large number of configuration options. A deep learning model can be configured with different hyperparameters and neural architectures. Recently, AutoML (Automated Machine Learning) has been widely adopted to automate model training by systematically exploring diverse configurations. However, current AutoML approaches do not take into consideration the computational constraints imposed by various resources such as available memory, computing power of devices, or execution time. The training with non-conforming configurations could lead to many failed AutoML trial jobs or inappropriate models, which cause significant resource waste and severely slow down development productivity. In this paper, we propose DnnSAT, a resource-guided AutoML approach for deep learning models to help existing AutoML tools efficiently reduce the configuration space ahead of time. DnnSAT can speed up the search process and achieve equal or even better model learning performance because it excludes trial jobs not satisfying the constraints and saves resources for more trials. We formulate the resource-guided configuration space reduction as a constraint satisfaction problem. DnnSAT includes a unified analytic cost model to construct common constraints with respect to the model weight size, number of floating-point operations, model inference time, and GPU memory consumption. It then utilizes an SMT solver to obtain the satisfiable configurations of hyperparameters and neural architectures. Our evaluation results demonstrate the effectiveness of DnnSAT in accelerating state-of-the-art AutoML methods (Hyperparameter Optimization and Neural Architecture Search) with an average speedup from 1.19X to 3.95X on public benchmarks. We believe that DnnSAT can make AutoML more practical in a real-world environment with constrained resources.",https://ieeexplore.ieee.org/document/9402095/,2021 IEEE/ACM 43rd International Conference on Software Engineering (ICSE),22-30 May 2021,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SA47457.2019.8938039,Restricted Boltzmann Machine as Image Pre-processing Method for Deep Neural Classifier,IEEE,Conferences,"The paper presents a novel approach to image preprocessing for feature extraction that is designed for reduction of dimensionality of the classifier which is in this case the convolutional neural network (CNN). The proposed method uses Restricted Boltzmann Machine(RBM) as an Aggregation Method (AM) for binary feature descriptors. The assumption of this technique is that the RBM is performing an dimension expansion of the feature space. Also the type of the data undergoes the transformation from binary to floating point. The conventional approach in convolutional neural networks uses as an input the image that consists of one (grayscale) or three channels (RGB). The method presented herein allows to have the number of channels configurable, as it depends on the size of the Restricted Boltzmann Machine (RBM). The size of the entire network and its parallel implementation makes the architecture usable in real-time systems with reduced memory size.",https://ieeexplore.ieee.org/document/8938039/,2019 First International Conference on Societal Automation (SA),4-6 Sept. 2019,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CVPR46437.2021.01042,Retinex-inspired Unrolling with Cooperative Prior Architecture Search for Low-light Image Enhancement,IEEE,Conferences,"Low-light image enhancement plays very important roles in low-level vision areas. Recent works have built a great deal of deep learning models to address this task. However, these approaches mostly rely on significant architecture engineering and suffer from high computational burden. In this paper, we propose a new method, named Retinex-inspired Unrolling with Architecture Search (RUAS), to construct lightweight yet effective enhancement network for low-light images in real-world scenario. Specifically, building upon Retinex rule, RUAS first establishes models to characterize the intrinsic underexposed structure of low-light images and unroll their optimization processes to construct our holistic propagation structure. Then by designing a cooperative reference-free learning strategy to discover low-light prior architectures from a compact search space, RUAS is able to obtain a top-performing image enhancement network, which is with fast speed and requires few computational resources. Extensive experiments verify the superiority of our RUAS framework against recently proposed state-of-the-art methods. The project page is available at http://dutmedia.org/RUAS/.",https://ieeexplore.ieee.org/document/9577287/,2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),20-25 June 2021,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/HPCA51647.2021.00028,Revisiting HyperDimensional Learning for FPGA and Low-Power Architectures,IEEE,Conferences,"Today's applications are using machine learning algorithms to analyze the data collected from a swarm of devices on the Internet of Things (IoT). However, most existing learning algorithms are overcomplex to enable real-time learning on IoT devices with limited resources and computing power. Recently, Hyperdimensional computing (HDC) is introduced as an alternative computing paradigm for enabling efficient and robust learning. HDC emulates the cognitive task by representing the values as patterns of neural activity in high-dimensional space. HDC first encodes all data points to high-dimensional vectors. It then efficiently performs the learning task using a well-defined set of operations. Existing HDC solutions have two main issues that hinder their deployments on low-power embedded devices: (i) the encoding module is costly, dominating 80% of the entire training performance, (ii) the HDC model size and the computation cost grow significantly with the number of classes in online inference.In this paper, we proposed a novel architecture, LookHD, which enables real-time HDC learning on low-power edge devices. LookHD exploits computation reuse to memorize the encoding module and simplify its computation with single memory access. LookHD also address the inference scalability by exploiting HDC governing mathematics that compresses the HDC trained model into a single hypervector. We present how the proposed architecture can be implemented on the existing low power architectures: ARM processor and FPGA design. We evaluate the efficiency of the proposed approach on a wide range of practical classification problems such as activity recognition, face recognition, and speech recognition. Our evaluations show that LookHD can achieve, on average, $ 28.3\times$ faster and $ 97.4\times$ more energy-efficient training as compared to the state-of-the-art HDC implemented on the FPGA. Similarly, in the inference, LookHD is $ 2.2\times$ faster, $ 4.1\times$ more energy-efficient, and has $ 6.3\times$ smaller model size than the same state-of-the-art algorithms.",https://ieeexplore.ieee.org/document/9407181/,2021 IEEE International Symposium on High-Performance Computer Architecture (HPCA),27 Feb.-3 March 2021,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SEFM.2009.36,Right Propositional Neighborhood Logic over Natural Numbers with Integer Constraints for Interval Lengths,IEEE,Conferences,"Interval temporal logics are based on interval structures over linearly (or partially) ordered domains, where time intervals, rather than time instants, are the primitive ontological entities. In this paper we introduce and study Right Propositional Neighborhood Logic over natural numbers with integer constraints for interval lengths, which is a propositional interval temporal logic featuring a modality for the 'right neighborhood' relation between intervals and explicit integer constraints for interval lengths. We prove that it has the bounded model property with respect to ultimately periodic models and is therefore decidable. In addition, we provide an EXP SPACE procedure for satisfiability checking and we prove EXPSPACE-hardness by a reduction from the exponential corridor tiling problem.",https://ieeexplore.ieee.org/document/5368086/,2009 Seventh IEEE International Conference on Software Engineering and Formal Methods,23-27 Nov. 2009,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SAIS53221.2021.9483964,Robot First Aid: Autonomous Vehicles Could Help in Emergencies,IEEE,Conferences,"Safety is of critical importance in designing autonomous vehicles (AVs) that will be able to perform effectively in complex, mixed-traffic, real-world urban environments. Some prior research has looked at how to proactively avoid accidents with safe distancing and driver monitoring, but currently little research has explored strategies to recover afterwards from emergencies, from crime to natural disasters. The current short paper reports on our ongoing work using a speculative prototyping approach to explore this expansive design space, in the context of how a robot inside an AV could be deployed to support first aid. As a result, we present some proposals for how to detect emergencies, and examine and help victims, as well as lessons learned in prototyping. Thereby, our aim is to stimulate discussion and ideation that-by considering the prevalence of Murphy's law in our complex world, and the various technical, ethical, and practical concerns raised-could potentially lead to useful safety innovations.",https://ieeexplore.ieee.org/document/9483964/,2021 Swedish Artificial Intelligence Society Workshop (SAIS),14-15 June 2021,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ASAMA.1999.805407,Robot media communication: an interactive real-world guide agent,IEEE,Conferences,"Describes a guide system and the software architecture for an autonomous, interactive robot based on a multi-agent system. A robot navigation system has been developed allowing the robot to guide people through halls in various types of exhibitions. Our approach uses an infrared location system in the hallway ceilings, making the environment part of a sensor-distributed robot system. The real-world guide agent is composed of a guide agent on a hand-held mobile computer and a robot agent on an autonomous mobile robot. The guide agent plays the role of ""robot media"" in order to integrate information in the information space of the mobile computer and the physical space of the exhibits in order to guide visitors through the physical space. This research aims to develop a cooperative adaptive system using two-way communication among spaces, media and human beings to construct transparent knowledge boundaries between the real space and the virtual space. The virtual space is generated from computer data using shared space technology and it creates a distributed intelligence in order to manage the communication and control the guide in a laboratory. We have experimented with and verified this software architecture using a prototype autonomous mobile robot equipped with a compass.",https://ieeexplore.ieee.org/document/805407/,"Proceedings. First and Third International Symposium on Agent Systems Applications, and Mobile Agents",6-6 Oct. 1999,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICSMC.2008.4811760,Robot navigation using KFLANN place field,IEEE,Conferences,"This paper presents an implementation of place cells for a robot navigation using the K-iterations fast learning artificial neural networks (KFLANN) clustering algorithm. The KFLANN possesses several desirable properties suitable for place cell robot navigation tasks. The technique proposed is able to autonomously adjust the resolution of cells according to the complexity of the environment. This is achieved through two parameters known as the tolerance and the vigilance of the network. In addition, a navigation system consisting of a topological map building and a place cell path planning strategy is presented. A physical implementation of the system was developed on an autonomous platform and actual results were obtained. The experimental results obtained indicate that the system was able to navigate successfully through the experimental space and also tolerate unexpected discrepancies arising from motor and sensor errors present in a real environment. Furthermore, despite abrupt changes in an environment due to the deliberate introduction of obstacles, the system was still able to cope without changes to the program. The experiment was also extended to include a kidnapped robot scenario and the results were favorable, indicating a positive use of allothetic cue recognition capabilities.",https://ieeexplore.ieee.org/document/4811760/,"2008 IEEE International Conference on Systems, Man and Cybernetics",12-15 Oct. 2008,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CEC.2016.7743924,Robot-to-human handover with obstacle avoidance via continuous time Recurrent Neural Network,IEEE,Conferences,"Parallel with the development of service robots, it is vital for the robots to carry out handovers autonomously. Robot-to-human handover is a coordination in time and space for a robot to deliver an object to human. A good robot-to-human handover should consider human safety and preference, natural motion planning that mimics human and adaptability to the changes of the environment. Conventional handover motion mostly rely on sampling-based algorithms that emphasizes on kinematic and dynamic analysis. This kind of motion planning could become complicated and slow in response if the handover motion is implemented in a dynamic environment where real time motion planning is required. To simplify the implementation of robot-to-human handover, a motion learning and generation framework that based on Continuous Time Recurrent Neural Network(CTRNN) is proposed. The proposed framework is equipped with the capabilities of object recognition, motion generation based on past learning experience and obstacle adaptation. As compared with conventional method, the proposed framework could be easily extended to handover motion with high dimensional configuration spaces as the motion can be generated from the learnt experience. In the proposed framework, the handover behaviour can be learnt via human-guided motion teaching which provides an intuitive and visible solution for motion planning. The proposed framework has been experimentally evaluated on a customized design robot via robotto-human handover testing. Based on the testing, the feasibility of the proposed framework had been justified.",https://ieeexplore.ieee.org/document/7743924/,2016 IEEE Congress on Evolutionary Computation (CEC),24-29 July 2016,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/C-CODE.2017.7918964,Robotic navigation based on logic-based planning,IEEE,Conferences,"Logic and Planning are interesting artificial intelligence problems in the context of robotic systems, i.e., robotic navigation. For such an autonomous system one of the requisites is that the goal has to be achieved without intervention of human being. We present a practical implementation of autonomous robotic navigation based on logic-based planning. We achieve this by using strength of PROLOG in order to generate plan to reach goal position from an initial. We utilize First Order Logic (FOL) that automatically asserts and retracts facts at runtime dynamically. All possible plans are computed using local search strategies (e.g., Depth and Breadth First) on state space representing a real, dynamic, and unpredictable environment. In order to navigate in the environment following optimized plan - one with fewest states, a balanced size 4-wheel differential drive robot has been carefully constructed. It can turn 90° and actuate forward by controlling linear (νt = 0.25m/s) and angular (ωt = Π/8 rad/s) velocities of two rear motorized wheels. It is also equipped with an Ultrasonic sensor to avoid collision with obstacles. The system is evaluated in an environment comprising of corridors with adjacent rooms. Graphical User Interface (GUI) is developed in .Net (C#) to map situation in Prolog and transmit plan to hardware for execution. Average time calculated for a plan to generate is 0.065 seconds. The robot moves block by block where each block in the state space represents 2m2 area. In addition to minors, our major contribution is that we offer a unified scheme for robotic navigation without calculating odometry data with the assumption the robot cannot be kidnapped nor slipped.",https://ieeexplore.ieee.org/document/7918964/,"2017 International Conference on Communication, Computing and Digital Systems (C-CODE)",8-9 March 2017,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IITA.2008.146,Robust Fuzzy-Possibilistic C-Means Algorithm,IEEE,Conferences,"In allusion to the disadvantages that fuzzy c-means algorithm is sensitive to noise and possibilistic c-means is easy to generate superposition cluster center, a novel algorithm (FPCM) which simultaneously produces both memberships and possibilities was proposed in 1997. However, FPCM still uses a norm-induced distance, as a consequence, its performance on the noisy data is not strong enough. In this paper, a new algorithm using the ""kernel method"" based on the classical FPCM is presented and called as robust fuzzy-possibilistic algorithm (RFPCM). RFPCM adopts a new kernel-induced metric in the data space to replace the original Euclidean norm metric in FPCM. Experiments on the artificial and real datasets show that RFPCM has better clustering performance and is more robust to noise than FPCM and PCM.",https://ieeexplore.ieee.org/document/4739656/,2008 Second International Symposium on Intelligent Information Technology Application,20-22 Dec. 2008,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICDE.2019.00128,Robust High Dimensional Stream Classification with Novel Class Detection,IEEE,Conferences,"A primary challenge in label prediction over a data stream is the emergence of instances belonging to unknown or novel class over time. Traditionally, studies addressing this problem aim to detect such instances using cluster-based mechanisms. They typically assume that instances from the same class are closer to each other than those belonging to different classes in observed feature space. Unfortunately, this may not hold true in higher-dimensional feature space such as images. In recent years, Convolutional neural network (CNN) have emerged as a leading system to be employed in many real-world application. Yet, based on the assumption of closed world dataset with a fixed number of categories, CNN lacks robustness for novel class detection, so it is unclear on how such models can be used to deal with novel class instances along a high-dimensional image stream. In this paper, we focus on addressing this challenge by proposing an effective learning framework called CNN-based Prototype Ensemble (CPE) for novel class detection and correction. Our framework includes a prototype ensemble loss (PE) to improve the intra-class compactness and expand inter-class separateness in the output feature representation, thereby enabling the robustness of novel class detection. Moreover, we provide an incremental learning strategy which maintains a constant amount of exemplars to update the network, making it more practical for real-world application. We empirically demonstrate the effectiveness of our framework by comparing its performance over multiple realworld image benchmark data streams with existing state-of-theart data stream detection techniques. The implementation of CPE is on: https://github.com/Vitvicky/Convolutional-Net-PrototypeEnsemble",https://ieeexplore.ieee.org/document/8731449/,2019 IEEE 35th International Conference on Data Engineering (ICDE),8-11 April 2019,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CVPR46437.2021.00844,Robust Neural Routing Through Space Partitions for Camera Relocalization in Dynamic Indoor Environments,IEEE,Conferences,"Localizing the camera in a known indoor environment is a key building block for scene mapping, robot navigation, AR, etc. Recent advances estimate the camera pose via optimization over the 2D/3D-3D correspondences established between the coordinates in 2D/3D camera space and 3D world space. Such a mapping is estimated with either a convolution neural network or a decision tree using only the static input image sequence, which makes these approaches vulnerable to dynamic indoor environments that are quite common yet challenging in the real world. To address the aforementioned issues, in this paper, we propose a novel outlier-aware neural tree which bridges the two worlds, deep learning and decision tree approaches. It builds on three important blocks: (a) a hierarchical space partition over the indoor scene to construct the decision tree; (b) a neural routing function, implemented as a deep classification network, employed for better 3D scene understanding; and (c) an outlier rejection module used to filter out dynamic points during the hierarchical routing process. Our proposed algorithm is evaluated on the RIO-10 benchmark developed for camera relocalization in dynamic indoor environments. It achieves robust neural routing through space partitions and outperforms the state-of-the-art approaches by around 30% on camera pose accuracy, while running comparably fast for evaluation.",https://ieeexplore.ieee.org/document/9577932/,2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),20-25 June 2021,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA48506.2021.9562105,Robust Place Recognition using an Imaging Lidar,IEEE,Conferences,"We propose a methodology for robust, real-time place recognition using an imaging lidar, which yields image-quality high-resolution 3D point clouds. Utilizing the intensity readings of an imaging lidar, we project the point cloud and obtain an intensity image. ORB feature descriptors are extracted from the image and encoded into a bag-of-words vector. The vector, used to identify the point cloud, is inserted into a database that is maintained by DBoW for fast place recognition queries. The returned candidate is further validated by matching visual feature descriptors. To reject matching outliers, we apply PnP, which minimizes the reprojection error of visual features&#x2019; positions in Euclidean space with their correspondences in 2D image space, using RANSAC. Combining the advantages from both camera and lidar-based place recognition approaches, our method is truly rotation-invariant, and can tackle reverse revisiting and upside down revisiting. The proposed method is evaluated on datasets gathered from a variety of platforms over different scales and environments. Our implementation and datasets are available at https://git.io/image-lidar.",https://ieeexplore.ieee.org/document/9562105/,2021 IEEE International Conference on Robotics and Automation (ICRA),30 May-5 June 2021,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/RATFG.1999.799239,Robust real-time human hand localization by self-organizing color segmentation,IEEE,Conferences,"This paper describes a robust tracking algorithm used to localize a human hand in video sequences. The localization system relies mainly on an automatic color-based segmentation scheme combined with the motion cue. An automatic self-organizing clustering algorithm, is proposed to learn the color clusters unsupervisedly in the HSI space without specifying the number of clusters in advance. The schemes of growing, pruning and merging of 1-D self-organizing map (SOM) are facilitated to find an appropriate number of clusters in the forming stage of SOM. The training and segmentation in our approach is fast enough to make possible real-time applications. This segmentation scheme is capable of tracking multiple objects of different colors simultaneously. A motion cue is employed to focus the attention of the tracking algorithm. This approach is also applied to other tasks such as human face tracking and color indexing. Our localization system implemented on a SGI O2 R10000 workstation is reliable and efficient at 20-30 Hz.",https://ieeexplore.ieee.org/document/799239/,"Proceedings International Workshop on Recognition, Analysis, and Tracking of Faces and Gestures in Real-Time Systems. In Conjunction with ICCV'99 (Cat. No.PR00378)",26-27 Sept. 1999,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CVPR.2019.01204,Robustness of 3D Deep Learning in an Adversarial Setting,IEEE,Conferences,"Understanding the spatial arrangement and nature of real-world objects is of paramount importance to many complex engineering tasks, including autonomous navigation. Deep learning has revolutionized state-of-the-art performance for tasks in 3D environments; however, relatively little is known about the robustness of these approaches in an adversarial setting. The lack of comprehensive analysis makes it difficult to justify deployment of 3D deep learning models in real-world, safety-critical applications. In this work, we develop an algorithm for analysis of pointwise robustness of neural networks that operate on 3D data. We show that current approaches presented for understanding the resilience of state-of-the-art models vastly overestimate their robustness. We then use our algorithm to evaluate an array of state-of-the-art models in order to demonstrate their vulnerability to occlusion attacks. We show that, in the worst case, these networks can be reduced to 0% classification accuracy after the occlusion of at most 6.5% of the occupied input space.",https://ieeexplore.ieee.org/document/8953599/,2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),15-20 June 2019,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/EEIS.1995.513798,Rotation-invariant MLP classifiers for automatic aerial image recognition,IEEE,Conferences,"This paper describes the application of Multi Layer Perceptron (MLP) neural networks to the problem of Automatic Aerial Image Recognition (AAIR). The classification of aerial images independent of their orientation is required for automatic tracking and target recognition. Rotation-invariance is achieved by using rotation invariant feature space in conjunction with feed forward neural networks. The performance of the neural network based classifiers in conjunction with 3 types of rotation-invariant AAIR global features: the Zernike moments, central moments, and polar transform are examined. The performance of the Zernike based classifier is compared with that of the classical central moments, and polar transform. The real part of the phase spectrum of the Fourier plane is employed in combination with the MLP for rotation and translation invariance. The advantages of these approaches are discussed. Although a large image data base would be necessary before this approach could be fully validated, the initial results are very promising.",https://ieeexplore.ieee.org/document/513798/,Eighteenth Convention of Electrical and Electronics Engineers in Israel,7-8 March 1995,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CloudTech49835.2020.9365867,Run Time Optimization using a novel implementation of Parallel-PSO for real-world applications,IEEE,Conferences,"The majority of optimization algorithms and methods generally necessitate a considerable run time to reach their goal. Most of them are used mainly in real-world applications. This article concentrates on an efficient and well-known algorithm to solve optimization problems: the Particle Swarm Optimisation algorithm (PSO). This algorithm needs a considerable run time to solve an optimization problem with a high dimension space and data. The article also concentrates on OpenCL, which defines a common parallel programming language for various devices such as GPU, CPU, FPGA, etc. In order to minimize the run time of PSO, this paper introduces a new implementation of PSO in OpenCL. By decomposing the PSO code into two fragments, each one can run simultaneously. The experimental results covered both the sequential and parallel implementations. Furthermore, show that the PSO' OpenCL implementation is faster than the Sequential-PSO implementation. The OpenCL profiling results show the timing of each part of the executing of PSO in OpenCL.",https://ieeexplore.ieee.org/document/9365867/,2020 5th International Conference on Cloud Computing and Artificial Intelligence: Technologies and Applications (CloudTech),24-26 Nov. 2020,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1145/3510003.3510226,SAPIENTML: Synthesizing Machine Learning Pipelines by Learning from Human-Written Solutions,IEEE,Conferences,"Automatic machine learning, or AutoML, holds the promise of truly democratizing the use of machine learning (ML), by substantially automating the work of data scientists. However, the huge combinatorial search space of candidate pipelines means that current AutoML techniques, generate sub-optimal pipelines, or none at all, especially on large, complex datasets. In this work we propose an AutoML technique SapientML, that can learn from a corpus of existing datasets and their human-written pipelines, and efficiently generate a high-quality pipeline for a predictive task on a new dataset. To combat the search space explosion of AutoML, SapientML employs a novel divide-and-conquer strategy realized as a three-stage program synthesis approach, that reasons on successively smaller search spaces. The first stage uses meta-learning to predict a set of plausible ML components to constitute a pipeline. In the second stage, this is then refined into a small pool of viable concrete pipelines using a pipeline dataflow model derived from the corpus. Dynamically evaluating these few pipelines, in the third stage, provides the best solution. We instantiate SapientML as part of a fully automated tool-chain that creates a cleaned, labeled learning corpus by mining Kaggle, learns from it, and uses the learned models to then synthesize pipelines for new predictive tasks. We have created a training corpus of 1,094 pipelines spanning 170 datasets, and evaluated SapientML on a set of 41 benchmark datasets, including 10 new, large, real-world datasets from Kaggle, and against 3 state-of-the-art AutoML tools and 4 baselines. Our evaluation shows that SapientML produces the best or comparable accuracy on 27 of the benchmarks while the second best tool fails to even produce a pipeline on 9 of the instances. This difference is amplified on the 10 most challenging benchmarks, where SapientML wins on 9 instances with the other tools failing to produce pipelines on 4 or more benchmarks.",https://ieeexplore.ieee.org/document/9794084/,2022 IEEE/ACM 44th International Conference on Software Engineering (ICSE),25-27 May 2022,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/WACVW54805.2022.00011,SAPNet: Segmentation-Aware Progressive Network for Perceptual Contrastive Deraining,IEEE,Conferences,"Deep learning algorithms have recently achieved promising deraining performance&#x2013;s on both the natural and synthetic rainy datasets. As an essential low-level preprocessing stage, a deraining network should clear the rain streaks and preserve the fine semantic details. However, most existing methods only consider low-level image restoration. That limits their performances at high-level tasks requiring precise semantic information. To address this issue, in this paper, we present a segmentation aware progressive network (SAPNet) based upon contrastive learning for single image deraining. We start our method with a lightweight derain network formed with progressive dilated units (PDU). The PDU can significantly expand the receptive field and characterize multiscale rain streaks without the heavy computation on multiscale images. A fundamental aspect of this work is an unsupervised background segmentation (UBS) network initialized with ImageNet and Gaussian weights. The UBS can faithfully preserve an image s semantic information and improve the generalization ability to unseen photos. Furthermore, we introduce a perceptual contrastive loss (PCL) and a learned perceptual image similarity loss (LPISL) to regulate model learning. By ex-ploiting the rainy image and ground-truth as the negative and the positive sample in the VGG-16 latent space, we bridge the fine semantic details between the derained image and the ground-truth in a fully constrained manner. Comprehensive experiments on synthetic and real-world rainy images show our model surpasses top-performing methods and aids object detection and semantic segmentation with considerable efficacy. A Pytorch Implementation is available at https://github.com/ShenZheng2000/SAPNetfor-image-deraining.",https://ieeexplore.ieee.org/document/9707514/,2022 IEEE/CVF Winter Conference on Applications of Computer Vision Workshops (WACVW),4-8 Jan. 2022,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IGARSS.2018.8519369,SMOS Data Assimilation for Numerical Weather Prediction,IEEE,Conferences,"This paper presents the Soil Moisture and Ocean Salinity (SMOS) mission data assimilation activities conducted at the European Centre for Medium-Range Weather Forecasts (ECMWF) to analyse soil moisture for Numerical Weather Prediction (NWP) applications. Two different approaches are presented based on SMOS brightness temperature and SMOS neural network soil moisture data assimilation, respectively. For the first approach, SMOS brightness temperature data assimilation relies on forward modelling. Long term results, spanning the SMOS period, of SMOS forward modelling, monitoring and data assimilation are presented. They emphasize the relevance of SMOS data for monitoring and to support NWP model developments. For the second approach, a SMOS soil moisture product has been produced based on a Neural Network (NN) trained on ECMWF soil moisture. So, the SMOS-ECMWF NN soil moisture product captures the SMOS signal variability in time and space, while by design its climatology is consistent with that of the ECMWF soil moisture, which makes it suitable for data assimilation purpose. This approach, initially tested for 2012 in a global scale stand alone approach, shows that SMOS NN data assimilation slightly improves the two-metre air temperature forecast in the short range at regional scale. For NWP applications this approach has been further developed with a near real time production of the SMOS-ECMWF NN soil moisture product, with the implementation of the SMOS NN data assimilation in the ECMWF Integrated Forecasting System (IFS), and with high resolution (9km) global scale testing compatible with the current ECMWF NWP system.",https://ieeexplore.ieee.org/document/8519369/,IGARSS 2018 - 2018 IEEE International Geoscience and Remote Sensing Symposium,22-27 July 2018,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICIP.2019.8803262,SSSDET: Simple Short and Shallow Network for Resource Efficient Vehicle Detection in Aerial Scenes,IEEE,Conferences,"Detection of small-sized targets is of paramount importance in many aerial vision-based applications. The commonly deployed low cost unmanned aerial vehicles (UAVs) for aerial scene analysis are highly resource constrained in nature. In this paper we propose a simple short and shallow network (SSSDet) to robustly detect and classify small-sized vehicles in aerial scenes. The proposed SSSDet is up to 4× faster, requires 4.4× less FLOPs, has 30× less parameters, requires 31× less memory space and provides better accuracy in comparison to existing state-of-the-art detectors. Thus, it is more suitable for hardware implementation in real-time applications. We also created a new airborne image dataset (ABD) by annotating 1396 new objects in 79 aerial images for our experiments. The effectiveness of the proposed method is validated on the existing VEDAI, DLR-3K, DOTA and Combined dataset. The SSSDet outperforms state-of-the-art detectors in term of accuracy, speed, compute and memory efficiency.",https://ieeexplore.ieee.org/document/8803262/,2019 IEEE International Conference on Image Processing (ICIP),22-25 Sept. 2019,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IJCB52358.2021.9484360,STERLING: Towards Effective ECG Biometric Recognition,IEEE,Conferences,"Electrocardiogram (ECG) biometric recognition has recently attracted considerable attention and various promising approaches have been proposed. However, due to the real nonstationary ECG noise environment, it is still challenging to perform this technique robustly and precisely. In this paper, we propose a novel ECG biometrics framework named robuSt semanTic spacE leaRning with Local sImilarity preserviNG (STERLING) to learn a latent space where ECG signals can be robustly and discriminatively represented with semantic information and local structure being preserved. Specifically, in the proposed framework, a novel loss function is proposed to learn robust semantic representation by introducing l2,1-norm loss and making full use of the supervised information. In addition, a graph regularization is imposed to preserve the local structure information in each subject. Finally, in the learnt latent space, matching can be effectively done. The experimental results on three widely-used datasets indicate that the proposed framework can outperform the state-of-the-arts.",https://ieeexplore.ieee.org/document/9484360/,2021 IEEE International Joint Conference on Biometrics (IJCB),4-7 Aug. 2021,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICMLC.2002.1167432,SVM-based incremental active learning for user adaptation for online graphics recognition system,IEEE,Conferences,"User adaptation is critical in the future design of human-computer interaction systems. Many pattern recognition problems, such as handwriting/sketching recognition and speech recognition, are user dependent since different users' handwritings, drawing styles, and accents are different. Hence, the classifiers for solving these problems should provide the functionality of user adaptation so as to let all users experience better recognition results. However, the user adaptation functionality requires the classifiers have the incremental learning ability in order to learn fast. In this paper, an SVM-based incremental active learning algorithm is presented to solve this problem. By utilizing the support vectors and only a small portion of the nonsupport vectors as well, in addition to the new interrogative samples, in the iterative training and reclassification cycle, both the training time and the storage space are saved with only very little classification precision being lost. Theoretical analysis, experimentation, evaluation, and real application samples in our online graphics recognition system are presented to show the effectiveness of this algorithm.",https://ieeexplore.ieee.org/document/1167432/,Proceedings. International Conference on Machine Learning and Cybernetics,4-5 Nov. 2002,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ITSC45102.2020.9294259,Safe Reinforcement Learning for Autonomous Lane Changing Using Set-Based Prediction,IEEE,Conferences,"Machine learning approaches often lack safety guarantees, which are often a key requirement in real-world tasks. This paper addresses the lack of safety guarantees by extending reinforcement learning with a safety layer that restricts the action space to the subspace of safe actions. We demonstrate the proposed approach using lane changing in autonomous driving. To distinguish safe actions from unsafe ones, we compare planned motions with the set of possible occupancies of traffic participants generated by set-based predictions. In situations where no safe action exists, a verified failsafe controller is executed. We used real-world highway traffic data to train and test the proposed approach. The evaluation result shows that the proposed approach trains agents that do not cause collisions during training and deployment.",https://ieeexplore.ieee.org/document/9294259/,2020 IEEE 23rd International Conference on Intelligent Transportation Systems (ITSC),20-23 Sept. 2020,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/GLOBECOM46510.2021.9685965,SamE: Sampling-based Embedding for Learning Representations of the Internet,IEEE,Conferences,"We have developed SamE, a novel sampling-based embedding technique for learning representations of the Internet. SamE can classify Internet hosts in a scalable and cost effective manner without sacrificing the classification performance. Machine learning has been applied to Internet traffic analysis for a variety of purposes, including botnet detection and application identification. For example, as a major threat on the Internet, a botnet is a group of computers that collaborate together to launch cyberattacks. To analyze related hosts such as the collaborating constituents of a botnet, graph embedding techniques seem to be promising. However, when applying existing graph embedding techniques to Internet-scale traffic data, the time and space complexities become prohibitively high for practical use. To make graph embedding applicable to Internet-scale problems, SamE only samples a subset of nodes to learn elemental representations and aggregates learned elemental representations to generate synthetic representations for all nodes. We have applied SamE to real-world Internet-scale traffic data, and the experimental results show that SamE outperforms existing methods by reducing the data samples required for representation learning by 99% while achieving the same level of classification performance in botnet detection and application identification.",https://ieeexplore.ieee.org/document/9685965/,2021 IEEE Global Communications Conference (GLOBECOM),7-11 Dec. 2021,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IPDPSW55747.2022.00166,"ScaDL 2022 Invited Talk 1: Design of secure power monitors for accelerators, by exploiting ML techniques, in the Euro-HPC TEXTAROSSA project",IEEE,Conferences,"The evolution of High Performance Computing (HPC) has to face with several obstacles, including power/thermal management of the cores and the presence of heterogeneous computing platforms. Within the Euro-HPC project TEXTAROSSA, started in spring 2021, several HPC applications exploiting AI and with the need of processing big chunks of data in a secure context, are properly accelerated by leveraging ad-hoc designed accelerators to be implemented in hardware. Such customized heterogeneity of execution has many benefits, but increases the problem of power management, since the accelerators, possibly generated through high-level-synthesis, are neither providing run-time information on their power consumption, nor allows to control the security of the information flow against implementation attacks. In such scenario, any global power manager / resource orchestrator, can operate only with a partial picture of the overall systems and not in real-time, with the risk of being trapped in poor power optimizations and unbalanced resource exploitation. The goal of the talk is to show how is possible to exploit popular ML techniques for a twofold purpose: &#x2022;Automatically generate an on-line power monitor, to augment the hardware description of any piece of hardware, in particular that of cores and accelerators, capable to provide on-line power estimated in less that few milliseconds. &#x2022;Select, in the space of all the possible power monitors, those that are not leaking information that can be used to mount side-channel attacks.",https://ieeexplore.ieee.org/document/9835282/,2022 IEEE International Parallel and Distributed Processing Symposium Workshops (IPDPSW),30 May-3 June 2022,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CVPR42600.2020.01202,Scalable Uncertainty for Computer Vision With Functional Variational Inference,IEEE,Conferences,"As Deep Learning continues to yield successful applications in Computer Vision, the ability to quantify all forms of uncertainty is a paramount requirement for its safe and reliable deployment in the real-world. In this work, we leverage the formulation of variational inference in function space, where we associate Gaussian Processes (GPs) to both Bayesian CNN priors and variational family. Since GPs are fully determined by their mean and covariance functions, we are able to obtain predictive uncertainty estimates at the cost of a single forward pass through any chosen CNN architecture and for any supervised learning task. By leveraging the structure of the induced covariance matrices, we propose numerically efficient algorithms which enable fast training in the context of high-dimensional tasks such as depth estimation and semantic segmentation. Additionally, we provide sufficient conditions for constructing regression loss functions whose probabilistic counterparts are compatible with aleatoric uncertainty quantification.",https://ieeexplore.ieee.org/document/9156974/,2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),13-19 June 2020,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISRCS.2013.6623773,Scalable machine learning framework for behavior-based access control,IEEE,Conferences,"Today's activities in cyber space are more connected than ever before, driven by the ability to dynamically interact and share information with a changing set of partners over a wide variety of networks. The success of approaches aimed at securing the infrastructure has changed the threat profile to point where the biggest threat to the US cyber infrastructure is posed by targeted cyber attacks. The Behavior-Based Access Control (BBAC) effort has been investigating means to increase resilience against these attacks. Using statistical machine learning, BBAC (a) analyzes behaviors of insiders pursuing targeted attacks and (b) assesses trustworthiness of information to support real-time decision making about information sharing. The scope of this paper is to describe the challenge of processing disparate cyber security information at scale, together with an architecture and work-in-progress prototype implementation for a cloud framework supporting a strategic combination of stream and batch processing.",https://ieeexplore.ieee.org/document/6623773/,2013 6th International Symposium on Resilient Control Systems (ISRCS),13-15 Aug. 2013,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICMLA.2007.110,Scalable optimal linear representation for face and object recognition,IEEE,Conferences,"Optimal component analysis (OCA) is a linear method for feature extraction and dimension reduction. It has been widely used in many applications such as face and object recognitions. The optimal basis of OCA is obtained through solving an optimization problem on a Grassmann manifold. However, one limitation of OCA is the computational cost becoming heavy when the number of training data is large, which prevents OCA from efficiently applying in many real applications. In this paper, a scalable OCA (S-OCA) that uses a two-stage strategy is developed to bridge this gap. In the first stage, we cluster the training data using K-means algorithm and the dimension of data is reduced into a low dimensional space. In the second stage, OCA search is performed in the reduced space and the gradient is updated using an numerical approximation. In the process of OCA gradient updating, instead of choosing the entire training data, S-OCA randomly chooses a small subset of the training images in each class to update the gradient. This achieves stochastic gradient updating and at the same time reduces the searching time of OCA in orders of magnitude. Experimental results on face and object datasets show efficiency of the S-OCA method, in term of both classification accuracy and computational complexity.",https://ieeexplore.ieee.org/document/4457247/,Sixth International Conference on Machine Learning and Applications (ICMLA 2007),13-15 Dec. 2007,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IV48863.2021.9575822,Self-Supervised Action-Space Prediction for Automated Driving,IEEE,Conferences,"Making informed driving decisions requires reliable prediction of other vehicles' trajectories. In this paper, we present a novel learned multi-modal trajectory prediction architecture for automated driving. It achieves kinematically feasible predictions by casting the learning problem into the space of accelerations and steering angles - by performing action-space prediction, we can leverage valuable model knowledge. Additionally, the dimensionality of the action manifold is lower than that of the state manifold, whose intrinsically correlated states are more difficult to capture in a learned manner. For the purpose of action-space prediction, we present the simple Feed-Forward Action-Space Prediction (FFW-ASP) architecture. Then, we build on this notion and introduce the novel Self-Supervised Action-Space Prediction (SSP-ASP) architecture that outputs future environment context features in addition to trajectories. A key element in the self-supervised architecture is that, based on an observed action history and past context features, future context features are predicted prior to future trajectories. The proposed methods are evaluated on real-world datasets containing urban intersections and roundabouts, and show accurate predictions, outperforming state-of-the-art for kinematically feasible predictions in several prediction metrics.",https://ieeexplore.ieee.org/document/9575822/,2021 IEEE Intelligent Vehicles Symposium (IV),11-17 July 2021,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IOLTS.2015.7229845,Self-awareness and self-learning for resiliency in real-time systems,IEEE,Conferences,"While the notion of self-awareness has a long history in biology, psychology, medicine, engineering and (more recently) computing, we are seeing the emerging need for self-awareness in the context of complex Systems-on-Chip that must address the often conflicting requirements of performance, resiliency, energy, cost, etc. in the face of highly dynamic operational behaviors coupled with process, environment, and workload variabilities. Unlike traditional Systems-on-Chip (SoCs), self-aware SoCs must deploy an intelligent co-design of the control, communication, and computing infrastructure that interacts with the physical environment in real-time in order to modify the systems behavior so as to adaptively achieve desired objectives and Quality-of-Service (QoS). Self-aware SoCs require a combination of ubiquitous sensing and actuation, health-monitoring, and self-learning to enable the SoCs adaptation over time and space. This special session targets self-learning and self-awareness in two domains. The first one is a self-learning runtime reliability prediction approach by reusing Design-for-Test (DfT) infrastructure. The other one discusses real-time systems and applications to wireless communication, signal processing and control.",https://ieeexplore.ieee.org/document/7229845/,2015 IEEE 21st International On-Line Testing Symposium (IOLTS),6-8 July 2015,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ROBOT.2000.844830,Self-learning vision-guided robots for searching and grasping objects,IEEE,Conferences,"An approach to control vision-guided robots is introduced. It allows searching and grasping differently shaped objects that may be located anywhere in the robot's work space, even not visible in the initial fields of view of cameras. It eliminates the need for a calibration of the robot and of the vision system, it uses no world coordinates and no inverse perspective or kinematic transformations, and it comprises an automatic adaptation to changing parameters. The approach has been implemented on a calibration-free vision-guided manipulator with five degrees of freedom (DOF) and was evaluated in real-word experiments.",https://ieeexplore.ieee.org/document/844830/,Proceedings 2000 ICRA. Millennium Conference. IEEE International Conference on Robotics and Automation. Symposia Proceedings (Cat. No.00CH37065),24-28 April 2000,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/INDICON.2016.7839069,Selfie continuous sign language recognition using neural network,IEEE,Conferences,"This works objective is to bring sign language closer to real time implementation on mobile platforms with a video database of Indian sign language created with a mobile front camera in selfie mode. Pre-filtering, segmentation and feature extraction on video frames creates a sign language feature space. Artificial Neural Network classifier on the sign feature space are trained with feed forward nets and tested. ASUS smart phone with 5M pixel front camera captures continuous sign videos containing on average of 220 frames for 18 single handed signs at a frame rate of 30fps. Sobel edge operator's power is enhanced with morphology and adaptive thresholding giving a near perfect segmentation of hand and head portions. Word matching score (WMS) gives the performance of the proposed method with an average WMS of around 90% for ANN with an execution time of 0.5221 seconds during classification. Fully novel method of implementing sign language to put sign language recognition systems on smart phones to make it a real time usage application.",https://ieeexplore.ieee.org/document/7839069/,2016 IEEE Annual India Conference (INDICON),16-18 Dec. 2016,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SaCoNeT.2018.8585543,Semantic Networks Based Approach for SaaS Management in Cloud Computing,IEEE,Conferences,"Nowadays, Cloud Computing has emerged as a new model for hosting, managing and delivering services (IaaS, PaaS, SaaS) over the Internet. Enterprises and vendors are continuously migrating their services to the Cloud, this resulted an exponential amount of data, services and resources stored in data centers. Since there is no standard about Cloud service description or publication, therefore a key requirement that every Cloud provider needs to take into consideration is efficient management of resources and services by providing automated solutions, for a better service publication and discovery. The proposed approach aims to manage Cloud SaaS services for an efficient publication by classifying them into sets according to their domain to reduce the search space then interconnected the SaaS services of the same domain in a Semantic Network using the similarity measure (Input/Output similarity) between concepts. The proposed solution takes advantages from multidimensional index framework, WordNet Domain Ontology and semantic Web. A guided publication process and an implemented prototype are introduced validating the system using a real data set corpus.",https://ieeexplore.ieee.org/document/8585543/,2018 International Conference on Smart Communications in Network Technologies (SaCoNeT),27-31 Oct. 2018,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AERO47225.2020.9172454,Semi-Supervised Machine Learning for Spacecraft Anomaly Detection & Diagnosis,IEEE,Conferences,"This paper describes Anomaly Detection via Topological-feature Map (ADTM), a data-driven approach to Integrated System Health Management (ISHM) for monitoring the health of spacecraft and space habitats. Developed for NASA Ames Research Center, ADTM leverages proven artificial intelligence techniques for rapidly detecting and diagnosing anomalies in near real-time. ADTM combines Self-Organizing Maps (SOMs) as the basis for modeling system behavior with supervised machine learning techniques for localizing detected anomalies. A SOM is a two-layer artificial neural network (ANN) that produces a low-dimensional representation of the training samples. Once trained on normal system behavior, SOMs are adept at detecting behavior previously not encountered in the training data. Upon detecting anomalous behavior, ADTM uses a supervised classification approach to determine a subset of measurands that characterize the anomaly. This allows it to localize faults and thereby provide extra insight. We demonstrate the effectiveness of our approach on telemetry data collected from a lab-stationed CubeSat (the “LabSat”) connected to software that gave us the ability to trigger several real hardware faults. We include an analysis and discussion of ADTM's performance on several of these fault cases. We conclude with a brief discussion of future work, which contains investigation of a hierarchical SOM-architecture as well as a Case-Based Reasoning module for further assisting astronauts in diagnosis and remediation activities.",https://ieeexplore.ieee.org/document/9172454/,2020 IEEE Aerospace Conference,7-14 March 2020,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICIP.2019.8803816,Semi-Supervised Robust One-Class Classification in RKHS for Abnormality Detection in Medical Images,IEEE,Conferences,"Abnormality detection in medical images is a one-class classification problem for which typical methods use variants of kernel principal component analysis or one-class support vector machines. However, in practical deployment scenarios, many such methods are sensitive to the outliers present in the imperfectly-curated training sets. Current robust methods use heuristics for model fitting or lack formulations to leverage even a small amount of high-quality expert feedback. In contrast, we propose a novel method combining (i) robust statistical modeling, extending the multivariate generalized-Gaussian to a reproducing kernel Hilbert space, with (ii) semi-supervised learning to leverage a small expert-labeled outlier set. Results on simulated and real-world data, including endoscopy data, show that our method outperforms the state of the art in accurately detecting abnormalities.",https://ieeexplore.ieee.org/document/8803816/,2019 IEEE International Conference on Image Processing (ICIP),22-25 Sept. 2019,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICGEC.2010.33,Semi-supervised Kernel Based Progressive SVM,IEEE,Conferences,"Most existing semi-supervised methods implemented either the cluster assumption or the manifold assumption. The performance will degrade if the assumption was not proper for the data. A method was proposed by combining both the cluster assumption and the manifold assumption. A semi-supervised kernel which reflected geometric information of the samples was constructed through warping the Reproducing Kernel Hilbert Space. Then the semi-supervised kernel was used in SVM which was based on cluster assumption, and a progressive learning procedure was used in the proposed method. Experiments had been took on synthetic and real data sets, and the results showed that, compared with the progressive SVM with common kernel and the standard SVM with semi supervised kernel, the proposed method using semi-supervised kernel in progressive SVM had competitive performance.",https://ieeexplore.ieee.org/document/5715381/,2010 Fourth International Conference on Genetic and Evolutionary Computing,13-15 Dec. 2010,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SEAI55746.2022.9832166,Semi-supervised Video Deraining Based on Enhanced Spatio- Temporal Interaction Network,IEEE,Conferences,"Video rain removal is an important task in computer vision. Although the video rain removal method based on deep learning has achieved great success in recent years, there are still two main challenges: how to use the large amount of information between consecutive frames to extract cross-space and the powerful spatio-temporal characteristics of the time domain, and how to restore the use of high-speed methods to produce high-quality rain-removing videos. In this article, we propose a new end-to-end video rain removal framework called Enhanced Spatio-temporal Interaction Network (ESTINet), which greatly improves the current state-of-the-art video rain removal quality and speed. The enhanced spatio-temporal interaction network uses the advantages of deep residual networks and convolutional long and short-term memory to capture the spatial characteristics and temporal correlation between consecutive frames at the cost of few computing resources. In addition, Different prior formats are designed for labeled synthetic data and unlabeled real data, and a semi-supervised learning mechanism is designed through different prior formats.",https://ieeexplore.ieee.org/document/9832166/,2022 IEEE 2nd International Conference on Software Engineering and Artificial Intelligence (SEAI),10-12 June 2022,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SYNASC54541.2021.00037,Severity Prediction of Software Vulnerabilities based on their Text Description,IEEE,Conferences,"Software vulnerabilities represent a real challenge nowadays, often resulting in disruption of vital systems and data loss. Due to the multitude of software applications used within a company, system administrators often end up in the situation of facing multiple vulnerabilities at the same time, having no choice but to prioritize the most critical ones. Administrators commonly use vulnerability databases and metric systems to rank vulnerabilities; however, it usually takes from days to weeks for the metrics to be published since these metrics are established by human security analysts and the number of daily discovered exploits is constantly increasing. Therefore, newly discovered vulnerabilities, especially those without an available patch, represent the largest problem. In this paper, we propose a deep learning approach to predict the severity score and other metrics of a vulnerability using only its text description, which is available on discovery. We use a Multi-Task Learning architecture with a pre-trained BERT model for computing vector-space representations of words. Our best configuration achieves a mean absolute error of 0.86 for the severity score and an accuracy of 71.55&#x0025; for the severity level.",https://ieeexplore.ieee.org/document/9700266/,2021 23rd International Symposium on Symbolic and Numeric Algorithms for Scientific Computing (SYNASC),7-10 Dec. 2021,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/KIMAS.2003.1245110,Sharing learning policies between multiple mobile robots,IEEE,Conferences,"Learning of a complex task usually requires a long learning period. In order to reduce the time of learning, the task is divided into several subtasks. Multiple agents can be used to serve a complex task by learning these subtasks concurrently. With a good knowledge sharing mechanism, the learning policy can be shared or exchanged among these agents and can enhance their learning efficiency. The learning policy is a mapping from system states to actions. The mechanism of sharing or exchanging learning knowledge among multiagent system is proposed. An index of expertise, which indicates the skill level of each learning agent, is presented. This index is used to select the best preferable advice among multiple advices, which can increase the probability of finding solution in the search space. The experiment in which the learning knowledge is exchanged between a mobile robot and a computer simulated agent is implemented in order to verify the validity of the proposed algorithm. The experimental results show that the learning efficiency of the advisor agent is increased and the advisee robot can use the given advice for avoiding collision with obstacle successfully in the real world implementation.",https://ieeexplore.ieee.org/document/1245110/,IEMC '03 Proceedings. Managing Technologically Driven Organizations: The Human Side of Innovation and Change (IEEE Cat. No.03CH37502),30 Sept.-4 Oct. 2003,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SSPD.2017.8233263,Short Codes and Entanglement-Based Quantum Key Distribution via Satellite,IEEE,Conferences,"Quantum key distribution (QKD) provides the opportunity to deliver unconditional communication security. The most robust version of QKD relies on quantum entanglement. Very recently, ubiquitous deployment of such entanglement-based QKD over large distances has moved closer to reality, as verified by quantum entanglement distribution from a low Earth orbit satellite. We will demonstrate that this robust form of QKD via space will require a renewed focus on short-block length error-correcting codes in order to facilitate the reconciliation phase of the key distribution. Focusing on discrete variable QKD and adopting the low data rates consistent with measured entanglement distribution from space, we quantify the benefits of state-of-the-art short- block length codes in the context of device- independent QKD. Our results highlight the trade- off between the attainable key throughput vs the communication latency encountered in space-based implementations of this ultra-secure technology.",https://ieeexplore.ieee.org/document/8233263/,2017 Sensor Signal Processing for Defence Conference (SSPD),6-7 Dec. 2017,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/LARS-SBR-WRE48964.2019.00060,Sim-to-Real in Reinforcement Learning for Everyone,IEEE,Conferences,"In reinforcement learning (RL), it remains a challenge to have a robotic agent perform a task in the real world for which it was trained in simulation. In this paper, we present our work training a low-cost robotic arm in simulation to move towards a predefined target in space, represented by a red ball in an RGB image, and transferring the capability to the real arm. We exercised the entire end-to-end flow including the 3D modeling of the arm, training of a state-of-the-art RL policy in simulation with multiple actors in a distributed fashion, domain randomization in order to close the sim-to-real gap, and finally the execution of the trained model in the real robot. We also implemented a mechanism to edit the image captured from the camera before sending it to the model for inference, which allowed us to automate reward computation in the physical world. Our work highlights important challenges of training RL agents and moving them to the real world, validating important aspects shown by other works as well as detailing steps not explained by some of them (e.g. how to compute the reward in the real world). The conducted experiments show the improvements observed as the techniques were added to the final solution.",https://ieeexplore.ieee.org/document/9018558/,"2019 Latin American Robotics Symposium (LARS), 2019 Brazilian Symposium on Robotics (SBR) and 2019 Workshop on Robotics in Education (WRE)",23-25 Oct. 2019,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SoutheastCon48659.2022.9764110,Sim2real: Issues in transferring autonomous driving model from simulation to real world,IEEE,Conferences,"In this research we investigate the issue of sim2real performance, which is the problem that occurs when results from training a model in simulation do not carry over to comparable real world results. We train an Amazon Web Services DeepRacer car using DeepRacer-for-Cloud software to navigate a simulated oval track. We then test the DeepRacer car on a real-world track. We consider different action spaces, different reward functions and different values of entropy (exploration) during training to see which gives the best real-world performance. Our results show that the simpler action space, simpler reward and smaller entropy give the best sim2real performance.",https://ieeexplore.ieee.org/document/9764110/,SoutheastCon 2022,26 March-3 April 2022,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CISP.2009.5303973,Simple Ensemble of Extreme Learning Machine,IEEE,Conferences,"In this paper, a novel approach for neural network ensemble called Simple Ensemble of Extreme Learning Machine (SE-ELM) is proposed. It is proved theoretically in this study that the generalization ability of an ensemble is determined by the diversity of its components' output space. Therefore SE-ELM regards the diversity of components' output space as a target during the training process. In the first phase, SE-ELM initializes each component with different input weights and analytically determines the output weights through generalized inverse operation of the hidden layer output matrices. The difference among components' input weights forces those components to have different output space thus increasing the diversity of the ensemble. Experiments carried on four real world problems show that SE-ELM not only runs much faster but also presents better generalization performance than some classic ensemble algorithms.",https://ieeexplore.ieee.org/document/5303973/,2009 2nd International Congress on Image and Signal Processing,17-19 Oct. 2009,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICAIIT.2019.8834627,Simple Implementation of Fuzzy Controller for Low Cost Microcontroller,IEEE,Conferences,"Fuzzy logic inference system requires a long enough computational process, starting from fuzzification, matching rule base, drawing conclusions and defuzzification. By using C programming, fuzzy logic systems can be carried out by a low capability microcontrollers, for example PIC16F877A that has a flash memory capacity of 8KB and the maximum clock speed is 20 MHz. But if it will be applied to a microcontroller that has a smaller size of flash memory, there is a possibility that it cannot be realized, especially if it uses the gaussian membership function and each membership function inputs was devided in to more than three memberships. To overcome this problem we need a method to accelerate the fuzzy logic computation process. One of that method is using a linear interpolation from the table of input-output fuzzy system. In this paper, the linear interpolation was illustrated by using bilinear fuzzy interpolation. Based on the GNU Octave simulation, it was obtained data that the time being needed to complete the simulation for fuzzy bilinear Look Up Table has 7 times faster than the fuzzy system computation, whereas from the real implementation, it was obtained that fuzzy bilinear Look Up Table gives 2 times faster and can save program memory space around 25% compared to ordinary fuzzy system.",https://ieeexplore.ieee.org/document/8834627/,2019 International Conference of Artificial Intelligence and Information Technology (ICAIIT),13-15 March 2019,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IJCNN.2000.859462,"Simulating the evolution of 2D pattern recognition on the CAM-Brain Machine, an evolvable hardware tool for building a 75 million neuron artificial brain",IEEE,Conferences,"This paper presents some simulation results of the evolution of 2D visual pattern recognizers to be implemented very shortly on real hardware, namely the ""CAM-Brain Machine"" (CBM), an FPGA based piece of evolvable hardware which implements a genetic algorithm (GA) to evolve a 3D cellular automata (CA) based neural network circuit module, of approximately 1,000 neurons, in about a second, i.e. a complete run of a GA, with tens of thousands of circuit growths and performance evaluations. Up to 65,000 of these modules, each of which is evolved with a humanly specified function, can be downloaded into a large RAM space, and interconnected according to humanly specified artificial brain architectures. This RAM, containing an artificial brain with up to 75 million neurons, is then updated by the CBM at a rate of 130 billion CA cells per second. Such speeds will enable real time control of robots and hopefully the birth of a new research field that we call ""brain building"". The first such artificial brain, to be built at STARLAB in 2000 and beyond, will be used to control the behaviors of a life sized kitten robot called ""Robokitty"". This kitten robot will need 2D pattern recognizers in the visual section of its artificial brain. This paper presents simulation results on the evolvability and generalization properties of such recognizers.",https://ieeexplore.ieee.org/document/859462/,Proceedings of the IEEE-INNS-ENNS International Joint Conference on Neural Networks. IJCNN 2000. Neural Computing: New Challenges and Perspectives for the New Millennium,27-27 July 2000,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ITAIC49862.2020.9338925,Simulation Modeling for Inertial Work-pattern of UAV Optoelectronic Gimbals,IEEE,Conferences,"At present, the simulation of photoelectric gimbals is only semi physical simulation, which can not meet the training requirements of full digital simulation. Based on working principle of the optoelectronic gimbals, a two-freedom-degrees optoelectronic gimbals model composed of inner ring, outer ring and bracket was built. Based on direction cosine theory and quasi Newton numerical iterative technique, for the actual equipment of the optoelectronic gimbals, the inertial work-pattern simulating & modeling algorithm (IWPSMA) were proposed in this projection, which to keep the axis stable in the inertial space. The model and IWPSMA were simulated, with the UAV moving in a circle, the azimuth changes periodically in 360°, and the elevation angle change nonlinearly in 0°-35°. The turntable model and algorithm are loaded into the 3D simulation scene. The experimental results show that the fidelity of the model and the real equipment are high, the boresight axis of the model keeps relatively stable in the inertial space, the computer resource occupation rate is low, and the 3D picture is smooth and stable.",https://ieeexplore.ieee.org/document/9338925/,2020 IEEE 9th Joint International Information Technology and Artificial Intelligence Conference (ITAIC),11-13 Dec. 2020,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ITAIC54216.2022.9836889,Simulation of dry-type air-core reactor under strand-to-strand short circuit fault,IEEE,Conferences,"With the development of power industry, dry-type air core reactor has been more and more widely used, and more and more safety accidents are caused by its faults. In theory, the dry-type air core reactor with multi strand parallel winding can effectively avoid the occurrence of inter turn short circuit fault, but there are still faults in the actual operation, so there is a lack of research on the inter strand short circuit fault of the reactor. In order to study the influence of inter strand short circuit fault on reactor, a field circuit coupling finite element simulation model was established to simulate the spatial magnetic field and current distribution characteristics of reactor under normal operation and inter strand short circuit fault. The results show that the working state of the reactor hardly changes when the same turn inter strand short circuit fault occurs; the different turn inter strand short circuit fault will significantly affect the space magnetic field near the short circuit turn, and the closer the short circuit point is to the end, the more serious the fault is; the different turn inter strand short circuit fault will significantly change the current of the reactor, especially the real part current. The research work fully reveals the change of operation characteristics of reactor under inter strand short circuit fault, and provides a reference for the occurrence and diagnosis of reactor fault.",https://ieeexplore.ieee.org/document/9836889/,2022 IEEE 10th Joint International Information Technology and Artificial Intelligence Conference (ITAIC),17-19 June 2022,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CCECE.2003.1226239,Simulation of three-rate neural network control for stochastic model of a fighter aircraft,IEEE,Conferences,"The nature of the multirate dynamics of the process makes it very attractive for applications, since the multirate phenomena are complex. This paper presents a research methodology for describing a three-rate continuous-time stochastic control system as state-space (SS) type decentralized models of multiinput/multioutput (MIMO) stochastic control subsystems with the three neural networks (NNs) of different structure. The block diagram of the decentralized control subsystems with the radial basis control NNs for the ""fast"" and ""intermediate"" subsystems, and with one linear layer control NN for the ""slow"" subsystem is designed. An illustrative design example - NN control of an experimental fighter aircraft model in the short approach to landing task - was carried out using the proposed three-rate SS decentralization technique. The simulation results with use of software package Simulink demonstrate that this research technique would work for real-time MIMO stochastic systems.",https://ieeexplore.ieee.org/document/1226239/,CCECE 2003 - Canadian Conference on Electrical and Computer Engineering. Toward a Caring and Humane Technology (Cat. No.03CH37436),4-7 May 2003,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ITME53901.2021.00044,Situation Prediction of Fire Management System Based on BP Neural Network,IEEE,Conferences,"This paper introduces the principle of BP Neural Network to achieve Training algorithm, which work with Hopfield neural network associative memory to achieve prediction fire in the semi-closed space of the community. The BP Neural Network is regarded as a nonlinear mapping from input to output. Based on the BP neural network algorithm by the software monitoring technology obtain the prediction model which predict the output value is closed to the real value The high effectiveness of Artificial Neural Network is verified by the comparison of specific field simulation. Once the fire happened, People can get fire of extinguishing materials in time by the color of the smoke and fire emitted to prevent the expansion of the fire. Consequently, the fire disasters can be predicted and prevented through the pattern of the fire model.",https://ieeexplore.ieee.org/document/9750579/,2021 11th International Conference on Information Technology in Medicine and Education (ITME),19-21 Nov. 2021,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IJCNN.2019.8851808,Skip-GANomaly: Skip Connected and Adversarially Trained Encoder-Decoder Anomaly Detection,IEEE,Conferences,"Despite inherent ill-definition, anomaly detection is a research endeavour of great interest within machine learning and visual scene understanding alike. Most commonly, anomaly detection is considered as the detection of outliers within a given data distribution based on some measure of normality. The most significant challenge in real-world anomaly detection problems is that available data is highly imbalanced towards normality (i.e. non-anomalous) and contains at most a sub-set of all possible anomalous samples - hence limiting the use of well-established supervised learning methods. By contrast, we introduce an unsupervised anomaly detection model, trained only on the normal (non-anomalous, plentiful) samples in order to learn the normality distribution of the domain, and hence detect abnormality based on deviation from this model. Our proposed approach employs an encoder-decoder convolutional neural network with skip connections to thoroughly capture the multi-scale distribution of the normal data distribution in image space. Furthermore, utilizing an adversarial training scheme for this chosen architecture provides superior reconstruction both within image space and a lower-dimensional embedding vector space encoding. Minimizing the reconstruction error metric within both the image and hidden vector spaces during training aids the model to learn the distribution of normality as required. Higher reconstruction metrics during subsequent test and deployment are thus indicative of a deviation from this normal distribution, hence indicative of an anomaly. Experimentation over established anomaly detection benchmarks and challenging real-world datasets, within the context of X-ray security screening, shows the unique promise of such a proposed approach.",https://ieeexplore.ieee.org/document/8851808/,2019 International Joint Conference on Neural Networks (IJCNN),14-19 July 2019,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICWAPR.2007.4421653,Small-shaped space target recognition based on wavelet decomposition and support vector machine,IEEE,Conferences,"A kind of method for small-shaped space target recognition was proposed in this paper based on feature extraction with wavelet decomposition and formative support vector machine (FSVM) with sequential minimal optimization (SMO) algorithm. Firstly, the significance and characteristics of space target recognition were discussed and a two-stage recognition strategy was designed. And then aiming at the characteristics of small-shaped space target recognition, a new method was implemented based on feature extraction with wavelet decomposition and FSVM with SMO algorithm. Simulation results show the good performance of the algorithm proposed in this paper: the correct rate is more than 97% within 1360 simulation samples of ten classes of small shaped space targets; meanwhile the algorithm is characterized with high speed of near real time in both implementation of training and testing.",https://ieeexplore.ieee.org/document/4421653/,2007 International Conference on Wavelet Analysis and Pattern Recognition,2-4 Nov. 2007,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AERO47225.2020.9172439,"Smart & Integrated Management System - Smart Cities, Epidemiological Control Tool Using Drones",IEEE,Conferences,"This paper describes the development of a real application using Drones over urban regions to help the authorities at epidemiological control through a disruptive solutions based on a customizable Smart & Integrated Management System (SIGI), devices and software based on the Enterprise Resource Planning (ERP) concept. Compound by management software, Drones and specific IoT devices, both referred to as sensors, the sensors collect the data of the interest areas in real time, creating a specified database. Based on the data collected from the interest areas, SIGI software has the ability to show real-time situational analysis of these areas and allows that the administrator can optimize resources (material and human) improving the efficiency of resource allocation in these areas. In addition to the development of the management software, the development of sensors to collect the information in the field and update these information to the database of the management software, are considered. The sensors will be recognized as IoT devices for the collection of meteorological data, images and command / control Drones. Initially the system will be customized, using an Artificial Intelligence tool, to collect data and identify the outbreaks of the dengue mosquito, zika and Chikungunya, nominee by risk areas. After the definition of the potential risk areas, in a complementary way, a totally customized Drone will be used to map these areas of interest, generating aerial photographs, identifying and geotagging the potential “targets”, which will allow the agents to identify potential mosquito breeding sites. After the identification of breeding areas, the next step will be the effective combat of the vectors, using the Drones to fly over the areas of interest, where biological defenses will be “dropped” over the targets to combat mosquitoes. Due some Drone flight restrictions over the cities, the whole process will be monitored by a situation room, that will be able to control the Drone remotely, access the air space controller, reads the sensors installed in the city (field), that will measure, for example, rainfall through weather stations installed in risk areas and subsequently processed by Intelligent System Integrated Management (SIGI), which will result to the information public official reflecting the situational analysis of the areas, which will enable a better management of available resources, helping the public agent, preventively in the decision making.",https://ieeexplore.ieee.org/document/9172439/,2020 IEEE Aerospace Conference,7-14 March 2020,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/UEMCON.2018.8796749,Smart City Software Revolution - Blackboard Systems for Smart City Solutions,IEEE,Conferences,"This paper examines the possibilities and requirements of using the blackboard software system in the context of a smart city. Using Unreal Engine 4, a virtual smart city was developed to test the capabilities and difficulties which would accompany the use of the blackboard. A number of simulations were run to generate insight regarding the viability of implementing the blackboard in a real-world setting. The tasks that the blackboard attempted addressed the following goals: a weather reaction system; a parking space availability system; and a daylight reaction system. An autonomous car running on preset routes was implemented in two separate ways to compare the performance between a blackboard system and blueprints.",https://ieeexplore.ieee.org/document/8796749/,"2018 9th IEEE Annual Ubiquitous Computing, Electronics & Mobile Communication Conference (UEMCON)",8-10 Nov. 2018,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/BigComp51126.2021.00051,Smart Energy Management System based on Reconfigurable AI Chip and Electrical Vehicles,IEEE,Conferences,"Almost every larger city in Europe has ambitious smart city projects. This is particularly true for Hamburg, a Hanseatic city in the north of Germany. Hamburg is the smartest city in Germany according to a Federal Association for Information Technology. Although there are no megacities in the European Union (the largest city in the European Union is Berlin with 3.7 million inhabitants), the increasing urbanization is apparent and produces problems to be solved. At the same time rural depopulation creates conjugated problems.One category of these problems is mobility. Mobility can be regarded as the need to move persons and freight. In densely populated cities an increasing amount of transport users have to share a decreasing amount of space with conflicting needs. At the same time in rural areas, a dwindling supply of local public transport makes the mobility of the remaining residents more difficult. The same applies to parcel delivery or the supply of goods. Autonomous systems have great potential to create a sustainable and livable environment. The author has initiated a publicly funded project to investigate technologies of autonomous mobile systems which interact with a smart city. The test area intelligent urban mobility (Testfeld intelligente Quartiersmobilitat) at the campus of Hamburgs University of Applied Sciences is created to do research on connected and autonomous mobile systems like multipurpose robots and other mobility users like pedestrians with a smartphone. A particular focus is on neighborhood mobility. This means that distances of less than 3 kilometers usually have to be covered. The special type of needs in neighborhood mobility has two important aspects that affect development of autonomous mobile systems: It is slow mobility and the transport users are especially vulnerable. The acceptance of the residents of autonomous systems is equally important, as is the protection of privacy when collecting environmental data. They are expected to make decisions on their own in complex environments. The real world usually differs from a simulation or an experimental setup in a laboratory - a problem commonly referred to as Sim-2-Real gap. Active and non-destructive exploration is expected from an autonomous system to solve unexpected problems. Machine learning methods come into play which in turn have their own pitfalls. The author has built a specialized laboratory to investigate machine learning technology applied to autonomous systems. In this laboratory miniature autonomous vehicles are developed. The general idea of this experimental setup allows research on new methodologies for autonomous systems in a very small scale",https://ieeexplore.ieee.org/document/9373129/,2021 IEEE International Conference on Big Data and Smart Computing (BigComp),17-20 Jan. 2021,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ECICE50847.2020.9302010,Smart Lock Security System Based on Artificial Internet of Things,IEEE,Conferences,"Artificial intelligence and Internet of Things (AIoT) technology were applied to develop an integrated system for remote space authentification and power management. The system is applied to various commercial uses such as the control of mobile or cloud authorization in buildings and the management of shared offices or hospitals. To develop the proposed system, AIoT smart lock with low power consumption was designed to control the access and manage the power consumption of other IoT devices effectively in the venue. AIoT communication firmware and devices were developed to enable smart control and management. Finally, an energy management system helps enterprises monitor the power consumption of devices in real-time and grant authorization for device monitoring and management. The proposed system sends monitoring data to a cloud server and enables the managing of commercial activities and unmanned venues. Through hardware and software integration, this study created the AIoT smart lock, promoted the use of IoT devices in relevant industries, and enhanced the competitiveness of product research and development.",https://ieeexplore.ieee.org/document/9302010/,"2020 IEEE Eurasia Conference on IOT, Communication and Engineering (ECICE)",23-25 Oct. 2020,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS51168.2021.9636018,Smart Pointers and Shared Memory Synchronisation for Efficient Inter-process Communication in ROS on an Autonomous Vehicle,IEEE,Conferences,"Despite the stringent requirements of a real-time system, the reliance of the Robot Operating System (ROS) on the loopback network interface imposes a considerable overhead on the transport of high bandwidth data, while the nodelet package, which is an efficient mechanism for intra-process communication, does not address the problem of efficient local inter-process communication (IPC). To remedy this, we propose a novel integration into ROS of smart pointers and synchronisation primitives stored in shared memory. These obey the same semantics and, more importantly, exhibit the same performance as their C++ standard library counterparts, making them preferable to other local IPC mechanisms. We present a series of benchmarks for our mechanism - which we call LOT (Low Overhead Transport) - and use them to assess its performance on realistic data loads based on Five’s Autonomous Vehicle (AV) system, and extend our analysis to the case where multiple ROS nodes are running in Docker containers. We find that our mechanism performs up to two orders of magnitude better than the standard IPC via local loopback. Finally, we apply industry-standard profiling techniques to explore the hotspots of code running in both user and kernel space, comparing our implementation against alternatives.",https://ieeexplore.ieee.org/document/9636018/,2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),27 Sept.-1 Oct. 2021,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.23919/FRUCT.2012.8253108,Smart space logistic service for real-time ridesharing,IEEE,Conferences,The paper describes a logistic service-based approach to real-time ridesharing based on smartspace concept. Smart-M3 information platform is used as smart space infrastructure for presented approach. The service is based on Smart-M3 RDF ontology which is formed by ontology slices of participants' mobile devices. The paper presents an algorithm for finding appropriate fellow- travelers for drivers as well as definition of acceptable pick-up and drop-off points for them.,https://ieeexplore.ieee.org/document/8253108/,2012 11th Conference of Open Innovations Association (FRUCT),23-27 April 2012,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCD.1995.528945,Smart-pixel array processors based on optimal cellular neural networks for space sensor applications,IEEE,Conferences,"A smart-pixel cellular neural network with hardware annealing capability, digitally programmable synaptic weights, and multisensor parallel interface has been under development for advanced space sensor applications. The smart-pixel CNN architecture is a programmable multi-dimensional array of optoelectronic neurons which are locally connected with their local neurons and associated active-pixel sensors. Integration of the neuroprocessor in each processor node of a scalable multiprocessor system offers orders-of-magnitude computing performance enhancements for on-board real-time intelligent multisensor processing and control tasks of advanced small satellites. The smart-pixel CNN operation theory, architecture, design and implementation, and system applications are investigated in detail. The VLSI implementation feasibility was illustrated by a prototype smart-pixel 5/spl times/5-neuroprocessor array chip of active dimensions 1380 /spl mu/m/spl times/746 /spl mu/m in a 2-/spl mu/m CMOS technology.",https://ieeexplore.ieee.org/document/528945/,Proceedings of ICCD '95 International Conference on Computer Design. VLSI in Computers and Processors,2-4 Oct. 1995,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/PerCom53586.2022.9762405,SmartSPEC: Customizable Smart Space Datasets via Event-driven Simulations,IEEE,Conferences,"This paper presents SmartSPEC, an approach to generate customizable smart space datasets using sensorized spaces in which people and events are embedded. Smart space datasets are critical to design, deploy and evaluate robust systems and applications to ensure cost-effective operation and safety/-comfort/convenience of the space occupants. Often, real-world data is difficult to obtain due to the lack of fine-grained sensing; privacy/security concerns prevent the release and sharing of individual and spatial data. SmartSPEC is a smart space simulator and data generator that can create a digital representation (twin) of a smart space and its activities. SmartSPEC uses a semantic model and ML-based approaches to characterize and learn attributes in a sensorized space, and applies an event-driven simulation strategy to generate realistic simulated data about the space (events, trajectories, sensor datasets, etc). To evaluate the realism of the data generated by SmartSPEC, we develop a structured methodology and metrics to assess various aspects of smart space datasets, including trajectories of people and occupancy of spaces. Our experimental study looks at two real-world settings/datasets: an instrumented smart campus building and a city-wide GPS dataset. Our results show that the trajectories produced by SmartSPEC are 1.4x to 4.4x more realistic than the best synthetic data baseline when compared to real-world data, depending on the scenario and configuration.",https://ieeexplore.ieee.org/document/9762405/,2022 IEEE International Conference on Pervasive Computing and Communications (PerCom),21-25 March 2022,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SERE-C.2014.24,Software Reliability Virtual Testing for Reliability Assessment,IEEE,Conferences,"The basic condition of software reliability assessment is failure time, which must be acquired during a test based on operational profile or on real usage. Failure data from software development or other non-software reliability testing (SRT) cannot be used for reliability evaluation because such data do not include usage information and failure time. This paper presents a software reliability virtual test (SRVT), which constructs the software input space model and the known failure input space model through which possible failure time can be determined by matching the randomly generate inputs. An experiment comparing SRT and SRVT with different thresholds is introduced to verify SRVT. Results indicate that SRVT saves a large amount of testing time while providing reliability assessment with acceptable accuracy.",https://ieeexplore.ieee.org/document/6901643/,2014 IEEE Eighth International Conference on Software Security and Reliability-Companion,30 June-2 July 2014,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/WICT.2012.6409060,Software effort prediction using unsupervised learning (clustering) and functional link artificial neural networks,IEEE,Conferences,"Software cost estimation continues to be an area of concern for managing of software development industry. We use unsupervised learning (e.g., clustering algorithms) combined with functional link artificial neural networks for software effort prediction. The unsupervised learning (clustering) indigenously divide the input space into the required number of partitions thus eliminating the need of ad-hoc selection of number of clusters. Functional link artificial neural networks (FLANNs), on the other hand is a powerful computational model. Chebyshev polynomial has been used in the FLANN as a choice for functional expansion to exhaustively study the performance. Three real life datasets related to software cost estimation have been considered for empirical evaluation of this proposed method. The experimental results show that our method could significantly improve prediction accuracy of conventional FLANN and has the potential to become an effective method for software cost estimation.",https://ieeexplore.ieee.org/document/6409060/,2012 World Congress on Information and Communication Technologies,30 Oct.-2 Nov. 2012,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/GrC.2010.13,Solving Course Timetabling Problem Using Interrelated Approach,IEEE,Conferences,"University timetabling is very hectic resources allocation job against tough constraints. The problem is broadly recognized on account of its crucial significance for curriculum activities. Its intensive complexity has challenged the researchers from diverse disciplines for several decades. In the research paper, a novel interrelated approach is employed that primarily depends on Genetic Algorithm supported by Local Search algorithm. Local Search systematizes the events in each timetabling chromosome up to certain degree. Later on GA is likely to obtain more feasible solution available on the search space. The approach has been applied on real dataset and the research direction is validated by promising outcome. The bottom line is minimizing computational time for GA by initializing the set of partial solutions. In addition, exploitation of the resources usage and effective events deployment are key objectives.",https://ieeexplore.ieee.org/document/5576024/,2010 IEEE International Conference on Granular Computing,14-16 Aug. 2010,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICTAI.2017.00154,Some Neighbourhood Approaches for the Antenna Positioning Problem,IEEE,Conferences,"The problem of positioning antennas in cellular networks is a known problem in the field of telecommunications. It consists of selecting from a set of candidate sites, the best locations to install the base stations in order to maximize the network coverage while minimizing the number of the used stations. In theory, the problem is NP-hard. To solve it in practice, we propose in this work the adaptation of two metaheuristics based on the local search that are the Iterated Local Search (ILS) and the Breakout Local Search (BLS) and provide a new algorithm inspired from both the ILS and the BLS algorithms. The latter is based on a local search process and a perturbation in the exploration of the search space. It is distinguished by its mechanism of reinitialization of the search and by its way of generating a new starting solution. To validate our approach, we have implemented, tested and compared these algorithms to several other methods on a real instance of the problem. The experimental results obtained show that the proposed approach improves the performance of these methods in most of the cases.",https://ieeexplore.ieee.org/document/8372057/,2017 IEEE 29th International Conference on Tools with Artificial Intelligence (ICTAI),6-8 Nov. 2017,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SSCI50451.2021.9660133,Space and Time Efficiency Analysis of Data-Driven Methods Applied to Embedded Systems,IEEE,Conferences,"One of the applications of data-driven methods in the industry is the creation of real-time, embedded measurements, whether to monitor or replace sensor signals. As the number of embedded systems in products raises over time, the energy efficiency of such systems must be considered in the design. The time (processor) efficiency of the embedded software is directly related to the energy efficiency of the embedded system. Therefore, when considering some embedded software solutions, such as data-driven methods, time efficiency must be taken into account to improve energy efficiency. In this work, the energy efficiency of three data-driven methods: the Sparse Identification of Nonlinear Dynamics (SINDy), the Extreme Learning Machine (ELM), and the Random-Vector Functional Link (RVFL) network were assessed by using the creation of a real-time in-cylinder pressure sensor for diesel engines as a task. The three methods were kept with equivalent performances, whereas their relative execution time was tested and classified by their statistical rankings. Additionally, the space (memory) efficiency of the methods was assessed. The contribution of this work is to provide a guide to choose the best data-driven method to be used in an embedded system in terms of efficiency.",https://ieeexplore.ieee.org/document/9660133/,2021 IEEE Symposium Series on Computational Intelligence (SSCI),5-7 Dec. 2021,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICACI.2018.8377475,Space-time constrained optimization for deep locomotion controller,IEEE,Conferences,"Physics-based methods synthesize motion for virtual characters following physics principles in real world. Given the high dimensions and continuity of joint actuation, designing a controller for virtual characters is a challenging task. Existing methods normally construct the controller based on Finite State Machine, which is a manual process and requires expert knowledge. This paper uses deep neural network, as the locomotion controller, to control the motion of virtual characters. The network learning is conducted with the Deep Deterministic Policy Gradient. We propose to integrate space-time constraints, as part of the reward function, during the learning process. The experiment results confirm that the introduction of the space-time constraints avoids the problem of generating awkward gait (as observed in existing methods).",https://ieeexplore.ieee.org/document/8377475/,2018 Tenth International Conference on Advanced Computational Intelligence (ICACI),29-31 March 2018,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/INFOCOMWKSHPS50562.2020.9162585,Sparser: Secure Nearest Neighbor Search with Space-filling Curves,IEEE,Conferences,"Nearest neighbor search, a classic way of identifying similar data, can be applied to various areas, including database, machine learning, natural language processing, software engineering, etc. Secure nearest neighbor search aims to find nearest neighbors to a given query point over encrypted data without accessing data in plaintext. It provides privacy protection to datasets when nearest neighbor queries need to be operated by an untrusted party (e.g., a public server). While different solutions have been proposed to support nearest neighbor queries on encrypted data, these existing solutions still encounter critical drawbacks either in efficiency or privacy. In light of the limitations in the current literature, we propose a novel approximate nearest neighbor search solution, referred to as Sparser, by leveraging a combination of space-filling curves, perturbation, and Order-Preserving Encryption. The advantages of Sparser are twofold, strengthening privacy and improving efficiency. Specifically, Sparser pre-processes plaintext data with space-filling curves and perturbation, such that data is sparse, which mitigates leakage abuse attacks and renders stronger privacy. In addition to privacy enhancement, Sparser can efficiently find approximate nearest neighbors over encrypted data with logarithmic time. Through extensive experiments over real-world datasets, we demonstrate that Sparser can achieve strong privacy protection under leakage abuse attacks and minimize search time.",https://ieeexplore.ieee.org/document/9162585/,IEEE INFOCOM 2020 - IEEE Conference on Computer Communications Workshops (INFOCOM WKSHPS),6-9 July 2020,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/RAICS.2013.6745450,Sparsity-based representation for categorical data,IEEE,Conferences,"Over the past few decades, many algorithms have been continuously evolving in the area of machine learning. This is an era of big data which is generated by different applications related to various fields like medicine, the World Wide Web, E-learning networking etc. So, we are still in need for more efficient algorithms which are computationally cost effective and thereby producing faster results. Sparse representation of data is one giant leap toward the search for a solution for big data analysis. The focus of our paper is on algorithms for sparsity-based representation of categorical data. For this, we adopt a concept from the image and signal processing domain called dictionary learning. We have successfully implemented its sparse coding stage which gives the sparse representation of data using Orthogonal Matching Pursuit (OMP) algorithms (both Batch and Cholesky based) and its dictionary update stage using the Singular Value Decomposition (SVD). We have also used a preprocessing stage where we represent the categorical dataset using a vector space model based on the TF-IDF weighting scheme. Our paper demonstrates how input data can be decomposed and approximated as a linear combination of minimum number of elementary columns of a dictionary which so formed will be a compact representation of data. Classification or clustering algorithms can now be easily performed based on the generated sparse coded coefficient matrix or based on the dictionary. We also give a comparison of the dictionary learning algorithm when applying different OMP algorithms. The algorithms are analysed and results are demonstrated by synthetic tests and on real data.",https://ieeexplore.ieee.org/document/6745450/,2013 IEEE Recent Advances in Intelligent Computational Systems (RAICS),19-21 Dec. 2013,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICDE53745.2022.00269,Spatial-Temporal Hypergraph Self-Supervised Learning for Crime Prediction,IEEE,Conferences,"Crime has become a major concern in many cities, which calls for the rising demand for timely predicting citywide crime occurrence. Accurate crime prediction results are vital for the beforehand decision-making of government to alleviate the increasing concern about the public safety. While many efforts have been devoted to proposing various spatial-temporal forecasting techniques to explore dependence across locations and time periods, most of them follow a supervised learning manner, which limits their spatial-temporal representation ability on sparse crime data. Inspired by the recent success in self-supervised learning, this work proposes a Spatial-Temporal Self-Supervised Hypergraph Learning framework (ST-HSL) to tackle the label scarcity issue in crime prediction. Specifically, we propose the cross-region hypergraph structure learning to encode region-wise crime dependency under the entire urban space. Furthermore, we design the dual-stage self-supervised learning paradigm, to not only jointly capture local- and global-level spatial-temporal crime patterns, but also supplement the sparse crime representation by augmenting region self-discrimination. We perform extensive experiments on two real-life crime datasets. Evaluation results show that our ST-HSL significantly outperforms state-of-the-art baselines. Further analysis provides insights into the superiority of our ST-HSL method in the representation of spatial-temporal crime patterns. The implementation code is available at https://github.com/LZH-YS1998/STHSL.",https://ieeexplore.ieee.org/document/9835423/,2022 IEEE 38th International Conference on Data Engineering (ICDE),9-12 May 2022,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/GRC.2005.1547267,Spatio-temporal granular logic and its applications to dynamic information systems,IEEE,Conferences,"The main research content of the paper is a granular logic with meaning of space and time. Temporal operators Until (U) and Since (S) are studied. U and S are introduced into the granular logic, to have a temporal granular logic. Subsequently, a spatio-temporal changing function /spl pi/: X/spl times/T/spl rarr/X is proposed, which are functions of two arguments mapping a point in space and a point in time, to another a point in space. Where X is the state space, T is a time-lines. Temporal granular logic is defined in the state space, thus the spatio-temporal granular logic is constructed. Further, the deductive systems of the logic are discussed. Finally, the applications of the logic are illustrated with real examples.",https://ieeexplore.ieee.org/document/1547267/,2005 IEEE International Conference on Granular Computing,25-27 July 2005,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS.2014.6943162,Spatio-temporal motion features for laser-based moving objects detection and tracking,IEEE,Conferences,"This paper proposes a spatio-temporal motion feature detection and tracking method using range sensors working on a moving platform. The proposed spatio-temporal motion features are similar to optical flow but are extended on a moving platform with fusion of odometry and show much better classification accuracy with consideration of different uncertainties. In the proposal, the ego motion is compensated by odometry sensors and the laser scan points are accumulated and represented as space-time point clouds, from which the velocities and moving directions can be extracted. Based on these spatio-temporal features, a supervised learning technique is applied to classify the points as static or moving and Kalman filters are implemented to track the moving objects. A real experiment is performed during day and night on an autonomous vehicle platform and shows promising results in a crowded and dynamic environment.",https://ieeexplore.ieee.org/document/6943162/,2014 IEEE/RSJ International Conference on Intelligent Robots and Systems,14-18 Sept. 2014,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICNN.1994.374437,Spatiotemporal computation with a general purpose analog neural computer: real-time visual motion estimation,IEEE,Conferences,"An analog neural network implementation of spatiotemporal feature extraction for real-time visual motion estimation is presented. Visual motion can be represented as an orientation in the space-time domain. Thus, motion estimation translates into orientation detection. The spatiotemporal orientation detector discussed is based on Adelson and Bergen's model with modifications to accommodate the computational limitations of hardware analog neural networks. The analog neural computer used here has the unique property of offering temporal computational capabilities through synaptic time-constants. These time-constants are crucial for implementing the spatiotemporal filters. Analysis, implementation and performance of the motion filters are discussed. The performance of the neural motion filters is found to be consistent with theoretical predictions and the real stimulus motion.<>",https://ieeexplore.ieee.org/document/374437/,Proceedings of 1994 IEEE International Conference on Neural Networks (ICNN'94),28 June-2 July 1994,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/VLSID.2018.26,Special Session: Design of Energy-Efficient and Reliable VLSI Systems: A Data-Driven Perspective,IEEE,Conferences,"Summary form only given, as follows. The complete presentation was not made available for publication as part of the conference proceedings. The amount of data generated and collected across computing platforms every day is not only enormous, but growing at an exponential rate. Advanced data analytics and machine learning techniques have become increasingly essential to analyze and extract meaning from such “Big Data”. These techniques can be very useful to detect patterns and trends to improve the operational behavior of computing systems, but they also introduce a number of outstanding challenges: (1) How can we design and deploy data analytics mechanisms to improve energy-efficiency and reliability in IoT and mobile devices, without introducing significant software overheads? (2) How to leverage emerging technologies (e.g.,3D integration) to design energy-efficient and reliable manycore systems for big data computing? (3) How to use machine learning and data mining techniques for effective design space exploration of computing systems, and enable adaptive control to improve energy-efficiency? (4) How can data analytics detect anomalies and increase robustness in the network backbone of emerging large-scale networking systems? To address these outstanding challenges, out-of-the-box approaches need to be explored. In this special session, we will discuss these outstanding problems and describe far-reaching solutions applicable across the interconnected ecosystem of IoT and mobile devices, manycore chips, datacenters, and networks. The special session brings together speakers with unique insights on applying data analytics and machine learning to real-world problems to achieve the most sought after features on multi-scale computing platforms, viz. intelligent data mining, energyefficiency, and robustness. By integrating data analytics and machine learning algorithms, statistical modeling, embedded hardware and software design, and cloud computing content, this session will engage a broad section of Embedded and VLSI Design conference attendees. This special session is targeted towards university researchers/professors, students, industry professionals, and embedded/VLSI system designers. This session will attract newcomers who want to learn how to apply data analytics to solve problems in computing systems, as well as experienced researchers looking for exciting new directions in embedded systems, VLSI design, EDA algorithms, and multi-scale computing.",https://ieeexplore.ieee.org/document/8326889/,2018 31st International Conference on VLSI Design and 2018 17th International Conference on Embedded Systems (VLSID),6-10 Jan. 2018,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA48506.2021.9561078,Spherical Multi-Modal Place Recognition for Heterogeneous Sensor Systems,IEEE,Conferences,"In this paper, we propose a robust end-to-end multi-modal pipeline for place recognition where the sensor systems can differ from the map building to the query. Our approach operates directly on images and LiDAR scans without requiring any local feature extraction modules. By projecting the sensor data onto the unit sphere, we learn a multi-modal descriptor of partially overlapping scenes using a spherical convolutional neural network. The employed spherical projection model enables the support of arbitrary LiDAR and camera systems readily without losing information. Loop closure candidates are found using a nearest-neighbor lookup in the embedding space. We tackle the problem of correctly identifying the closest place by correlating the candidates’ power spectra, obtaining a confidence value per prospect. Our estimate for the correct place corresponds then to the candidate with the highest confidence. We evaluate our proposal w.r.t. state-of-the-art approaches in place recognition using real-world data acquired using different sensors. Our approach can achieve a recall that is up to 10% and 5% higher than for a LiDAR- and vision-based system, respectively, when the sensor setup differs between model training and deployment. Additionally, our place selection can correctly identify up to 95% matches from the candidate set.",https://ieeexplore.ieee.org/document/9561078/,2021 IEEE International Conference on Robotics and Automation (ICRA),30 May-5 June 2021,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISDA.2008.125,Spider Search: An Efficient and Non-Frontier-Based Real-Time Search Algorithm,IEEE,Conferences,"Real-time search algorithms are limited to constant-bounded search at each time step. We do not see much difference between standard search algorithms and good real-time search algorithms when problem sizes are small. However, having a good real-time search algorithm becomes important when problem sizes are large. In this paper we introduce a simple yet efficient algorithm, Spider search, which uses very low constant time and space to solve problems when agents need deep (but not exhaustive) path analysis at each step. We expect that Spider search is the first in a new class of tree-based rather than frontier-based search algorithms.",https://ieeexplore.ieee.org/document/4696381/,2008 Eighth International Conference on Intelligent Systems Design and Applications,26-28 Nov. 2008,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.23919/EUSIPCO.2019.8902815,"State Space Models with Dynamical and Sparse Variances, and Inference by EM Message Passing",IEEE,Conferences,"Sparse Bayesian learning (SBL) is a probabilistic approach to estimation problems based on representing sparsity-promoting priors by Normals with Unknown Variances. This representation blends well with linear Gaussian state space models (SSMs). However, in classical SBL the unknown variances are a priori independent, which is not suited for modeling group sparse signals, or signals whose variances have structure. To model signals with, e.g., exponentially decaying or piecewise-constant (in particular block-sparse) variances, we propose SSMs with dynamical and sparse variances (SSM-DSV). These are two-layer SSMs, where the bottom layer models physical signals, and the top layer models dynamical variances that are subject to abrupt changes. Inference and learning in these hierarchical models is performed with a message passing version of the expectation maximization (EM) algorithm, which is a special instance of the more general class of variational message passing algorithms. We validated the proposed model and estimation algorithm with two applications, using both simulated and real data. First, we implemented a block-outlier insensitive Kalman smoother by modeling the disturbance process with a SSM-DSV. Second, we used SSM-DSV to model the oculomotor system and employed EM-message passing for estimating neural controller signals from eye position data.",https://ieeexplore.ieee.org/document/8902815/,2019 27th European Signal Processing Conference (EUSIPCO),2-6 Sept. 2019,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICMLA.2019.00022,State Summarization of Video Streams for Spatiotemporal Query Matching in Complex Event Processing,IEEE,Conferences,"Modelling complex events in unstructured data like videos not only requires detecting objects but also the spatiotemporal relationships among objects. Complex Event Processing (CEP) systems discretize continuous streams into fixed batches using windows and apply operators over these batches to detect patterns in real-time. To this end, we apply CEP techniques over video streams to identify spatiotemporal patterns by capturing window state. This work introduces a novel problem where an input video stream is converted to a stream of graphs which are aggregated to a single graph over a given state. Incoming video frames are converted to a timestamped Video Event Knowledge Graph (VEKG) [1] that maps objects to nodes and captures spatiotemporal relationships among object nodes. Objects coexist across multiple frames which leads to the creation of redundant nodes and edges at different time instances that results in high memory usage. There is a need for expressive and storage efficient graph model which can summarize graph streams in a single view. We propose Event Aggregated Graph (EAG), a summarized graph representation of VEKG streams over a given state. EAG captures different spatiotemporal relationships among objects using an Event Adjacency Matrix without replicating the nodes and edges across time instances. These enable the CEP system to process multiple continuous queries and perform frequent spatiotemporal pattern matching computations over a single summarised graph. Initial experiments show EAG takes 68.35% and 28.9% less space compared to baseline and state of the art graph summarization method respectively. EAG takes 5X less search time to detect pattern as compare to VEKG stream.",https://ieeexplore.ieee.org/document/8999043/,2019 18th IEEE International Conference On Machine Learning And Applications (ICMLA),16-19 Dec. 2019,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICDM.2003.1250930,Statistical relational learning for document mining,IEEE,Conferences,"A major obstacle to fully integrated deployment of many data mining algorithms is the assumption that data sits in a single table, even though most real-world databases have complex relational structures. We propose an integrated approach to statistical modelling from relational databases. We structure the search space based on ""refinement graphs"", which are widely used in inductive logic programming for learning logic descriptions. The use of statistics allows us to extend the search space to include richer set of features, including many which are not Boolean. Search and model selection are integrated into a single process, allowing information criteria native to the statistical model, for example logistic regression, to make feature selection decisions in a step-wise manner. We present experimental results for the task of predicting where scientific papers will be published based on relational data taken from CiteSeer. Our approach results in classification accuracies superior to those achieved when using classical ""flat"" features. The resulting classifier can be used to recommend where to publish articles.",https://ieeexplore.ieee.org/document/1250930/,Third IEEE International Conference on Data Mining,22-22 Nov. 2003,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CVPR42600.2020.00219,StegaStamp: Invisible Hyperlinks in Physical Photographs,IEEE,Conferences,"Printed and digitally displayed photos have the ability to hide imperceptible digital data that can be accessed through internet-connected imaging systems. Another way to think about this is physical photographs that have unique QR codes invisibly embedded within them. This paper presents an architecture, algorithms, and a prototype implementation addressing this vision. Our key technical contribution is StegaStamp, a learned steganographic algorithm to enable robust encoding and decoding of arbitrary hyperlink bitstrings into photos in a manner that approaches perceptual invisibility. StegaStamp comprises a deep neural network that learns an encoding/decoding algorithm robust to image perturbations approximating the space of distortions resulting from real printing and photography. We demonstrates real-time decoding of hyperlinks in photos from in-the-wild videos that contain variation in lighting, shadows, perspective, occlusion and viewing distance. Our prototype system robustly retrieves 56 bit hyperlinks after error correction -- sufficient to embed a unique code within every photo on the internet.",https://ieeexplore.ieee.org/document/9156548/,2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),13-19 June 2020,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ROBOT.1996.506888,Stereo sketch: stereo vision-based target reaching behavior acquisition with occlusion detection and avoidance,IEEE,Conferences,"In this paper, we proposed a method by which a stereo vision-based mobile robot learns to reach a target by detecting and avoiding occlusions. We call the internal representation that describes the learning behavior ""stereo sketch"". First, an input scene is segmented into homogeneous regions by the enhanced ISODATA algorithm with minimum description length principle in terms of image coordinates and disparity information obtained from the fast stereo matching unit based on the coarse-to-fine control method. Then, in terms of the segmented regions including the target area and their occlusion status identified during the stereo and motion disparity estimation process, we construct a state space for the reinforcement learning method to obtain a target reaching behavior. As a result the robot can avoid obstacles without explicitly describing them. We give the computer simulation results and real robot implementation to show the validity of our method.",https://ieeexplore.ieee.org/document/506888/,Proceedings of IEEE International Conference on Robotics and Automation,22-28 April 1996,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICTAI.2016.0014,Strategy Inference in Multi-Agent Multi-Team Scenarios,IEEE,Conferences,"Creating simulations for multi-agent multi-team interactions is a daunting task. It is non-trivial to compose a situation where each individual agent maintains their own 'personality' while still following the assigned policy dictated by a team's central command. Further, the complexity is inflated by ensuring that each of these agent policies is coordinated into a cohesive team strategy. Finally, peaking the complexity, is evaluating the performance of the team's strategy against other teams' strategies in real-time. This is the work of this paper, proposing SIMAMT, the simulation space for multi-agent multi-team engagements, and testing it. We will first cover the system and how well it models the virtual environment for strategic interaction. Second, we will deliver results from a practical test of strategy inference within such an environment using the SIE (Strategy Inference Engine).",https://ieeexplore.ieee.org/document/7814574/,2016 IEEE 28th International Conference on Tools with Artificial Intelligence (ICTAI),6-8 Nov. 2016,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SNPD51163.2021.9705000,Structure-Preserving Deep Autoencoder-based Dimensionality Reduction for Data Visualization,IEEE,Conferences,"Here, we propose a structure-preserving deep autoencoder-based dimensionality reduction scheme for data visualization. For this, we introduce two regularizers for regularizing autoencoders. The proposed regularizers help the encoded feature space preserve the local and global structures present in the original feature space. A chosen reduced dimensionality of two or three for the encoded feature space enables us to visualize the extracted latent representations of the data using scatterplots. The proposed method has two variants, depending on which regularizer it uses. The proposed approach, moreover, is unsupervised and has predictability. We use three synthetic datasets and one real-world dataset to illustrate the effectiveness of the proposed method. We also visually compare it with three state-of-the-art data visualization schemes and discuss several future research directions.",https://ieeexplore.ieee.org/document/9705000/,"2021 IEEE/ACIS 22nd International Conference on Software Engineering, Artificial Intelligence, Networking and Parallel/Distributed Computing (SNPD)",24-26 Nov. 2021,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CONIELECOMP.2017.7891831,Study of direct local search operators influence in memetic differential evolution for constrained numerical optimization problems,IEEE,Conferences,"This paper analyzes the influence of the depth of direct local search methods in constrained numerical optimization problems in order to use as a local search operator (LSO) within a memetic algorithm. To perform this study, five direct local search methods (Random Walk, Simulated Annealing, Nelder-Mead, Hooke-Jeeves, and Hill Climber) are implemented separately to analyze their behavior within constrained search spaces by using a proposed measure named proximity rate, which measures the closeness of the solutions found by the LSO and the known optimal solution. Finally, all methods are used as LSO, separately, in a memetic algorithm based on Differential Evolution (MDE) structure, where the best solution in the population is used to exploit promising areas in the search space by the aforementioned LSOs. The comparative analysis has been performed on twenty-four benchmark problems used in the special session on “Single Objective Constrained Real-Parameter Optimization” in CEC'2006. Numerical results show that there is not a negative influence of LSO's depth within MDE approach; since regardless of the number of fitness evaluations allowed during the LSO search process, the MDE approach obtains competitive results.",https://ieeexplore.ieee.org/document/7891831/,"2017 International Conference on Electronics, Communications and Computers (CONIELECOMP)",22-24 Feb. 2017,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/EIConRus49466.2020.9039234,Study of the Algorithm for Assessing a Person’s Emotional State by a Face Image Using Actions Units,IEEE,Conferences,"The paper is devoted to the study of algorithm for basic emotions (fear, happiness, anger, sadness, disgust, surprise) recognition from a single face image in the feature space of facial action units. For classification k-nearest neighbors (k-NN) algorithm was applied as well as our own knowledge-based algorithm, which uses emotions description, presented in facial action coding system. Testing was performed on three publicly available databases and on our own database. The results for k-NN algorithm demonstrate comparatively high recognition level (98.2 % and 97.1 % for 6 and 7 classes respectively on Cohn-Kanade database, 10-fold cross-validation), but also its sensitivity to the homogeneity of training and test sets, which was shown during cross-database testing. In contrast, the proposed algorithm is not based on machine learning, demonstrates slightly weaker but stable results, does not require much space for storing data, and is suitable for real-time implementation.",https://ieeexplore.ieee.org/document/9039234/,2020 IEEE Conference of Russian Young Researchers in Electrical and Electronic Engineering (EIConRus),27-30 Jan. 2020,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CSSE.2008.246,Study on Adaptive Intrusion Detection Engine Based on Gene Expression Programming Rules,IEEE,Conferences,"High false alarm rate and time-space cost of rule extraction and detection limit the application of machine learning in real intrusion detection system (IDS), and IDS cannot satisfy most system performance requirements simultaneously. In this paper, a Constraint-based gene expression programming rule extraction algorithm (CGREA) is proposed which guarantees the validity of rules and reduces the evolution time through grammar constraint and probability restriction. Additionally, an adaptive intrusion detection engine (AIDE) is applied to automatically renew the detected order of rules according to the performance metric. The KDD CUPpsila99 DATA is used for evaluation and results show that the rules, which are extracted by the CGREA algorithm within a few evolution generations, can not only achieve high detection rate but also detect unknown attacks. Moreover, the AIDE based on CGREA increases the attack detection rate, and adapts itself to different performance requirements with different sequences of rule detection.",https://ieeexplore.ieee.org/document/4722502/,2008 International Conference on Computer Science and Software Engineering,12-14 Dec. 2008,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CSIE.2009.234,Study on Multi-agent Simulation System Based on Reinforcement Learning Algorithm,IEEE,Conferences,"Multi-agent simulation system based on reinforcement learning algorithm is a micro-individual acts of modeling and simulation methods, which have wide applicability, distribution, intelligent and interactive features etc. Firstly, studying on reinforcement learning algorithm, and then analysis and design the multi-agent simulation system structure, multi-agent system main modules, the implementation of the definition and finally, carefully design the multi-agent simulation system software, and multi-agent simulation collective system simulation and surrounded the location gathered from the space simulation experiment, the results showed that: Construct a multi-agent simulation system based on reinforcement learning algorithm, achieve real-time simulation of multi-agene, and multi-agent to get effect quickly, and to quickly construct surrounded conduct by mobile groups, the conduct of the system to achieve the global optimum effect.",https://ieeexplore.ieee.org/document/5170590/,2009 WRI World Congress on Computer Science and Information Engineering,31 March-2 April 2009,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICMLC.2002.1167430,Study on discretization based on rough set theory,IEEE,Conferences,"Discretization of attributes with real values is an important problem in data mining based on rough set. And discretization based on rough set has some particular characteristics. The method of discretization based on rough set and Boolean reasoning is discussed. Determination of. candidate cuts is discussed in detail. A theorem is proposed. to show that all bound cuts can discern the same objects pairs as the whole initial cuts. A strategy to select candidate cuts is proposed based on the theorem. Under the strategy, the space complexity and time complexity of improved algorithm decline obviously. The experiments results also confirm that.",https://ieeexplore.ieee.org/document/1167430/,Proceedings. International Conference on Machine Learning and Cybernetics,4-5 Nov. 2002,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/INFCOM.2012.6195649,SubFlow: Towards practical flow-level traffic classification,IEEE,Conferences,"Many research efforts propose the use of flow-level features (e.g., packet sizes and inter-arrival times) and machine learning algorithms to solve the traffic classification problem. However, these statistical methods have not made the anticipated impact in the real world. We attribute this to two main reasons: (a) training the classifiers and bootstrapping the system is cumbersome, (b) the resulting classifiers have limited ability to adapt gracefully as the traffic behavior changes. In this paper, we propose an approach that is easy to bootstrap and deploy, as well as robust to changes in the traffic, such as the emergence of new applications. The key novelty of our classifier is that it learns to identify the traffic of each application in isolation, instead of trying to distinguish one application from another. This is a very challenging task that hides many caveats and subtleties. To make this possible, we adapt and use subspace clustering, a powerful technique that has not been used before in this context. Subspace clustering allows the profiling of applications to be more precise by automatically eliminating irrelevant features. We show that our approach exhibits very high accuracy in classifying each application on five traces from different ISPs captured between 2005 and 2011. This new way of looking at application classification could generate powerful and practical solutions in the space of traffic monitoring and network management.",https://ieeexplore.ieee.org/document/6195649/,2012 Proceedings IEEE INFOCOM,25-30 March 2012,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICASSP40776.2020.9053766,Supervised Deep Hashing for Efficient Audio Event Retrieval,IEEE,Conferences,"Efficient retrieval of audio events can facilitate real-time implementation of numerous query and search-based systems. This work investigates the potency of different hashing techniques for efficient audio event retrieval. Multiple state-of-the-art weak audio embeddings are employed for this purpose. The performance of four classical unsupervised hashing algorithms is explored as part of off-the-shelf analysis. Then, we propose a partially supervised deep hashing framework that transforms the weak embeddings into a low-dimensional space while optimizing for efficient hash codes. The model uses only a fraction of the available labels and is shown here to significantly improve the retrieval accuracy on two widely employed audio event datasets. The extensive analysis and comparison between supervised and unsupervised hashing methods presented here, give insights on the quantizability of audio embeddings. This work provides a first look in efficient audio event retrieval systems and hopes to set baselines for future research.",https://ieeexplore.ieee.org/document/9053766/,"ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",4-8 May 2020,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICSMC.2006.384813,Supervised Learning of Motion Style for Real-time Synthesis of 3D Character Animations,IEEE,Conferences,"In this paper, we present a supervised learning framework to learn a probabilistic mapping from values of a low-dimensional style variable, which defines the characteristics of a certain kind of 3D human motion such as walking or boxing, to high-dimensional vectors defining 3D poses. All possible values of the style variable span an Euclidean space called style space. The supervised learning framework guarantees that each dimension of style space corresponds to a certain aspect of the motion characteristics, such as body height and pace length, so the user can precisely define a 3D pose by locating a point in the style space. Moreover, every curve in the Euclidean style space corresponds to a smooth motion sequence. We developed a graphical user interface program, with which, users simply points mouse cursor in the style space to define a 3D pose and drags mouse cursor to synthesis 3D animations in real-time.",https://ieeexplore.ieee.org/document/4274578/,"2006 IEEE International Conference on Systems, Man and Cybernetics",8-11 Oct. 2006,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ROMAN.1994.365922,Surface display: a force feedback system simulating the surface of an object,IEEE,Conferences,"Force feedback is an interface based on the phenomenon of contact. In the implementation of a virtual force feedback environment, the object is defined in a computer and the user is in the real world. Therefore, an interface device is required to transmit touch sensation to the user's finger when he or she touches a virtual object in virtual space. In this paper, methodology to realize force feedback is categorized from this view point, and the idea of ""surface display"" is presented as a force feedback method in a virtual environment.<>",https://ieeexplore.ieee.org/document/365922/,Proceedings of 1994 3rd IEEE International Workshop on Robot and Human Communication,18-20 July 1994,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/TENCON.2010.5685846,Symbol-based soft relaying strategy for cooperative wireless networks,IEEE,Conferences,"We propose an improved Soft Forwarding (SF) protocol which is based on the symbol-by-symbol detection at the relay node, unlike in the conventional relaying strategy which implements a bit-by-bit signal analysis at the relay node. A relay node which is typically acts like a repeater can avoid unnecessary computation complexity and above all, is resource efficient if compared to the baseline SF. The proposed strategy is implemented based on Maximum Likelihood Detector (MLD) criterion. We simplify the MLD rule to one-dimensional real space such that all metric values are compared in a straight line making the ML performance analysis tractable. Simulations have shown that the proposed schemes outperform the conventional schemes in error rate performance.",https://ieeexplore.ieee.org/document/5685846/,TENCON 2010 - 2010 IEEE Region 10 Conference,21-24 Nov. 2010,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICDE.1987.7272393,Symbolic processing: Issues and opportunties,IEEE,Conferences,"Symbolic processing is distinguished from other types of computation, e.g. numeric, commercial, or real-time, by the fact that its metadata is both complex and complicated. That is, the data objects manipulated by symbolic programs have descriptions which are not simple and not always predictable. The metadata is complex in the sense that the data objects are multi-faceted, having a variety of attributes and relationships to other objects. Processing this multidimensional data is done in slices, involving just a few types of attributes at a time, so different programs must deal with different projections (or views) of the data. The metadata is complicated in that it is recursive, meaning that the interpretation of a data object requires processing other instances of the same object class, to an unpredictable extent. Thus complexity and complication in metadata translate directly into data management and resource (space and time) management problems in processing.",https://ieeexplore.ieee.org/document/7272393/,1987 IEEE Third International Conference on Data Engineering,3-5 Feb. 1987,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SCC.2017.68,SynAdapt: Automated Synthesis of Adaptive Agents,IEEE,Conferences,"Distributed autonomous multi-agent reasoning and classification systems have been thought of to be the basis of intelligence and have wide applications in the space of operational intelligence in closing the loop between sensing, analytics, and actions. This paper targets multi-agent systems that employ rulebased logics (i.e., rules that determine the output/response of an agent depending on the range of the input values) with pre-defined rules to accurately perceive the environment, and provide associated reactions. Such rule-based systems do not perform well in scenarios, where human generated rules cannot adapt to dynamic variations in the data distribution arising due to dynamic changes in the environment, especially if data dimensionality is very high. Examples of such scenarios exist wherever the sensed data arrives from the physical world - such as weather data, physical sensor data, human behaviour controlled data, etc. Clearly, to meet the adaptivity requirements of such scenarios we require the agents to possess adaptive reasoning capability such that they can adapt the underlying rules with respect to the changing environment. Developing such adaptive agents requires the developer to additionally possess considerable expertise of state-of-the-art machine learning techniques, apart from possessing knowledge of the agent's target domain. To address the above issues, we automate the process of development and deployment of adaptive agents. We present the fundamental design concepts behind the development of SynAdapt: a new adaptive meta-learning based multi-agent synthesis framework, that automates the synthesis of adaptive multi-agent systems from high-level user specifications. SynAdapt provides the following key features: a) Automated synthesis and deployment of adaptive agents from high-level user specification, b) Agents synthesised by SynAdapt can select a learning strategy that is particularly suited for given user specifications and input dataset, and c) Agents synthesised by SynAdapt can leverage adaptive ensemble learning techniques to deal with concept drift.",https://ieeexplore.ieee.org/document/8035021/,2017 IEEE International Conference on Services Computing (SCC),25-30 June 2017,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/GLOBECOM46510.2021.9685850,TTDeep: Time-Triggered Scheduling for Real-Time Ethernet via Deep Reinforcement Learning,IEEE,Conferences,"Schedule scheme is essential for real-time Ethernet. Due to the inevitable change of network configurations, the solution requires to be incrementally scheduled in a timely manner. Solver-based methods are time-consuming, while handcrafted scheduling heuristics require domain knowledge and professional expertise, and their application scenarios are usually limited. Instead of designing heuristic strategy manually, we propose TTDeep, a deep reinforcement learning schedule framework, to incrementally schedule Time-Triggered (TT) flows and adapt to various topologies. Our novel framework includes 3 key designs: a period layer to capture the periodical transmission nature of TT flows, the graph neural network to extract and represent topology features, and a 3-step selection paradigm to alleviate the huge action search space issue. Comprehensive experiments show that TTDeep can schedule TT flows much faster than solver-based methods and schedule nearly twice more TT flows on average compared to handcrafted heuristics.",https://ieeexplore.ieee.org/document/9685850/,2021 IEEE Global Communications Conference (GLOBECOM),7-11 Dec. 2021,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/WI-IAT.2009.201,Tank War Using Online Reinforcement Learning,IEEE,Conferences,"Real-Time Strategy(RTS) games provide a challenging platform to implement online reinforcement learning(RL) techniques in a real application. Computer as one player monitors opponents'(human or other computers) strategies and then updates its own policy using RL methods. In this paper, we propose a multi-layer framework for implementing the online RL in a RTS game. The framework significantly reduces the RL computational complexity by decomposing the state space in a hierarchical manner. We implement the RTS game - Tank General, and perform a thorough test on the proposed framework. The results show the effectiveness of our proposed framework and shed light on relevant issues on using the RL in RTS games.",https://ieeexplore.ieee.org/document/5285131/,2009 IEEE/WIC/ACM International Joint Conference on Web Intelligence and Intelligent Agent Technology,15-18 Sept. 2009,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/QRS-C.2018.00071,Task Resource Planning and Verification Method Based on Intelligent Planning,IEEE,Conferences,"In order to meet the demands of rapid resource scheduling and capability recombination in a rapidly changing environment, real-time and dynamic resource planning and flexible scheduling are carried out in a relatively short time within a wide space. The paper proposes a fast generation method of task system resource guarantee plan based on intelligent planning, allocation mechanism and verification method. In this paper, we use an ontology-based semantic representation model and Hierarchical Task Network (HTN) to get the corresponding sequences of the resource type, breaking through the hierarchical task network planning combined with business process driven technology, to implement the task decomposition and resource guarantee scheme generation technique. Then we use Multidimensional Dynamic List Scheduling (MDLS) resource allocation algorithm to complete screening and the operation of the specific distribution of resources. During the resource allocation, we use conflict resolution algorithm to avoid resource conflict, to ensure resource combination optimization.",https://ieeexplore.ieee.org/document/8431999/,"2018 IEEE International Conference on Software Quality, Reliability and Security Companion (QRS-C)",16-20 July 2018,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/PADSW.2000.884672,Teleoperation system for real world robots-adaptive robot navigation based on sensor fusion,IEEE,Conferences,"The authors propose a teleoperation system with an autonomous robot which is able to solve tasks even without a large load for the operator and the system. Most teleoperation systems require skilled operators and expensive interfaces to solve tasks because they assume that the operator controls a robot completely. For these problems, we propose a teleoperation system which consists of an operation system and an autonomous robot. The operation system has a man-machine interface and allows a user to specify the working space and the tasks to be done. The autonomous robot follows the instruction from the operation system to solve the specific tasks. The paper focuses on navigation problems of the autonomous robot as an essential part of the proposed system. Namely, the autonomous robot should keep on the instructed paths in the real world to achieve a goal of the tasks. Our approach is based on a sensor fusion method based on two learning schemes: self-organizing map (SOM) and reinforcement learning. These learning schemes allow the system to be able to solve the tasks in an unreliable environment such as outdoors. Computational simulations reveal the effectiveness and robustness of the proposed method in the navigation problem.",https://ieeexplore.ieee.org/document/884672/,Proceedings Seventh International Conference on Parallel and Distributed Systems: Workshops,4-7 July 2000,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1145/3238147.3238227,Template-Guided Concolic Testing via Online Learning,IEEE,Conferences,"We present template-guided concolic testing, a new technique for effectively reducing the search space in concolic testing. Addressing the path-explosion problem has been a significant challenge in concolic testing. Diverse search heuristics have been proposed to mitigate this problem but using search heuristics alone is not sufficient to substantially improve code coverage for real-world programs. The goal of this paper is to complement existing techniques and achieve higher coverage by exploiting templates in concolic testing. In our approach, a template is a partially symbolized input vector whose job is to reduce the search space. However, choosing a right set of templates is nontrivial and significantly affects the final performance of our approach. We present an algorithm that automatically learns useful templates online, based on data collected from previous runs of concolic testing. The experimental results with open-source programs show that our technique achieves greater branch coverage and finds bugs more effectively than conventional concolic testing.",https://ieeexplore.ieee.org/document/8999998/,2018 33rd IEEE/ACM International Conference on Automated Software Engineering (ASE),3-7 Sept. 2018,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICDM.2006.157,Temporal Data Mining in Dynamic Feature Spaces,IEEE,Conferences,"Many interesting real-world applications for temporal data mining are hindered by concept drift. One particular form of concept drift is characterized by changes to the underlying feature space. Seemingly little has been done in this area. This paper presents FAE, an incremental ensemble approach to mining data subject to such concept drift. Empirical results on large data streams demonstrate promise.",https://ieeexplore.ieee.org/document/4053168/,Sixth International Conference on Data Mining (ICDM'06),18-22 Dec. 2006,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SNPD.2008.105,Testing Component-Based Real Time Systems,IEEE,Conferences,"This paper focuses on studying efficient solutions for modeling and deriving compositional tests for component-based real-time systems. In this work, we propose a coherent framework that does not require the computation of the synchronous product (composition) of components, and therefore avoids a major bottleneck in this class of test. For this framework, we introduce an approach and associated algorithm. In our approach, the overall behavior of the system is obtained by restricting free runs of components to those involving interactions between them. This restriction is achieved through the use of a particular component called assembly controller. For the generation algorithm, compositional test cases are derived from the assembly controller model using symbolic analysis. This reduces the state space size (a practical size) and enables the generation of sequences which cover all critical interaction scenarios.",https://ieeexplore.ieee.org/document/4617482/,"2008 Ninth ACIS International Conference on Software Engineering, Artificial Intelligence, Networking, and Parallel/Distributed Computing",6-8 Aug. 2008,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CITISIA50690.2020.9371771,Text Analysis for Honeypot Misuse Inference,IEEE,Conferences,"Transformation of raw text is required for computational text analysis using Natural Language Processing methods. Computational text analysis leverage on human brain limitations to automatically index documents for retrieval and topic generation for topic distribution correlations in corpus of voluminous documents. Natural language non-parametric and parametric Topic modeling with Expectancy Maximization and Gibbs sampling render technique to build Machine Learning models for evaluation with log-likelihood, topic coherence and coefficient of determination of held-out document. This research extends the concept of Natural Language Processing to automate analysis of High interaction honeypot system call documents to deduce system resources misuse by malcode during real-time engagement with the user-space applications of the deployed honeypot.",https://ieeexplore.ieee.org/document/9371771/,2020 5th International Conference on Innovative Technologies in Intelligent Systems and Industrial Applications (CITISIA),25-27 Nov. 2020,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/THS.2013.6699006,The CERT assessment tool: Increasing a security incident responder's ability to assess risk,IEEE,Conferences,"We set out to create an assessment and situational awareness tool for incident response. Extracting the risk assessment expertise and creating a systemic step-by-step workflow that could be followed by non-experts was challenging; however, what proved to be even more difficult was the mapping of that workflow to a common, natural language used by non-experts while still supporting the incident response. We at the Digital Intelligence and Investigation Directorate (DIID) have developed a way to maintain the velocity of incident response through the creation of a feed-forward decision support system to assist a security responder deal with the scale and challenges of assessing risk in critical information systems. Unfortunately, many applications fall short of expectations because the technology is used inappropriately: the wrong tool applied in the wrong way. Taking interaction techniques combined with a decision support system and applying them to one particularly demanding area - security incident response - leads to the conclusion that there is a proper and formal way to approach maintaining situational awareness in this complex domain. The CERT Assessment Tool increases a security incident responder's ability to assess risk and identify the incident response plan of critical information systems. The interface has four primary affordances to the user: (1) digital storage of the collected interview data with tagging of the information to create meta data of the objects as well as standardize terminology by reusing objects, (2) structured data that enables situational awareness of all systems on site and flexibility and recursion of system attributes, (3) guidance questions that provide runtime support for the system currently being assessed and a general direction to better assess each system based on historical data, and (4) real-time rules that make recommendations to the user through `push' notifications, which enables a user to identify and mitigate risk in information systems security affecting the safety of a system or the implementation of the security plan. The creation of a security decision support system framework to represent a series of steps to view the entire space of a security incident allows us to use techniques specifically designed or selected to align with one of the three identified stages of incident response - pre-incident (perception), during the event (comprehension), or after the event (projection). This combination of rules based on machine learning and push notifications are a first step in how computers will be able to support and advance the decision support technologies that are the backbone of this system.",https://ieeexplore.ieee.org/document/6699006/,2013 IEEE International Conference on Technologies for Homeland Security (HST),12-14 Nov. 2013,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/C5.2012.11,The Development of a Programming-Project Sharing Environment on Virtual Space,IEEE,Conferences,"Recently, there are classes using GUI programming environment such as Squeak Etoys or Scratch in primary education. When children study in such environment, children do not have enough opportunity to view the other's works and show their own work to parents and it is difficult to perform mutual evaluation and review lesson. In this paper, we propose an environment where children or parents can view the children's works on virtual space. It can provide opportunities for children to do mutual evaluation or review the past lesson. We have deployed this system to classes using GUI-programming environment and conducted a questionnaire for children. As a result, seven out of eleven children could view the other's works well. Moreover, eight out of ten children could find a scene in which other children watch the work together, and among these children, six of them viewed this attracting work.",https://ieeexplore.ieee.org/document/6195219/,"2012 10th International Conference on Creating, Connecting and Collaborating through Computing",18-20 Jan. 2012,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CRC52766.2021.9620166,The Development of an Omnidirectional Mobile Robot Based on Hub Motor,IEEE,Conferences,"To enable the ability of moving in a narrow space, the robots are required to move in all directions. However, traditional robots with omnidirectional mobile function are easily wearing, with poor bearing capacity, and with complex structure. We designed and proposed an omnidirectional mobile robot (OMR) with the active split offset caster (ASoC) wheelset as the driving wheelset. Specifically, we first established the mathematical model of the robot based on the differential drive principle. Then the mathematical model is verified with synchronized real and simulated movement of the designed robot. A motion capture system is used to track the actual motion trajectory. Finally, a road test experiment is applied to test the stability of the designed robot in a real environment. The tiny differences between real and simulated trajectory show that the designed robot has the ability to move omnidirectionally in a narrow space. The road test experiment proves the advanced adaptability in real environment. Therefore, the designed robot can assist in a narrow space.",https://ieeexplore.ieee.org/document/9620166/,"2021 6th International Conference on Control, Robotics and Cybernetics (CRC)",9-11 Oct. 2021,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CIE.2002.1185854,The HyperClass: education in a broadband Internet environment,IEEE,Conferences,"The 1980s saw the advent of the PC in education, the 1990s saw the coming of narrow band Internet to education, the first decade of the new Millennium seems set to see the spread of broadband Internet in education. What will it mean for education when students and teachers can access Pentium 4 computers with 214 meg bandwidth from wherever they are? The author with Lalita Rajasingham and Nobuyoshi Terashima in collaboration with colleagues around the world, have been studying this question. In particular they have been looking at the application on the Internet of a technology called HyperReality. The idea of the technology is that it provides a space where physical reality and virtual reality and human intelligence and artificial intelligence can interact in a manner that becomes increasingly seamless. The space could be a class-hence the idea of a HyperClass. The paper looks at the experimental work already carried out between Japan and New Zealand to develop such a system. It addresses the implications for education of real students, teachers and objects freely interacting with virtual students, teachers and objects and with artificial intelligence in a class. Finally it draws on the work of Tiffin and Rajasingham in designing a global virtual university to look at the possibilities for developing HyperUniversities, HyperColleges and HyperSchools which would allow the intersection of global and local dimensions in education.",https://ieeexplore.ieee.org/document/1185854/,"International Conference on Computers in Education, 2002. Proceedings.",3-6 Dec. 2002,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CAIA.1988.196126,The NASA systems autonomy program: applying AI in space,IEEE,Conferences,"Summary form only given. The National Aeronautics and Space Administration (NASA) has initiated an aggressive program to develop, integrate, and implement autonomous systems technologies starting with knowledge-based (expert) systems and evolving towards 'autonomous', intelligent systems. Research thrusts to achieve these capabilities are centered around machine learning; coordinated real-time decision-making by multiple, cooperating intelligent agents; management, maintenance, and real-time control of distributed databases; software verification and validation; and fault-tolerant, multiprocessor architectures capable of operating in a heterogeneous environment. A summary of the research plans and the progress made in each of these areas is discussed.<>",https://ieeexplore.ieee.org/document/196126/,[1988] Proceedings. The Fourth Conference on Artificial Intelligence Applications,14-18 March 1988,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICMLA.2009.15,The Neuro Slot Car Racer: Reinforcement Learning in a Real World Setting,IEEE,Conferences,"This paper describes a novel real-world reinforcement learning application: The Neuro Slot Car Racer. In addition to presenting the system and first results based on Neural Fitted Q-Iteration, a standard batch reinforcement learning technique, an extension is proposed that is capable of improving training times and results by allowing for a reduction of samples required for successful training. The Neuralgic Pattern Selection approach achieves this by applying a failure-probability function which emphasizes neuralgic parts of the state space during sampling.",https://ieeexplore.ieee.org/document/5381535/,2009 International Conference on Machine Learning and Applications,13-15 Dec. 2009,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SysCon47679.2020.9275860,The Role of Attribute Ranker using classification for Software Defect-Prone Data-sets Model: An Empirical Comparative Study,IEEE,Conferences,"Feature selection, is an issue firmly identified with size decrease of data-sets model. The target of feature selection is to recognize features in the data-set as significant, and dispose of some other feature as unimportant and repetitive data. Since feature selection diminishes the dimensionality of the data-sets model, it holds out the probability of increasingly successful and quick activity of data mining algorithm which can be worked quicker and all the more adequately by utilizing feature selection. In this research paper we will investigate feature extraction Principal Component Analysis (PCA) with attribute ranker search technique. In practice, Principal Component Analysis with attribute ranker search strategy isn’t just used to improve extra storage space or the computational accuracy and efficiency of the classification algorithm, however can likewise enhance the prescient presentation by diminishing the scourge of dimensionality — particularly on the off chance that we are working with software defect-prone or non-defected data-sets models. We have used 10 datasets models, these datasets models basically REPOSITORY model of NASA which contain binary class defected and non-defected datasets models. We have also used 6 classifiers for comparatively analysis between objective model and real datasets model. We illustrated the comparatively analysis between PCA Ranker Search method with No-PCA. We have also compared classifiers efficiency with each other. The most efficient and useful classifier are the Bagging and Multilayer Perceptron at all attribute ranking search method. But the comparatively analysis between the classifiers that Naïve bayes and MultiLayer Perceptron have well increased the correctly classified instances percentage in overall software fault forecast. One more comparison between the PCA Ranker Search Method and No-PCA that is attribute ranker search method is really good for increasing accuracy and efficiency for software fault forecast dataset model as compare the no-PCA method.",https://ieeexplore.ieee.org/document/9275860/,2020 IEEE International Systems Conference (SysCon),24 Aug.-20 Sept. 2020,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRMS.2009.5270085,The analysis and modeling for the input space of real-time embedded software,IEEE,Conferences,"Software reliability testing is one of the important tasks in software reliability engineering, in which the failure data can be used to evaluate and validate the software reliability. In this paper, the input space of real-time embedded software is analyzed first. And the model of input space constructed with usage space and input value space is presented, with which a formal modeling method, the usage profile in network graph form, is presented. With the usage profile, the constraint conditions of operations and the dynamic actions of software users can be expressed closely to actual situation. The software reliability testing cases can be generated with random sampling according to the software usage profile.",https://ieeexplore.ieee.org/document/5270085/,"2009 8th International Conference on Reliability, Maintainability and Safety",20-24 July 2009,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICIT.2002.1189350,The application of a reinforcement learning agent to a multi-product manufacturing facility,IEEE,Conferences,"An intelligent agent-based scheduling system, consisting of a reinforcement learning agent and a simulation model has been developed and tested on a classic scheduling problem. The production facility studied is a multiproduct serial line subject to stochastic failure. The agent goal is to minimise total production costs, through selection of job sequence and batch size. To explore state space the agent used reinforcement learning. By applying an independent inventory control policy for each product, the agent successfully identified optimal operating policies for a real production facility.",https://ieeexplore.ieee.org/document/1189350/,"2002 IEEE International Conference on Industrial Technology, 2002. IEEE ICIT '02.",11-14 Dec. 2002,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ChiCC.2014.6896010,The design and optimization method of near space intelligent target generator,IEEE,Conferences,"This paper presents the design and optimization method of near space intelligent target generator to simulate the physical characteristics of the near space vehicle. Combined with High Level Architecture distributed simulation technology, a common, repeatable and verified platform for the near space vehicle has been provided. This method used 3D modeling software Creator and 3D visual rendering software Vega, two-dimensional map and three-dimensional vision were constructed to form a simulation environment, which enhanced the authenticity of the simulation. Based on particle swarm optimization, the intelligent path planning study of near space vehicle was conducted in this environment to make up for the inadequate intelligence of traditional target generators.",https://ieeexplore.ieee.org/document/6896010/,Proceedings of the 33rd Chinese Control Conference,28-30 July 2014,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/FIE.2001.963683,The design of pedagogical agent for distance virtual experiment,IEEE,Conferences,"Experimental learning about electric-machinery is high risk and time-consuming, students do not always enjoy the laboratory, the teacher also needs to pay full attention to all students and equipment, besides it is hard to provide enough machine sets and space for learning. So it is our motivation to design a virtual laboratory for the teaching of electric-machinery experiment by Internet under the more real environment without concern about danger or limitations. It is taught by an experienced teacher using a pedagogical agent which is the most important key factor. Taking the above features into consideration, an interactive virtual laboratory based on an expert system has been designed and implemented to improve the learning, operation and control in electric-machinery experiments. The system set up is a highly intelligent and interactive mechanism. Through the system, students could have a more complete environment for distance learning.",https://ieeexplore.ieee.org/document/963683/,31st Annual Frontiers in Education Conference. Impact on Engineering and Science Education. Conference Proceedings (Cat. No.01CH37193),10-13 Oct. 2001,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/POWERI.2006.1632564,The development of artificial neural network space vector PWM and diagnostic controller for voltage source inverter,IEEE,Conferences,"This paper presents the development of neural-network-based controller of space vector modulation (ANN-SVPWM) for voltage-source inverters (VSI). This ANN-SVPWM controller completely covers the undermodulation and overmodulation modes with operation extended linearly and smoothly up to square wave (six-step) by using theory of modulation between the limit trajectories. The ANN controller has the advantage of the very fast implementation of an SVM algorithm that can increase the switching frequency of power switches of the static converter. Furthermore, a ANN diagnosis method for real-time fault detection of power switches is proposed in this paper. The ANN controller uses the individual training strategy with the fixed weight and supervised models. The complete ANN-SVPWM and diagnostic controller can be used in power applications such as APF, STATCOM, UPFC and motor drives. A computer simulation program is developed using Matlab/Simulink together with the neural network toolbox for training the ANN-controller.",https://ieeexplore.ieee.org/document/1632564/,2006 IEEE Power India Conference,10-12 April 2006,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICIT.2009.4939636,The investigation of ANN space vector PWM and diagnostic controller for four switch three phase inverter fed induction motor drive,IEEE,Conferences,"This paper is to present the investigation of neural-network-based controller of space vector modulation (ANN-SVPWM) for four switch three phase inverter (FSTPI) fed induction motor drive. This ANN-SVPWM controller completely covers the under modulation and over modulation modes with operation extended linearly and smoothly up to square wave (six-step). The ANN controller uses the individual training strategy with the fixed weight and supervised models. Furthermore, ANN diagnosis method for real-time fault detection of power switches is proposed in this paper. The complete ANN-SVPWM and diagnostic controller can be used in power applications such as motor drives. A computer simulation program is developed using Matlab/Simulink together with the neural network toolbox for training the ANN-controller.This method has been validated experimentally using kit ACE 1104 (DSPACE) .",https://ieeexplore.ieee.org/document/4939636/,2009 IEEE International Conference on Industrial Technology,10-13 Feb. 2009,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IJCNN.2015.7280318,The on-line curvilinear component analysis (onCCA) for real-time data reduction,IEEE,Conferences,"Real time pattern recognition applications often deal with high dimensional data, which require a data reduction step which is only performed offline. However, this loses the possibility of adaption to a changing environment. This is also true for other applications different from pattern recognition, like data visualization for input inspection. Only linear projections, like the principal component analysis, can work in real time by using iterative algorithms while all known nonlinear techniques cannot be implemented in such a way and actually always work on the whole database at each epoch. Among these nonlinear tools, the Curvilinear Component Analysis (CCA), which is a non-convex technique based on the preservation of the local distances into the lower dimensional space, plays an important role. This paper presents the online version of CCA. It inherits the same features of CCA, is adaptive in real time and tracks non-stationary high dimensional distributions. It is composed of neurons with two weights: one, pointing to the input space, quantizes the data distribution, and the other, pointing to the output space, represents the projection of the first weight. This on-line CCA has been conceived not only for the previously cited applications, but also as a basic tool for more complex supervised neural networks for modelling very complex high dimensional data. This algorithm is tested on 2-D and 3-D synthetic data and on an experimental database concerning the bearing faults of an electrical motor, with the goal of novelty (fault) detection.",https://ieeexplore.ieee.org/document/7280318/,2015 International Joint Conference on Neural Networks (IJCNN),12-17 July 2015,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/MMRP.2019.00013,Three-Dimensional Mapping of High-Level Music Features for Music Browsing,IEEE,Conferences,"The increased availability of musical content comes with the need of novel paradigms for recommendation, browsing and retrieval from large music libraries. Most music players and streaming services propose a paradigm based on content listing of meta-data information, which provides little insight on the music content. In services with huge catalogs of songs, a more informative paradigm is needed. In this work we propose a framework for music browsing based on the navigation into a three-dimensional (3-D) space, where musical items are placed as a 3-D mapping of their high-level semantic descriptors. We conducted a survey to guide the design of the framework and the implementation choices. We rely on state-of-the-art techniques from Music Information Retrieval to automatically extract the high-level descriptors from a low-level representation of the musical signal. The framework is validated by means of a subjective evaluation from 33 users, who give positive feedbacks and highlight promising future developments especially in virtual reality field.",https://ieeexplore.ieee.org/document/8665368/,2019 International Workshop on Multilayer Music Representation and Processing (MMRP),23-24 Jan. 2019,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SEAMS.2012.6224393,"Timing constraints for runtime adaptation in real-time, networked embedded systems",IEEE,Conferences,"In this work, we consider runtime adaptation in networked embedded systems with tight real-time constraints. For such systems, we aim to adapt the placement of software components on networked hardware components at runtime without violating real-time constraints. We develop constraints for such an adaptation process and show the applicability to networked embedded systems like automotive in-vehicle networks. Then, we analyze two approaches for finding solutions in the resulting search space for adaptations, one based on planning algorithms and the other based on constraint solving. While planning approaches start from the current configuration and aim to find a migration sequence and a valid configuration, constraint solving approaches first find solutions and then check for a possible migration sequence. Based on simulations for the automotive domain, we show that approaches based on planning algorithms scale poorly, while constraint solving approaches can find solutions effectively.",https://ieeexplore.ieee.org/document/6224393/,2012 7th International Symposium on Software Engineering for Adaptive and Self-Managing Systems (SEAMS),4-5 June 2012,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/RTAS48715.2020.000-1,Timing of Autonomous Driving Software: Problem Analysis and Prospects for Future Solutions,IEEE,Conferences,"The software used to implement advanced functionalities in critical domains (e.g. autonomous operation) impairs software timing. This is not only due to the complexity of the underlying high-performance hardware deployed to provide the required levels of computing performance, but also due to the complexity, non-deterministic nature, and huge input space of the artificial intelligence (AI) algorithms used. In this paper, we focus on Apollo, an industrial-quality Autonomous Driving (AD) software framework: we statistically characterize its observed execution time variability and reason on the sources behind it. We discuss the main challenges and limitations in finding a satisfactory software timing analysis solution for Apollo and also show the main traits for the acceptability of statistical timing analysis techniques as a feasible path. While providing a consolidated solution for the software timing analysis of Apollo is a huge effort far beyond the scope of a single research paper, our work aims to set the basis for future and more elaborated techniques for the timing analysis of AD software.",https://ieeexplore.ieee.org/document/9113112/,2020 IEEE Real-Time and Embedded Technology and Applications Symposium (RTAS),21-24 April 2020,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICTAI.2015.130,Toward Efficient Agreements in Real-Time Multilateral Agent-Based Negotiations,IEEE,Conferences,"Negotiations among autonomous agents have gained a mass of attention from a variety of communities in the past decade. This paper deals with a prominent type of automated negotiations, namely, multilateral multi-issue negotiation that runs under real-time constraints and in which the negotiating agents have no prior knowledge about their opponents' preferences over the space of negotiation outcomes. We propose a novel negotiation approach which enables an agent to reach an efficient agreement with multiple opponents. The proposed approach achieves that goal by, 1) employing sparse pseudo-input Gaussian processes to model the behavior of opponents, 2) learning fuzzy opponent preferences to increase the satisfaction of other parties, and 3) adopting an adaptive decision-making mechanism to handle uncertainty in negotiation. The experimental results show, both from the standard mean-score perspective and the perspective of empirical game theory, that the agent applying the proposed approach outperforms the state-of-the-art negotiation agents from the recent Automated Negotiating Agents Competition (ANAC) in a variety of negotiation domains.",https://ieeexplore.ieee.org/document/7372227/,2015 IEEE 27th International Conference on Tools with Artificial Intelligence (ICTAI),9-11 Nov. 2015,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SMARTCOMP55677.2022.00036,Toward an API-Driven Infinite Cyber-Screen for Custom Real-Time Display of Big Data Streams,IEEE,Conferences,"Graphical User Interfaces (GUI) and real-time in-teractive Big Data charts play a key role in a wide variety of Big Data applications. The software libraries that are available at today are not suitable for displaying huge volumes of data in a single chart, because they require all the data to be collected at a single node. In this work, an innovative approach to the problem is presented, that consists in using a network of cyber-devices that is created and configured via API calls and that interfaces with a Scala Spark server application through a multiplicity of communication technologies, to produce and display a variety of time-space- infinite Big Data stream visualizations, including line plots, pie charts, histograms, that are updated at real-time as new data come, without ever collecting the data or the charts markup at a single node. The proposed approach characterizes for being (i) Web-based, (ii) API-based, (iii) Cloud-based, (iv) portable, (v) customizable/extendable, (vi) plug and play, and for relying on (i) Node-RED, (ii) MQTT, (iii) Scala, (iv) Akka HTTP, (v) Spark, (vi) Kafka, (vii) Docker. Remarkably, the same network used for Big Data visualization can be reconfigured in a matter of milliseconds and used for Big Data (streams) filtering, transformation, merge, analytics, and for training Machine Learning models, storing trained models on a Cloud storage, using stored models for one-shot or stream predictions, and much more. Although being at an advanced stage, we consider this research as a work in progress, since an extensive benchmarking and application to variegated real-world scenarios are still to be carried out.",https://ieeexplore.ieee.org/document/9821037/,2022 IEEE International Conference on Smart Computing (SMARTCOMP),20-24 June 2022,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IEMBS.2007.4353479,Towards Asynchronous Brain-computer Interfaces: A P300-based Approach with Statistical Models,IEEE,Conferences,"Asynchronous control is a critical issue in developing brain-computer interfaces for real-life applications, where the machine should be able to detect the occurrence of a mental command. In this paper we propose a computational approach for robust asynchronous control using the P300 signal, in a variant of oddball paradigm. First, we use Gaussian models in the support vector margin space to describe various types of EEG signals that are present in an asynchronous P300-based BCI. This allows us to derive a probability measure of control state given EEG observations. Second, we devise a recursive algorithm to detect and locate control states in ongoing EEG. Experimental results indicate that our system allows information transfer at approx. 20 bit/min at low false alarm rate (1/min).",https://ieeexplore.ieee.org/document/4353479/,2007 29th Annual International Conference of the IEEE Engineering in Medicine and Biology Society,22-26 Aug. 2007,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICWS.2014.79,Towards Effectively Identifying RESTful Web Services,IEEE,Conferences,"In recent years, RESTful Web services have been rapidly developed and deployed, because of the advantages of lightweight, flexibility and extensibility, etc. However, most RESTful services are described in heterogeneous and ordinary HTML pages, which makes them really difficult to be identified and crawled automatically from the Internet. In this paper we propose a hybrid classifier framework called co-NV for automatic identification of RESTful services on the Web. In our framework, web pages are analyzed and filtered according to the contents and structure characteristics of HTML documents, with Naïve Bayes classifier and Vector Space Model (VSM) respectively. Experiments with real RESTful services prove that our framework works effectively with high precision and recall rate, and is very practical.",https://ieeexplore.ieee.org/document/6928939/,2014 IEEE International Conference on Web Services,27 June-2 July 2014,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ASAP.2018.8445099,Towards Hardware Accelerated Reinforcement Learning for Application-Specific Robotic Control,IEEE,Conferences,"Reinforcement Learning (RL) is an area of machine learning in which an agent interacts with the environment by making sequential decisions. The agent receives reward from the environment based on how good the decisions are and tries to find an optimal decision-making policy that maximises its longterm cumulative reward. This paper presents a novel approach which has showon promise in applying accelerated simulation of RL policy training to automating the control of a real robot arm for specific applications. The approach has two steps. First, design space exploration techniques are developed to enhance performance of an FPGA accelerator for RL policy training based on Trust Region Policy Optimisation (TRPO), which results in a 43% speed improvement over a previous FPGA implementation, while achieving 4.65 times speed up against deep learning libraries running on GPU and 19.29 times speed up against CPU. Second, the trained RL policy is transferred to a real robot arm. Our experiments show that the trained arm can successfully reach to and pick up predefined objects, demonstrating the feasibility of our approach.",https://ieeexplore.ieee.org/document/8445099/,"2018 IEEE 29th International Conference on Application-specific Systems, Architectures and Processors (ASAP)",10-12 July 2018,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICSE-SEIP52600.2021.00027,Towards Inclusive Software Engineering Through A/B Testing: A Case-Study at Windows,IEEE,Conferences,"Engineering software to be inclusive of all those that might/could/should use the software is important. However, today, data used to engineer software can have inherent biases (e.g. gender identity) with inclusiveness concerns. While much attention has been given to this topic in the AI/ML space, in this paper, we examine another data-centric software engineering process, A/B testing, for which we have a dearth of understanding today. Using real-world data from the Windows out of box experience (OOBE) feature, we provide a case-study of how inclusiveness concerns can manifest in A/B testing, practical adjustments to A/B testing towards inclusive software engineering, and insights into ongoing challenges. We discuss implications for research and practice.",https://ieeexplore.ieee.org/document/9402142/,2021 IEEE/ACM 43rd International Conference on Software Engineering: Software Engineering in Practice (ICSE-SEIP),25-28 May 2021,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/NOMS54207.2022.9789914,Towards Interpretable Anomaly Detection: Unsupervised Deep Neural Network Approach using Feedback Loop,IEEE,Conferences,"As telecom networks generate high-dimensional data, it becomes important to support large numbers of co-existing network attributes and to provide an interpretable and eXplainable Artificial Intelligence (XAI) anomaly detection system. Most state-of-the-art techniques tackle the problem of detecting network anomalies with high precision but the models don&#x2019;t provide an interpretable solution. This makes it hard for operators to adopt the given solutions. The proposed Cluster Characterized Autoencoder (CCA) architecture improves model interpretability by designing an end-to-end data driven AI-based framework. Candidate anomalies identified using the feature optimised Autoencoder and entropy based feature ranking are clustered in reconstruction error space using subspace clustering. This clustering is seen to separate true positives and false positives and how well this is done is evaluated using entropy and information gain. A two dimensional t-SNE representation of anomaly clusters is used as a graphical interface to the analysis and explanation of individual anomalies using SHAP values. The solution provided by this unsupervised approach helps the analyst in the categorisation, identification and feature explanation of anomalies providing faster root cause analysis. Therefore, our solution provides better support for the network domain analysts with an interpretable and explainable Artificial Intelligence (AI) anomaly detection system. Experiments on a real-world telecom network dataset demonstrate the efficacy of our proposed algorithm.",https://ieeexplore.ieee.org/document/9789914/,NOMS 2022-2022 IEEE/IFIP Network Operations and Management Symposium,25-29 April 2022,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCV.2013.129,Towards Motion Aware Light Field Video for Dynamic Scenes,IEEE,Conferences,"Current Light Field (LF) cameras offer fixed resolution in space, time and angle which is decided a-priori and is independent of the scene. These cameras either trade-off spatial resolution to capture single-shot LF or tradeoff temporal resolution by assuming a static scene to capture high spatial resolution LF. Thus, capturing high spatial resolution LF video for dynamic scenes remains an open and challenging problem. We present the concept, design and implementation of a LF video camera that allows capturing high resolution LF video. The spatial, angular and temporal resolution are not fixed a-priori and we exploit the scene-specific redundancy in space, time and angle. Our reconstruction is motion-aware and offers a continuum of resolution tradeoff with increasing motion in the scene. The key idea is (a) to design efficient multiplexing matrices that allow resolution tradeoffs, (b) use dictionary learning and sparse representations for robust reconstruction, and (c) perform local motion-aware adaptive reconstruction. We perform extensive analysis and characterize the performance of our motion-aware reconstruction algorithm. We show realistic simulations using a graphics simulator as well as real results using a LCoS based programmable camera. We demonstrate novel results such as high resolution digital refocusing for dynamic moving objects.",https://ieeexplore.ieee.org/document/6751235/,2013 IEEE International Conference on Computer Vision,1-8 Dec. 2013,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICDCSW53096.2021.00009,Towards Understanding the Adaptation Space of AI-Assisted Data Protection for Video Analytics at the Edge,IEEE,Conferences,"Edge computing facilitates the deployment of distributed AI applications, capable of processing video data in real time. AI-assisted video analytics can provide valuable information and benefits in various domains. Face recognition, object detection, or movement tracing are prominent examples enabled by this technology. However, such mechanisms also entail threats regarding privacy and security, for example if the video contains identifiable persons. Therefore, adequate data protection is an increasing concern in video analytics. AI-assisted data protection mechanisms, such as face blurring, can help, but are often computationally expensive. Additionally, the heterogeneous hardware of end devices and the time-varying load on edge services need to be considered. Therefore, such systems need to adapt to react to changes during their operation, ensuring that conflicting requirements on data protection, performance, and accuracy are addressed in the best possible way. Sound adaptation decisions require an understanding of the adaptation options and their impact on different quality attributes. In this paper, we identify factors that can be adapted in AI-assisted data protection for video analytics using the example of a face blurring pipeline. We measure the impact of these factors using a heterogeneous edge computing hardware testbed. The results show a large and complex adaptation space, with varied impacts on data protection, performance, and accuracy.",https://ieeexplore.ieee.org/document/9545916/,2021 IEEE 41st International Conference on Distributed Computing Systems Workshops (ICDCSW),7-10 July 2021,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICC.2010.5501969,Towards a Theory of Generalizing System Call Representation for In-Execution Malware Detection,IEEE,Conferences,"The major contribution of this paper is two-folds: (1) we present our novel variable-length system call representation scheme compared to existing fixed- length sequence schemes, and (2) using this representation, we present our in-execution malware detector that can not only identify zero-day malware without any a priori knowledge but can also detect a malicious process while it is executing. Our representation scheme - a more generalized version of n-gram - can be visualized in a k-dimensional hyperspace in which processes move depending upon their sequence of system calls. The process marks its impact in space by generating hyper-grams that are later used to evaluate an unknown process according to their profile. The proposed technique is evaluated on a real world dataset extracted from a Linux System. The results of our analysis show that our in-execution malware detector with hyper- gram representation achieves low processing overheads and improved detection accuracies as compared to conventional n-grams.",https://ieeexplore.ieee.org/document/5501969/,2010 IEEE International Conference on Communications,23-27 May 2010,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IC2E.2018.00038,"Towards an Adaptive, Fully Automated Performance Modeling Methodology for Cloud Applications",IEEE,Conferences,"The advent of the Cloud computing era along with the wide adoption of the distributed paradigm has enabled applications to increase their performance standards and greatly extend their scalability limits. Nevertheless, the ability of modern applications to be deployed in numerous different ways has complicated their structure and enormously increased the difficulty of extracting accurate performance models for them. This capability is crucial for many operations, such as the identification of the most appropriate execution setups for an anticipated workload, finding bottlenecks, etc. In this work, we propose a fully automated performance modeling methodology that aims at the creation of highly accurate performance models for a given maximum number of deployments. The main idea of the proposed methodology lies on the ""smart"" exploration of the application configuration space, the selection and deployment of a set of representative application configurations and the construction of the performance model through the adoption of Machine Learning techniques. Moreover, taking into consideration the often unstable and error-prone nature of the cloud, the proposed system attempts to overcome transient cloud failures that occur during the application deployment phase through the re-execution of the parts that failed. Our evaluation, conducted for a set of real-world applications frequently deployed over cloud infrastructures, indicates that our system is capable of both constructing performance models of high accuracy and doing so in an efficient manner, fixing application deployments that present errors without requiring human intervention.",https://ieeexplore.ieee.org/document/8360323/,2018 IEEE International Conference on Cloud Engineering (IC2E),17-20 April 2018,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IV.2014.39,Towards the Identification of Consumer Trajectories in Geo-Located Search Data,IEEE,Conferences,"Modern geo-positioning system (GPS) enabled smart phones are generating an increasing volume of information about their users, including geo-located search, movement, and transaction data. While this kind of data is increasingly rich and offers many grand opportunities to identify patterns and predict behaviour of groups and individuals, it is not immediately obvious how to develop a framework for extracting plausible inferences from these data. In our case, we have access to a large volume of real user data from the Point smart phone application, and we have developed a generic and layered system architecture to incrementally find aggregate items of interest within that data. This includes time and space correlations, e.g., are people searching for dinner and a movie, distributions of usage patterns and platforms, e.g., geographic distribution of Android, Apple, and BlackBerry users, and clustering to identify relatively complex search and movement patterns we call ""consumer trajectories."" Our pursuit of these kinds of patterns has helped guide our development of information extraction, machine learning, and visualization methods that provide systematic tools for investigating the geo-located data, and for the development of both conceptual tools and visualization tools in aid of finding both interesting and useful patterns in that data. Included in our system architecture is the ability to consider the difference between exploratory and explanatory hypotheses on data patterns, as well as the deployment of multiple visualization methods that can provide alternatives to help expose interesting patterns. In our introduction to our framework here, we provide examples of formulating hypotheses on geo-located behaviour, and how a variety of methods including those from machine learning and visualization, can help confirm or deny the value of such hypotheses as they emerge. In this particular case, we provide an initial basis for identifying semantically motivated data artifacts we call geo-located consumer trajectories. We investigate their plausibility with a variety of time and space series clustering and visualization models.",https://ieeexplore.ieee.org/document/6902904/,2014 18th International Conference on Information Visualisation,16-18 July 2014,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ASCC.2017.8287268,Towards the realtime sampling-based kinodynamic planning for quadcopters,IEEE,Conferences,"This paper presents a sampling-based kinodynamic planning algorithm for quadcopters which have a high dimensional state space amid state constraints. The proposed approach utilizes BIT∗ as the main planner to perform the ordered search directly in state space, without decoupling the geometric and dynamic constraints. Instead of using a general BVP solver for non-trivial local steering problem, a neural network based approximation is adopted to boost the computational efficiency. The potential of the proposed approach towards a real-time onboard implementation is demonstrated by various simulations and a real flight test.",https://ieeexplore.ieee.org/document/8287268/,2017 11th Asian Control Conference (ASCC),17-20 Dec. 2017,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ROMAN.2017.8172430,"Towards the use of consumer-grade electromyographic armbands for interactive, artistic robotics performances",IEEE,Conferences,"In recent years, gesture-based interfaces have been explored in order to control robots in non-traditional ways. These require the use of systems that are able to track human body movements in 3D space. Deploying Mo-cap or camera systems to perform this tracking tend to be costly, intrusive, or require a clear line of sight, making them ill-adapted for artistic performances. In this paper, we explore the use of consumer-grade armbands (Myo armband) which capture orientation information (via an inertial measurement unit) and muscle activity (via electromyography) to ultimately guide a robotic device during live performances. To compensate for the drop in information quality, our approach rely heavily on machine learning and leverage the multimodality of the sensors. In order to speed-up classification, dimensionality reduction was performed automatically via a method based on Random Forests (RF). Online classification results achieved 88% accuracy over nine movements created by a dancer during a live performance, demonstrating the viability of our approach. The nine movements are then grouped into three semantically-meaningful moods by the dancer for the purpose of an artistic performance achieving 94% accuracy in real-time. We believe that our technique opens the door to aesthetically-pleasing sequences of body motions as gestural interface, instead of traditional static arm poses.",https://ieeexplore.ieee.org/document/8172430/,2017 26th IEEE International Symposium on Robot and Human Interactive Communication (RO-MAN),28 Aug.-1 Sept. 2017,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICIVC47709.2019.8980828,Traffic Lights Detection and Recognition Algorithm Based on Multi-feature Fusion,IEEE,Conferences,"Detection and recognition of traffic lights is important for intelligent assisted driving. Traditional color space based traffic lights detection algorithms could be easily affected by other objects (such as buildings, car taillights) in the surrounding environment, and the detection accuracy and real-time performance are not ideal enough. Generally, the deep learning based methods have better advantages of real-time and accuracy performance for the normal scene with obvious traffic lights targets. However, the small traffic lights targets detection rate and accuracy in night-time of these methods are still can't be satisfactory. To solve this problem, this paper proposed a novel traffic lights detection and recognition algorithm based on multi-feature fusion, which can be implemented in two steps (detection and recognition). For the first step, the SLIC (simple linear iterative clustering) super-pixel segmentation algorithm is used for purposes reducing the image data processing complexity and improving the real-time performance. The mean-shift algorithm was used to cluster the HSV (Hue, Saturation, Value) color space components respectively for enhancing the target data and reducing the interference from other targets. For the second step, the feature information extracted by CNN (Convolutional Neural Network) and HOG(Histogram of Oriented Gradient) feature are fused. The SVM (Support Vector Machine) classifier is trained on a data set of traffic lights established by our own. To verify the proposed algorithm in this paper, amount of experiments were carried out in real traffic scenes. Experimental results show that this algorithm almost has the same real-time performance with YOLO_V3 neural network and a better accuracy.",https://ieeexplore.ieee.org/document/8980828/,"2019 IEEE 4th International Conference on Image, Vision and Computing (ICIVC)",5-7 July 2019,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/GLOBECOM38437.2019.9014245,Traffic Offloading and Power Allocation for Green HetNets Using Reinforcement Learning Method,IEEE,Conferences,"In order to satisfy the boosting mobile traffic demand, the deployment of small cells has been regarded as a feasible solution. But the growth of network infrastructure leads to a tremendous increase of energy consumption. Using renewable energy harvested from the environment to power the small cell base station, forming green heterogeneous networks (HetNets), can help reduce the conventional energy consumption. However, the stochastic user demand and random renewable energy harvesting amount have brought new challenges for the network operation. Based on reinforcement learning, this paper proposes a decentralized and a centralized base station operation scheme. Assuming each base station operates individually, the energy efficiency maximization problem is modeled as a general-sum game. After defining the state, action and reward of each base station, the problem can be solved by multi-agent reinforcement learning. Assuming there is a centralized controller, then the whole network can be seen as a huge agent, thus, the problem can be solved using deep reinforcement learning since the state space is too complicated. Simulation results show the centralized scheme shows higher performance but need more signaling overheads. The energy efficiency is lower for the decentralized scheme but it can be realized easier in real life. Both our proposed scheme can achieve significant energy efficiency improvement compared to the greedy scheme.",https://ieeexplore.ieee.org/document/9014245/,2019 IEEE Global Communications Conference (GLOBECOM),9-13 Dec. 2019,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICSGEA51094.2020.00079,Traffic-DesNet: A Robust Deep Learning-based Flow Prediction Model By The Using of DesNet,IEEE,Conferences,"Real-time and accurate short-term traffic flow forecasting has become a critical problem in intelligent transportation systems (ITS). Short-term traffic flow is not only affected by time, but also by space. Ordinary short-term traffic prediction methods only solve the impact of time but not the impact of space. Convolutional neural networks can use convolution kernels to resolve spatial correlations and dependencies between time and space, so they can more accurately predict short-term traffic flows. We introduce DenseNet for short-term traffic flow prediction, use convolution kernels to extract features between time and space, and superimpose the features extracted from the upper layer to extract new features. It can effectively use high-level information to discover new features at the bottom again. To better capture the implicit correlation in some areas. It can be found through experiments that the model is not only superior in accuracy to traditional methods but also has a significant improvement in efficiency.",https://ieeexplore.ieee.org/document/9260368/,2020 5th International Conference on Smart Grid and Electrical Automation (ICSGEA),13-14 June 2020,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IJCNN.2000.861451,Training neural networks with threshold activation functions and constrained integer weights,IEEE,Conferences,"Evolutionary neural network training algorithms are presented. These algorithms are applied to train neural networks with weight values confined to a narrow band of integers. We constrain the weights and biases in the range [-2/sup k+1/+1, 2/sup k-1/-1], for k=3, 4, 5, thus they can be represented by just k bits. Such neural networks are better suited for hardware implementation than the real weight ones. Mathematical operations that are easy to implement in software might often be very burdensome in the hardware and therefore more costly. Hardware-friendly algorithms are essential to ensure the functionality and cost effectiveness of the hardware implementation. To this end, in addition to the integer weights, the trained neural networks use threshold activation functions only, so hardware implementation is even easier. These algorithms have been designed keeping in mind that the resulting integer weights require less bits to be stored and the digital arithmetic operations between them are easier to be implemented in hardware. Obviously, if the network is trained in a constrained weight space, smaller weights are found and less memory is required. On the other hand, as we have found here, the network training procedure can be more effective and efficient when larger weights are allowed. Thus, for a given application a trade off between effectiveness and memory consumption has to be considered. Our intention is to present results of evolutionary algorithms on this difficult task. Based on the application of the proposed class of methods on classical neural network benchmarks, our experience is that these methods are effective and reliable.",https://ieeexplore.ieee.org/document/861451/,Proceedings of the IEEE-INNS-ENNS International Joint Conference on Neural Networks. IJCNN 2000. Neural Computing: New Challenges and Perspectives for the New Millennium,27-27 July 2000,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS51168.2021.9636842,Trajectory-based Split Hindsight Reverse Curriculum Learning,IEEE,Conferences,"Grasping is one of the most fundamental problems in robotic manipulation. In recent years, with the development of data-driven methods, reinforcement learning has been used in solving robotic grasping problems. However, grasping is a long-horizon and sparse reward task, whose natural reward only appears when the task is successfully achieved. Therefore, it brings great challenges to the deployment of reinforcement learning methods. To tackle this difficulty, we propose a new method called Trajectory-based Split Hindsight Reverse Curriculum Learning. This method of reverse learning from the goal can greatly improve the learning efficiency and the final performance of the tasks. Specifically, based on referred trajectories, the agent starts to learn in a small state space near the goal and then gradually in larger state spaces until covering the entire state space. Through split hindsight experience replay, the sampled trajectory is divided into segments that match the current subspace's size; then, they are modified to successful trajectories to enable more efficient learning. In both simulation and real-world experiments, our method surpasses the existing methods and achieves the goal-oriented grasping tasks with higher success rates and better data efficiencies. The detailed experimental results can be viewed at https://youtu.be/7uNRzmRZhDk.",https://ieeexplore.ieee.org/document/9636842/,2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),27 Sept.-1 Oct. 2021,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICIP40778.2020.9191037,Transductive Prototypical Network For Few-Shot Classification,IEEE,Conferences,"Few-shot learning is the key step towards human-level intelligence. Prototypical Network is a promising approach to address the key issue of over-fitting for few-shot learning. Nevertheless, the original Prototypical Network only uses one or few labeled instances to represent the corresponding class, which easily deviates from the real class distribution leading to the imprecise classification results. In this paper, we propose Transductive Prototypical Network (Td-PN), a universal transductive approach that refines the class representations by merging scarce labeled samples and high-confidence ones of target set. Our proposed Td-PN first maps the samples to a classifying-friendly (discriminative) embedding space by redesigning a weighted contrastive loss, then utilizes the transductive inference to obtain the powerful prototype representation for each class. Experiments demonstrate that our approach outperforms the state-of-the-art algorithms.",https://ieeexplore.ieee.org/document/9191037/,2020 IEEE International Conference on Image Processing (ICIP),25-28 Oct. 2020,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SEAMS.2017.11,Transfer Learning for Improving Model Predictions in Highly Configurable Software,IEEE,Conferences,"Modern software systems are built to be used in dynamic environments using configuration capabilities to adapt to changes and external uncertainties. In a self-adaptation context, we are often interested in reasoning about the performance of the systems under different configurations. Usually, we learn a black-box model based on real measurements to predict the performance of the system given a specific configuration. However, as modern systems become more complex, there are many configuration parameters that may interact and we end up learning an exponentially large configuration space. Naturally, this does not scale when relying on real measurements in the actual changing environment. We propose a different solution: Instead of taking the measurements from the real system, we learn the model using samples from other sources, such as simulators that approximate performance of the real system at low cost. We define a cost model that transform the traditional view of model learning into a multi-objective problem that not only takes into account model accuracy but also measurements effort as well. We evaluate our cost-aware transfer learning solution using real-world configurable software including (i) a robotic system, (ii) 3 different stream processing applications, and (iii) a NoSQL database system. The experimental results demonstrate that our approach can achieve (a) a high prediction accuracy, as well as (b) a high model reliability.",https://ieeexplore.ieee.org/document/7968130/,2017 IEEE/ACM 12th International Symposium on Software Engineering for Adaptive and Self-Managing Systems (SEAMS),22-23 May 2017,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICDM.2010.146,Transfer Learning via Cluster Correspondence Inference,IEEE,Conferences,"Transfer learning targets to leverage knowledge from one domain for tasks in a new domain. It finds abundant applications, such as text/sentiment classification. Many previous works are based on cluster analysis, which assume some common clusters shared by both domains. They mainly focus on the one-to-one cluster correspondence to bridge different domains. However, such a correspondence scheme might be too strong for real applications where each cluster in one domain corresponds to many clusters in the other domain. In this paper, we propose a Cluster Correspondence Inference (CCI) method to iteratively infer many-to-many correspondence among clusters from different domains. Specifically, word clusters and document clusters are exploited for each domain using nonnegative matrix factorization, then the word clusters from different domains are corresponded in a many-to-many scheme, with the help of shared word space as a bridge. These two steps are run iteratively and label information is transferred from source domain to target domain through the inferred cluster correspondence. Experiments on various real data sets demonstrate that our method outperforms several state-of-the-art approaches for cross-domain text classification.",https://ieeexplore.ieee.org/document/5694061/,2010 IEEE International Conference on Data Mining,13-17 Dec. 2010,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/WCNC.2018.8377359,Transferring knowledge for tilt-dependent radio map prediction,IEEE,Conferences,"Fifth generation wireless networks (5G) will face key challenges caused by diverse patterns of traffic demands and massive deployment of heterogeneous access points. In order to handle this complexity, machine learning techniques are expected to play a major role. However, due to the large space of parameters related to network optimization, collecting data to train models for all possible network configurations can be prohibitive. In this paper, we analyze the possibility of performing a knowledge transfer, in which a machine learning model trained on a particular network configuration is used to predict a quantity of interest in a new, unknown setting. We focus on the tilt-dependent received signal strength maps as quantities of interest and we analyze two cases where the knowledge acquired for a particular antenna tilt setting is transferred to (i) a different tilt configuration of the same antenna or (ii) a different antenna with the same tilt configuration. Promising results supporting knowledge transfer are obtained through extensive experiments conducted using different machine learning models on a real dataset.",https://ieeexplore.ieee.org/document/8377359/,2018 IEEE Wireless Communications and Networking Conference (WCNC),15-18 April 2018,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICTAI.2014.37,Triangulation Versus Graph Partitioning for Tackling Large Real World Qualitative Spatial Networks,IEEE,Conferences,"There has been interest in recent literature in tackling very large real world qualitative spatial networks, primarily because of the real datasets that have been, and are to be, offered by the Semantic Web community and scale up to millions of nodes. The proposed techniques for tackling such large networks employ the following two approaches for retaining the sparseness of their underlying graphs and reasoning with them: (i) graph triangulation and sparse matrix implementation, and (ii) graph partitioning and parallelization. Regarding the latter approach, an implementation has been offered recently, presented in [AAAI, 2014]. However, although the implementation looks promising and with space for improvement, an improper use of competing solvers in the evaluation process resulted in the wrong conclusion that it is able to provide fast consistency for very large qualitative spatial networks with respect to the state-of-the-art. In this paper, we review the two aforementioned approaches and provide new results that are different to the results presented in [AAAI, 2014] by properly re-evaluating them with the benchmark dataset of that paper. Thus, we establish a clear view on the state-of-the-art solutions for reasoning with large real world qualitative spatial networks efficiently, which is the main result of this paper.",https://ieeexplore.ieee.org/document/6984473/,2014 IEEE 26th International Conference on Tools with Artificial Intelligence,10-12 Nov. 2014,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.23919/FUSION49465.2021.9626895,Tuning Multi Object Tracking Systems using Bayesian Optimization,IEEE,Conferences,"Tracking-by-Detection has become the major paradigm in Multi Object Tracking (MOT) for a large variety of sensors. Regardless of the type of tracking system, hyper parameters are often chosen manually instead of doing a structured search to reveal the full potential of the system.In this work we tackle this problem by utilizing Bayesian Optimization (BO) to tune tracking systems, enabling to find the best combination of hyper parameters for Gaussian Mixture Probability Hypothesis Density Trackers (GM-PHD) in two different tracking applications. We use the Tree-structured Parzen Estimator (TPE) algorithm [1] [2] with an Expected Improvement (EI) acquisition function as a blackbox optimizer. TPE supports to conveniently incorporate domain expert knowledge by modeling prior probability distributions of the search space. In our experiments we use the popular MOTA metric as optimization objective.Evaluation is performed in a simulation scenario with an in depth discussion of the found parameters and a real world example that uses the MOT-20 challenge dataset [3] demonstrates the unconditional applicability of the approach. We finish with a conclusion on Bayesian Optimization for MOT systems and future research.",https://ieeexplore.ieee.org/document/9626895/,2021 IEEE 24th International Conference on Information Fusion (FUSION),1-4 Nov. 2021,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SPAWC51858.2021.9593170,Turning Channel Noise into an Accelerator for Over-the-Air Principal Component Analysis,IEEE,Conferences,"T In recent years, the attempts on distilling mobile data into useful knowledge have led to the deployment of machine learning algorithms at the network edge. Principal component analysis (PCA) is a classic technique for extracting the linear structure of a dataset, which is useful for feature extraction and data compression. In this work, we propose the deployment of distributed PCA over a multi-access channel based on the algorithm of stochastic gradient descent to learn the dominant feature space of a distributed dataset at multiple devices. Over- the-air aggregation is adopted to reduce the multi-access latency, giving the name over-the-air PCA. The novelty of this design lies in exploiting channel noise to accelerate the descent in the region around each saddle point encountered by gradient descent, thereby increasing the convergence speed of over-the-air PCA. The idea is materialized by proposing a power-control scheme controlling the level of channel noise accordingly. The scheme is proved to achieve faster convergence than in the case without power control by experiments on real datasets.",https://ieeexplore.ieee.org/document/9593170/,2021 IEEE 22nd International Workshop on Signal Processing Advances in Wireless Communications (SPAWC),27-30 Sept. 2021,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/VRW52623.2021.00262,Turning a Messy Room into a Fully Immersive VR Playground,IEEE,Conferences,"In this study, to enable a VR experience with an HMD even in a space with obstacles, we developed a real-time construction system of a reality-based VR space that does not impair the atmosphere of the virtual world. In addition, we aim to construct a VR space that is easier to recognize its structure by classifying ""objects that are boundaries of space"" and ""ordinary obstacles"" using a deep learning network and superimposing virtual objects corresponding to each type of real object. We implemented a real-time construction system for a VR space that reflects the distribution of objects in a real space using two depth cameras mounted on HMD, and created guidelines and applications for using the proposed system.",https://ieeexplore.ieee.org/document/9419119/,2021 IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops (VRW),27 March-1 April 2021,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IEMBS.1997.757650,Two dimensional interleaved differential k-space sampling for fluoroscopic triggering of contrast-enhanced three dimensional MR angiography,IEEE,Conferences,"The purpose of this work is to demonstrate how 2D differential k-space sampling can be implemented with interleaving of the view (phase encoding) order, providing smoother temporal behavior and no spatial resolution penalty when compared to standard sequential k-space ordering. Implementation in a clinical setting provides a method for real-time bolus tracking and timing for contrast-enhanced 3D MR angiography.",https://ieeexplore.ieee.org/document/757650/,Proceedings of the 19th Annual International Conference of the IEEE Engineering in Medicine and Biology Society. 'Magnificent Milestones and Emerging Opportunities in Medical Engineering' (Cat. No.97CH36136),30 Oct.-2 Nov. 1997,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISIC.2000.882942,Two suggestions for efficient implementation of CMAC's,IEEE,Conferences,"The CMAC (Cerebellar Model Articulation Controller) suffers from two important problems: the huge amount of memory needed for its implementation in many common situations, and the lack of a systematic way for selecting appropriate values for its parameters, particularly number of quantization intervals. This paper presents two proposals for addressing these difficulties: 1) a dynamic implementation that requires memory only for those weights needed to represent the training data set, and that performs linear interpolation when a query using other weights is requested; and 2) consists of the definition of an index of correlation from which the optimal number of quantization intervals that should be assigned to each dimension of the input space that can be found. Experiments are performed for two synthetic cases and for one set of real data. These are used to model the dynamic behaviour of a real sensor-based car. Figures are given to show the memory savings and mean squared error obtained.",https://ieeexplore.ieee.org/document/882942/,Proceedings of the 2000 IEEE International Symposium on Intelligent Control. Held jointly with the 8th IEEE Mediterranean Conference on Control and Automation (Cat. No.00CH37147),19-19 July 2000,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1145/3510003.3510124,Type4Py: Practical Deep Similarity Learning-Based Type Inference for Python,IEEE,Conferences,"Dynamic languages, such as Python and Javascript, trade static typing for developer flexibility and productivity. Lack of static typing can cause run-time exceptions and is a major factor for weak IDE support. To alleviate these issues, PEP 484 introduced optional type annotations for Python. As retrofitting types to existing code-bases is error-prone and laborious, machine learning (ML)-based approaches have been proposed to enable automatic type infer-ence based on existing, partially annotated codebases. However, previous ML-based approaches are trained and evaluated on human-provided type annotations, which might not always be sound, and hence this may limit the practicality for real-world usage. In this paper, we present TYPE4Py, a deep similarity learning-based hier-archical neural network model. It learns to discriminate between similar and dissimilar types in a high-dimensional space, which results in clusters of types. Likely types for arguments, variables, and return values can then be inferred through the nearest neigh-bor search. Unlike previous work, we trained and evaluated our model on a type-checked dataset and used mean reciprocal rank (MRR) to reflect the performance perceived by users. The obtained results show that TYPE4Py achieves an MRR of 77.1 &#x0025;, which is a substantial improvement of 8.1&#x0025; and 16.7&#x0025; over the state-of-the-art approaches Typilus and Typewriter, respectively. Finally, to aid developers with retrofitting types, we released a Visual Stu-dio Code extension, which uses TYPE4Py to provide ML-based type auto-completion for Python.",https://ieeexplore.ieee.org/document/9793925/,2022 IEEE/ACM 44th International Conference on Software Engineering (ICSE),25-27 May 2022,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IEIT53597.2021.00077,UAV Control in Smart City Based on Space-Air-Ground Integrated Network,IEEE,Conferences,"Unmanned Aerial Vehicle (UAV) is an important part of the wireless network system of the future smart city. As a difficult point in the large-scale application of UAV, UAV control gradually attracts people's attention. Aiming at the problems of UAV control in smart city application, a near real time online learning architecture of UAV control based on the software-defined space-air-ground integrated network (SSAG) was proposed. This architecture uses the two-layer software defined network (SDN) controller architecture of SSAG framework to separate UAV control. The upper-tier SDN controller is responsible for the scheduling of UAV configuration, while the lower-tier SDN controller is responsible for regional coordination of UAV. The upper-tier SDN controller updates the tendency of network states by acquiring network states information in time interval. By simulating the network state in the next time interval, the optimal strategy of UAV scheduling of the next time interval is obtained by using the strategy iteration algorithm. Finally, an example is given to verify that the near real-time online learning architecture can accurately predict the UAV requirement, and increase the throughput of the network system compared with the traditional approach.",https://ieeexplore.ieee.org/document/9526159/,"2021 International Conference on Internet, Education and Information Technology (IEIT)",16-18 April 2021,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/RCAR47638.2019.9043987,UAV Path Planning Based on Biological Excitation Neural Network and Visual Odometer,IEEE,Conferences,"Unmanned aerial vehicle(UAV) have been widely used in military and civil fields due to their compact structure, flexible mobility, low cost and other advantages. With the development of artificial intelligence in recent years, more intelligent and advanced algorithms have appeared, in which machine vision, as an important branch in the field of artificial intelligence, has also been greatly developed. The limitation of space, load, endurance and computing capacity hinders the application of intelligent algorithms on UAV. In the paper a semi-autonomous control platform of the quadrotor UAV was developed and the upper and lower dual control core architecture is implemented. Based on the hardware platform, the improved visual inertia odometer (VIO) and the biological excitation neural network are used to improve the flight performance and the ability of autonomy. To solve the problem of the synchronization for VIO, a cubic spline interpolation function was employed. A biological excitation neural network was extended to solve UAV on-line path planning. It provides an on-board path planning approach for UAV in the 3D world considering the dynamic obstacles. Finally, the feasibility and stability of the designed system were verified by flight experiments.",https://ieeexplore.ieee.org/document/9043987/,2019 IEEE International Conference on Real-time Computing and Robotics (RCAR),4-9 Aug. 2019,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AERO.2017.7943775,UAV tracking and following a ground target under motion and localisation uncertainty,IEEE,Conferences,"Unmanned Aerial Vehicles (UAVs) are increasingly being used in numerous applications, such as remote sensing, environmental monitoring, ecology and search and rescue missions. Effective use of UAVs depends on the ability of the system to navigate in the mission scenario, especially if the UAV is required to navigate autonomously. There are particular scenarios in which UAV navigation faces challenges and risks. This creates the need for robust motion planning capable of overcoming different sources of uncertainty. One example is a UAV flying to search, track and follow a mobile ground target in GPS-denied space, such as below canopy or in between buildings, while avoiding obstacles. A UAV navigating under these conditions can be affected by uncertainties in its localization and motion due to occlusion of GPS signals and the use of low cost sensors. Additionally, the presence of strong winds in the airspace can disturb the motion of the UAV. In this paper, we describe and flight test a novel formulation of a UAV mission for searching, tracking and following a mobile ground target. This mission is formulated as a Partially Observable Markov Decision Process (POMDP) and implemented in real flight using a modular framework. We modelled the UAV dynamic system, the uncertainties in motion and localization of both the UAV and the target, and the wind disturbances. The framework computes a motion plan online for executing motion commands instead of flying to way-points to accomplish the mission. The system enables the UAV to plan its motion allowing it to execute information gathering actions to reduce uncertainty by detecting landmarks in the scenario, while making predictions of the mobile target trajectory and the wind speed based on observations. Results indicate that the system overcomes uncertainties in localization of both the aircraft and the target, and avoids collisions into obstacles despite the presence of wind. This research has the potential of use particularly for remote monitoring in the fields of biodiversity and ecology.",https://ieeexplore.ieee.org/document/7943775/,2017 IEEE Aerospace Conference,4-11 March 2017,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SCC.2016.75,UCLAO and BHUC: Two Novel Planning Algorithms for Uncertain Web Service Composition,IEEE,Conferences,"The inherent uncertainty of Web service is the most important characteristic due to its deployment and invocation within a real and highly dynamic Internet environment. Web service composition with uncertainty (U-WSC) has become an important research issue in service computing. Although some research has been done on U-WSC via non-deterministic planning in Artificial Intelligence, they cannot handle the situation that uncertain Web services with the same functionality exist in a service repository and could not get all of possible solution plans that constitute an uncertain composition solution for a given request. To solve above research challenges, this paper models a U-WSC problem into a U-WSC planning problem. Accordingly, two novel uncertain planning algorithms using heuristic search called UCLAO* and BHUC, are presented to solve the U-WSC planning problem with state space reduction, which leads to high efficiency of finding a service composition solution. We have conducted empirical experiments based on a running example in e-commerce application as well as our large-scale simulated datasets. The experimental results demonstrate that our proposed algorithms outperform the state-of-the-art non-deterministic planning algorithms in terms of effectiveness, efficiency and scalability.",https://ieeexplore.ieee.org/document/7557495/,2016 IEEE International Conference on Services Computing (SCC),27 June-2 July 2016,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/COMSWA.2007.382562,Ubiquitous Semantic Space: A context-aware and coordination middleware for Ubiquitous Computing,IEEE,Conferences,"Ubiquitous computing poses the challenge of increased communication, context-awareness and functionality. In a highly dynamic and weekly connected ubiquitous environment, continuous access to the network (synchronous communication) is very difficult. So it's necessary to go for tuple space which provides asynchronous communication without any loss in data. Tuple space offers a coordination infrastructure for communication between autonomous entities by providing a logically shared memory along with data persistence, transactional security as well as temporal and spatial decoupling properties that make it desirable for distributed systems [2] such as ubiquitous computing. In order to automate the task and the system to be intelligent, context awareness is a must. This can be achieved by using semantic web technology. Existing middleware's for ubiquitous computing concentrates on RPC communication paradigm and deals with context-awareness separately. In our approach of constructing the middleware we provide common solution to both communication and context-awareness using ubiquitous semantic space. Ubiquitous semantic space [5] brings together tuple space, semantic web technologies and ubiquitous computing. Hence in this paper, we introduce a context-aware and co-ordination middleware framework for ubiquitous environment using ubiquitous semantic space. Ubiquitous semantic space uses ontologies to define the semantics of various concepts. Using ontologies facilitates different agents in the environments to have a common semantic understanding of different contexts. Ontology is represented using ontology web language, OWL [6]. We have modeled a ubiquitous semantic space ontology structure suitable for communicating conceptual information among the agents. Our model also incorporates context-triggered action which is more useful for real-time ubiquitous application having reactive behavior. For enabling context-triggered action, our model has reactive space into ubiquitous semantic space. Reactive space, stores rules written in SWRL [22], semantic web rule language and fired using JESS [3] reasoner at the appropriate time. The middleware could easily adapt to changes in the environment. The structure of the ubiquitous semantic space is designed in a fashion to have privacy among the communicating devices and the agents. Hence our middleware uses a decentralized architecture which supports asynchronous communication, context-awareness, context-sensitive communication, Privacy sensitive, adaptive to context-changes and reactive to emergency situation.",https://ieeexplore.ieee.org/document/4267986/,2007 2nd International Conference on Communication Systems Software and Middleware,7-12 Jan. 2007,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AIVR46125.2019.00072,Ubiquitous Virtual Humans: A Multi-platform Framework for Embodied AI Agents in XR,IEEE,Conferences,"We present an architecture and framework for the development of virtual humans for a range of computing platforms, including mobile, web, Virtual Reality (VR) and Augmented Reality (AR). The framework uses a mix of in-house and commodity technologies to support audio-visual sensing, speech recognition, natural language processing, nonverbal behavior generation and realization, text-to-speech generation, and rendering. This work builds on the Virtual Human Toolkit, which has been extended to support computing platforms beyond Windows. The resulting framework maintains the modularity of the underlying architecture, allows re-use of both logic and content through cloud services, and is extensible by porting lightweight clients. We present the current state of the framework, discuss how we model and animate our characters, and offer lessons learned through several use cases, including expressive character animation in seated VR, shared space and navigation in room-scale VR, autonomous AI in mobile AR, and real-time user performance feedback based on mobile sensors in headset AR.",https://ieeexplore.ieee.org/document/8942321/,2019 IEEE International Conference on Artificial Intelligence and Virtual Reality (AIVR),9-11 Dec. 2019,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/PerComWorkshops51409.2021.9431061,Ultra-fast Machine Learning Classifier Execution on IoT Devices without SRAM Consumption,IEEE,Conferences,"With the introduction of edge analytics, IoT devices are becoming smart and ready for AI applications. A few modern ML frameworks are focusing on the generation of small-size ML models (often in kBs) that can directly be flashed and executed on tiny IoT devices, particularly the embedded systems. Edge analytics eliminates expensive device-to-cloud communications, thereby producing intelligent devices that can perform energy-efficient real-time offline analytics. Any increase in the training data results in a linear increase in the size and space complexity of the trained ML models, making them unable to be deployed on IoT devices with limited memory. To alleviate the memory issue, a few studies have focused on optimizing and fine-tuning existing ML algorithms to reduce their complexity and size. However, such optimization is usually dependent on the nature of IoT data being trained. In this paper, we presented an approach that protects model quality without requiring any alteration to the existing ML algorithms. We propose SRAM-optimized implementation and efficient deployment of widely used standard/stable ML-frameworks classifier versions (e.g., from Python scikit-learn). Our initial evaluation results have demonstrated that ours is the most resource-friendly approach, having a very limited memory footprint while executing large and complex ML models on MCU-based IoT devices, and can perform ultra-fast classifications while consuming 0 bytes of SRAM. When we tested our approach by executing it on a variety of MCU-based devices, the majority of models ported and executed produced 1-4x times faster inference results in comparison with the models ported by the sklearn-porter, m2cgen, and emlearn libraries.",https://ieeexplore.ieee.org/document/9431061/,2021 IEEE International Conference on Pervasive Computing and Communications Workshops and other Affiliated Events (PerCom Workshops),22-26 March 2021,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CCWC47524.2020.9031228,Ultra-thin MobileNet,IEEE,Conferences,"Convolutional Neural Networks (CNNs) are deep learning architectures which play an important role in object detection, image classification, face recognition, autonomous driving applications, etc. MobileNet is a light CNN model which was developed especially for embedded vision applications. But still, it is quite challenging to deploy the baseline model into memory constrained micro-controller units. Design Space Exploration of the above-mentioned model can make it less memory and computationally intensive. This paper proposes some modifications to the existing baseline MobileNet architecture to make it more efficient and suitable to be deployed on real-time embedded platforms. The intent behind developing such an architecture is to reduce the size, the number of parameters, computation time per epoch and the overfitting problem considerably without letting the accuracy drop below the baseline accuracy level. We achieve good accuracy levels by using the Swish activation function instead of the standard activation function ReLU and introducing a regularization method called random erasing instead of Drop out into the network. We decrease the model size by using Separable Convolutions in place of Depthwise Separable Convolutions, changing the channel depth, choosing an optimum width multiplier value and eliminating some layers with the same output shape, without much drop in the accuracy levels. We train the model with the above-mentioned modifications from scratch on the CIFAR-10 dataset and obtain a much lighter architecture as compared to the baseline MobileNet V1. We name the new DNN architecture as Ultra-thin MobileNet having a size of 3.9 MB only which is deployable in real-time embedded processors with limited memory and power.",https://ieeexplore.ieee.org/document/9031228/,2020 10th Annual Computing and Communication Workshop and Conference (CCWC),6-8 Jan. 2020,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IoTDI.2018.00014,UnTran: Recognizing Unseen Activities with Unlabeled Data Using Transfer Learning,IEEE,Conferences,"The success and impact of activity recognition algorithms largely depends on the availability of the labeled training samples and adaptability of activity recognition models across various domains. In a new environment, the pre-trained activity recognition models face challenges in presence of sensing bias- ness, device heterogeneities, and inherent variabilities in human behaviors and activities. Activity Recognition (AR) system built in one environment does not scale well in another environment, if it has to learn new activities and the annotated activity samples are scarce. Indeed building a new activity recognition model and training the model with large annotated samples often help overcome this challenging problem. However, collecting annotated samples is cost-sensitive and learning activity model at wild is computationally expensive. In this work, we propose an activity recognition framework, UnTran that utilizes source domains' pre-trained autoencoder enabled activity model that transfers two layers of this network to generate a common feature space for both source and target domain activities. We postulate a hybrid AR framework that helps fuse the decisions from a trained model in source domain and two activity models (raw and deep-feature based activity model) in target domain reducing the demand of annotated activity samples to help recognize unseen activities. We evaluated our framework with three real-world data traces consisting of 41 users and 26 activities in total. Our proposed UnTran AR framework achieves ≈ 75% F1 score in recognizing unseen new activities using only 10% labeled activity data in the target domain. UnTran attains ≈ 98% F1 score while recognizing seen activities in presence of only 2-3% of labeled activity samples.",https://ieeexplore.ieee.org/document/8366975/,2018 IEEE/ACM Third International Conference on Internet-of-Things Design and Implementation (IoTDI),17-20 April 2018,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IV47402.2020.9304826,Uncertainty-aware Energy Management of Extended Range Electric Delivery Vehicles with Bayesian Ensemble,IEEE,Conferences,"In recent years, deep reinforcement learning (DRL) algorithms have been widely studied and utilized in the area of Intelligent Transportation Systems (ITS). DRL agents are mostly trained with transition pairs and interaction trajectories generated from simulation, and they can achieve satisfying or near optimal performances under familiar input states. However, for relative rare visited or even unvisited regions in the state space, there is no guarantee that the agent could perform well. Unfortunately, novel conditions are inevitable in real-world problems and there is always a gap between the real data and simulated data. Therefore, to implement DRL algorithms in real-world transportation systems, we should not only train the agent learn a policy that maps states to actions, but also the model uncertainty associated with each action. In this study, we adapt the method of Bayesian ensemble to train a group of agents with imposed diversity for an energy management system of a delivery vehicle. The agents in the ensemble agree well on familiar states but show diverse results on unfamiliar or novel states. This uncertainty estimation facilitates the implementation of interpretable postprocessing modules which can ensure robust and safe operations under high uncertainty conditions.",https://ieeexplore.ieee.org/document/9304826/,2020 IEEE Intelligent Vehicles Symposium (IV),19 Oct.-13 Nov. 2020,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.23919/ACC45564.2020.9147911,Unmanned Aerial Vehicle Angular Velocity Control via Reinforcement Learning in Dimension Reduced Search Spaces,IEEE,Conferences,"Search space dimension reduction strategies are studied for reinforcement learning based angular velocity control of multirotor unmanned aerial vehicles. Reinforcement learning approximates the value function iteratively over the state-action space, which is 6-dimensional in the case of multirotor angular velocity control. An inverse-dynamics approach is applied to reduce the 6-dimensional state-action space to a 3-dimensional state-only search space while estimating the uncertain model parameters. The search space dimension is further reduced when the state variables are only allowed to vary following either a motion camouflage strategy or a hyperbolic tangent path. Simulation results show that the modified reinforcement learning algorithms can be implemented in real time for multirotor angular velocity control.",https://ieeexplore.ieee.org/document/9147911/,2020 American Control Conference (ACC),1-3 July 2020,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICPR48806.2021.9412661,Unsupervised Domain Adaptation for Object Detection in Cultural Sites,IEEE,Conferences,"The ability to detect objects in cultural sites from the egocentric point of view of the user can enable interesting applications for both the visitors and the manager of the site. Unfortunately, current object detection algorithms have to be trained on large amounts of labeled data, the collection of which is costly and time-consuming. While synthetic data generated from the 3D model of the cultural site can be used to train object detection algorithms, a significant drop in performance is generally observed when such algorithms are deployed to work with real images. In this paper, we consider the problem of unsupervised domain adaptation for object detection in cultural sites. Specifically, we assume the availability of synthetic labeled images and real unlabeled images for training. To study the problem, we propose a dataset containing 75244 synthetic and 2190 real images with annotations for 16 different artworks. We hence investigate different domain adaptation techniques based on image-to-image translation and feature alignment. Our analysis points out that such techniques can be useful to address the domain adaptation issue, while there is still plenty of space for improvement on the proposed dataset. We release the dataset at our web page to encourage research on this challenging topic: https://iplab.dmi.unict.it/EGO-CH-OBJ-ADAPT/.",https://ieeexplore.ieee.org/document/9412661/,2020 25th International Conference on Pattern Recognition (ICPR),10-15 Jan. 2021,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CCDC49329.2020.9164102,Unsupervised Feature Transfer for Batch Process Based on Geodesic Flow Kernel,IEEE,Conferences,"The problem of misalignment of the original measurement model is caused by nonlinear, time-varying characteristic of the batch process. In this paper, a method based on geodesic flow kernel (GFK) for feature transfer is proposed. By mapping data into the manifold space, the feature transfer from source domain to target domain is implemented. Distribution adaptation of real-time data and modeling data is performed to reduce the distribution difference between them. The historical data through distribution adaptation is used to establish a regression model to predict the real-time data, by which the unsupervised batch process soft sensor modeling is realized. The application of predicting the concentration of penicillin between different batches during the fermentation of penicillin demonstrated that the prediction accuracy of the model can be improved more effectively than the traditional soft sensor method.",https://ieeexplore.ieee.org/document/9164102/,2020 Chinese Control And Decision Conference (CCDC),22-24 Aug. 2020,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SMC52423.2021.9658877,Unsupervised Person Re-identification via Diversity and Salience Clustering,IEEE,Conferences,"Obtaining large-scale annotations for person re-identification tasks is very difficult and hard to deploy in real scenarios. Existing unsupervised person re-identification methods focus on one-shot learning and domain adaptation, but they still rely on the labels of the source domain or a small number of sample labels. To solve this problem, we formulate a very simple but effective unsupervised person re-identification method: Diversity and Salience Clustering (DSC). The method generates a stable clustering feature space for unsupervised re-identification by considering the diversity and salience of pedestrian samples. As a pure unsupervised person re-identification model, our approach does not use any camera annotations, pedestrian labels and source domain data. Extensive experiments on two image datasets demonstrate that our proposed method outperforms the state-of-the-art unsupervised person re-identification methods.",https://ieeexplore.ieee.org/document/9658877/,"2021 IEEE International Conference on Systems, Man, and Cybernetics (SMC)",17-20 Oct. 2021,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CCNC49033.2022.9700501,Unsupervised root-cause identification of software bugs in 5G RAN,IEEE,Conferences,"Developers of complex system like 5G Radio Access Networks (RAN) need algorithms that can automatically locate the root causes of software bugs. Existing methods mainly use supervised learning to track down root causes and only a few of these provide enough information to identify the function in which a software bug occurs. Supervised learning methods work well when scenarios can be repeated, and the normal behavior is somewhat similar. In RAN, where thousands of different configurations are used, software is updated frequently, and each node has its own traffic intensity, using unsupervised learning that does not require any pre-training can be more suitable. The few existing methods that use unsupervised learning to locate the root cause of software bugs can only detect delays or software hangs, and are not able to identify the many types of bugs that occur in RAN. We propose a multi-step method that uses unsupervised learning to analyze kernel and user space traces in system logs. The methods can guide developers by suggesting top-k candidate functions that are likely to contain a software bug. Our methods, MultiSpace and CallGraph were evaluated using an advanced 5G testbed in which many different software bugs that are common in RAN, were injected. The results shows that MultiSpace and CallGraph, can detect a wider range of software bugs than previous methods and only adds an average CPU load of 1.3% on the testbed. An important aspect is also that our methods scale well with large amount of data produced by real time systems, like RAN, and can analyze the data much faster.",https://ieeexplore.ieee.org/document/9700501/,2022 IEEE 19th Annual Consumer Communications & Networking Conference (CCNC),8-11 Jan. 2022,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/COGINF.2009.5250777,User-oriented healthcare support system based on symbiotic computing,IEEE,Conferences,"We propose a multi-agent-based healthcare support system in ubiquitous computing environment. By utilizing knowledge about healthcare and various information including vital sign, physical location, and video data of a user under observation from real space, the system provides useful information regarding health condition effectively and in user-oriented manner. This paper describes a user-oriented healthcare support system based on concept of symbiotic computing, focusing on design and initial prototype implementation of the system.",https://ieeexplore.ieee.org/document/5250777/,2009 8th IEEE International Conference on Cognitive Informatics,15-17 June 2009,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ITT51279.2020.9320781,Using Deep Learning And Machine Learning In Space Network,IEEE,Conferences,"The UAE have achieved the goal to be a country to host its potential satellites in the space i.e. Dubaiset1, Khalifa set 1 and others by using the technologies like deep space navigation, autonomous satellite and planetary spectrum generator. But, these technologies are currently dependent on the command center which are manually operated. Thus, increasing the chance of error. Due to this, the demand of software which integrate deep learning and machine learning in the commanding centers are coming under high demand. This will make the probability of an error to a very minute percentage. The Artificial Intelligence which will be used in the Planetary Spectrum Generator and autonomous satellite to create a safe passage of communication directly from the rovers on different planets and satellites in the orbit to the command center without any human interference. The software will work with deep learning which currently all the space stations work with and will combine it with machine learning to make it into a complete network of different data collected from different satellites into the planetary spectrum generator(PSG) and thus giving the correct information with less time and perfect efficiency.",https://ieeexplore.ieee.org/document/9320781/,2020 Seventh International Conference on Information Technology Trends (ITT),25-26 Nov. 2020,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCW.2019.8756759,Using Deep Q-Learning to Prolong the Lifetime of Correlated Internet of Things Devices,IEEE,Conferences,"Battery-powered sensors deployed in the Internet of Things (IoT) require energy-efficient solutions to prolong their lifetime. When these sensors observe a physical phenomenon distributed in space and evolving in time, the collected observations are expected to be correlated. In this paper, we propose an updating mechanism leveraging Reinforcement Learning (RL) to take advantage of the exhibited correlation in the information collected. We implement the proposed updating mechanism employing deep Q-learning. Our mechanism is capable of learning the correlation in the information collected and determine the frequency with which sensors should transmit their updates, while taking into consideration a highly dynamic environment. We evaluate our solution using environmental observations, namely temperature and humidity, obtained in a real deployment. We demonstrate that our mechanism is capable of adapting the transmission frequency of sensors' updates according to the ever-changing environment. We show that our proposed mechanism is capable of significantly extending battery-powered sensors' lifetime without compromising the accuracy of the observations provided to the IoT service.",https://ieeexplore.ieee.org/document/8756759/,2019 IEEE International Conference on Communications Workshops (ICC Workshops),20-24 May 2019,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/Indo-TaiwanICAN48429.2020.9181327,Using Quantization-Aware Training Technique with Post-Training Fine-Tuning Quantization to Implement a MobileNet Hardware Accelerator,IEEE,Conferences,"In recent years, the internet of things (IoT) has been developed near the public's life circle. At the edge device, for real-time data analysis of data, a lightweight deep learning neural network (DNN) is required. In this paper, the lightweight model MobileNet is used to design an energy efficiency hardware accelerator at the edge device. In the software framework (Tensorflow), the quantization-aware training technique with post-training fine-tuning quantization is applied to quantize the model to improve training convergence speed and parameter minimization. In hardware design considerations, fixed-point operations can reduce computational complexity and memory storage space as compared to floating-point operations, which directly affects the power consumption of the circuit. The proposed MobileNet hardware accelerator can achieve low power consumption and is suitable for the edge devices.",https://ieeexplore.ieee.org/document/9181327/,"2020 Indo – Taiwan 2nd International Conference on Computing, Analytics and Networks (Indo-Taiwan ICAN)",7-15 Feb. 2020,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICST.2017.20,Using Semantic Similarity in Crawling-Based Web Application Testing,IEEE,Conferences,"To automatically test web applications, crawling-based techniques are usually adopted to mine the behavior models, explore the state spaces or detect the violated invariants of the applications. However, their broad use is limited by the required manual configurations for input value selection, GUI state comparison and clickable detection. In existing crawlers, the configurations are usually string-matching based rules looking for tags or attributes of DOM elements, and often application-specific. Moreover, in input topic identification, it can be difficult to determine which rule suggests a better match when several rules match an input field to more than one topic. This paper presents a natural-language approach based on semantic similarity to address the above issues. The proposed approach represents DOM elements as vectors in a vector space formed by the words used in the elements. The topics of encountered input fields during crawling can then be inferred by their similarities with ones in a labeled corpus. Semantic similarity can also be applied to suggest if a GUI state is newly discovered and a DOM element is clickable under an unsupervised learning paradigm. We evaluated the proposed approach in input topic identification with 100 real-world forms and GUI state comparison with real data from industry. Our evaluation shows that the proposed approach has comparable or better performance to the conventional techniques. Experiments in input topic identification also show that the accuracy of the rule-based approach can be improved by up to 22% when integrated with our approach.",https://ieeexplore.ieee.org/document/7927970/,"2017 IEEE International Conference on Software Testing, Verification and Validation (ICST)",13-17 March 2017,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IOTSMS52051.2020.9340189,Using Siamese Networks to Detect Shading on the Edge of Solar Farms,IEEE,Conferences,"Solar power is one of the most promising sources of green power for future cities. However, real-time anomaly detection remains a challenge. Internet of Things (IoT) is an effective platform for real-time monitoring of large-scale solar farms. Using low-cost edge devices such as the Raspberry Pi (RPI), it is possible to not only read power and irradiance values from in-situ sensors, but to also apply machine learning and deep learning algorithms for real-time analysis and for detecting anomalous behaviors. This paper presents the design and implementation of an edge analytics application that uses RPI as an edge device. The Isolation Forest algorithm was first used to detect shading anomalies. A Siamese neural network was then trained to create a latent-space mapping. An anomaly detection model based on the latent space and a neural network and kNN was developed. These models could detect shading anomalies with an F1-Score of 0.94. Embedded variants of the model based on TensorFlow Lite and TensorRT were evaluated to service a large number of solar panels at 1Hz. The results are that a single RPI could do parallel anomaly detection of 512 solar panels at 1 Hz with 0% failures. The TensorRT variant consumed more resources than the TensorFlow Lite implementation, but the maximum CPU utilization remained below 75%.",https://ieeexplore.ieee.org/document/9340189/,"2020 7th International Conference on Internet of Things: Systems, Management and Security (IOTSMS)",14-16 Dec. 2020,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.23919/MIPRO.2019.8756928,Utilizing Apple’s ARKit 2.0 for Augmented Reality Application Development,IEEE,Conferences,"When it comes to practical augmented reality applications, mobile platform tools are the most deserving. Thanks to the nature of mobile devices and their everyday usage, the ideal basis for this kind of content has inadvertently formed itself. Consequently, within the iOS development environment, Apple's Xcode program enables application development using the ARKit library which delivers a host of benefits. Amongst the plethora of advantages, this paper focuses on utilizing features such as the ability to measure distances between two points in space, horizontal and vertical plane detection, the ability to detect three-dimensional objects and utilize them as triggers, and the consolidated implementation of ARKit and MapKit libraries in conjunction with the Google Places API intended for displaying superimposed computer-generated content on iOS 11 and later iterations of Apple's mobile operating system.",https://ieeexplore.ieee.org/document/8756928/,"2019 42nd International Convention on Information and Communication Technology, Electronics and Microelectronics (MIPRO)",20-24 May 2019,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AERO.2015.7119180,Utilizing Artificial Intelligence to achieve a robust architecture for future robotic spacecraft,IEEE,Conferences,"This paper presents a novel failure-tolerant architecture for future robotic spacecraft. It is based on the Time and Space Partitioning (TSP) principle as well as a combination of Artificial Intelligence (AI) and traditional concepts for system failure detection, isolation and recovery (FDIR). Contrary to classic payload that is separated from the platform, robotic devices attached onto a satellite become an integral part of the spacecraft itself. Hence, the robot needs to be integrated into the overall satellite FDIR concept in order to prevent fatal damage upon hardware or software failure. In addition, complex dexterous manipulators as required for onorbit servicing (OOS) tasks may reach unexpected failure states, where classic FDIR methods reach the edge of their capabilities with respect to successfully detecting and resolving them. Combining, and partly replacing traditional methods with flexible AI approaches aims to yield a control environment that features increased robustness, safety and reliability for space robots. The developed architecture is based on a modular on-board operational framework that features deterministic partition scheduling, an OS abstraction layer and a middleware for standardized inter-component and external communication. The supervisor (SUV) concept is utilized for exception and health management as well as deterministic system control and error management. In addition, a Kohonen self-organizing map (SOM) approach was implemented yielding a real-time robot sensor confidence analysis and failure detection. The SOM features nonsupervized training given a typical set of defined world states. By compiling a set of reviewable three-dimensional maps, alternative strategies in case of a failure can be found, increasing operational robustness. As demonstrator, a satellite simulator was set up featuring a client satellite that is to be captured by a servicing satellite with a 7-DoF dexterous manipulator. The avionics and robot control were integrated on an embedded, space-qualified Airbus e.Cube on-board computer. The experiments showed that the integration of SOM for robot failure detection positively complemented the capabilities of traditional FDIR methods.",https://ieeexplore.ieee.org/document/7119180/,2015 IEEE Aerospace Conference,7-14 March 2015,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AERO47225.2020.9172453,Utilizing Reinforcement Learning to Autonomously Mange Buffers in a Delay Tolerant Network Node,IEEE,Conferences,"In order to effectively communicate with Earth from deep space there is a need for network automation similar to that of the Internet. The existing automated network protocols, such as TCP and IP, cannot work in deep space due to the assumptions under which they were designed. Specifically, protocols assume the existence of an end-to-end path between the source and destination for the entirety of a communication session and the path being traversable in a negligible amount of time. In contrast, a Delay Tolerant Network is a set of protocols that allows networking in environments where links suffer from high-delay or disruptions (e.g. Deep Space). These protocols rely on different assumptions such as time synchronization and suitable memory allocation. In this paper, we consider the problem of autonomously avoiding memory overflows in a Delay Tolerant Node. To that end, we propose using Reinforcement Learning to automate buffer management given that we can easily measure the relative rates of data coming in and out of the DTN node. In the case of detecting overflow, we let the autonomous agent choose between three actions: slowing down the client, requesting more resources from the Deep Space Network, or selectively dropping packets once the buffer nears capacity. Furthermore, we show that all of these actions can be realistically implemented in real-life operations given current and planned capabilities of Delay Tolerant Networking and the Deep Space Network. Similarly, we also show that using Reinforcement Learning for this problem is well suited to this application due to the number of possible states and variables, as well as the fact that large distances between deep space spacecraft and Earth prevent human-in-the-loop intervention.",https://ieeexplore.ieee.org/document/9172453/,2020 IEEE Aerospace Conference,7-14 March 2020,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCV48922.2021.00239,V-DESIRR: Very Fast Deep Embedded Single Image Reflection Removal,IEEE,Conferences,"Real world images often gets corrupted due to unwanted reflections and their removal is highly desirable. A major share of such images originate from smart phone cameras capable of very high resolution captures. Most of the existing methods either focus on restoration quality by compromising on processing speed and memory requirements or, focus on removing reflections at very low resolutions, there by limiting their practical deploy-ability. We propose a light weight deep learning model for reflection removal using a novel scale space architecture. Our method processes the corrupted image in two stages, a Low Scale Sub-network (LSSNet) to process the lowest scale and a Progressive Inference (PI) stage to process all the higher scales. In order to reduce the computational complexity, the sub-networks in PI stage are designed to be much shallower than LSSNet. Moreover, we employ weight sharing between various scales within the PI stage to limit the model size. This also allows our method to generalize to very high resolutions without explicit retraining. Our method is superior both qualitatively and quantitatively compared to the state of the art methods and at the same time 20&#x00D7; faster with 50&#x00D7; less number of parameters compared to the most recent state-of-the-art algorithm RAGNet. We implemented our method on an android smart phone, where a high resolution 12 MP image is restored in under 5 seconds.",https://ieeexplore.ieee.org/document/9710425/,2021 IEEE/CVF International Conference on Computer Vision (ICCV),10-17 Oct. 2021,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA.2019.8793556,VPE: Variational Policy Embedding for Transfer Reinforcement Learning,IEEE,Conferences,"Reinforcement Learning methods are capable of solving complex problems, but resulting policies might perform poorly in environments that are even slightly different. In robotics especially, training and deployment conditions often vary and data collection is expensive, making retraining undesirable. Simulation training allows for feasible training times, but on the other hand suffer from a reality-gap when applied in real-world settings. This raises the need of efficient adaptation of policies acting in new environments.We consider the problem of transferring knowledge within a family of similar Markov decision processes. We assume that Q-functions are generated by some low-dimensional latent variable. Given such a Q-function, we can find a master policy that can adapt given different values of this latent variable. Our method learns both the generative mapping and an approximate posterior of the latent variables, enabling identification of policies for new tasks by searching only in the latent space, rather than the space of all policies. The low-dimensional space, and master policy found by our method enables policies to quickly adapt to new environments. We demonstrate the method on both a pendulum swing-up task in simulation, and for simulation-to-real transfer on a pushing task.",https://ieeexplore.ieee.org/document/8793556/,2019 International Conference on Robotics and Automation (ICRA),20-24 May 2019,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICPR48806.2021.9412896,Variational Inference with Latent Space Quantization for Adversarial Resilience,IEEE,Conferences,"Despite their tremendous success in modelling high-dimensional data manifolds, deep neural networks suffer from the threat of adversarial attacks - Existence of perceptually valid input-like samples obtained through careful perturbation that lead to degradation in the performance of the underlying model. Major concerns with existing defense mechanisms include non-generalizability across different attacks, models and large inference time. In this paper, we propose a generalized defense mechanism capitalizing on the expressive power of regularized latent space based generative models. We design an adversarial filter, devoid of access to classifier and adversaries, which makes it usable in tandem with any classifier. The basic idea is to learn a Lipschitz constrained mapping from the data manifold, incorporating adversarial perturbations, to a quantized latent space and re-map it to the true data manifold. Specifically, we simultaneously auto-encode the data manifold and its perturbations implicitly through the perturbations of the regularized and quantized generative latent space, realized using variational inference. We demonstrate the efficacy of the proposed formulation in providing resilience against multiple attack types (black and white box) and methods, while being almost real-time. Our experiments show that the proposed method surpasses the state-of-the-art techniques in several cases. The implementation code is available at - https://github.com/mayank31398/lqvae.",https://ieeexplore.ieee.org/document/9412896/,2020 25th International Conference on Pattern Recognition (ICPR),10-15 Jan. 2021,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/BigData52589.2021.9671727,Variational Open Set Recognition (VOSR),IEEE,Conferences,"Open set recognition models address the real-world scenario where classes of data unobserved during training are encountered in testing after deployment. Closed set classifiers wrongly attempt to classify instances from an unknown class as belonging to one of the known classes from the training set, which reduces the model’s accuracy. Ideally, these unknown instances should be recognized as such, while known instances should continue to be accurately classified. Unfortunately, state-of-the-art open set methods solve this problem by making restrictive assumptions on the variance and/or boundedness of the distributions of known classes. In this paper, we propose a novel method, Variational Open-Set Recognition (VOSR) that eliminates these assumptions. VOSR incorporates a closed set classifier, an unknown detector, and a novel Structured Gaussian Mixture Variational Autoencoder (SGM-VAE) that guarantees separable class distributions with known variances in its la-tent space. Further, by encouraging a large distance between class-specific distributions, VOSR increases the likelihood that instances from unknown classes lie in low-probability regions and thus are more readily identifiable. In rigorous evaluation, we demonstrate that VOSR outperforms state-of-the-art open set classifiers with up to a 14% F1 score increase in identifying instances from unknown classes in multiple image classification and human activity recognition datasets.",https://ieeexplore.ieee.org/document/9671727/,2021 IEEE International Conference on Big Data (Big Data),15-18 Dec. 2021,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCV.2005.246,Vector boosting for rotation invariant multi-view face detection,IEEE,Conferences,"In this paper, we propose a novel tree-structured multiview face detector (MVFD), which adopts the coarse-to-fine strategy to divide the entire face space into smaller and smaller subspaces. For this purpose, a newly extended boosting algorithm named vector boosting is developed to train the predictors for the branching nodes of the tree that have multicomponents outputs as vectors. Our MVFD covers a large range of the face space, say, +/-45/spl deg/ rotation in plane (RIP) and +/-90/spl deg/ rotation off plane (ROP), and achieves high accuracy and amazing speed (about 40 ms per frame on a 320 /spl times/ 240 video sequence) compared with previous published works. As a result, by simply rotating the detector 90/spl deg/, 180/spl deg/ and 270/spl deg/, a rotation invariant (360/spl deg/ RIP) MVFD is implemented that achieves real time performance (11 fps on a 320 /spl times/ 240 video sequence) with high accuracy.",https://ieeexplore.ieee.org/document/1541289/,Tenth IEEE International Conference on Computer Vision (ICCV'05) Volume 1,17-21 Oct. 2005,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SISY50555.2020.9217076,Vehicle Control in Highway Traffic by Using Reinforcement Learning and Microscopic Traffic Simulation,IEEE,Conferences,"The paper presents a simple yet powerful and intelligent driver agent, designed to operate in a preset highway situation using Policy Gradient Reinforcement Learning (RL) agent. The goal is to navigate safely in dense highway traffic and proceed through the defined length with the shortest time possible. The algorithm uses a dense neural network as a function approximator for the agent with discrete action space on the control level, e.g., acceleration and steering. The developed simulation environment uses the open-source traffic simulator called Simulation of Urban MObility (SUMO), integrated with an interface, to interact with the agent in real-time. With this tool, numerous driving and highway situations can be created and fed to the agent from which it can learn. The environment opens the opportunity to randomize and customize the other road users' behavior. Thus the experience can be more diverse, and thus the representation becomes more general. The article describes the modeling environment, the details on the learning agent, and the rewarding scheme. After evaluating the experiences gained from the training, some ideas for optimization and further development goals are also proposed.",https://ieeexplore.ieee.org/document/9217076/,2020 IEEE 18th International Symposium on Intelligent Systems and Informatics (SISY),17-19 Sept. 2020,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SAMI.2016.7422985,Vehicle navigation by fuzzy cognitive maps using sonar and RFID technologies,IEEE,Conferences,"Emerging concept of the so-called intelligent space (IS) offers means for use of mobile autonomous devices like vehicles or robots in a very broad area without necessity for these devices to own all necessary sensors. From this reason also new navigation methods are developing, which utilize IS means, with the aim to offer maybe not so accurate but first of all cheep and reliable solutions for a wide variety of devices. Our paper deals with the examination of possibility to interconnect sparsely deployed RFID tags with sonars. As signals produced by these two technologies are often affected by uncertainty and incompleteness we use fuzzy logic for their processing as well as control of the entire navigation process. For this purpose a special type of a fuzzy cognitive map was proposed. The paper describes real navigation experiments with a simple vehicle and evaluates them by selected criteria. Based on obtained results their explanations and conclusions for potential future research are sketched.",https://ieeexplore.ieee.org/document/7422985/,2016 IEEE 14th International Symposium on Applied Machine Intelligence and Informatics (SAMI),21-23 Jan. 2016,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1049/icp.2022.0587,Violence detection for surveillance systems using lightweight CNN models,IET,Conferences,"Due to significant safety concerns, today, many places are filled with CCTV cameras, but it still uses manual security to overlook these, which leaves much space for human error, be it negligence or a dangerous situation. This may result in many anomalies such as violence, hostage, or fires. To prevent this, an intelligent and automated system has been implemented that tries to overcome this using deep learning techniques. Using Deep Learning models utilizing Convolutional Neural Networks (CNN), specifically the lightweight models - MobileNetV2 and ResNet50V2 are used to identify threat movements in the given frames over three datasets, namely UCF crime, Real life violence situations, and UBI-fights. The violence in the video is detected using frames, and accuracy is measured. A threat is detected by the system from the video frame, based on which the system for the situation generates an alert. In extreme cases, alerts are sent directly to nearby police stations and emergency services with accurate location information while also indicating the suspicious activities at an instance of time.",https://ieeexplore.ieee.org/document/9800155/,7th International Conference on Computing in Engineering & Technology (ICCET 2022),12-13 Feb. 2022,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICAIS53314.2022.9742880,Virtual Design Method of Indoor Space Based on 3D Visual Cycle Modeling,IEEE,Conferences,"Aiming at the problem that traditional virtual design of living room space has always been unsatisfactory, it cannot achieve both beautification and practicality, and proposes a virtual design method of living room space based on three-dimensional vision. According to the overall design effect of the room, on the basis of image and color processing algorithm design, based on virtual reality and visual simulation technology, carry out the development and design of distributed 3D interior design system, and use 3ds MAX to carry out the 3D construction of distributed 3D interior design. Model, realize the indoor hierarchical structure design on Multigen Creator modeling software. The test results show that the visual effect of distributed 3D interior design using this method is better, and the feature expression ability is increased by 7.6&#x0025;.",https://ieeexplore.ieee.org/document/9742880/,2022 Second International Conference on Artificial Intelligence and Smart Energy (ICAIS),23-25 Feb. 2022,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISCID.2015.303,Virtual Resource Scheduling Prediction Based on a Support Vector Machine in Cloud Computing,IEEE,Conferences,"In this study, a virtual resource scheduling prediction algorithm based on a support vector machine (SVM) is proposed to handle the complex, dynamic, changing environment of the cloud platform. First, virtual resource sequences were reconstructed by reconstructing the phase space. Then, the reconstructed virtual resource sequences were used as inputs into an SVM for training and predicting. Finally, a prediction experiment was conducted using actual virtual resource data. The experimental results showed that SVM improved the prediction accuracy and stability of the virtual resource, in addition, the SVM could satisfy the real-time performance and high-accuracy requirements of virtual resource prediction.",https://ieeexplore.ieee.org/document/7468910/,2015 8th International Symposium on Computational Intelligence and Design (ISCID),12-13 Dec. 2015,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ACC.2006.1657291,Virtual air-fuel ratio sensors for engine control and diagnostics,IEEE,Conferences,"Virtual air-fuel ratio sensors for an internal combustion engine using recurrent neural and wavelet networks have been developed. A nonlinear state-space modeling strategy is proposed for the architecture of the stated recurrent neural network which is trained using some variants of real time recurrent learning (RTRL) algorithm. A two-stage training approach is proposed for improving the accuracy of the RNN topology. Additionally, wavelets as activation functions have been employed to construct a single-layer network called wavenet. The wavenet is used to model the exhaust air-fuel ratio that has proved a more challenging task in a purely neural net-based architecture using sigmoid activation functions. The methodology has been implemented in a V8 spark ignition engine through rapid prototyping tools for the real time generalization and performance evaluation. Observations and comments are made on the test patterns used for the training. Some of the limitations of such a data driven approach are highlighted. Representative experimental results for the 8-cylinder engine test data are listed. The virtual sensor may be used for more precise average air-fuel ratio control and enhanced reliability engendered through the diagnostic capabilities of the sensor",https://ieeexplore.ieee.org/document/1657291/,2006 American Control Conference,14-16 June 2006,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ROBOT.1995.525277,Vision-based reinforcement learning for purposive behavior acquisition,IEEE,Conferences,"This paper presents a method of vision-based reinforcement learning by which a robot learns to shoot a ball into a goal, and discusses several issues in applying the reinforcement learning method to a real robot with vision sensor. First, a ""state-action deviation"" problem is found as a form of perceptual aliasing in constructing the state and action spaces that reflect the outputs from physical sensors and actuators, respectively. To cope with this, an action set is constructed in such a way that one action consists of a series of the same action primitive which is successively executed until the current state changes. Next, to speed up the learning time, a mechanism of learning form easy missions (or LEM) which is a similar technique to ""shaping"" in animal learning is implemented. LEM reduces the learning time from the exponential order in the size of the state space to about the linear order in the size of the state space. The results of computer simulations and real robot experiments are given.",https://ieeexplore.ieee.org/document/525277/,Proceedings of 1995 IEEE International Conference on Robotics and Automation,21-27 May 1995,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/MASCON51689.2021.9563527,Visual Perception Stack for Autonomous Vehicle Using Semantic Segmentation and Object Detection,IEEE,Conferences,"An autonomous vehicle requires a reliable system for high vehicle precision and relative estimation of its state for the safety of humans during the autonomous movement of the vehicle in an environment dominated by human drivers. Such systems have a complex environment involving multiple sensors (e.g. Vision modules, Global Navigation Satellite System (GNSS), LIDAR, RADAR). Through this paper, environment perception stack for self-driving cars is proposed to improve the intelligence for decision making and improve the safety measures. Semantic image segmentation, based on Fully convolutional Network architecture is implemented and the output received from the model is then used for implementing 3D space estimation and lane estimation. Considering the real-time cooperation required between the autonomous vehicles and other vehicles in the frame, a 2D object detector is implemented on the stack to detect different classes of objects and their relative distances are calculated. The proposed system is then implemented on the CARLA simulation software and generated outcomes are further discussed in the paper.",https://ieeexplore.ieee.org/document/9563527/,2021 IEEE Madras Section Conference (MASCON),27-28 Aug. 2021,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/FSKD.2017.8393254,Visual control system design of wheeled inverted pendulum robot based on Beaglebone Black,IEEE,Conferences,"The wheeled inverted pendulum robot has broad prospects of applications in real life. It can use two coaxial wheels to achieve the body self-balancing, forward moving and turning. But the general wheeled inverted pendulum robot seldom has vision function to perceive enviromental change. In order to realize the robust visual control, a wheeled inverted-pendulum vision robot with attitude sensors, photoelectric encoders, ultrasonic sensors and so on is designed based on Beaglebone Black board. The moving object is separated in the space domain by obtaining the image sequence which is sent by a robot-mounted camera, and the modeling, identification and tracking of target sequence are implemented in the time domain. The balance PD, speed PI and steering PD controllers are designed to realize the dynamic balance, forward and steering function of the robot. To satisfy the functional requirements of the visual tracking system, an improved tracking-learning-detection algorithm based on kernelized correlation filtering is used, and a tracking anomaly based on spatial context is detected to determine the tracking state and reduce the error rate. Experimental results show that the robot reaches the requirement of design and achieves better visual control effectiveness.",https://ieeexplore.ieee.org/document/8393254/,"2017 13th International Conference on Natural Computation, Fuzzy Systems and Knowledge Discovery (ICNC-FSKD)",29-31 July 2017,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICIAP.2003.1234077,Visual self-localisation using automatic topology construction,IEEE,Conferences,"The paper proposes a machine learning method for self-localising a mobile agent, using the images supplied by a single omni-directional camera. The images acquired by the camera may be viewed as an implicit topological representation of the environment. The environment is a priori unknown and the topological representation is derived by unsupervised neural network architecture. The architecture includes a self-organising neural network, and is constituted by a growing neural gas, which is well known for its topology preserving quality. The growth depends on the topology that is not a priori defined, and on the need of discovering it, by the neural network, during the learning. The implemented system is able to recognise correctly the input frames and to reconstruct a topological map of the environment. Each node of the neural network identifies a single zone of the environment and the connections between the nodes correspond to the real space connections in the environment.",https://ieeexplore.ieee.org/document/1234077/,"12th International Conference on Image Analysis and Processing, 2003.Proceedings.",17-19 Sept. 2003,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/INFOCOM41043.2020.9155483,Voiceprint Mimicry Attack Towards Speaker Verification System in Smart Home,IEEE,Conferences,"The advancement of voice controllable systems (VC-Ses) has dramatically affected our daily lifestyle and catalyzed the smart home's deployment. Currently, most VCSes exploit automatic speaker verification (ASV) to prevent various voice attacks (e.g., replay attack). In this study, we present VMask, a novel and practical voiceprint mimicry attack that could fool ASV in smart home and inject the malicious voice command disguised as a legitimate user. The key observation behind VMask is that the deep learning models utilized by ASV are vulnerable to the subtle perturbations in the voice input space. To generate these subtle perturbations, VMask leverages the idea of adversarial examples. Then by adding the subtle perturbations to the recordings from an arbitrary speaker, VMask can mislead the ASV into classifying the crafted speech samples, which mirror the former speaker for human, as the targeted victim. Moreover, psychoacoustic masking is employed to manipulate the adversarial perturbations under human perception threshold, thus making victim unaware of ongoing attacks. We validate the effectiveness of VMask by performing comprehensive experiments on both grey box (VGGVox) and black box (Microsoft Azure Speaker Verification) ASVs. Additionally, a real-world case study on Apple HomeKit proves the VMask's practicability on smart home platforms.",https://ieeexplore.ieee.org/document/9155483/,IEEE INFOCOM 2020 - IEEE Conference on Computer Communications,6-9 July 2020,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CIG.2018.8490413,Wall Building in the Game of StarCraft with Terrain Considerations,IEEE,Conferences,"StarCraft is a Real-Time Strategy game, which has a large state-space, is played in real-time, and commonly features two opposing players, capable of acting simultaneously. One of the aspects of the game is building walls. In this paper, we present an algorithm that can be used for wall building for an agent playing the game of StarCraft: Brood War.",https://ieeexplore.ieee.org/document/8490413/,2018 IEEE Conference on Computational Intelligence and Games (CIG),14-17 Aug. 2018,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SmartWorld.2018.00233,Water Level Estimation Based on Image of Staff Gauge in Smart City,IEEE,Conferences,"In the automatic measurement of water level, the hydrological satiation also uses a traditional staff gauge to calibrate the water depth measured by the sensor. And in the most of water-covered areas, such as underpass overpasses, low-lying roads and tunnels, staff gauges have been installed to monitor road water. Therefore, the automatic water level measurement method based on staff gauge image, has drawn more and more attention. So we propose a method for the water level estimation based on images of staff gauge in this paper. First, we proposed a two-step method for segmenting the staff gauge. We implemented the initial positioning by HSV color space and morphological processing, and then we defined a component map to achieve precise positioning. Next, we split the characters on the staff gauge image and recognized those characters through a Convolutional Neural Network. Finally, we measured the water level by using a quadratic function to determine the mapping relationship between pixels and the metric values. Compared with the traditional method of extracting water line, the two-step positioning method improves the water level detection accuracy. The measurement result in a real world image of a pond shows more accurate detection with the algorithm and the feasibility of applying it to road water monitoring.",https://ieeexplore.ieee.org/document/8560211/,"2018 IEEE SmartWorld, Ubiquitous Intelligence & Computing, Advanced & Trusted Computing, Scalable Computing & Communications, Cloud & Big Data Computing, Internet of People and Smart City Innovation (SmartWorld/SCALCOM/UIC/ATC/CBDCom/IOP/SCI)",8-12 Oct. 2018,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/PerCom53586.2022.9762400,Weight Separation for Memory-Efficient and Accurate Deep Multitask Learning,IEEE,Conferences,"We propose a new concept called Weight Separation of deep neural networks (DNNs), which enables memory-efficient and accurate deep multitask learning on a memory-constrained embedded system. The goal of weight separation is to achieve extreme packing of multiple heterogeneous DNNs into the limited memory of the system while ensuring the prediction accuracy of the constituent DNNs at the same time. The proposed approach separates the DNN weights into two types of weight-pages consisting of a subset of weight parameters, i.e., shared and exclusive weight-pages. It optimally distributes the weight-pages into two levels of the system memory hierarchy and stores them separately, i.e., the shared weight-pages in primary (level-1) memory (e.g., RAM) and the exclusive weight-pages in secondary (level-2) memory (e.g., flask disk or SSD). First, to reduce the memory usage of multiple DNNs, less critical weight parameters are identified and overlapped onto the shared weight-pages that are deployed in the limited space of the primary (main) memory. Next, to retain the prediction accuracy of multiple DNNs, the essential weight parameters that play a critical role in preserving prediction accuracy are stored intact in the plentiful space of secondary memory storage in the form of exclusive weight-pages without overlapping. We implement two real systems applying the proposed weight separation: 1) a microcontroller-based multitask IoT system that performs multitask learning of 10 scaled-down DNNs by separating the weight parameters into FRAM and flash disk, and 2) an embedded GPU system that performs multitask learning of 10 state-of-the-art DNNs, separating the weight parameters into GPU RAM and eMMC. Our evaluation shows that memory efficiency, prediction accuracy, and execution time of deep multitask learning improve up to 5.9x, 2.0%, and 13.1x, respectively, without any modification of DNN models.",https://ieeexplore.ieee.org/document/9762400/,2022 IEEE International Conference on Pervasive Computing and Communications (PerCom),21-25 March 2022,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ETFA.1993.396414,Well suited modelling and evaluation techniques based on GSPN for real production systems,IEEE,Conferences,Modeling of production systems used in automobile factories with behavior which is complicated by failure occurrence and repair or maintenance actions is reported. The method is based on generalized stochastic Petri nets (GSPN) which are well-suited for complex system evaluation. To reduce the state space complexity an equivalent model is proposed for the production lines. It permits addressing the problem of production management and the enhancement of the production rate by the addition of buffers. A software package which aids in the creation of models from a module library is developed.<>,https://ieeexplore.ieee.org/document/396414/,Proceedings of IEEE 2nd International Workshop on Emerging Technologies and Factory Automation (ETFA '93),27-29 Sept. 1993,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CVPR.2011.5995702,What you saw is not what you get: Domain adaptation using asymmetric kernel transforms,IEEE,Conferences,"In real-world applications, “what you saw” during training is often not “what you get” during deployment: the distribution and even the type and dimensionality of features can change from one dataset to the next. In this paper, we address the problem of visual domain adaptation for transferring object models from one dataset or visual domain to another. We introduce ARC-t, a flexible model for supervised learning of non-linear transformations between domains. Our method is based on a novel theoretical result demonstrating that such transformations can be learned in kernel space. Unlike existing work, our model is not restricted to symmetric transformations, nor to features of the same type and dimensionality, making it applicable to a significantly wider set of adaptation scenarios than previous methods. Furthermore, the method can be applied to categories that were not available during training. We demonstrate the ability of our method to adapt object recognition models under a variety of situations, such as differing imaging conditions, feature types and codebooks.",https://ieeexplore.ieee.org/document/5995702/,CVPR 2011,20-25 June 2011,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/LCNW.2014.6927697,Where is that car parked? A wireless sensor network-based approach to detect car positions,IEEE,Conferences,"The global trend of increased urbanization makes space rare in city environments in general and for parking in particular. In addition, cars become bigger and often use more than one parking space. As a result neighboring parking spaces can be affected by a parked car. So, a basically free parking space might be too narrow for an arriving car depending on the arriving car's size. Therefore, means to detect car positions on parking spaces in a fine granular way are required to detect such situations and avoid inefficient parking space searches. Wireless sensor networks provide the possibility to sense the exact occupation of a parking space and potential influences on neighboring parking spaces. However, current solutions focus only on the detection if a parking space is occupied or not. In our work, we present a sensor deployment and a machine learning-based approach able to provide the mentioned more fine-granular detection level. We have conducted an extensive real-world evaluation of our solution, in particular considering different characteristics of today's car bodies. In our tests, our approach achieved an accuracy of more than 98%.",https://ieeexplore.ieee.org/document/6927697/,39th Annual IEEE Conference on Local Computer Networks Workshops,8-11 Sept. 2014,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICC.2018.8422973,WiParkFind: Finding Empty Parking Slots Using WiFi,IEEE,Conferences,"With ever increasing number of vehicles, shortage of parking space is becoming a serious problem. Going to shopping, school, and workplace can be a headache as finding an available parking spot is getting harder causing wasted time and gas. In this paper, we present WiParkFind: a low-cost, non-intrusive, and real- time parking occupancy monitoring system based on WiFi signals. The channel state information (CSI) of received WiFi signals is analyzed by using a machine learning technique to capture distinctive characteristics of CSI data that are strongly correlated with the number of empty parking slots in order to detect whether there is an empty slot, and how many empty slots are available. Compared with contemporary approaches based on magnetic sensors deployed on individual parking slots, WiParkFind utilizes low-cost off- the-shelf WiFi devices, dramatically reducing the cost for purchasing, installing, and maintaining a large number of sensors, and backend server systems. A proof-of-concept system of WiParkFind was developed and deployed in a department parking lot. The results demonstrate that the average classification accuracy of WiParkFind over a week of data collection is 78.2%, and the accuracy is improved to 90.8% with a tolerance of one empty slot.",https://ieeexplore.ieee.org/document/8422973/,2018 IEEE International Conference on Communications (ICC),20-24 May 2018,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1145/3125503.3125568,Work-in-progress: testing autonomous cyber-physical systems using fuzzing features from convolutional neural networks,IEEE,Conferences,Autonomous cyber-physical systems rely on modern machine learning methods such as deep neural networks to control their interactions with the physical world. Testing of such intelligent cyberphysical systems is a challenge due to the huge state space associated with high-resolution visual sensory inputs. We demonstrate how fuzzing the input using patterns obtained from the convolutional filters of an unrelated convolutional neural network can be used to test computer vision algorithms implemented in intelligent cyber-physical systems. Our method discovers interesting counterexamples to a pedestrian detection algorithm implemented in the popular OpenCV library. Our approach also unearths counterexamples to the correct behavior of an autonomous car similar to NVIDIA's end-to-end self-driving deep neural net running on the Udacity open-source simulator.,https://ieeexplore.ieee.org/document/8094374/,2017 International Conference on Embedded Software (EMSOFT),15-20 Oct. 2017,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ASE.2019.00077,Wuji: Automatic Online Combat Game Testing Using Evolutionary Deep Reinforcement Learning,IEEE,Conferences,"Game testing has been long recognized as a notoriously challenging task, which mainly relies on manual playing and scripting based testing in game industry. Even until recently, automated game testing still remains to be largely untouched niche. A key challenge is that game testing often requires to play the game as a sequential decision process. A bug may only be triggered until completing certain difficult intermediate tasks, which requires a certain level of intelligence. The recent success of deep reinforcement learning (DRL) sheds light on advancing automated game testing, without human competitive intelligent support. However, the existing DRLs mostly focus on winning the game rather than game testing. To bridge the gap, in this paper, we first perform an in-depth analysis of 1349 real bugs from four real-world commercial game products. Based on this, we propose four oracles to support automated game testing, and further propose Wuji, an on-the-fly game testing framework, which leverages evolutionary algorithms, DRL and multi-objective optimization to perform automatic game testing. Wuji balances between winning the game and exploring the space of the game. Winning the game allows the agent to make progress in the game, while space exploration increases the possibility of discovering bugs. We conduct a large-scale evaluation on a simple game and two popular commercial games. The results demonstrate the effectiveness of Wuji in exploring space and detecting bugs. Moreover, Wuji found 3 previously unknown bugs, which have been confirmed by the developers, in the commercial games.",https://ieeexplore.ieee.org/document/8952543/,2019 34th IEEE/ACM International Conference on Automated Software Engineering (ASE),11-15 Nov. 2019,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CSCWD49262.2021.9437611,Zero-Shot Learning Based on Knowledge Sharing,IEEE,Conferences,"Zero-Shot Learning (ZSL) is an emerging research that aims to solve the classification problems with very few training data. The present works on ZSL mainly focus on the mapping of learning semantic space to visual space. It encounters many challenges that obstruct the progress of ZSL research. First, the representation of the semantic feature is inadequate to represent all features of the categories. Second, the domain shift problem still exists during the transfer from semantic space to visual space. In this paper, we introduce knowledge sharing (KS) to enrich the representation of semantic features. Based on KS, we apply a generative adversarial network to generate pseudo visual features from semantic features that are very close to the real visual features. Abundant experimental results from two benchmark datasets of ZSL show that the proposed approach has a consistent improvement.",https://ieeexplore.ieee.org/document/9437611/,2021 IEEE 24th International Conference on Computer Supported Cooperative Work in Design (CSCWD),5-7 May 2021,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCSEE.2012.81,[Title page i - Volume 1],IEEE,Conferences,The following topics are dealt with: environmental cost internalization effect analysis; pricing decision theory; international carbon emission trading; device description technology; proportional-derivative controller; chaotic financial system; rolling bearing fault diagnosis; multichannel MAC protocol; 802.11-based wireless mesh networks; Uyghur broadcast news continues speech sensitive-word spotting system; viscoelastic ray tracing simulation; underground cavity stability; spacecraft attitude control; inertia matrix generalized mixed vector variational-like inequalities; stub tooth involute gears; expression recognition; BP neural network; file access security; CFD numerical simulation; exhaust gas flow pattern; hot galvanizing bath; grading steel ball; wear discrete element method; orbit determination; space-based optical observations; Adaboost blob tracking; parallel switching power supply module system; multisource heterogeneous data integration; DSP-based SVPWM vector control system; asynchronous motor scanned document image segmentation; Voronoi diagram; aircraft object recognition model; adaptive threshold Canny edge detection algorithm; data synchronization; open service-aware mobile network API; OAuth based authentication; Open Telco API; online fresh supermarket logistics delivery information system; image encryption algorithm; fast 1-DDCT algorithm; parallel computing; safety education system; virtual reality technology; water quality; aeronautical component repair business optimization; high-speed BiCMOS fully differential operational amplifier; cloud computing; enterprise information system; software development cost; Java multithread technology; wireless sensor network; NC remote video monitoring; military communication network; e-government affair system; particle swarm optimized wavelet neural network; face recognition; grass stem biomechanics; RFID system; Internet of Things; gyroscope test instrument rotating floor DSP control system; and AC synchronous generators.,https://ieeexplore.ieee.org/document/6187941/,2012 International Conference on Computer Science and Electronics Engineering,23-25 March 2012,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.23919/DATE54114.2022.9774559,coxHE: A software-hardware co-design framework for FPGA acceleration of homomorphic computation,IEEE,Conferences,"Data privacy becomes a crucial concern in the AI and big data era. Fully homomorphic encryption (FHE) is a promising data privacy protection technique where the entire computation is performed on encrypted data. However, the dramatic increase of the computation workload restrains the usage of FHE for the real-world applications. In this paper, we propose an FPFA accelerator design framework for CKKS-based HE. While the KeySwitch operations are the primary performance bottleneck of FHE computation, we propose a low latency design of KeySwitch module with reduced intra-operation data dependency. Compared with the state-of-the-art FPGA based key-switch implementation that is based on Verilog, the proposed high-level synthesis (HLS) based design reduces the operation latency by 40&#x0025;. Furthermore, we propose an automated design space exploration framework which generates optimal encryption parameters and accelerators for a given application kernel and the target FPGA device. Experimental results for a set of real HE application kernels on different FPGA devices show that our HLS-based flexible design framework produces substantially better accelerator design compared with a fixed-parameter HE accelerator in terms of security, approximation error, and overall performance.",https://ieeexplore.ieee.org/document/9774559/,"2022 Design, Automation & Test in Europe Conference & Exhibition (DATE)",14-23 March 2022,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AI-CSP52968.2021.9671151,i-DERASSA: e-learning Platform based on Augmented and Virtual Reality interaction for Education and Training,IEEE,Conferences,"With the Coronavirus 2019 disease (COVID-19) spread, causing a world pandemic, e-learning can provide an optimal education and training solution. COVID-19 has wholly disrupted the education system. Switching to e-learning could be the enabler to create a new, more effective method of educating students when correctly applied. This paper proposes an elearning platform based on 3D interaction using augmented reality (AR) and virtual reality (VR) designed to meet particular learning objectives divided into levels and subjects, which aims to facilitate the teaching process and administrative workload in schools. We provide details on the platform, VR and AR courses/exercises conception and implementation. Many concepts may be hard to explain in a classroom or visualise in a textbook, e.g., anatomical concepts, molecular structures, space phenomena, or complex abstract topics. The AR and VR technologies make it much easier to achieve, creating a rich, interactive experience that combines real and virtual worlds. VR that provides information through realistic 3D models in immersive environments can better present concepts and skills. Our main contribution is the integration of AR and VR interaction in the web; we develop courses/ exercises selected from the Algerian Ministry of Education and Teaching learning program based on 3D interaction in virtual and augmented scenes. We hope by the integration of VR and AR into education attempts to increase the level of participation and maybe even the understanding of abstract and complex concepts.",https://ieeexplore.ieee.org/document/9671151/,2021 International Conference on Artificial Intelligence for Cyber Security Systems and Privacy (AI-CSP),20-21 Nov. 2021,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/RCAR52367.2021.9517479,sEMG-based Gesture Recognition by Rotation Forest-Based Extreme Learning Machine,IEEE,Conferences,"The motion information contained in surface electromyography (sEMG) signals contributes significantly to the prosthetic hand control. However, the accuracy and speed of gesture recognition from sEMG signals are still insufficient for natural control. In order to alleviate this problem, this paper propose a rotation forest-based extreme learning machine method (RoF-ELM) to improve the recognition performance based on sEMG signals. Firstly, the active motion segments were picked out and pre-processed by sliding window. Then, 104 features were extracted from each sample, and SVM-RFE method was used to reduce the feature space dimension. Finally, the RoF-ELM classification model was constructed and tested based on the EMG data for gestures Data Set containing a total of 72 recordings, each of which consists of six basic gestures. 100 trials were implemented to validate the performance of the proposed method, the results show that the RoF-ELM method have the highest accuracy (91.11&#x0025;) across different subjects with relatively short runtime compared with decision tree (DT), ELM, random forest (RF), and RoF methods.",https://ieeexplore.ieee.org/document/9517479/,2021 IEEE International Conference on Real-time Computing and Robotics (RCAR),15-19 July 2021,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IDEA49133.2020.9170686,“Error back propagation based Recurrent Neural Networks for Intrusion Detection system”,IEEE,Conferences,"Internet usage is increasing daily, which adds to the system's vulnerability. Network security is always a major problem for network administrators and is constantly changing due to the additional application space and requirements of a smart and efficient network. Simple and more efficient software tools include victims on the security side of the protocol that hackers use to perform various types of attacks on the network. The purpose of this review is to establish and coordinate processes to prevent one member against new and known attacks, and to act as a separate security system or an independent network. The neural network connects the body's immune system to receive memory, errors, and synchronized learning. This paper discusses interrelated processes that are largely based on the application of artificial intelligence for intrusion detection and learning processes.Proposed approach finds the type of session i.e. either normal or intrusion where if intrusion found than class of intrusion was detected. Here artificial neural network was used for finding the patterns in the input data. In this work Back propagation is used for the ANN in recursive manner. Proposed algorithm gives an effective framework which has been a promising one for distinguishing interruption of various kind where, one can get the detail of the class of attack also. Experiment has been conduced on real data set where various set of testing data were pass for comparison on different evaluation parameters. Proposed approach detects all sort of attacks applied on the network such as DoS (Denial of service), (R2L) Remote to local, (U2R) User to remote, Probe etc. In this work a Random Forest Tree is used for the detection of intrusion in network. Proposed approach improved 6.87% accuracy, 12.06% Precision and 1.15% recall.",https://ieeexplore.ieee.org/document/9170686/,"2nd International Conference on Data, Engineering and Applications (IDEA)",28-29 Feb. 2020,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/FUZZY.2008.4630539,“Tell me the important stuff” - fuzzy ontologies and personal assessments for interaction with the semantic web,IEEE,Conferences,"The semantic Web attempts to make the Web an universal medium of data exchange. To achieve this it needs to both make data sources accessible to machine-based systems, such as software agents and allow humans to easily create, understand and search for these data sources. Crisp ontologies are extremely useful for improving data extraction from structured data. However many ontological approaches require an unnatural level of precision and rigidity when dealing with real user queries or data sources. Previous work has suggested that fuzzification of ontologies may increase their utility. A major area of activity in the search and ubiquitous computing space is the development of location aware services. This paper suggests that in the mobile and context-aware semantic Web environment, fuzzification needs to extend to the representation of the importance of particular. Additionally, mobile devices tend to require simple interfaces and work with low bandwidth, this implies that obtaining small numbers of relevant results is extremely important. The work previously done in representing uncertainty in geographical information systems may assist in this development. This paper suggest that the combination of the use of fuzzy ontologies and fuzzy spatial relations may be effective in increasing the usefulness of the mobile semantic Web.",https://ieeexplore.ieee.org/document/4630539/,2008 IEEE International Conference on Fuzzy Systems (IEEE World Congress on Computational Intelligence),1-6 June 2008,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy')
